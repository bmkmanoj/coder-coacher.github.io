<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Docker and ServiceFabric - A taste of two platforms - Michele Bustamante | Coder Coacher - Coaching Coders</title><meta content="Docker and ServiceFabric - A taste of two platforms - Michele Bustamante - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Docker and ServiceFabric - A taste of two platforms - Michele Bustamante</b></h2><h5 class="post__date">2016-09-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OIhC0AbKLsw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello oh I'm on Hey hi Sydney can I just
call you all Sydney would that be all
right
so welcome my name's Michelle we're here
for an hour
right and I'm thinking I'm gonna pack
two platforms and a quick review in an
hour are you ready for that
it's okay good all right so my name's
Michelle LaRue Bustamante I'm here from
San Diego and I'm part of a company
called salon so we do architecture
consulting and many other things but of
late you know I've actually been
building a docker and service fabric and
micro services practice so we've
actually been playing a lot with both of
the platforms and just finished a
hackathon on service fabric which was
really fun and I think it'd be great to
just have a little bit of an open
dialogue while I show you some demos and
talk a little bit about I guess the
sentiment and the feeling that I get
with the teams that we've been working
with the various platforms and and a few
of them you know on Amazon on Azure so
it's a fun space and it's a very vibrant
space right now who loves micro services
okay you're not sure yet that's all
right it's a buzzword right just like
SOA from way back so yes I'm on fire
today let's go so docker and service
fabric ready to go let's talk about
micro services the too long didn't read
version cuz there's plenty of those
sessions here but just a quick digest
hello world 1992 who remembers that me
neither
okay hello world today yes that's about
it right who can build an app today
without dealing with mobile responsive
web devices on the front api's on the
back maybe some API management in the
middle because that's the new thing as
well everything's got a back end we've
got services and tiers and you know
api's and web apps so it's complicated
right I mean I remember being able to
build one thing I mean I kind of
remember being able to build one thing
executable and run away for lunch and
come back and hope it compiled and now
it's like all these things everywhere
but that in itself is not microservices
right this is just you know building
apps today
usually a lot of facets and you have
different team members that are experts
in the various areas right not everybody
does mobile not everybody does back-end
and security and all the things but how
did we get here so we started with
monolith which in the old days was one
big thing like an executable today's
monolith is really just an app that's
not well decoupled and you know when we
started with the big monolith back in
the day the decoupling started in a
different way right we started with
trying to break down data and scale it
in the back end so we had the client and
the server right and then eventually
that moved into distributed components
like you know we had Corbeau we had D
comm and the idea behind those were I'd
be able to distribute components and
scale somehow and then eventually we
realized that in the scale either
because of connections right and so then
we move to the interoperable version of
that which would have been SOA that old
buzzword and service-oriented
architectures promise was to decouple
but also help you with isolation and
segmentation of business functions right
so almost micro service like right that
perspective there's a lot of synergies
between that time and the things we
talked about and the things that we're
talking about today it's just there's
more to it now right and more pieces so
service-oriented architecture was
another sense of decoupling and then we
moved to let's distribute things across
the cloud and build paths and services
that we can leverage in our ecosystem
right so now we're stitching together
things from other people from us from
our development team from you know from
third parties and and again paths as
well so you know just a continuation of
the decoupling of the reuse and so on
and now micro services so you know the
point of micro services yet another
layer of decoupling only hopefully maybe
done better if we're going to embrace
this approach we have to embrace a
couple of other things that will make it
successful
like the whole DevOps story right
without automation and DevOps you
probably simply can't be successful with
micro services because now there's just
and you might argue well we should be
doing that anyways right so in a way
maybe it's forcing us to better
practices
forcing us to think about better
practices because now we're decoupling
ups to a smaller granularity and
therefore having to have ways to manage
that better makes sense
so decoupling it's so hot right now
that is sort of where we're heading and
the idea is we take this picture and
turn it into smaller units of
functionality that we can isolate from
one another that we can have a separate
life cycle of development deployment
management update and the real goal
behind all of this really is not the
developer and not the technology people
it's the business because the business
wants to say I want new features here in
here and they're tired of waiting for
those new features because we're afraid
how many people are ever afraid to
update the apps they work on today
because of regression testing okay right
because of course you have regression
tests across the entire application and
every moving part to where you're not
afraid to do that right so you know the
promise of this is hopefully that sense
of isolation if we do it right there is
that if right like you have to do it
right so put another way I can build a
very badly designed monolith and I can
build a very badly designed
microservices architecture as well which
means I still have to design I still
have to take a step back and try to
think through what am I actually doing
here yes okay so that's my TL DR on that
front
and then talking design one of the
things I like to talk about is how do we
approach thinking about design before we
look at the platform so I've had to go
into some customers where they don't
know yet if they want to go with docker
or service fabric literally or maybe
they're going with docker but they don't
know yet if they're going to go with
docker on Amazon or Azure or they don't
know yet if they want to use docker data
center or mezzos TCOs or go with a raw
docker deployment where they pick and
choose the tooling themselves and manage
it themselves using swarm and deciding
behind the scenes whether they want to
use console or whether they want a
zookeeper and the list goes on right so
there are some choices and that's a good
thing and a bad thing sometimes and so
before you even get there you can
actually take this
step back and just look at your app
right how do I break down an app into
parts how do I break it into these you
know separate functional services these
these segments of functionality that can
have their own lifecycle and update
cycle now I know what you're thinking
maybe that of course how can I possibly
know that these can have their own
lifetime or lifecycle and update cycle
and the answer is well you might not get
it perfect the first time but you can
take a shot at it so I always suggest
take a shot at it first take a look at
the services across your app so this is
an order system it's got you know the
typical things that you have you have
customers you have orders you have
shopping carts you have products and
fulfillment on the backend these are
separate functions that might have
different behavior different workflows
different interactions with the backends
the corporation and/or the customer and
then maybe there's some reuse services
at the back related to notifications and
emails and whatnot but at the end of the
day they have their own functionality
that they can own which means they might
get updated separately you might have
different teams on them suggestions
services payment services accounts and
managing the users and the accounts and
the permissions so the list goes on this
is just an example if you can try to
break that up and then look at the data
and say okay well which of these has
isolated state what are these services
owned what are the actual you know
back-end that each of these services
could say this is my I prime the primary
owner of this data model if you will now
that doesn't mean there aren't the
concept of relations because we all know
when you update an order you might have
to relate it to a customer and likewise
when I edit my customer you know I want
to maybe go and look at their orders or
something like that so there are the
need to have these query patterns
through the system that might somehow
have the need to pull together data and
I always suggest don't try to tackle
that problem first when you're looking
at the micro services architecture
tackle that after you've looked at these
are logical segments of my application
and then let's think about well what are
the potential relations and we can't
really fully answer that question until
you actually know the query patterns
through the system you need to
understand I'm going to need to get this
data this way to understand whether or
not you could go get that from two
collections or two services or whether
or not it could be eventually consistent
on the back end where you have a
collection of the aggregate data that
you can query in different ad hoc ways
while still correlating you know the
data primarily with the service owner so
there's interesting choices there which
is why I say try not to solve that
problem first now this is not really a
talk about how do I design all of my
micro services obviously we could talk
about that in lots of different ways but
I just wanted to give a taste of that
because what I find is that sometimes
you can leverage a piece of a platform
to solve some of these problems and I
think that's part of what I want to get
to so thinking about the services in the
breakdown and thinking about the fact
that some of that data model might just
be a cache which means maybe it's a
cluster of cash or Redis or something
right is that going to be a container or
is it going to be a service in animal
Amazon or in Azure I don't know that's
where we start tying in the platform and
making decisions that we leverage pieces
of our platform so that's why I say
think of it this way first and then
apply the platform and the tools you
might be able to leverage another thing
might be there are no data at the
backend in terms of you know maybe I'm
leveraging an aggregate data model way
back there that's not owned by the
service but I'm talking to an analytics
engine or something like that doing some
machine learning and so on so again
you're just trying to isolate what are
these services have state one of these
services have function and maybe
leverage other pass functions what of
these are just you know executable
functions like email you know it's just
an email service it doesn't have to hold
a state it's just going to do some stuff
so get that out there and then maybe you
know think about what the front-end
looks like now some people talk about
micro services in a way where even the
UI part is owned by the service meaning
we might omit some of the representation
and that's something that you can decide
upon so I don't I usually try not to go
there right away because I think that
adds complexity to your development and
life cycle of development it's a lot
easier to think about your front-end as
these things that maybe I will
compartmentalize enough that they have
their own update life cycle like
subdomains across the system but at the
same time not necessarily worry about
container izing them right away so
that's that's a choice - it's a style it
depends on the team and what you're
accustomed to what you're ready to do
make sense so there's just an idea of a
picture moving along now we talk about
platforms yeah so docker obviously
docker is somewhat of a poster child of
the you know of the micro services world
and it's a really nice platform to work
with it's lightweight it feels good for
developers it feels good for DevOps
there are some things we have to
consider once we get past the isn't as
awesome but luckily there are answers to
those considerations so I kind of wanted
to start with the simple you know parts
which is you know the first and the
first thing we think about when we think
about containerization is how
lightweight it is right so when we deal
with virtualization we usually have the
guest OS this is the classic discussion
and when we deal with containers we're
talking about very lightweight binary
sitting on top of you know the VM to
where I can actually have density beyond
what I would with the virtualization
stack so example would be 33% of CPU
being used here or i/o and when you look
over at the container side it's 2% you
know that's just one number it depends
on your app but there have been some
folks that have published stats of their
real deployments when they moved from
virtualization to containerization and
their real stats so there's nothing to
argue it's fast it's light it gives you
more server density we have to talk
about what the meaning of server density
is though and how we achieve it
successfully okay so hold that thought
so you know and the other promise of
this of course is the idea that whatever
I build in a container the image that is
the base of the
container is immutable which means if it
works on my machine it's gonna work when
I publish it to the development
environment to the UAE environment and
to prod and we love that right because
now I actually have predictability so
one of the things I've seen in terms of
using docker I've had companies that are
just small companies that are startups
and it's one guy and it's nice for him
to be able to know that he can build and
test and run and build these containers
that maybe he's not even doing clustered
operations with swarm it's just there's
a couple of machines load-balanced and
we only have one instance of each
container on each box but he can manage
that and it's manageable and it can work
on his machine and he can deploy it and
link the containers and it still works
in the cloud right so there's something
nice and and lightweight and agile about
that very nice and then of course when
we get into production distributed
environments we have to deal with things
like well how do I really achieve the
dynamic deployments with load balancing
and and density across the machines so
I'm gonna get to that in a minute okay
so let's talk about just docker pure
first how many people already played
with it yeah okay so then you know
you'll know all this stuff basically
let's see if I can so let's go to which
of my environments it looks like
actually I have a working internet I've
had a couple of interesting things going
on there this week but it seems like
it's all good okay so I'm going to just
go and take a look at
a quick a couple of demos and I'm going
to start with just docker basics just
for the sake of those that are new and
then I'm going to talk a little bit
about my sauce as one example of how you
can manage the distributed and
environment for example oops
I can type okay so first thing I'm gonna
do is just you know and let me see if I
can get this all moved up a little notch
in case it's not easy to see how many
people can see past that it looks like
it's a bit low you're getting okay there
alright that should be good so what I'm
gonna do first is just okay so let's
take a look at the images I have on the
machine so I have a couple of things
that I've downloaded and then I've got
this one that I want to delete actually
so that was me not finishing my cleanup
so I'm going to show you how do you
delete an image isn't that awesome
so demo demo one oops wait sorry
I I think I'll do it this way hang on
our docker
I didn't say dogger so
demo1 my tab isn't working okay let me
see what's running docker PS shows me
what docker images are running I think I
left something still as a container here
so we're gonna have to take a look at
that and that's where we are okay so see
that demo demo 1 so I need to get rid of
that guy
and that one is called yes image demo
demo 1 and the name is demo 1 oh there
we go ok so we're going to say remove
demo there we go ok and we'll say
got to get my name again okay and I'm
gonna remove the image demo demo one
okay so instead of showing you that
after I show it you two before it's all
good
okay so docker images now is clean of
that and there we go
alright so first things first I have a
application here so I'm gonna just show
you that app dot yes and in that app
it's really just an Express app with
node very simple just for the sake of
showing you know a message so Ola from
node on host so instead of I don't know
Ola will type something else just to
show that it's updated because Ola would
be San Diego right where are we
crikey is that how you spell that I
don't know somebody said that to me this
week I thought it was fun so there you
go okay okay so there's an edit on apps
yes so this is just a simple node app
let me go to my Joker file so you know I
build my app I'm in sublime I'm in my
favorite editor I'm basically you know
doing development the usual way but
instead of you know running it directly
in Express or being on Windows VM and
running it in iis and so on right I can
containerize it
so I created a docker file and I say
look my base image is going to be
whatever operating environment I plan to
deploy to so this is Linux based right
so I'm gonna choose a base image that's
based on the same version of Ubuntu or
whatever is my plan to deploy in this
case I chose notes limb you know it's
limb just gave me a prepackaged version
of node a lightweight version of node
that I could run this example on which
was also based on the same you know
Linux space image so the idea is like an
onion I'm layering on top so I can build
these base images that I don't have to
change or mutate and then I can add my
software to the end right so here what
I'm doing is running node mom running
Express getting those things installed
and then moving my source over so we're
basically copying the source over and
then we're exposing the internal port so
that's the internal image
when it runs as a container there's an
internal port and then that will be
mapped to the dynamically created or
fixed port that we choose when we run
the container so there's the concept of
the internal port and then the external
port that's on the VM now that's going
to become important when we think about
distributing those containers across
your cluster and because you're probably
not going to want to fix the port you're
gonna want that to be dynamic so that it
can be load balanced right but we'll get
to that and then we're just gonna say
where the entry point of the app is
right so this is going to be running
like a node app and so we're gonna save
this I don't think I even changed it and
that's my docker file so docker build
and we'll say demo demo 1 dot so we're
basically gonna build the image and
that's going to go ahead and fetch NPM
and you know add those things to the
image and build this immutable image
which is the thing that I can run is a
container so now this immutable image is
the thing that I could publish to a
docker hub registry or wherever my
registry is for my company which is
another important discussion where do I
publish these things so we'll come back
to that and then we've got the concept
of you know so right now I'm just gonna
run it locally I'm not publishing yet
but when I do publish that means that
theoretically I could have a process
that will pull that image and push it
out to the VMS in the cloud for example
for my dev QA you know acceptance and or
staging and prod right so right now I'm
just running locally I should have this
image so we'll say docker images and
here we go I've got docker demos demo
one again so docker run we'll call this
demo one and port
80 80 80 80 and then demo demo one image
and go so let's do this docker PS do we
have it running yes
so that should mean that I can browse to
that and go to 8080 so localhost and
8080
oops no tell you something wrong oh
sorry that's my fault
excuse me it's not localhost because I'm
running this in kinematic and it's a
VirtualBox so oops
okay so crikey there you go now we know
it was the right one at least right
don't spell that right I don't know yeah
okay all right so I don't think that
constitutes a demo fail because it
actually went so there you go
all right so so that's just one example
so here I am am in my development
environment I'm running this is nice and
clean you see how fast that was there
right I mean not building of the app
because obviously I'd have to write some
code can't do that for you but you know
if I go docker PS I can see that the
containers running I can say docker stop
and that will be stopped so if I go back
over here and refresh it's obviously
going to be gone make an update publish
it again docker start demo one and then
superfast and there you go now of course
it's only got three lines of code but
still it's always fast it's pretty fast
there you go so once I'm ready to
publish then I'm probably going to get
some sort of process where I check in
and it produces an image so you can do
that automatically with docker hub for
example you can obviously use Jenkins
and have a build step that actually does
the darker build for you from your repo
link it up to bitbucket link it up to
your github repo git repo whatever that
is and then if you wanted you could do
full automation CI CD although I find
that it
in our latest you know projects together
with our teams that do all that stuff
I'm not a CI CD person I just listen to
what they advise and you know it just
seems like the control over that step
scene it's more appropriate you know we
do the automatically build and get the
image into the repo but we don't
necessarily want that automatically
pushing out to prod or whatnot we want
to have some control especially since
you might be checking in a lot and that
means you'd be building a lot as well
right it's all just a thought at the end
of the day what I was going to show you
there though is that in my github repo
you can do things like connect sorry
version you're let's see which one I
want to show you hang on you can do
things like when you check in or push it
will you can tag it and have it version
the deployments for you so you can have
v1 and have that title latest and then
check in v2 and make that the latest and
v3 make that the latest so you can have
the concept of which version do we want
to pull so you always have this
immutable image still and then maybe you
know QA is always on the latest but if
you're using another secondary QA or UAT
maybe that one should be the one before
because they're still stabilizing on
that before it goes to production gets
promoted so something like that so I can
also do things like check in to github
and have that automatically linked so
for example this guy although here let's
go to CI this is an automated build so
basically if I check in a change to that
particular one it will automatically run
a build so let's see what happens here
I'm gonna go into let's see what
directory I'm in there we go hello world
CI okay so this is another docker file
very similar simple if I wanted to make
a change and then let's say I make this
this is hello from San Diego so now
we're not in San Diego anymore so we'll
call this Sydney
and escape save and now we'll do a job
sorry get commit new location and then
get bush and then while that's doing
that I can go over here and I can see if
the build is running I think I'm an
error last build but that's okay it
should show that I'm gonna start a build
in about a second you'll see that
popping up here one of these moments and
it'll q-see start the build so then I'm
gonna have an image in this repository
and the images in the repository like I
said can be version can be tagged and
then eventually you'll pull the image
you want and run it locally so for
example I can go back to this repo here
and let's say I go to here and say
docker well let's see Joker yes and what
yes Hey okay so I'm just gonna take a
look and see ooh
I'm not scrolling the right place there
we go docker demos ok good so I don't
have one yet for the hello world so
we're gonna do a docker run hello world
CI P 80 80 or 81 and then we'll call
that das blonde
hello world CI and that's probably it
wait what'd I miss what is oh I forgot a
space
yep okay so dr. PS what do we have
running HelloWorld CI if I don't have
that locally it would pull it from the
repo and run it so that would be the
equivalent of what we end up doing when
we push from Jenkins or something right
okay so why don't I go through all that
because there are some things that are
simple and then there is the concept of
I want to build this application it's a
solution with many services I want to
deploy that to a cluster out on you know
in my cloud basically right
load-balanced cluster and so I need the
ability to potentially and I'm gonna use
Azure container service as an example
just because but either way the idea is
I'm probably going to want to have an
orchestration engine which is clustered
and that's going to be responsible for
scheduling deployments of all of the
services meaning that I have a mechanism
whereby I can you know set up things
like service discovery set up load
balancing across those services have
health and restarts on the services so
when a service is no longer responding
somebody's listening to that heartbeat
somebody's restarting the service or
somebody's replacing it and making sure
there's another one running so all of
those things are the stuff of complexity
those are the things you don't do when
your one guy you know in a dev shop you
know deploying to a load-balanced pair
of machines you need tools for that
otherwise you're building your own tools
otherwise you're picking from all the
open-source tools which do exist out
there and becoming an expert at one of
them to figure out how you're going to
manage this stuff but UI helps and you
know tools help simplify a lot of these
process and they've come a long way you
know a year ago when I was doing
discussions about docker it would be
that we didn't quite have networking
baked and now you know docker what is it
1.2 now yeah I always get the numbers
right wrong but I guess the point being
the latest release right now has all of
that functional we've got service
discovery baked in swarm is working
better than it did before now they have
the docker data center which is the
kind of proprietary version that is a
paid version a licensed version of all
the tools in a package ready for you
including the registry where you would
publish your your your actual images and
so that's where we get into things like
although I'm not showing it on this
picture somewhere over here is a
registry right somewhere over here is
the place where our private images for
our corporate organization and get
published and promoted between dev and
QA and prod and so on and so I probably
don't want to put that in docker hub you
know open registry I'm either going to
pay for a private registry or I'm gonna
host it myself and hosting it yourself
is not trivial you have to make sure
that that's a first-class citizen that
it's got disaster recovery around it and
so on right so having that come in a
package like docker datacenter is
helpful having that available to you in
places like Amazon where they have a
service for that is is helpful in Azure
we don't necessarily have that built in
but we do have things like mesosphere
talk data center operating system a DC
OS which at least provide the tools
around the orchestration with swarm or
marathon which is the the one that I'm
going to be using and also the agent
node management around that so where you
have these scale sets which is the VM
that has docker engine on it and the
agent that listens to marathon so that
the heartbeats of your service are being
sent somewhere and so that something can
monitor manage and listen to those
health heartbeats and handle scaling you
ask for more services send those across
the nodes right so with that again just
thinking about how you design more of a
real system putting another perspective
you're gonna have on Azure for example
and again this would be different on
Amazon and various deployments just as
an example you're gonna have your load
balancer coming in with requests and
you're going to want that to load
balance across your if you're using
websites and putting those in containers
as well as your services behind the
scenes in containers and those can be
distributed across again any number of
nodes in your cluster possibly multiple
tiers
and then you want to have that load
balancing dynamic discovery working make
sense so with that an example of this
again there are many tools for this if
you use the docker DCOs with marathon
then you're going to have a marathon
load balancer endpoint that you could
have on each node in your agent cluster
and then that will keep track of the
services across all the nodes so that
when a request comes in for service one
it knows where all the service ones live
and there could be three on one machine
and one on another machine and several
you know across many machines because of
server density which is where we get
into when I schedule services and say I
want 20 instances and I only have 5
machines that means each machine
potentially gets multiple instances just
to help with managing requests and and
isolating those in those individual
containers right makes sense ok all
right so yes so I guess to that let's
have a quick look I'm gonna go to here
and so what I have right now is I'm
tunneled in actually to 8989 local is to
a Adger deployment of the azure
container service so there's a template
for this I've got a template that's
deployed right now which is a single
node for the management orchestration
cluster it's not a cluster sorry it's a
management node and then I've got three
other nodes and so what I'm gonna do is
just give you a quick example of what
kinds of things I might do for I'm going
to start with you know marathon load
balancer I'm going to install that
package here so we're gonna install that
package on the cluster and what that's
going to do is put the agent node on
each of my three agents which is my my
my server notes right and then it's
going to install the marathon management
load balancer in the in the management
node so that I can actually go and take
a look so it looks like this is done I
don't know yeah okay
so gut so now I'm gonna go to the
dashboard so this dashboard shows me
actually how many nodes I have I think
there's my three connected nodes to the
cluster now it has these components
which are related to the load bouncer
that I just installed I don't have any
tasks actively running right now I don't
have any failures right now that's good
so it's giving me a bit of a dashboard a
bit of a visual around what's going on
in the health of the cluster makes sense
okay now with that let's go over to this
guy and so this and I'm gonna just
refresh this is my marathon interface so
let's get this reloaded and I'm so happy
Internet's working that makes all the
difference doesn't it what else would I
do up here yeah although that might have
been fun to watch for a second but
anyway okay so I've got this marathon
load balancer right now and I'm actually
going to widen this up so we can see the
full screen as much as possible and yeah
okay
so what I'm going to do is create an
application now I would automatically
and and push out the application that
way but you can describe in a JSON file
what you're going to push and and have
that be scheduled so what I'm gonna do
is go over here and grab this JSON file
and what it does is it describes a
couple of important things one is okay
I'm going to deploy a web app call the
web and then I'm using Network Bridge
which means that it's going to
dynamically assign ports to every one of
the instances I deploy and that's really
important because we obviously need it
to be dynamic and we want the marathon
load balancer to automatically know
where all those instances live and
that's what's going to be provided for
us for 10,000 is how the marathon load
balancer will look for this service
so basically port 10,000 requests to
marathon load balancer know okay we're
gonna go and look for all the web
instances this app so I've got to have
three instances to start with
and I guess that's the main thing and
this is the host we're going to be
hitting which is my asher host okay so I
could have different subdomains for
different services if I want to do
something more complex like routing with
forward slash service one two three I
have to add a little bit more logic but
we're going to just leave that alone for
now because you know it's an hour okay
so I've got that copy to the clipboard
so let's get away from that for a sec
and go to JSON mode which means I can
paste this guy in and say create
application and now you can see it's
deploying alright it's pretty fast at
some point what we're gonna see is all
three instances are running there you go
okay I should be able to hit that
service let's see if I have a link handy
over here there so here's the service
now eventually if I keep refreshing
we're gonna see this port change right
this is the internal health requests
coming from the internal server so
there's health pings to see if the
service is alive at each instance this
is showing me you know this is my web
app right so as I hit it it's showing me
this information so this is coming from
inside the app and at some point when I
keep refreshing we should see that it
changes from you know one seven to 1703
to something else because there's about
three different I don't know it's just
taking time see connecting I know you're
out there somewhere while you're
thinking I'm gonna come back over here
okay so you can see here I've got ten
this is the internal IPS that were
dynamically allocated to the instances
so I've got a five oh five oh five but
these are the ports the unique ports
that were assigned so if I kind of come
in here and look at each port I should
see oops that's not what I meant to do
sorry I meant to go here and see that
that's 1702 and then if I go back here
again and click on this one this is 1703
you see
and so on right so each of these guys
has their own separate IP dynamically
allocated now while I'm doing this I'm
gonna go ahead and scale this and add
like 24 or maybe more let's go to I
don't know 40 what the heck right so
let's scale the application so now what
we're gonna see is that it's you know
gonna start adding you see staging
staging staging all the tasks if I come
over here we're gonna see ten staging
see it's processing the tasks getting it
done it's gonna give me some errors if I
ask for too many nodes that more than it
kind of handle resource wise to deploy
things like that so far so good across
my nose failure rates increasing so
we've got issues it's going down okay
right that's exciting I just want to
stay here and watch that for a minute
okay so shiny things why is that not
working I'm not sure I think it's just
internet I'm gonna hand away that but it
should be working
because you saw it work the first time
right and away I'm just gonna let that
refresh and think come on really come on
you could do it
well
while it's thinking what we do know is
that it's staged a bunch of these guys
and that maybe there are Samaras you can
actually go in and look at the
standardout and errors and so on so it's
it's pretty interesting right so it's
it's still working on it
it's only got 11 of the 40 delight it
just deployed it's still thinking but I
guess the point that I'm really trying
to make is that the scheduling
distributed the nose across more of
these across the whole cluster to where
each of these nodes now has more
services on them so I've got six staged
each node is going to show me what it's
got available what it's got scheduled
well this is the total scheduled so
that's not correct but and then this is
1703 and you'll see that we probably hit
the other ones here so 1706 so you can
get the idea of the distribution across
all the nodes right and the most
important part of that is that now with
Marathon load balance are built in like
this I can just hit the port 80 IP that
you saw totally working right and and it
will figure out which know to hit right
so that's that's really pretty sweet and
we didn't have that before so I think
that what the message I'm trying to give
you is that the tools are evolving
nicely to where if you spend some time
with them you can actually you know
manage this stuff and have all these
multiple little services running around
and build in some sort of health
monitoring recovery when when services
are no longer working have them replaced
and so on because the tools can do that
you just have to sort of spend some time
with them and understand where to watch
for the alerts and you know where to set
up your health monitoring endpoint which
is usually a default 200 response that
kind of thing
okay okay so that's a quick look at the
docker side you know again the point
being the simplicity is really elegant
around just working with containers and
pushing them out and having them you
know the image is immutable and running
them and they work everywhere and the
idea of the tooling helping me with the
scale and distribution
load-balancing really takes a load off
it just means make sure you're thinking
about that right so let's talk then
about service fabric because there's a
couple other topics that we haven't hit
on which is maybe like the state
management aspect so if I have services
and I've got this concept of each
service should own a certain data model
and state and I want that state to scale
and you know the first thought I might
have is well I certainly don't want to
build you know Redis and link that into
my container structure because then it's
not clustered and it's not reliable and
I would need that Redis to be clustered
and reliable and have a quorum when I
write to it so that I know and Redis is
a bad idea maybe that's more of a you
know caching type of model so let's talk
about MongoDB or Cassandra or something
like that the point being I still need
to think about the state right what is
that ownership of state for each of
these services that I'm putting in my
micro service architecture and I don't
really get that for free in the docker
world although you know I can build that
I could have a MongoDB cluster I can
decide to use collections as my vertical
behind the services and then I can
figure out how I'm going to relate those
things whether I'm gonna use event
sourcing models for if I update an order
let's let's you know create a trigger
that will also update the customer table
and make a relation you know doing
things like that can get complex like
event sourcing or can be you know super
simple if it's just a relational
transaction and then there's somewhere
in the middle there right
it just depends on the model it depends
on the query patterns and that's where
the hard part comes in typically but
let's talk about maybe service fabric
because some of the tools that they
provide actually build some of that in
which I think is neat so we've got this
other perspective on microservices and
tooling that adds some value in a
different way but then and then flickers
because I don't know why really you will
give it a good session rating
you I don't know what that is
have I done something wrong brand at the
back have I have I said something that
offended the projector should I
reconnect is there is there a reason for
this I just did that I'm gonna try again
okay calm down sister let's try again
I'm doing that slowly because maybe it
will isn't that wonderful
okay hold everything good thoughts of
people good thoughts are you kidding me
okay tell you what we're not gonna do
the slideshow we're just gonna show the
slides because that appears to be okay
so the animation will be pretend like so
I don't know what that is
okay so as your service fabric again you
know I'm focusing on this as a platform
because it is the microservices sort of
pads platform that Microsoft has pulled
together and it is pretty sweet in some
other ways so it's very different from
the docker experience because you're
gonna be Visual Studio you're gonna be
in your familiar environment as a
potential topology you know I'm gonna
still maybe have my friend and web apps
that I could deploy in pass and keep
nice and simple but they're gonna call
back maybe into a services tier so one
of the things I wanted to touch on is
the idea that when you think micro
services it may only be the services
layer that become the micro services
that you want scaling and and and
deploying in the middle tier and having
those frequent lifecycle updates and
feature editions but the websites are
still the way they are and then maybe
there's a back-end that's still the way
it is
so you're inserting yourself in the
middle as a micro service layer and
that's a good way to consider migrating
for example take a few pieces that need
to scale and that you want to
compartmentalize and start pushing those
into the micro services tier
so that might be one model and I'm just
illustrating that here as well right so
I've got my cluster at the back which is
my virtual network with with my actor
service fabric cluster and actually I I
put here I actually this is Web API and
services so I'm actually not really
showing the front end on this I just got
confused on myself so the front end here
what I mean by that is you need a way to
get in to the service fabric cluster the
cluster actually by default if you
create stateless stateful services
actors and so on those actually are not
accessible to outside the cluster unless
you expose an HTTP endpoint which would
be like an Owen based service web API if
you will and then I could put an API
management gateway in front of this or
just have a web app call it directly so
this is the whole cluster is what I
should have been saying so the load
balancers coming into the cluster and
I'm opening up port 80 services that are
just traditional web stateless services
and then I'm calling back into my
stateless or stateful compute so each
service can be either a stateless thing
that is scaled out across the naming
service which is not accessible to the
outside world outside of a cluster or a
stateful compute which could be anything
from a it can be you know again you can
have guest esse cuticles which are Java
which are other platforms you can also
have managed code which is your
traditional dotnet application you know
c-sharp code and so on you've got the
concept of stateless and stateful
reliable services the reliability aspect
comes really with the state which is I
can have a service that has the concept
of state and then it will replicate that
state across the nodes and make sure
there's a quorum any time I do a safe
and make sure that if I can't save to
the quorum to complete a quorum then it
will reject the message so we can
guarantee that it's been saved and
replicated before we get a return from
the method if you will and I can have
stateful services like
reliable cues so now I can potentially
replace my other cueing mechanisms if
I'm in this micro-services space with
just a reliable cue that I know will
hold on to the message literally until
it runs and so it's durable and it's
replicated and it's guaranteed yeah so
we use that a lot and then there's
actors so an actor model is interesting
because you have your traditional sort
of perspective on actors which is IOT is
a classic example right east device
isn't after it holds a state and then
you know when it's hydrated it's there
in memory and accessible quickly and
replicated across the cluster so it's
available to any requests and then when
it hasn't been used for a while or
there's too many requests in parallel
millions of things running around it
will flush the ones that are not so
active so that it can conserve you know
compute memory and so on but it still
preserves the stage it's still concert
it's still preserved make sense so all
that's handled for you but actors also
have another interesting value prop and
that is I can do things like workflows I
can create a workflow where I kick off a
job from a cue and the actors job is to
negotiate the process of completing a
payment order or something like that and
it's going to send an email to the user
and a notification and save something to
our distributed store maybe save
something to our analytic back-end so
that we have not only the state in the
cluster that is replicated and durable
but also a copy of state for queries on
the backend like ad hoc queries so
that's where we get into the idea that
an actor doesn't have to just be typical
actor model perspective it can also be a
workflow management tool and then you
can chain them start with pulling
something off the queue calling another
actor and another actor and the state is
managed by the actor so we always know
where we are in the workflow it's done
yeah so it's really an interesting model
so when you do a new project in Visual
Studio you're just going to have a
symbol similar experience you usually do
but you'll start with a template that
creates a service fabric project and
you'll have this concept of I have on my
github I know you can't really see
right now wait let me make it a bit
bigger in my github repo
I have already posted a pretty
comprehensive readme and I'm just trying
to get back here on this entity
calculator app and what that has is a an
example of this actor workflow model
with reminders that do triggers an event
sourcing so it's pretty interesting if I
do say so myself okay but the point is
that the app owns the services so if I
you know again just trying to scroll
down here without losing the page ya
know that's where a slide deck would be
nice like you know animations and such
here why don't we do this that's what I
meant to do okay
sorry yeah what can you fix Oh is this
all on film hey how's it going hey don't
worry about it it's fine I already
scrolled it up I'm sorry okay
I didn't hear what he just said okay so
click what show okay but I thought the
slideshow was broken didn't we establish
that it won't be broken now are we fixed
what was the problem get that on film at
least my god
I mean don't leave me hanging like this
okay alright so basically the idea is
that the app owns the services it
deploys so when I create a service
fabric application I'm gonna add
services to it like here's my queue my
reliable queue and here's my actor and
another actor and here's my stateless
service but the problem you can already
probably gather is how is that a micro
service if I'm putting everything in one
app how is that if I deploy all that
together how is that a micro service it
doesn't feel the same as docker right
when I'm in docker have a thing it's an
image I'm building an image and I'm
gonna push that one little light thing
up and over the wall and it's gonna work
and then I'm gonna have to stitch that
to another thing and another thing and
that's where the networking comes in and
the load balancing across containers and
so on but you know the point is not that
you can't do that here the point is that
the sense the way you go about it is
different I'm gonna have this app it's
gonna be the whole thing while I'm
building it by myself but when I have a
team across you know countries and
distributed partners I'm going to have
to build additional apps that just
deploy the one or two services that go
together all the time and that becomes
my unit of deployment lifecycle
publishing and update and you can also
with apps deploy one update to one of
the pieces in the app and it
only deploy that one thing version that
one thing but therefore version the
whole app and even do rollbacks and such
so we'll talk about that now that app
slide dick again which is awesome by the
way thanks for you know that here's an
example of what I'm going to run right
now is an app that has a web front-end
go through API management to hit the
load balancer and hit my stateless Web
Services and it's going to place orders
for like a concert like ticket orders or
something we just did this for a
hackathon actually so it's really kind
of fun and the backend stateful services
we have a queue and that queue is going
to be the asynchronous process that we
know is guaranteed reliable durable
queued message to figure out if after
receiving that you know that that
credit-card token from because we never
want to hold all that stuff in the
backend we only get the token that says
yes your credit card can handle the
processing and then we queue that and
then we talked to the credit card
ordering and we see if we can process
the order and if there's enough tickets
for you you know how sometimes you order
something on manna Amazon and then after
they say oh sorry we didn't have it in
stock after all and it gave you the
message that kind of thing so making it
a sync so it can scale when there's a
lady Gaga concert in town and a million
people go to order at once we need to
queue all those orders we can't possibly
hit them all at once make sense
so we're queuing it up in a state full
service and then the actor picking up
the queue when it can and now there's
each order is an actor so there's like
potentially millions of orders all at
once processing as they can and when
they complete it's durable is persisted
and we can always get back to that order
and so in order to have a queue
queryable pattern on the backend we
store in an adoptee b so we actually
fire it off to an azure queue and a
function which is another form of Micra
service and Azure like a lambda and
Amazon will pick up that queue and take
that order and process you know saving
it to a store that we can query ad hoc
in the meantime my actors are still
available so that the person that just
placed the order can be waiting on the
website and see success or something
like that so it doesn't get put to rest
or retire
until it's not active for a while right
so until it's purged so let's talk about
that let's go into service fabric and
I'm just gonna start by showing what
that looks like so here is my
application completed right so I'm not
gonna start a new app anybody can do
that you create a new app you're going
to get this service fabric app what
should we see here and it's showing a
list of my services in there and so you
know my services are these guys right so
I've got an actor an order actor I've
got a service which is my cue and then
I've got the Web API front-end that's
picking up the requests and so when I
run this what it's doing actually if I
take a look at the local cluster let me
just go there my local cluster shows
that I have an app already deployed and
that it's available in running and in my
local cluster if I were to you know just
to illustrate that I have a partition
available and so on let me do that
honking horns really Wow okay so what
I'm gonna do is I'm going to take a look
at this is my actual API endpoint at
localhost 8080 - it's the entry point to
the whole service fiber cluster and I
have a place to place an order here for
example I can go oh not delete how about
we go to orders and I can you know post
an order for example right and so if I
place an order here and try it out then
it's going to hit the API and and run it
I can also do that though in my cluster
that's in the cloud
by just coming up to this website and
placing an order so if I go over to web
UI orders and it's just thinking a
little
can always check the cluster it looks
good there are no orders right now I can
go to the front I can place an order all
right so order tickets now and I can
place one order or I can do this thing
which is load test so I'm going to go
ahead and make a load test of 100 orders
so let's let that post so what that's
done is queued up to the function to do
100 orders and it's going to go and
iterate through that and in the meantime
I have these partitions that I've
created so I have this queue it's
durable and upfront because it's a
stateful service when I talk about state
co-located with the service I'm actually
able to decide up front with a queue how
many partitions do I think I need how
many concurrent you know requests for
orders when Lady gaga comes to town do I
think I'm gonna need because you want to
pick the highest number first and then
if I only have three nodes in my cluster
it'll distribute those partitions across
the three nodes but if I grow to 50 now
and soar 100 nodes or 3,000 nodes then
it will distribute those partitions
across all of the nodes and it will
track all of that for me all I have to
do is decide how many do I need when
we're thinking about data partitioning
it's a little bit different because now
you're going to think oh all the
customers that begin with the letter A
BC through Z those are my partitions so
I'm thinking about what is the finest
grain thing that's reasonable for
looking up partitions so you think about
that upfront but the rest is done for
you because the state has managed if
that makes sense so you can see if i
refresh if it hasn't finished processing
I should get a look at how many things
are in the queue it's still loading up
here so let's see or it might have
finished who knows let's see if I can do
that again and I'm gonna run another one
because I think it might actually be
done because it's in Azure so it's
probably not so slow I'm gonna do one
more and then I'm gonna go to load test
and it should show me like I said
partitions with stuff in it
there we go 3 2 2 3 so it's distributing
the requests across the partitions you
see that so as the loads coming in
and it's doing that all for me and it's
persisting it makes sense so I think
that's just one aspect to know about it
the other thing is that each individual
order of this processing is its own unit
every single ticket order is an actor
which means the partitioning is per
order which means it scales to the
widest amount there could be millions of
those things running around which is the
purpose of actors but the state is
persisted and I didn't have to think
about it all I had to do is decide what
is the order ID or the thing that will
be my partition number that is unique
which is my ticket order ID make sense
okay so what gets really interesting is
when I go into visual studio and do
things like hey let's make something
fail and try to do a deployment or a
rollback so if I want to do an upgrade
in place let me see about this one I'll
just run one example of a contrived
exception let's close this down for a
sec and what's happening is you can
inform service fabric of your health so
if you feel like something should be a
service failure which should trigger a
rollback or or at least trigger UI
notifications of failure right then I
can indicate hey we've not been able to
contact a payment service for you know
like at least three ties let's report
the error or I can't even run my config
is wrong let's report the error
immediately and so when I deployed this
change that I just made so let's go
ahead and do it publish to the cluster
I'm gonna go and publish and I'm gonna
choose to do this locally for now just
because that'll probably be a bit faster
and I'm gonna say hey I want to upgrade
the application which means I'm going to
take the application as I have it and I
don't want to lose any state
I just want to upgrade Institue which
means it's going to roll in my update to
the services and if everything's healthy
and state the new services and retire
the old ones without me having to do
anything and so if I go and configure
the upgrade settings it'll let me choose
things like a monitored upgrade and the
failure action will be rollback
so there's that and then lastly I need
to version the manifest so what is my
change right I'm changing the order
service so the new version is going to
be one point one let's say and that's
going to now version the application
right so okay yeah yeah yeah okay so I'm
think probably I think that should do it
and then I'm going to try and do a
publish so this will do a publish
locally and if I look at my local
cluster while it's publishing we should
be able to watch this is my local right
we should be able to watch the status
upgrade in progress you see that and we
should be able to watch let's see if we
can get to oh that's weird I'm not
seeing what I'm expecting
oh that's so strange okay I don't sure
why that's happening I must have
deployed you know what it is I deployed
it to the cloud I didn't deploy it to
local so my upgrade is going to the
wrong spot it's not really upgrading
there you go
let's try that again I can fix this I'm
sorry it's rebuilding right now sorry
yeah yeah I think the failure though is
related to what I just said well we'll
see yeah your cluster ooh yeah okay I'm
gonna choose the right cluster angles
and we're gonna do an upgrade and we're
gonna just check that I've got the right
version so I'm gonna version this
service package okay well we're at time
so I'm going to just give this a shot
and what I should see then in my cluster
up here is if the upgrades in progress I
don't know why it showed upgrade in
progress or maybe I was just getting
confused because it said zero and that
should have been a tickler it wasn't
doing it let's see so it's still
building probably I won't show up here
until it's finished building so while
that's building I'm gonna let that go
for a sec and then let's just sort of
again wrap up the discussion so a couple
things hey I've got screenshots for
things that maybe you didn't see it so
for example of creating progress that
happened okay so the target version will
show what it's going to move to and then
it'll show you how it's getting through
the upgrade domains across the notes
while it's happening and when it's done
it should show you the new version which
means it was successful yeah when it
fails it will show you exceptions and
we'll actually while it's upgrading
eventually show rollback in progress and
so it actually won't deploy the new
services so that's a really helpful
interesting thing so it's it's
interesting when you compare the
platform's right this picture is really
just showing what I suggested before
which is the thing that I have on my
github which is a hierarchical set of
actors to where when I create a new
sales order in New York it actually
updates the totals for you si and then
Americas and in global with reminders
which are durable that remind themselves
when the actors finished saving its
state to go tell its parent to save and
so on and so on and so it's creating
this chain of actors saving updated
state to where eventually I actually
have a correct total across all the way
up to global of all of the sales orders
across the organization so that
eventually that event sourcing model is
actually kind of a thing you can build
into service fabric really easily and I
think that's pretty neat because that's
a very complicated model to build so
that's something to look at you can read
my blog about that and then I think I've
already covered this so let's keep
moving on there we've talked about the
density and scale we talked about
partitions so there's just a visual
around that so I guess it comes down to
this so when you think about choosing a
platform you know we've talked about how
you break down your services that you
might want to consider State and
ownership of state and then you might
want to consider how do I relate the
state and how do i do aggregate ad hoc
queries so and then there's the side of
it which is the managing of the
networking and the clustering and the
density of services across the nodes but
I've now showed you two platforms that
actually can handle the density and
distribution and automatic port mapping
and load balancing of the actual
services which is kind of cool right I
would say and then you know the whole
idea of registry and discovery is
related to that when I ever deploy
another service it registers with the
discovery system so that when somebody
wants to call it you know the load
balanced environment knows how to find
you know instances of the service across
all of your notes right self-healing
another thing the rollback that I just
showed you for example and being able to
detect when a service is not working so
having those pings in place both of
those environments do that so it's just
a matter of leveraging the tools
available to you instead of going raw
docker for example you can do similar
things with docker data center although
that
doesn't have all of the UI built-in you
can do similar things with Amazon ec2
container services but again doesn't
have the whole UI built in so one of the
choices that we've recently gone through
is that you know the DC OS doesn't have
to be on a sure container services but
in general that platform has you know
with marathon the tooling and the
visuals around managing the ecosystem
and the discovery and the load balancing
and the health and so that's nice and
then with service pack you get a little
bit of the same with a different
development experience and the state
management side of it is really sweet on
the service fabric side so you know
finding the right fit for you is
probably going to be obviously what
platform are you sort of having an
affinity to naturally what do you know
already Linux windows etc what's more
comfortable time line cost time to
market and whether or not you have time
to learn that new ecosystem or
environment or you can think about
things like migrating slowly right
so one thing we know is micro services
are part of your future our future she's
creepy I just think it's funny so we can
take a quick look and see if that works
although that could be a good or bad
thing to end the show for mmm let's see
what we got over here I'm taking a risk
we'll see if it upgraded I don't know
what you saw the picture right so there
okay thanks for coming out for an hour
and a bit more I hope that was enjoyable
to you and even with the comic relief in
the middle so have a great day</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>