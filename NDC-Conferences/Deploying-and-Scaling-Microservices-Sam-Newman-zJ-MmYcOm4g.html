<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deploying and Scaling Microservices - Sam Newman | Coder Coacher - Coaching Coders</title><meta content="Deploying and Scaling Microservices - Sam Newman - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deploying and Scaling Microservices - Sam Newman</b></h2><h5 class="post__date">2016-09-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zJ-MmYcOm4g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so well welcome to win the
finishing straight right on a now in a
three-day conference your brain is sort
of towards the end of the third day
starting to dribble out of your ears
right so it's after lunchtime now so
this is this is the snooze slot right so
I'm hoping going to try and keep you
awake I've also got stand here otherwise
the camera can't see me I'm going to try
and keep you awake I am going to put
anyone here from outside of Australia a
few I'm really sorry about the weather
it really sucks i was in iron and a
couple of weeks ago fully expecting this
to be what was happening in ireland's it
normally is when I go to Ireland and it
seems that we've just switched the
weather's over but it's been miserable
here so i do apologize for that I can't
do anything about it but I do apologize
nonetheless I hope some of you've seen
that any see Sydney will be coming back
next year I'm on the programming
committee and we are very keen to make
it's a really great conference we're
still trying to work out how to adapt
the NDC model for Australia if you've
got feedback please do let the
organizers know not just giving feedback
to speakers but just letting us know
what we can do to make things better
next year but we're here to talk about
deploying and scaling microservices my
name is sam newman if you thought I was
somebody else you can leave now you've
got a window where it's acceptable I it
is kind of a weird day for me so I've
been working for the last 12 years at a
company called thought works this is I
probably done a hundred and fifty
conference presentations in the last 12
years before i joined thought walks had
done zero and today is my last day
working for thought books literally
after this talk i have to walk down the
road up pitt street and go and handle my
keys back so it's very weird I had a
great time then a lot of years often a
start-up but they are a great company
and if you need help on the stuff I'm
going to talk about today there are far
worse companies and m's go talk to also
they're always hiring and so if you're
looking for a great career if your grads
to an experienced person please do give
them a call you can say I sent them I
don't get referral bonuses anymore so
I'm not you know lighting my own pocket
here
I also wrote a book called building
microservices I feel like I should also
point out right now that i know this
conference has a sort of healthy slice
of sort of microsoft oriented
technologies i only have any Microsoft's
technology in my house so that I can
play games that haven't been ported to
Mac so that's just cavy acting that out
of the way I'm hoping some of the stuff
I'm going to talk about will be
generally applicable but just please be
aware that I am NOT a dotnet person I am
NOT a Microsoft D and so the last thing
I did anything in this space was
actually helping a company owned by
microsoft get off of windows onto linux
so if you need deep dark details of the
dotnet stack in relation to this stuff i
will do my best but please do not be
surprised if i go so we'll see how it
goes are we to talk about three things
today are we're going to talk about some
core principles around how we want to
think about packaging up and deploying
I'm microservices so there's our work
services are interesting things people
seem to like them a lot people keep
buying that my book which I'm very happy
about um I can give you both discounts
if you want to buy them 20 or more
that's fine by me but there's a you know
that they're nice consensually than a
nice idea but but people actually
struggle that knowing how to actually go
about deploying them actually some basic
stuff and packaging them up gets quite
interesting so talk about some core
principles around deployment of
microservices the search stand you in
good stead matter what platform you use
once you've got done through that we
then go look at the actual types of
artifacts that you might want to create
and then based on those artifacts we're
going to take a look at three different
deployment platforms that many people
have been using for deploying
microservices now I think this is going
to be useful i hope not only and getting
some insight into the three platforms
themselves in order to talk about which
your coop entities me sauce and docker
swarm but i'm hoping more generically
it's going to be useful becoming to give
you some criteria to think about when
analyzing other types of platforms that
you might want to use for deploying
microservices you may not have access to
those specific technology
you might be looking at other things you
might be looking at her Roku or pivotal
cloud foundry or whatever else but
hopefully you should leads come out with
some concrete questions to ask of the
next suited and booted vendor the walk
through your front door so let's start
at the beginning let's talk about core
principles and let's start with the
basics about what microservices are I
draw them like this I draw them with
hexagons it's a nice shape they have
names on them I have my services I want
them to be modeled around business
domains around business capabilities
we've seen that by doing this that we
end up with more stable service
boundaries because changing service
boundaries is expensive and we also
expose seems that make sense to be
exposed rather than exposing arbitrary
technical boundaries until they're more
about modeling those I've done other
talks in the past or you can take a look
at the book in terms of this talk the
thing I want you to focus on and
probably the most important principle to
embrace when using marker services is
actually this one that these things
should be independently deployable it
should be the norm not the exception
that you can make a change to a single
service and deploy that into production
without having to deploy anything else
this is what gives you the ability for
teams to own services to have autonomy
about their release cycles this is how
you reduce your cycle time this is how
you keep the scope of your releases
small keep the risk of those releases
small if you're in a place where you
having to do lockstep releases you're
going to have large scope releases those
large scope releases will be scary
because they're scary you take more time
around them which means they tell you
have longer between the releases which
mean the releases get bigger means the
releases are scarier so you take more
time around them and so on and so forth
and when they fail you end up with more
problems because it's a larger scope of
change so it's harder to track down what
failed the secret isn't all this is not
a micro service appoint actually this is
a malefic systems point keeping your
release batch size as small as humanly
possible is one
significant one of the best things you
can do in terms of releasing software
frequently and safely and with Marcus oh
Jesus it's no no exception but the key
thing is if you have a situation where
you're releasing multiple services in
lockstep you are probably in a world
which has significantly more pain than
if you had a more monolithic
architecture so independent
deployability so we're gonna be looking
at platforms and deployment techniques
we're going to be thinking about
ensuring we can we have that card here
that we can do we can deploy our
services independently of each other we
also need to give some thoughts though
about how we actually get our software
to where it needs to be how do we go
from a laptop to a live environment most
of you are you know have a laptop it's
kind of nice now you know when I first
started working at ThoughtWorks my
laptop was a toshiba tecra and at my
first client they got burgled and
everything in that office got stolen
apart from our laptops because even the
burglars took one look at those things
and thought yeah no um so but now I can
actually develop right I got very
powerful machine here but it's not a
production machine I still going to work
out how I go from here to here and it's
only start thinking about things like
build pipelines whether or not you
explicitly modeling these on not
conceptually we have this idea that
there are multiple stages involved in
taking software out of my hands and move
through moving it through into a
production environment many of you
actually hands up how many people here
are using some sort of continuous
integration process right probably all
of you some of you might even have to
use TFS but you know that's your own
fault um so you know you it is amazing
it's actually you know I TFS is
legitimately bad and he said you know I
my experience of a couple of years ago
but it's surprising how many people use
TFS not because they wanted to pick the
tool they wanted the most they pick TFS
because they had an enterprise agreement
with Microsoft and they didn't have to
pay for license fees that's a terrible
reason to pick what is a vital piece of
infrastructure hopefully it's got better
but you know anyway so here I'm a laptop
I've checked some code into sauce
roll I then have some kind of CI process
that detects that check in and can then
trigger some automated processes
normally the very first thing we're
going to do is something like you know
build the software make sure it compiles
this is it surprising phase but a decade
ago I did most of my time when I first
joined thought works for about five
years with doing cadiz integration
infrastructural automation and a
surprising amount of that we're going to
clients and just helping them make sure
that when they check the code in that it
compiled together before we even start
worrying about tests so we compile it
and 49 and then what we would normally
would do here is we would actually
create some kind of artifact that we are
going to deploy the bundler code
together right we'll talk about what
those actual artifacts might look later
and then we take that artifact and we
move it through these stages and again
you might have a CIA tall or continue
living tool that allows you to model
this workflow or you might have that in
your head somewhere we ever might have a
release manager tracking it but
conceptually we're taking this software
that we've checked in and we're moving
it through these stages saying yes are
you good enough okay you're the
automated tests are passed now we're
going to do a uat cycle did the uat
parser our users happy right maybe now
we do some performance tests and if the
performance tests for that version of
the software pass we will push it into
production some people this is an
entirely automated process that's what's
called continuous deployment so a
check-in is validated for our number of
steps fully automated hands off and so
you have a large number of those
check-ins will go straight through to
production things to think about when
you have this process what you want is
you want one artifact for all
environments the reason for this is
twofold firstly rebuilding and the same
artifact from the same version of the
code over and over again for different
environments just wastes time and that's
kind of odd right why would we do that
we want these things to be fast the
second reason you don't want to rebuild
artifacts for different environments is
because you can problems can creep in we
want the version of the code we'd ran
the tests on to be what we deployed
because if we
rebuild our software may be passing some
different compiler flags we can change
the behavior the software such that we
will unearth problems in production that
we really should have picked up at tests
so we want the verge on the code that we
ran our tests on the artifact we on our
tests on to be the thing we deploy we
also want to have the same deployment
process everywhere we're like to the
same deployment process in production as
we using performance as we use in uat
and again it is because we want to find
problems early I would like during my
test cycle two views the same deployment
process I'm going to use in production
because if it's a problem with a
deployment process I find out then not
in production so all the platforms we're
going to look at are going to help us
with these things they're going to help
us in terms of having one artifact for
all environments and they're going to
help us in terms of being able to use
the same deployment process everywhere
it's probably as a sidebar sort of i'm a
CLI guy i like my command line interface
I like dashboards and gooeys for viewing
things in terms of carrying our actions
I like having a CLI available to me
because I can script it I can share it
obviously we've got chat BOTS now that's
which is effectively just a shared
communal command-line interface this is
the deployment script I strive for in
any company in any situation i'm working
in i want something that looks like this
we have the name deploy that's a good
name it's of action I know that means
that's great then I have the name of
some service this is the returns service
okay the next parameter I'm giving it
this grit is some version identifies
could be a build number this could be a
tagger name some way of finding out
which version of the software you want
to deploy normally I want to support
local which means it will be the version
of the software i built locally on my
machine or maybe latest which might be
the latest Greenbuild so the bill that's
got through far enough through my build
process or it could be an explicit
version and finally I'm going to give it
an environment and environment is a
place it's a location it's an isolated
set of resources
and data which which met where I deploy
my cell is potentially in two different
topologies so this is the idea that I
might have a smaller cut down uat
environment which is just their first
sort of people to poke around have a
look at or a very fully fledged
performance environment so mime you a
team maybe my service is just a couple
of nodes load balanced on a single
database instance I'm sort of trying to
keep my environment smaller it's less
work I save some money it's faster to
provision as well we're always looking
for fast feedback whereas in production
my production setup maybe actually a
primary replica database for nodes load
balanced across two data centers so this
is kind of interesting like this is a
very familiar thing that many of you
will see this idea that your art if your
your environments have very different
topologies but I said earlier that I
want the same artifact for all
environments and this becomes a problem
because what people often the way people
often solve the environment issue is
what they do is they create different
artifacts for different environments and
I think for my approach to work you need
to separate out those two concepts what
you want is a universal artifact when I
say I want to deploy what version 1 2 3
4 5 of my code there is one artifact
version 1 2 3 4 5 of my code what I need
a way of is defining the topology
separate from the artifact and again all
things we can look at later and allow
you to have that topology and that
configuration stored separately from
that core artifact and at this point
again nothing markers so is specific
it's exactly the same principle as I've
used for building deployment for the
last decade so core principles fourth
microservices independent deployability
make a change to something deploy it
into production in isolation of
everything else and if you can't do that
get good at that before you add anything
else before we add a second service
before we had a third service one
artifact for all environments and use
the same deployment process everywhere
so let's go into Bob I think this is the
Marmite topic really why I should say
Vegemite topic
although Vegemite is awful so really I'm
a Marmite fan we've had this
conversations and then recently Cole's
delivered pro might to our house as a
replacement we couldn't work out what it
was a replacement floor we think it was
for the floor cleaner because that stuff
is vile anyway some people love this
stuff because it's a bit I'll do cuz I'm
like a cyst apni ops ii person this is
great you know other people go how is
this interesting but actually is bear
with me it won't take long um artifacts
how do we package our thing up what is
the thing what is the artifact we want
for our services what can we do with our
things what do you want from an artifact
well you know we'd like it to be easy to
create we want simple build processes
want simple packaging process is
simplicity is good we've got enough
problems on our hands of your building
my career services because we've got to
deal with cap theory and physics so
let's keep our packaging simple nice and
easy to use we want to make it easy to
deploy turns out some of the artifact
types that are easy to create actually
push problems into the deployment space
we want something that allows us to
abstract out the underlying technology
stack in a micro services world this is
important because often what you're
doing with murko services is you're
trying to give yourselves more options
you're giving us out yourselves the
ability to try a different technology in
a different service it's actually a nice
way of sort of incrementally adapting
new ideas into your organization the
monolithic system you tend to get to
make one choice about technology stack
idiomatic style database microsoft says
you've got the option to say let's try
something new here the problem is if
every time you do that you have a
fundamentally different deployment
process your whole deployment process
gets quite complicated and difficult and
so ideally want an artifact type that
sort of hides away the detail of what's
happening under the hood we also would
like a artifact type that is good for
developers and also good for operations
people you may be one on the same person
or you may not I love the time mostly
artifact choices are driven sometimes by
what's easy for developer people then
operations people take a look at it and
go this is horrendous they then create a
different artifact
right for their deployment processes and
what you end up with is one deployment
process for developers and a different
deployment process operations people and
we're back to this problem where
operations people find problems later on
because developing people aren't using
the same processes then so something
that's good for both so let's start at
the beginning let's start with tar balls
giant bundles of stuff in Linux will say
a tarball because use at our command
this could be a zip file now just like
oil leaks you know where you get like a
load of guts Eagles at something become
mesh together in oil and then people
died in those things tar balls aren't
always good because their thing is
they're just they're really messy the
reason we have these zip files or tar
balls of destruction is because one
reason one reason only they are very
very easy to create I just do some
compilation and they go yeah a zip it up
they'll be fine and then normally what
you do is you load that up onto the
magical capita pole of doom and Qatar
poulter across the yawning chasm between
you and the poor bastards that have to
deploy this thing and they receive it as
willingly as goals receive an oil slick
the problem here is that I don't
actually create easy to deploy outcomes
you've packaged a whole bunch stuff up
in the zip file when you actually want
to deploy you often have to move files
in two different locations you might
have to change ownership and permissions
of who can you can sit where you may
need to change execution flags on those
sorts of things well often you see with
sort of zip file or tar ball based
artifacts is then a lot of work in the
scripts to open things up and move
things into the right location this is
where people start using loads of
PowerShell puppet chef or ansible
because the artifact types them selves
have lots of work that needs to be done
uh sometimes they can abstract out the
tech stack it's a bit agnostic do good
jobs that you can do not good jobs of
that are they good for devs and good for
ups I'm sorry I'm a semi ops person and
I think these are a terrible idea and I
hate them and if you know when were the
last time you wanted to try out some
software on your machine and the process
was download the zip file copy these
you had no or do something else you know
you want a nice you know new get install
command line right for your own stuff on
your own laptop why don't you want the
same things for your production systems
so don't do this let's look at other
options well the whole bunch of tech
stack specific types of artifacts that
can be useful I think things like nuget
packages pip far you know also you get
packages jar files pips gems like some
of these can work for services some of
them are more fraught a lot of these
packages are often really built with
shared libraries in mind right shared
code you know you want this is your sees
your dependency management your package
management not really built with service
deployments in mind someone can kind of
work with all a little bit iffy they
tend to be somewhat easy to create
because if you know that technology
stack you ought to know the toolchain
there for now how to create the
artifacts and native to that tool chain
they're not always easy to deploy though
any of you here a ruby anyone's middle
Ruby round here no nibble a couple of
people I thought they kept the non marks
of stuff downstairs um but there's a
there's a whole host of like really
interesting problems associated with
trying to deploy gem files into
production systems to a lot of the Ruby
packaging deployment tool chains
actually open up interesting security
holes on your production systems so this
is a bit different but obviously don't
abstract out the technology stack
because by definition their technology
stack specific these deployments and by
definition are they also they're not
really good for dev and ops because lots
people suddenly now have to become an
expert in whatever text act they are to
get up to speed with the tool train that
you just started to pick it's less of an
issue if you're very homogeneous right
if we are if you're going to be just a
ruby shop it really makes a lot of sense
for your ops people to be really good
just knowing how to deal with these
sorts of things now I will cause say at
this point that there is one technology
stack out there that obviously has the
perfect form of text at specific
packaging and that is go
because everything go is awesome it's
just the best thing in the world you
know obviously this is the most buzz
word friendly talk here at NDC and I
felt it would be remiss of me not to say
go and and dakka dakka dakka dakka Go Go
micro-services go doc I'm like services
doh Go Go Dhaka unko is actually kind of
interesting because the sort of the
native form of packaging and go is a
statically linked binary because of the
way sort of the the you know the
packaging is handled they actually
intend to be very very small tiny little
binaries of completely self-contained
and that's kind of interesting because
they're actually quite easy to move
around and actually easy to deploy but
it just like copping binaries around
still feels a little bit hinky to me and
if you were to think about service
deployment you'll still need get need to
register that service put start/stop
scripts in beta may go is kind of fun
though anyway so let's skip over that
one as well kind of thing that's a good
choice for us now here's one that's
often overlooked greatly turns out most
of us are using an operating system that
has natively a very good mechanism for
installing software that just kind of
works like we're talking here about the
operating system specific packaging tool
chains how many is here how many people
here I've done something like this you
know logged on to the bun to box just
gonna install this maybe even a home
brew cask install talking you get
install these things kind of work right
they handle fetching the right versions
of software for hand is doing
dependencies that we rely upon and
actually if you look at this this isn't
that different from that is it I've got
some go get me this thing why are we
missing these two flags nodes are
actually easy enough to pass through as
variables if you want own wrapper
scripts it's not that hard to do this
these kind of are really nice and easy
to use so why don't we do it will often
because we fall the first hurdle
creating these operating system specific
artifacts is in general not easy hands
up how many people here have tried
bundling dotnet services as MSI's
people hands up who thinks that was fun
right so in the linux space our
packaging managers are just as difficult
to work with there were a bit different
across different platforms there is
actually a package manager which is a
kind of a meta package manager called
fpm which sort of tries to sort of
simplify the process and supports
multiple different online looks or
platforms and it sort of it simplifies
the process by only supporting about
fifty percent of what you can do within
the underlying things fpm stands for
effing package manager which gives you
an idea about how painful these things
are so we don't tend to put the work in
and it's a real shame the problem is
it's pain now so we avoid it but i think
if we bear that pain we really do get
the benefits because once you've done
that they're really easier to deploy we
will use the command line the developers
know those command lines operations
people known as command lines even more
so you have support tools around these
things so on linux platforms for example
i can do things like you know checks
signing of those packages to make sure
logging wrong packages I've got tools to
tell me if I need to patch those
services and this is fantastic we have
this whole supporting tool tray and
infrastructure that we're not using just
because we're too lazy to package and
properly in the first place we also
completely avoid the need for all these
scripts to move things into the right
places because all these packaging
software and lat all this packaging
formats allow us to define things like
what users these things will run out
what permissions the file should be what
locations these things will be inside
the package itself there's no need now
for elaborate chef or puppet
configurations because I just may puppet
you know config my PowerShell configures
becomes install this package this
version make sure it's installed done
not pages and pages of setup and
configuration you've got a debug can
keep fresh the abstracts how the
technology stack as well right you don't
care what's running something launches
on that port I don't know what
underlying language most of stuff I run
on my linux box is written in but it
works so why care good for devs goofy
ops well I think lots people love this
stuff
crazy for this stuff devs don't often
because developers don't work on the
same operating system as production
systems I mean even those of you in the
window space is probably the case way a
lot of you will be deploying them to
Windows Server you did what you're
actually developing some of you on
Windows XP Stella it all happens
seventy-five percent of all the ATMs in
Australia store on on Windows XP it's
the smartest thing I think Microsoft of
a did from a bottom line point of view
was end of life in support for windows
XP it meant all the banks in Australia
have to go and negotiate on a per client
basis support agreements with Microsoft
and they know they can't change a last
bank i work sat down the road here all
the devs had these lovely light macbooks
a bit getting given and they open the
lids and it was running windows XP and
that was just last year anyway so that's
the problem so we don't because
developers are often in charge of doing
initial build scripts they don't put the
effort into making easy to create so
they don't do it they just punt on it I
don't know the tool chains don't see the
value the operations people would love
you for this stuff they really would
love you for this stuff I really will it
doesn't solve all our problems so we
think about what happens I can I have a
build process I check in some code to
result that cheeky and I generate brand
new artifacts and I store those
artifacts it's my time use my OS
specific artifacts I'm going to stall
that probably in some kind of artifact
repository which is specific to your
operating system look quite sure the
equivalent is in the.net space but it
might be a Deb repo or a young repo for
se Sentar some red hat based systems and
that's great and so what I want to
deploy my service I bring up a host or
maybe I just log into a host or whatever
it is and I run my command line and it
pulls down that version of the software
runs it locally the issue is here is of
course one of the things we talked about
earlier we want interdependent well
independent employability of these
things and sometimes we can have a whole
bunch of situations where deploying more
than one service on to the same host be
a virtual physical host can have issues
can have cool
visions let's think of a simple example
of the kind of collision that we can
have to when deploying more than one
service on to the same host I said
before with the operating system
specific artifacts that you can do
things like define the dependencies that
you have as in for this software to run
for my sister on I need these other
things on this machine so in this
example the green service needs version
1.5 of a thing called a and the blue
service needs version 2.1 of a thing
called a sometimes this is okay and they
can be kept separate a lot of times they
can't especially if what you were lying
on for example are not in are
incompatible subsystems that need to run
that same system so you often have to
really understand the transitive
dependencies are of these systems it can
be quite challenging anyone here who was
deploying jboss apps in the in the 2000s
will remember a lovely thing called the
unified class loader where you would
just get random different classes
assigned to you if you had more than one
thing in the application container which
is blew my mind so this is you sometimes
get collisions here and that's just
because there is no isolation so we've
got a bit of risk now associated with
deploying more than one service in the
same place likewise we have the same
problem from a point of resources you
know I check in a new version of the
purple service and I've is that there's
a bug in the code and now it's
significantly more CPU hungry simple
significantly more resource hungry and
on that host it basically consumes all
the resources and thereby all the other
services on that machine stopped working
and so we have a situation here without
lack of isolation between those services
causes problems during deployment this
makes people dismiss makes people less
confident that they can release
independently this is why most
organizations that have been using micro
services for a while normally end up in
a situation where they have one host /
service a netflix for example their
hosts are they they have very big Amazon
machine images like 64 gig vm
and because their services are very big
as well in terms of the amount of
traffic they handle other places this is
more difficult right if you take say a
nap with fairly modest resource
requirements and you break it into
smaller parts those smaller parts
themselves have even smaller resource
requirements and it becomes a difficult
difficulty because that can get quite
expensive I'll come back to why in a
moment but nonetheless this is a nice
idea right we have no isolation I've got
my host I'm only deploying one service
on that I don't worry now about there
being other services other dependencies
that could collide with other services
I'm don't worry now about using up all
the CPU on that service affecting
anything else than that service and
everything else is protected this is a
great model this also opens up in a new
kind of artifact for us because normally
you don't do that on physical machines
normally you're deploying it onto some
kind of virtual machine technology
you're taking your big physical box and
you're breaking up into little parts
that means that what we can start
thinking about is packaging our services
as custom virtual machine images so
taking the operating system and the
service art you know the art show host
service code and saying rather than the
artifact of deployment being you know my
java application my net application
instead it's my dot net application on
that operating system packaged up as one
binary so real estate that come to a you
over in Melbourne Netflix loads the
Amazon people will do is quite routinely
they'll create what's called an ami and
Amazon machine image out of their build
pipelines there's some really
interesting reasons why this is a good
idea the first thing actually is it it
allows you to synchronize sort of
provisioning of the infrastructure
itself and the service so now what I'm
rather than thinking about ok so what
I'm going to do is I'm going to bring up
a host and I leave that hose running
leaving from running around for a long
time I just deploy new or newer and
newer versions of my service on it over
time you know this thing has been around
for a long time you know it kind of
works be not quite sure how
it people have logged in and monkeyed
around with the box you can never really
recreate that stay it's quite difficult
to crafter in this world when I execute
and launch my image it's a brand new
machine that means I can be very sure
that that machine has been created from
stuff that's actually checked into
source code which means i have
reproducible builds this is how you go
from pets to cattle in the cloud world
there's also provision really fast as
well they're not always easy to create
though so the am I tool chain isn't too
bad there's a manator from netflix that
can help you build these things when you
start getting into other virtualization
platforms though creighton's custom
images can be really painful the vmware
tool change certainly sucks now our
dashi corp who do some awesome tools one
of the tools they create a thing called
packer pack with sort of like the FPM is
like a verte is i could have a
meta-level virtual machine image
creation tool so it can create our new
virtual machine images for digital ocean
I think joint cloud now vmware vsphere
and a whole bunch other virtualization
technology that can be quite useful
because it can also create vagrant
virtual machine images which you can
then run on your laptop in sort of
isolated environments so if you are
interested in cost of images and you're
going to be supporting more than one
platform definitely could pack up easy
to deploy again this can vary so with
amazon ami is when you create that ami
it in the cloud you only ever reference
it by pointer effectively you pass
around an ID when you create a custom
vmware image that could be several
gigabytes in size if you've packaged up
their windows OS that could be 20 gig
are your network of people are not going
to like it one for every CI bill
checking you're creating a 20 gig vm
that you'll then moving around on their
networks they get cross about those
things so again this can vary they are
excellent at abstracting out the
technology stack though now you just
launched the thing you'll search is
already running you'd even know what's
there good for devs good for ops I think
they're good for operations people
you're turning on an appliance
your whole application now is this a
series of appliances that have a fairly
well understood States from an
operations point of view from a
developer point of view though you have
to be careful you have to have other
ways of launch an application because
the lifecycle of creating these things
can take quite a while so this is good
to a point but VMs are not free
virtualization isn't free and then as
you can think of is like imagine that
your your physical machine is a draw is
your sock drawer and you come in and
like you look at yourself and your socks
are everywhere right they're all mix
together your Red Sox in with your blue
socks with your green socks you think
I'm going to bring some order out of
this chaos and you get some wooden
dividers and you put this wooden
dividers in your drawer and you've got
nice now Scott nice compartments you can
separate things out isolated it's easier
do you have more space or less space now
in your sock drawer you have less the
dividers themselves take up space then a
load space they take up space the more
dividers you put in to give you more
small component component component
stock components a compartment in your
drawer the more divided as you put in
the more proportion of the volume of
your draw is going into dividers and the
less space is available for socks most
of virtualization technology we use has
similar issues as you further and
further subdivide a physical machine
into smaller and smaller VMs more of the
resources the machine itself are going
to ensuring the isolation of those
resources than actually being used to
run your services this can mean that it
can be very expensive to have lots of
small VMs this is actually the reason
why people have got interested in Dhaka
I think developers get interested in
dhaka kasoor look I can run a command
line and I've downloaded like 15
untrusted pieces of codons my production
system you know friends don't let
friends install stuff in the puppet
docker hub it's like finding a USB stick
in the gutter and plugging in
commissioning going I she'll be right
the reason it's great is because the
stock it gives you a nice tool chain on
top of container technology and
container technology is different is
subtly different for virtual machines
with virtual machines you most of the
what virtual machine technology or use
will have a thing called a hypervisor
that's sort of a separate sub system
that ensures isolation between resources
those virtual machines themselves have a
full operating system stack a full
operating system kernel and that's
taking up more resources as well
container technology works by having one
single operating system kernel and
effectively the colonel manages
isolation of resources so you can kind
of think of it like branches on a tree
in each branch is a separate container
as you have one colonel that's really
nice you don't have and you don't have
the overhead of the hypervisor so that
means your cost of isolated hosts is
drastically reduced because more of your
resources are running your services then
run the isolation which means you can
get away with smaller cheaper machines
much more cost-effective so also quite
easy to run this stuff you use download
doc when running it kind of works it's a
lot less setup than some of the other vm
technologies and resources are low as
well these things are all so blindingly
fast to provision container technology
in general is much much faster spinning
up a full-fledged vm on amazon which is
not bad it's certainly faster than some
not as fast as Google but farsan d'Azur
is going to take you a couple of minutes
right spin up a decent-sized vm two
minutes still pretty good but two
minutes are I hit enter and I'm
expecting it you know even before my
prompt has actually returned my you know
I'm a container should already be
provisioned we're talking milliseconds
here fractions of a second here to
provision containers when you login to
Gmail Google spins up multiple
containers just for you on demand to
handle your inbox right you're not
waiting two minutes for your inbox to
render so these are great we really fast
are they easy to create what so the key
thing tells know about doc his doctors
effectively a set of scripts on top of
content or just make it easier to use
are they easy to create as a few gotchas
but I show Sassy's then creating custom
VMs easy to deploy yes the tool trainer
on docket is really really nice
do they have checked out the tech stack
do a very good job of that the same as
custom vm xin this way they good for
devs and good for ops here the issue
mostly is is developers love this stuff
right operations people alike but the
whole load of tall chains that I don't
have now for running production
operations and when the biggest issues
here is actually managing docker across
multiple machines the court docket all
chain is really about single machine
management and so when you want to go
across more than one machine is your
motors you're very likely going to want
to do in a production environment you
need to bring any other tools on top and
that's we're going to look at next so we
can look at some platforms that allow us
to take images docker images and
sometimes other things and handle those
well across multiple machines so again
we want at my deployment platforms I'm
going to give you three with a compare
three but there are loads out there so
I'm gonna give you some sort of cool
things to think about when looking at
their platform as you want as we
mentioned before we need something that
allows us to separate our artifact from
the environment topology we need
something that makes handing these
things easy right because actually when
you have microservices you've got more
small things and therefore you need
things that can handle lots of small
things you can probably get away with
sort of less effective tools operating
at scale when you've got a small number
of large things when you got a large
number of small things any good
automation need good script we want
something that supports docket images
two of these actually you know one of
these platforms only supports docx
images the other two have a lot more
flexibility than that we can look at
three we look at dhaka swarm this is
sort of the free available out of the
box our and multi machine management for
docker and that comes from dhaka
themselves look at me sauce which is the
oldest tool here but we the most mature
as well argue but sort of like a giant
Swiss Army knife like a really giant
Swiss Army knife because so it can get
me hard to get your head around and
finally we can look at kubin Elias which
is sort of the oldest is the if the sort
of the oldest and the newest tool here
so Cuba Aires is based on abstractions
that Google use internally
but it's an open-source cleanroom
reimplement ation of their internal
approach to managing containers so
things to understand is poo benetti's is
not what Google runs internally because
they're not stupid enough to let you
have that for free it's sort of their
sort of external version of it start
with docker swarm Duggar swarm works
like how most these systems work where
you have the machines you're going to
run your doctor images on and you have
some central controller with daca swarm
you have a thing called the swarm
manager on your node you have things
called swarm nodes and the store manager
talks or swarm notes the nice thing
about docker is if you get used to the
command line so you know how to run get
processed listings get logs from a
single docker node like on your machine
all of those worry about seventy to
eighty percent of all those commands
also work at the dock a small level so
you can run this safe seven commands
like docker PS docker log on the swarm
manager and it effectively multiplexer
out so it makes it really easy for
people that have used for single docker
machine management start working with
myth images in a docker swarm
environment it's the small manager's job
to work out okay given what you've asked
me to do where should I go and put these
nodes so its handling affect what we
call the scheduling so it can scatter
different Dhaka nodes so different
docker images containers across all
these machines for us one of the things
it uses is different scheduling
strategies it's a similar thing that
both cupones and meat sauce have these
as well but it's worth diving into what
a surging strategy you can do for you so
for example one of the sort of built-in
sharing strategies is bin packing so bin
packing is looking to optimize resource
consumption so based on the profile of
enemies you want to deploy it's going to
try and use up all the resources on a
machine so this is actually a very good
approach for optimizing for a small
number of machines so in this example
here we've used up all the resources on
this node so we've got over to the other
node here and in this node doesn't have
anything at all this could be a really
useful thing to do if you're in a
dynamic provisioning environment where
you could turn swarm nodes off to save
money so an amazon for example to turn
it off if you're not using it this may
not though make sense if what you
actually want is
dillion see because you can you could
end up with all the nodes of a given
service on one machine which is pets you
give you a single point of failure so
something like that you could then use a
spread strategy and you when you sell 15
instances you spread it will try and
evenly distribute that load across all
of these notes for you the other is even
a random changing strategy for just like
spreading your stuff in random places
I'm guessing just for a laugh because I
can't really think why you'd want to do
it but anyway other nice things you know
this one of the best things about swab
is the way in which embraces the normal
doc at all change so it's some of you
may have used docker compose so this is
an example of a very simple docker
compose file so this allows you in yeah
more format to sort of stand up a
topology and this example we're saying
I've got one node which is called web it
exposes some ports it has a linked
volume and it actually has links which
is sort of a software-defined networking
thing Lou through to register these two
things can talk to each other and by the
way I've got ready snowed and that's the
image I'm going to use this actually
allows you to stand that little
topologies for your individual services
it works quite nicely and this stuff
works as well on docker swarm the one
thing I would say for independent
deployability you're deploying one
service by itself that means you want
these docker composed file to just be
the topology for one service the
exception actually is when you want to
stand up say a whole collection of
services for so maybe test purposes then
docker compose and things like it can be
quite useful so may be nice to have five
or six sources together for set
performance tests they can be useful
there that's kind of handy so what do we
think of docker swarm well there are
some problems ish I users say it doesn't
be balanced so if a node went down it
wouldn't necessary distribute your
containers and it wouldn't restart this
file containers those things have now
changed but it's fairly recent and I say
recent as in it was about six months ago
that these things were fixed the reason
I'm hesitant here is because when you go
into production on a platform like this
it kind of needs to work and for me when
you've only had the ability to rebalance
nodes when they fail or restart
containers when they fell fairly
recently and I've heard the stories
Whitson always work consistently I get a
little bit edgy so the new changes that
are in there that's a key thing you're
looking for from a shredding platform
you want the ability to run this stuff
out what you often really looking for is
so the characteristics what we call
autonomic systems you want the system's
themselves to be self-healing if i'm
distributing 50 instances of a service i
don't have to come along every 10
minutes and check they're still 50
instances in my service I want the
sharing platform itself to ensure that
there are always 50 nose running so it's
all like this auto scaling groups in
Amazon could do this for you and and
Dockers only docker swarm has only been
able to facilitate plays very very
nicely with the rest of docker however
the case studies are very thin on the
ground last time i did research this was
about six months ago i found two case
studies for people using docker swarm in
anger one was from rackspace one was
Mary Riley who actually published my
book I found out that so the rackspace
case study was where they'd use docker
swarm to implement a platform as a
service the o'reilly case study was
actually an in case study of them using
the Rackspace pass that was in turn
using docker swarm so actually those two
case studies were actually the same case
study again I'm a bit risk adverse I
want to like this is not necessarily
where I want to take my risk and so
docker swarm is a bit you know is it
it's very easy to use and get up and
running but these are the things that
give me hesitation about picking it as a
production platform talk about me sauce
meat sauce again is very simple as
similar right we have a meet us master
the miso smarter stalks to the actual
machines where we going to our our jobs
it talks to a thing called a missus
agent and at this point we have to talk
about one of the things that makes me
saucepan and confusing because we have
to talk about what me sauce calls a
framework I'm me source framework isn't
really confused me for a while because I
think of a framework as something that
helps me use another thing right you
know so like hibernate is a framework
for not understanding databases um but
something to help you something else
frameworks in me sauce are something
quite different and it not really
helpful we do this all the time in
computing we have terms that we
understand what we mean and then we
decide in a different context use that
exact same words mean something
completely different it's not helpful a
me source framework is actually really a
pair it's two things it's more like a
plug-in or to be a better phrase for it
what it is is a scheduler and that's a
bit of software that would run on the
master and works out how to sort of send
out the jobs and it's an executor and
that's an environment in which a job run
so you can have one me source framework
for her dupe and actually the space that
me sauce is often used in is the space
of things like data processing running
Hadoop spam swarm on sparks a Hadoop
spark and you other MapReduce type style
jobs so I have a Hadoop scheduler and so
when I want to in my Hadoop job I
talked the Voodoo pleasure and it will
have it will support sort of my
ecosystem and then I run an executor and
my jobs run inside that executor and so
this is quite a pluggable system this
already is great because then you can
start thinking that I could have one
mrs. cluster that I could use for
multiple different purposes which is not
necessarily the case for daca swarm the
plug in the framework that we would use
for running microservices would be
marathon to marathon is a framework for
running persistent long running jobs so
it's the thing that will ensure sort of
the state is managed so marathon
supports natively supports docker images
it does support other types of
applications running inside it it is a
thing that's going to maintain desired
state this framework model is really
interesting because when a new
technology comes out you can actually
drop it in and support it more natively
so for people like your you know your
data processing people that really know
about running Hadoop jobs and that stuff
right using the Hadoop shed use a nice
level of distraction but you're making
multiple uses of the same underlying
hardware this can actually make for a
really good use physician of your
physical resources
also think of other types of frameworks
you can drop in so aromatic is an
interesting framework with marathon is
for long-running persistent jobs that
maintain desired state aromatic is for
really short wedding jobs it sort of
running one shot containers so I run it
an executing it dies it's all very
useful is it hands up though who here
has heard of Amazon lambda right
aromatic is kind of half of Amazon
lambda running now on your own
infrastructure it's the part that gives
you a command line that lets you are an
API to execute that container all you
need to do a stick an API gateway on top
of it and aromatic kind of gives you
amazon lander running on me sauce
alongside now you've got your service
architecture guard that time annoys me
alongside your done service architecture
on your servers anyway it's very
confusing but a very powerful model so
nice us very is great if you've got
mixed workloads it C it's also very
interested if you look at experimenting
with their d source stuff that their
data center operating system that
provides of high-level abstractions for
running things like Cassandra you can
mix all those work clothes on the same
infrastructure it is very powerful it is
very fully featured it is very widely
used there are some great case studies
out them out there how many people here
have an iphone how many people here have
you Siri on your iPhone you are running
on Amy source cluster so Apple did some
talks like last year talking about how
they use meat sauce to scale series
actually wrote their own framework
rather than using marathon and they're
running like 30,000 node mera missiles
clusters so there are some great case
studies at very large scale here but
there are lots of moving parts you sort
of as a few things to get your head
around let's talk about Cooper nellies
you know throwing UV sexy it's got a
google it's got some quite a few
companies behind it core OS are behind
it Red Hat's openshift v3r which is
basically a rebadged
gaged cupen Eddie's and Microsoft
actually had one of the earliest
examples of public hosted cupones on
Azure they got that out there for Google
you can run sort of turnkey Cuban it is
on a lot of hosting platforms similar
architecture very familiar architecture
now now we're going to think with an API
server we now talk to things that are
called Pew blitz that's quite cute I
like that Gilles know and like all the
other platforms I've got some command
line staff or I can make API calls this
central server and that's going to
schedule my jobs and it deploys things
as well now the things it deploys a
slightly different we're not deploying
now instances of a docker container
we're actually deploying a thing called
pods so pods are a tightly coupled set
of containers that will run at one node
so you might have to or you can tell it
all together in one node and that get
deployed together all the time you might
also Bund in their configuration this
could be how you put your configuration
files in there so your configuration
files could be one way of keeping said
from the artifact to go in here or four
volumes positive cells are kind of
mortal this isn't a conceptual service
this is just like I'm deploying a pod so
sort of when you schedule something
you're scheduling pods to be deployed
now this abstraction bugs me because it
really feels like it's an abstraction
based specifically on performance
optimizations that Google use internally
you're not going to want to deploy pods
with loads of services inside them
because you want independent scheduling
of your services give you an independent
deployment of those services therefore
it when you have these pods anything you
really like to have in them very often
are going to be maybe a fight of some
configuration files or maybe like a
volume that's it you don't really going
to be combining multiple services
together inside one pod kooba ladies and
meat sauce and docker swarm all have
scheduling strategies that allow you to
say when I deploy this service could you
try and deploy it next to this one if
you can for performance optimizations so
you can already handle that stuff at the
schedge new layer so quite why did it in
pods as always bugged me a bit and this
is an internal abstraction it's been
made external I don't think is always
very helpful
and this isn't really a service either
this sort of feels like it's trying to
be a higher level of abstraction and the
other stuff we talked about it's not
quite but it does have a concept of a
service which meat sauce doesn't really
and service definitions look like this
it's in Jason which really annoys me all
the examples in Jason they support ya
maltu and yam was much more readable
anyway so here we see some stuff we see
we've got a name of a service we've got
some port information this is so similar
layout of dockers form this again like
dr. swarm could be how docker compose
could be how we define our topology for
our different environments and so
basically what a service is is a mapping
between a set of pots and some metadata
and ports so you see here this thing
called a selector and what that's going
to do is say this service consists of
any pods that map this selection
criteria and so on the machine what's
actually happening is you've got a
little thing called a service proxy and
as pod to deploy the service proxy is
going do matching service as I know
about are you do right I'm going to
expose you actually can you've got other
things inside q Bernie's that allow you
to do things like sort of sort of load
balancing as well so you can actually
which is again a nice feature and then I
thought is great okay quietly I don't
really want to actually work at the
level of the service conceptually I
think about scaling up a service I'm
talking about deploying a service but in
Cuba no it's all a bit different because
you don't scale a service you actually
scale the pods you don't deploy a
service you actually deploy the pods and
this sort of doesn't really spit very
easily with me and so actually most
teams that I spoken to and the company
working at will end up with abstractions
on top of this stuff to hide away this
sort of odd disjointed set of
abstractions anyway I could raise it is
a slightly simpler set up the me sauce
are you say simpler I'm saying
difference is not that great anymore to
be able to know now looked at play
around and committing more anger it's
also more single purpose you don't have
this idea that you can do mix work
clothes doesn't have the same pluggable
execution
concept the measles does this is going
to be more single purpose for you he's a
little bit closer and abstraction to a
platform as a service but it isn't
actually close enough to Heroku like
pass for it to be really a big
differential in my mind it adds some
things which are useful replication
control stuff low battery type stuff but
still not high enough level for me to
say that I would definitely use it
because of the high level abstraction
pods can be a bit confusing it is fairly
new it still does have issues they have
problems at the beginning of the year I
know it's at the end of last year where
they couldn't actually scale build 100
machines those things have been fixed
people are working through I said I have
got an impressive industry support I
would say that you can run these things
you know you can go on fire / cupones
instance on google or microsoft you can
also the same thing for me sauce though
as well so in terms of sort of
summarizing this from thinking about
your three platforms right now I'm
interested in sort of a darker based
deployment for my micro services I
pretty much take docker swarm out of the
mix immediately it might get there
eventually it's not there now that's not
in our features there i would probably
it's just baking do these off and i
would suggest going to somewhere like
Google and just spinning up a meat sauce
instance spin up a cuban areas instance
and then just sort of like to work with
and then think of that from the sort of
deployment process point of view and
then try setting these up yourself and
see what they're like um they both have
they all have little quirks and little
interesting issues I I found it quite
funny I was driving into Cooper nation
see how it does secret storage and the
way cupones stores secret information is
in plain text which is odd anyway these
are some there are some alleged to be
hard problems here but yet to summarize
talk about the core principles of smart
grid service deployment independent
deployability make sure that whatever
you do around your tool chain around
your artifacts around your platforms
allows you to make one change with
service and deploy it by itself into
production one artifacts for all
environments don't rebuild our
to fax that introduces delay and
introduces the potential for error so
separate your topology from your
artifact to make that work and use the
same deployment process everywhere
consider using docket images as
artifacts because it gives you much
cheaper isolated execution environments
your services even if you can't do stock
images consider things like custom
images if you're on a cloud platform
think about Amazon am i sore
digitalocean think about droplets i also
have shared with you a few criteria for
selecting a platform i will leave you
with one just sort of caveat this whole
space is a little bit new and edgy right
it's not just that the platforms
themselves on you and have pretty more
Stern than any other space in
microservices these deployment platforms
but the concept of what good looks like
is changing for us all the time so just
be aware that I think it's it kind of
very important when you get to a certain
scale marcoses that you have a platform
that gives you these abstractions but
you may actually end up picking
something and then revisiting a decision
in a year from now it is essential you
actually bake these things off for
yourselves going to go to a vendor great
but don't go to just one vendor get at
least two of them into bake-off I was
going to do some other things if you
want to buy my book directly for my
Riley you can use a code off d that gets
you forty percent off I've also got a
blog and a podcast Sam you and I oh and
I think we're a little bit over but I'm
happy to go into the break to answer
some questions so any questions and r
over there so one of the problems we
have with independently deploying the
microservices the version dependencies
of other micro services depending yeah
what are some tactics around that so
you've kind of got so firstly is knowing
whether or not you're going to break
somebody else right ideally you don't
ideally do expansion only changes and
you don't make back sync battle changes
right that's idea the key thing is even
when you think you're doing it you may
get it wrong um so then I would say
about your testing approach to catch
those cases we need to know it's going
to happen that's actually Beth scurry
there is the third second person down
who I didn't do too did a talk a couple
days ago about tour called packs which
does contribute if in testing that helps
catch the changes before they occur but
sometimes you have to introduce
backwards incompatible changes so then
you've got two options one option is
okay I've got I'm breaking you you're
calling me I'm gonna break you breaking
you so I'm actually going to leave the
old version of me lying around so you
can still call the old version of me and
I'm going to deploy a brand new version
next to it and then i can give you time
so when you're ready to make your
deployment trained to use the new API
them to use the new backwards
incompatible change you redirect to me
instead then I can retire the old
version that requires work around
service discovery knowing what you're
running requires conversation about this
stuff and some places I've seen they
can't they don't get the clients to
update and as you leave these things
lying around which has a bigger
footprint when you have multiple
different versions of the same service
lying around they are effectively
branches of code you've got to fix a bug
you've now got to fix in more places the
other option is when you make it
backwards incompatible change is
actually effectively on the same service
version to have two different endpoints
so it's the same version of the code but
you have two different endpoints um I've
used both approaches I prefer the latter
because it simplifies my topology and
avoids that sort of branching issue but
both a viable probably time for one more
question okay well if you do think of
anything else i'm at sam newman I hope
you've enjoyed NDC please as I do give
feedback and please do think about
coming back next year tell your friends
and your colleagues but I hope you enjoy
the rest of today</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>