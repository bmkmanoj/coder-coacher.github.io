<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Developing for Mixed Reality with HoloLens - Rafał Legiędź | Coder Coacher - Coaching Coders</title><meta content="Developing for Mixed Reality with HoloLens - Rafał Legiędź - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/NDC-Conferences/">NDC Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Developing for Mixed Reality with HoloLens - Rafał Legiędź</b></h2><h5 class="post__date">2018-02-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aKkzIoQfIvs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hello thank you for being here
thank you
thank you for attending my talk
especially that in room one there is
like a good selection of speakers on
stage so special thanks to you that you
are here what we are going to talk about
today is hololens and mixed reality and
how to kick off some development on that
so let me first introduce myself I'm the
ruffled leggings you can find me on
twitter at traffic and I'm a software
engineer from Poland I work for a
consultancy company called
salt brain it's in Krakov and mainly for
last several months I'm involved in
creating those AR experiences that you
can run on hololens I'm talking with
different businesses helping them out
creating proof of concepts and get them
understand what we can do with this kind
of technology also I am a conference
organiser I organize Def Con conference
in Krakov Poland for last 8 years and I
try to be a conference speaker so
whenever I think I have something
interesting to say I apply to
conferences and as I'm here for example
so let's first explain the abbreviations
that might appear in my talk so VR and
they are V R stands for virtual reality
and this is basically the experience
that you can get with a VR headset that
you put your head and then you're being
brought into the fully synthetic
environment like your view is totally
uploaded and everything everything what
you see is rendered by the device I mean
the computer or the device itself and
your mind has been tricked into thinking
that the real world actually doesn't
exist that the the your word is actually
this this virtual world that you are in
and if you ever try to oculus HTC vive
or anything like that you probably know
that that it's really really really
immersive and also you can see a lots of
videos on YouTube with people freaking
out in those headsets right so that that
we can see that the immersion effect is
really good and it's fully synthetic
environment
the other hand we have AR which stands
for augmented reality and this is
something usually being run on mobile
devices and this is overlaying digital
content over the view of the physical
world over some some devices screen so
whatever the device C for the lenses is
being displayed in the screen and then
some digital content is being overlaid
onto that and sometimes it's in the form
of some wearable goggles or glasses so
but we have mixed reality in the title
so it's not augmented reality nor which
variety apparently and Microsoft is
using this term a lot recently
especially last year so what is mixed
reality and I I'm gonna explain that
because I can see lots of confusion
about that mixed reality so we have
those two words like a physical one a
digital one and it'll cure that in
between there's a whole spectrum of
experiences that we can get depending on
the application or device that we were
using so all the AR experiences are
really close to that physical reality
end of spectrum because still the
physical world is really centric to the
user when using most of a our devices
nowadays because they run on the phone
so the user is still fully aware of the
environment around him or her and well
the immersion level isn't that good some
of them devices are a little bit more
sophisticated they have some more
sensors there might be in form of some
goals so they can deliver a little bit
better experience on the other hand we
have VR because well it's only on this
digital reality and because as I
mentioned it's whenever we are using VR
we are fully in the digital world there
is nothing coming from the external real
world actually and even like when we're
kind of immune to impulses coming from
the real world when we're really being
immersed into the virtual one so that's
why most of them VR experiences lay on
this side and hololens the device we're
talking about this
is it's actually an AR device
I mean Microsoft likes to say it's an
mixed reality device but it's in my
opinions an AR device it lays on this
side of the spectrum or physical reality
is because the user still is in this
physical reality but the way Hollins
works how it can blend digital content
into the users environment and that's it
it's in the form of glasses
it somehow delivers more sophisticated
air experience that the user can be
actually tricked into thinking that some
some digital stuff is actually the real
one etc and the digital stuff interacts
with physical things and vice-versa and
some digital digital objects can be
occluded by physical objects so the
immersion level in this case is really
really better than whatever we have seen
earlier in our a our devices and it's
also worth to mention what has been
released what was released in mid
October last year those were mixed
reality headsets and those are basically
VR devices
that's that are coming from Microsoft
and partnering companies like Isis Acer
samsung Lenovo and Dell I don't remember
what else and basically those are VR
devices but the main difference between
oculus vive for example and those
devices is that and those of Isis there
has been a technology from Collins used
so there is no longer need for external
external trackers to track the head
movement and controllers movement so
we're kind of not constrained into the
disk we're being tracked by those
external trackers but but the headset
kind of tracking itself by itself so
it's a little bit more those are a
little bit more sophisticated VR devices
and this whole spectrum
that's what mixed reality is so
ever is in that spectrum that's mixed
reality so every experience here is
mixed reality it's nothing new so AR is
mixed reality actually so I think at
least I I cleared the confusion in some
of us because I I was confused for a
long time as well as so : sis this is
this this is the device I have it with
myself so whoever would like to try it
after the talk I'm happy to show it who
has tried it already yeah that's what I
think who have heard about it earlier
yeah that's what I thought because I
mean it's nothing new it has been
announced in it was announced in late
2015 then released in mid 2016 so it's
been a while since it's on the market
and actually there are lots of companies
embracing this kind of technology and
and they're actually selling this to
other businesses and I have seen like
real products being deployed in
factories and stuff so it's it's a real
thing and it's happening and well it's
an independent computer say it's
self-contained it doesn't need any
connection to any other computers so we
can go free it's in form of the goals so
we put it in our hand head and use
gestures to operate it also well it also
runs Windows 10 so we can expect that
the tooling to create applications for
it is pretty common I was saying that
we'll get to that later and well you put
it on your head fire rate the fired up
fired the application on it and all of
the sudden you have some Holograms in
front of you it can interact with if we
can manipulate them and well that
depends on an application of course but
it looks something like this like you're
wearing a headset and then you see this
augmented reality think so how is that
possible that we can get this kind of
experiment where we can expect that
there is a loss of well fine engineering
inside and and it is well when it comes
to sensors you can simplify it by saying
that there is a Kinect connected to your
forehead but it's well it has something
more
those four cameras here on the sides
those are environmental understanding
cameras so basically there are scanning
user environmentally all the time
whenever the device is on it's kind of
the environment and really it's whenever
and remembers whenever the surface is it
just passes it forward to the system
just saves it in the system and we have
RGB camera def sense or mixed reality
capture module which allows us to record
whatever the user using : C or stream it
somewhere and we show that later also we
have four microphones here which are
pointed kind of down towards users mouth
to be able to catch voice commands and
to be sure that the voice command comes
from the users manufacturer not the
person standing next to the user and
next things are optics so it wouldn't be
possible to display those graphics that
would appear like some of them would
appear like closer to us some some would
appear in a further distance for up from
us
it wouldn't be appear without some some
advanced optics mounted there so I'm not
going to dare into details how what kind
of technology is being used there well
there's one disadvantage of this is that
the technology being used here it
doesn't allow for large fields of view
so it's not like we can see the
Holograms being rendered like everywhere
but we're constrained to like this kind
of rectangle in front of us so that all
the digital content is being rendered
here and that's it so that that's why we
can see like a lot of head movement when
you are looking at people using hollow
lenses but a part of that it's it's
quite awesome it has to age the engines
so it's basically 720p per eye and it's
enough but we do not describe those two
screens in terms of resolution we might
know from regular screens because well
we do not have to render like all the
pixels because they are there for free
like the rendered by the real word we do
have to only render Holograms there was
3d objects and
we are only caring about those pixels or
points they're calling them light points
that builds up Holograms so that's
that's the difference and also we have
holographic processing unit which is a
specially designed custom processor it's
a core processor for hallways that is
responsible for processing all the
sensory data that is coming to
homelessness so reading the environment
or processing user gestures etc it all
goes through this and then being passed
to for further processing and a part of
that there's also like a regular
processor but it's 32-bit not 64 so that
might be a surprise for some also it has
bluetooth Wi-Fi battery is that a low
for like two or three hours of active
use and well that would be pretty much
it when comes to the most most things
when Council Harbor and how do we
interact with this piece of hardware how
do we interact with holograms and how do
we each two comments so we had the
paradigm which is called wgv which
stands for gaze gestures and voice so
gas is basically you can describe it as
as a laser coming from your forehead and
that's the primary kind of pointing
mechanism there so whenever we would
like to interact with some object inside
application we have to kind of look at
it and then if we're pointing in this
object we can then issue some comment to
that object and usually it's visualized
by a cursor that is being rendered at
the end of this this ray coming from our
forehead and whenever that way hits an
object or some other thing it did the
cursor is being rendered at that point
we'll see that in the example and then
gestures we have two gestures there
they're basically the one is they bloom
gesture which is like that this is the
system gesture and this is for closing
applications or firing up start learning
and we can't use it
our applications this is this is system
just so we can't override it but we have
the other gesture which is air tap which
is basically like this this is this is
basically a click so whenever we are
looking at something this is the focus
object and then we can issue a comment
like that and then handle this click on
this object and we will see that in
Tunisia in an example but based on that
just so there are some more just
gestures derived so there's a whole
gesture which is basically the air tap
but we're holding the fingers together
and then there is manipulation and
navigation gestures which are like hold
and then moving your hand around and
then we can read the Delta of our
fingers moving and being moved in the 3d
space so that we can do some
manipulation on our objects and we will
see that and then examples in examples
later too
and also we have voice where basically
it's we can program voice commands we
can detect English words only and it
it's just in Mechanics for issuing
commands as well some ways we can look
at something and say something and then
something will happen or just handle
some comments like globally without
focusing on some object so that's pretty
much it when it comes to interaction so
given that this technology we have all
sorts of different scenarios that we can
solve with this with this device and
technologies so just to name a few and
really if you look in the internet there
are plenty of them I mean you can see
companies growing on this technology I'm
going I'm gonna just show you just a few
of them which are pretty famous one is
from a city company you can see
basically like a trading or financial up
here where all the data is being
rendered in form of some 3d objects and
this person can just interact with those
data using gestures and gaze and and
some other person sitting somewhere else
can also see that because you can share
this environment and well for all of
those examples you can google them you
will see videos on YouTube on how or on
how they work actually and the other
example is for they they announced
recently that they're using whole lines
for
purposes and enhance enhance their
design process and what we can see here
for example there are two people wearing
whole Lance's and this is one of the
cool features of Holland's where you can
basically share the environment the
holographic environment so those two if
you write the app where those two apps
can talk to each other like shared
session then the coordinate system of
those two applications like a line and
then both users see can the same set of
holograms and they can interact with
them together so that unlocks pretty
cool collaborative scenarios I would say
and you can add up like several hallways
I don't know what's that they did the
trash whole time probably 20 something
or something I mean in terms of
performance and then we have Schneider
Electric which is they created a up for
service workers so there's a guy walking
towards some kind of appliance or piece
of machinery and then you can detect if
this is this kind of machinery that we
are supporting and then overlays some
digital content onto that machine
showing some steps or some instructions
or whatever we might think it's suitable
but if you would look at this video
basically it's it's maintenance app and
it shows like arrows like unscrew this
bolt or whatever I mean we can you can
just render anything
and this in group company from Sweden
they're manufacturing the manufacture
elevators they also have an application
for maintenance workers so whenever
someone wants to do some work on some
elevator he can get or she can get some
contextual information about this
particular machine that that the work is
being done on and we can have like a
model being displayed of the piece that
we're going to do some work on or some
manuals video tutorials like whatever we
want right based on the on the thing
we're working on but those those
scenarios they do not use one of the
coolest features of hallways which is
special market as I already mentioned
before
I was describing the sensors we have for
environmental understanding cameras
there and there from the moment the
colas is turned on it constantly scans
the user environment I mean you can't
stop it you can you can turn off whole
lines and then it stalls but whenever is
turned on and even laying like that it
just scans everything it can see like up
to like three meters or something like
that and it stores it on the operational
system level on hard drive and and it
builds like internal models of locations
that it detects it is bad so we can't
access it directly I mean we can't go
anywhere in the system and just see what
kind of what kind of locations are saved
but as developers and creators of the
apps we can have access from our
application to this data so basically in
our application we are getting we're
using special data we are getting a full
3d model of our location so basically we
can have like the mesh of the room we
are in right and that mesh is aligned
with all the surfaces so we have floor
and walls and some chairs etc so we have
this mesh and in this guys this match is
just visualized it has some material
applied so that it it you can see that
shoe alley because because well
everything what this black is actually
transparent on Hall answers so if I
would run it on Hall lines I would see
like that is this wide grid like played
over on every surface here but what's
exciting here is along with this mesh we
are getting also the colliders colliders
is is a kind of mechanism in the in the
engine that allows us to to program
interactions between digital objects so
if we if we program some Holograms some
3d objects in our application and have
colliders everywhere where we have
surfaces in our real world then we can
you can program some interactions
between our for example our digital ball
and our physical floor because that ball
will
the collider that we can't see but it is
aligned with the floor and then you have
all those scenarios where you can
actually interact with physical objects
right so we have for example those are
games released at the beginning when
they when Hollins was released to show
the the possibilities of special mapping
go say so for example have games where
where agents are walking on your floor
or jumping on your table and it looks
fantastic it's really convincing and
this game for example this is fragments
my favorite basically your room is being
transformed into a crime scene and you
are in detective and you actually have
to look for clues that are hidden summer
in your room so we have to actually
crawl on the on your floor and just peek
here and there and just look for the for
clues which is awesome and you have some
hologram people walking around and they
can actually see it on your table or
sofa which is super convincing as well
because because because when they were
programming this application they detect
they can detect where are the seat table
platforms and then they can program this
agent to just find suitable platform and
just see they're just it's super awesome
I mean this experiment is pretty awesome
and the other thing rubber right this is
I think this is the most common one it's
an alien invasion that the aliens are
drilling through the walls and there
they start shooting at you and you can
shoot them as well I mean this is this
is really cool I mean you should try it
if you haven't so to link right we are
here for for a session about developing
so this is the the goal the previous
slides were was the intro and here we go
into the the core thing so the most
common tools for developing apps for
Hall lands are luma key and c-sharp well
and and we can write this G sharp in
monodevelop or Visual Studio and this is
not the only scenarios you can use
directives in C++ you
Unreal Engine or C sharp F sharp the egg
source buicks I don't remember the name
but whatever I mean there's
options but the most common one and the
best supported one is this one
I mean it's supported by Microsoft by
unity and the community that that grew
around this technology also uses mainly
this setup so and it's well it's really
nice to use out same writing C short
scripts and using unity which is pretty
nice IDE gives us well I mean we can
quickly create some proof concepts for
example I will show you that in a second
also although although unity God support
for Hall answers not that long not that
long ago and we can use scripting API
from unity to write our apps you can
well there were people from in Microsoft
and other companies that were creating
the first demos that we're using that
and they observed some patterns some
commonly used sonars etc etc so that
they extracted everything that that they
thought it might be useful and then they
created this package which is called
mixed reality toolkit it you might have
heard about it but it but area it was
called hollow toolkit and this is an
open source library with a set of
components scripts and some shaders
materials etc that really speeds up the
development especially at the very
beginning I will show it later and and
my demo is going to be based on this one
just to show you how quickly how quickly
we can get into some up into just for
the proof concept thing and also we have
mixed reality design labs which is also
an open source project but this is more
like more like demos on how things could
be can be done properly so that we can
just wait for the code and learn
upon it I'm gonna show you well this is
the last light demo and now let's jump
into gonna show you quickly this is the
oops okay so this is mixed reality
toolkit and well it's on github it's all
well documented I would say we have we
have everything grouped into categories
so that whenever we want something to
handle like input like gestures or voice
it's here for sharing is here special
mapping etcetera results etc etc it's a
very well maintained maintained project
I would say especially that it is very
fresh I would say so well and if we go
to input for example Internet should
have yeah we have all the all the things
we need including diagrams whoops but
what I want to show I know to make it
bigger
okay what I want to show is that if you
would like to for example programs and
gestures by directly in unity would have
to write some code like significant
significant amount of code but using
mixed character toolkit it all all boils
down to implement some interfaces so
basically if you want to handle air tap
on an object we just we just implement
implement input click handler interface
and just implement one method there
which makes it way more easier than just
going through the Unity scripting API
and I was trying to recommend using that
at least at the beginning and the other
thing design labs as I said this is a
settle
like ready things that we can go through
and see the code and how the things are
done how materials are done how shady
there is out that are done how
interactions are done and this is also
done by Microsoft this is kind of a part
of could be as a point of reference to
see how things could be done okay and I
think now it's the demo time I have
still my time 24 minutes so for that
okay so this is unity how many of you
have already seen that
okay so quite quite a lot so what I'm
gonna show you is how to create like
really really simple proof of concepts
because I mean based on my experience
most of the people businesses that asked
me to first that asked me to do
something for whole lines it's basic to
visualize some model that they have and
to put it on whole lines and just render
it and just do some simple interaction
that's usually the first step and well I
wanted to show you just that how to put
some Holograms hologram into whole lines
and just do some rotation on it
as simple as that and use special
mapping so that we will be able to
actually put it on our floor so yeah
this is okay so this is this is unity
this is the idea of unity and well to
just go quickly for the components this
is the main senior that's the place
where we layout the whole scene of our
application that I mean all the objects
cameras lights and all that stuff music
I mean
and this is the car key pane where we
have all our objects that are being used
in this chain right now this is the
project pane where we have all the
assets imported so that we can use them
in the program and here is the inspector
pane which is basically like the
displaying properties of the things that
that are highlighted in the hierarchy
like
when I hired that this object then I get
I get the properties of this object and
can act upon it let me get my cheat
sheet okay so yeah so I have a model of
a forklift and I would like to place it
somewhere on the floor here and well
sometimes I mean most most common
scenario is that the customer sends me a
model from solid brain or AutoCAD and
the the first step here would be to
convert that model from solid to mash
and that's that's something that I'm not
going to go here into details because
that's pretty complicated thing I would
say and there are tools out there to do
that and well let's let's start then I
started this empty environment and
already imported whole toolkit or mixed
reality toolkit
as I mentioned I did that before because
that takes time to import it so I wanted
to have it in place so as we can see you
know okay okay so here is a whole book
we can you see that pretty clear okay
and as we can see as you seen already on
the github in the github repository they
were like categories there and
everything here is more or less in the
same I mean folders here are more or
less the same as categories so if you
want to look for some components for
input like gestures for example or
camera we all can input input folder and
then just go to prefabs and we have
everything we need here right okay so
first things first
we need to delete the camera that is
being put here by default because this
is not it's not configured for whole and
SUSE it's just configured for regular
games with the transom on the computer
or a regular screen so we need to delete
that one and I'm gonna I'm gonna drag
some components from
charity toolkit like Holland's camera
which is camera configured for hololens
and then input manager this is something
that is being being responsible for for
adjusters for example and and voice
commands also and also guys like the the
the pointing mechanism and also I need
the cursor what I want to show you here
is that what I want to show you here is
that it's super easy to kick off I mean
what I'm going to show is that I'm not
actually going to write that much code
because that's that's kind of easy with
with this toolkit our music and also we
need special mapping because we need
access to every floor so I'm gonna get
the special mapping I'm gonna zoom out
zoom in in a second to see for you to
see what kind of components I imported
oops see camera input manager cursor and
special mappings and so those are the
components I'm using and the naming is
pretty straightforward
I guess so okay I think I could run it
but I could run it well the usual
question is is there an emulator do I
have to have halt as to run the
application to test it etc so the answer
is you do not have to have Collins I
mean obviously you have to have it to do
to like properly test it but for the
like a regular walk through and just
creating some some common staff or just
proof concepts you basically don't need
the device and there are two ways you
can test it one is you can run it in the
in the unity editor directly which is
possible from sometime and also you can
build it build it and after the build
this visual studio solution would be
created and then you can run it in their
whole lens emulator emulator in Visual
Studio but this is some I would say it's
not this emulator is quite heavy so I
don't have enough patience for example
to use
so I do recommend using it in in unity
because we don't have to switch between
environments and just do builds in here
and there and because that just breaks
the workflow so what I want to show you
is how to how to run it in inside unity
but the disadvantage is that when we are
in unity we do not have the model of our
room for example right and we would like
to test spatial mapping so how to get
the room into unity so we can do it in
two ways
first would be we can fire up for once
this emulator I mentioned and we can get
a room model from there because
Microsoft delivers like five models
there are some rooms which are basically
the 3d models like regular models those
are those are basically meshes which
just looks like room and we can get some
from there or we can fire up our Hall
answers which I will show you how to do
it and just scan some room and just grab
that model export that model and just
import it in unity I do have one model
prepared so I will just import it as an
asset this is a regular 3d object
nothing nothing fancy but it it allows
us to simulate that room inside unity
and what I have to do is basically
select the special mapping component and
just drag that room into the special
corpse into the special place in there
which is room model and this is this
works only for unity editor it won't go
anywhere to our whole ins app that would
be deployed in on Holland so it's only
for testing purposes here and basically
we can hit run and bam we have a room
model and we can using WASD and a mouse
we can basically walk inside the table I
don't know if you well you can see it's
a room right somehow
right it looks like it wrong it looks
and we can see the cursor that the blue
thing that hits the whoops where is it
the hits the the surface and it and it's
it sticks to that so that we can we have
this this point where we are where our
gaze is pointing we have at this point
visualized so this is very helpful so
yeah that's that's this so we have some
simple drag-and-drop
I created an AB where I have cursor I
can I have a gaze and I have special
mapping right so next step next steps
are something set up for for oops for
unity environments so that we get we get
to we mean to set up unity environments
of that it knows that we are doing whole
lands development so if we are having
mixed reality toolkit it's as simple as
it's as simple as just getting here
mixed charity toolkit option and the
mini and just apply an extra tea project
settings so basically we need to tell
tell the environment that we are
targeting Windows Universal uwp
application we are enabling enabling
mixed reality direct3d and dotnet bark
and scripting barking because we are
going to write scripts in c-sharp
okay hopefully it worked and also as we
are using special mapping and we will be
deploying that to hallways we need to
ask the device for the user for the I
mean the device for the permission that
we are going to we are going to use
spatial data so that's the same as
permission in mobile labs we need to
mean to ask for that so that's that's
here with spatial perception capability
setting and then we're done with the
setup so right now we need that forklift
and I have this asset prepared so I'm
going to import it yep so what we would
like to do is to be able to look at the
floor and air tapping and then the
forklift should just appear on that
floor at that point right let's just
check if if that if that 4/5 actually
works and renders there you go here is
our forklift and we can just right
whoops and here's our forklift like
hanging on the ceiling and that's
because I put it in the zero zero zero
coordinate and zero zero zero coordinate
it's where our head is when the
application run right so that's why it's
it's hanging from the ceiling but we
want to delete this and we will write
some global script which means it would
be it won't be it won't be attached to
any we any particular object in our
scene so we just need to create some
some empty object because we can't have
scripts in unity which are just
somewhere we have to attach them to some
object but in this case we need some
some object to just hold the script not
not a physical object that will be
rendered so I'm just creating some empty
which I'm just gonna name program and as
you can see I've added this and to add a
scrip to that we are going to inspector
in just whoops get out of here and I'm
just clicking add component and here I'm
just I'm gonna type program and that's
gonna be my script there you go and
double click on this there you go and
this what pseudo will pop out and here
where here is where we i will be writing
my script and they will say that i will
basically write some something like five
hundred seven lines okay let me zoom in
a bit you can see it okay so this is
that this is how scripts look like in
unity they all inherit from mobile
behavior and they have several entry
points that you can program in those
those methods and some of them are i
mean two of them the most important ones
are start and updating and there are
some more of them like enable awake
fixed update it's late update but i'm
not going to go here into detail because
that's not that necessary right now but
start method is being run when the
application starts will start method it
start method is being invoked on every
object that is on the scene at the
moment of application start then update
method is something that is being run
like 60 times per second at the same
time I mean as often as the the framer
framerate is so this is where all the
animations are happening right if you
are rotating something like continuously
it it will be happening in object method
but what we want to do is we want to
handle here
the air tab and check if we are looking
at the floor right if we are looking at
the wall we don't want to forgive to
appear there we all wanna we want it to
appear on the floor so I want to
implement air tapping so I'm just gonna
I'm gonna implement I input click
handler interface that comes from the
the speaks charity toolkit a simple
staff gonna implement that and I have
only one method to implement here which
is own input clicked and that's where
all the magic will happen that's it
and to handle that because by default by
default the gesture would be handled on
the object that we are looking at some
this is this will be the focused object
but here we're not going to look at
anything particular well we're going to
look at the mesh the special mass but
we're not going to attach any script to
that much because it's not the way how
we do it but we wanna gonna do we want
to handle that gesture kind of globally
so that if we do not have any object
focused just fall back to this script
and handle it up here so for that to
happen
I just need to push fall back in put
handler which would be this scraped game
object is a reference to this object and
and all the air tabs that that are going
to be full backed will go to this
handler so if we will be looking at
anything like our floor and do a gesture
active gesture it will fall back to this
method and invoke on input click method
so on input click we should have we
should get the point where we're looking
at right because we want some details at
that point focus details and we're gonna
use focus man focus manager which is
which is from mixed variety toolkit and
this is something funny I mean I come
from I used to do a lot of like
enterprise stuff with c-sharp etc so
you haven't seen a lot of single towns
or public fields and all the staff and
all of the sudden see sharp scripts in
unity you see all of them I mean you see
lots of single towns and you use like
public fields and it's all normal so
yeah I got too used to that
so we're gonna get focused details here
based on our gaze you see I write
instant cyber because those are all
single thoughts that's crazy yeah so
what are we get here focus details will
be those will be details about this
point that I'm looking at so I'm gonna
get normal vector there I'm gonna get
coordinate of this point and some other
things but what I'm mostly interested in
here right now is if it's a floor so I'm
gonna do just simple calculation here
basically I'm gonna check the normal of
this of this point and how much it
diverse from like app vector and if it's
gonna be like some you know some I will
set some threshold if it's gonna be like
15 degrees or less then I will assume
it's a floor I mean as simple as that
float angle from app yeah you're in
unity of doing a lot of vectors things
that's kind of something you have to get
used to it and well I'm just I'm just
invoking the method that will get to
vectors and will calculate the angle
between vectors so just gonna do this
vector free up means the vector are
simple Z and I'm gonna get the normal
the normal of the point I'm looking at
so that I had this normal vector from
there and when I calculate it and this
angle from app will be less than 15
degrees then I am assuming it's a floor
right as simple as that in this case and
well I didn't mention that earlier but
there is in mix 32 ki there is a
component called special understand
and this component delivers us some cool
features about on processing special
masses so that we can detect we can
really detect I mean that there are
really components for like detecting
floors ceilings and places like empty
places on walls where we can play stuff
so it's highly highly recommend using
that but for like demo purposes I just
use this just to show how to write
simple simple script so if we know we
are looking at the floor we need that
that the forklift model
so obviously I'm gonna do a public field
here for lifts prefab and I'm gonna
prefab is something that that sits in my
asset fall assets folder and this is the
object it's not in the same but I'm
gonna I'm able to instantiate it right
and put it in my scene so I'm gonna do
it right now so I'm gonna instantiate
instantiate my prefab as simple as that
right and then I'm gonna place it this
is my instance of my object and just
gonna change the transform transform of
it its position to my focus details
point so basically I'm getting the point
where I'm looking at like the
coordinates at that point it's setting
that forklift
transform right the position as simple
as that and after that I would like to
disable that that ugly mash I have I'm
having everywhere on my special data so
that it will so that I won't have that
white mesh so I will think that this
forklift actually sits on there like
real floor right so special mapping
manager of course instance draw visual
meshes false
and well I still need that proof up here
right I have to do this dependency
injection thing so in unity
dependency injection looks like this
wait a sec for it to refresh and I know
if you if you can see that but here the
new field appeared on my script those
are public fields I have on my script
and this is the 40 proof up here so I'm
doing my dependency injection look like
this I'm checking the that's script and
just just doing this and that's it I
just passed the reference this is the
you know the possess the drug dependency
injection stuff like that I don't know
but well we can run it and cross your
fingers gonna click here and bam we have
that fork right at this point and our
mash disappeared right so that when we
will run this on Hollins everything
that's black will be transparent and we
will have their forklift sitting on our
floor so let's just add one more thing
here basically we would like to grab
that forklift and just rotate it as
simple as that so that's gonna we're
gonna select our prefab here and and I
will attach a script to that and just
implement rotation on Dragon and that's
it so I'm clicking add component as I
did before but this time on this this
object and fork lift
now now you're familiar with this but
right now I don't want to the air tap on
this model I need to do manipulation on
it so I'm gonna implement I money
collation handler interface and it has
four methods to implement one is own
manipulation cancelled completed started
I don't want to implement any of those
because I don't care now about this what
I need is manipulation updated so so
that I need the point where I can get
the Delta of my hand movement and then
use it to rotate the object as simple as
that so for this to happen I need to
implement all manipulation updated
method and get the Delta somehow and
it's it's an even data that is being
passed here and cumulated the Delta so I
need to rotate that object this script
is attached to that job to that object
so in transform transform field I have
the transform the position of this
object so I need to invoke a method
which is called rotate with a surprise
and I need to pass some values here so
basically what I what I have to pass
here is the vector like XYZ and and
depending on which value is where it
will rotate on particular axis so first
value is we wanna I rotate it like this
like along the y axis and we need to use
this gesture so this is the x value of
delta right so we're gonna pass 0 for
like exhortation and pass here Delta X
right X value of of my manipulation and
for Z that we're going to pass 0 ok so
two lines of code
let me place it come on oops
there we go okay I'm moving okay I
probably missing the - there but you see
it's rotating and well as you've seen I
haven't written like lots of code it's
really simple and this is like the most
common scenario for me I mean the most
of the time and just do these kind of
things I mean I spend most of the time
converting objects from SolidWorks to
two meshes than the actual program it
but but yeah as simple as that sometimes
I do something more like I program
animations in unity so that you can
click through the object and it will
just just just turn into pieces and
stuff
and so that it gives it a little bit
more insight so I still have some few
minutes so I'm not going to deploy it
right now because that would take too
much time but basically the workflow
looks like this we have to build it here
in unity we're adding the Machine the
open scene that we want to export we
have everything set up because I've
clicked through the menus before and
basically built here and what happens
after we build here is the visual studio
solution is being created and then we go
to visual studio open that and then we
can deploy it to hallways and we can
deploy it in several ways we can do it
the USB cable we can do it we multiply
Wi-Fi or we can build a package and just
just just load it through them through
the device portal which I will show you
in a second so I'm not going to do this
I'm just going to try to show you
okay
okay so it should work wait a sec
okay so first let me let me show you
this device portal so basically when you
connect to your Hollens by entering your
Holiness's IP address to the browser you
will get the device port or of the
device and here you have some pretty
cool options and I will show you some
few of them I mean the rest is like
Diagnostics or uploading apps or
deleting apps or some some kind of
maintenance stuff but I'm gonna show you
this for example so that you can see
that it tracks my movement right and if
I would click update here it will show
what it already knows about this this
room right that's the model it's
building inside right you see so this is
what what holland's knows and from this
place we can we can export that model
actually and import that in unity as
I've shown you right so for example I
was because I work remotely
I traveled to the office like once a
month and I wanted the model of the
office which is in Krakow but I wanted
to have it at home so I basically was
going through the office with hall
assistant and exported model that's it I
had the model so so yeah that's that's
very helpful thing and the next thing is
this mixed reality capture think I
already mentioned at the beginning
that's where we can do the streaming the
thing is that we have a big lag here you
see and this is I don't know why I mean
it's how it works for everyone so but it
still we are able to show someone how
things work so let me fire the
application I deployed it earlier so
that we won't take some too much time
okay I already see that I need to look
around to grab some more data because I
mean I'm constantly I mean the Holland's
cost of these scans the environments so
it updates the mash all the time and if
I'm despite of this mash it changes as
as quickly as the whole colons have more
information about the space so when I
already have my floor I can just hurt up
come on right up there
and just look at my beautiful forklift
yep
well that's pretty much it as simple as
that any questions that's all I wanted
to show you I'm happy to answer any
questions or give you the the coal-hole
ones to play with of course a caring
yeah it's very similar yeah I mean it's
it's more advanced but yeah and you can
buy those yeah of course I mean you can
just go to Microsoft calm and just just
just just buy them and there are only
free thousand euros so for a deaf for a
deaf kid which has no warranty so the
the Enterprise version like that the
real one the consumer one is five
thousand euros it's well but it's
targeted to the businesses it's not the
consumer market no no no no no I mean
it's it's not target it's too expensive
you have some games there but those game
are basically created by developers who
just want to show up and that's it you
can't you can't expect basically to earn
money from the store for all as no you
can't can't expect that there are more
than 300 apps but as I said it's managed
just so show off by developers just to
see something's going on but no
I mean there are several devices out
there on the market actually I mean
they're Google glass still lives its
life
there's absinthe there is a meta - etc
etc and all of them all of them right
now are targeted to businesses I mean no
one is doing consumer consumer targeted
glasses and that's what Google did in
2013 with Google glasses very first
version and it quickly occurred that the
market isn't ready for that so well
magiclip is well it's still kind of a
fairy tale
or something like that I mean they're
doing it since I don't know I don't
remember I mean eight years or something
like that
and there are just getting money and the
recently released something but it's
still like renders so I don't know I
mean I'm keeping my fingers crossed for
them because it looks really cool but
until we get something and get the
community around it and all that stuff I
mean Hollis and Microsoft did the great
job because it's on the market it's
expensive sure but the community's out
there the support is out there and you
can actually sell this stuff and that's
it I mean yeah I mean yeah you can see
them in many places yeah not only
factories well it scans up to free
meters from you so but if you want to
scan something out there you have to go
there that's the question yeah it's like
three meters and it's all depends on the
owner on all the lightning as well if
it's darker it wouldn't scan that well
yeah
well I expect it will have just nothing
there there wouldn't be match they're
outside yeah I think you could precise
post precise this match and just do some
stuff afterwards but yeah my default it
will just not detect it
I mean the same thing is for example
with mirrors if you look at the mirror
with this it would just think there's
another room there
it's tricked by the mirror I mean as
simple as that right I don't know I mean
it's something like that I mean I mean
you can't rig this device with a move
there with the mirror yeah yeah it
builds it beans old model yeah yeah oh
well yeah I mean I mean moving parts is
not the best thing here I mean it
constantly scans so and if for example I
would fire it up here and you would
start living it would just constantly
update what it says so sometimes I will
have your silhouette here and then after
a few seconds I will have you there
etcetera so it just scans and it's up to
you if you would like to freeze your
current state and then post-process it
or not so it depends on the scenario
actually no it's no it's not not not
that I mean it probably could apply some
algorithms based on what camera sees but
it's not like it's built building there
are things that you can apply some image
processing of course and detect things
but it's like using some other stuff
like machine learning or whatever yeah
okay how does it cope with can you can
you sort of tell it in advance you're in
this room here is the model does it
tally that up with what it sees I mean
can you
for example if you're in a factory where
you couldn't scan the whole building can
you give it the model of the building
and say work out where I am in this
space
I am definitely in this space does that
well no you have to have you have to at
least visit once no no but I mean if you
know what it if you know the space is is
there any idea of saying you're going to
be operating in this space here as a
model tally didn't tell me what you're
seeing up with the model you know so you
kind of you can then could you then know
that there is a wall 20 meters away for
example if you have a model then yeah
and you can tally that up with what
you're seeing
yeah I mean you can you can put
something which is called special
anchors here in there and those are
those special points that can be
remembered and you can do calculations
based on that so we can you can do
something like that yeah yeah no I don't
know I have no idea this is black magic
for me I have no idea how it how the
text that it is in this room I mean this
is this is black magic I don't know
because we fire it up and it it has like
for example this device probably has
like kinds of spaces remember they're
like separate spaces and if I were it's
turned off if I would turn it on it will
some how much it will look at this space
and we'll probably with like 99% of
sureness it will match this room it will
detect that it is in this room but how
it does it I have no idea it's probably
combination of several things I have no
idea how now sometimes it is sometimes
it's confused and then it will ask you
it will pop up the window and we'll say
I've no idea where you just help me
select the space that arguing or let me
create like a new one but that's usually
depending on that usually happens when I
have bad lightning that's its most usual
case
cool I don't think I understood the
question oh yeah I mean well it depends
I mean I'm talking to many executive
guys and it's it depends it depends on
the mindset usually because you have to
kind of see the potential and things and
sometimes I'm talking with guys that you
know they are doing like they're
migrating from for example access to
mobile applications inside their
companies and then I'm telling them
about using a RS of programs like not
now but if if you're talking to someone
who's like who guys like a mind open and
and feels it then you can you can talk
about the future and how it can be
employed etc so for example I already
delivered like several proof of concepts
and well one company wants to talk more
about this and how we can build up on it
and some of them are like alright we're
done etc so it really really depends but
there is an interest that's that's for
sure well you have to sell it sam'l it's
new to everyone it's new to developers
so think about people like you know
executives or managers etc who are not
that close to the technology so you have
to actually build awareness what's
possible for them like for the first
side it's like a nice toy I would buy it
for my kids right but then we have to
kind of you know explain them that it's
not for kids that's that's that's a real
thing it's not the future it's like it's
right now
yeah what is happening yeah future is
now any more questions thank you then</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>