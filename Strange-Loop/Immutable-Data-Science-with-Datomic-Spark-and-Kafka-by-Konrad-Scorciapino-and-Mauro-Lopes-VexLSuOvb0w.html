<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Immutable Data Science with Datomic, Spark and Kafka&quot; by Konrad Scorciapino and Mauro Lopes | Coder Coacher - Coaching Coders</title><meta content="&quot;Immutable Data Science with Datomic, Spark and Kafka&quot; by Konrad Scorciapino and Mauro Lopes - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Immutable Data Science with Datomic, Spark and Kafka&quot; by Konrad Scorciapino and Mauro Lopes</b></h2><h5 class="post__date">2015-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VexLSuOvb0w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks so much for joining our talk
there are many great talks to 10:00
today so we feel honored I have you guys
here all right
the problem with Wi-Fi for starting a
bit late I'm Conrad
I'm Maru we're software engineers from a
company called new bank we were a
technological company providing
financial services we got works of with
all kinds of modern technologies we use
functional programming machine learning
logic programming etc and our talk today
is immutable data architecture with the
atomic spark and Kafka so the goal is to
share our experience working as
engineers in the data science team
building data architecture mostly for
two purposes to help support the machine
learning happening so modelers putting
models in production and using them and
also creating models and also their
access so people from different parts of
the company using that and we have
organized it in four parts background in
which you give an overview of our
architecture we show the the a bit about
the technologies that you need to
understand for the rest of the talk then
we talk about scoring which is basically
you have a model and how do you use that
to make a prediction then training how
do you build this model in the first
place and finally analyzing how do you
get data in the company how to use it
how to share it so let's start with
background we use a micro
service-oriented architecture so we have
many micro services and we are finance a
finance company so you'd expect that our
micro services are like customers
accounts transactions geolocation
purchases etc and each micro service
owns its own database each micro
circuits on the owns its own data the
the logic to modify the data that it
owns and they communicate with each
other using HTTP but also synchronously
using cue song Kafka
we have lots of cues for everything they
they have usually an event semantics
which will shown so here is an example
of interaction of three sources in the
bottom in the orange service you can
imagine it like as a low-level financial
processor service which can publish a
message to you saying okay this this guy
acts has a new entity why
and then another service reads that and
say okay it's a it's a transaction I'm
going to store it and I'm going to
notify that there is a new transaction
around or whoever is interested and then
finally there is a notification service
that says okay there's a new transaction
and the guy wants to be communicated of
that so let's send him push notification
and you can see that I've dotted the
arrows because they are not direct
connections each service here is just
sending a message to to cue just adding
an item in the queue and that cue gets
read by the other by the other service
and another important distinction for
our architecture is that we is our
database you're not using regular
database regular sequel relational
database not also standard no sequel
database schema is reusing the atomic
which is a different thing we are a bit
constrained in time here but we'll give
a small review that will allow us to
understand our solution so I usually
like to think of regular relational
databases as boards so here we have a
very simple example we gotta get not a
guy main Quercus job philosopher city
somehow so we've got all this
information about this guy and okay it's
in a particular state well but what
happens when the guy is that guy moves
around for example he moves around to
Richmond you have to raise the old value
to make room for the new one
I think general working databases like
visi you get the feeling that's an
object you have to deal with the data is
is far away from you by contrast in the
atomic the the database can be seen as
just piles of facts so here for the same
example we have a one pile with named
Quercus when by with job philosopher and
run with the city some phone and the
atomic also adds a new dimension the T
equals one in the bottom left is the the
transaction in which these facts were
inserted it can also be seen as the
logical time so it gives us ordering and
what happens when when the Quercus moves
to another CD so a new fact is inserted
with the new city
it happens in France in transaction ID
number two so it's another instant and
as you can see we have the the new value
and we still have the old value so how
can you tell what is current into this
database just look from above so the the
current name is Quercus the current job
is philosopher and the current city is
richmond and how is that stored in the
database
it's stored as a sequence of facts so
here we have all those four facts we had
in the previous screen they and each
fact isn't a V T purple so it's a net ID
an attribute a value and transaction ID
they all have in this example they all
have 1000 as the entity ID because they
all refer to the same person the
attribute all attributes start with a
person / this is just a convention to
show that they are all referred to a
person so and the third column will have
the values and in the fourth column we
have one for the first three facts
because they were inserted as at the
same time and two for the other effect
and how is it queried
the query is just a structure with three
clauses fine in and we're in find you
have the output what do you want to get
back from the query in in you have the
input and the where clause you have a
series of of constraints to link from
the input to the output so in this case
we have as input the name you want to
see the job of that person and B in both
clauses both constraints means the same
it's the same person since it's a logic
query language we can keep the same
constraints and change the input to the
output and it's to a valid query so if
we had the Java but it didn't know all
persons with that job we can just
exchange and with find we could even and
you could even not have the input and
the database would return all pairs with
name and job for all persons in the
Torah start there now let's talk about
scoring so we have lots of data in the
company and we want to make predictions
about future data the artifacts that
programmatically do this in motion
machine learning are called models we
deal with models in two stages training
and scoring training consists of getting
consistent getting a series of of data
points X 0 X X 1 and to exam and then
I'm a muddler and data scientist creates
a black box using machine learning
algorithms during the train phase then
when the model is in production we use
the black black box with a new data
point IX to give a score a pretty
you notice that x0 x1 and the accent
back the black box must be in the same
format the same attribute so for now
let's talk about specifically scoring so
how we get X how do we get the data that
we want to score remember that it must
be at the right time and must be must
have the right format so one thing we
can look at is our production
environment can't we use it somehow oh
we have there's lots of micro-services
and we have this queues and the queues
often have even semantics and they
contain information about the entity
that's carried around for example we
have have a topic called new purchase
that contains that is triggered when
there is a new purchase happening so you
want to if you want to do a later
analysis on that purchase to see if that
could be parts in fraudulent analysis
you could use that as a trigger to start
the processing and you can also use the
information contained in the topic in
the new purchase will likely contain
information about the purchase so we
created a first approach based on that
so we have created a new service there
using a cue that it's already worked in
already works in production and this new
service contains the models that we want
to run so it consumes from the queues
and reads its content however this
information is often not enough for
example friend to analyze and the
example we gave when than analyzing if
the the purchase is fraudulent you
wouldn't want only the information about
the purchase but about the merchant
about the the best of the the customer
about the location where the purchase
was made so you would need to query
api's from other services to enrich your
data and then pass it through the black
box
and this has several downsides for
example it effects production so you're
essentially querying for using features
that are used in production for this
other thing that you're doing this
analysis that showed me you are adding
complexity on the service so we have to
maintain the the endpoints to do this
queries or create generic libraries to
have generic endpoints and also if one
service goes down it affects the other
and also traceability it's it's hard to
to figure out why a service model give
an output that you find suspicious if
you don't have the input for example if
you don't have a version of the model
that was run and often the output is
something simple like true or false and
the input is a lot of that for the
example we gave the the purchase the
merchant information may be a list of
purchases so we'd have to store all that
so we tried a second approach which is
squaring data directly from the database
so to illustrate it let's go back to our
example we have the the guy here quercus
philosopher richmond and let's add
another factor to make it more fun let's
decide that at t3 he has a life crisis
and decides to become an astronaut
so what happened here we just added a
new fact there and how can we describe
what's happening here we can see that
right now at t3 Quercus is an astronaut
that we've lives in Richmond but can you
get that definition better we can just
keep that right skip that right now and
say that say that Act III Quercus is an
astronaut that lives in Richmond and the
same thing applies for T equals true for
anything you can just grab a bit put the
cursor in a certain transaction ID and
you can you can get a snapshot of that
the F that database at that time it's a
valid database you can query the
database you can use it
any other database and what and what is
interesting is that if you have the idea
if you have if you know that you're
creating only information about queries
and you and if you have the this cursor
you don't you don't need really to to
use the information that comes from the
queue to to process anything you cannot
can use only the cursor and the ID and
get all the information that you need
but it's not that simple
because there are multiple degrees so
you cannot just use a transaction ID as
the current point in time to get the the
value of the database so for example in
in a similar time frame we have three
transactions in the left database and
for transactions in the right database
so how can we do that can we cannot use
the same transaction ID so we would need
one transaction ID for each database but
there's there's one simplification you
can make so instead of using the
transaction ID we use the timestamp this
might be problematic for some cases but
in our domain it it seems to it's it's
fine so we use a temp step to get the as
of value of across all the B's so we we
unify them all now let's talk about
scoring so we've built a service first
Corinne which suits our approach so we
need a service where all the data is
available or all services become one our
other kales intertwines it's a dark land
that's what we called it mortar
it's a really only current service it
does not interfere with broad so it
turned out not being so complicated as
we first thought
the name is more of a joke now and what
do we mean by no interference instead of
calling a lady related HTTP endpoint in
each service it simply simply consumes a
cue so the the red arrows are the
endpoint calls so now we have this so
the the black and the yellow circle is
the the current the the model service
and the red circle is the query service
so the model service consumes from the
same cue that the service is consumed
and uses the query service so we don't
have a central core courage server that
would be a bottleneck
you simply have piles of data stay
stored in a storage device so as long as
that storage is scaleable which is the
case of dynamodb that our solution is
also scalable so the current services do
not compete with prod for resources we
can scale them horizontally now let's
talk about using this architecture so we
have three parts you have the queues we
have the black ball with the yellow dots
which is the model service and you have
the red ball with the volcanoes which is
the crane service and how can we use
this tree first let's start with the
queues so on a simple message comes for
example new new purchase it comes it has
an ID it has a merchant it has a
purchase date has lots of halves lots of
things and it also has a matter
information most importantly time stamp
which means what what time can we be
sure that this information will be there
in the data
and what we do is show that all that
information away except for the ID and
the timestamp which are the things that
we need to gather data from the database
so we use this information to do to
query our pairing service so this is
another example of a query and the first
two lines you can see that there is an S
off which is used to which you use to
filter all facts that come after it and
essentially freeze the database we used
there's four eight which was the ID in
the previous light it's the ID that will
we also use and then there's the query I
want to go really deep in the query it's
simple it's a simple query but you can
notice that there in the where clause it
starts with a dollar sign in all the
first tokens / / / / so this what are
these these are just database so you see
that in the same query we are just
accessing transparently different
databases different data sources and now
let's talk how that all that combines so
this is the model servicing the left you
can see Kim's in the middle you can see
the service and in the right you can see
the query service so in the left their
their main queues purchases push token
notification payments etc up to model
output of which we'll talk later but our
our model is interested only in
purchases so it's it subscribes to this
purchases topic and then says here's how
you're going to get an ID this is a
cursory together and then when I miss
new message comes it gets it that ID and
also a timestamp and with the tag ID it
makes a query like the one we done
before and gets an output that output
with some processing key which can
happen there is the X that we were
looking for in the in the first place
gather X who pass through the back box
we get an output and then we combine the
output the input and math information
publish this into another queue which is
the model output queue so what does this
model output queue contain it contains
these three parts input output and mara
input is just the idea we got from the
queue timestamp is just the timestamp we
got also from there but means that it's
available in the database and the
trigger which is the for for the party
queues it's the queue name we've got the
output that the model gave for example
true or false and you have met
information for example the model which
model produced this the score which
version was it running this this version
is a get hash and also the response time
of the response so why is this useful
this is very for traceability because
like I said things go wrong and you want
to figure out why your model outputted
some early response so why do what you
could do is take a look at grab grab
that that this information of the model
run that fits in a database that we
store back in the database require for
that we get e ID we get the timestamp
try to reproduce the situation using the
same queries or just a small operation
of the queries used to score them and
understand what's this this information
about and if that is even that doesn't
help you can simply revert to up to the
commit that that was used to produce
that output and then pass the same input
it will access the database which the
doesn't change because it has a cursor
and it should produce the same output
now let's talk about training so going
back to the other slide we had we showed
that the model has a training phase and
the scoring phase we just talked about
the scoring phase now let's talk about
training so at scoring time the modern
service hazard an ID a time stamp and a
query and gives all of those two the
current service and it gets a piece of
data in a certain format in training
time the same thing happens but instead
of the modern service in production
I'm a muddler data scientist uses the
same query service so we use the exact
same query that we would use in prod and
it's so the same interaction the same
code path which avoids a lots of of
problems so in other architectures
sometimes you cannot do this you'll have
two different code paths or you use two
different data sources for for training
and scoring so this is an advantage and
for training specifically we query a lot
of data at a time so we can use a bunch
of of query services in parallel this
solves the problem of getting all the
data but it doesn't solve the problem of
fitting out that meta data in the memory
of one single machine and and we also
want to fit all derived data in one
single machine because the model will
get the raw data from the database and
will produce calculate more more data
and the solution we choose for dealing
with data is using apache spark patches
park is a large-scale data processing
framework it's high performance and it's
easy to use and the point is that you
have a spark cluster and you can use it
for your processing and for holding data
so here's the the base the base object
which you talk about is the ER D D and
it's resilient distributed data set so
it's it's like a big data collection the
descripted among several services so we
can fit data that doesn't the pen that
doesn't fit in your memory for example
it's also immutable so you can change it
when you try any change or change
through Map Reduce these operations that
actually produce another RDD so here is
the next an example of this happening we
started an RDD of objects with
attributes x and y and then we map using
a function that multiplies X by Y and
then what you have to get another RDD
and then we just do a reduce and get
finally a value and what's what's our
use case we have we have all this this
datums that we need to you need for
training and they have their own
properties idea mount it said that
depends obviously on the model and we
want to get all that into an RDD and
then derive other IDs that are more
convenient to apply machine learning
models and then just produce that black
box after using the models and how do we
do that just basically by slicing our
our whole data into n slices we estimate
the size of the database we're trying to
get and divided by about the how much
how how large are the the exact tree
that we have in spark which are like the
small machines you have available and
you take you and then you you end up
with a number and you just say ok you
create an RDD with
I have I want to end Lices and this is
lies one I have ends lies as this is
lice two and so on and then you map that
RDD into an RDD containing the data
itself of those slices but how is it
possible to do this using our
architecture let's take a closer look at
how to chart queries so our strategy is
that we start with a regular query which
we choose a base entity and we filter
the rest of the query by using the
modular operation so if you have a
simple query here that returns all pairs
of purchase amounts and the bill amount
of each purchase so this is the starting
query how do we start it we simply
insert those three lines with the arrows
so insert the arguments that the
parameters in in and the mode operation
so now we have a shard Neelix and a
short count in the query the shard count
will be the same for all machines and
the shard index will change from one
machine to the other so in the so
suppose we have that example 35 machines
running this query so we use 0 35 for
the machine number 0 135 for machine
number 1 and so on and until 34 35 and
the mod modulo operation acts on the
purchase entity so we get the entity ID
we calculate model 35 if it's 0 we
proceed with the rest of the query which
could be arbitrarily complex with
function calls and so on
and if it's if it's not 0 with if it's 0
we proceed if it's not 0 we discard the
rest of the query so effectively each
machine processes only 1 over 35 of the
total data set
now let's talk about something else
that's something that's unrelated to
modeling but also is about data which is
their access so in any modern company
people use data people need data to work
and that pretty much doesn't depend on
what's your role what's your job
engineers needed analysts needed
modelers of course and in our company
we're divided in small teams and there
is there's a tendency in for for
everybody to create their own their own
ways their own data how their own dumps
their own functions to deal with this
data at their own code paths and
something we can do to avoid this is
making them all user our current service
so on an approach we we had is to to try
to make everybody learn the our current
language user crying service they would
get fresh up-to-date data all the time
they wouldn't get dumped that need to be
really refreshed and so on and they even
would could use even up to date the
functions they they sometimes have to
instead of having to rewrite something
that's in our code base maybe they can
even use that so the best of all in a
way that would allow them to not even
write if they don't know how to write
because the problem is that our foreign
language is very powerful it's not
impossible to to learn people from all
backgrounds have learned people
designers people working in finance etc
they have all learned but there's a
higher barrier of entry than you would
expect so our approach is was to improve
that high barrier of learn
of entry and what it did was improving
the experience of doing queries through
saved queries so I said we have lots of
themes but they are also
cross-functional so there is usually one
person who's an engineer is the data
scientist was somehow more data more
tech savvy and who knows how to use that
law how to make queries because we use
this in our daily jobs this the squaring
mechanism and so what we do is ask them
to create and save the square so other
people in their teams can use it and so
other people initially just click to run
those queries and then they start to
modify modify the parameters and and
eventually a little by little learning
data log until someday they really want
to get something done and do a big leap
and learn it and it's also interesting
that this approach allows them to to
save not only what I'm calling here
procedures which are like imagine a
credit gets all people who are late if
they run it now and run it later to give
different results but what if they set a
cursor like we've done with our analysis
they effectively effectively sharing
data so instead of sharing lists of
ideas of people in certain condition
they people can just share the square is
the squares you'll never change and
there's an UI for that it's it's more
beautiful this just my artistic rim
oranda me basically we have this all the
squares the person can click can see can
run can change the parameters and in
this way interact with the with our data
without having necessarily to program or
even no queries so deeply and how are we
how are you going with this approach so
it turns out everybody is using our
service inside the company so all teams
used it we have more than 1 million
executions hundreds almost all teams
have thousands of
runs and we have also hundreds of saved
queries that the team's considered
useful for for later usage one also
another nice thing to have is start
procedures so we have three kinds of
start procedures and we have one example
here for each one so we have for example
generic generic function like random
number generator so if we want to do
some some tests some in market tests
with women with some customers we want
to use the random number of function to
to filter them and to classify who get
through this part of the test or not we
also have an interest function chocolate
interest it could be you know in the
center of a common library that all
services use so the current service can
also take advantage of that without
having to implement and you have
acquired to see if there's if the
forgiving person is late with a payment
or not and it's stored in the database
itself so there are three ways of doing
it and we also got some testimonials
from users in our company one one
business analyst says she likes it
because it helps her take snapshots of
the data and see how it evolves over
time all engineers like because they can
they can see how how the database ended
up in a certain way they can debug
corner cases it's really useful and that
analysts like it like before like the
service because they can see that they
can do their analysis get to some
results and just a few minutes before a
meeting they can rerun the query and
analysis and get up to date results
because because we don't use the attempt
of the data for some final remarks what
we have done
cretin solution for scoring composed of
breathing kills and from production
using a model service that queries to
query uses a query service to gather
data that does interfere with production
also for training or what you have is
using the same code path that's used in
in training to make sure you're getting
the same data make sure you're using the
same queries and also being able to
scale that by using Apache Apache spark
and they also provide the solution for
data access which is basically providing
our current service for everybody and
making sure they can learn it and just
as important as important as what we
have done is what we did not have to do
by using this approach we do not have to
copy data around because the that is
always up to date
everybody always use the the Center
database that services use we did not
have to duplicate functions and logic
functions in all services can be used
inside the queries we did not have to
create different views for different
teams so there is one central central
source of truth and that's what we want
to show you thank you if you have got
any questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>