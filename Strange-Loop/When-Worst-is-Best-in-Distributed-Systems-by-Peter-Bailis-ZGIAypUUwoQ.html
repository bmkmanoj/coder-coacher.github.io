<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;When &quot;Worst&quot; is Best (in Distributed Systems)&quot; by Peter Bailis | Coder Coacher - Coaching Coders</title><meta content="&quot;When &quot;Worst&quot; is Best (in Distributed Systems)&quot; by Peter Bailis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>&quot;When &quot;Worst&quot; is Best (in Distributed Systems)&quot; by Peter Bailis</b></h2><h5 class="post__date">2015-09-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZGIAypUUwoQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today I want to talk about a somewhat
crazy question when we're building
systems which is what would it mean if
we designed computer systems to handle
the absolute worst case scenarios so for
instance if you were provisioning a
cluster of distributed services let's
say you're producing hardware for your
startup very reasonably or needed reason
about questions like how bad is our load
going to be in the worst case scenario
and if we take this question to an
extreme like what is the absolute worst
case our our service might be up against
you know in the worst case of your if
you're if you're essentially building an
internet service you might have to
handle 7.3 billion simultaneous users
that is everyone on planet earth log in
to your site at once now you could
probably buy enough hardware to service
7.3 billion users simultaneously action
your service but in all likelihood in
fact for probably every application
today even Facebook right we're gonna
end up with a bunch of idle resources if
we in fact provision for this crazy
absolute load now similarly if we're
trying to build hardware update this
laptop here is actually built for you
know III put this laptop through a fair
bit of abuse in reality right if we want
to ask well you know how could he build
a piece of hardware that can withstand
sort of absolute worst-case
environmental conditions we might say
well what would it take to put this
laptop on something like the Mars rover
well in fact a bunch of people actually
do build chips and do build Hardware for
sort of interstellar space travel and in
the fact the matter is if you want to
build chips that survive in space it
ends up costing hundreds of thousands of
dollars in order to protect against
things like radiation hitting your chips
and bombarding them when you don't have
an ozone layer in front of them
so in fact we could build chips and in
fact worked in all of our laptops that
if we sent our laptops to Mars they
would work but for most of us but for
the average case sitting here in st.
Louis Missouri on earth we'd end up
paying a lot more for the packaging that
we don't end up using there's a final
example right we care a lot about
security right so we hear a lot of
security breaches hackers get into our
systems and so on and it's actually a
fairly important problem today but
especially we talk about security we can
get very paranoid right so instead of
just worried about people trying to get
into
systems we might worry what happens if
suddenly our developer team turns
against us how hard would it be to
protect against the possibility that all
of our developers are writing malicious
code and their binaries are activated
and they're leaking this information now
this is like a particularly crazy
question it turns out that actually some
really cool research on this topic but
not surprisingly actually handling this
worst case where we're totally paranoid
all my developers are against me ends up
really increasing the cost of both
deployment and development of the code
not surprising right the answering by
answering this question by designing our
computer systems for worst case
scenarios we find that frequently
designing for the absolute worst-case
ends up penalizing the average case we
end up making trade-offs that
essentially hamper our performance when
bad things don't actually occur so in
terms of this trade-off we might
visualize it as a graph if on the y-axis
we have our performance in the average
case when everything's going fine we
haven't hit sort of worst case scenarios
if we and more sort of corner cases that
we handle the more and more we start to
penalize ourselves and this actually
happens you know a lot of a lot of the
time for instance even when we're not
building software for instance if you're
going up for a run it's a beautiful day
in st. Louis today you're probably go
around go out in a short shorts and a
t-shirt okay and that's very reasonable
choice of clothing right you're able to
move quickly and you're gonna not get
too hot of course there is some
possibility that there's a freak
snowstorm that lasts for a year and
you're trapped outside in your short and
t-shirts or your shirt and shorts the
problem here is if you really wanted to
guard against that worst case scenario
you'd probably have to put on a winter
parka you might have bring food for the
year might have to bring a portable
camping stove very reasonably you'd end
up in a situation where you weren't
necessarily running outside but you said
to be sort of trudging with this sort of
doomsday set of set of sort of resources
clothing and so on so the question is if
this is sort of a dismal trade-off where
the more sorts of worst-case behaviors
in worst-case sort of environmental
conditions that we want to account for
the worse we perform sort of in the
average case when bad things don't
happen
in this talk I want to ask would it mean
to sort of break this curve to get the
best of both worlds where by designing
for the worst case we can actually
improve the average case as well this
doesn't always apply but in this talk I
want to highlight when it can apply and
what it means for the way in which we
build and think about software systems
so in this talk I'm gonna describe
basically three major points
I study distributed systems and so I'm
spend most the time talking about
distributed systems in the network
essentially by handling worst-case
Network behavior we can frequently
improve the average case as well now
this the talk title talks about
distributed systems it turns out that
over the course of this talk turned out
there's a couple other instances we
found where in fact this trade-off isn't
really a trade-off at all we're again
designing for the worst case actually
improves the average case and this has
some pretty interesting implications for
both sort of general optimization
problems and in fact sort of
human-computer interaction accessible
design and finally I'll leave you with a
few lessons that essentially describe
how you can use these type of techniques
this type of thing reasoning not just in
your distributed systems but also when
you're building software systems in
general so to kick things off let's talk
about distributed systems the network I
said in my research I study distributed
systems and this because I think
distributed systems really matter so
almost every application today that is
essentially non-trivial is or as either
becoming distributed so if you want to
build a rich application today there's a
good chance you have dependencies on
external data sources you have api's
that you end up requesting when you're
building a mashup you have a large-scale
service that's comprised of a large
number of micro services or you have a
service-oriented architecture where no
one component does all of the different
sort of plays all the different roles in
your application and very realistically
there's some point in your application
where you're opening a socket now the
core the core
you probably are opening a socket so
what happens when you open a socket
right what's that what's the defining
characteristic of distributed programs
it's that they operate over a network
right so in order to actually exchange
bits there's some communication medium
across which we're communicating that's
more than just say invoking a function
call and so the corollary here given
that almost every applications becoming
distributed and distribution happens
over a network is it almost every
non-trivial application today needs to
worry about what happens when we send
and when we think about and build
systems that operate over the network so
it turned out that networks as you if
you built distribute systems which I
imagine you have networks actually make
designing programs very hard so a lot of
things can go wrong
first off networks can delay messages or
packets so if I send a message to you it
might very well be the case that in a
sort of nicely behaving Network we have
a latency of a millisecond or half
millisecond but as soon as there is for
instance congestion in the network and
suddenly my top-of-rack switch starts to
buffer packets my latency become very
unpredictable can can spike to ten
milliseconds or hundreds of milliseconds
and effectively for modern production
systems we have to imagine the
possibility that packets may be delayed
arbitrarily long especially for
deploying on sort of modern cloud
computing infrastructure like Amazon now
if we dial up latency or these delays to
infinity we essentially have Network
failures so because our networks
communicate over physical media right
over either you know wires in our data
centers or over wireless signals as we
do with you know your Wi-Fi or your
phone we have to account for the
possibility that networks may be
unavailable that packets may be entirely
dropped on the floor
in fact in distributed systems we can
actually capture both of these behaviors
in which definitely was called an
asynchronous network model where I can't
tell if I'm missing a message whether
the message has been delayed or whether
or not it's been lost and this makes
things very challenging the way which
we'd like to handle this asynchronous
network behavior is very simple which is
let's not rely on the network at all so
if we provide what's often known as
availability
we can build systems where any replica
any copy of our processes can respond to
any requests we send out it so for
instance we have two copies of a
database server here the server on the
Left can respond to requests and so can
the server on the right in fact under
this definition of availability any
partitioning of the network any Network
behavior that might arise for instance
if these two servers can't communicate
we'll still be able to proceed just like
normal left and right continue to
service requests independent of one
another
similarly if our server on the right for
instance fails if server on the Left can
continue to serve requests it's
essentially as if nothing has happened
from the projector of all other requests
on the left-hand side of the partition
so this is a very powerful idea right if
we can achieve this availability we
don't we specially don't have to worry
about Network failures these delays and
drops become a non-issue in terms of our
systems design notably we've been
talking about this in terms of failures
right where the left server and the
right server can no longer communicate
however this note this notion of
availability applies also when these
servers can talk for instance if our
system is available even when the
network is fine we still don't have to
exchange messages in order to actually
process requests which is very powerful
in fact we say that there's no
coordination between our servers in our
system so availability addresses this
worst case scenario if we don't have to
talk if then the network can do whatever
it wants and we're fine however if we
can talk and we have this availability
property we still don't have to talk
right we don't have to make use of the
network and this leads to several
important benefits that correspond to
average case benefits specifically if we
don't have to talk we've built one of
these coordination free systems first of
all we can scale out our system
potentially indefinitely so if we have
two servers here that are that are
servicing their requests the left
servers and talk to the right server the
right server is not talking the left
server fairly straightforward if I throw
more resources of the problem let's say
I provision a hundred new easy-to
instances
of my process that run my process all of
these new instances can start taking
requests without communicating with the
previous with the previously running
servers this is a pretty huge benefit
that we get essentially guaranteed
linear scalability just because as we
add more service they can continue to
they can start processing requests
without affecting load on the rest of
the system in contrast right if we were
going to have an alternative mechanism
let's say we chose a coordinated
mechanism we elected one of these nodes
a primary well any request that would
have to go to the primary wouldn't
benefit whatsoever
from adding these additional resources
this seems almost trivial but it doesn't
come for free we have to explicitly
design for coordination free execution
in order to guarantee that no matter
what happens and if had enough data
layout independent of request patterns
and so on we can effectively throw more
resources the problem and get more
performance now what I just described
here st with adding more resources what
about the resources we have without
making use of all of the hardware that
we give a system well if we want to run
say something like distributed
transactions and here I'm going to show
numbers from Amazon ec2 I have eight
servers in the system with eight
different data items on them
and I want to know how fast can we run
transactions or multiple multi item
operations on these servers we can plot
the number of servers we touch per
transaction on the x-axis and the
throughput or the number of operation we
perform every second on the y-axis so
when we have one item per transaction
I'm gonna touch server a and item a and
then end my transaction if I have two
items per transaction
I'll choose two servers I might choose B
and D before my operation on each and
then end my transaction not surprisingly
right if I coordinate one Pomona's
transactions can be fairly expensive if
I run something like in memory locking
where when I want to touch B and D I
grab a lock on B I grab a lock on D I
perform my operations when I release the
locks it's gonna be fairly slow in fact
in this coordinated approach note this
is a log scale on the y-axis we have a
huge penalty
when we start performing multi server
transactions specific we have almost a
400 X decrease in throughput when
whenever I'm touching B and I'm waiting
to go to D and you come in and you want
to and you want to access B but you have
to wait for my lock it's gonna be very
slow essentially were bounded by how
quickly we can communicate not
surprisingly if we don't use
coordination let's say just say for the
purpose of this workload we don't grab
any locks I'm touching B I'm waiting to
go to D you come in and you try to touch
B you're allowed to just perform your
operation you run concurrently then we
don't have this throughput hit at all
you know when we're touching seven
servers per transaction we have almost
three order of magnitude performance
gain notably even right I said this
talked about distributed systems but in
fact there are benefits even on a single
node sometimes so when we have one item
per transaction on the far left of this
graph which is the coordination free
cases gets better performance and order
of magnitude performance improvement
what's happening here is that we're
running this on a on a server with 16
cores and 32 threads so 32 Hardware
threads and in fact well the chordae
case all the threads queue up on one
core waiting for their chance to get the
lock and the coordination free case we
can make use of all of that parallelism
available to us in the hardware we can
actually execute in parallel on all 32
Hardware contexts at once these
coordination free systems can in fact
improve throughput often considerably
and the key idea here is that if I don't
have to wait for you if I can from my
operations independently of what you're
doing on the data then we can each run
at the same time now a related concern
related performance related concern is
latency so given that our sit our
servers and our proxies are actually
deployed on on planet earth there's a
upper bound to how quickly we can
communicate namely provided we use sort
of standard networking technology we
don't communicate faster than the speed
of light the maximum of three what we
can achieve are the maximum latency we
can achieve a minimum laves we can cheat
on 133 milliseconds for our GT so simply
to send a message around the equator of
planet earth requires 133 milliseconds
now if we get very clever and we tried
it for instance drill through the center
of the earth like high-frequency traders
might like us to do the maximum through
but we can imagine the minimum latency
we can achieve is eighty-five
milliseconds and again here right if
we're queuing up all of our requests one
after the other the maximum through boot
can achieve here is on the order of 15
operation for a second even for it even
if we have this hypothetical borehole
through that through the core of the
planet this is not very good so these
whore nation free systems give us these
benefits enabling infinite scale-out
improving throughput by increasing
concurrency and actually ensuring fairly
low latency all in this sort of failure
free case I haven't mentioned the word
fails whatsoever it turns out that yes
if we do have failures oh if we do have
failures here then we will actually
guarantee an always-on response that is
by providing availability that is number
four here we in fact get one through
three as well but it's not this doesn't
just apply to the failure setting okay
now though many of you probably sitting
you're in your seats and saying gosh
what about the cap theorem right how are
we getting this coordination free
execution and in aren't there some
trade-offs here that you're not telling
you about for those who aren't familiar
with a cap theorem there's a very famous
result from Eric Brewer who's a
professor at Berkeley and now is
actually spending some time at Google
and through or along with the students
in the 1990s worked on a search engine
one of the first large-scale search
engines called Inc to me Brewer and
company realized that if you want to
build a web scale service and this is
one of the first web scale services
you're gonna have to make hard
trade-offs if you want to make sure that
for instance if one of your index
servers is not down users don't get a
404 in fact Brewer in is in the students
realized that in fact if you want to
build a large-scale service you'll have
to trade off between availability and
always giving the right answer or the
correct answer in fact there's a
trade-off and there's a very nice paper
about Hart book called harvest and yield
it's worth looking up if you haven't
read this that talks about trading off
for instance query all of your indexing
servers versus querying those that are
available at the
of query if you want to get the absolute
right answer you'll need to wait
potentially in the event of failures the
takeaway from the cap theorem is fairly
simple it's that certain strong
properties that make it easy to program
distributed systems require
unavailability or in other words require
coordination for instance if I want
replica 1 in your own replica 2 and we
have a single replicated data item X
where X is 0 if we want to make sure
that I see your rights will have to be
unavailable specifically if you update x
equals 1 on your replica and I read from
my replica there's no way I can read
you're right unless if our replicas
communicate it's as simple as that
and in fact for a fun afternoon read I
suggest you actually take a look at the
cap the paper proving the cap theorem
from Nancy Lynch and Seth Gilbert that
actually makes exactly this arguments
very intuitive we need to communicate if
we need to share stain now this is sort
of dismal we had all these sort of nice
properties we got from our coordination
free system the question is well did any
of that matter if we have to give up on
these nice properties it seems like
coordination free system design is sort
of hopeless and in fact this is a very
common sort of incorrect conclusion that
I see a lot we will talk about
trade-offs or unit veil ability and
consistency and so on the line goes
something like availability is too
expensive we have to give up all these
nice properties it only matters during
failures so who cares well it'd be sort
of sad if that was the end of the story
and perhaps surprising perhaps
surprisingly is that many guarantees
that we actually depend on today in
today's systems don't actually require
giving up availability in fact if we
treat this worst-case systems analysis
as a design tool we end up expanding the
space of algorithms and protocols we use
to build these systems specifically what
we found in our research is that many
legacy implementations of today's
database systems in particular are
designed for a single node contact
or a very tightly coupled cluster of
servers and they inherently use
coordination I'll go through an example
in the next slide so because of the way
systems are built and have been built
historically they in fact use
coordination however coming back to the
theme of this talk what's the worst that
could happen when we posed as a research
question
what if we built systems that didn't
have to coordinate that weren't allowed
to coordinate right by pushing open that
spectrum within that is the space of
possible distributed systems design we
found that in fact in many cases we can
build designs that avoid coordination
and less strictly necessary
and moreover frequently coordination
free implementations are actually
possible let's go through a very simple
example of how asking what's the worst
that could happen how can we build a
system that under all scenarios
guarantees a response for a very common
semantics found in today's database
systems in the example I'll give is
called read committed isolation it's a
fancy name for a very simple property
the slightly simplified goal is that
we'll never read uncommitted data so if
I write X goes 1 then x equals 2 you'll
never read x equals 1 you'll only see
the final value of my update so this is
a pretty useful property turns out about
every database system today implements
this guarantee in fact it dates back to
1976 when Jim Gray who won the Turing
Award for the development of the
transaction concept actually thought
about what it meant to provide read
committed in Gray's initial formulation
and in many sense
he used coordination to implement this
guarantee specifically since 1976 most
databases have implemented read
committed by simply grabbing locks on
records so I want to update X I grab a
lock on X I make my update x equals 1
make my second update x equals 2 then I
release my lock once I commit my
transaction now this is correct the
problem is as we saw earlier holding
those locks is fairly slow on the single
node systems Jim Gray was caring about
basically uniprocessors single node
doesn't matter so much the disk was the
slow thing today the network is the slow
thing and these types of implementations
no longer scale
going back to our question well how can
we build a system that didn't coordinate
and moreover is coordination strictly
necessary
turns out we might be able to do a
little bit better so how we avoid
coordinating using these locks well one
possibility and there are several is for
instance to use multi versioning for
example if I have a private register
ex-prime that I write to instead of
updating the primary copy of the
database
I write X for N equals 1 X prime equals
2 and then once I commit my transaction
I copy X prime over to the real X you'll
never see my intermediate State this is
a very very simple invitation it's as
simple as it sounds when fact there are
more of a high performance imitations
this we can consider but even this
simple implementation can order can
offer literally order of magnitude
performance games over these classic
implementations
despite providing the exact same
semantics which is we'll never see this
uncommitted replica database state so
what this highlights again is that by
focusing on this worst case by asking
how do we build systems that under no
circumstances coordinate we actually
deliver a large number of benefits in
practice and accounting for this worst
case in terms of network behavior
actually ends up improving the average
case in terms of performance and
scalability of the systems when we
deploy them the punchline here is that
in distributed systems design systems
have behaved well during network faults
can actually behave better during cases
where there are no failures in non
faulty environments as well and with
good designs where we've explicitly
accounted for the worst case we've we've
built essentially systems from the
ground up with the assumption that bad
things are going to happen we can in
fact benefit legacy implementations in
fact Martin Clement is gonna have a talk
on transaction sort of myths
opportunities and surprises tomorrow
that I encourage you to check out where
they're going to a little more detail
about some of these topics the research
both from my own group and from a large
number of other groups highlights than
the fact when we take coordination free
execute
as a first-class concern in our systems
in either at the language level that's
Peter talked this morning in language
like Daedalus and bloom at the system
level as in some of our work on AI
confluence and ramp or do the data type
level in terms of sort of buzzword you
may have heard of like CR DT s we can
frequently provide meaningful guarantees
that occupy really a different corner of
the design space the traditional systems
thought of doing at all so I hope I've
convinced you that distributed systems
have a lot to benefit in terms of this
worst-case thinking when we think about
sort of what happens when networks are
faulty and what happens when they're not
but I was preparing this talk I was
surprised and learned that there were a
number other situations under which this
type of thinking can similarly benefit
us and I want to highlight a few sort of
interesting examples like we came across
when I pulled some colleagues about but
instances where worst actually is best
as well the first of which is talking
about replication for fault tolerance so
if I have a database system for instance
or I have a stateful service and I want
to make sure that for instance when one
of my instances goes down I don't lose
data I very well might end up
replicating this service right so I
might have multiple copies by
replicating in order to not lose data
for many operations I actually increased
the capacity that my system can can
handle at any given time as well for
instance I have a primary backup
replication scheme I give serve
read-only queries from my backups so I
go to another scenario typically I hope
in many of your deployments failover is
handled automatically so for instance vo
again again a primary backup system here
I got my primary highlighted in green if
I have my sort of machinery to
automatically failover
my primary where when the primary goes
down I elect a secondary or I electro
replacement then I can use this same
failover mechanism for a bunch of cases
that aren't failures at all simple
example let's see you want to upgrade
your database from version 9 to version
9 point two of the software if you
didn't have automatic failover you might
turn off your front-end server
services go through each of your
database servers and upgrade them and
then once you've made the upgrade you
turn back on your front ends and traffic
resumes you essentially have a global
barrier and a lot of unavailability in
order to perform essentially a routine
software maintenance operation in
contrast we can failover your servers
you can start killing them one by one
kill a server upgrade its software turn
it back on if you have automated
failover in your clusters the nodes that
when they're down will essentially have
their work migrated to other servers
within the instance where within the
service and in fact this is a common
strategy used in many cluster compute
frameworks for instance when you have a
slow node in something like Hadoop or
SPARC you actually use the same you can
use conceptually the same rebalancing
tools to migrate stateful tasks from one
server to another that'll improve the
throughput despite the fact there's no
real failure there's just some slowness
on one or more of your nodes in your
system in fact if you're running a
cluster manager like meso s-- where you
want to say revoke resources that you've
granted to a process or alternatively
you tell your application team sorry we
need to reprovision this node because a
new service has higher priority you
don't actually have to go through the
manual process of notifying than
necessarily you can just kill the node
revoke the resources and continue on so
this mechanism that comes into play only
when there are sort of rolling outages
or when they actually have server
failures you can be exploited to your
benefit when you're forming common
operational roles as a final example
from computer science let's talk about
micro services micro servers are very
cool or-or-or if you're back in the you
know mid 2000 in target service-oriented
architecture so you know if you care
about you or if you think about a
typical service it can have some some
distribution of latencies so in this
service here is a fictional micro
services is your service let's say the
99.9% i/o latency is 100 milliseconds so
let's say 99.9% of requests complete in
one millisecond and you're 99.9 percent
light potential latency 99.9% of your
requests take 100 milliseconds
this leads to an average latency of 1.2
milliseconds so this this average is
actually pretty good then the tail
latency not so good so you might have
for a few we can say well gosh how do I
reduce GC stalls in this node for
instance how do I bring that down let's
say you get an order of magnitude
improvement in your tail latency here
under this distribution here for your
average latency you'll see less than 10%
in decrease in latency so really you're
putting if you put in a lot of work you
can order magnitude reduction tail
agency you're really not seeing it in
terms of most of your requests however
this is just one service and chances are
if you're running a service-oriented
architecture your serves as part of a
much larger patchwork of services that
are essentially orchestrating the applet
the the the runtime of a larger
application specifically right you might
be one of say hundreds of servers that
are being queried at once in order to
build a front-end sort of HTTP request
and if we imagine you have a hundred of
these servers here that they're being
queried by some front-end then your tail
latency is very likely to appear on
average for every request so the front
end lap the average front end latency
with 100x fan-out for this distribution
is around 64 milliseconds in fact if you
now in this case if you were to do the
same exact reduction reduce your tail
latency by a factor of 10 the front end
average latency which previously only
reduced by less than 10% reduce it you
know essentially sees a tenfold increase
what this demonstrates is that your
services corner case in effect maybe the
average case for the consumers of your
service and this is simply a matter of
statistics the more requests we draw
from a distribution the more likely we
are to hit outliers in that distribution
okay so these examples are all other
instances in sort of large-scale systems
design and turned out like when I talked
some colleagues from human-computer
interaction turn out there's a very
interesting application of sort of worst
is best in what's known as universal
design
so the idea man Universal dine is very
interesting for those you don't know
this is this picture here is a curb cut
it's essentially those things on the
sidewalk we're instead of having to step
down six inches to get off of the
sidewalk you can essentially walk down a
little ramp so curb cuts were originally
designed in order to help people in
wheelchairs I simply not have to
essentially hop six inches off the curb
every time they wanted to get off and
sort of wheel themselves up a a six inch
wall in order to get onto the curb now
what happened when they designed these
curb cuts for wheelchair users is that
people like you and me or or those of us
who aren't actually in wheelchairs came
to benefit as well for instance evil
riding bikes can use curb cuts I use
curb cuts when I cross the street as
well we don't have to just leave step
down six inches all the time this is the
classic instance of this design pattern
of Universal Design which essentially
says by building systems that
accommodate sort of everyone who might
be using a sidewalk we end up not
benefiting not just those people for
whom the system was originally designed
that is the the wheelchair users but
about we benefit everyone else similarly
right when I watch Netflix I frequently
turn on subtitles and I imagine the
number of people in the audience who
turn on subtitles is much much larger
than the number of people who
necessarily need subtitles in order to
absorb the content however by reasoning
about in building in different ways to
absorb this video content we actually
improve the user experience for everyone
this idea behind Universal Design is
very important for essentially
accessibility but it has benefits that
transcend simply the corner cases or the
sort of the the outliers in terms of the
users who might actually access our
services for instance w3c makes a very
interesting case that there's a strong
business case behind accessibility there
is a close correlation between the
design practices that improve
accessibility and design practices that
improve things like mobile web design
usability and even things like SEO so by
handling sort of does
for everyone we in fact improve the
experience for everyone it's not simply
that we're reasoning about outliers
it's that by put it by sort of changing
by considering a larger scope of users
we end up improving the design
experience for not just the people on
the fringes but for more or less
everyone involved one final example it
turns out that actually simply reason
about not always best so if we're trying
to maximize some sort of function as we
might do an optimization here we have X
on the x-axis and f of X which is cut
off on the on the y axis if we have some
idealized function like this it's very
easy to choose sort of the optimal
solution right we can we can look here
and say okay f of X is maximized at this
point in the middle here now this is an
idealized function what happens is that
in the real world
functions aren't always so ideal for
instance we have a function like this
which is less well behaved we're very
well going to have the same globally
optimal point in the middle here but
only if we can very accurately choose
the value of X that is this solution is
fairly brittle in that if we miss
estimate our parameters we might very
well miss the target and then if the
solution that's much worse in fact the
worst possible point on the graph
instead we might do is try to opt for
what we call a stable or a robust
solution in fact sometimes choosing the
absolute best is not actually best
unless if we're extremely sure that
we've hit the right mark on the x axis
and there's a whole body of literature
in the optimization space if you're
interested called robust optimization
that studies find in the stable solution
in fact in real world optimization
problems for instance trying to decide
should we you know how do we route for
instance taxi cabs or how should we
deliver packages very frequently we see
functions that look like those on the
right as opposed to those on the left so
I hope I've convinced you that designing
for the worst case can in fact improve
the average case I think there are some
interesting opportunities to apply this
type of thinking in your designs going
for
as well specifically this type of
thinking right doesn't always apply as
we saw at the start of the talk there
are a couple instances where think about
the absolute worst-case was not a good
thing however in your applications if
corner cases are fairly common or
similarly are unpredictable it might be
worth thinking about what the worst case
means for your operating environment and
what it means for your application when
things you didn't think we're gonna go
wrong did go wrong you know in a
nutshell this is a bit illogical but
really when normal isn't what we think
thinking about the worst case is a
remarkably powerful tool because the way
we define normal in terms of our designs
the way in which we say this is expected
and this is unexpected
ends up having a profound impact on the
way in which we build systems in fact
simply going through the exercise of
asking what is the worst thing that that
could happen can be very instructive in
helping us find point to the design
space we might have not otherwise have
thought about we might not otherwise
have curb cuts waiting we might not
otherwise have thought about gosh what
happens we build systems that never have
to talk it sounds ridiculous and yet
somehow in terms of combating you know
the cognitive processes in our minds
it's very helpful and pushes us towards
new spaces in the design space go back
to our early examples right we saw that
for instance in cluster provisioning it
was ridiculous to that try to provision
a cluster for 7 billion users but if we
ask this question you know what we might
not need to worry about the 7.3 billion
users we might need to answer questions
like what's our what's our strategy for
scale-out how far can we actually scale
our services before we need to for
instance buy a new data center or switch
8 ec2 instance types or moved multiple
availability zones this sort of
worst-case thinking puts a much on us on
a trajectory that we should maybe not go
to the far you know logical extreme czar
the conclusion but we can kind of think
well gosh what happens between here and
7.3 billion users similar in to in terms
of hardware you know we probably won't
be setting MacBooks to Mars any time
soon but it is useful for us to ask
questions like what happens if we have
bid flips in our hardware right do we
need to pay for
like ECC where's the right point in that
trade-off space and have we actually
chosen the right set of sort of
worst-case scenarios that we're willing
to pay for in terms of security sure
yeah
most likely all of our developers aren't
trying to compromise our data we should
probably have a strategy in place in
place that allows us to manage or audit
internal data accesses so by thinking
about these extreme cases we in fact
make sure that we have drawn the
boundaries in the right place every
system has to draw some kind of boundary
it's unlikely that for all cases a
little ability systems that handle every
worst-case scenario but knowing which
scenarios we care about in which
scenarios we've opted to not sort of
accommodate is extremely important I
think at a very very high level thinking
about the worst-case is a very important
tool for a second
essentially examining your biopsies when
you're building software and you're
building systems who is this software
designed for what environment to expect
this to be running in and what happens
if we're wrong so in conclusion reason
that were skate scenarios can be a very
powerful design tool this has been
really key in some of our research on
coordination avoiding database system
designs in fact this can improve
performance and robustness for a number
of other scenarios in addition to sort
of combating bias easily run into when
we build and operate software in
practice so I'm Peter Bayliss there are
a number of people who helped out with
this talk who I really appreciate who
gave some of the later examples and I'd
love to take any questions thanks for
your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>