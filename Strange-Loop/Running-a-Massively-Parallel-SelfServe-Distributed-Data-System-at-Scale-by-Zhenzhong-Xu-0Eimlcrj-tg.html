<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Running a Massively Parallel Self-Serve Distributed Data System at Scale&quot; by Zhenzhong Xu | Coder Coacher - Coaching Coders</title><meta content="&quot;Running a Massively Parallel Self-Serve Distributed Data System at Scale&quot; by Zhenzhong Xu - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Running a Massively Parallel Self-Serve Distributed Data System at Scale&quot; by Zhenzhong Xu</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0Eimlcrj-tg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I know you guys just had a wonderful
lunch but I still want you two guys to
imagine you guys going to a Michelin
three-star restaurant right now so as we
walk pass through the door the
restaurant is already packed with happy
customers you find a seat look down at
your manual click the couple things you
wanted to try the waitress comes over
take your orders and go all the way into
the back order a packaging now it kind
of stuck with this eagerness of waiting
for that hot sizzling food at least
they're gonna be for a while right so
let's imagine what's happening in the
kitchen right now what happens to your
order along with all the other orders
from all the other customers here's what
I'm imagining the kitchen is very clean
it's very well organized the head chef
is heading the show but the line cooks
on many of their own stations they all
tend to have their own responsibilities
there and the communication style
between the head chef and line cook is
very concise and effective it just makes
it look so easy for well-managed the
restaurant like this to pump out that
few hundred orders every single night
how do they do it so I recently read
this book called Kitchen Confidential by
Anthony Bourdain so you revealed a lot
of secrets in the kitchen but I think
what's important for me is I realize
there's a whole lot of similarities
between running or catching and building
a distributed system I can take couple
quick examples here so in Netflix we
have hundreds of micro services that
works together to achieve a single goal
which is to satisfy our customers with
the best online streaming experience but
when you look down at each individual
micro service level they all tend to do
one thing have one owned only one
responsibility and that's it and does
very well right so if you compare this
with the line chef the work the by line
chef has nothing innovative about it
the real job for them is all about that
mindless repetition to get that wine
ingredient right before packs into the
excellent chef this is very similar the
communication patterns so you might have
noticed in the kitchen there are more
declarative style of communication
patterns being used for example the the
head chef mine say give me two well bake
the potato with skin removed so on the
contrary you will rarely hear imperative
statements like go ahead watch those two
- potato for me bake them for 10 minutes
and peel the skin for me right now right
so there are reasons behind all these
details so today let's all explore some
of this data details together alright so
today's topic is running a massively
parallel self-serve distributed data
system as skill so we're gonna cover
what exactly this real-time what exactly
is this real-time data infrastructure
we're building and Netflix what we need
it
and the total cannon first we have two
major challenges that's exactly we're
gonna be talking about one challenge is
how do we actually build such massive
parallel system as skill the other
challenge is we it is a self-serving
multi-tenancy system so how do we
actually make the system to cope with
failures and also adapt to changing
environments so we're going to go over
those challenges and solutions and our
principles so what is this real-time
data infrastructure were building on
Netflix with 100 100 million users
subscribers worldwide we already have
and for almost any internet-connected
screens it's capable of player playing
Netflix so which actually the entire
ecosystem is generating a lot of events
data that we actually leverage in the
backend so things like when people click
on the play button click on the pause
button
so all those translates to events and we
actually use all the events for doing
some other purposes for example use
those events for improving customer
experience by doing better
recommendation algorithms
personalization algorithms
and also unaccountable priests inside we
do use those data for making decisions
such like purchase decision content
purchase decisions the list goes on so
on the other side on the architecture
side of the story so with the recent
rise of the event-driven architecture
you might have heard you know the
notification pattern events or sin
pattern CQRS which stands for common
core irresponsibility segregation
pattern those are very tends to be
become very popular especially with
micro-services architecture so it turns
out building a real-time reliable
innovative distribute I mean
stream processing system actually helps
micro services to actually allow them to
better making innovations so with the
patterns we just mentioned couple
patterns we just mention here along with
the stream processing pattern
traditionally those patterns were
implemented on top of enterprise
application integration and enterprise
messaging bus so this implementations is
proven to be real-time but it's also
proven to be not so scalable I listen
now to our scale on the other side of
the story the batch system is proven to
be very very scalable you can run very
large amount of data and do analytics on
top but at the same time everybody knows
about it typically resulting a long wait
before the job finishes so what we want
to build is this streaming platform that
allows us to be both scalable and
real-time by now you guys probably have
heard a lot of the data driven culture
in Netflix I'm not going to repeat the
same thing there but I want to splash a
couple colors into it so because of this
micro service is the architecture we
have in Netflix a lot of the team's
tends to own one or more micro services
and they tend to need to make large
amount of local decisions so we won't be
empowered and to make those decisions
without having to consult essential as
the decision-making
so want to be able to help them to
consume any data they want across
organizations across technology barriers
this is where we want to provide a
streaming platform that allows people to
be able to consume the data they want
and also able to write the job they want
to get analytics they want so what
exactly is key to Keystone streaming
system we help publish collect move and
compute event data in near real-time at
cloud scale Keystone is a collection of
micro services and components that can
typically divide it into two different
layers the layer on top is what we call
control plan that does all the
management typically sort of like a head
chef the components on the button is
more of like land chef they do the heavy
liftings and make the work happen so we
have the producer API pops up queuing
system we build on top of Kafka and we
have the stream processing service we
typically internally abstract a
different stream processing engines so
our users don't have to worry about the
typical implementations of them and
there are cases where people do want to
write on top of a specific engine api's
and we also do a lot to do that Keystone
is also a single self content logical
platform as a service so even though
it's comprised of multiple micro
services to our user it feels just like
a single service Keystone is a
multi-tenant self serving to Keystone is
a self-healing call failure tolerant
service guarantees at least once
delivery semantics it's also adaptable
to changing environments so this is a
high-level architecture for the entire
Keystone there are two boxes over here
the Box in the middle left is where our
router the Keystone data pipeline
architecture where it handles the
internet flix data back-end that
basically
routes all the data in the flick
generous events data generates allowed
into the batch season and also feeds
into the stream processing system the
box on the right side is where we want
to set up the stream processing platform
that allows people to hook into their
own jobs and does the processing in real
time so let's jump into the challenges
and solutions we have faced so the first
challenge is scale so this screenshot is
taken from the Keystone data pipeline UI
where people can define how do they want
to route their data either into a hive
store which allows them to do batch
processing or you know routes into a
Kafka cluster allows you to do stream
processing the user can typically define
a topic where they can hook up their
producers to writing to they can
optionally also to define transformation
logic such as filters or projections the
filter will currently supports XPath
filters right there and projection which
supports both black lists and the white
list projections so all this
transformation essentially runs in the
routing layer which is a stream
processing job and eventually the user
can defy us think they can route all
these events to for their final
processing so after taking a look at the
users field let's also take a look at
the architecture side of a view so this
is the autonomy of a single string how
it looks like so when the producer
produces the events into the kafka
cluster which is the current service
over here the stream processing job is
responsible for consuming the payloads
from the copper cluster does the
processing to the actual payloads and
then output to the sink this is a normal
case but in failures a dueling failure
turns the stream processing job is also
responsible for maintaining those states
by doing chip hauntings and office
management so this is more directive in
view of the entire system especially
focusing on
the Kafka class at the CUNY Service and
the flink John manager castor as you
guys can see here the if you guys are
familiar with Kafka cluster technology
it's essentially allows the preachers
producer to shut all the events and
produce those events in two different
partitions each partition is essentially
come a lock that associate each payload
with offset message with offset number
so on the right side of the picture
there the router we have is implemented
on top of the flink apart if link
technology I'll quickly describe how
Frank actually work if you guys not
familiar with it
so flink can be is composed of a job
manager cost and the task manager
cluster the jar manager cluster
basically does all the tasks accordion
Nations does failure handling so let's
say if one of the test manager goes down
you will basically be able to reschedule
a job into another task manager if it's
available the past test manager
basically allows different difference
allows parallel job to be happening that
each one of these tasks is responsible
for consuming from well more partitions
from the kafka cluster so as you guys
can see here the parallelism defined
from the Kafka site is different from
the parallelism defined on the flink job
manager the job so this allows clear
separation so we can scale the two
different services independently so one
of the other thing I want to mention
here so if you guys look closely at the
first parallel tasks it's a source
operator that consumes records from
Kafka and runs this map function
internally map operator and then
generates output to the sync operator so
this is a massive parallel or another
term for is actually called impersonal
parallel workload so that means this
workload is actually easily paralyzed
the compared to some other workload for
example my previous tile job which
reduce sorting over the network and
shuffling over the network
so those more complex
style job is a completely different
beast to talk about so we're not going
to focus those here
it requires largely local stage
management requires triple team
management requires optimization some
data localities so we're going to
continue to focus on the parallel cases
here couple more things I want to
describe here is typically in a stream
processing job you want to be able to
handle the back pressure so in this case
you want to back pressure to be able to
propagate all the way from the sink to
the stream processing job when happens
when when the back pressure happens you
want that propagation to happen from the
sink to the stream processing job all
the way to the front of kafka cluster if
enough pressure is built up on the kafka
cluster the bad pressure eventually
propagates to the producer you will
start seeing producer job that data are
there that's what we want and the
typical way to enforce that is to this
multiple ways of doing that depends on
the delivery semantics we want to
guarantee so let's say you want to
deliver at least a once
semantics we want to be able to retry on
those payload indefinitely and to build
up those back pressures okay so we just
briefly talked about separation of
concerns so we want to separate the
messaging layer and the stream
processing service so we can make each
service to scale individually and each
service wants to manage their own
authoritative States we're gonna get
more into this as we go into the
architecture side of the story so we
also want each service to independently
manage their dependencies so for example
cough car cluster is built on top of
deployed on top of ec2 instances on top
of AWS but on the other side the string
service stream processing job is a more
natural fit to run container platforms
so internally we have this container
runtime platform code Titus it provides
all this capability to us to allow us to
run flanca jobs on top of them in
containers
allow us to do resource provisioning
scheduling bin parking capacity
guarantees through capacity groups also
guarantees resource isolation they also
implemented their own network drivers to
throttle the network loads it's pretty
reliable switching for Network isolation
so far and also they provide this
feature which is very useful to us to
provide this per container IP address so
we don't have to worry about port
mapping resulted in service discoveries
so let's talk briefly about delivery and
processing semantics they're typically
there are three different processing
semantics here so at most the ones at
least the ones and exactly once so and
most ones just so this typically relates
to the fundamental distributed system
problems so network is not reliable and
there's partitioning how can happen at
any point of time so when when your
requests are what do you do with it so
for a most of us it's a very simple
solution we request times out or you
cannot deliver the data I just drop the
data so the data delivery is now
guaranteed in Keystone pipeline we
actually guarantee at least once
delivery so that requires to manage our
string internal States to be able to
track those offsets and doing failure
scenario we need to be able to go back
to the previous offset and redeliver
those events and exactly once it's a lot
harder to achieve but we're also going
to talk slowly about those so there are
two different ways of achieving at least
a wise processing semantics one way is
to do synchronous step hauntings Ravello
loop so event loop is overlay overloaded
turn here but just bear with me so this
event loop I'm talking about here is
basically allowing the events to reach
the one processing thread the one
process where is gonna do synchronous
processing so it's gonna go
through process you went event and the
checkpoint in which is a common methods
individually so each what each one of
the staff fails we just want to retry
the entire event loop before me making
progress to the next event
however there are some slight variations
to this approach so certain I all bound
workloads can be parallelized but the
ideas still be still be the same there
so if you can if you have the
user-defined tasks you want to be able
to run in parallel you can actually do
those in the processing methods but
eventually still want to do the commit
against this one particular batch to
make sure the officer you commit
actually ties to this one particular
batch only so the second approach is
called lightweight a synchronous
snapshot there's a paper published for
this particular method I'm now going to
go into too much details for this but on
a very high level what it does is the
source operator essentially emits a chip
on barriers at interval point point of
time so allows this checkpoint to flow
through together with the payload
themselves so at this one this barrier
flow through the inter - rocky sink like
a job the diagram eventually will reach
the sink operator so when the sink
cooperator actually doesn't commit those
are the commit points when the barrier
hits them so I do want to mention
there's a slight difference between at
least exactly once
processing semantics and at least the
ones I mean exactly ones deliver
semantics the processing semantics
essentially guarantees even though this
recur we might be processing multiple
times but the side effect it cost to the
local states the state the state's we
manage for these operators is
essentially the same as we processing
exactly once the exactly once
processing semantics typically garen is
guaranteed weezing the system where the
barrier can be reached so if you have a
stream processing job that needs to
deliver to a
no sink which doesn't support this
barrier or doesn't support important
operations you will typically not get
exactly one delivery semantics but
instead this will refer back to at least
once deliver semantics
so speaking enough about all those
single strings so we do have per stream
monitoring and alerting this is couple
example you know representing all the
signals we monitor there's one thing I
want to mention here so when we do
monitor our Kafka class and progress of
all our streams we typically monitor the
offsets we love to commit right so we
compare those offsets with the log he'll
see how much lag we have that indicates
how much catch-up time we need to
perform this is done externally by
having a separate service to monitor for
those offsets compared with the locket
sometimes there are different approaches
by monitoring the kafka so if you
actually commit in the offsets in to
Kafka directly you can also have the
option to monitor for that
Kafka consumer lag metric but that tends
to be not very reliable because we can
get very philosophical about this this
reassessing is like humans they actually
caramelize on time so what what I mean
by that is Kafka's systems sometimes you
know if things break the magic will be
gone it doesn't mean there's no lag it
just you're not able to observe it so
it's very helpful when you have to have
an external service that does monitor
the lag so after looking at that one
single string let's look at more complex
scenario where we have a fan-out
scenario here so as I mentioned the
Keystone data pipeline actually is a
copper architecture star that supports
post batching system and the real-time
system so we do currently support read
different style of sync
wine is which rightly write the pillow
the pillow events into l3 in a sequence
file format and the
Big Data Platform sister Sheila picked
those files up and process them into hi
system this typically happens within
couple minutes we do have a lesser
search sink that allow us to do some
sort of log aggregation and the third
type is we do route some of the strings
into a Kafka cluster which allow us to
do stream processing on those secondary
Kafka clusters and the reason why we
want why we want to do those to have a
secondary Kafka cluster is we want to
control our fountain cough our cluster
so no arbitrary consumer contractually
consumed from the file fronting so we
have the full control of the system to
make abstraction a lot more easy to do
failure scenarios let's say the alas
search cluster is going down and since
we're running a multi-tenant system we
don't want this one particular thing to
affect all the other jobs especially
other strings running two different
things so we want to have these logical
isolations between different streams how
did we achieve that I'm still taking the
flink job as an example here as I
mentioned before the flink job is
comprised of a job manager cluster and
the task manager cluster the job manager
you leverages zookeeper for coordination
and the task assignments and the task
manager performs checkpointing by
storing its local states into a remote
stay backend and during failure
scenarios the task manager actually use
those checkpoints to refresh its own
state and restart from the previous trip
point the way we want to isolate two
different strings is to basically to
provide different isolation PI spaz for
both zookeeper and ESRI stay back end so
even though zookeeper in this reach
services - Alicia shared in a single
region
so as long as we keep her service tunnel
go down the isolation is guaranteed
there and this also applies to diploma
or diploma model so imagine we want to
deploy
a upgrade to an existing job so we want
a new task manager we just spun up to
form its own cluster without being able
to connect to the old cluster job
manager so we also want to have
isolation there so this one step
guaranteed stream level isolation and
deployment level isolation and what
about regional I long island isolation
you might ask so in Netflix we have
difference for three different regions
in across across a word so each regions
in either mode that typically means we
have redundancy across those theaters
data centers let's go more into this
thing okay so this particular event is a
very popular event in Netflix at peak
Holloway actually has more than four
gigabytes per second incoming data and
with all those fields actually generates
more than 20 gigabytes art I don't know
what's the biggest knockoff car cluster
you guys Rob but for us there can become
a problem so we need to solve that and
also by the way it's typically not
recommended to run half a classroom more
than a hundred or more than 200 brokers
so we do follow that recommendation
there as well so the way we serve is to
using using hierarchies though what I
mean by that is once the events is
written to the front in Kafka cluster if
the degree of Pharoah is huge what we
can do is we can route those events into
a secondary Kafka cluster and allow
additional thing out to consume from
that secondary Kafka classes so that way
we can scale the cluster little bit
better now you might be asking now what
happens if this particular string gets
too big so currently it's 4 gigabytes
per second what if it goes over 10
gigabytes or 20 gigawatts to the per
second what should we do about it right
so this only solve the fan out what
about the incoming stream so my
colleague colleague is actually working
on something that allow us to spend a
single table
across multiple car clusters I'm now
gonna go too much details into that if
you guys interest I'll provide a link at
the end of these slides so we discussed
a couple basic scenarios let's look at
the total infrastructure scale so we do
press we do have more than 500 plus
billion events generated every day we
process more than 1.3 trillion events
last time I checked per day we have more
than 800 topics more than 1800 streams
the currently we have about 4000 kafka
instances spread across about 20 to 30
different profile clusters clusters
worldwide and we currently have more
than 9000 stream processing containers
they're running that's running stream
processing jobs so with that kind of
scale it's not hard to imagine that we
need multiple fronting car clusters to
support all this data so even though
kind of a block to the name is away but
you guys can actually still see those
those we named those car clusters after
mountain range range names I do have a
point
so you guys will see after couple of
slides so with all these kafka clusters
I'm mentioning this more than 20 equals
to 30 to 40 Cal clusters across the
entire company so when this size grows
the only certain thing will happen is
one of them will fail at some point of
time so we need to be able to handle the
cluster fell over and the typical
way for us to do this right now is when
kafka cluster goes into a bad condition
we've seen cases like because of some
network partitioning issues we are
partitioning issues that causes a
controller to get out of sync all with
as its own brokers and there's some
weird decay issues which can be caused
by long GC time so all of these things
can actually cause the kafka cluster to
go into unrepairable state this
especially happens more when running in
the cloud so what we do is when that
happens when we detect that we spin up a
new curve
cluster reroute all the producers out
traffic into the new Kafka clusters and
at the same time and at the same time we
reroute the router to basically consume
funds this two different Kafka cluster
and the reason for that is we want to
still train all the data from the old
cluster what's your reading from the new
cluster there so after some time
hopefully the old Kafka cluster will be
recovered and we fell back to the old
classroom that's how we do it today that
I need to bring up a concept here how
many of you guys have actually have
heard of a pat versus cattle okay great
awesome so just quickly briefly talk
about it so
cat versus pedal is this concept when
Randy bus initially brought up to
discuss the different DePaul model in
data center versus in the cloud so in
data center every single machine has its
own name right so when the disk goes bad
or network goes bad for that one
particular machine the obstacles in fix
that machine and then bring the service
back up in data center I mean in a cloud
it's completely different so when a
service calls done on particular machine
they don't try to repair the machine
what they do is to swap that machine out
for is actually saying instance and run
the same software instances on top of it
right so that brings up a good point so
you got to remember that Kafka cluster I
show you guys we do have names for those
castle clusters they're all different
sizes they have different topics in them
so we also currently have this
initiative we want to make the Kafka
cluster as immutable infrastructure as
well so and this is just a phishing it's
probably gonna be another talk when we
have the time so the these are the
principles principles we just talked
about so when designing a large-scale
distributed system you want to be able
to treat all the failures at the
first-class citizen just assume all the
services can fail even the data center
casual affair as well
does happen it doesn't happen often but
it does happen so once you have
separations of concern between
micro-services
you want to be able to scale the
independent or eat independently and
managing independently as well so also
want to impress him you immutability
over there okay let's talk about the
self-serve and a multi-tenant problem we
have faced the problem is we have
hundreds of customers if in a thousand
they all have diverse customer
requirements they all want a combination
of different features we provide they
all have different platform trade-offs
they want to pick for example when
there's failure they want to sometimes
say well they have to pick between
latency versus duplicates we want to
allow be able to allow them to do that
so when in a multi-tenant system change
is the only constant so what are the
changes so one customer comes to us when
the provision on your stream they
typically expect to be provisioned
within minutes and sometimes they were
to tweak their filters or projections or
whatever custom logic they want to use
you know just because they want to test
in production and also the scaling
activity happening at all time so when
the customer comes in we do initially
asked them for some initial estimation
of the the traffic traffic volume but as
time goes on this topic can actually
spike in traffic volume to the point
maybe that original Coughlin's cost
wasn't able to handle this so internally
we need to be able to scale all our
platforms that includes moving this
topic into a different cluster and you
know scale the stream processing job to
be able to handle the additional
payloads there are a bunch of
infrastructure upgrades can be happening
so for example the targets that were
just mentioning is our container round
time they go through bi-weekly updates
that requires periodical container
refreshes essentially there's also
requirements from our side to
- to upgrade underlying stream
processing engines for example and
majority of the platform thing actually
want to guarantee provide innovative and
reliable platform to our customers so
that means we move since really fast so
that sounds pretty hot already but
there's more so these different failure
modes so the infrastructure can actually
fail as I mentioned the the failure can
happen at any level the instance might
be gone the container might be gone the
AWS availability zone might be entirely
gone even to the point might be AWS
infrastructure can be gone it doesn't
happen very often but it does happen
sometimes and also embrace failure where
any component can be temporarily
unavailable any point of time as well to
solve the problem for customer to have
diverse requirements who want to be able
to build all this building blocks for
example the the source operators the
transformational printers we just
mentioned and the same Sinko operator so
all those are composable progressed the
customer can pick and choose so let's
get to the declarative reinstallation
part I do want to give you guys a
challenge first before coming to this
once you guys to think about why caching
orders mostly immutable means you cannot
modify them in place it's typical
typically append-only this depends to
contracts like leasing cultures and
hospices purchasing cultures as well why
is that and I'm gonna we're going to
talk a little bit about the declarative
work enslaved the reconciliation
approach how does that help us with
scaling eventual consistent and helped
us with less human interactions and by
the way we actually our chain is only
about 10% when you need to be able to
manage this entire infrastructure we
cannot afford having a lot of human
interactions with the system so
everything needs to be automated so
declared of reconciliation what is
declared declare basically
occasion pattern that allows the control
plan that does the orchestration the
managing of the data plan the
reconciliation is a mechanics that
drives in star system towards its
intended goal state so let's talk about
the Goldstein's and current states and
then we're going to take a concrete
example there so the goal States is
something that the customer defined that
they want to eventually achieve and this
can be break down into multiple smaller
goal states for different components and
the current states is for each component
to go out there and try to figure out
what state is it currently in right so
what does it help us with the goal
States and current states when a single
component has its defined goal state
that means eventually you want to go
towards that direction and it also has
its current states it can compare this
to different states and compute a death
so as soon as you can peel the death you
know exactly we need to do to actually
achieve that state so let's take a
concrete example here so when it comes
when the customer comes to us it defines
a particular string they want to route
that typically just involves a
particular topic the custom
transformation logic and the sink of the
sink where they want the payload to go
into they also come with some certain
expectations you know low latencies at
least once delivery guarantees and all
that kind of stuff what the customer
don't care about is where is this topic
is which cluster it belongs to you know
how do you run your stream processing
job what's the underlying stream
processing technology using so what we
do is on the control plan we break the
simple goal state into multiple sub goal
States and signed two different
components and now since becomes easy
right so as I mentioned when the
component in cells can figure out their
goal states and the current state they
just need to perform this real
constellation that allows the entire
system to drive towards their goal state
and how do we do this reconciliation
it's natural to think of a state
machines right so when we talk about
goal States where you want to eventually
achieve and you have a starting state
and you have your current state this
will allow you to allow the Intercity
interaction move towards that goal state
so this example of our deployment engine
diploma workflow deployed on top of the
stream processing job so it allows both
the low latency and the load to placate
trade-offs that can be specified by the
customer now let's also talk about some
of the failure cases when you go or when
you run Kafka cluster the typical source
of truth for that culture class actually
residing zookeeper I don't know about
you guys but zookeeper has been pretty
bad for us especially in a cloud
environment so you probably don't want
to rely on too much especially in a
distributed system like this right I
mean it's supposed to be reliable but
you want to add that additional layer of
safety there so what happens if the
zookeeper goes away is this reconcilable
system actually recoverable from this I
think we do so as long as we store all
the customer required gold stays in our
top level control plan persistent
storage durable storage as long as those
are durable and persistent we can
recover the entire data plan from those
single source of choose this also
applies to the stream processing
platform where the container long-term
might be not available for some extended
period of time so as long as the
existing running containers are doing
their job I think we're okay the only
problem is we're not going to be able to
submit new jobs temporarily until they
recover so this reconciliation
architecture also allows the single
source choose to be enforced reconciling
to the data plan continuously what about
the control plan goes away so as long as
the data plan is actually still running
with the current job that only means the
customer
Cimino jobs we still functioning
correctly with the existing job so it is
a variability hit but at least it
doesn't affect the current job so what
this single source of truth allow us to
do is to allow eventual consistent
convergence on goal state and this goal
state is important because they are
defined by the customer so these are the
principles we just talked about as well
want to leverage reusable building
blocks we want to leverage the single
source of truth and declared every
constellation to build the system to
cope with changing environments and also
tolerate to failures now lets you know
regarding that challenge I give you guys
earlier so why do we why is the chick
catching order and contracts those are
immutable I mean open only it's because
humans are pretty bad at computing this
so they want to be able to quickly add
that one append only stuff to figure out
what's a diff there and also a couple
other challenges I want to give you guys
you know after this talk you can think
about it how many more similarities you
can think of you know between
distributed architecture and kitchen
management thus our principles actually
apply to all of them and that's it I
think we have some time for some
questions and I do have some references
there and I have to mention here because
my manager asked me to we're hiring cool</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>