<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Building switches like servers: the strange world of overlapping networking&quot;  by Sonja Keserovic | Coder Coacher - Coaching Coders</title><meta content="&quot;Building switches like servers: the strange world of overlapping networking&quot;  by Sonja Keserovic - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Building switches like servers: the strange world of overlapping networking&quot;  by Sonja Keserovic</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WDfWd-Utcgo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well thank you for coming this talk
today I will talk about how we build
network switches as servers at Facebook
so hopefully you'll find this topic
interesting and to illustrate why I'm
giving this talk a little bit about
myself so I work as an engineering
manager at Facebook I work on I support
F bas team F law stands for Facebook
switching open switching system and what
we do we build software for our custom
switches and we are part of Facebook
infrastructure but what is interesting
and relevant to this talk I actually
never worked in networking
before joining Facebook my past
experience were either distributed
systems when I worked at Twitter or for
a long time language compilers and
runtimes when I worked at Microsoft so
when I heard about this project I
thought it was so fascinating that I had
to be part of it things that kind of
project was attempting to do with kind
of changing the industry and so I hope
by the end of this talk that I convinced
all of you why is this really exciting
so here is an overview of what I'm
planning to cover today first time go
over introduction just give you context
about Facebook network then I will talk
a bit about how this network was built
in the past and also very important how
we operated this network and then what
were there some problems with with those
ways and also probably the biggest and
most interesting part of the talk is
going to be about the approach we took
to solve these problems and then I'll
briefly touch on some new things we are
contemplating doing so let's look at the
how Facebook network looks like so first
this is image of all people using
Facebook and their relationships there
so this is our social network
and as you know there's lots of people
using Facebook and other apps and in
order to support these people and give
them good experience we had to build
vast infrastructure so things like
really big data stores really advanced
caching mechanisms even things like
building advanced developer tools to
support thousands of engineers working
at Facebook and obviously that meant
building a physical network that can
handle this much traffic so let's look
at that network at Facebook we usually
divide this networking these three big
domains data center network backbone
Network and edge network data center
network is where all our servers are
connected together usually inside one
physical data center building then these
data centers are interned connected
themselves via backbone network backbone
network consists of very high throughput
optical links that usually go between
major metro areas and as you've
remembered from the previous picture
since this is a global network some of
those links are in undersea cables for
example we just announced we are putting
another undersea cable in between
Eastcote US East Coast and Europe in
collaboration with Microsoft because
both of our companies need more
throughput and more connection between
those two regions and then finally edge
network consists of pops or points of
presence this is where Facebook network
connects to the wider Internet as well
as where people using Facebook connect
to us usually we are there is peace so
what focus of this talk is going to be
and where we made these changes of how
we build network is going to be in a
data center networking so
since where I'm gonna focus for rest of
the talk so this is how data center
network topology looks like and I know
this is fairly fairly complicated but
don't worry you don't really have to
understand why it looks like this way
there are a couple of key components and
I will just kind of make sure as I
cannot go through this image to kind of
point them out so on the bottom here are
rack switches so if you know how data
centers look like inside there are lots
and lots of racks they're filled with
servers and the top of that rack there
is a top of the rack switch or rec
switch or RSW one interesting thing to
know just about this part of the network
is there is no redundancy
on this layer of topology meaning if the
racks which goes down all the servers in
this rack lose connection
rack switches connect to what we call
fabric switches which are bit bigger an
interesting thing is that every rack
switch connects to four fabric switches
so there's a redundancy on that layer
and the network is designed in such a
way that if any one of those four fabric
switches goes down traffic is still
flowing fine
then the next level up is where we have
these fabrics which is connecting to
spine switches and again they have four
of them so there's redundancy in that
level and then the last layer is where
spine switches connect to edge switches
again there's four of them and that's
where the network where the network
basically leaves data center you might
ask yourself why is this so complicated
and the reason is what we're trying to
solve as a problem here is how to
connect tens of thousands of servers
efficiently together and efficiently is
a you know probably a topic for yet
another talk so just trust me on that
and if we just had a big switch let's
say with 100,000
portes we could just connect all those
servers to that switch and we would be
done but there is no such thing nobody
invented anything like that yet so
that's why we have to invent these
wastes in these kind of topologies to
connect these servers together and so so
so far key points were there multiple
layers and then there's redundancy on
all but the lowest layer in this
topology and the final key point is that
in terms of hardware and software there
are different requirements on different
levels so hardware requirements you can
think in terms of number of ports or
speeds of those ports are different than
every level and then on in terms of
software you can think which protocols
these switches need to support or what's
the scale requirements how many routes
they need to be able to handle so that's
the third third key point important for
the rest of the talk
so what let's look how we built this
type of network in the past so we used
to build network by buying network
equipment from various vendors and so
these vendors all had a certain
different type of offerings for their
customers so they would bundle software
and hardware together and then they
would have the wide array of various
offerings and then customers would pick
basically what they needed based on the
demands of their networks what that
meant in practice as we were building
and expanding this network that our
network was very heterogeneous there was
lots of different devices from different
vendors and from different generations
of those devices as we kind of grew over
time so what that meant in practice from
operational standpoint was that all of
them had different ways to monitor how
they're working they would expose
different ways to configure them so some
would have command line interface some
would expose api's they would kind of
have or both
so so that was very different and
depending on what the vendor offered you
could customize some things or not what
was actually very common for all of the
vendors in their devices was the the
software updates apart from some
occasional bug fixes were mostly not
happening so especially if kind of we're
talking about major new features that
was not very common so what would
typically happen device would get
installed in a data center and through
their life cycle would kind of go on
pretty much unchanged again maybe
there's some bug fixes until they was
decommissioned that they're like at the
end of the lifecycle and then again from
operational standpoint we had teams of
engineers who would build scripts and
kind of try to kind of deal with all
these differences between different
devices trying to automate things and
kind of make it easier to operate over
time so I'm sure you can start guessing
that there were some problems with this
approach and let me just kind of cover a
couple of those as you can imagine kind
of for the global network like this it
was very very important for us to
automate as many things as possible and
that was pretty hard when you kind of
all these devices have different sort of
ways to interact with them also to build
really good automated tools what we
really needed was looking into internals
of these switches and their internal
States and different vendors would kind
of give us different levels of
visibility if you want so it wasn't
really usually wouldn't get everything
we needed the other problem we
encountered even when we did have bug
fixes or the other major upgrades to do
on software and the vendor had them
ready since we didn't do this very often
it took a long time so you know any
process you can imagine you do if you
don't do it regularly it just takes a
long time
it is error-prone when you have to do it
so the good story I'd like to share
about this case is that a few years ago
we decided we wanted to upgrade from
ipv4 to ipv6 I wasn't at Facebook at
that time so this is story I heard and
so what happened was that so we had to
work with all these vendors to make sure
that their devices had good ipv6 support
and what we discovered was that as we
were testing the that you know we would
find bugs and it was not surprising
because most of their customers were
still using ipv4 said this was not as
mature but the problem was that as we
would report a bug then we would have to
wait fairly long time to get fixes from
these vendors and then when we would get
them what will happen is that we would
find new bugs and and the reason was
that they would bundle things together
right so they would provide fixes to
other customers or they would add
features for other customers and they
would bundle everything together and
kind of give us a new image so we had to
repeat this cycle several times until we
were ready certain and we kind of
thought okay all of these devices have a
good ipv6 support but then there's
another part of the problem so we you
know we started up upgrading these
devices and if you remember that
topology picture so upgrading those
higher levels you know we could manage
better because you know there is
redundancy there but on that lowest
level there is no redundancy on the top
of the rack switches so what that meant
when we got a new image from a
manufacturer for those devices was there
we had to stop traffic on those racks
and we had to do it on every single rack
we had in every single data center and
you can imagine service owners who had
services running in those racks were not
super happy about network team
interrupting their traffic and there's
no good automated process to deal with
this so what happened was that we had to
start massive cross infrastructure
efforts there are lots of people lots of
their only job was just coordinating
this massive effort to make sure we can
kind of go through every single rack and
upgrade this this literally took years
and as you can imagine it's not
something we want to kind of do very
often so that that was the another
problem the the last problem I want to
highlight is as we were building this
network we were trying to stay ahead of
the projections of how much of this
capacity we need to provide in the
future and this was dry driving some
interesting discussions about well you
know maybe you know it would be easier
to support this if we try these new
protocols or it would be easier to do
this operational type of thing and
automate more things if we kind of had
better visibility into these internals
and without having vendors giving us
this support it was practically
impossible so let's look at our approach
and and things we decided to do to try
kind of do better and so we decided we
are gonna design our own hardware and
software for network switches how hard
can it be right so we kind of there some
design principles or kind of ideas
people had how to do this better for us
so we decided we will kind of separates
Hardware design software we want to make
sure that these two things are not
bundled as tightly as vendors do them so
we want to have them so to be able to
evolve independently so if we have a
better version of a software we can just
kind of deploy that without kind of
worrying about hardware on a hardware
side we also wanted to make sure that
the kind of our hardware is designed and
as much as possible in a kind of modular
fashion so because we can't predict
future workloads it's hard to tell what
type of hardware for for network we're
gonna need and so we were thinking if we
can design it so it's kind of made up of
these modules then we can can maybe
combine them in interesting ways we can
kind of
be able to go get ahead of these like
changing workloads and finally with lots
of things we do we decided to open
source so you've both hardware designs
and software key software components are
open source and they are part of our
Open Compute Project so let's let's look
a little bit about what's interesting
and how how these separate pieces were
designed so here's the way we come to
that point of the talk about how this
hardware and software is similar to
servers and services so we design
hardware to how because we already
design servers is a part of OCP project
they're already server designs that are
open sourced so is it decided to build
on that and so our switch hardware was
designed to resemble very much servers
and what that meant in practice is that
it had a kind of more memory and more
powerful CPUs compared to vendors
because they were kind of more similar
to to the server's themselves and the
reason why we wanted that is because on
the software side we wanted to be able
to run shared components things we
normally use on in a Facebook server
fleet to run on servers as well on the
switches as well and for example in
practice that means that our software
runs on a standard Santos image that we
use on all our servers but I think most
interesting actually and and the things
that was driving force was that is this
key principle of operations over
features because even though now we are
in this business of creating or
developing our own hardware and software
for switches we are operator first and
foremost and so the what we want is to
make sure that what we come up with can
be operated easily kind of consequence
of that is that often time
having like a really cool feature that
somebody wants to try but that would
make operations harder is like not gonna
happen so we sort of want to make sure
that even everytime the add new thing or
try new thing that we don't kind of
change that bar on operations so let's
let's look a little bit more how this
worked in practice so we started this
project I think around four years ago it
took around a year to develop the first
switch with a small software team of
software and hardware engineers and we
started deploying these switches in
production around two and a half years
ago we started first from that lowest
level in a network topology on the top
of the rack switch because those were
actually the easiest the requirements as
are there changes they're kind of
easiest on that bottom layer and today
big parts of our network on top of the
rack switches run these use these
switches we call them wedges for example
all the new data centers have them we
didn't replace the existing ones we kind
of wait till there's a time to
decommissioning and then replace them
and then also we decided a while back to
move to that kind of next layer in
network topology in a fabric switches so
we developed those and we call them back
back and we started deploying them in
production earlier this year so we're
pretty well this is a time frame of how
this happened so as I mentioned couple
of years ago started with wedge just
this top of the rack switch it
forty because it was for 40 gigs
networks and in data centers then we
moved down to the fabric switches and
one interesting thing going back to that
principle of modularization was this
this is actually designed to be kind of
a one box but inside its twelve veg
switches we never or we never used this
in production much because by that time
there was need to start working on
hundred gig data centers so we build
hundred gig versions of both of these
devices and that's what 100 and back
back and then also earlier this year we
released reg 100's that had some more
security features so let's look at the
some interesting thing about software
which is where I work so if on this
previous time time line and all these
different network switches interesting
thing is it all runs the same software
and and that was kind intentional so
what is that software so there are two
key forwarding processes that run on a
switch we call them agent and bgp agent
is a process that's responsible for
programming a special chip that does
hardware forwarding and you might ask
well and the BGP implements the very
common network protocol so you might ask
why why didn't we use this principle we
have of designing these switches as
servers and just let's say well wedge
100 has 32 ports so you can imagine if I
just put 32 network cards and then I
have a switch why do I need any special
other hardware well the reason is
wouldn't be fast enough so if you try to
do packet switching and CPU I think it
would be three orders of magnitude
slower maybe with some super
advanced optimizations you could get it
to two orders but it's still a
non-starter to the chip we are buying
for 100 Gig switches we are buying it
from Broadcom can do 1.6 terabits per
second so we can't really do that with
CPUs so that's what agent does agent
programs this ASIC and then ASIC does
the hardware forwarding other things
there are other things that and so these
key forwarding processes are what makes
a switch a switch but there are other
processes that that run on thisis as I
mentioned we wanted to make sure we can
take advantage of things we already had
in in Facebook's infrastructure so we
there are their processes the things
that normally runs on all the server's
now run on our switches so good examples
of this would be our logging demons or
configuration demons or demons that are
in charge of exporting metrics they just
exports which specific metrics from
switches and then that's all collected
by the rest of Facebook infrastructure
one other other I guess manifestation of
that principle to design the software
same as we design other things we are
already familiar with Facebook which is
building services is that both of these
forwarding processes are services and
their more specifically thrift services
the lots of or most or services that
Facebook internally used thrift to talk
with each other to do our PC and then
agent and BGP do the same thing they use
drift to communicate with other services
rest of infrastructure an agent is open
source you can kind of go and take a
look at that on github but if you take a
step back you kind of might notice that
well geez when I have a service that
needs to make RPC calls to some other
service in presumably some other
you know I need to have Network that's
working and now you're telling me that
network is a service as well and also
needs a drift and RPC to function so
there is some sort of circular
dependency maybe there or or a strange
loop you might say but not to its it
actually works even though it's a
strange thing
we don't need drift to bootstrap and
start network sir so that works fine it
get networked first and then you can
make your articles but this is not the
the kind of the only thing that's
interesting or or maybe these things are
the most interesting sting to me
but but we are actually now operating
network very similarly and how we
operate services so it's very common for
large-scale services to be upgraded
frequently and so that's what we do with
our agent and bgp for example agent gets
updated to a new version on the whole
fleet every two or four weeks to two to
four weeks and when we do that we again
employ the standard practices people do
when they upgrade their services in a
large fleet we do canary testing we find
the places in a network that are not
critical and deployed new version and
then we motivate and monitor and make
sure everything looks good and then we
pick small percentage or somewhere else
and repeat the process and then we pick
bigger percentage until the whole fleet
is upgraded so that's very unusual this
you know as I kind of explained in the
past this was never happening
I do think you might have noticed is
like well gee if you're upgrading this
so frequently how our service owner is
coping with that because now you're kind
of touching every top of the racks which
every few weeks well there's this neat
feature that the Broadcom the
manufacturer of the chip we are using
has which lets us say
the state of the switch in hardware and
you know if you are fast enough and
reboot into the new version of the key
forwarding agent we can then pick up
that state and kind of continue working
so it just needs to be relatively fast
but otherwise that there's no traffic
interruptions so that was really cool
and that lets us kind of do these things
and then finally as with every other
large-scale system you know failures are
common it's not a question if it's one
so we had to invest obviously in
addition to various test systems we have
into disaster recovery systems too so we
have to make sure if anything goes wrong
with these forwarding agents and they
stop working that we can recover really
quickly and then we can spend time
trying to find the root cause so what
did we learn from this process so we
learned we could move fast
right it was you know several of these
hardware iterations the software like
numerous iterations it opens up
possibilities for us to try experiment
new things obviously now we have
visibility into everything you know both
hardware state software states we can
build the good automation based on that
but we also learn this required certain
adjustments and everybody sort of
working in in this part of the
infrastructure which means network
engineers obviously software engineers
production engineers all of us have to
adjust to this new world where these are
not or not just switches and not just
servers but this kind of strange mix of
both for example network engineers used
to go and work with vendors to get fixes
or to request features now they work
with the internal team there are some
differences on how that works and you
know what works what doesn't we had to
share much more operational
responsibilities and learn from each
other but you know I think that's a good
thing it just kind of gives us all much
better perspective on the system
but also we learned that since now we
are upgrading that software fairly often
and we also run other shared demons or
processes that we depend on that are
owned by other teams there are different
risks right so we had to learn how to
manage that because we now have much
more complex dependencies and what's
interesting is then on these
dependencies we have the teams that
let's say provide these shared blogging
daemons they do upgrades on their own so
should they kind of when they do the
changes and they do the same process we
do they go canary and a subset and then
kind of gradually increase that
percentage so we don't control a
frequency or when they do that so they
also have to make sure that their
changes are now not affecting switches
and when they do for example canary
testing interesting things is that they
sort of start to need to become aware of
the network topology itself for example
that becomes interesting when let's say
somebody who owns the logging process
wants to test their changes on fabric
switches if you remember fabric switches
there's four of them and there is
redundancy on that level but if somebody
runs something on four of them that kind
of creates some problem right and all
four of them stop working then you know
that would be bad lots and lots of racks
that connect to those would lose their
traffic so they have to start
understanding oh and I'm carrying my
change I can only pick one out of those
four so I don't kind of interrupt
traffic they didn't have to worry about
that before so let's look a bit about
what we are contemplating doing next in
this space so so overall we think this
this worked really well for us and we
are constantly learning new things also
our fleet of top of the rack switches is
growing as we building new data
and now as we started deploying fabrics
which is same same is true for them but
they have different place in the
networks of different requirements we're
still learning from from that but based
on that and based on the needs of our
network as we expand is we are going to
decide on what other device in our
network to support so which one we want
to do next both hardware and software we
want to be able to use all of that
information we have to build even better
tools lots of stuff that we could do
particular interesting things around
automated remediations so when things go
wrong if we can automatically fix them
that's really important with the size of
this network now we finally have some
ability to experiment with new protocols
and new topologies so earlier we open
source a new protocol called open R and
we're we are very curious to start using
and experimenting with this in data
center it's currently used for other
purposes and we couldn't do this before
so open R has some advantages over BGP
hopefully we'll try bgp has been around
forever so so we want to kind of try you
know if you can do some something better
and we're hoping a couple of things that
might be better are speed of convergence
that's really important in networks
property things like a simplifying
policies again if you remember that huge
topology it's all like managed by
various policies so maybe we can
simplify that and also open arc can help
us try experimenting with different
topologies we're also planning not to
bring Facebook down because Facebook
needs its network so we're continuing
investing in making our disaster
recovery and testing systems better and
more sophisticated but there's lots of
interesting things that that we could
still do there and then we are committed
to continue sharing everything we find
out
both hardware and software with the
wider community via OCP so we like to
say at Facebook that we are just 1%
finished so you know I hope I kind of
Illustrated some interesting things and
we are always hiring so if at the end of
this talk I convinced you that this is
indeed very interesting area there's
lots of exciting stuff happening please
let us know but at least thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>