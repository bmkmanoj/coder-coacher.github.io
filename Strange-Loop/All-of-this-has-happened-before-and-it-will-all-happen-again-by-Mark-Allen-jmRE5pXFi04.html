<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;All of this has happened before, and it will all happen again.&quot; by Mark Allen | Coder Coacher - Coaching Coders</title><meta content="&quot;All of this has happened before, and it will all happen again.&quot; by Mark Allen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;All of this has happened before, and it will all happen again.&quot; by Mark Allen</b></h2><h5 class="post__date">2014-09-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jmRE5pXFi04" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm very excited to be here my name is
Mark Allen and this is my first strange
loop and thank you for coming to my talk
we're going to talk about some old
things today we're gonna talk about
history of programming languages and
some of the issues and problems that
early language designers sought to
overcome and whether or not they met
those design goals or not in a number of
different cases specifically we're gonna
look at four different languages today
Fortran which was released in 1957 and
is still around today although not
certainly not in the form that it was
released then Lisp which is also still
around we're gonna talk about algal and
COBOL and then I have a conclusions
which didn't show up on my slide for
some reason and I also have a very
extensive bibliography that that I
encourage you all to examine because
there's just a lot of great stuff in
there some of which were going to talk
about today and and some of which you'll
just have to explore on your own in any
case I am excited to be here and thank
you for for coming to the talk so in
1972 a computer scientist named Jean
Sammet who worked at IBM at the time
wrote a paper for the ACM that talked
about the history and future of
computing and this pose quote that I've
pulled is a really interesting one to me
because it was written a long time ago
and there was sort of this assumption I
think that maybe that computer science
would establish itself as a as a true
science in the sense that you know you
have experiments that you can repeat
that you essentially can use the
scientific method on like you can expose
hypotheses and you can test them and
develop theorems and things like that we
do have those to some extent right like
there are computing theorems that that
that are established sort of at the
bounds of physical reality and I think
Joe Armstrong talked about that really
well this morning and we also have other
theorems that you know we've been
associated with and the thing that comes
to mind immediately is is like Eric
Brewers cap fair
right which is a theorem but it's not
like a physical theorem it's you know
more of a software design constraint
essentially but what she wrote was
virtually everyone agrees that today
programming is an art and not a science
and many although somewhat fewer people
would contend that programming can and
should be made into a science and it
just made me think is programming today
more of an art or more of a science and
that's what I'd like for you to think
about as we walk through these slides
together also accompanying this article
in 1972 was this chart now I know it's
an eye chart and trust me it's an eye
chart even at a very high resolution but
this gives you an idea of just how many
languages that were being looked at in
her article that she wrote in 1972 and
this is really you can you can't read it
down here but this starts at 1951 and it
walks all the way over here to 1971 so
this is a span of 20 years and you guys
can see just how amazingly diverse and
also sort of convergent a lot of the
languages of those of that era were
which is really fascinating to me in a
lot of different ways I wanted to give
some context to this diagram by kind of
describing the state of computing art
before 1954 so the the first sort of
commercial computer that was available
was univac which was built by the Eckert
muchly company and they were eventually
acquired by Remington Rand and after
Remington ran they were acquired or
merged together with Sperry and they
became Sperry Rand and then eventually
they turned into unit unisys so they had
this very long evolution of company but
it's still around today and there was a
guy that worked there named William
Schmidt and he wrote this this
essentially this automatic compiling
feature called short code and what short
code did was it essentially allowed you
to describe an algorithm with without
using actual octal numbers that you
would have to insert physically into
program locations in memory and so it
was a great labor saving device it was
it
isn't exactly a higher-order language I
mean essentially you'd have a program
that was very compact and condensed but
it wasn't really human readable it was
still very machine oriented so we were
humans commuting communicating with
machines at the context of machine
language so there was the humans that
really had to adapt to how machines talk
to one another and the other thing that
was really amazing at this time in at
Remington Rand was Grace Hopper started
working with actually she she got hired
at the male company and when it was
acquired by ran she she went with it but
she wrote the first compiler which was
called a zero and it was in 1952 which
is pretty mind-blowing to me and the
thing that was even more mind-blowing
and this isn't excuse me this isn't in
the bibliography but she describes
really clearly in conference proceeding
called the history of programming
languages that was held in 1978 that
none of the management people at
Remington believed her they essentially
could not believe that they had this
program that could kind of take human
readable instructions somehow translate
them into machine code and then execute
them but that's what she did and that
was the program that did it this this
program a zero and its successors a a 2
and a 3 are gonna play a part later on
down the line when we started talking
about COBOL in 1952 she wrote a paper
that describes some of the things that
she was working on this paper was called
the education of a computer and she
wrote this quote which I which I quite
liked because it reminds me of my day to
day job and what she wrote was the
programmer who has been supplied with
code he translates into instructions to
the computer until the novelty of
inventing programs wears off and
degenerates into the dull labor of
writing and checking programs this duty
now looms as an imposition on the human
brain how true that is today so the more
things change the more they stay the
same that's that's also a recurrent
theme in this talk John Backus wrote a
paper later much later where he tried to
describe the context of computing in
1954 for for contemporaries that came
along after him and what he wrote was
that the goal of of
early languages to was to overcome
hardware limitations for the most part
so in those days computers were far more
primitive as I'm sure you can imagine
and they actually omitted a lot of
features that we take for granted today
some of them for example only had
circuits that could compute ants so they
didn't have ORS and so to overcome that
they had to implement ORS in software
and so they had kits essentially these
little kits that and that allowed you to
do binary ORS
I'm sorry logical ORS and had primitive
input/output operations like onto paper
tape or punch cards there was also a lot
of insufficient hardware registers so
some of the state-of-the-art computers
of the day had like five Hardware
registers and the other thing that's
really cool about it researching this
was sort of discovering how much
efficient work was actually accomplished
with these systems of the day and one of
the things that Hopper describes also
was using these systems that had a
thousand words of memory and a word is
sixteen bits to to do computation and
she talked about how they swap things
onto tape and pull them back off a tape
and so forth I was really really
interesting so we had all these
limitations and there there was this
this sense of automatic programming so
there was this term that was floating
around out there which was this idea
that before programming was this this
job where you as the programmer would
sit down with like a piece of graph
paper much like the program that Joe
showed in his keynote and he would
actually physically write out like okay
at memory addresses zero zero zero eight
I need to input this octal number and
then it you know zero zero zero eight
sixteen or sorry sixteen I need to input
this octal number and so forth and
that's actually how you would construct
your program so what these kids would
allow you to do is sort of provide
mnemonic operations so it was a little
bit more like assembler that we're
familiar with today
it had symbolic addresses right so you
could give things names and then you can
reference memory locations with names
instead of the actual physical location
there were library subroutines so like
let's say you need to compute a square
root or something and you didn't
actually want to figure out how to do
that
you could use a library routine to do
that for you and then there was
floating-point operations so if you had
math problems that required
floating-point precision
and then you were gonna have to you know
either deal with with floating point
operations yourself because there
weren't there were no floating point
processors at that time so you were
gonna have to do it in software and you
don't want to write that yourself
probably because it's really messy and
actually if you want a hard programming
challenge writing a floating point a
processor and assembler is probably a
pretty good one so if you're looking for
if you're looking for a task to take on
in the hallway that that might be a good
one to go for so our story evolves to
1954 and I just wanted to give some some
wider broader context of what was going
on in 1954 the President of the United
States at that time was Dwight
Eisenhower and the first children were
being given polio vaccines which had
just recently been invented almost a
year before that and also the USS
Nautilus which was the first atomic sub
was commissioned by the United States
Navy so that's kind of the context of of
what was going on in the 50s at least
for a few bullet points and I want to
talk about Fortran and some simply I
want to talk about the evolution of
Fortran and how it came to be so in 1953
there was a gentleman at IBM named dr.
Cuthbert Hurd and he's shown in this
picture with TJ Watson senior who was a
you know pretty big muckety-muck you
might have heard of him
and he wanted to build a general-purpose
computer for business and he wanted to
do it before univac before Rand could do
it so that IBM could own the patents and
and what he found was talking with
people that he worked with was that
constructing software for these for
these computers even though they were
general purpose machines could do all
kinds of computation was really
laborious and very difficult and so he
authorized a project run by John Backus
who were gonna get to in a second that
he called Fortran formula translator and
this was a really interesting piece of
pop culture Kitsch that came in the 70s
this this lower this lower picture is
actually from a board game
that was developed in the 70s called
Fortran and I just particularly like it
because it says input reason output
pleasure
that's the kind of board game I want to
play so let's talk about john backus
john backus was a really a pretty
interesting fellow he was a graduate
student at Columbia University he was
studying mathematics he happened to be
walking by an IBM showroom that was in
New York City and he had saw from the
from the window he saw a large automatic
calculator which was actually the size
of probably this stage and he came in
and asked questions about it and when
the when the person that was showing him
the machine learned that he was a
mathematician took him immediately took
him upstairs for a job interview
they asked him some brain teasers and
then they hired him on the spot to be a
programmer and he didn't even know what
programming is but as you can see in the
first bullet point he hated programming
and when I put programming in scare
quotes what I really mean by that is
this laborious process of you know
writing out this really longhand way of
employing values into memory locations
and then actually executing on top of
them so in 1953 he invented speed coding
and speed coding is sort of this
mnemonic device like short code that
we've already talked about it's sort of
an alternate implementation with the
same idea and then in 1954 Cuthbert Hurd
came to him and said hey we're gonna
build this new computer and we want you
to write a language for it that will
make programming easier for people to
address machine questions too
and so he led the Fortran team from 1954
to 57 and then he invented Backus normal
form which we're going to talk about
later on when we get to Algol and he
also won the National Medal of Science
in 1975 and he won the ACM Turing award
in 1977 and just as a priest discursion
here the evolution of John Backus his
thoughts about programming languages is
really fascinating in a future talk
maybe about just John Backus but he
evolved from designing a language called
Fortran which was used worldwide and is
still in use today and kind of moved to
a position where he felt like these
declarative programming structures were
really harming the advancement of
computing as a profession and as a
discipline and he advocated in his
Turing speech using something more akin
to functional programming
more algebraic denotations he was a big
fan of a programming language we're not
going to talk about today called APL but
it is really fascinating and if you're
interested in the history of languages
check it out it's pretty cool here's a
here's some pole quotes that I have from
his paper describing the history of
Fortran so again this is giving context
of what it was like in 1954 when he
started working on a project early
systems were slow and expensive and
through experienced programmers learned
or sorry programmers began to doubt
writing efficient programs could
actually be automated also some
marketing departments claim their
systems could have a human level lexical
understanding of programmer intention
now does anyone here think that a
computer program has human level lexical
understanding of your intention let the
record show there are no hands here's
another quote one was accustomed to
finding lots of peculiar but significant
restrictions in a system when it finally
arrived that had not been mentioned in
its original description hmm where have
I heard that before now here's the pole
quote just summarize this whole
experience it's difficult to convey the
strength of skeptic skepticism about
automatic programming in general and its
ability to produce efficient programs in
particular as they existed in 1954 all
right so that's where we're at on the
ground floor people were very skeptical
that you could actually take this
process of writing things out by hand
and automating it to the point where
what you had at the end of that process
was actually more efficient than what a
human could devise with their own brain
power so these were the design goals of
Fortran as stated in his paper first
they're going to virtually eliminate
coding and debugging we're going to make
programming reliable cheaper and faster
and it will also be an example for
others to copy all right fantastic I'm
in favor of this now how many of you
think that these design goals were met
no hands
you're all very smart but but this is
really interesting actually they worked
on this project for a long time much
longer than back is actually originally
estimated in fact he went out on
pre-sales calls to customers very large
customers in the United States and whole
them that within six months six months
they're going to have a working system
available for them to start using and
this was in 1954 now when the system was
actually delivered in 1957 this is what
he had to say about it it will suffice
to say that the compiler produced code
of such efficiency that its output would
startle programmers who studied it the
degree of optimization performed was not
equalled again until optimizing
compilers began to appear in the middle
and late 1960s so that's a pretty
impressive achievement now the things I
wanted to point out to you here what we
didn't meet any of the design goals as
they were stated but but we did meet one
design goal and actually maybe even two
design goals these are meta goals one is
we prove that you could take a system
that could have sort of interaction with
a machine at a human level and produce
efficient programs from it programs that
would run faster than and then if a
human sat down in hand-wrote code and
that is pretty much still true today in
many cases not always true but in many
cases it is true and he also discovered
that it was it it was easier for people
to understand how to interact with
computers from Fortran than previously
before because before it was really a
very exclusive group of people that
could understand even how to program a
computer much less make it do anything
useful so Fortran really broadened the
appeal and the ability of people to
interact with a computer in a way that
was a little more natural to them than
the numbers and mnemonic codes alright
so this is another poll quote that I
really liked it was an exciting period
we were often astonished at the amazing
transformations which made the program
efficient but which we would have not
thought to make as programmers ourselves
so here we have a case of a system that
was built to be efficient and is
actually efficient in surprising ways
and I think with that many of us in this
room have experienced that effect on our
own programs so what are some of the
flaws that we had well specifically
things that the Backus writes about as
in the paper were debugging problems one
of the things that he wrote about was
this was a poll quote not on the slide
we were hopelessly optimistic about the
problems of debugging Fortran and hence
syntactic air checking and Fazil
in the first distribution were weak and
the other thing that was really
interesting to me in studying this was
the design choices that they made as a
language implementer they didn't feel
that building a language as a discipline
would be difficult the actual disk the
the difficult problem that they wanted
to solve was building this compiler that
was going to be super efficient and
overcome all the skepticism of their
peers but what he wrote later on about
20 years later was we knew nothing of
many important issues that worked that
turned out to be important these
included block structure conditional
expressions and type declarations so all
of those things were missing from the
first version of Fortran and even though
they were missing people and companies
and customers were using this software
language to build really important
applications for their business so I
think that's a pretty interesting thing
too
one other thing which I've touched on
before but which he wrote about
explicitly was estimation problems I
mentioned the time scale that he got
that wrong and there were other problems
that are on the slide that that I'm not
gonna go into necessarily but Fortran
was the first the first sort of the the
pioneering I would say a computer
language that still lives on with us
today although if you look at the the
historical record you'll see that the
early programs bear no resemblance in
any way to to modern programming that's
in the Fortran family so next I want to
talk about Lisp how many of us people
are in the audience today cool well
we're gonna talk about lists quite a bit
it's really fascinating topic but first
before that I want to do a biographical
sketch of John McCarthy many of you may
be familiar with with John McCarthy
already but if you're not let's do let's
do this brief sketch he essentially an
invented Lisp in 1959 but you may not
also be aware that he worked on algal he
was one of the original members of the
Algol Committee in 1958 he worked on it
through 1963 one of his contributions
that you may be familiar with is
if-then-else he also invented the term
and technique of garbage collection
we're gonna get into that a little bit
later like how it came to be and why he
invented it he also coined the term
artificial intelligence he predicted
computing utilities in 1961
where you pay by the hour which also
sounds really familiar to modern people
he won the ACM Turing award in 1971 and
the National Medal of Science in 1990
and this is what he had to say about a
paper he wrote in 1981 about Lisp which
I particularly like because I think it
captures McCarthy's attitude about his
language pretty succinctly
he says list seems to be the second
oldest surviving programming language
after Fortran so maybe we should plan on
holding one of these newspaper
interviews in which Grandpa is asked to
it what he attributes to having lived to
a hundred that sounds like McCarthy to
me I love that quote that's from a paper
in debt he wrote in 1981 so here are the
design goals that McCarthy laid out in a
paper that he wrote in 1978 what were
the design goals of lisper explicit what
was it what problems were was he trying
to solve when he came up with Lisp and
it may is an interesting historical note
that there was actually a predecessor
system to Lisp that was implemented in
Fortran and it was called Fortran Lisp
processing and it was actually not
exactly what McCarthy was looking for it
built on some of the ideas that you
could have all these lists out there and
that those were really useful data
structures to do operations on and do
computations on but they weren't exactly
what he wanted he wanted a succinct
algebraic style of expression so first
of all he wanted to enable symbolic
reasoning instead of numerical
computation that's really what Fortran
was focused on as it as a programming
language we want to count you know
compute all these things math problems
or computing proofs of math problems you
know computing vectors for various
engineering problems those sorts of
things the representations of data and
expressions is lists in memory that's
one of the things he wanted to do was be
able to prove out that a computer
language could use these data structures
successfully as a programming language
he wanted to use functional composition
and recursion as programming idioms so
instead of writing a declarative style
of where you have explicit loops we were
gonna have recursion we're gonna have
functional composition where you build
up small functions that do one thing and
then you're going to combine them
together in interesting ways and do
larger things and then finally if you
wanted automatic
memory management which was actually a
requirement of list because of the way
that that things happened so here's some
things that that arose from those things
first he introduced this idea of lambda
into Lisp and this was an idea that he
needed so that he could give a function
and name and pass it around as a as an
as a proper argument to other functions
he writes in another paper which I
didn't have a pole quote from but
essentially what he said was I just
needed to to have a convenient way of
naming something and it wasn't
necessarily that he liked lambda
calculus per se but that he liked the
this idea that you could use this this
this mathematical term and name
something and then pass it around so he
developed garbage collection to deal
with the erasure problem and and in
prints I had this idea if there's two
operations that you may be familiar with
there's car and cutter which basically
pulled ahead of the list and the tail of
the list respectively
so when McCarthy came up with these
functions he had this problem which is
okay well what do I do with the rest of
the list I don't need it anymore but
still sitting around in memory and if
I'm gonna compose those things into a
new list then I'm gonna have to reclaim
memory from somewhere somehow someway
and so that's how this idea of garbage
collection came to be in existence was I
have all this detritus sitting around
and I need to reclaim the memory space
that it's occupying right now so I'm
gonna go ahead and come up with this
technique that I've invented called
garbage collection to solve that problem
now McCarthy had a student named Steve
Russell and he read through McCarthy's
paper called the the symbolic
computation it has a very long title in
s-expressions
and it's called part 1 part 2 was never
published just in case you ever go
looking for it but anyway Steve Russell
hand coded this eval function which is
defined in that paper and he wrote in a
machine code for this IBM 704 which is
the same computer that John Backus was
working on and that served as the first
interpreter for the language it may
interest you all to know that that
McCarthy intended to have a compiler and
we're gonna get into that and a couple
of slides too but but here's something
that came out from the eval
that implementing eval actually arose a
new interesting thing
he writes that writing eval required
inventing notation represent
representing list functions as list data
with no thought no thought that it would
be used to express this programs in
practice now to me that was a really
shocking thing to learn this idea that
the that you have this data as program
is sort of a spontaneous thing that
occurred not by design but actually just
by happenstance is is really interesting
to me as an amateur historian so here's
some things that that that McCarthy
noted about the flaws of Lisp he first
said that interpreted programs ran about
60 times slower than compiled programs
in the first release of lists there was
no lexical scope for variable bindings
that came later and that he planned that
the awkwardness and here he means the
number of parentheses specifically that
the awkwardness of the S expression
would be solved by the use of M
expressions do any of you have you heard
of this before in expressions a few of
you okay so there was this idea that
that they were going to actually write
lists programs in some declarative style
like Algol without parentheses and that
there would be a machine translator that
would essentially take those things and
turn them into actual list programs that
would then be interpreted and executed
by the computer but as he writes in the
paper that receded into the indefinite
future so that never happened and the
parentheses stuck around now I mentioned
before that they had this idea of
building a compiler this is what
McCarthy had to say about that the
Fortran people were expressing their
shock at having spent 30 man years on a
compiler and it didn't seem like we
could get these graduate students to
hold on hold still long enough that we
could get 30 man years of work out of
them so essentially the reason that Lisp
never really had a compiler until much
later was that he didn't feel like he
could get his grad students to write one
for him so this is another lovely pull
quote that I have this is from a
phenomenal paper that I heartily
recommend to you in apart with what Jo
talked about this morning this comes
from a Dykstra paper called the humble
programmer the humble programmer was
written in 1972
and it was Dykstra's discursion on how
we need to eschew complexity and embrace
simplicity and what he writes about Lisp
is that it's jokingly been described as
the most intelligent way to misuse a
computer I think that description is a
great compliment because it transmits
the full flavor of liberation it has
assisted a number of our most gifted
fellow humans in thinking previously
unpossible unpossible impossible
thoughts let me commend that paper to
you again it is fantastic that is very
short it's about 12 pages long and it's
also available freely and it's in the
bibliography so if there's one thing you
come away from this talk with highly
recommend reading through that paper so
that's a list and now we're gonna move
on to algal our people here familiar
with algal in passing like you know you
recognize the name okay interesting
so algal was was started in 1958 there
was this specification that was mostly
on paper as far as I know it was I think
it may have been implemented into
compilers in a few places and in 1960
they kind of met together again and
decided to do a more complete and full
specification due to ambiguities and
misinterpretations in the original
document so there was an international
team of people let me back up a little
bit further in in the early 50s a team
of European specifically kind of got
together and at the time computing in
Europe was even more primitive than it
was in the United States and when when
Fortran was released there was a I
wouldn't say a fear but there was an
idea running through the currents of
Europe that that there needed to be some
kind of European computing language and
that they didn't want to necessarily
have you know 50 different ones of those
they wanted to have a single universal
computing language and what they did is
they reached out to a group of Americans
through the Association for Computing
Machinery to come together discuss these
ideas and then come up with a language
specification and that's how algal
actually came to be that's that's the
the origin of the of the language itself
now it's gone through three specific
iterations
there was the 58 paper there's the 1960
paper which is what we're going to talk
about today and there was a later
vision in 1968 that is is a interesting
historical artifact around human
interaction with other humans there were
a lot of political backbiting and
misgivings about a level 68 and if
you're interested in that I commend you
to the Wikipedia page which covers it in
in in Nice brevity so here are some
design goals of a level and they wanted
as I mentioned they wanted to create a
language as close as possible to
standard mathematical notation and be
readable with little further explanation
they want to use this language for the
description of computing processes and
publications and they wanted to advance
a universal agreed-upon language without
worrying about specific implementation
concerns so at the time there were all
these computer manufacturers they all
had different word sizes they all had
different memory types they all had
different ways of storing things and
retrieving things they all had different
instruction sets so it's basically a
giant hot mess what they wanted to do
was come together and say okay without
regards to any specific implementation
we want to go ahead and and build this
computer language sort of in the
abstract and then later on we'll figure
out how we're actually going to
implement it on machine a and machine B
and machine C and so now I'm going to
talk about some of the people behind
algal quickly the first one from the
American side is a man named Alan Perlis
he was the original member of the
delegation in 58 along with people like
John Backus and John McCarthy who we
already talked about perlis is a pioneer
in promoting computer science as a
discipline distinct from mathematics so
one common thread that I found
researching a lot of these individuals
is almost all of them were trained as
formal mathematicians he was president
of ACM for a while he won the Turing
award in 66 for compiler design most of
which can be drawn directly from Algol
he's also really famous for writing a
paper in 1982 called epigrams on
software programming which i've been
widely quoted and I now have a couple of
pull quotes free because I quite liked
them the first one he wrote is there are
two ways to write an air-free program
only the third one works here's another
one I like some program languages
managed to absorb change but withstand
progress and there's about 200 more like
that so if
if you want to read the full deck then
you can go to the bibliography and pull
it
there's also Peter nor you may know him
from Bacchus noir form which we're going
to talk about because it was invented
for Algol specifically he was the editor
of algo bulletin in Europe they didn't
have the ACM so they invented this this
group of people that subscribe to this
newsletter called the algo Bolton Peter
noir was the editor of that for a long
time I already mentioned back it's
normal form
he wrote one of the first aval compilers
in Denmark and he won the Turing award
in 2005 we had talked about Dijkstra too
but just just to sketch a migrant a
biography of him he's probably best
known for his graph traversal algorithm
he also wrote a really foundational
paper and concurrent programming
invented the mutex he wrote a letter to
the editor called go-to considered
harmful
which as a meme sort of lives on to this
day with X considered harmful he also
wrote the first implementation of an
algal compiler in the Netherlands in
1960 he won the ACM Turing award in 1972
already talked about his paper for that
he's a he's a giant in the field of
computer science and a really
fascinating person to read his artifacts
and yeah it's fantastic and interesting
so here's some first that Algol
introduced into computing science and
programming languages in general inanity
introduced recursion as a practical way
of doing programming it introduced code
blocks with lexically scoped variables
that was an innovation they had
call-by-value and this thing called
call-by-name
which is not very well understood at
least not by me
call-by-name is a really early form of
lazy evaluation so the idea was is that
you could define in your in your in your
procedure you could define a variable
that would only be evaluated when it was
called in the procedure and not previous
to that so you could delay computation
of that value until you actually needed
to consume it
that sounds pretty familiar to to most
modern ears there's a first
Algol also had the first intrinsic
boolean type it introduced a lot of
keywords that you would be fine familiar
things like if-then-else on to for loops
while loops a whole bunch of stuff like
that and it also had the first formally
specified grammar the grammar was
introduced at a conference
at UNESCO in 1959 by John Backus alone
and then later
Peter norc sort of clarified it and
refined it in the algo report in 1960
this is what Alan Perlis wrote about the
report that they came out with with
normal form he said that this language
proved to be an object of stunning
beauty and neither Bacchus nor any other
adequate formal semantic definition
technique was available to the Algol 60
designers we now know that semantics was
considerably more difficult to treat
than syntax so what perlis is talking
about here was so Backus came up with
this idea that we can formally define a
grammar right we can say that this
keyword needs to occur in this position
and then it can be followed by these
legal values and that has you know some
kind of meaning what he wanted to do
actually was define formally the grammar
and the semantics and what we found is
that that's really hard in fact there
may not be a way to do that and so this
was an idea that at the time was
completely new to people and they were
trying to grapple with it
Algol was never very popular in the
United States and one of its stated goal
or unstated goals but a hope of many of
the Europeans especially was that it
would replace Fortran and that obviously
is not has failed the semantic meaning
of formal grammar was open to
interpretation
yeah that sounds familiar so it's like
it's like how people interpret HTTP rfcs
you can think of it that way that has a
formal grammar but loosely defined
semantics and it opened new avenues of
inquiry in computer science and I have
in the bibliography a paper that was
written in 1961 that pretty much blew my
mind
the paper in short describes a
syntactically directed compiler so we
take this formal grammar that's in B and
F and you feed it into the compiler or
into this parser and it actually builds
a compiler for you based on what the
grammar says and this was an idea that
came from Edgar irons in 1961 and was a
completely new innovation that no one
had ever thought of before and to know
today that that that sounds like a
pretty modern idea fascinating paper
also very short 25 lines of Algol
actually he describes how this
work so good paper to look at next I
want to turn to COBOL this is the last
language we're going to talk about today
and I know I'm running short on time so
I'm have to go pretty fast through this
COBOL design goals were basically driven
around this idea that we want a single
programming language that worked across
multiple machines that used maximum
English instead of mathematical notation
yes they actually had an argument about
whether you should have a plus symbol
and the word add or subtract or a minus
symbol apparently people felt like you
know people that weren't programmers
would be confused by by these simple
arithmetical operators it needed to be
easy to use even if that meant the
language would be less expressive and
another goal broaden the base of who can
speak problems to computers and we
talked about a couple of people that I
already touched on one is grace Murray
hopper maybe some of you are familiar
with with her she's she is also an
amazing person in computer science
history and and worthy of further study
and and and talks and so forth she got a
PhD from mathematics in Yale she retired
from the Navy as a Rear Admiral in 86
she famously visualized in a second it's
in the bibliography it's on YouTube I
highly encourage you to watch it it's
two minutes long she popularized the
term debugging when she actually found a
physical moth in a computer in 1946 led
automatic programming team at Remington
Rand we've already talked about that and
she also developed this language called
flow-matic which was a major source of
inspiration for COBOL the next person I
want to talk about is another lady named
Jean Sammet she got an MA in math from
Illinois in 1949 she worked at Sperry
Rand that I mentioned before with grace
Murray hopper on the UNIVAC one and she
started working on COBOL force Ivania
Electric in 1958 261
she joined 19 eidm and 61 and developed
for Mac and is also a major major reason
why a lot of these historical computing
artifacts exist today she was very
interested in the history of computer
programming languages and did a lot with
the ACM especially to ensure that those
things would live on so here's what she
wrote about COBOL it was quite clear
quite clear that the disputes between
people from the same organization were
often as great and sometimes greater
than between people from different
organizations and this
virtually to all organizations involved
whether manufacturers or users that
sounds pretty familiar to modern ears
here's another one
there was a strong IBM anti IBM bias in
this committee from me and some but
certainly not all of the others since I
was not working for IBM I can freely
though not with pride to admit that in
some cases suggestions or decisions were
made on the basis of doing something
differently than IBM did it that also
sounds familiar to modern ears here's
what she writes about the experience of
having COBOL experience with real users
and real compilers is desperately needed
before freezing a set of language
specifications have we known from the
beginning that COBOL would have such a
long life I think many of us would have
refused to operate under the timescale
that was handed to us now it's important
to know and I didn't mention this before
but they had six months to work on a
language specification and they had to
develop their entire specification as a
stopgap because there were a different
there was a consortium of eight
different computer manufacturers and
they all des threatened to basically go
off and do their own thing now they were
kind of kept in line by this short term
report that issued in six months and the
fact that the US government enforced the
COBOL had to work on their systems so
here's some things that COBOL is
introduced it introduced this notion of
typed records or data definitions it was
at one point the most widely used
programming language it's still in use
in many companies today probably to
their chagrin it met its design goal of
implementation in penance and this was
largely due to the influence of
government contracts so that's pretty
interesting here's a couple of
conclusions that I will leave you with
and I'm gonna have just a brief time for
questions sorry about that so first of
all many early practitioners expected
problems related to programmer air and
efficiencies would be solved by
languages but we now know that that's
not necessarily the case indeed it's
really a human problem not a
technological problem even when it was
explicit goal to reduce what we might
term cognitive load it proved difficult
to achieve satisfactory results in
practice and I think that's also still
true today and I would strongly
recommend again browsing through this
bibliography and reading one or two of
these papers they're really fascinating
and well worth your time even as I'm
very modern audience that's my slides
today thank you very much for your
attention
I appreciate your ear tenants today
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>