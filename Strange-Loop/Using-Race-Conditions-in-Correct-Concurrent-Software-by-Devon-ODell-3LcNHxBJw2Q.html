<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Using Race Conditions in Correct Concurrent Software&quot; by Devon O'Dell | Coder Coacher - Coaching Coders</title><meta content="&quot;Using Race Conditions in Correct Concurrent Software&quot; by Devon O'Dell - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Using Race Conditions in Correct Concurrent Software&quot; by Devon O'Dell</b></h2><h5 class="post__date">2015-09-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3LcNHxBJw2Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Devin Adele I'm a software
engineer at fastly I work on our core
caching infrastructure we're based on
top of varnish so that's sort of my
platform every day personally I'm a
really big fan of performance work and
debugging work debugging mostly because
when you do performance work you have to
do a lot of debugging work so it's just
easier to be a fan of it because
otherwise your life so miserable i'm
also a pretty big Frank Zappa fan and
don't know if I'm a bigger Frank Zappa
fan or performance fan but I'm clearly a
little bit bigger than I was in that
picture anyway a lot of the time when
people add fastly give talks they're
talking about sort of our distributed
system the challenges coming with
implementing a you know large global
network distributed systems problems but
I'm going to focus at something at a
smaller level so something that happens
in our points of presence our pops and
in fact even something that happens on
an individual machine level so as a
content delivery network what we do is
effectively caching content for our
customers and delivering it to our
customers users and doing that at a
massive scale and there's there's
problems with being able to do that at a
systems language level like see one of
these problems is as many of you
probably know sharing memory is really
hard to do and especially if you need to
do that across process concurrently so
on a cache node we may have many
processes and these processes are sort
of able to allocate and free memory from
their own private heaps and because
there's no real general-purpose way for
us to share these any data between these
heaps concurrently safely or really at
all because their private heaps these
processes can't really communicate at
least not through memory directly and it
would be really great if we had some
kind of system to do that right because
then we could start off sort of problems
with things like hash eviction lists or
indexes of objects on SSDs or
or that kind of thing that that could
help processes help each other complete
tasks so we implemented a concurrent
allocator it's called micro slab and it
allows us to basically do just this it's
a shared state memory allocator that
works concurrently between processes and
the talk today is sort of going to be an
exploration about sort of how you get
from the idea of doing this to making
something that performs predictably
under a high concurrent load so first
off as you might have guessed by the
name micro slab this is a slab allocator
a slab allocator is a class of memory
allocator that hands out fixed sized
objects right so we don't have the same
problems with slab allocators with
fragmentation because it's really easy
we can just sort of put objects back in
and get them out because they're all
fixed size but what's a slab well the
slab is just some contiguous section of
memory that's broken up into a bunch of
these fixed sized objects and if we want
to allocate from this lab all we have to
do is just sort of you know what we want
to do is call our allocation routine
oops and apparently that's not going to
work but we've pulled objects off the
front if we want to free them we're
going to free them back they're also
going to the front this looks like a
stack it is a stack it's also called a
free list but we're sort of looking at
this from a single process perspective
so what happens if we add concurrency
into the mix and and really the question
that we need to answer is how do we
actually reason about the correctness of
our allocator in the face of concurrency
well something that's really useful to
do is to sort of define a protocol so
even though we're not really like doing
sort of defining message passing
semantics or something we can think of
our alligator as a sort of protocol
where we have requests for objects that
receive responses containing objects we
also have requests to free objects back
onto the alligator and those free
requests are responded to when the free
completes and i think we probably should
all intuitively agree that a request
comes before a response but
I'm going to go ahead and make that
totally clear because that's important
that that's really a constraint right
and we have other constraints that we
can define on our allocation protocol so
for instance we can't perform an
allocation and have a subsequent
allocation return the same object
without having an interstitial free and
a similar pattern a constraint holds for
freeing objects so how do we describe
this well we describe this with
execution histories and in this
particular case we have processes a and
B issuing allocation requests and they
both receive responses these requests
and responses are interleaved but they
do have an order so the execution
history allows us to sort of think about
the particular ways that we can sort of
order the operations of our of our
software and how it's going to execute
now it should also be well maybe not
obvious but we can also permute these
execution histories so we could reorder
these execution execution lines in in
four factorial ways so we get 24
different particular execution histories
but not all of them are valid right so
for instance if we reorder so that
begets a response before it sent its
request that's breaks causality and
that's mind-bending like we can't do
that so so we say that this has violated
our protocol so let's put it back so
these constraints that we've declared on
our protocol really help us better
reason about the correctness of it and
and it seems like it's providing us a
way to sort of talk about an execution
history that that can help us become
convinced that our protocol is correct
or that an implementation of it is
correct so what other constraints do we
need to put on this particular execution
history to sort of get some some better
feelings of correctness well the first
thing that we can do is we can constrain
the type of history that we're talking
about so an execution history
reordering of you know arbitrary
elements and arbitrary orders but we can
we can talk about sequential histories
as well in a sequential history is a
history where any request from our
protocol is succeeded immediately by a
response so this is an atomic
effectively operation requests and
responses then become atomic within
these execution histories and that seems
to be like kind of a property that we
were missing from our execution history
and if we want concurrent access to our
allocator seems like Adam icity is
something that we need so what does this
look like well here we have requests and
responses and and their atomic which is
maybe easier to see if they're grouped
like this the previous execution history
obviously was not a sequential history
because the requests and responses were
interleaved and what this means is that
if we introduce sequential consistency
into our allocator we now have a
constraint where no allocation can
happen with at the same time as another
allocation no free can happen at the
same time as another allocation so we
have this sort of ordering it seems of
events but notice that while we can't
enter leave our requests and responses
that all of these operations have to
complete atomically we can still reorder
them all with respect to themselves okay
and reordering request and response
pairs can still yield a valid sequential
history and so that I am talking about
reordering because what we're actually
allowed to reorder within history's
about to become really important so just
keep keep this reordering in mind but
first let's kind of look at some code
like what what is what is a stack
effectively look like what is our free
list going to look like well we're going
to have an allocate and free interface
the allocation for a slab allocator is
just going to read the head of the stack
it's going to read the next element off
of the head and it's going to put that
next element as the new head of the
stack simple freeing we have an object
we just read the stack head that becomes
objects next element we make the stack
head the object that we're free so what
do we have to add to this to get
sequential consistency well we can just
lock right we can just put walks around
in and right that's great so lock base
synchronization is is nearly like the
canonical sort of a solution to shared
memory or sharing memory in
multiprocessor and multi process systems
and they're easy to reason about right
because they serialize access to the
critical sections right the part in
between the lock and unlock and this
serialization creates a sequential
behavior so that's cool let's let's take
a look actually at how a lock itself is
implemented right because it's useful to
understand the guarantees that the lock
makes and how it makes them to really
understand why the lock protects our
structure and the kind of lock that
we're going to be talking about just
basically for simplicity purposes as a
spin lock and in particular we're going
to talk about a test and set spin lock
so the way that this lock works is that
it snapshots the state of the of the
mutex of the lock resource it sets the
state to locked and then it says what
was the state of the snapshot when I
read it if the state was locked we
didn't acquire the lock because it was
already locked right so we just spin and
we keep doing that as long as we have
seen that we didn't change the state
from unlocked to locked if the previous
state was unlocked we have acquired the
lock right so in that case we just exit
now in the case of unlocking that's
pretty simple all we have to do set the
state to unlocked we're good so this is
how an implementation might look like in
the C language this code is actually
sort of simplified from concurrent the
concurrency kit library which provides a
number of really great interfaces to
concurrent data structures spin locks
and other things and this looks correct
ray like this this basically looks like
the sort of flow chart that we just
looked at but how can we reason or
define the correctness of this
implementation formally well because an
atomic operation
by definition is individual indivisible
any atomic operations to our mutex to
this M variable right here are
sequentially consistent among themselves
so there's a couple things to notice
about this one is this sort of test in
set operation is it actually embedded in
a larger sequence of histories which is
actually entering the lock function and
exiting the lock function or trying to
acquire the lock and acquiring the lock
and in the case of multiple processes
let's imagine that that process be
previously held the lock right and
process a is trying to to acquire it
well we we have a stronger constraint
actually than just sequential history
that we have to apply to this because
sequential histories would allow us to
reorder these operations these atomic
lock release or the store request
response in process be an atomic lock
acquire in process a we can't do that
because that violates the guarantee of
mutual exclusion the lock is giving us
in the first place so if we've concluded
that out of the set of all execution
histories we have some subset of
sequentially consistent execution
histories it looks like we're even
moving towards a stricter subset of
those that satisfies some other
desirable property and that property is
linearize ability so linearize abul
history is a sequential history but it
has an additional constraint on it that
requested response pairs cannot be
reordered with respect to each other and
linearize ability was actually defined
by the Herlihy twing paper that we saw a
few slides back so what about linearize
ability is actually better so this Tia
Welch paper gives us a couple of
properties in particular linearize Abal
linearize ability is easier to verify
formally and this is because linearize
ability has two other properties number
one it applies to individual objects so
this
different from serializability and
database systems we're sort of the the
entire set of multiple operations on
multiple objects is gets the sort of
cereal treatment whereas with linearize
ability individual operations on single
objects get the sort of cereal treatment
this property of applying operations to
individual objects also makes linearize
about algorithms or linearize abell data
structures composable so if i want to
apply a linearized vil operation to some
object all of those lab operations
applied to the objects linearized as
well so given that sort of definition we
might think that there's only one
particular execution history here that's
valid which is maybe the one presented
but notice actually that we can sort of
still reorder the entry and exit of
these functions without breaking or
protocol and in fact we can even reorder
it a little bit more right so as long as
the unlock happens before the lock this
is linearized vault now it seems that
there's something special about these
these atomic instructions why why did
they provide this linearize abul this
linearized bill guarantee well in a
paper that predates me from Leslie
Lamport he's talking about in queuing
and dq'ing elements from a concurrent
list or concurrent queue and what he's
telling us is it's actually sufficient
for there to be an atomic moment in in
lock and unlock operations we're sort of
this transaction appears to complete and
and this is the paper that's sort of
credited for coining the toy the term
linear risible by the way but this
atomic moment in our functions is also
called the linearization point and prior
to this paper there was sort of an
insistence that sort of the entire
function had to be atomic so this this
gives us a new way of thinking about
linearize ability
and how we can apply it to objects so in
our functions that use this lock based
synchronization plan these these
linearize abul moments are anywhere
within the locks critical section and
with respect to our locking interface it
also has linearization points which are
the atomic operations changing the state
of the mutex so now we have a formal
basis for understanding our lock and
understanding its correctness how does
this perform right at the beginning I
said you know we sort of want an
alligator to perform well and that makes
sense because you know performance
matters but what do I actually mean by
performance because that's also a really
nebulous term right so what I mean is
that we get some decent throughput but
we also get predictable throughput with
concurrent load and that's really hard
to do so does our locking strategy
provide that well with so this graph is
showing the number average number of
acquisitions per second well not
acquisitions but lock allocate free
unlock operations per second across you
know sort of threads on the x-axis and
and the test was run for 10 seconds so
although with no contention we sort of
complete and something like 18
nanoseconds as soon as we add one thread
contending and you could also think of
this as processes if you want our
throughput immediately drops by an order
of magnitude like a factor of 20 25
something like that and it continues to
degrade as we add more threads which is
maybe easier to see on this logarithmic
plot so we're crossing nearly four
orders of magnitude of throughput
degradation as we add contending threads
and this is going to actually put a
limit on how our allocator will behave
because these operations rap sort of the
the stack mutation so recall that this
test and set algorithm the way that it
works is it reads the state and then it
writes a state
and then it tests a state effectively
well why do we write every time right
this seems kind of wasteful why are we
going to go and write some value back to
memory every time through the loop if
the locks already unlocked why are we
wasting these rights and sending them
back to memory well it smells like
something we can improve on and sure we
can so let's introduce this sort of
inner loop and instead of every time we
go through our lock acquisition trying
to set it to lock every time through we
have an inner loop that if it was locked
we just wait for it to be locked so
we're not doing any right anymore right
we're just reading this memory now but
at first blush this this kind of feels
incorrect right because what if we've
got you know a thousand threads or
processes in here and aren't they all
going to wake up and see that the mutex
is unlocked and then race sure they are
and and this is a race condition right a
race condition simply said is a sort of
a system behavior where the output wear
behavior of the system is dependent on
sort of the sequence or timing of
uncontrollable events like scheduling
maybe so can they be safe yeah we
linearize at this atomic test and set
that provides us a guaranteed global
order of operations on this mutex so
race conditions really only become bugs
when they happen out of the order that
we intended with respect to our protocol
and thinking about race conditions in
this way can maybe help us get some
additional throughput out of our lock
and later we'll see how it can also help
us get even even more predictable
throughput because if we actually look
at the performance it's a little better
but it's not that much better and more
importantly it's still not predictable
so let's try another trick let's add
some some local state right so if we
implement some exponential back-off now
we're not sharing any memory but we
might think this is bad for another
reason right maybe we
going to oversleep right maybe the lock
is unlocked halfway through our back off
and then we're going to sleep for too
long right is that bad is that going to
happen well no actually this turns out
to be a pretty decent way to get
additional throughput out of our spin
lock however we still have this like
kink in the graph it basically x equals
2 right and that's depressing we could
probably keep trying like every trick in
the book on our locks but really what it
looks like is we're just changing a
coefficient in exponential decay right
and in order to understand why let's
we're going to talk about something
called a progress guarantee so let's say
we've got two processes and some spin
lock that's available to both so we have
the first process that enters the
function it switches the state to locked
and then it exits the function so now
it's within its critical section
somewhere maybe so now our next thread
or next process comes in it tries to
grab the lock but it can so it's
spinning and it's not making progress so
there is actually no progress guarantee
being made in this system because the
first process is not under any
obligation ever to release the lock
there's nothing about a locking protocol
that defines when an unlock must happen
so furthermore our second process has no
choice but to wait until the unlock
happens before it can make progress so
in our total system we have no progress
being made so that's bad so maybe we
should have look into something that
that gives us system progress so we can
talk about one thing which is lock
freedom and that defines a system where
given contention on some resource at
least one thread or process will make
progress at any particular contended
moment so you you get this sort of
system-wide throughput guarantee
so maybe maybe we want to use that
solution and maybe we want to embed that
into our data structure instead of just
you know simply using locks around these
sort of sections that clearly need
protection to prevent you know
concurrent access to this from from
becoming a race condition that does
happen out of order so this is where
non-blocking algorithms come in so lock
freedom is a class of non blocking
algorithm like I said where we guarantee
progress of one thread at least and and
it seems like that this might be
something that actually provides some of
the predictability that we're looking
for so how do we implement something
like this well we're not tied to test
and set processors and most modern
processors in fact all commodity
processors for machines that I'm aware
of that people are using in data centers
and most mobile devices all support
compare-and-swap as well as a number of
other atomic read-modify-write
operations so what does compare-and-swap
do well given some value and some place
in memory what we're going to try to do
is compare these values if they're not
equal we just return false if they are
equal we additionally have a new value
and we're going to try to swap that in
to the destination and if we do that we
return true and now the processor is
providing us also an atomic interface to
this and therefore we can also use it
for linearizing so if we apply this to
the allocator we get something like this
right up at the top here you can see
sort of a stack of or the state of the
stack so if we were executing here we
would as before read sort of the state
of our stack the variables a and B here
or sort of corresponding to a and B
elements in the stack up there and the
next thing that we're going to do is
we're going to try to swap be to be the
head of the stack using this compare and
swap operation so if we succeeded we've
acquired a and
use the new head of the stack right that
makes sense but what if this
compare-and-swap returns false well if
we go back to sort of this color coded
example if it return false then then we
know what happened right either well
period the head was changed right
somewhere in in between our read of the
head and our attempt to compare and swap
that seems like a race so so what could
have happened well we could have had
some process free and now we have a new
element at the front of the stack maybe
another allocation happened and a got
pulled off of the stack so this seems
like really racy behavior but again
because of the linearization point
provided by our atomic compare-and-swap
we we are guaranteed in this case that
the state we've observed will be
consistent the a is no longer there
we're going to retry so what about free
well free is actually simpler because we
only have an object we read the head you
know it's the same thing we try to swap
the new object into the head atomically
if we can't you know we had another one
of these classes of race conditions
happen and we just retry write
effectively this compare and swap is
letting us detect a state change and
retry so now this is great right we've
got this concurrent stack and the stack
is we can treat it like a free list so
it's our allocator right let's just race
a bunch of cards against it right we've
already seen like this is concurrent
safe right so let's imagine that we have
this this process and it goes in and it
reads everything it reads the front of
the the slab head it's going to read the
next element and it's going to stop
executing before it gets to the
compare-and-swap because that's actually
a thing that happens all the time in the
meanwhile we have another process that
comes in and it's got some execution
time so it's going to go ahead and
allocate and a comes off of the head of
the slab so let's play this game again
let's take another process and it's
going to come in this process is going
to allocate
and B is now off of the slab so we still
haven't executed our compare-and-swap
yet but it's fine because now c is the
head so we'll just retry if we were to
execute right now but instead of doing
that let's say that now our second
process decides that it's done with
object that it got and it's going to go
ahead and put it back okay so now we can
see we read a was the stack head we read
B was the next element but now the state
of our stack has changed from under us
and this is no longer true so let's
finally do our compare and swap
operation well we got a but now B's back
in the list right and that violates the
semantics of our allocator protocol so
this is actually a classic concurrency
bug and it's called the ABA problem but
it's it's only a problem for us because
of our workload right we've defined that
we need to have multiple concurrent
effectively readers and writers to our
allocation pool depending on your
workload you may not actually need to
solve this problem depending on sort of
the amount of state that you care about
from your structure and the number of
concurrent processes that you have on
either side of maybe the push or pop
operation or whatever other operations
you might have on another similar
structure so solving this problem costs
definitely time overhead it can also
cost memory overhead the way that we're
going to solve it does so it's
definitely something to keep in mind as
to whether you can change your workload
to one that doesn't have to solve it for
our purposes we do so what we're going
to do is we're going to add a generation
onto our stack and this is just like a
counter and it's basically going to
count the number of allocations that
happened right this is providing
additional state into our structure that
we can use to check the validity of our
assumptions based on our snapshot at the
beginning so by introducing this counter
if we're checking it we can we can pop a
off the stack and then we can push be in
push a and then we can't actually ever
have a BA happen because the pop would
have incremented the generate
counter from anybody who had read it the
first time so what does this look like
in code well instead of using our
objects like we used to have object
pointer a be up here now we're
representing this sort of entire slab
head and the reason that we have to do
this is now we're comparing to two words
of memory that are side by side it's
important to note that these do actually
have to be contiguous the the head
pointer and the generation counter and
that's because this deek as this
double-wide calves it's a compare and
swap operation but it works over 22
words of memory or two double words of
memory in the case of you know 64-bit
system but whatever I guess it depends
on how you define word so this solves
the problem because of the reasons that
I I mentioned previously but another
thing to point out here is that we're
reading the generation before the head
and that's actually super important not
actually for our allocator but i want to
point it out because if you go and you
start writing your own concurrent data
structures this becomes very important
so uniqueness is also a property of our
alligator and that's implied by the sort
of you can't allocate the same thing
twice and you can't free the same thing
twice right that implies a uniqueness of
objects in the system so any object that
you allocate is distinct from any other
object in a general-purpose stack that's
not necessarily true we might have
element a that we can push onto the
stack 50 million times and it's fine
right if we do have that case we could
have a case where a concurrent reader
from the stack or somebody trying to pop
reads a as the stack head somebody else
pushes another a onto the stack and then
we read the generation counter right so
now we see a at the front and we have
the latest generation counter so we're
going to mutate our stack again and it's
going to be the wrong state so reading
the generation counter first solves that
because we're guaranteed that if any
other operations happen the generation
counter will be different the next time
or when we actually operate our
compare-and-swap do we have to do the
same thing for free well no free stays
the same and it
this is really because free only depends
on the stack head first eight right the
reason that we need to do this
generation counter the reason that ABA
becomes a problem for us really is
because we read the stack head we read
the next element and that joint state
that that multi-level state is
effectively what's coming back inviting
us and causing us or combined with our
workload as I said coming back and
biting us to cause the ad a problem okay
well it turns out that that's actually
all of the problems because I'm not
going to talk about cache coherency your
memory ordering in this talk so that's
all basically the race conditions that
we're going to see here so we've we've
solved this problem now we have a
correct concurrent lock-free slab
allocator okay so what is the throughput
look like that's great that's
predictable right we have the this is
the same test but with our concurrent
allocator we see that as we increase
contention in some cases we're getting
even better throughput right it looks
linear it definitely looks linear on the
logarithmic plot but why is this right
like what what is it about this property
of lock freedom that that's really
helping us out so much well to do that
we need to look at something other than
throughput like throughputs great but it
doesn't tell you the whole story and in
particular we need to look at latency
plots okay so this is a min maxon 25th
to 75th quartiles box and whiskers plot
of our testin testin set lock that was
the the test and set with the nested
loop and what we see is that sort of in
the general case up to 75th percentile
we've got pretty reasonable latency but
from 75th to max value we've got this
just enormous variation it's
unpredictable and this is because of the
lack of progress guarantee from the lock
you you aren't guaranteed to make
progress and this introduces jitter
additional Eden introduces a queuing
effect into the system so if you're
familiar with littles law that can sort
of explain this this jitter and latency
it takes your system out of a stable
state
I should also mention that I have also
skimmed the top one percent off of these
numbers to you know just account for any
like huge amount of system jitter that
might have been introduced and it
skimmed off of all of these plots I
haven't talked about pthreads a lot I
suspect that a number of people are
using pthreads for reasons that have to
do with having more threads than you
have cores in the system which is a good
case for not using spin locks in fact
but what we see here is that we actually
have significant variance in
distribution from our 25th to 75th
percentile as well and we're also still
in the same order of magnitude on our
y-axis here so when we look at our
exponential back-off spinlock in general
it seems fine but we still at from from
75th percentile to max we still have all
of this jitter all of this variance even
though we've we've decreased latency by
an order of magnitude so this decrease
in latency is actually the only thing
that is giving our test and set spinlock
additional throughput and this comes
back back to littles law right which
says that the number of outstanding
items in a queuing system in average
outstanding items in a queuing system is
equal to the arrival rate of new
requests so contention on the log right
plus the time it takes to service a
request so latency of block because we
were able to reduce the latency in our
exponential back-off spinlock that's the
only reason that we got the throughput
but because of the queuing effect of
locks that is why our throughput still
remains unpredictable under high
contention so finally let's look at our
latency for our concurrent allocator
that looks really good right that is
predictable right we're two orders of
magnitude town the the deviation between
men and max is still in the same order
of magnitude everything looks really
good this is what we need right under
under contention actually it performed
or under no contention it performed
worse I'd be happy to talk about that
afterwards if you're interested but as
we add contention we see that we still
get this very predictable latency and
that gives us our throughput again this
is a that coupled with the progress
guarantee gives us the throughput
consistency as we add contention on the
structure so takeaways well one thing
that I really want to drive home because
I think maybe I didn't do it enough is
that racing processes with each other
it's not only like common when when
we're addressing throughput and latency
issues in our software it actually is
kind of a crucial part of designing
non-blocking algorithms it's a crucial
part of trying to improve throughput in
perhaps our locking functions where we
see that doing something that reduces
latency can help us increase throughput
by racing these processes in a way
that's friendlier to the system and and
furthermore we can do that safely
because our systems provide us with
these atomic read-modify-write
primitives like test and set like
comparin compare-and-swap fetching fee
you know whatever operation you want to
do and and these linearization points
provide us with a total globally
observable order between all processes
in the system that they all agree on all
the time because it linearize 'as good
so that's a that's we're wrapping it up
I definitely did not cover all of the
things about a slab allocator or
performance things that you want to keep
in mind in particular per thread slabs
what to do when your slab is out of
memory stealing from elsewhere that kind
of stuff if you're interested in this
code or using this in your systems
definitely check check out you slab bio
which right now is just redirecting to
our github page we've open sourced this
and it's under the Apache to license
also before I wrap up I want to thank my
colleague Nathan Taylor so he actually
presented this talk at sir
yesterday but the conference's were
happening concurrently so it couldn't be
at both I also really can't overstate
how much he helped both with content and
you know teasing out bugs and some of
the testing stuff and giving these
slides a consistent total order so
that's all I've got for you thank you
for coming if you've got questions I
think there's only two minutes left so
I'm going to head out to the fastly
booth that's just down the hall so feel
free to come see me and ask questions
there and thank you for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>