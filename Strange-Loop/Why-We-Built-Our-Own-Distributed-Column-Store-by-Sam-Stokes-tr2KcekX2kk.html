<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Why We Built Our Own Distributed Column Store&quot; by Sam Stokes | Coder Coacher - Coaching Coders</title><meta content="&quot;Why We Built Our Own Distributed Column Store&quot; by Sam Stokes - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Why We Built Our Own Distributed Column Store&quot; by Sam Stokes</b></h2><h5 class="post__date">2017-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tr2KcekX2kk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello strange loop I'd like to talk to
you about distributed column stores and
why we built one
I'm Sam Stokes I'm an engineer at
honeycomb which is a small start-up in
the San Francisco Bay Area as a fearless
in the crowd when I'm not making
software
I'm usually making cocktails and
sometimes bad jokes and I'd like to
introduce you to retriever
what is retriever well Retriever is a
distributed columnstore it's also an
analytic query engine and it's got a
skinless data model and hang on a second
I'm really getting ahead of myself a bit
because you're probably wondering why
would someone build their own datastore
it's pretty much rule one right that you
don't build your own database and if you
do you especially don't call it a
database you call it a datastore which
is why we build the datastore so
Retriever is a special purpose datastore
that we built to power honeycomb and so
to talk about retriever I have to
explain a little bit about what
honeycomb does well you can think of
honeycomb as a debugger for production
systems what does that mean well we want
to help engineers understand and
troubleshoot distributed systems as you
might be aware especially if you've been
to any other tools at this conference
distributed systems can be kind of
tricky beasts or some sometimes so
honeycomb sits in between sort of
traditional metrics tools and
traditional log aggregation tools that's
a bit abstract so let me only get a bit
more concrete how does honeycomb work
well your systems the ones you want to
understand better they send us events
events are kind of similar to structured
logs if you're familiar with that cool
buzzword
they're also JSON blobs here's an
example event
and this represents a server processing
some kind of web request COC events have
a bunch of fields this one has like an
endpoint field what end point on the web
app was being hit the hostname that
processed the request some statistics
about response time and so on so you
send us events we store your events and
you can ask for them back and then you
can issue queries on those events and
the reason you might want to do that is
you can ask us to calculate things for
example here we're saying for every
endpoint that events have come in for
calculate the average response time that
we spent processing the request for that
endpoint and we're going to visualize
something else the amount of time that
was spent in the database processing
those requests okay so we take your
queries and we turned them into pretty
graphs like this there's a lot going on
here so the top graph I'm kind of
showing the this is the average response
time for every endpoint and the bottom
graph this is a visualization called a
heat map so don't worry too much if you
aren't familiar with heat maps because
that's not what this talk is about but
heat maps are a pretty cool way of
visualizing the distribution of response
times so if this is time spent in the
database you can imagine that these two
bands are sort of showing here are some
requests that hit the database cache and
here are some that missed and took
longer but the idea is so we can draw
graphs and graphs are awesome graphs
help you like look for patterns and
detect anomalies but we want you to be
able to always jump back and forth
between the graphs of the calculations
that you've drawn and the raw data that
came in that you sent us that we
generated those graphs from and what
that lets you do is you can find
something we're on the graph and then
you can say well hang on why did that
happen what was actually going on there
and you can go back and look at the real
event with all of the data that you sent
us and all of the detail
and look for signs of weirdness and
there are some other questions you can
ask this kind of system an example is
which of your users are seeing the
highest response times so this isn't a
question about the global average
response time across the whole site this
is saying literally for each user find
the ones that are seeing bad response
times and you might express this in a
query something like show me the average
response time broken down by user ID and
ordered by the average response time
what about I'm running a deploy and my
deploys were all not gradually across a
fleet of servers and I want to know how
many servers has my deploy got - well I
can express that in this kind of system
if I'm tagging my events with a bill by
D and so then as offensive processed I
can say show me the number of unique
host names that have sent me events for
each build ID and you could kind of see
the new builds trending upwards as the
deploy rolls out what about for all of
our customers who are on our most
expensive pricing plan are any of them
affected by this error we're just in
troubleshooting and the idea there is at
the point when you're sending us an
event maybe you're consuming some logs
maybe you've instrumented your code but
the person who's doing that work doesn't
want to care about which customer is
high value they don't know about who's
on which pricing plan they certainly
don't want to have to update their code
when a customer changes pricing plan or
you sign up somebody new but it's still
a really useful query to be able to ask
if you have some other representation of
which customers are on which plan and so
the idea is we need to be able to query
really flexibly on the data that we've
got well how do we do that so what do we
do with all the data that you're sending
us how do we store them in a way that we
can serve those kinds of queries and get
graphs out well spoiler alert
the answer is recruiter so for the rest
of this talk I'd like to describe to you
what it was like building retriever and
some of the decisions that we made I'll
start with a pretty high-level overview
of what it looks like and how it works
I'll dive deeper into how we store the
data on disk and how we read it back and
especially some of the details of how we
read it back really fast when it's
spread across multiple machines and
finally because this is a database we
run in production data store this is a
data store we run in production remember
don't build a database we run this for
for paying customers so there are some
interesting things we need to do to make
that actually work in the face of the
real world so what do you need in a data
store that can serve this kind of
features well we have we need to issue
queries and we have a fairly flexible
sort of query model so we have these
operations like break down saying take
the events that you've sent and break
them down by the value of some field
like break down by end point or break
down by user ID and you can filter which
is you know discard things that don't
match the filter and we want you to be
able to break down and filter by any
field you don't have to say in advance
I'm only going to break down by end
point or I'm only going to break down by
user ID you can make up your mind
afterwards and so you're not pre
defining an index or setting up a schema
and then having to comply with that we
need queries to be able to produce two
different kinds of results so we have
time series graphs and in fact some
weird and wonderful visualizations like
the heatmap thing as well and we also
have queries that can return just the
raw data they came in according to some
filters and we need to calculate the
kinds of statistics that are useful for
operations purposes really that's what
we're trying to solve here is the sort
of problems that operations people face
so percentiles are really useful
Layton sees if you're still measuring
average latency you should really try
percentiles we can calculate the
histogram for an entire distribution of
response times or of packet sizes or
other things like that count distinct is
another really useful operation we need
all of that to be really fast and the
reason for that is we want users to be
able to do a sort of question-and-answer
workflow with their systems so something
goes wrong the user forms a theory about
what went wrong and why so they ask a
question for example is this only
happening for certain users they ask a
question to test the hypothesis they get
an answer back maybe it confirms the
hypothesis in which case they might want
to refine further and drill down
wellmaybe turns up a hypothesis was
wrong and they need to try a different
tack and in both of these cases it's
really important for the QA to be really
fast you don't want to sort of set a
query running and then go have a cup of
tea and then forget what you were doing
we're running this multiple customers
and so we need to be able to do things
like enforce quotas and other properties
that are tied to our business model and
so we need some sort of custom tools
that can manage that sort of thing and
let us go in and you know poke a
customer in chase their quota and maybe
most importantly we're a start-up we
have an extremely limited budget and an
even more limited team an awesome team
but a limited book so this thing needs
to be very simple we are not building a
general-purpose database here and we
don't need to we know exactly what data
is coming in and a what rates with what
access patterns we control the query
interface so we know what kinds of
questions can be asked of our data so we
have this very precisely defined use
case we can write a special-purpose data
store we also don't need to support
updates because once an events come in
it's happened already you don't need to
go and change it and we don't need to
support the kinds of fancy features you
would get in a more general-purpose data
store
like joins or transactional semantics so
we need to keep it simple so again we
need to issue all kinds of queries
really quickly and not spend hundreds of
thirteen years doing it so what does the
system look like that can handle this
sort of workload well it turns out
someone already built one
it's called scuba scuba is a system that
was built at Facebook they released a
paper on it called scuba diving into
data at Facebook and this being strange
loop there's a pretty high chance that
one of these twelve people are sitting
in the audience somewhere and scuba was
built to solve exactly the same problem
because Facebook has this problem as
well they operate at very large systems
of scale and so they built a distributed
event store and the idea behind scuba is
you ingest events a very high volume
from lots of producers lots of systems
at once and you store them all every
single event and you store them
distributed across many servers and that
lets you scale out the amount of stuff
that you're storing and it also lets you
speed up queries because when you want
to query the events you can fan the
query out across all those servers and
do a lot of the processing in parallel
and then to make things even faster they
take those events and they store them in
RAM which is ludicrously fast obviously
so the problem with scuba is it's a
Facebook tool we can't use it
which means our customers can't use it
but we can be inspired by it they were
kind enough to write a paper about it
so we thought well what if we could
build a really simple system that's
inspired by the design of scuba so
Retriever is similar inspired by scuba
we are a distributed event store we
store the events that come in
distributed across servers and we get
fast queries by fanning out the queries
to multiple servers but we do a few
things a little differently from scuba
and the most important and obvious one
is we store data on disk and the reason
why we store data on disk rather than
RAM is because we aren't Facebook and we
don't have the infinite money cheat code
it turns out that Ram is expensive and
it turns out that SSDs are really fast
and especially if you can do certain
things with your access patterns you can
actually get really fast queries just by
reading from SSDs also file system
pretty good at caching stuff these days
because we're storing data on disk that
means we can also depend on the file
system for a lot of the semantics that
we need from our data store and that's
pretty useful it means we can do
something called column-oriented storage
which some of you may be familiar with
this already I'll be doing a recap of
that later for you it'll be more of a
recap but I want to make sure everyone's
on the same page about that and we use
Kafka which I'll talk about more later
for ingesting events and that gives us
all kinds of cool operational properties
which we can rely on so some of you may
prefer to see a map of the territory
before you go on a hike so let's do a
quick overview of what writing and
reading looks like this is the
architecture of Retriever a customer
sends events they are published to a
caf-co topic if you're not sure what
that means yet I'll cover calculator but
I'm sure a lot of you are so we publish
events to kafka retrievers consume from
kafka they receive all the events that
come in and write into disk and we have
retrievers operating in pairs like this
and then to read a query is issued by
our API it's gonna pick a retriever to
run the query
it'll issue the query to that retriever
that retriever will fan out the query to
other retrievers that have the data it
needs
they'll compute the results and send
them back awesome
well that was extremely high level let's
explain how some of that actually works
this is a picture of it's a
column-oriented picture by the way I did
warn you about the bad jokes so our data
model is that customers have one or more
data sets and data sets you can think of
them as basically tables in a
traditional database data sets are
partitioned across multiple servers each
data set is assigned to a number of
partitions typically we have about three
partitions per data set our largest data
set right now is sitting on 39
partitions and those data set partitions
contain events with met events before
let's recap so here are a couple of
different events these are both
processing web requests and so you see
these two events have a slightly
different shape the second event has
this error field but that's the idea
events are just JSON blobs you send us
lots of them so I'd like to visualize
these events in a slightly different
format for a reason that will come
apparent very quickly so I've drawn
these events in a table and what we've
got here is in a fairly traditional way
I've got the fields of the events in the
columns of the table and then each row
is one event so here we've got five
events showing you know a few requests
to the foo path and some two bar and
then we have this one other event that
has none of that data and just has like
a log message in it and the point about
this is there's no schema attached to a
data set like you might get in a
traditional you know my sequel type
database where each table has a data set
that has a schema so you can send us
events of all shapes and sizes into the
same data set and we're just happily
store them we're not going to say like
this one is the wrong type for this data
set and events can have an arbitrary
number of fields we have some events
with hundreds of fields and that's
actually a pretty normal case you might
be thinking well why the hell would I
have an event with a hundred fields in
it well let's imagine these events are
representing the processing of a web
request and there's already some stuff
I've left out here
just for space so we've got the path
we've got the response time to process
the request we've got the status code
but I don't have the host name in here I
don't have the headers of the requests
maybe those are useful maybe while
you're processing web requests you want
to log how much time you spend in the
database for each request that it should
be useful to know if your database
starts running slow maybe you want to
include application level information
such as the user ID that the request was
for or maybe something more specific
like when we did this particular
database call it returned this many rows
and so you can imagine if you have if
you have these really rich events
annotated with lots of data they can
have quite a lot of fields but not all
of those fields are going to be
populated all the time and that's what
we see here that all of these fields can
potentially contain null values a couple
of columns I'd like to point out so we
have this index column on the Left we
assign every event and index as it comes
in and that's going to come in later but
the idea is these are unique for each
event and every event has a timestamp
kind of by definition an event is a
thing that happened at a time so that's
just the time they've even happened
okay so how do we store these we have a
few choices because what we need to do
is take these structured events and
store them on disk as files and files
are just streams of bytes so we need to
take this structure and serialize it
somehow and broadly we have two choices
about how to do this for this sort of
data we can serialize it in what's
called a row oriented way or what's
called a column oriented way so
apologies if you already know about
column oriented storage this part is
going to be very familiar so row
oriented is sort of a traditional choice
this is what a lot of databases do but
what I've done with the table here is
I've cut out some of the fields and
focused in on just a few events just
from for simplicity so the idea behind
row oriented storage is you store
everything pretty much in one file
and you take all of the fields for a
given record a given event and you just
lay them out on disk next to each other
so one request or one event comes in
we're going to store the fields we just
got the path from the response time at
the status and then a nothing for the
error
another event comes in we do the same
thing again another event comes in so we
stole this stuff sequentially and a
couple of things a couple of things to
point out here one is this this empty
space that we have to leave if there's a
null value and there's a few different
ways you can encode this but most of
them you have to somehow indicate that
hey that normally be a record here but I
don't have it in this case and that's
one thing if you've got one null value
but if you have 100 columns and most of
them are null most of the time that's
actually kind of a lot of wasted space
just to say that there's nothing there
and the other thing the way you read
this is you read a whole record and then
you read another whole record and that's
great if what you're doing is mostly
reading if that's what you mostly want
to do to get the entire contents of a
record and a lot of like ORM use cases
that is what you want to do you just
want to load your user object into
memory and then do stuff with it in the
application but if you're only concerned
with a couple of these fields you're
actually doing a lot of i/o that you
don't need so an alternative way of
serializing data like this is called
column-oriented so I've expanded the
table again to include those index and
timestamp fields because I'm going to
need them the idea of column oriented
storage is instead of storing all of
your events in a single file you make a
file for every field in the events
otherwise known as column so you have a
file per column so here's how we would
store as an example the path field we
have a fiat file called path tostring
because it's a string so an event comes
in with index 0 we write the index value
and then we write the value of the field
which is through another event comes in
we write the index we write the value of
the field another event same thing
so this is how we would encode one
column of this table just to hammer home
the point here's how we would do it for
the response time field we write the
index we write the value and so on and
so on
so you might be wondering well how do we
do this when there isn't a value two
right so for the error field the first
event that came in didn't have an error
so we just don't write anything we're
not even gonna create this file in fact
until we have a value two right so the
next event comes in and we do have a
value so we just start at the index one
we pry it in next one we write the value
next event has no error you get the idea
we don't write anything so this is the
whole column file for error and imagine
there were a hundred events that came in
and only one of them had an error you'd
only have written one for one value
there the timestamp column that I
mentioned earlier is stored in a very
similar way we write the index we write
the timestamp the reason this is worth
looking at is because the timestamp is
guaranteed to always be present we can
use this column also as kind of an index
in other words this tells us what index
is exist and because every event has an
index this tells us what events exist
the reason we double up the index of the
timestamp like this is because almost
all your queries are going to be based
on events in some time range and this
lets us do a quick filter against
timestamp but I'll we can talk about
that later so how do we read data that
we've written in this slightly awkward
kind of way well we don't just have one
file to read anymore we have potentially
a variable number of files so the first
question we need to ask if we want to
read is what files do we have what
columns do we have available to scan
well these are just files on disk as I
said earlier which means we can
basically just run LS like we can ask
the file system what files do you have
so in this case it might say I have path
response time status and error
let's say we're calculating the average
response time I know I just told you all
to do percentiles but this is an easier
example let's say we want to do the
average response time for events that
have status 200 and let's go through how
we would read that so first we need to
open the files that we want to read we
can figure out what files we're going to
need by looking at the query that we're
trying to run so we're going to need the
index file because we always need the
index file we need to know what events
there are otherwise - the timestamp
column we're going to need status
because that's what we're filtering on
so we need to check if the events meet
meet our query and we're going to need
the response time column so let's open
all those files and you can think of
this star is like a file pointer so
we're gonna be reading through these
files so the first thing we do is read
an index find out is their event is
there an event to read but we got an
index its index 0 now we need to check
the filter so we'll read from the status
file until we hit that index 0 that we
just read well it just so happens that's
the first value so we get there straight
away now we have the status value we can
check the filter does it match yes it
does so we read from the response time
file until we hit index 0 again well we
find it so we saved that value for later
and we'll do all the calculation at the
end then we start again reading index
it's 1 read from the status file until
we hit index 1 here we go
check the filter well this one doesn't
match so we can skip this event start
again reading index index to read from
the status file until we hit index 2
here we go
it matches great read from the response
time file until we hit index 2 well hang
on this is index 1 that's not what we're
looking for
well that's because this is the event
that we skipped because the previous
status didn't match so we just keep
going here we go index 2 so we forget
about that 23 we don't want that one
here we've got the value 657 so we
collect that and so on so this is how we
scan through all of
files and we can do all these reads in
parallel so long as we have enough i/o
capacity so let's jump back to the table
view to sort of show what it was that we
just did so I've highlighted all of the
values that we looked at so we opened
these files and we read these values and
we skipped over one value in the
response time column but what we didn't
do is go anywhere near the path file we
didn't touch the arrow file if there
were a hundred other files we didn't
touch them either so we read all of this
data without reading most of it this is
kind of the mantra of column-oriented
storages only read what you need and
this is why it's so nice for these
analytical queries because most of those
who's going to run a calculation on a
hundred fields right like what would
that even or that expression even looked
like okay so we've got some data on disk
and that's how we can execute a query
against a single node because everything
I talked about is reading from files on
the disk but this is a distributed
datastore so the data exists across
multiple nodes so how do we how do we
run that query across all of the nodes
well a clients got a query to issue
let's say it's the same average query so
the client knows what data said its
querying for so it's going to check what
partitions that data set exists on and
then the partitions have retriever nodes
listening on them the clients going to
pick one retriever to be the root for
the query and then it sends the query to
that retriever and the retrievers know
what other retrievers exist on that
partition so the root node will forward
on the query to the other nodes on the
other partitions for the same data set
and then they can all scan their their
data on disk in parallel and they can
all do a partial that they can compute
part of the answer to the question and
then they can send their partial results
back to the root
and the route merges the results and
sends them back to the client and then
we have our answer
well that sounds simple but there are a
few things we need to watch out for so
the data is partitioned which means only
every node only has part of the answer
and there are a few things we can trip
up there we need to be careful in the
way that we combine those partial
results so we get the right answer so
here's an example if you're computing an
average of a set of numbers you can't
split that set of numbers into two sets
and average those sets and then average
the averages if you do that you'll get
the wrong answer I don't know if this is
entirely readable but say you're
computing the average of one two three
and three the answers to 2.25 if you
split that up into one two three take
the average of that to get two and then
the average of three is three and the
average of those two is 2.5 which is
different so instead we need to make
sure that the way we're computing the
partial results is something we can send
back to the root in such a way that the
root can combine those results and get
the right answer so for an average you
can do it if you if you compute the sums
of the partial data and the counts of
the partial data and then send both of
those back and then you can add up all
the sums adeb accounts do the division
and get the right average so we have a
few different types of calculation we
can run and we need each of these
calculations to work in this way so that
we can do a distributed query and get
the right answer so some other examples
if we're calculating values for
different groups so this is our
breakdown operator we're basically going
to accumulate the answers so the
statistics for each group in a sort of
map structure and you can each node can
compute its own groups and its own
values then send the group's back to the
root and the root can do the merge and
so long as the values in the group
Armour jabal then you can you can merge
appropriately for count distinct we
don't do this by collecting all the
unique values because that would be
really memory intensive
but there's a trick called hyper log-log
where this is a again this is not what
this talks about but hyper log-log is a
way of estimating the number of unique
values in a given set and it turns out
that hyper log-log is a data structure
that you can actually send across the
network and it combines in this way
similarly for percentiles there's a
probabilistic trick called tea digest
which combines nicely and this is a
property called commutativity which
comes up a whole lot in distributed
systems if you've seen any talks about
CRD teas that's another place where this
sort of pattern comes up there's one
more problem we need to deal with with
distributed queries which is we have
this root node that's merging all the
results but that might be a lot of work
if we've found out to ten nodes or
thirty nine nodes and maybe there's
partial results are all of the groups
were collecting and maybe we have a lot
of groups you know if that's user ID you
might have a million user IDs you might
have more than that in certain cases and
so we don't want to overwhelm the root
node you know run out of memory or just
spend too long computing it's kind of a
shame if we've done all this work to
distribute the query and then we still
have one guy doing all of the
computation at the top so we can sort of
do this recursively and if we have more
than a certain number of nodes that were
querying we just have the leaf nodes
also become roots and they do their own
distributed query pattern and then
because we have the commutativity
property everything rolls out nicely all
right so we can write data and we can
read data so we're done right ship it
well we kind of have to run this thing
right we have customers that pay us
money this is the real world nodes fail
so we do have to solve a few more
problems so let's talk about how we do
that and I need to embark on a brief
detour on this hike to talk about Kafka
can I get a quick show of hands how many
of you are familiar with Kafka Wow
everyone in the room practically
how many of you are using Catherine
production right now about half the room
cool all right well this I'll skate
through this reasonably quickly then
all right so retriever relies pretty
heavily on Kafka for ingesting our
events so as we saw at the start events
go in through Kafka a calf kegger
there's lots of nice properties it gives
us the ability to distribute rights out
across multiple nerds it gives us
replication kind of for free and we rely
on it for fault tolerance and disaster
recovery how exactly well as all of you
probably know Catherine is a distributed
log it's not exactly like a message
qubit you can think of it that way you
publish messages to topics which are
again kind of like tables topics are
partitioned which is the unit of scaling
and within a partition of a given topic
Kafka guarantees that messages are
totally ordered which means messages
will come out of a topic partition in
the same order that they went in which
is really useful and Kafka actually
stores the messages on disk whether or
not anyone is listening to them it's not
a lot of message queues will just when a
message comes in they'll route the
message straight to any consumers and if
there's no one listening they'll just
drop the message on the floor that
caf-co writes everything and you can use
that for a couple of things you can get
a sort of publish/subscribe semantics
because the data is just sitting there
anyone can read it if they like you can
have multiple readers and it also means
you can go back in time and replay
messages from some point so how do we
use this well plans publish events to a
CAF kotappa
and the Kafka topic is partitioned and
our data sets are assigned to partitions
so this is the partitioning I was
talking about earlier we have a number
of partitions on the Kafka topic and the
datasets know which of those partitions
they are assigned to so the client who's
writing the data chooses one of the
partitions for the data set and right
now it just does that at random it turns
out that random partitioning is actually
pretty
for this kind of behavior so what that
means is the clients just randomly
writing data between all of the
partitions on a data set and then we
have retrievers consuming from Kafka on
each partition and writing events to
disk and because Kafka lets us attach as
many subscribers as we want we just have
two retrievers consuming each partition
and that gives us replication for free
because Kafka is guaranteeing that the
events are coming in the units are
coming out in the same order that they
came in then we know we're gonna have an
exact copy of the data on two different
nodes and that's pretty useful so one
problem we need to solve part of our
business model is to price based on
storage so every customer is paying for
a certain amount of storage they have a
quota and we're not in the business of
just storing data forever collecting
infinite amounts of it that would not be
useful so when a customer's nearing the
edge of their quota we want to start
aging out the oldest data that they've
sent us well this is where we can rely
on the file system to do a lot of the
work for us
so as events are coming in we split them
up into segments and you can think of
these as sort of time slices of the data
coming in so at some point we decide
we've written enough data enough events
into this segment so we start a new
segment and because we have just files
on disk
we represent segments just as
directories in which the files live so
when it's time for a new segment we just
open a new directory and start writing
the files into that directory instead of
the old one the old one sits where it
was which means when we need to
calculate how much space our customer is
occupying we can calculate how much
space each segment occupies again we can
just ask the file system hey how much
data is in this directory how big is
this file and when we need to do when we
need to delete data we can just say hey
what segments are there list the
directories find the oldest one they're
numbered
numerically so we can just sort them and
just delete the directory containing the
oldest data we didn't need to do any
kind of in memory manipulation and worry
about concurrency what about fault
tolerance so this is the big bad world
we run on AC 2 nodes go down processes
crash so why might Pro why might receive
a crash well we might have a bug we
might have a network outage which means
it's unable to connect to Kafka we might
just be deploying this this is this is a
piece of software that we're actively
working on we need to deploy to it we
need to take the process down we have to
replicas of the data so that should be
fine but we don't want to miss events as
they're coming in customers are always
writing data we don't want to just drop
those events on the floor we don't want
our two replicas to get out of date so
what we do and this is another thing
where we can just use CAF could have
pretty much solved this problem
so each retriever tracks the kafka
messages have what's called an offset
associated with them and that's just how
far as it's writing on disk how far it's
gone and so offsets are also
incrementing so as every event comes
into retriever we track what offset it
came in with and when we boot up
Retriever we just say hey the last event
I saw had this offset so give me
everything since then Kathir guarantees
the order so we know we we won't have
missed anything and to make sure that
works we right every now and then a
checkpoint file and that says here's the
last offset that I definitely finished
writing to disk but we need to store
something else as well we need to store
the index of the message and this is the
index that we're assigning to the events
as they're coming in so that determines
that the offset tells us where to
reconfirm but we have another problem
because we're writing to multiple files
and we might have had the problem where
we written we wrote to the path file for
example we wrote the value for the path
and then we crashed before we got a
chance to write the value for the
response time or some other field so
we're kind of inconsistent at that point
we've
written half the event and that would be
bad because we don't want to them be
returning inconsistent data so when
weary consume we look at this checkpoint
and we say okay this index checkpoint is
the last message that I'm sure I
finished writing and anything after that
point is suspect so when we start up we
just take all those files and truncate
them at the point where that indexes and
sum between those two together we know
we've truncated at that point we're
rican Suman from the corresponding
offset so we're going to get all the
messages that came since that point and
all the stuff we just deleted we can
continue writing so that's great but
what if a node goes away completely like
ec2 hosts can just disappear well we
have another replica so we find another
node on the same partition it's got all
the data that we need now we need to
copy the data over to the new node so
that it can catch up well these are
files on disk let's use a sync and it
turns out that our sync is really good
at this kind of thing it can even detect
if you've already run so we can sort of
recover from an incomplete bootstrap
once we're done with our sync because
that takes some time
well now we're a little bit behind but
that's fine because we cannot we also
are synced the checkpoint file so we
just reckon soom from whatever the
checkpoint says now we're up to date so
that's sort of how we get some of these
operational properties we get
replication thanks to Kafka we get full
Tolerance thanks to Kafka we get quota
management by using the file system and
we can bootstrap a new nodes using rsync
and Kafka and that is retriever but what
I'd like you to take away from this talk
well one column-oriented storage is
pretty cool especially if you're doing a
sort of analytical query sort of use
case because you only need to read what
you need Kathir is pretty cool it
basically solves a lot of distributed
systems problems for you so if you're
building a distributed system try not to
solve fault tolerance and replication
yourself those are really hard just rely
on Kafka to do it
file systems are pretty cool they have
all kinds of nice semantics they can
speed up breeds for you they can do
atomic renames they let you use a sink
but what all that is really a way of
saying is sometimes it's fine to solve
the really hard problems if that's what
your business needs but make sure first
that you've looked for ways to make them
easy thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>