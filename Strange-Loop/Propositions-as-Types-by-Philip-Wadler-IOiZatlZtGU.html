<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Propositions as Types&quot; by Philip Wadler | Coder Coacher - Coaching Coders</title><meta content="&quot;Propositions as Types&quot; by Philip Wadler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Propositions as Types&quot; by Philip Wadler</b></h2><h5 class="post__date">2015-09-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IOiZatlZtGU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">are you ready to learn about the
hilarious subject of computability
theory an algorithm is a sequence of
instructions followed by a computer now
you all think of computers as machines
but for an awful long time what a
computer meant was a person the person
who executed the algorithm algorithms go
back to Euclid's elements in classical
Greece and to eponymously al-khwarizmi
in 9th century Persia but a formal
mathematical definition doesn't appear
until the 20th century when you have
proposals by Alonzo Church curt girdle
and Alan Turing all appearing within a
year of each other
it's like buses you wait 2000 years for
a definition of effective computability
and then three come along at once why
did this happen so at the dawn of the
20th century one of the foremost
proponents of formal logic was David
Hilbert in good again and what he wanted
to do was put all mathematicians out of
business what he wanted was an algorithm
that given a statement in formal logic
would determine if that statement was
true or false this was called the anti
dual problem because it sounds a lot
better in German it just means decision
problem and what Hilbert was depending
upon was the idea that logic is complete
meaning every provable statement is true
and every true state
disprovable sounds reasonable right
except of course in 1930 in vienna curt
girdle published his proof of the
incompleteness theorem and this meant
that Hilbert was to use a technical term
screwed so what girdle showed was that
any logic powerful enough to represent
arithmetic could encode the following
statement this statement is not provable
the way he did this is he used a clever
technique called girdle numbering to
encode statements and proofs as numbers
which is why he depended on arithmetic
so don't worry about the details about
how he did that although it is one of
the world's first functional programs
but think about this statement this
statement is not provable boy right as
soon as you have this statement written
down you are in trouble why
right so it's very much like what we
heard with the liars paradox only now
it's proved ability so what happens well
if it's false then it is provable and
you've proved something that's false
this is really bad news you don't want
to do that so the alternative is that
it's true but now you must have a
statement that is true but not provable
so that's not as bad but it's still
really annoying especially if you're
Hilbert now as long as people thought
that there would be a solution to the
introduced problem you didn't need a
formal definition of algorithm you would
just write down the algorithm that was
the solution and it would be like
Justice Potter's definition of
pornography I know it when I see it but
if your goal is to show the introduced
problem is undecidable then you need a
formal definition of algorithm so you
can show that no algorithm is going to
work so the race was on the first
solution was proposed by Alonzo Church
in Princeton he came up with this thing
called lambda calculus in 1932 and by
1936 kid you used it to show that yes if
an algorithm is what you can express in
out lambda calculus that is the case
that the introduced problem is
undecidable he actually did this by
means of something else that was
undecidable which we now call the
halting problem there is the complete
definition of lambda calculus much
briefer than those of you that use
languages like C or Java it's only got
three constructs variables function
definition and function application
right and it's the world's coolest
programming language because it was
defined a decade before one had
computers for many years me and my
colleagues have worked with functional
languages which are based on lambda
calculus and for many years people in
industry have managed to pretty much
ignore everything we do of course these
days landis have become very trendy who
have lambdas in C++ you have lambdas in
Python and you have lambdas in Java so
there's Duke the icon for Java looking
very smug congratulations Duke you have
finally caught up with where Alonso
church was in the 1930s so here we are
back to Kirk girdle again he came
visiting in Princeton and he thought
that a Church's solution was thorough
his precise words were thoroughly
unsatisfactory so church went to girdle
and he said look you come up with your
own definition and all show that mine is
as good as yours and girdle did he came
up with a second definition of
computable effectively computable which
he called general recursive functions
and this was actually written up by
church's student cleany with after
vision and
Church duly went off and he showed the
two definitions were equivalent and so
he went back to girdle expecting this
would resolve the matter and girdle said
oh my definition is the same as your
definition hmm
my definition must be wrong then the
impasse was resolved by this man Alan
Turing at Cambridge who of course came
up with what we now call Turing machines
and again he showed that they that if
Turing machine was your definition of
algorithm that the anti-drugs problem
was undecidable and Turing proved his
definition was equivalent to churches
and hence to girdles what Turing did
that was different was not the
mathematics it was the philosophy he
gave an argument that anything that a
computer could do could be done by a
Turing machine where again computer
means a person following a sequence of
instructions and girdle was finally
convinced that all three equivalent
definitions did capture effective
computability philosophers often argue
is mathematics invented or is it
discovered three times guys three
different independent definitions all
turn out to be equivalent that's
powerful evidence that you've not
invented something you've discovered
something right it's not just sports
fans who are impressed by a hat trick
Kurt girdle was 28 when he undermined
the work of David Hilbert Alan Turing
was 23 still an undergraduate when he
undermined the work of Alonzo Church who
was 33 and Kurt girdle who was then in
ancient 30 so to all you young people in
the audience please keep explaining to
your elders when we are wrong
okay
so now you've got the background and we
can start talking about the brilliant
idea of propositions as types so here is
gajja Jensen he was part of he was
trying to carry out what was called
Hilbert's program and he came up with a
new way of writing down logic here it is
so we're just going to focus on so this
is called natural deduction Jensen in
his PhD thesis wrote came up with
natural deduction which is the main form
of logic we use today he came up with
sequent calculus which is the second
most used formalism in logic today and
he also came up with using the upside
down a to mean for all so there's a
little goal for all you PhD students so
here's this is actually from Jensen's
paper and we're just going to look at a
fraction of this so the fractions having
to do with implication and conjunction
so implication is that funny backwards
see it really is a backwards see it's
consequence backwards and ampersand
which means an and you can see that
let's see where's implication here's
implication and you can see it's exactly
the same except that Jensen wrote his
letters in German and then conjunctions
up at the top there so it looks other
way so what do we have here
the first line so the first rule they're
the rules come in pairs this is the
important thing so on the Left we have
rules with I meaning introduction rules
and you have the connective implication
or ampersand below the line the rules on
the right are called elimination rules
and there you have a connective above
the line so you can see a implies B on
the left there and what does the one on
the right say the elimination rule it
says if you know a implies B and you
know a those are two hypotheses above
the line and then below the line we at
the conclusion what do you know if you
know a implies B and you know a hey you
Noby so that's how you make use of an
implication how do you create or
introduce an implication well it says so
those brackets around the a mean assume
a don't prove a just assume a is true if
you assume a is true and from assuming a
is true you can get a proof of B then
you know a implies B and the second line
say well if you've got a proof of a and
you've got a proof of B you've proved a
and B and what can you do with that you
can do two different things so the
middle rule set on the bottom says if
you have proved a and B you may conclude
a and the other rule says if you've
proved a and B you may conclude be okay
are there any questions all right I will
zip on but do ask a question if there's
something that's confusing you I'm sure
it will be confusing other people as
well so here's a little proof it's a
proof that if you know B na then you can
conclude a and B now this seems bloody
obvious right of course if I know B na I
can conclude a and B but it would be
nice to actually have a formal proof so
here's the formal proof it says okay
let's assume B and a and on the Left we
say belief assume DNA we certainly know
a and the right we say well from for
soon B and a we can certainly conclude B
that's using the two different
elimination rules now we've proved a and
B so we know a and B and now we can
discharge our assumption so with no
assumptions now we know that B na
implies a and B now the key thing that
Genson did that was amazing is he had
the insight that the rules come in pairs
and the pair's cancel out and he used
this to prove what's called the sub
formula property which says that if you
have a proof you can always normalize it
by applying these rules so that the only
formulas that appear in it so the
formulas are our profits
here are going to be the conclusion the
hypotheses and parts of those what are
called sub formulas no other formulas so
I'll give you an example of that in a
minute
but let's look at the simplification
rules the top one says okay I assumed a
I prove V so I know a implies B and I've
also got a proof of a so I can conclude
B but there's a simpler way of doing
this right we don't need to assume a we
were just given a proof of a so
everywhere in that proof on the left
that you have an assumption that a is
true just replace it by the proof you
were given on the right that a is true
and now we've got another direct proof
of a another direct proof of B that
doesn't use the formula a implies B so
we've got rid of a formula that doesn't
appear in the conclusion or the
hypothesis similarly if from a and B you
can conclude a and B and then from that
you conclude a well there's a much
simpler way of doing that right just
you've got a proof of a just use that so
proofs can be simplified so let's do an
example of that here's a roundabout way
of proving a and B so let's say that
somewhere I've got a proof of BN a but
we we've just proved right that B and a
implies a and B so by modus ponens I can
conclude a and B so we've got a
roundabout proof and notice this proof
has formulas in it like B and a implies
a and B that don't appear in the so
we've got two undischarged hypotheses
here DNA on the left and some discharged
hypotheses the DNA which has a little
Zed on it as discharged by the rule
arrow implication which has a Zed saying
right this is the rule that discharges
those hypotheses we've got two
undestroyed type auspices B and a and a
conclusion and B and a bunch of other
stuff including stuff like B and a
implies a and B that doesn't appear as a
hypothesis or a conclusion can we get
rid of it well yes
because our second-to-last rules in
introduction and our last rules in
elimination so we can just do what I
said and take those two places where we
assumed VNA and replace them by the
proof of B and a on the right so it
simplifies down to this and once we've
taken that proof on the right and moved
it on the top
yes we've now got two places here where
anti is on top of an D so we can
simplify again and now we've got a
direct proof this is a much simpler
proof of the same thing now notice that
when you substitute something into a
proof it might actually have more nodes
in it but it will always be simpler in
the sense that you've gotten rid of a
sub formula and you keep doing that till
you've gotten rid of all the sub
formulas and that always works
why did gets in care about this because
it says okay well just doing the proofs
could always be done in a direct way
that they weren't roundabout that's kind
of cool but in particular you can do the
following one of the formulas you have
is false and a logic is consistent if
you cannot prove false you really don't
want to have any proofs of false sitting
around if you did have a proof of false
then what the sub formula property tells
you is the proof would look like false
and only consisting of sub formulas of
that well there are no sub formulas of
false right it's like what part of no
don't you understand so it's very easy
to look at the proof rules and say ah
okay there's no proof rule that ends in
false and therefore you couldn't get it
in any other way so it gives a simple
proof of consistency among other things
now at pretty much the exact same time
that Jenson can was coming up with this
new way of formulating logic this was
when church was coming up with lambda
calculus now church originally used
lambda calculus as a kind of macro
language for a logic and it turned out
flat
the calculus is really powerful in fact
it's so powerful it let you write down
the equivalent of infinite formulas and
with those you had an inconsistent logic
one that could prove everything so that
was kind of bad but he did write in his
original paper it may have uses other
than its uses of logic
in particular turned out to be good for
defining algorithms but also he wanted a
consistent system and to do that he's
the same way of getting rid of paradoxes
that Russell used he used a type theory
so in 1940
Church wrote down the simply typed
lambda calculus so here I've got terms
of the lambda calculus in red and
conclusions in blue and as well as
functions I'm now dealing with pairs you
can also deal with things like record
variance and pretty much every data type
you name you can build up out of these
ideas but I'm just going to show you
functions and pairs so lambda X n where
X is a term lambda xn is a term it's a
function it's it's a function that given
a value of type a returns a value of
type B the function from A to B and what
does this mean well X must be a variable
of type a and it must appear in some
term and whose type is D so lambda xn
would be a function from A to B and then
if L is a function from A to B and M is
an argument of type a well of course if
you apply L - M the result has type B
and a pair is built from two terms of
types a and B and gives you an a/b pair
and of course first and second extract
if you've gotten a B pair first would be
an A and second would be a B it's a very
simple definition of a small fragment of
simply typed lambda calculus so here's
an example of a program lambda zed
returned the pair ii of zed first of zed
so what does this do it swaps the
elements of the pair and so it's type is
going to be taken ba
hair and return an ad pair right so at
this point I'd like you to all reach
under your seats
you should find there some rose-colored
glasses please put on those rose-colored
glasses you will then only see the blue
bits and off the red bits and that
should look kind of familiar right so
the blue bit of what church did is
exactly what Denson did now it actually
took a long time to see this because the
proof I showed you that of Jensen's
result Jensen didn't prove it that way
he had to invent sequent calculus to
prove it it's kind of ironic Jensen
needed a roundabout proof to show the
absence of roundabout proofs but profits
a little later came up with the form of
proof that I showed you and then Church
came up with this and there are rules
for evaluating lambda expressions
they're very simple the first rule says
if you've got lambda xn applied to M
what does that mean well take the actual
parameter M and substitute it everywhere
for the formal parameter X and what does
it mean given an MN pair to take the
first component of course it just means
return m and again if you put on your
rose-colored glasses you see that
evaluation corresponds exactly to
simplification of proofs the one thing
you need to know is that this process
terminates that was actually first proof
by Turing of all people so remember the
hard thing about the halting problem is
you can never solve it it's undecidable
there's no algorithm that tells you if a
given algorithm whole torn off but if
it's in simply typed lambda calculus if
you've typed it and you're not using
what's called the fixed point operator
that is unlimited general recursion yeah
that's the idea that girdle came up with
then you're guaranteed that it
terminates so we have functional
programming languages where we include
the fixed
I'd operator and you can write every
general recursive program let's do
everything a Turing machine can do but
we have other ones which are simply
typed like this and examples of such
languages or things like ACTA or the
proof system in caulk and those are
guaranteed to always terminate so the
holding problem which you think of as a
very hard problem is actually completely
solved by this idea of types and then
here's an example so here's the program
that swaps two elements of a pair apply
to a particular pair and of course we
just substitute in the argument Y X for
the formal Z and we get that and then we
simplify again and we get that so notice
what these rules are telling us is that
as we evaluate a program it stays well
typed so what have we seen propositions
in logic correspond to types in a
programming language proofs in the logic
correspond to terms programs in the
programming language and simplification
of proofs corresponds to evaluation of
programs so it's not as shallow idea
it's a deep idea with a lot of
structures propositions as types proofs
as programs simplification of proofs as
evaluation of programs hmm you say
that's really cool but hey you know it's
just kind of an accident that it happens
once like that right well no it's not an
accident
oh so this is sometimes called the curry
Howard isomorphism so this is a drawing
due to Luca car deli illustrating the
correspondence between types on the top
and formulas of logic on the bottom so
the idea showed up many times in the
20th century you can find it in the work
of Breuer hiding and Kolmogorov the
intuitionists
but it also it was noted by Haskell
curry and then the form that I'm showing
you really was clarified by William
Howard who then went on to say well wait
a minute if implication and correspond
to these things what do for all and
there exists correspond to and that came
up with a new kind of type that hadn't
existed before called dependent types
and these are the basis today of many
proof systems so it's often called the
curry Howard isomorphism it's often
called the BHK interpretation it's
sometimes called propositions as types
anything that's really important will of
course have lots of names so there's
Howard's original paper which wasn't
that published in a Fester if dedicated
to curry so there's what I showed you
write propositions as types proofs as
programs and normalization or
simplification of proofs as evaluation
of programs as I said well fine but you
know it's just for this one particular
logic in fact I showed you only works
for something called intuitionistic
logic it's just kind of an accident
right well no it works for everything so
here's just a couple of ways in which it
works and the interesting thing to note
here is for instance of the logician
Hindley came up with type schemes and
the computer scientist Robin Milner came
up with the type system of standard ml
which is the basis for the type systems
used in all functional languages now
like f-sharp and Haskell and notice that
hey a logician is now called the
hindley-milner system right but a
logician and a computer scientist
independently discovered the same thing
there's something called polymorphic
lambda calculus which is actually the
basic for generics in java and it was
discovered once by the logician rod who
called it system F and once by the
computer scientist Reynolds who called
it polymorphic lambda calculus so Curie
Howard is a double-barrelled name that
predicts there will be other
double-barrelled names
every good idea will be discovered twice
once by a logician
and once by a computer scientist
and pretty much every functional
language you can name has as its core
the lambda calculus that's pretty much
the definition of a functional language
and interestingly pretty much every
proof assistant you can name such as
 or Agda or what-have-you
has at its core dependent types as and
using lambda terms to represent proofs
in exactly the way that we were
discussing that goes back to the autumn
of math system of de bruyne in the 1970s
some people actually don't call it curry
Howard they call it curry Howard de
Bruyne so it's a very powerful idea and
of course just like any programming
language there are bits in all of these
things they're bits in all of these
things that are completely arbitrary but
their core is not arbitrary their core
is something that was written down once
file logician and once by a computer
scientist that is it was not invented
but discovered most of you use
programming languages that are invented
and you can tell can't you
so this is my invitation to you to use
programming languages that are
discovered
so Turing's important contribution was
some philosophers philosophy so I'm
going to indulge in a wee bit of
philosophy to conclude let's say that we
tried to talk to aliens we've actually
done this right this is a plaque on the
Voyager and this diagram on the left is
to try to where the show is relative to
various pulsars there are marks on that
which are actually in binary giving you
the frequency of the Pulsar and of
course the length of the line is the
distance of Saul from the various
pulsars and then on the right there's a
picture of some people now if aliens
were to look at this right the bit on
the left they would probably be able to
work out ah that's the length of the
line is distance and they could probably
work out binary numbers and that's
frequency and there's a little thing
saying oh it's frequency of hydrogen
they could probably work that out the
thing on the left well that would depend
right on what the aliens were like if
Star Trek is correct then the aliens
would look at the bit on the left and
say oh they look just like us except
they don't have pubic hair
but if aliens its Star Trek's not
correct and aliens are really alien they
just might think that the bit on the
right is some scribbles that they cannot
decipher it all depends so some things
you can work out easily and other things
you might or might not be able to so
let's say we tried to communicate with
aliens in a programming language so
there's a movie called Independence Day
and in it they destroy the aliens by
giving them a computer virus and there
is the computer virus and if you look at
it closely you can see that it's written
in C it's actually a dialect of C that
only has open brackets so this movie
came out in the mid-90s how do I know
it's see and not Java well this was the
mid-90s that was before Java spread
throughout the known universe so this
seems kind of unlikely right whether
it's C or Java it's unlikely that you
could program an alien computer using it
right but what about lambda calculus
what about lambda calculus lambda
calculus isn't invented it's discovered
if aliens know the fundamentals of logic
if they know the rule of modus ponens
then they must also know lambda calculus
so if we sent them on a plaque a formula
in C I think they might have trouble
deciphering it but if we sent them a
formula in lambda calculus I think that
is something that they would be able to
work out so lambda calculus would be
more like the thing on the Left that's
easy to decipher than the squiggles on
the right so right we should call lambda
calculus the universal programming
language well let's think about that for
a minute it's become common these days
to talk about multiverses this is from a
play called constellations which has the
coolest stage direction in it I've ever
seen this it only has one stage
direction it says
vertical rule sorry a horizontal rule in
the script corresponds to a change of
universe so this has entered our popular
culture and it's also entered science so
scientists say why is the week
electronic electron force the strength
it is if it was just a little bit
stronger electrons would repel and we
wouldn't have matter and we wouldn't
have life a little bit stronger
electrons would clump together and we
wouldn't have matter and we wouldn't
have life why is it the strength it is
well because we're in a universe where
we can see it so there must be matter
and life so that's why the electron
force has to be so they reason using
multiple universes and multiple universe
might have something like different
electron force different gravity that's
fairly easy to imagine what about a
universe without logic without modus
ponens I find that very difficult to
imagine so we really cannot call I'm
sorry to say but we can't call lambda
calculus the universal programming
language right and the reason we can't
do it is because calling it universal is
too limiting
so in conclusion I'd like you to
remember is that when you've got a tough
job you should think that this is a job
for lambda calculus
thank you very much and I will now take
some questions
yes I have tried many times to put this
shirt online and every time I've done it
the online firm has said you can't do it
that's a copyright violation so if you
can tell me a way that I can do it let
me know and I will do it
any other questions yeah oh not just
lambda calculus they've ignored lots of
things right so the question was I
claimed that industry has ignored lambda
calculus for many years and the question
is why is this the case it if you look
at good ideas like garbage collection
garbage collection came around in the
late 50s early 60s it finally became
prevalent in the mainstream community
with the advent of java and even then
many people fought against it so it's
like 25 30 years maybe 35 years before
garbage collection got into widespread
use um even though we think of it as a
very rapidly moving field fundamental
good ideas often take a very long time
25 or 30 years to be adopted which I
will rant now for a minute
in the UK we're supposed to do impact
studies to show the impact of our
research and impact must be an idea that
was published 20 years ago and used
within the last 20 years so I want to
use oh okay
Java makes use of generics which comes
among other things from the work of
Robin Milner on the hindley-milner type
system and I helped contribute to
generics and Java so that's clearly an
impact story well no because the
original work by Robin Milner was too
old account it was more than 20 years
old if you look at what happens in the
development of logic rule comes along in
the mid-1800s Fraga and hilbert around
1900 all the stuff I showed you around
1935 and it was 1935 that gehen sin' and
church came up independently with
traduction in lambda calendar calculus
it wasn't until in 1908 1960s 1969 that
Howard wrote this down it was just
circulated as a Xerox note it wasn't
published until 1980 so that's like 45
years it takes a long time for good
ideas to get out and so it's worth
having a long perspective if you want to
have the best and most interesting ideas
if you want things that are discovered
and not invented that was an excellent
question
thank you it let me rant any other
questions yes
so the question was in curry Howard we
have a correspondence between logic and
lambda calculus are there any others and
of course right we saw a few others
right so on the left here we have lots
of different varieties of logic and on
the right we have lots of different
features of programming languages by the
way there's one thing that's emitted
from here which is the most important
one which is distribution and
concurrency and of course there are many
many different solutions to distribution
and concurrency what is the right one
wouldn't it be great if there was some
logic that correspond to the
distribution and concurrency that might
give us a hint as to an idea that is
discovered rather than invented and
indeed linear logic on the Left seems to
correspond to things called session
types on the right and in fact that's a
major focus of my current research so
there's a possibility that we will be
able to discover an approach to
concurrency and distribution again that
is discovered rather than invented but
that's still ongoing work of course once
we're done it will then take another 25
years before people adopt it
so that wasn't quite your question
because these are all different
varieties of logic and different
programming features your question is
does it happen with other things well
yes it happens with things like
thermodynamics and information theory Oh
is there a third column or fourth comp
yeah people so many people say the
category theory is the third column and
that corresponds to both of these things
so yes there this actually goes deeper
there's some very nice paper by
physicists talking about these
correspondents because they get
exploited in quantum physics so yes this
extends through even to things like
quantum physics so thank you very much
for that question
that's an important point but there
might be other things waiting to be
discovered
question Oh brilliant question so this
fellow is those things look the
logicians always get there before the
computer scientists will the computer
scientists ever get there first that's a
very good question
I suspect the logicians always get there
first because they've been at it longer
but for instance linear logic was not
discovered twice linear logic when
Sharad came up with it was actually
published in the journal theoretical
computer science because already this
correspondence between linear logic and
concurrency this notion that there might
be something there was there at the time
he discovered it so in that sense the
two came together and even it turns out
John Reynolds had done something almost
like linear logic just a few years
before called syntactic control of
interference in concurrent systems so
some people refer to Reynolds coming up
with this idea as just before rod as
Reynolds revenge yeah oh very good
question are there any probabilistic
correspondences I suspect so but no I
don't know what they are that's
something I don't know about and I think
that'd be a very interesting area of
research yes
my favorite question what does this say
about the impact of computer science on
knowledge in general right this a lot of
what we do day to day has arbitrary bits
to it this I've tried to explain to you
is deep and not arbitrary we are
discovering things rather than inventing
things clearly that has to have
implications everywhere and there's a
growing movement that says look the ways
of thought applied in computing the
things that you guys are all good at
give us new insights into how
information is structured and
structuring of information is not going
to just be important for understanding
how computers should be designed it can
also be important for understanding how
the universe works so this idea of using
ideas of how information is structured
to examine many many different fields
goes by the name of informatics and some
people work in departments of computer
science I actually work in a department
a School of Informatics and I think
informatics is a much better word to use
right computer science there are only
two things wrong with it the word
computer and the word science right
because it's not about just the computer
it's about patterns of information and
you don't put science in your name if
you're a real science
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>