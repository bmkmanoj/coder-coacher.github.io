<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Just-So Stories for AI: Explaining Black-Box Predictions&quot; by Sam Ritchie | Coder Coacher - Coaching Coders</title><meta content="&quot;Just-So Stories for AI: Explaining Black-Box Predictions&quot; by Sam Ritchie - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Just-So Stories for AI: Explaining Black-Box Predictions&quot; by Sam Ritchie</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DiWkKqZChF0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everyone welcome to just-so stories for
AI I'm Sam I work on the machine
learning infrastructure team at stripe
and what my team does is we build tech
that helps other engineers build models
to catch fraud we'll go into this more
but that's that's all I need to know for
now about me so the outline of the talk
is we're gonna go into that we're gonna
go into some techniques for detecting
fraud we're gonna go into some of the
choices you can make when you're an
engineer who's attacking this problem
the typical thing you hear there are
interpretable models that you can
understand and then there are these
black box like super accurate like
neural networks random forests things
like this they're just hard to get but
to get the job done so we're gonna talk
about why that's not necessarily true
some of the most complicated models
often are the most explainable I'm gonna
anchor that with the technique we use at
stripe I think it's novel but a
technique we use at stripe to make very
accurate predictions on fraud for our
customers and then offer them an
explanation of why this you know insane
model did what it did whether it's a
true explanation or not you know that's
up to you there's a number of ways to
interpret this and finally we're going
to talk about why this is an important
problem why this might be like the most
important thing you could be thinking
about now if you work in these systems
this is the door into these I gotta have
to say the AI safety thing into ethics
and morality and free will and you know
all this stuff all those things ok so
this is a you know let's let's talk
about you you are you know you're young
you've always wanted to start a business
you love animals you're you're itching
to sort of affect the world in some way
but you're a little lost you're browsing
Reddit one day and you come across this
photo so you realize that like this is
real like people have this thing they're
doing where they're grooming their dogs
and geometric shapes and this is like
just crushes you you know you realize
you found your life's purpose you're
going to stop this from happening so you
come up with a business plan you were
going to take money you're gonna
put it into ad campaigns or something
it's gonna go to the dogs love and
happiness is gonna come out the other
side and this is my pitch or what stripe
is if you don't know stripe is a company
that makes it's so easy to start a
company and take money from people that
even this idea can be a wonderful legit
business really stripe that is what
stripe is but stripe is also a company
that develops tools for people working
with money you shouldn't have to worry
about it fraud detection is one of those
components that we have so you're
humming along things are going great
you're I guess accepting some number of
donations and fraud strikes you start
seeing in your dashboard charges that
don't really make sense there they all
and this happens to donate this is the
purpose for the example sites that take
donations often see a kind of fraud
called card testing where people will
you know download from the internet like
a big list of card numbers or just like
try them which seems insane but if you
let them get through it'll work and they
just start running like one dollar
donations on donation sites they're
taking advantage of your love for these
these puppies to test out cards and the
goal is to come up with like cars that
pass the dollar donation so they can go
use them somewhere else and steal money
this is not good for you this keeps you
up if you hit one percent or so you're
gonna start to get you know have issues
with with I guess Visa or card issuers
but through stripe so you pick this
component off the shelf this is the
thing I work on this is a component
called radar so radar is what stripe
gives you to stop fraud so yeah again I
build infrastructure this kind of sits
behind this thing so the main weapon you
have is this this these rules so you're
able to write rules that will you know
effectively either block allow or put
into a review queue different charges so
a rule is a model a rules the first
model we're gonna look at today and
we're going to develop this concept rule
is a model with a built in explanation
so it's a model in that you put in
information about the charge so things
you
they get compared to these rules in this
case if the card is a prepaid card and
it's over a thousand dollar charge and
it's not from the US go ahead and block
it so the input is those three features
they're called in the output is a
decision to block or not and in the
bottom you have some stats about what
would have happened if you deployed this
rule and did in to production to block
your charges
so again the question then is how do you
come up with these things
accuracy is not enough
typically fraud rates are low you're not
going to get you know what this picture
shows we have fraudulent charges in the
gray area on the Left we've got
legitimate charges on the right and this
circle in the middle is like but just
some rule let's say that in this case is
getting some fraud and getting some non
fraud so like when you come up with
these rules you're not in your head
thinking about accuracy because if you
have one percent of your charges as
fraud you get like 99% accuracy out of
the box that's it's nice it sounds nice
but you're not catching anything again
this is if you decided to do no rules
what you're really trying to do is
optimize these two metrics that I'm
gonna introduce now because we need them
later one of them the idea is that you
want your rule to be specific enough
that it only lives on the left side of
this so your rule circle you want to
move left you want to catch when your
rule you know does its thing only fraud
right the problem is that fraud doesn't
have like a really obvious there's no
algorithm for detecting fraud you're
just kind of looking at patterns and
guessing so you might say you know
dollar charges from this place now the
more specific you get the more charges
you're gonna hit but the less overall
fraud you're gonna catch so you kind of
play this game with yourself where
you're coming up with a bunch of rules
each of which lives in this left-side
overall you want to cover as much of it
as possible but you want to try not to
block good charges so that first idea is
called precision how precise is your MA
is your rule how precise is your
decision how like if you have this sort
of one shot one kill like that's a
precise rule and recall is how much of
your overall fraud
you catch with a roll so anyway why is
this not a great solution like the
reason why is that fraud evolved 'he's
right so you come up with a rule it's a
great rule you really like you read on
the internet basically you did your SEO
homework for fraud you deploy the rule
and suddenly people stopped like pasting
their credit cards they write a little
script that just types the number in
okay your rule fails meanwhile you're
still catching the stuff but it was on
the right before that we cared about so
fraud is adapting all the time you know
in your little example of your business
now you're spending all your time in the
dashboard just being suspicious of
everything like this is not a good
situation so how can we make this better
there's only one exam there's only one
answer to this question the answer is to
sprinkle machine learning on the problem
so thank you xkcd the answer is
basically to bring in more powerful
models that can work on the data we have
behind the scenes so this is what again
this is kind of the full product of
radar you have rules and then you have
us working to do what we can to lessen
that load and catch fraud so like what's
ideal here how do we want to use machine
learning ideally we would catch all
fraud right and ideally for every charge
we block we would offer up an
explanation of why we did that and as we
saw keep reiterating this a rule has a
built in explanation where why did you
block the charge well because you told
me to this is less obvious as we move on
to all of the different options for
machine learning models that you can
possibly grab so this is a heat map
basically the darker something is the
more it's able to kick the butt of other
models the higher up you are the more
accurate your models are the lower you
know often there's this correlation
between models lower in this list are
more interpretable models higher up or
more accurate so you have this tension
here that often people talk about I just
have to say when I family made this
slide I was surprised to see that
there's a passive-aggressive model for
from the bottom I don't know what this
is
come find me afterward and tell me if
you do so
you know to anchor this let's talk about
the decision tree right in the middle so
again a model is a function little Scala
code a model is a function from features
so things you know about the world to
some prediction of fraud right ideally
like this is what you want to be able to
ask for every charge that comes in how
likely is it that this charge looks like
fraud that we've seen before how do you
implement this function you can't just
sort of write it there's no obvious way
to do this so a machine learning model
this function the process of training a
machine learning model is the process of
using a bunch of your data you have to
basically implement that function for
you so you take your training algorithm
you take a data set you try to soak up
as much information and meaning out of
this as possible and the result of the
training algorithm is a model which
again is a function that hopefully will
in this case when you pass it in these
red charges so as these things come
through the pipeline you want a high
number a high probability of fraud out
for those charges and a low for the
other ones so you want to model that
matches the patterns that were in your
data before again they might change but
the better your model is the more
accurate the better it's soaking up all
this meaning the more you're gonna catch
these very very subtle edge cases that
rules just just can't grab okay so
decision trees a decision tree is kind
of the second model we're looking at
after rules we'll talk about how this is
actually a collection of rules but what
a decision tree is is like a 20
questions game basically like you make
this thing your features come in the top
you follow the questions and what's at
the end what's that the the leaves of
the tree is a percent chance that it's
basically of the charges that you of the
things you look the most like how many
of those were fraudulent right and what
you're looking for here is you want
every path down this tree to be as
specific or as precise as possible so
you want it to be that there's a very
high confidence or a very low confidence
basically you want to be very sure once
you've walked all these branches and
overall you want every charge to sort of
to slot in the tree so in this case
what's nice about this choice of model
say we were to pick this and we were to
implement this behind the scenes to be
kind of the backing of our machine
learning right we have this beautiful
property that every path through the
tree is also a rule right so if we show
if we block fraud for the user we can
then present to them this explanation of
why we did this we can say you know if
you had taken this rule that's the
particular path through the tree and if
you had deployed it at this time it
would also have caught this fraud so
right now like if the amount in USD is
above 20 this is the kind of most the
leftmost branch in the in the tree the
only difference between these two is
that the rule just straight up blocks or
doesn't block there are these
probabilities at the edge of the leaves
all you need to do to turn a decision
tree into a set of rules is just pick a
threshold you just sort of behind the
scenes choose something and you deploy
it that way okay so a decision tree
what's good about decision trees
decision trees are interpretable
decision trees like what does this mean
it means that you can simulate the
algorithm yourself so you've got your
function you kind of have some trust if
that works
that's you know your precision your
recall but you can then look inside the
black box and you can just run the
algorithm through for any charge that
comes in so every decision has this
built in explanation and for a product
this is important for radar this is
important because you have to have trust
with the user right if we disturb look
at an example of what happens as we get
more accurate and less explaining but if
you just start blocking charges that are
really subtle you might be right but
people aren't going to know why you're
doing it and they're gonna be suspicious
that you're blocking good charges and
stop in their business so this is kind
of the trade-off this is why
explanations matter in the fraud context
so not a black box
okay so wire decision trees like what's
not good about this shallow trees aren't
very accurate like things are
complicated in life sorry like two
decisions is often not enough to explain
like why something is fraud so if you
want to be accurate with these things
you have
to grow them really really deep you have
to have a lot of branches and the
problem then is like a really deep
decision tree is not very explainable
right like if you show someone a rule
that's 50 or 100 or a thousand predicate
long it's just it's insane it's never
gonna work
they conceivably could have written it
but there's no way to hold this in your
head the other problem is that the
deeper a tree gets the more it over fits
to the data it saw in the in the sort of
crazy case every single row of your
training data can just be a path in the
tree and the problem with that is like
your new fraud probably won't look
exactly like your old fraud and so you
frozen this moment in time of what your
training you to look like it's probably
not gonna apply to anything in the
future which is the whole point of all
this final problem is that we know a lot
more about users behind the scenes than
we are really like ok releasing so
there's an 80% chance of stripe as like
seeing a card if it shows up even if you
just launched your business so you know
we can't really expose certain pieces of
data like oh this is a really unlikely
purchase for this user to have made at
this time of night from this kind of
business here's what they usually do
like this information can't leak out we
don't want them to be part of the
explanation so we're a little shackled
if we only use decision trees right okay
so we gotta go higher up we got to go to
a higher floor decision trees right
there in the middle we'll talk next
about random forests so random forests
are like second from the top this is
what we actually use this is a
generalization of the decision tree idea
and I'm gonna go through how it works
I'm gonna go through how it works it's
kind of wild just to give you an idea of
what it means when people say that a
random forest is a black box that it's
hard to understand it's hard to
understand why it's doing what it's
doing ok so the random forest the idea
here is that you the intuition here is
you have your data you build the
decision tree ok that's fine if you had
more data if you just had many many data
sets you could build and you build a ton
of decision trees maybe that would be a
more
accurate way to assess what's going on
right if stripe had 200 times the data
maybe we'd be able to grab 200 times you
know the info out of it so there's a
statistical technique to deal with this
is called bootstrapping I still don't
really have my head around like why this
isn't just an insane technique that
should not work the idea is you just
like just randomly sample the data you
have and each time you do this roughly
you'll end up you'll duplicate some
stuff you'll miss some stuff you'll end
up with roughly like 2/3 of the amount
of data you had going into every time
you do this bootstrap sampling but you
can pull as many samples as you want so
the idea is you just keep doing that you
just randomly sample your data and you
just keep training decision trees as
deep as you want these these you know
enormous tree is trained out of your
data and you just keep going and going
and going and then beyond that each tree
you only send some random subset of the
features that you knew about so you
don't tell every tree everything you
don't give it the opportunity to fork on
every possible thing you just pick it's
typically like the square root of the
number of features you just randomly
send off a batch the final thing when
you have a prediction how does the
function work so this is the kind of why
is the simulate ability question you
take your features you ask every tree
what it thinks and then when you're done
you just average all the answers
together and this turns out to be like
really really accurate sort of strangely
I would say it's very flexible it'll
pick up on patterns in the data that are
you know very very subtle and the
decision tree would over fit on and you
can basically just keep cranking up the
number of trees and keep making them
better so this is good the con is like
you can't simulate this as a black box
right I was wondering if I get a chuckle
on this it's like the only place maybe
that I could break this out okay so why
is it a black box like it's a black box
because it's not simulate a bowl that's
typically what people mean I would say
why is it not simulate able one model we
have out now at stripe has a couple of
new trees each tree has 12,000 splits
thousand possible decisions to student
math two and a half million possible
explanations of why this charge was
fraudulent so this is like not okay
right this is like asking someone why
they you know why they pushed you down
the stairs or something and they just
give you a printout of the entire state
of their brain at that time like that's
the explanation that's it that's why
they did what they did
totally not okay as a justification and
this is what would happen
naively if we don't have a way of
explaining these black boxes I mean this
is I actually hate this this is really
what we do if we don't have an
explanation you just say you know what
there's a lot of contributing factors to
the risk level of this payment based on
activity across the stripe network
there's a lot of stuff it's a
complicated world out there just just
relax just trust the model it's kind of
Adrian izing and it doesn't really work
when you also have these rules that are
so inefficient and not good but I just
it's such a beautiful story I just want
to believe it so what we want is
something like this this is a little bit
better right you want something that
like this is an explanation that says
this card here's the last four numbers
has been linked to a large number of
attempted payments across the stripe
network over the last hour oh my god
that kind of sounds like fraud I don't
know if it is I'm not sure if that's the
real reason but if that's what the model
thought I'll go with it another one here
it's been used from an unusually large
number of IP addresses across the stripe
network over the last 24 hours so this
is an explanation generated off of a
random forest model that kind of
explains it has some truth to it it is
true whether it was kind of the reason
the forest did what it did
I'll talk about how we came up with it
and just keep this in your mind and see
whether you agree that this makes sense
so how do you do this the technique we
used to generate these is a technique
called post hoc explanations the idea is
you just kind of give up on any idea
that you could simulate the model you
just treat it as a black box you treat
it is that wonderful pure function that
it is just forget about it and you train
another little model that
bolts onto it and tries to explain why
it's doing what it's doing right and so
the intuition behind why this is easier
is I don't know what to bring up like
conspiracy theories or but it's it's
really hard to make real predictions but
once something happens like everybody's
got a story for why the thing happened
right or I guess another another way to
get this across is this I guess my mom
will say to me like Oh what are the
chances of these people running into
each other oh you know after the fact
the chance is a hundred percent and you
can come up with any story you want to
explain why it happened right so we're
we're explaining what the models doing
but we have this lower effort task in
front of us right okay so we we started
with this paper it's called a model
explanation system it's fantastic the
the concepts it puts out are great I was
totally bowled over I have no idea
what's going on like deep in this paper
so we sort of weren't inspired by the
paper and we avi Brian Eric awesome on
my team extracted from this paper a
method of explaining blackbox models
which we'll go over now so the intuition
here like that came from this paper is
what is an explanation it's like we
talked about before like for a blackbox
model an explanation is a rule that had
you deployed it would also have caught
the same charge and possibly would have
agreed with the model on a lot of other
charges right so it's precision is high
every every explanations precision is
high and the advantage is that we can
generate a ton of possible explanations
and then when a charge comes through we
know what the model said then we can do
the conspiracy theory thing and turn
around and pick the one that we like the
most taking at a time is really hard
once you know what happened it's easy
okay so let's build up to how we do this
the intuition here is that as we talked
about every path through these trees is
a rule right so there's like millions of
possible explanations here so we do what
any good statistician we do we just
start sampling these possible
explanations any one of them might work
well and any anything goes right so we
just we use the tree as a sort of a
source of possible
explanations every split in the tree
we'll call it predicate so the name here
is the name of the feature that the
split operates on the operation is like
greater than less than equals doesn't
equal super simple and then the constant
is what the value is so you can check if
something's in a category you know is
the country in this bucket etc so for
example here unique card IP address in
20 in the last 24 hours
that's how many times is this card been
used from the same IP address in the
last 24 hours
so this predicate says that's greater
than 10 great
so an explanation like we said is a list
of these things a list that are all true
so you're you're you you and them all
together and they're sorted by how
precise they are and precision here
again I find the word like I find the
word confusing but the idea simple so
I'll just keep saying it and saying it
and then maybe you'll absorb it
precision again is like four
explanations if the explanation applies
to a charge how many of those calls did
the the you know the big the black box
model also make if it's a hundred
percent precision that means that this
explanation may be applied to a small
number of charges but every one of them
agreed with the model right if you have
low precision that means you don't agree
at all there's not a good explanation so
the structure of an explanation model as
we said is a list of explanation sorted
by their precision so every explanation
what we want out of this system is we
want to just come up with stuff just
make up explanations each one of which
has high precision so applies to some
subset of the charges very well and then
we can just keep generating explanations
until we get coverage over everything
until we get high recall the idea being
that for every fraudulent charge or for
every positive example there's some
explanation that we can pull ok this is
the most dense slide I have and like
this kind of sucks but just bear with me
this is the algorithm to do it I got it
I got to do it for this talk but it's
it's less like don't think you're
supposed to memorize this because
obviously nobody will
the goals is to show how like weird an
arbitrary this is and how unrelated this
is to the idea of simulating a model
that we had before yet the result is
actually pretty good and kind of makes
sense so okay the idea is first you just
generate a ton of possible predicates so
you you just have your set of examples
you start moving through the tree and
just just picking things out of your
millions of possible ones right step two
is you weight them all by the precision
they have so you weight them by how much
they agree with the model given some
like lower level of recall so the idea
is if you've got something that
perfectly predicts one example this is
not very helpful very high precision not
good
finally you pick like a thousand of
those weighted by their precision so you
sample those you set those aside so you
have n of these things you have a
thousand of these things and you go
ahead and you you take the you do you
get the N by n like possible two
predicate explanations out of this this
is by the way why you can't just sort of
try everything because once you start
multiplying these and getting longer
explanations things go to hell quickly
so you get your you get your top
thousand you go make your you know
million two-level explanations you go
back to step three you start sampling
those weighted by their precision and
you just keep going at this and you go
as kind of deep as you can expect people
to understand we don't like you know
target that per merchants like everybody
is you know there's there's no one who's
like specially targeted to handle like a
twenty level deep explanation sixth
tends to be the limit of people's
people's patience so you end up with six
thousand possible explanations right and
then you go through and you sort all
those by their precision you pick one
and then the final step is you go back
into your set of calls that the model
made and you like everything that your
explanation explained you flip back from
fraud to not fraud so you remove it from
your data set and then you go back and
you do it again and you do this like 80
times so there's this insane process of
just trying out a bunch of stuff that is
not tied really to like the reality of
whether something's fraud or not you're
just trying out these plausible like
potentially plausible scenario
and trying to find the most plausible
thing that fits your models explanation
and it takes a long time but what you
end up with is that you know list of
explanations any one of which could
apply to a new charge that comes in okay
so internally in our systems what does
this let us do what this lets us do is
whenever a charge comes in we're able to
give a prediction for all the features
we know about the charge we're able to
give you know a threshold and then we
can supply this list this explanation
which is a list of these predicates
right so why was the prediction 48
percent well the real answer is all the
connections between the neurons and who
knows what the hell is happening but a
plausible explanation that might also
explain the same behavior is that you
know the card was used on more than 10
IP addresses in the last day the billing
country did not match the card country
and it was a MasterCard obviously and
then we diluted a little more like you
could just show this but we dilute it a
little more we just take the the top
thing which is the most precision and we
go ahead and we render it we just have
like a you know template for every one
of these so this becomes you know it
strips out even more information and
says this is an unusually large number
of IP addresses that this this card was
used from okay so in like this always
makes me uncomfortable to think about
because in what sense like is this an
explanation of what the model is doing
like there's no mention as the truth
here we're not talking about like you
could just as easily let let's put it
this way
you could just as easily train an
explanation model on the false positives
of the real model and you could offer
someone this is not a product anyone
will take it's stripe I think you could
also give someone like your plausible
reason about why the model might be
wrong you know so this explanation for
why you might want to doubt the machine
learning tech that's totally fine to
like the explanation machinery does not
care what the truth is it's just trying
to predict it's just trying to justify I
guess what's going on
so I think these just-so stories this
story this is the story of how the
elephant got his trunk it had nothing to
do with evolution the elephant got into
a battle with alligator crocodile who
grabbed its trunk and stretched it so
far that now it's long and it seems kind
of plausible to me it's a fantastic just
snow story so we're gonna talk about
next a few other ways in which black box
models are explainable but just kind of
chew on this idea that this is an
explanation but it's designed to be a
persuasive explanation that's not tied
to what's actually going on inside the
model whatever that means you can
explain anything well even if there are
systemic errors underneath that you
might be missing so final comment on
this is it's like a totally
inappropriate technique by itself a user
should only trust this if they have some
reason to believe that inside stripe or
whatever organization somebody else is
working hard to ensure that the model is
actually being accurate and fair right
so let's explore that so the observation
here what we're gonna go into and what
this the model we just talked about
reveals is that black box models are
explainable in certain ways they have
much richer internal representations
than a simple model like a decision tree
so overall the question of like why did
the model do what it do do what it did
it's not really answerable globally
because it's meant to absorb information
from huge amounts of training examples
but you can learn a lot you can explain
individual decisions very very well
there's a technique called lime which
it's got a great Python library it's a
chance for locally interprete Balad
elects planation lime is based on this
idea that you know what of what a black
box model is doing is it's carving up a
classifier anyway it's carving up the
the boundaries between fraud and non
fraud and it's very very complex and
nonlinear way in the feature space right
so you've got this incredibly
complicated function but what that means
is that for any individual example
you're pretty
likely to be close to some decision
boundaries right so the idea here is you
know what would have to change about the
individual charge that would make it go
from fraud to not fraud right so you see
you can if you have this black box or
the rich internal representation you can
start asking at all these questions and
you can get out a pretty nice answer and
this has a beautiful like image data is
really nice for this right because
there's no way even our old explanation
system would work where the features are
individual pixels but okay so here's
here's an example from the line paper
right this is a husky that was
classified as a wolf raise your hand if
you think this is like a good
classification I mean it's a it's a Mis
classification but like do you kind of
get why it did what it did said right
like it kind of makes sense so an
explanation for an image is the pixels
that if they were different would most
affect the outcome and a decision right
so for this man you know the explanation
is actually the snow around the face
totally not obvious again you have this
machinery for coming up with
explanations turns out that in your
training data you had a lot of wolves
sitting in the snow and a lot of Huskies
on couches or whatever so this one
example kind of invalidates say you
worked on this for a few months it's
like crushes your life but it just tells
you everything about what you need to do
to go make the model better so in that
this cannot happen for a simple model
another example here using the same
software by the way this is a prediction
probabilities of whether this email was
from like I think these are from Usenet
mailing lists so whether it's like an
atheist or a Christian list so it gets
it right it's like well we're talking
about Darwin fish these are these are
some atheists right here I'll tell you
that why did it do it well there's a 15%
of the decision was from the word
posting hosts contributed another 14%
like not helpful right you've over
fitted heavily on this aspect of your
training set and
only a black box model is sort of able
to be explained in this way final one we
got to go but it's not really tied to my
thread but it's just so awesome
another there's this technique called AI
rationalization the idea is you just you
go back to the first sort of thing you
disconnect any meaning in the model from
what's happening and you try to get say
a model that's playing like a game like
Frogger you try to get it to explain
what it's thinking at that point in the
game right so an example of the output
of this this is an AI that can play
Frogger it's gotten to the logs and so
it's like printing out its
stream-of-consciousness
so this is amongst the logs dang this is
hard
I need frogging to avoid death from the
edges it's beautiful there's a great
paper that talks about a lot of this is
called the Methow sub model
interpretability
and it discusses this case of how
blackbox models are often much more
available to be examined but a strange
thing comes up when you read these
papers you get these comments about like
humans being blackbox models that have
post-talk explanations only right so
human decisions might admit post hoc
interpretability despite the blackbox
nature of human brains you're revealing
a contradiction between two popular
notions of interpretability like this is
what we've been talking about but the
human mentions little startling Peter
Norvig
who wrote the book on AI has a similar
comment in his article on how he
questions the value of explainable AI he
says you can ask a human what they're
doing but you don't really have any
insight into their decision-making
process they do what they do and then
they just make up something they make up
some BS as you dig deeper in this rabbit
hole this is an example some of you may
have heard of this is supported by the
research in Roger Sperry and 68 has a
bunch of studies on split brain patients
this is a case where you have someone
with grand mal seizures the only way to
relieve these seizures that sort of rip
across both hemispheres of the brain is
to cut the corpus callosum so you
totally isolate both hemispheres and
bear with me have you seen this it's
just so weird and awesome the experiment
you can
you then is you you you block someone's
visual field right your left and right
eyes are connected to different
hemispheres you flash the word egg in
front of someone's left eye their right
I can't see it you ask them what they
saw and they say I didn't see anything
nothing happened right
and then you say okay well reach behind
this screen with your left hand and pick
out the thing you didn't see okay that
doesn't make any sense but let's do it
most of the time people pick the egg
when they've seen the word egg right and
you say what are you holding and the
person says I don't know I I can't tell
I just picked up an object right you
show it to them and you say why did you
do that why did you pick the egg in your
hand and every time somebody has some BS
explanation like why I had extra
breakfast this morning like I must have
done that what's what's really happening
is I mean there seems to be no other way
to explain this except you have these
two independent seats of consciousness
in your mind right and your right
hemisphere is doing the thing it has
memory its conscious and you're just
sort of watching it the part of your
brain that's responsible for language is
watching what's happening and trying to
take ownership of the decisions very
weird so the the sense you get reading
about models and realizing that you are
a model is movies 20 years old I can't
explain this joke if you don't get it
you just have to see it it's so crazy at
the end so you know this is this is an
important like quite a concluding thing
there this book homo Deus is is
brilliant and lays out that there's like
an ism behind this idea the idea is that
we are on the same playing field or the
assumption in these comments is that
we're models you know these these
machine learning models are models we're
on the same playing field and we're
rapidly becoming outdated right like
everything is information everything is
algorithms the whole point is to make
better decisions why should it matter
whether we're the ones making them or
not so I mean we did this with driving
right like in a few years humans are not
gonna be driving like you're not gonna
put an eight behind the wheel when you
can have a car driven by an algorithm
so once you get the goals right like why
not give it why try to explain your
models often what people just if like
this line of thinking is often
accompanied by this thought that like
this is a harmful restriction on these
models right like there's a metaphor
from a book by max tegmark they imagine
yourself as an adult living amongst four
or five year-olds and like everything
you do about farming and getting food
and like creating society you have to
find a way to explain to these like
little four-year-olds running around
like at some point this is just going to
become totally tedious and if only you
could just realize like just go do what
you needed to do and not have to explain
it the your shackles would be broken
you see comparisons all the time to
medicine it's a technology that we don't
really get in a lot of cases but it
would be immoral to withhold antibiotics
so I think the crux of the answer to
this sort of critique of explanations of
needing explanations is the question of
goals right like for most things today
we don't encode we don't know how to
encode everything we care about in the
goals of the model so this is the
another quote here in the rush to gain
acceptance for machine learning and to
emulate human intelligence we should be
careful not to reproduce pathological
behavior at scale so explanations are
important because they help us clarify
what we care about there are things that
there are objectives we have that if we
don't encode them in the goals of a
model the model is not going to care it
doesn't know anything about meaning
right so explanations are almost the
tether we have on maintaining some
relevance rather than just letting
things spin off into meaninglessness so
explanations are the crux of a lot of
approaches to AI safety research DARPA
Scott a program Berkeley has the center
for human compatible AI there's a nip
symposium on this there's so much like
that people are honing in on this and a
big thing that I've been thinking about
lately is the there's a this is a paper
about it but they use general data
protection regulation has put into law
this idea that you have a right to an
explanation from an algorithm makes
decisions about
about your life so an explanation is a
civil right from this point of view now
I hope that in this talk we've talked
about a few kinds of explanations you'll
see that this is it's not obvious this
is a good idea but it's not obvious that
this is the solution or that this is
going to do what people think
Norvig is real point in this in this
essay or in this interview was that you
know say applies for a loan and gets
turned down whether it's by a human or
by a machine the explanation is not tied
to reality the explanation might be the
doesn't have enough collateral the read
explanation might be something about
skin color you can't tell from the
explanation right so what he's actually
recommending in this which is the
concluding point I have is that
explanations are important again is this
way to potentially clarify our ethics
and what we care about and as wonderful
as these beautiful technologies that can
just predict and you know generate
efficiency for us our explanations let
us see outcomes judge by interrogating
these blackbox models whether or not
these things are tracking the things we
care about in their societies we can
then go bake them back into our models
and almost formalize our ethics in a way
that we've we've never really had to so
in conclusion we talked about a lot we
talked about some models I hope I've
convinced you that black box models are
in fact very explainable don't trust
anybody who tells you otherwise this is
a really interesting accessible area
it's a great window into you know
questions of AI safety it's something
you can be doing now and I would almost
say that if if you're working with
models like you should be building these
into your products so be suspicious and
yeah come talk to me if you want to know
more about this and thank you so much
everybody</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>