<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;PyTorch: Fast Differentiable Dynamic Graphs in Python&quot; by Soumith Chintala | Coder Coacher - Coaching Coders</title><meta content="&quot;PyTorch: Fast Differentiable Dynamic Graphs in Python&quot; by Soumith Chintala - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;PyTorch: Fast Differentiable Dynamic Graphs in Python&quot; by Soumith Chintala</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DBVLcgq2Eg0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is summit I work at Facebook as
a research engineer and
today I'll be talking about high torch
thanks for coming this is my first
strange loop and it's it's my first
programming conference as well so it's
it's a very new experience I try to show
only only go to machine learning
conferences so I tried to make this talk
usually I give my torch talks to the
machine learning audiences so I try to
make this talk a little bit more about
general engineering and the challenges
we face by building it so this talk
needs a little bit of context before I
talk about PI torch itself I need to
talk a little bit about deep learning
and neural networks and so I'll be doing
that in the first few minutes of the
talk and then I'll be going towards PI
torch just generally about the package
how it works and the last section would
be about general perf like what we did
in terms of engineering and PI torch to
to make sure that our the performance of
PI torch is really fast and challenges
state of the art so starting with deep
neural networks the first first few
slides are really to bait you to be
interested in the talk they're just
pretty pictures and in recent times deep
learning and specifically neural
networks have become pretty popular and
typically neural networks have been
doing tasks where you give it a
particular input typically it's an image
or text sentence or a speech sequence
and then the network would perform some
tasks like I'll give you some output one
one task is semantic segmentation where
if you're given an image the neural
network then has to segment each
particular object of interest and
image and you can do crazier things you
can do captioning for each part of the
image you want a neural network to
describe what's going on on the image
and this is something you probably you
used at some point in life machine
translation
you know translate from one one one
language to another and these days if
you go to Silicon Valley there's like
hot dog apps that try to recognize
whether there's a hot dog or not or you
know there's all kinds of other apps
they're like Oh point the camera to
something and make it do something
intelligent and in robotics as well
neural networks start becoming sort of a
standard where you train your neural
network to do a particular task in an
end-to-end fashion where you only give
it some input-output data pairs of like
how to do the task for example a human
showing the neural network how to do the
task and then the neural network
automatically learning from from these
videos on what the task is and how to do
it
there's more interesting stuff like
synthesizing faces generating faces like
a neural network or like it it looks at
a bunch of face data sets and it
understands how to generate new faces
similarly a lot of synthesis problems
one of them is image transformation
problems so this is a neural network
called an adversarial Network that that
transforms the left image into the right
image and it's in this case it was
trained to transform horses into zebras
but again the neural network doesn't
really know what a zebra is over horses
is just given input-output pairs and
it's just doing the transformation and
these are all learned end to end and
they're all powered by neural networks
and trained using this one algorithm
called back propagation so typically in
all the neural networks that you you've
seen so far like all you've seen the
applications of a bunch of neural
networks and then all these neural
networks you have operations of certain
kinds almost all these neural networks
that I showcased there the underlying
operations that they do are a few one of
them is convolution where you're given
an input signal like a 1d 2d or 3d
signal and then you convolve that signal
with some masks that you learn and you
get an output operator and and neural
networks you have you have multiple
input channels and then for each input
channel you would generate multiple
output channels and then you chain these
things together and that's what you sort
of call a neural network and if you
implement a convolution naively it a one
that's in the neural network context it
would be six nested for loop and this is
probably not gonna be particularly fast
and other operations are something
similar to convolution we call pooling
where you take the window and you look
at it but instead of doing a
multiplication you can do other kinds of
operations like for example averaging or
taking the max element in the window and
then you do matrix multiplies so you
take your input and then you take some
some weights that you're learning
weights and then you just do matrix
multiply between them and you get your
outputs and you also do a lot of point
wise operations so typically they look
like this you have some for loop looping
over the your entire input matrix and
you do some operation on it and then you
write it to your output and you also do
reduction operations like taking the sum
of all elements in your tensor so it's a
reduction because you're going from n
elements to say
where K is less than n so these are the
typical building blocks there's like a
few more of in the longtail but
typically these are the most dominant
building blocks most most operators are
of this nature and neural networks and
what you do is you chain them together
you from the input you put one Operator
that's learner bill and then do another
operator on it and more operators and
then you just chain them on and on and
this this field this subfield is called
deep learning because you essentially
make these neural networks really deep
you chain them together quite deeply and
then if you have some kind of cycle
where you're one of the outputs is going
a to itself as an input then that's
called a recurrent neural network and
they're all trained with this algorithm
that we call gradient descent so let's
look at a tiny computation graph of a
neural network which has some inputs and
then your output is at the bottom it's
called next stage so with neural network
packages with all neural network
packages you can compute the gradient of
one node with respect to another node so
next hatch is the output and I want to
now compute the derivative next hedge
with respect to the learner bullets WH
at the top and you can do that
automatically with with neural network
packages and then you do what you call a
stochastic gradient descent step where
you update your weight so slightly to
produce the output that you actually
want to produce so I update my weight
with the derivative I I subtract the
derivative of the output with respect to
my weight and intuitively what this is
typically doing is if I have a neural
network that is trying to predict cats
then I give my in
food which is a cat image and then I get
my output and then you compute some loss
function which is my output actually
said dog which is of distance X away
from where cat is supposed to be and you
compute that distance and you take the
derivative with respect to that that
loss and then if you update your weight
with respect to this derivative then
your weight will go in a direction where
it will try to maximize towards the cat
the probability of producing cat the
next time and so you do this over and
over you do this particular equation
over and over with a lot of data and a
lot of these update steps you get you
get to the output that you train your
neural network to produce so that's a
neural network 101 and so all the neural
network frameworks are vertical
computation graph toolkits you just have
you just define a graph of computation
which is in which it might be a neural
network and then you give inputs and
then you give the graph and you say
compute this graph with these input
variables and then you get an output and
there's two kinds of typical computation
graph toolkits one is declarative
toolkits some of the popular ones and on
the declarative side or Tianna or
tensorflow and in these cases and
declarative toolkits what you do is you
declare what your model is supposed to
look like what your neural network is
supposed to look like
typically a Python script declares your
model and then you might compile it and
if you want to actually execute the
model you give it to the virtual machine
that that the toolkit has so the virtual
machine takes this model definition
compiled it into its own integrated
representation and then it executes it
inside its own virtual machine as a tiny
example here's an example of a small
program in tensorflow so here you you
first in line seven and eight you
declare what are your input and output
variable names and these are called
placeholder variables and then you have
a model definition that that that it's
basically at this declarative definition
that you're going to reuse again again
and again and finally you will execute
your your model inside a tensorflow
session which is the virtual machine
session and that that model definition
is taken to tensorflow as VM
implementation which is written in C++
somewhere and it's executed there and
the results outputs are given back to
you so those are declarative toolkits
and there's other kinds of toolkits
which are imperative toolkits PI torches
is one one of those kinds you don't have
a separate VM you use your host
languages runtime as the machine that
you're running or computations in as an
example here is a small PI torch program
where there's there's no separation of
model definition or declaration for that
matter it's just it's like writing
interpreted code and then in line 16 you
you call this backward function that
automatically computes the derivatives
with respect to the elements you care
about so typically if it's an imported
toolkit debugging is slightly easier
because you just write your definition
and you use the deeper tools in the same
language so if you write this in Python
you can just use pythons debuggers and
also like the code flow is generally
linear there's no separation between
vary you declare your model and where
you where you run it because if you have
a runtime error then you don't have to
map back to oh I ran into this error but
the stack trace is not at where I
declared my model it's it's at where I
ran it so it might be confusing in a
declarative model to try to map these
things back but in an imperative model
you get a stack trace
wherever you your error occurred so it's
much easier to debug the downsides are
that unlike in a declarative model where
you take your model define it and you
compile it and then it you send it to a
VM you have the compilation step and you
can write write additional passes you
can build like a full-blown compiler to
make that particular model way more
efficient but in an imperative toolkit
you cannot really compile your program
ahead of time because you don't know
what your particular program is at a
time so you can do stuff like static
analysis however you can take a
just-in-time approach to this
just like JavaScript does I will get to
that eventually
so coming to PI torch itself so PI torch
is has set it I mean for like first and
foremost it has an automatic
differentiation engine so that you can
run your own networks and train them and
so on but at the core of Pi towards you
have an entire a library just like numpy
but it has strong GPU support you can
compute do your computations on NVIDIA
GPUs the interior library is pretty
feature complete it's very similar to
numpy as as a quick example here's a
program on the left side is written with
the numpy api and on the right side is
written with the PI torch API I don't
really expect you to read this program
as just as an illustration on how
similar the api's are so to actually use
PI torch you would import the package
torch and then you can construct an D or
a matrices here it's a it's a matrix 2d
matrix and you can print them and you
can get their sizes and it
it's just standard stuff and you can do
indexing like you can create a tensor
and then you can slice only parts of the
tensor that you want and then do further
operations on it you can do all kinds of
math operations linear algebra
operations and if you want to convert a
PI torched answer to an umpire a and
vice versa it's pretty seamless and also
it is very efficient because we what we
typically what we do at the backend is
if you convert a number array to a PI
tours tensor we keep the same memory
pointer between both of them so it's
observer memory copy operation and it's
typically almost free in practice so if
you if you're a numpy user or if you
generally use the Python data science
ecosystem it's very seamless to go back
and forth and so like the consequence of
keeping the data pointer to be the same
between pi torch and Nampa is that if
you change the PI torch tensor the
numpad
vampire automatically changes and
vice-versa so the one of the benefits of
using pi tours you can use GPUs for all
of your computation and typically if
you're doing something like neural
networks or if you're doing stuff like
matrix factorization and stuff GPUs are
much faster and it's pretty transparent
and how to use GPUs if you have a
particular tensor that you created you
can just call X the like here is X is a
tensor that you just created you just
call the CUDA function on it and then X
will just be transferred on to the GPU
and any operations you do further on X
will be done on the GPU itself then
coming to the automatic differentiation
engine that we have it's used for a
neural network sleep learning
reinforcement learning we have the
we have the Otto grant package as part
of fight works and we have one class
that we define that's called variable
and variable is this loose wrapper
around the tensor and whatever
operations you do on your variable all
of its history is recorded so as an
example I defined a few variables here
and then I do operations like matrix
multiplies and additions on them and
what actually is happening is next hatch
the variable here has a pointer to the
function that computed it and that
function has a pointer to the inputs
that it had and so on until you get your
leaf variables that the user defined and
so this is how you can do automatic
differentiation because you have
pointers to everything that computed
that was that that was your parent that
that you that your the output of so on
the like on the final variable you can
call the backward method which will
compute the gradients of one node with
respect to another and that's pretty
much how almost all of Pi torches neural
networks work basically the neural
networks are Python programs there's
like almost no difference between what
you call a neural network or a Python
program in Python we built a small
convenience wrapper around the
autograder engine called the NN package
and all it does is it it defines all the
typical operations that you see in
neural networks like convolution and
pooling and so on it just builds
object-oriented wrappers around it and
so as an as a quick example this is a
small neural network that is defined
that's called a convolutional Network it
in the constructor of this new network
you just define some of the layers that
you would want to learn that that that
have learner bull weights and in the
forward function this is where you
define your neural network you're
basically your X here is the input to
the neural network and you're writing a
small Python program that defines how
the input maps to your output that is
returned and that is their neural
network that you're gonna that you can
use and to treat like you can for
example write Python for loops based on
some conditional of the input and so on
and that's that's how you define your
neural network itself and I did mention
that you you typically do this small
stochastic gradient descent update rule
to update your weights so that they get
better and better towards the task that
you're trying to train the neural
network for and for that we have an
optimization package and the way this
package works is typically you would
have some data set of input-output pairs
your inputs are taken value in the
neural network and you want some output
to be produced and this could be
anything from image recognition to ad
targeting and so you just loop over your
data set and you create some optimizer
beforehand let's say a stochastic
gradient descent optimizer you pass the
input to your neural network which is
model and then you get some output and
then you compute some some loss function
that tells the neural net that tells the
training procedure how far away your
output is from the variable that you
actually want to produce which is a
target variable and then you you compute
the gradients and then you do the update
rule and then you do the update rule
so that's tight word in a nutshell and
now I'll be talking about what we had to
do to make PI torch as fast as any of
the other neural network packages the
way we would wrote PI torch was one of
the worst cases for
like an engineering perspective because
it's like this very dynamic Python II
program and so we constantly had to
fight Python because Python is slow and
especially for like if you're trying to
fight nanoseconds Python is is really
slow like if you write some very simple
argument parsing like oh if my I have
some keyword arguments if this keyword
is true go to this code path or elves go
to this other code pad something as
simple as this can take microseconds and
that's very frustrating because you use
really fast GPUs and the entire
operation on the GPU probably will take
a microsecond as well the second thing
is Python as a single global interpreter
lock it's evil it's really horrible
especially when you're trying to do
stuff that's high-performance you are
queuing so typically what we see is in
neural network workloads we want to use
X GPUs we want to use a GPUs at a time
and if you're trying to Q R if you if
you're trying to cure our operations
through Python on all the GPUs just the
queueing becomes a ball like that is the
get like if you have eight GPUs and
eight threads queueing the operations on
each for each of the GPU eight python
threads then because of the gill we
can't really see full parallelism
because even if you have eight Python
threads each of them is waiting on
another thread because of this global
lock that exists so we we fight it in
various ways one of the simplest ways is
we just move all of our functions to
sleep C or C++ that are actually
important it's a subtle trade-off
because as users of Python as use of pi
torch itself
if you want to make sure it's very easy
to debug and extend while you're working
with the day to day but if you want
performance then the biggest hotspots
cannot be in Python so the reason we
went down Python like instead of using
C++ directly why we went to use Python
is because python is the most popular
data science language and it's very
convenient for all of our users to use
Python but we have to make these
constant trade-offs and fight Python all
the time and the other set of things I'm
going to talk to you about in terms of
performance is why why do we use GPUs so
much it might or might not be obvious to
you guys
so neural networks are embarrassingly
parallel and GPUs typically have 3000
like high-end GPUs have 3000 cores that
can run computations in parallel so as I
want to recap the typical operators you
have in neural networks one of them is
convolution if you think about it look
at this operation you're looking at a
window and taking some multiplications
and summing them into an output channel
and the the animation here is doing each
of them in a serial fashion like one
after the other like a sliding window of
sorts but there is no reason you could
you could just do each of these
instances completely in parallel and
typically all of the operators that I
described are like that so so in terms
and pooling is something similar you
know you do this you do the sliding
window across height and width matrix
multiply is something similar for
example to compute the yellow and the
green outputs you don't really need you
don't have any data dependence between
them you can compute them completely
independently and if you look at point
wise operations each of these operations
is not dependent on another eye so each
eye is independent and can be fully
paralyzed so eye
let me take a quick example to explain
so let's look at the simple use case
let's say a B C and D are tensors and I
want to compute a plus B times C so this
is kind of how the code would look I
first compute a plus B I get some output
and then I take the output and compute C
this operation is completely memory
bandwidth Bond because typically the
tensor Center workloads exceed the size
of the l1 or l2 caches of your CPUs or
or you know the reddit like the register
caches of your GPUs they they run as
fast as how fast you can get these
things into registers from main memory
and back out again and as I said l1 l2
caches are useless all of them all of
the functions that I described so far
except for a convolution and in some
cases matrix multiply they are all
memory bonds so if you have faster
memory these operations will run faster
so high-end GPUs have very fast memory
much faster than system main memory the
the latest GPUs that you can buy they
have this memory called HP m2 memory
which has about a tera something around
a tera terabyte a second in and out of
them and in and out of the GPU main
memory which is much much faster than
than CPU main memory that you see so
generally using GPUs regardless of the
parallelism is beneficial if your
operations are banded bond I think we've
I've seen in recent times people
implementing databases on top of GPUs
and I think the main reason is because
it's just higher memory bandwidth and
and GPUs have the highest memory
bandwidth that you can buy conveniently
and GPU is like paralyzing
parallelizable problems all the
operations I showed you and I described
to you are embarrassingly parallel let's
look at this particular six for loop for
example it's it's the naive
implementation of a convolution every
single four loop here can be exploited
for parallelism and and you basically
have unbounded parallelism right here
and one of the optimizations we really
need to do in terms of such workloads is
try to make them more compute bond if
you see this this this these two four
loops here I'm first taking a plus B and
then putting it back into main memory as
out one and then again I'm loading out
one into main from main memory into into
registers and multiplying it with C and
it really doesn't have to be like that
you can do stuff like operator fusion
you can take these two separate
operators and then you can just use them
at runtime and that's one of the biggest
benefits of compilation you can
generally do fusion and make your memory
bandwidth operators and to compute bound
operators this is one of our typical use
cases in neural networks where we have
these chains of convolution operations
which some kind of normalization
operation and a simple point-wise
operation and doing some operator fusion
here really gets as big benefits like
30% 40% faster overall over large
networks and if you have a compiler you
can reorder things you can send off one
particular operation to a particular
coprocessor that you have let's say you
know all of your matrix multiplies can
be sent to the GPU but all of your
sparse tensor operations can be like
sent to the CPU because forest
operations are nicer on the cpu in
general so it's it's been obvious to us
that we need a compiler especially as
GPUs themselves are getting fast
and CPUs and main memory is not keeping
up
so we built a compiler for pi torch it's
because pi torch is a fully dynamic
program we don't know what the what the
neural network is ahead of time we had
to build a just-in-time compiler and we
built a tracing JIT and the way tracing
jets work there's a very simple small
example don't kill me if you're a
compiler expert but let's take this
function foo which adds 1 to X its input
and then multiplies to to the result and
returns Z so you can give some input X
and it will produce Y of of the
corresponding result and in a tracing
JIT
what we built was you just annotate your
function that you want to trace with
with some function decorator and the
first time you run run this function it
will take the input tensor and then it
will run each of these operations and it
will record all the operations that
happened on tensors themselves so it
will run this line and then the the
trace recorder records that oh there was
an ADD operation that happened and there
was a result t1 and then there was a
multiplication operation that happened
on the t1 and that was returned and when
you run white 2 equals foo of X like
when you run the same function again it
actually shortcuts the entire Python
code path the entire code that you wrote
and instead it uses its trace it gives
the trace to our our C++ interpreter and
it just executes the trace itself and
not the original Python function and we
cache traces so typically if you run the
same function again and again we will
cache it and use the trace rather than
this original function there's two
benefits to it one we avoid Python vo
odd Gil we all read all that
slowness the second benefit is that we
can now write optimization passes to do
stuff like operator fusion like once you
get a trace we can pass the trace
through several optimization passes and
we would get a faster code and look more
details about our JIT it's an intimate
it has an intermediate representation
where tensors are first-class types and
it is optimized for reusable and cached
traces that is we expect our traces to
be run again and again and again unlike
for example if you write a JavaScript
tracer you write your tracer for a
branchy code and you don't really expect
the law do to like your traces to be
executed millions of times maybe like a
hundreds of times or so but I'm also not
an expert in JavaScript tracers so and
one of the gotchas that we have in our
tracer that we are trying to solve in
various ways is we cannot handle control
flow yet as a small example this is a
function that looks at the total sum of
the the input tensor and if the sum is
less than 5 it will go to the first code
return and if otherwise it will go to
the second return and something like
this when we execute this function for
the first time with our tracer it will
go through one of the conditionals
and subsequently if we give it a tensor
that will trigger the other conditional
it'll just give the wrong result and
we're trying to solve this in various
ways one of them is trying to parse the
Python ast itself and build a trace by
just from the ast we have other ideas as
well but that's kind of state of things
and thank you for listening this is my
last slide if you want to try PI torch
or if you like neural network
go to fight org give it a try and thanks
to everyone who's been part of the
community
and that's about it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>