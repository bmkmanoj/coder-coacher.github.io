<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Strength in Numbers: Unums and the Quest for Reliable Arithmetic&quot; by Ferris Ellis | Coder Coacher - Coaching Coders</title><meta content="&quot;Strength in Numbers: Unums and the Quest for Reliable Arithmetic&quot; by Ferris Ellis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Strength in Numbers: Unums and the Quest for Reliable Arithmetic&quot; by Ferris Ellis</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nVNYjmj_qbY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Faris Ellis and today I'm
here to talk about strength and numbers
or younam's and the quest for reliable
arithmetic and to start with this talk
is going to sort of bridge the whole gap
as a quick show of hands does anyone
here heard of younam's cool okay some
people as anyone here have like no idea
how floats work and they're just magic
things okay a few people good good so
this talk will span all of that but
we'll start from the bottom and sort of
quickly build up and so everyone is sort
of familiar with the basic idea of
arithmetic right you have like numbers
and operations you know four plus seven
equals eleven and in a slightly more
like programmer Eska way they look like
this if we're you know taking a
logarithm or something and to do this we
have integers and floats to represent
integers and real numbers and that's
that's about all you can sort of say and
in a quick snippet the elevator pitch
for computer arithmetic but of course
I'm here to talk about reliable
arithmetic which sort of implies that
something's kind of broken about this
and we can really quickly sort of try
this out because if you pop open any
Python raffle and you ask like is zero
point one plus zero point two equal to
zero point three we we get back no and
this isn't some trick this isn't you
know because a Python or because of the
operating system like this is in the the
silicon of your CPU this is floating
point arithmetic and you can try this in
Python or C or rust or whatever language
you want on a 64-bit processor if you
don't believe me this is what you'll get
back and so this sort of violates our
our fundamental understanding of
arithmetic and also the understanding of
pretty much anyone older than like six
and to add to this like if we're if
we're going with this is the fact that
it doesn't even really follow a pattern
right because you're a point where I'm
sorry one plus two still equals three
and zero point zero one plus 0.02 still
equals like zero point zero three so so
this first example sort of appears
bizarre and there are probably some
people in the crowd who are the the
computational veterans who may sort of
be like yeah you know whatever you're
making too big of a deal out of this but
but like computers run a lot of the
world so if you try to talk to anyone
like that isn't a computational veteran
of floating-point numbers and to explain
to them that like you know all the
computers in the world do this it's a
little disconcerting to them and I think
kind of shows the absurdity and like the
the concern with this and when I first
learned software sort of casually in
high school like I would that's what I
was taught for floating-point numbers
was like they're the magic things used
for real numbers like the end onward to
doing things and so if if that's your
understanding if you don't sort of have
a deeper knowledge like there's also no
explanation for this right this is
bordering that sort of like magic hand
wavey territory and I come from a math
and science background and so when I
first saw this later on in my career I
was like shocked maybe like a little bit
insulted because like I come from like
systems background server-side stuff so
like all the problems that I was used to
where people had bad understandings of
semantics or like too much faith in wall
clocks and so you can sort of trace that
down to like bugs or user errors or
badly design algorithms but uh but this
was like fundamentally part of computers
and so I went where anyone goes to debug
stuff I started printing things and this
is rust by the way on there there's no
need for deep knowledge of rust but it's
just a low-level language I'm familiar
with so we can really easily dig down
and look at some of the bits and stuff
in this and so to start with this is
just we have 0.3 here as a 64-bit float
and we're just printing it you can see
the execution output at the very bottom
0.3 right so so this matches up this is
what we'd expect and if we move on to
you know the comparison failure on the
other side of 0.2 plus 0.1 we get almost
0.3 but not quite we have like 15 I
think it's 15 zeroes and a 4
and it doesn't help that printing 0.1
and printing 0.2 so give us there 0.1
0.2 so they're not off like somehow the
fact of addition seems to have caused
this and so the next layer down from
this is actually to dig binary and start
spitting out bit patterns and to do that
though and have any meaning to it you
you need to understand how floats
represent numbers and they're not very
intuitive spoiler alert if you haven't
ever done this so I'm going to start
with a really simple analogy which is a
scientific notation and so scientific
notation right we have like a value
times 10 raised to an exponent being a
little more particular like the value is
between like you know one in 10
exponents an integer and to offer an
example like if we take you know seven
and a half billion roughly the
population of the world and we convert
it to scientific notation you know we
get seven point five x to the ninth and
it's a little bit more cleaner it's a
little bit more concise we've decoupled
sort of this inherent quantity and scale
attributes and so floats do this
similarly except we're in binary world
or in computers and so instead of base
10 we get base 2 and so we have some
value between 1 and 2 times 2 raised to
some integer and so if we take our prior
number 7 and 1/2 billion and we chug
this into this formula we get something
a lot less compact right and this is
much more intuitive to computers but to
us we just sort of see like it's big but
they actually can't even do like the the
based in binary of that value so we
actually like have this fraction ish
thing right where we have 1 plus a
fraction divided by base let's max value
right so F may be 0 in which case it's
just 1 or F may be 1 less than that to
raise the D so almost 2 and then this is
what we multiply times 2 to that
exponent and this is the the basic
formulaic idea behind floats this is how
we represent numbers and you can see you
know if we scale really big we get a
really big number if we scale
negative right we get a really small
number because we're raising to to like
the negative 15th or something and so if
we take our value again and plug into
that it gets a little bit cleaner right
we only have integers now which is
important because we're about to go into
binary and so for a sixth or for a
64-bit float we have these sort of
fields to to encode that information and
so we have one bit for sign right pretty
intuitive we have 11 for exponent and we
have 52 for fraction and if we actually
want to see what those are in our
previous example we can code up a
function really quick again don't worry
too much about the craft of rust but
just for anyone who may want to follow
along later on this is basically you
know makes a bit mask so so the bits
that we can an to sort of extract out
those chunks we shift it around anyone
here from the rest community yes I'm
using an unsafe transmute which lets me
basically say don't worry type system I
can totally just grab these bits leave
me alone
and so we can run this right we can do
this for this sort of anomaly value and
we'll get this and you can see here that
we have sine is 0 which is good means
the police work for that we have this
sort of very long binary pattern for the
fraction and a small amount for the
exponent and we can do this again for
our other value oops I think I have a
slide error here
yep but moving on
sorry so if we if we look at how this
actually aligns oh I'm sorry
notice lighter that is different and so
we can actually turn those binary values
into numeric values so this is the
base-10 printout of it and so we can see
our exponent bit gives us I believe 1021
we have 9 and a bunch of other numbers
for a fraction and so we can take this
back to our formula and see like what we
actually get out of this because this is
the sort of honest underlying math
behind it and if we plug those in you'll
see that there's a slight issue here
which makes us wrong because we have two
to the 1021 which will give us not 0.3
so so the the final trick that floats do
is is they they split that exponent
field they subtract half of its max
value and so if the exponent field is
one we end up with negative one thousand
23 and then the biggest it can get
before you get into the weird
representations like infinity and not a
number is a 2046 which gives you
positive 1023 and so for this we'll see
that we get this which lines up with
what was printed out but also has a
bunch of other stuff after right this
giant leads to 54 digit number but this
is actually the the precise exact
mathematical value for this float which
sort of is only like adding to the
confusion because this isn't what we get
when we print and so if we go and we do
it again this time for zero point three
that other value that doesn't quite
match up we get this back and if we take
that and plug it into our formula we get
this which is the exact mathematical
value that printed out zero point three
a minute ago so none of this is
particularly intuitive and so what
happened to to our zero point three well
as it turns out floats only have some
limited idea precision right and so for
those of you who you know remember your
readings from your textbooks 64 bit
floats give us 15 - 17 - as a precision
generally about 17 and so we we round it
and more than that you'll notice that
these two are often the fraction filled
by one right these two values that we
have so we have like a sort of weird
arithmetic missed by one error and if
you think about this for a moment like
all these fields are binary so so not
only that but like we are
we have like this chasm right between
these two values and that's because this
is this is floats idea of numbers right
we're in binary land we cannot represent
infinite number of values and so we pick
these little points that can be really
precise points what we pick them and
then we have to round to them and so if
we if we go back to our previous example
and look at at this value in this case
our precision at 15 bits gives us 2 with
a bunch of nines which sort of safely
buy stuff that you can read in the spec
if you want to go down that route is
rounded to 0.3 so at least we we know
that but but because of that chasm we
actually have this value on this plus
minus u LP and UOP is short for unit of
least precision it's that step it's that
chasm that gap between values because
we're rounding right so so we actually
don't know exactly the values right this
is the representation of 0.3 but it
isn't right it's it's really this tiny
little interval this tiny little range
that floating points lie and tell us the
total isn't there and so this is sort of
the the more honest layer of floats and
because of that this is what happens
when we do addition right 0.1 plus or
minus a unit of least precision plus 0.2
plus or minus a unit Lee's position does
not promise 0.3 and in fact in reality
what we get is is this we we round over
one ULP and so one with a little bit of
imprecision plus two with a little bit
of imprecision happens to hop over to
the next hop of three with a little bit
of imprecision and so you will P by the
way as an important note is those steps
but those steps are multiplied by that
exponent value and so normally like you
know if E is some value so that you know
it - tooth 1023 comes out to like 7 or
something the eops are still really
small but like II can be 2046 so so in
in that case the ULP start getting
really big they they get like 2 to the
970 first or or like 10 to the 290
second and for anyone here who's like a
physics major which I was in college
like you know total estimated atoms in
the universe or 10 to the 80th and like
this is like the air we're hopping
between in values like at the upper
bound of sixty four-bit floats it's like
I don't like I can't physically come up
with an example like in our universe
that like what this even means but but
this is that outer fringe right we we
have a speck this is this is floating
points and like we were devoting memory
and bits and and operations and cycles
and gates and our CPU to work with an
implementation that does this and so
what what have we learned about floats
so far so to do a quick recap before
moving on so they can't really do after
at arithmetic sometimes they have three
fields sign exponent in fraction they
they have base to scientific notation
there's this ULP you know these
precision of how we're stepping between
values they round all values to the USU
LP and they're dynamic range is far
larger or smaller in some cases than
like physically useful and so the the
sort of - I think highlight wants to
take away from this the two reasons why
we we encounter so many issues with
floats are these two here the fact that
they are sort of they are deterministic
ly doing arithmetic obviously right you
know it's not a question like if I type
it in this time will it do it again but
but we don't have like tooling or any
intuition for when these rounding errors
happen and cause issues um but they
happen and so we use floats and and we
get a value back and the computer sort
of like no really believe me it's this
but like if you have no context about
computation if you're a computer user or
if you're in some way upstream
service or of a piece of software that's
doing these computations you have no
idea what the sort of ramifications of
those rounding errors are and if you're
sort of academically interested there's
all sorts of fun examples that you can
throw floats very precise flow it's like
128-bit floats into equations and I'll
spit out things that are like
horrendously off like by factors of a
million but but generally it doesn't
happen but we still have this issue that
like we we have some aspect of a lie
they aren't honest and and because of
that if we're if we're looking to do
reliable arithmetic that you know 0.1
plus 0.2 equal zero point three a
floating-point is a fundamentally broken
number system and the reason why because
you may think like oh okay like 64-bit
doesn't work but like at some point we
like we can stretch out the precision
right and at that point like Oh it'll
work but but the issue is if you sort of
reverse the math like zero point one as
a base ten number is not representable
in base two seismic notation though
it'll always live in that ULP and no
matter how small you make that you LP it
turns out that you can find examples
where when you add one of them in
another one they hop a ULP
so like one kilobit floating-point
numbers like are like one megabit
floating-point numbers will still
exhibit this like this this only way
this only scale that we have to to
tinker with their performance versus
their memory usage saves us zero from
this and so we have a new contender
younam's and as a quick disclaimer for
anyone who's read about you knows i know
we had a decent number of hands there
there's three versions now the inventor
of younam's
who's not me is John Gustafson has been
very prolific and I think we're at like
two and a half or three years now so so
this talk is gonna focus on version one
younam's which I believe are the most
generally applicable and the most sort
of oriented towards just doing reliable
arithmetic version two are swords which
is an acronym for a set of real numbers
very specific use case but very
interesting idea and in Version three
are posits where he basically tries to
confine younam's to more flow test
things and basically say like
fundamentally there are issues with
floats and we can kind of beat them at
their own game but but we'll focus on
that of just version 1 and if anyone
wants to dig more into this he has
slides and presentations on his website
they're all free you can also watch
talks by him and if you really want to
dig deep into version 1 younam's and
listen to or read a free heard page book
on it there's also the end of air by him
this is not free you have to pay for
this on Amazon and this is what I
originally found that got me interested
in you knows I had a good friend of mine
Alex one day who had had found it out of
it I think a book store through a friend
and I was like you have to read this and
since then I've taken a deep interest in
this topic but moving on we actually
want to play with them and they're not
in our hardware and so for for this talk
as a demo I'm going to use Julia
Computing's UNAM library there's a
number of them out there there's one in
C that was done but I think Lawrence
Livermore labs they're multiple and
Julia there's one in Python I'm working
on one and rust it's nowhere near done
but but if you're looking to play around
at home and at least get a good idea
that I found that this is the best one
to just sort of you know type install
and you can play around with them and so
we mood up Julia and then we're using a
rebel here for those who aren't familiar
to Julia you know deep knowledge is
needed has a rebel like Python it's
about all you need to know
so using younam's imports the package
after we've installed it and then we can
ask for this weird thing at unum 3/4 of
0.1 and we get back this which is which
is kind of hopeful right we have a range
now so so we're at least getting a
little bit more into that level of
honesty of numbers and not only that but
we can ask for a bit pattern of it which
is a lot more helpful than having around
a masking function and so if if we take
this and we sort of decode it we end up
with basically this information and so
this is more to unpack and not really
intuitive either
and the easiest place the Stars with the
3/4 so younam's have a idea of an
environment you cannot interpret a UNAM
without this environment and in this
case the three for reference is how many
bits go to the exponent size and
fraction size field so you see exponent
size has three bits fraction size has
four bits and then those in turn
determine the size of the exponent in
fraction field so so sort of thing one
about younam's super dynamically sizable
and as a disclaimer by the way this is
purely a software talk if you are a
hardware engineer and I like internally
screaming about the issues of having
like dynamically sized numbers as
fundamental things and like bytes right
I this that is outside the scope of this
talk but gustafson does cover in his
books you can read through that and and
you'll also notice right that like the
exponent size like binary like you know
1 1 so that's 3 but we have 4 bits and
so in reality the exponent size and the
fraction size get +1 to them so so in
reality exponent size is 3 and binary
but it's meaning as 4 and same for
fraction size and then we have this
exponent of fraction and then we have
this thing called au bit that's turned
to 1 and we'll get to that in a little
bit but first let's let's actually plug
this into our equation to make sure that
this comes out right and because they're
dynamically sizable this equation gets a
little bit more complex basically you
know the fraction size and the exponent
size determine what that divisor is for
our fraction and how much we're taking
away from the exponent and if this looks
a little dense don't worry about too
much but that's that's why that's the
air and so we can we can step through
this pretty quick and actually see that
this comes out to this number here and
in fact if we do it with the other one
we get this range and that's hopeful
because this matches up with some number
below 1 and this matches up with some
more above 1 whoops so we're actually
within a valid interval so far and not
only that we see that we're actually
within one you
right these two values here are off by
one value in the fraction so so far so
good
the other important thing note is that
this is a literal interval notation that
were printed like we were saying all
values above you know 0.09 and change
and below this 0.1 and change right
there non-inclusive and yep so so this
is where the yupik comes in because what
the Ubud allows us to do is actually
represent this ULP range when Ayub it's
turned on it says I'm not the value
represented and I'm not the next value
represented I am somewhere in between I
actually have a way to show this and
this is cool right because unum suddenly
look like this on the number line right
we have a we have the fact that younam's
are contiguous we we actually can throw
any number at younam's and we may not
get back for perfect precision we may
get back something that is a range but
we get back something that's honest
that's actually I continue the
contiguous part of the number line and
so continuing on with this if we hop
back into to Julia and we add zero point
one and zero point two we get back
another interval right which isn't
surprising at this point but but this is
sort of the like the plot-twist part
because zero point three doesn't print
back the same interval which which is
perhaps frustrating but is also is also
honest and important and and speaks to
the fact that if we go remember back to
that slide on and ago about the
plus/minus VLPs you'll see that that's
what's happening here right zero point
one plus zero point two prints back a
larger range than zero point three the
lack of precision has been preserved and
so this this is curious because if if we
look at this how do we now compare them
how do we compare intervals which like
so far we sort of built up is like this
in escape a little thing like we can lie
about it but like this is in reality
what's happening
and oh and before that like you may also
wonder what is this thing that's
representing more than one ULP right
because 0.3 gets cast to one ULP of
width it's that it's that one interval
but but 10.1 plus 0.2 is even broader
and so the answer is is that we're now
in like actual interval land and so it
is au bound which is as you can probably
guess a fancy name for two younam's
sitting side by sine that define the top
and bottom of this of this interval and
the way that we can really like compare
them is to go sort of more towards set
style notation so we can ask things like
is 0.3 within the possibility of 0.1
plus 0.2 is it a possible value does
this interval fit inside this other
interval and what you may realize if you
think about this for a minute it's like
doing operations right like this
imprecision is just going to compound so
like almost always we're gonna end up
with something that spans more than one
ULP it's like how do we even do
comparisons with answers because at the
end of the day we're like is it this
value which may be exact or maybe like
plus or minus one one of these gaps but
in reality we get back something it's
like spans eight gaps and and this is a
challenge to how we think about it but
this is also the reality of the numbers
were working with and as also as we'll
see sort of the reality of the world in
a little bit so so what we've learned
about younam's version ones so far so
they have five fields right sign
fraction an exponent you bit fraction
size which are actually six fields
whoops
things you only catch while giving a
talk requires an environment to read the
numbers right so we have that unum 3/4
and if you if you play around with the
library I mentioned they have three four
and two two but as the only two options
but that is in no way like the the
limitations right you can have them be
as big or small as you want
Rex computing has another one where you
can make them whatever size you want but
getting started with it is not nearly as
easy and they do honest arithmetic
which returns intervals and so we've
sort of gone from looking for honest
arithmetic to getting it but maybe not
quite it being what we wanted and we
must do interval comparisons which are
less intuitive because of that and so
the the top two here are sort of the the
implementation facts of of younam's but
but the bottom two I think are really a
a bubbling to the surface of the fact of
trying to represent real numbers in any
way that is binary or finite right we do
honest arithmetic with that and we're
stuck with intervals and we have to
figure out like what that means and what
to do with it
and I baited when making this talk about
sort of diving deeper into the
technicalities of younam's and there are
other things to talk about
about how they different from floats
about how floats has to raise to the
fifty-second ways to represent not a
number which people now use for like
memory boxing not a number of boxing in
memory as a thing
javascript uses it but but I think
perhaps the far bigger picture thing to
talk about at this point it is what does
all this even mean right for like us as
software developers because this idea
flowing quick numbers has been around a
long time in fact if he dig into the
history of it it's basically been around
since the dawn of digital computing
right it never was like invented at some
point and had challengers it's just
always been there I at one point when I
found out about this this weird issue
with floating point numbers I started
with I went to a colleague and was like
can you believe this and I got back you
know this sort of like well yeah and I
had this moment where I was like I felt
like you liked floating point numbers
had exhibited some aspect of like
Stockholm Syndrome on like the the
software community because like people
don't even like find it concerning
anymore has always been there and so if
it's not there if we have this instead
what does that mean for everything that
we're doing with the technology these
days which may also be translated for
the more pessimistic people into did we
just show that like computer with
material numbers at least in the way
that we imagined arithmetic is
impossible and the answer is yes and no
because real numbers model
world right assuming we're not in pure
math land assuming we're not taking the
square root of two we were using real
numbers because we took a measurement
right we take measurements of the world
and measurements our intervals they come
with uncertainty right if you measure
the velocity of something like this
right this we can never promise that if
I I pick up something that it weighs
exactly like to the atom eleven point
two four three six pounds in fact
ironically and sort of the same you'll
pee since probably by like weight of
electron adding like you can't actually
like an infinite precision even get that
so like this is this is wrong like this
this is a measurement right this is the
real world we have some plus or minus we
have some uncertainty and these are the
things that we're trying to do
meaningful work with when we're doing
you know physics computations or when
we're doing graphics rendering and we
need this idea of sort of intervals in
general because some of you may look at
this and be like that's a really strange
like plus or minus thing like isn't
supposed to be like half of the last
value and you're and you're right that
usually this is a standard because this
is a convert conversion of five point
one kilograms plus or minus five
kilogram
right so so we don't even like have the
certainty of that we just sort of have
to know that we have these intervals of
knowledge and be able to do computation
and and get results out of it that have
meaning but some of you here aren't
scientists I guess probably the majority
of you here aren't scientists right your
software developers and engineers or
whatever title we get these days in our
job postings and and so I wanted to like
do a really compelling example of this
for for people in the software community
so like let's talk about neural networks
right for people who know no knowledge
of neural networks I probably am more on
your side than like industry expert in
it but I have dabbled around with them
but the the basic idea of neural
networks is you have inputs right
generally there floating point values
all right on the left here and they sort
of traverse these lines that are
weighted and a common pattern is to have
you know input traversal line its x it
all the lines that go to sort of one
these hidden nodes there could be a ton
of layers of hidden nodes that sort of
form that's crazy mesh that didn't look
great on a giant slide so I pick this
one
and each of those nodes apply some
function and so we're doing a lot of
sort of basic arithmetic in a sense
right like nothing crazy is happening
here apart from mostly like exponents
maybe like you know hyperbolic tangents
or some sort of strange function but
it's it's mostly just super compounded
repetitive basic arithmetic and we get
out some output and we're used to neural
networks like spitting out you know like
some certainty perhaps for a decision
and and so in reality as we've sort of
seen like that's that's not really what
they spit out they spit out in an
interval they spit out something that we
have to interpret much much more closely
to make decisions off of and and even
more interestingly if you want to go
down this route is the fact that those
functions that take these sort of
accumulated floating-point numbers and
do things with them are our functions
like this right this is a sigmoid
function and the the top graph is a
super zoomed up one but the bottom graph
is like I think - did I give - and
you'll notice they flatten out in fact
the sigmoid function will always return
something between where those are
tailing off - like 0 and 1 right and so
we we saw earlier floating point numbers
have these giant exponent fields right
we can represent like more atoms and
they are in the universe but that
doesn't matter here right in fact you
can do a pretty fast back of the hand
math and realize like once you get past
I forget what value exactly but at the
most a hundred right like what you're
getting back is definitely like just
really close to one and so we probably
don't need 15 digits of precision when
doing these things in neural networks we
probably need a lot less and and so
floating-point numbers seem very
wasteful if this is all we're sort of
doing with the sums of the numbers
passing through them and we see this
even more for hyperbolic tangent which
is another one used which returns back a
number between 1 and negative 1 and so
this is not an original idea in my part
in fact people that are industry leaders
have thought about this this is a paper
that came out from Google a few years
ago improving the speed of neural
networks on CPUs
because you know ideally a lot of times
GPUs are used for neural networks we're
now getting to the point of specialized
hardware I'll touch on that in just a
second but but if you're stuck with a
cpu right and like the traditions been
to do floating-point numbers you're
you're slow and we don't need floats
necessarily and so what this paper did
that can sort of be summed up here is um
I'm gonna have to read off this we
emphasize data I don't have the right
angle but at the bottom you'll see the
important part provides a 3x improvement
over floating-point baseline and and
what this has done is is they've
quantized that little field that they
were using that sigmoid or that
hyperbolic tangent with an 8-bit integer
or in some cases a 16-bit integer
because all you really need to do at the
end of day is like step between those
and like have an overflow representation
right because the overflow
representation then it is like it goes
to the function and it gets like round
well not round it but it goes back down
or whatever that the top line was and
and you can use sort of these super new
parallel instruction sets and Intel CPUs
SSE 2 and SSE 3 and 4 and so you can
pass it like 128 bit array of these
8-bit integers and do parallel
operations on them and they they made a
working example of doing speech
recognition with this right and and
since then this has gone on to actually
be how they do it in hardware google's
new tensor tensor flow hardware does
this and so you'll see here you can read
the the pair F if you want but
especially the little diagram where if
you're in this bounded range you know an
8-bit integer gives you all the
precision you need we can we can clearly
represent things because as the slide
says like knowing with 15 procedures
digits a precision like how much it's
raining is less important to neural
network than sort of like is it raining
a lot or a little with some steps in
between which you know we we would sort
of hope in a way like if neural networks
are moving towards intelligence we don't
care about 15 digits of precision for
rain like so so it shouldn't either as a
weird sort of human equivalent but but
more than that I I think there's an even
more powerful idea in this and
I'll pull a quote from Peter Alvaro
who's presented a strange leap in the
past from one of his keynote talks
meaning well is better than feeling well
and so if we're really to to be serious
about doing meaningful math then we need
to find ways with numbers to declare
needs over choosing options right we as
software developers look at something
and were like oh that needs an integer
or oh that needs you know a 32-bit float
or a 64-bit float but but there's a lot
of problems where we don't do that
anymore where we you know hand it over
to a compiler to something that knows
logic and does it reliably that can say
you know given these constraints given
what you want to do here's the best way
to do it and so if we think back to our
neural network example really we we as
developers ideally should be able to say
you know I need an output that gives me
you know a number with four degree four
digits of precision and the rest of it
right doesn't really have a meaning to
me that output is what has a meaning to
me and as it turns out to do that you
need a lot less than like a 64 bit float
or even a 32-bit float and so apart from
that we need or not a part of that to
wrap up we need better number systems
right and and if we're ever really to
hope that they catch on and do anything
at least in the software space there's a
hardware issue which I have mentioned
I'm not going into but we need better
numeric tooling we need ways to answer
these questions have computers help us
solve these sorts of problems and be
able to escape the idea that like you
know you just sort of like throw 64 bits
of a floating-point number at it and
kind of pray and hope that like what
spits out at the end is um is meaningful
and there are a number of kind of not
kind of definitely sad examples in
history of when number systems have had
serious issues
there's the Patriot missile system which
fail from floating-point rounding
compounded of representing time there's
things like the Arian rocket which
whether or not you buy them exactly on
floats is another question but
definitely was an error where he
we're sort of trying to solve this idea
of how to best compute and as it turns
out when that happens sometimes a rocket
tries to make a 90-degree turn going
into the atmosphere and blows up and so
the the message I want to leave you with
overall is uncertainty is unescapable
and normally it is to only lie to
ourselves and that's that's important
for all software we do whether that be
number systems or anything else
and and that's all I have thank you for
your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>