<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Expressing complex data aggregations with Histogrammar&quot; by Jim Pivarski | Coder Coacher - Coaching Coders</title><meta content="&quot;Expressing complex data aggregations with Histogrammar&quot; by Jim Pivarski - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Expressing complex data aggregations with Histogrammar&quot; by Jim Pivarski</b></h2><h5 class="post__date">2016-09-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mB4Chl0ly-g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm Jim Bob our ski I am from the
high-energy physics community and I'm
here to talk about a project that blends
some of the ideas that are unique to
hydrogen physics with functional
programming which really isn't so this
is sort of a marriage of two worlds so
I'm going to start talking about this by
talking about worlds in general so as I
see it out in the machine learning Big
Data community there are basically two
big clumps of software products there
are the statistical computing with stuff
like numpy Sipe ir matplotlib and
there's big data with spark park ad 3
Hadoop and such the thing that
distinguishes these clumps is that
everything on the left is it's mostly
natively compiled for speed with a nice
high-level language interface on top and
although a lot of them can be
parallelized the primary consumer is
laptop data analysis that's least that's
where it starts and then it expands out
from there whereas on the other side
it's mostly the java virtual machine
running in some large enterprise server
more emphasis on scale outs than single
processor speed and the data sets are
assumed to be big from the beginning the
very little laptop analysis in Hadoop so
I use that as a segue into talking about
a third whole ecosystem that you
probably haven't heard of the set of
software tools used by high-energy
physicists to do things like discovering
particles discovering the Higgs boson
and such so if any of you are are in the
high energy physics community have had
any interactions with them maybe some of
these things would look familiar
otherwise they probably don't it's
really a separate world some features of
this world is it's overwhelmingly
natively compiled sometimes they don't
have dynamic languages interface on top
of that some do some don't
a lot of focus on single processor
throughput and just to split hairs
notice I'm saying throughput not speed
not low latency this is not high
performance computing that would be a
fourth ecosystem so in high-energy
physics the the reason that they they
have this whole software stack is
because the data sets have always been
big so I tried to find some numbers on
this and the best I could come up with
this the SPS which was like Large Hadron
Collider of the 1980s and discovered the
W and Z bosons that was bringing in
about a hundred gigabytes of data per
year and so it's the 1980s 100 gigabytes
of data per year you have to have big
warehouses underground with rolls of
tape that gets pulled up today the LHC
Large Hadron Collider is bringing in
about 25 petabytes per year it's
probably the same rooms but you know so
for decades high-energy physics or help
seemed to be the only field that had
these really big data problems at least
that's how it was perceived and it was
mostly true
and so that's why they developed all
their own software now clearly this is
no longer true clearly hepps software
needs overlap with the SyFy our psyche
at Lin scikit-learn world and they also
overlap with the SPARC Hadoop no SQL
world individual physicists and projects
like Diane hep which is my employer are
starting to explore and develop these
connections and this is a very exciting
time because what we're doing we're
sitting on a very large ecosystem with a
lot of code developed and we're trying
to connect it to another very large
ecosystem with a lot of code developed
so it's kind of like sitting between
Lake Erie and Lake Ontario and opening
up Niagara Falls you can just write a
small amount of glue code connecting to
things and you just opened up a huge
number of possibilities
so so let's break down what we're going
to do with this software I will break it
into three main categories type 1 is a
high-energy physics software that serves
exactly the same function as software in
the lighter community and what we would
do with something like that is well
mostly we would replace it like the the
wider community has a lot more resources
for maintaining code catching bugs and
revising designs than physicists whose
primary occupation should be you know
discovering particles not writing code
the second category domain-specific
software for half applications so for
instance software packages with names
like Higgs combiner obviously we would
keep that because this really is a
unique problem and it will always be a
unique problem you know every field
probably has its own domain-specific
code the third type is perhaps most
interesting for a conference like this
it's hep software that would software
and also software concepts that would
benefit the wider community and these we
want to promulgate we want to advertise
and evangelize and so that's part of
what I'm going to be doing here in this
talk the cultural exchange goes in both
directions I'm going to argue that
histograms are an example of type 3 and
if you're thinking to yourself what I've
used histograms before this is not new
hear me out all right what I'm going to
argue is that hep has a unique and
useful approach toward building
histograms so but before I go in and
spend the rest of the talk talking about
histograms I really ought to define it
for in case anybody hasn't heard
histogram is a abstract concept in
statistics where you want to approximate
a distribution and you form that
approximate approximation by
partitioning along one of the features
so you like petal length of flowers and
then counting up how many
items in the data set fallin teach of
those bins and then how you represent
that either as the number in the bin
with a given you know the fit the
criteria or a fraction of the bin is a
philosophical debate two people have but
really the data is exactly the same so
I'm not going to do that as far as I'm
concerned to count four bins is great
you can make whatever y-axis you want to
make so now histograms are definitely
used outside of high energy physics it's
a basic statistical technique I racked
my brain and came up with just a number
of statistical packages outside of high
energy physics and I just looked up the
their histogram api's this was a true
experiment I've came up with a list of
packages before I before I looked them
up and what every one of these has is an
argument for passing in the input data
and another argument or set of arguments
for describing how you're gonna bin it
now I'm going to emphasize that point
they all require all of the input data
before giving you a histogram objects
that you can plot this is the difference
kept software and these are all of the
the packages that I could that I could
name that that make histograms some of
them are a rather obscure all of them in
high-energy physics treat histograms as
fillable containers not a charts that
you plot the constructor for this object
takes a pinning you know you specify the
bending somehow but you don't put the
data in at this point you get h which is
an empty histogram if you look at the
bin counts they're all 0 and then you
fill it however you want and that's
important the this ap I presume
that the dataset is too large for a
single function call so you will
probably need to not just you probably
need to go over data that's on disk at
least because there's no way that the
data is gonna fit into memory and you
probably have to go I've run this on
multiple machines and combined the
results later so just the API is
intrinsically single-pass so some of the
fancy things that the other packages I
mentioned on the previous slide do that
pre scan to set an optimal bin width or
number of bins you can't do any of that
because you could never do that in a
truly big aggregation and just
interestingly enough one of these
packages H book I found that the first
manual was 43 years ago so it's been
they've been doing it this way
consistently for a long time and it's
worked out
moreover histograms are like the basic
building block of all of these high
energy physics analyses so all of the
analysis code are used to build just
about anything else this is just like
some random plots like I found on the
web of things made out of histograms
some of these are from very old packages
some of them are fairly recent when
you're thinking about how histograms are
used in high energy physics analysis
scripts you should think it's kind of
like lists in Lisp or dictionaries in
Python or data frames and are they're
used well beyond their original
intention so for instance there are
there are quite a few very common things
that you want to plot that are made out
of these histogram objects but really
they aren't histograms and a
statistician would get all upset if you
called them histograms the first is
arguably a histogram it's stacked
histograms like if you want to make this
plot where you're showing the data and
then simulations of all the different
kinds of contributions you can get to
that data you stack them by
you make eight empty histograms one of
them you fill with only one contribution
the next one you fill with the first two
contributions all the way up and so the
last when you fill with all the
contributions so that when you overlay
them it looks like they're sitting on
top of each other in fact you can see
that this one here has been reweighed
'add and it's sitting on top of that
lower curve you can make efficiency
plots which is the probability of
passing some filter as a function of
some variable this is in no way a
histogram but you can make it by filling
two histograms one of them is called
numerator the islands called denominator
and the numerator you apply the cut then
you do bin by bin divisions to make that
plot
there are profile plots which is like a
slice in a dimension of the data where
in each one of these bins each of those
crosshairs there is the mean and
standard deviation of the Y data in a
bin of X and you build that by filling
the histograms with sums and sum squared
now the reason I wouldn't add I wouldn't
advocate going in using this API exactly
as is if there is a problem with it and
I'll explain how we encountered that
problem that hadn't been encountered for
43 years the problem is that
domain-specific knowledge
enters in two places first you need to
know something about what it is he wants
to aggregate in order to set the bidding
strategy in order to set a reasonable
bidding strategy then you need to know
the domain-specific knowledge again when
you're actually filling it with values
you have to compute those values and
interestingly enough in the merging if
you did this in parallel and you want to
merge jobs that does not require any
domain-specific information and that
part is easy so we ran into this because
we're trying to do some kind of physics
data analyses in Apache spark and spark
has a functional API the traditional
interactive analysis script would look
like the thing on the left and the spark
equivalent is on the right
so here you have the constructor you
have a for loop that you fill and then
you can plot a spark equivalence you
have an aggregate method that you give
it an empty container and it sends that
to all of its nodes and it runs an
increment function on it and then it
pulls the the results back and combines
them with a combined function adding
them together and you can just fill in
the blanks that the is where you put the
constructor this is what you put fill
and this is where you put add that's
fine
the problem comes in when you want to
make multiple histograms and a typical
analysis script you know a few hundred
you know lots of different things you
want to look at lots of different views
of the data then the problem is that
this increment function has to get the
histogram as an argument into the
function and has to return it as a
return value because over here you just
use global state you know XY and z are
global variables and you fill them over
here they have to be passed in every
time you call the function and pulled
out of the return every time you call
the function and you can do that by
making a tuple and having tuple indices
but you know it's it's looking kind of
bad for three histograms and you know
what kind of indexing mistake you might
make here and here it gets even worse if
you have 100 or 200 histograms so
solution to this the way that we can get
the best of both worlds
is to augment that original design still
have this thing where we are we
construct an empty histogram and later
fill it but now make the constructor a
higher-order function let it take a fill
rule as one of its constructor arguments
and the fill rule describes how to take
each datum and turn it into a value that
you've been on you decide you use that
value to decide which bin should get
incremented so now all of the
domain-specific knowledge is in the
constructor and you can just fill it
blindly right
you don't need to know anything about
the particular physics analysis you're
doing in order to fill it oh and by the
way I'm using fill as an in place
operation if you're working in a purely
functional environment where that's not
allowed
it's it's easy to to talk about non
in-place operations that's sort of
orthogonal to this so here's what this
looks like in spark you can define in a
library once and for all
increment and combine because they don't
involve any domain-specific knowledge
and then in aggregate the binning and
the fill rule are domain-specific
knowledge that go into the constructor
and then increment and combine are just
your library functions and then that hub
where that helps you when you're trying
to scale this up to a bunch of different
histograms is you can make I'm gonna
call this class label because it's gonna
make a bunch of labeled histograms named
histograms you can write this class also
in the library once and for all
it's something that has the same
interface as a histogram fill and add
but the end so you can construct a
package a group of histograms with this
nested constructor but the increment and
combine functions are exactly the same
incrementing combined function so you
didn't have to use a different one so we
can do an elaboration on this well I'm
sorry
one slide behind so the the thing I want
to emphasize about it is is this
packaging label has the same interfaces
histogram namely the fill and plus
methods and so histograms and
collections of histograms are now
interchangeable so here here's how we
elaborate on the interface we could
imagine making a lot of different
aggregators in general the
generalization of histograms and
aggregators that's you can compose so to
make the example on the previous page we
can split histogram down
- its basic Adams bin and count because
a histogram is binned
counting and you could just imagine
composing those two just like you can
compose the histograms in the label
thing and with the right set of
aggregators you can build anything you
want the first of these is a traditional
histogram bin of count so they're the
constructor has the the the binning
rules and the fill rule and then what
what we're saying that's new about it is
that the values should be counts now you
can make 2-dimensional histograms which
you might think of as heat maps you can
do that by doing bin a bin of count
because what's a heat map other than a
histogram in X where each bin contains a
histogram in Y that's a heat map and
then so bin a bin of count you can make
the profile plots I was talking about by
replacing count with some new aggregator
called the viet that aggregates a mean
and standard deviation you can mix and
match different bidding methods
irregular binning if the if you want the
spacings to be different
sparse pinning if you have something
like a hash map of counts instead of an
array of counts categorize this is what
I call a if the the beans are actually
strings categorical strings you might
know that as a bar chart you can make
complex trees of whatever it is you want
to you want to build up so here's one
where I've been something in X and that
in each bin I put a count of
minimization and maximization an average
sum of weights sum of weights squared
because these are useful things when you
are doing some advanced statistics
actually in those histogram packages I
didn't mention a lot of them are doing
things like collecting the sum of
weights and the sum of weights squared
because they know that users are going
to want that but that was hard-coded in
now it can be constructed
you can have high-level interfaces to
common patterns like the efficiency plot
I showed before you could create an
aggregator called fraction that builds
that for you or the stack plot you could
make a aggregator called stack that
builds up for you and the advantage of
this is that it guarantees that the the
bins of the the contained object will
align because they are cloned so a
package that does this
it's called histogram err because the
compositional thing is kind of like a
small grammar and so the website for
this is histogram org histogram err is a
suite of composable aggregators with a
language independent specification so if
you're wondering is this just a spark
thing no it's not it's a it's supposed
to be general several language versions
Python and Scala are the most complete
an interchangeable JSON format so that
you can get data that has been
aggregated in one system in one language
and move it to another system that maybe
has better plotting tools so you can
plot data I call it a plot data that
comes from hard-to-reach places
multiple filling backends aux I'll show
a couple examples of those and this
package has no built in plotting because
there are plenty of plotting libraries
out there some of them within the Heine
physics world and some of them
matplotlib boca did you know there are
plenty of plotting libraries so I'm just
going to call these front ends and
translate these objects into them and
then you can use whichever one you want
so from users perspective the user has
something that they want to aggregate
and so they think about how to express
that in these composed aggregators which
is kind of a declarative language
the constructor you could imagine the
constructor expression the nested thing
is a language and then you have multiple
filling backends to choose from
depending on where the data are that you
want to a great over and then you have
multiple plotting front ends and would
like this this list to grow so it's like
this octopus has got his fingers and
everything right and that's also how
this is this is acting as glue code
between the hundred physics world and
the big data world because if this
package can't has its fingers in both
then it can get data across coverage not
important other than just showing that
the that this is how we think about it
on the roads those are all that caught
the the aggregators and then whenever
you start a new language or a different
kind of back-end you just go down the
line implement each one and then it's
interchangeable with all the other
systems that know about histogram ER
these growable question mark is because
on the GPU we have to think about how
how you can deal with an algorithm that
is not fixed memory and so maybe
there'll be a solution for that but
actually if anybody has an ideas I'd
like to hear and it's designed from the
very beginning for distributed workflows
like this was the the motivating example
in SPARC he wants to fill in parallel
combine and then get everything back
down to the head node so you can plot it
if you are analyzing data on GPUs well
it's exactly the same picture except
that now instead of having a head node
and executors nodes you have a host and
device your CPU and your GPU so same
concept it really should be some of the
same code these are some screenshots in
the specification just to show you
what's what's up there you know a lot of
introductory material exactly laying out
what the the constructor arguments
should be in each language so that it's
really is interchangeable laying out
what the the fill and combine methods
should be and to make this really
explicit I decided to to write them in
various
simple Python not very performant Python
but just just to show exactly what it
ought to do and so we can do cross
language testing to make sure that they
all do exactly that the JSON format is
is fully defined and this is what a
traditional histogram looks like the
Jason forma was designed so that you can
read these so okay so and then the
tutorials page there are tutorials for
getting involved in this for getting
started as a user if you start playing
with this and I'll write a blog on it
please tell me and I will link to it
from here because you know I just want
whatever kind of introductory material
is is useful for somebody so some
examples of these back ends in spark if
you are working with rdd's you provide
an opaque function this is a scala
function you make bin of the counts is
it's the default so you making a
histogram by just saying bin and give it
a function and then the new increment
and new combine is doing some Scala type
stuff in order to infer all the types
from that spark SQL you write them down
as the spark SQL column syntax with the
dollar signs in them if you're familiar
with spark SQL this is this should be
familiar with you you can just put that
right into the constructor in Python and
numpy you can write them as strings
because in Python you have eval it has
an umpire optimisation that is for some
of these aggregators much faster the
Python has a JIT compilation back-end
that generates C++ code on the fly and
compiles and runs it in LLVM but now
that the difference is that it has to be
a string and it has to be C
that goes in there and this is
consistently faster in fact it's also
faster than generic C++ code GPU is a
special case of get that actually I
think I'm going to zip by but if you are
developing GPU algorithms you can use
this for diagnosing now last thing I
want to talk about is some problems we
encounter encountered along the way
because we just started into this with
making what aggregators did we want and
didn't realize that certain mathematical
properties have to be maintained in
order to be able to have them be
composable like this so I learned along
the way that all the aggregators must be
additive so that it's independent of
whether the datasets are partitioned it
must be homogeneous and the weights I
didn't say that that you fill with
weights but you can do that and so you
can fill with a weight of 2 to get the
equivalent of filling twice this these
properties taking together is often
called linearity it must be associative
so it's independent of where the data
sets get partitioned and it has having
an identity as zero so this is a monoid
so all of these aggregators have to be
linear mono and then your result it
doesn't depend at all on how its how the
data is partitioned we had to kill a
couple of desirable aggregators because
they didn't adhere to this and if you
are mathematically inclined and can
think of any algorithms that would
satisfy those property so that we can
get aggregators that do that I'd very
much like to hear it finally want to get
involved here's a website it's got
everything that you need this stuff is
up on maven central and pi PI you can
run it today do you want to do
interesting things with it like you
think it could fit into your application
do you want to rep histogram or skull as
high view def so you can use them in
hive do you want to do any of those
things I'm running out of time
then please talk to me
I would love for the octopus to have
more tentacles here I just thought no
right up eight more things I'd like to
see especially like to see a UNIX
command-line tool so you can talk into
it so so talk to me after this if you're
interested
okay we've got um we've got the full ten
minutes for questions yeah in the back
what was yes being able to make
histograms in hive would be wonderful
right that is a potential project but
you see you've got the Scala code so you
just have to make you Det you - they're
all aggregation functions this doesn't
matter wrapping up and making sure the
types match yeah
yeah so the question is are there any
restrictions on the functions that go in
and that's language dependent in no it's
kind of not independent of language they
have to be consistent with one another
so if you're aggregating over a data set
of events with type D and your functions
all have to have an argument of type D
right this is enforced in Scala not
enforced at all in Python right stuck
typed not enforced in Giulia because it
happens too late yeah it depends like
how its enforced depends
oh you oh yes
you shouldn't make functions with side
effects yeah because you don't know for
different back ends when and how these
are going to be computed in the numpy
back-end for instance it does one
histogram bian at a time so it starts at
the first bin does everything in there
second bin does everything in there
which is a completely different order
than if you were going through the data
row by row yes they shouldn't have side
effects but this is functional
yeah so that was that was what was wrong
with both of these both of these
adaptively binning to determine the the
bidding strategy as you go along in a
one pass you know like these
incrementing histograms way actually the
algorithm I use there's the same one
that is used in hive and quantiles
that's also adaptive the problem with it
being adaptive is that its history
dependent so if you partition
differently you'll get different results
they will be approximately the same but
now if you go on some ten node spark
cluster that is going to partition the
data into ten bits or a you know a
thousand core GPU that's going to
partition it much finer you'll get very
different results and we don't want that
it's you it would just be unreliable
more questions
No that's that tentacle over there
this would make a great actually it's
two of them a streaming tap like if you
want this to be tapped onto some bolts
in a in a storm network and just watch
the data as they go by and aggregate it
as it goes along and then outside of the
storm network send the aggregated
results because we remember they're
small they're just these little JSON
objects to something that say
continuously fills some animation on a
dashboard that would be cool right yeah
right
you packed in the high-energy physics
applications when I often would just
rapid-fire fill those our plot the
histogram as it's filling just to see a
cup and hit control-c if I it's not what
I wanted it to be then I have to wait
for the whole thing
so yeah please please talk to me if you
are interested in any of these
extensions the involvement can be a
little or a lot you know depending on if
you want to write some new language
implementation of it or if you want to
just tie in something to your favorite
application</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>