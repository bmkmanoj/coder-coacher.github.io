<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;A core.async Debugging Toolkit&quot; by David McNeil | Coder Coacher - Coaching Coders</title><meta content="&quot;A core.async Debugging Toolkit&quot; by David McNeil - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;A core.async Debugging Toolkit&quot; by David McNeil</b></h2><h5 class="post__date">2014-09-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oPv-TY_XQ60" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon my name is David McNeil I
am a closure developer at Milano core
Orlando cloud team at viasat and I'll be
talking about a library that we've built
for working with Korey sync code so core
async is a closure concurrency library
and as an aside all the work and all my
slides are talking about the closure the
proper closure version of these things
not the closure script version just
because that's the the context which I'm
working in but quarry sync is
fundamentally about the idea of creating
these concurrent threads that can
communicate asynchronously via channels
so it's all built to be concurrent so
you could have many threads all
producing items going into a channel and
on the opposite side many threads kind
of concurrently taking items out of
channels the basic operators you have
our first you have go so anything inside
of the go will run and they separate
concurrent thread and it's not an OS
thread it's not a JVM thread it's then
it's a lighter-weight mechanism that's
part of quarry sink itself you create
channels for communicating items by
calling Chan and here I'm showing
calling it with the value 3 which will
set a max buffer size in that channel of
three of three and then you can read and
write from channels so greater than less
than bang the greater than bang to a
channel you can write a value X and then
the the consuming side you can do less
than bang on a channel and that will
produce an item from the channel so we
take those operators and package them up
in but I'll kind of to call an
asynchronous function so here we've got
a function f which inside of it is using
go threads and channels to do something
in this case it starts 3 threads 1
writes a value of 1 to a channel the
other one writes a value of 2 to another
channel the third channel or the third
thread reads both those channels sums
the result prints out the answer and
this basic way of packaging up
asynchronous code into a function is the
basis on which the library we built runs
so if you had a function like this that
did
sink things then you could take this and
you could run it inside of our what we
call step machine so this shows creating
a step machine in general you can give
it configuration parameters in this case
just takes all the defaults telling that
step machine what function it needs to
run and it expects to have a function
that is doing core async stuff inside of
it and then this steps through it kind
of an arbitrary number of times in this
case ten times and for this stepping you
can kind of think of a debugger
debugging your regular code where you're
stepping line by line but in this case
in simplistic terms what you can think
of as doing is it's stepping those
threads from Cori sink call to Cori sink
call and then after stepping at ten
times we print out the state of the
machine so the state of the machine is
maintained as as the code is running and
it can be produced as a closure data
structure and here you see it lists all
of the threads all the channels what's
in the channels it shows you what's
takes and puts are blocked this is kind
of a broad use of the word blocked it
doesn't matter cisely to what you'll
find if you talk about parked or blocked
threads inside of Cori sink itself this
is just a more general term but here you
can see like what all the threads are
doing what the last thing was done
basically you can see the state of that
overall machine as it's running so if we
take it a step further and change that
original code to make use of some
features that aren't in Cori sink so
here I'm adding actual string labels to
the two channels involved channel 1 and
channel 2 and I'm giving labels to the
worker threads as well t1 t2 and t3 I'll
flip a couple of options on the step
machine I'll tell it to keep track of
all the history of all the items that go
through channels and of course you can't
do that in general but there's specific
cases where it might make sense to be
able to do that and then I put this
other switch to turn on state tracing
for the machine and what that will do is
every time the Machine the Cori sync
machine reaches a new state it'll record
it'll capture the state that that map I
showed before with all the threads and
channels and what the state is
so for the talk I made a little
throwaway visualization tool based on
this visualized library and it reads
through all those traces and it can show
you you know what happened during the
execution of the machine and you can
step through it this is showing a
snapshot at a certain point in time so
all the the rectangles here represent
either threads or channels the arrows
between them it analyzed like the whole
history of the trace and figured out
which threads put two which channels and
which threads take from which channels
and so the the direction of those arrows
indicate that the thread boxes they're
labeled t1 and t2 here you can see
they're annotated with indicators saying
if they're reading or writing and what
channel they're currently reading or
writing from similarly the channels are
annotated here with two vectors you see
C 1 and C 2 both have two vectors after
their name the first vector is all the
items that have ever passed through the
channel and the second vector is are the
items that are currently in the channel
so basically what like my mental model
when I'm thinking about this library is
that these core aceing are these
asynchronous functions that use core
async operators are like spring
activated windup toys so you could think
of having a toy and you know it's got
its exterior and then inside of it
there's various Springs and gears and
you can wind it up and then you can let
it run and as you're letting it run you
could imagine maybe just letting it run
a little bit you know just the turn of
one cog of one gear or you kind of let
it run till it winds down that sort of
thing but in any event this blob of core
a sink code is what's inside of that
machine so that's what I'm trying to
render here is that this border on the
outside represents the outside of that
function f that we were talking about
and this is just in general and then
inside of it there's various threads and
channels and things happening so there's
all this stuff going on inside the
machine which normally you can't see if
you're just using straight up quarry
sink but this tool lets you gain some
visibility into what's inside of there
and of course it's not all just what's
inside of the machine these machines
will inevitably have channels that allow
you to interact with it either on the
input side or on the output side
so this is my basic mental model that
I'm I have in mind with the library that
I'm talking about today so like I said
with the kind of windup toy you can
think of two ways of letting it step or
do its thing one is just take the
smallest increment possible and that's
what step machine does if you just say
if you ask a machine to step it will
choose one of the threads and let it run
from the place it currently is to
basically its next invocation of a
quarry sink call so it's kind of taking
the smallest increment occurred from a
quarry sink perspective the other
operation you could run is you could ask
it to step off and what this does is you
can think about that wind-up machine
it's like just let it do its thing until
it stops and just because it stops
doesn't mean that it can't do anything
in the future you might like touch it it
might start working again but at least
it's kind of quiescent for the moment
and depending on what you're doing
either one of these modes might make
sense if you're more worried about the
internal operation of how all your
threads are working it's just single
stepping might make sense but if you're
more focused on kind of the outside the
interface of how you work with it it
might make more sense to just kind of
let it run down between between steps so
both of these calls from the calling
threads perspective are asynchronous
that is the machine will just go off and
do its thing separate from you if you
want to sync up with it there's
corresponding calls to actually wait for
it to finish either a single step or
wait for it to wind completely down so
all these mechanisms I'm talking about
the basic idea is to turn this core
async execution from a black box into a
white box that you can see inside of to
be able to further reason about what hat
what's happening what the current state
of that machine is
so these are all means of accomplishing
transparency this is a list of all the
operators that are supported currently
by the library notably missing is our
transducers because this work predates
that so it's not in there here's an
example of using another one of the core
async operators partition which will
just divide up the items coming through
a channel in this case into pairs so we
write in one two three ninety nine we
get out pairs of those on the output
Channel and the point of this is that
not only can you use this step by Sync
library for visualizing and seeing what
your code does but you can also use it
for understanding what the built-in
operators do as well so in this example
you know our code is read is creating
channel one and then we create channel
two by a partition call and we just have
one thread t1 but if we look over here
and visualize what actually got created
we see our t1 or c1 but then we see
these additional channels and threads
that were created by the partition
operator itself so we gain visibility
into what those the built-in operators
are doing so other aspects of features
that we have you might have a large
network with lots of channels and go p--
threads talking on them and you might
just be debugging part of it so there's
mechanisms in place to allow you to send
items onto channels from outside of the
step machine and from inside the step
machine out I just showed a simple case
of a single function f but you could
break it down into F and G and H and
compose those and run them under the
debugger and it would do what you expect
I didn't talk about all tests it's
basically like C select for sockets
where you can do you can read from many
channels simultaneously and only one of
them that's available to be read from
will actually be read from you can
actually also use that in the other
direction so you can selectively try to
write to many channels at once and only
one will succeed that's not supported
today just because it was a blind spot
in my kind of view of how I used core
async but there's no fundamental reason
it couldn't be
extended to do that it supports sliding
and dropping buffers which is a feature
of core a sink and then also sports this
idea of conditional breakpoints so
instead of manually stepping it to where
you want you can register a breakpoint
which is basically a predicate function
that will be given the state of the
machine it can do whatever computation
it wants and say should that stop or not
so this is a case where you could start
it running and it will stop when there's
some important condition is met that you
want to to analyze better so the basic
idea the library is it's going to
accomplish two things it's going to make
the execution of quarry sink code
transparent the other big item it takes
on is trying to make it deterministic so
quarry sink by itself is
non-deterministic at its core it's about
starting up these concurrent processes
by being concurrent I mean the
definition is there's a non-causal
relationship there between what's
happening so that's non-deterministic in
more concrete terms you can think of
many threads all trying to take from a
channel that's empty and then when an
item arrives there's this question of
what thread does it go to and that
inquiry sync is non-deterministic by
design so I think a veil all these
issues as kind of a scheduling problems
so you've got many threads that can be
run and you need to make a scheduler
decision about which one to run next and
of course in quarry sync there's many of
them running like in parallel on your
machine so what stuff I think does is it
enforces them all to be run actually in
series and it'll make scheduling
decisions about which one to run next so
subjects link has this idea of you know
we'll plug in a scheduler if you
implement the closure protocol you can
plug it in it'll call you every time
it's got a decision to make and then you
decide what to do it comes with two
scheduled or algorithm algorithms the
first one by default is just a simple
deterministic scheduler so if you think
of the scheduling space is kind of this
maze you have to walk through it just
kind of says always go left kind of
thing it always does the same thing the
other one it has is that's provided with
it is random scheduler so here it uses a
pseudo-random number generator to like
generate a random number to inform the
decision about what
scheduling choice to make at each
juncture if you want to use that you
instantiate the step machine with a
random seed and then that will see the
the execution paths that will seed all
the random numbers that are generated to
compute the execution path and so if
your code is deterministic itself and
that the only source of non determinism
is quarry sink use then this will allow
you to deterministically run repeated
executions of the exact same random path
in pseudo random paths by giving it the
same random seed
so this is an example where we've got
some core async code that normally runs
just fine but there's actually a latent
deadlock hidden in this code so what
this is doing is setting up two channels
and then three threads and if you look
at it kind of closely you'll see that
worker thread 1 and worker 2 they're in
this kind of embrace what's the worker 1
rights just channel 1
worker 2 receives a message from it and
on channel 1 and then writes to channel
2 which w-want which worker 1 receives
so they're kind of talking to each other
both these channels I've explicitly you
know made them with size 0 which they
would have by default but there's no
buffering there which means the
producers and consumers have to
rendezvous at the time when an item is
produced if there's not an item there
and you try and taking from it try to
take from it you'll actually wait there
for an item to appear similarly if you
right you'll wait for a consumer to
appear or a producer consumer to appear
before your put completes so the point
is w1 and w2 are kind of locked in this
embrace where they're talking to each
other and that will work fine as long as
w3 doesn't run before w1 so w3 is
writing the channel 1 and it's maybe
it's kind of hard to see but it'll
actually cause it the machine to
deadlock if that happens before before
worker 1 completes so this thread sleep
in here is meant to represent like some
kind of long-running operation you'd
have in your code so maybe that usually
takes a while so usually this code works
fine but there's a race condition
between worker 1 and worker 3 which if
worker 1 loses then the machine will
deadlock
so with code like this and with the
ability to provide these random seeds to
the scheduler we can make use of an idea
that I think the Genesis for me and this
is read rapers talk at closure West last
year we're just talking about using test
check to programmatically find race
conditions so this is using test check
which is a closure property based
testing library kind of point out some
of the highlights here so what this is
doing is it's saying it's asking test
check to randomly generate a value are
which we'll use as a random seed to seed
our step machine and then we'll run that
function that we were just looking at
we'll ask the machine to step all the
way until it's done running and then
we'll look at the final output value
output state that arrives in there's a
token that's returned to either say all
that to indicate why it has completed or
why it the machine ran down either
because all the threads exited and
there's nothing else to do or because
all the threads are currently blocked
and they can't proceed so this will pick
a hundred different random numbers feed
them in and try to test that this
property is always held true that is the
machine always exits so if we run that
if you read through all this at the
bottom it tries to find the smallest
value that violates the property in this
case it found the value one will violate
the property just to check to see if
that's legit we can run it through so if
you run it through with the random seed
of zero we see that it's all exited we
run it through with that random seed of
one which it indicated was a problem we
see oh it everything's blocked which
means we've deadlocked so this is pretty
cool because what it does it takes this
concurrent code it's non-deterministic
it's going to be running different paths
at different times and it searches
through all the different scheduling
paths to find one that's problematic so
it's out there proactively trying to
find race conditions for us and not only
that but when it finds one it gives us a
value with which we can then reproduce
the failing race condition sequence and
of course once we can reproduce the
problem we're well on our way to you
know getting it fixed if you want to
visualize what this looks like this is
the actual deadlock case so here you see
worker one is writing the channel one
and then at the bottom worker two is
writing the channel two but there's
nobody reading from either of those so
it's it's going to sit there forever
I think I'm in good shape on time so any
questions at any time raise your hand
yes mm-hmm
right so if there's a deadlock how does
the test finish well what this code does
is it asks the it seeds the value into
the step machine which is a value of 1
which it's going to deadlock on and then
it says okay run until you can't run
anymore
and then this quiesce wait just it's
basically like we're waiting on a
promise for it to stop
so internally the step machine schedules
all these threads makes scheduling
decisions based on that random seed and
then the step machine is analyzing this
the state of all the threads and what
they're doing and it determines that
nothing else can be run so when nothing
else can be run it returns from this
quiesce wait call and says ok I'm done
running there's nothing else to do so
yes
right and it depends like oftentimes
your quarry sink code is spawning go
blocks which are running asynchronously
so really you're not blocked as a caller
into that so the test wouldn't
necessarily block it's just there's this
machine out there that you can't see
into that is in a state which is
deadlocked so yes so the way so this is
starting up these three threads it's
only going to run these threads ever in
sequence so at some point it decides to
run this go block from basically you can
think I've run from initiation to the
first core async call which is here
so it run it runs that at one point in
the overall sequence and nothing else is
running at the same time so just however
long that takes it takes a second and
then it finishes yes that's right
so the tick of the machine it's not per
line of code it's between core async
calls essentially yeah
right yeah I've got a slide on that if
you can just wait a couple slides so
I'll press on now
okay another source of non-determinism
are timeouts so timeouts if you call
timeout 200 it makes a channel which
after 200 milliseconds will produce a
value which is no but anyway you can you
can treat timeouts like channels but
that's another source of non-determinism
right because you don't know the
sequence which timeouts are going to
fire exactly the way step async handles
that is none of the timeouts you in your
code ever fire unless someone tells it
to fire so externally if you have one of
these step machines you can ask it what
are all the timeouts in the machine and
it'll give you a data structures like
this and then if you want to
programmatically fire one of those under
your control you make a call into the
step machine and saying say fire this
timeout now so you can kind of get
control of that source of
non-determinism alright this is your
your question so this is the same
example from the very beginning but now
I added on the namespace information to
show that we're using these closure core
async operators so if you want to use
step async the way it currently is what
you do is you bring in step async
version of these operators instead if
you do that then the function that you
write here with these if you don't run
it in a step machine then it'll just
delegate all that to built-in quarry
sink and it'll operate just like quarry
sink but it gives you the function that
you produce like this is a function that
could be run inside of a step machine to
do all this kind of controlled execution
so this leads to another way of
understanding what is step async it's an
alternate implementation of a big chunk
of quarry sink in particular the
channels and the operators once you have
the primitives actually most of the
operators are just a really straight
port of the quarry sink code there's a
few of them that do more exotic things
but many and we're just a straight port
it's just built on putting and taking
from channels it doesn't reinvent the go
macro so it just intercepts all the
calls to go and kind of registers what's
happening but then it delegates all that
work to the go macro so it's a different
it's an alternate implementation of the
Cori sink operators and we're core async
focuses on kind of these non-functional
parameters of being very efficient and
really enabling parallelism so not kind
of like squashing the opportunity for
concurrency step async instead focuses
on doesn't focus on those at all in fact
it's all serialized and it's horribly
inefficient but it's deterministic and
it's transparent and you can control it
programmatically from the outside so
this is a glimpse into implementation
details there's a def type which defines
the step machine itself it has a bunch
of values many of those are refs inside
the operators it's coordinated via the
the closure STM it's not necessary this
isn't necessarily the way you'd have to
write it it's just kind of stylistically
the way I chose to do it so to
understand the implementation you think
about regular core async you're making
these go threads you're making these
channels those really they're not like
unified in any way they're just objects
on the heap that happened to have
references to each other if you watch
Timothy Baldrige his video on garbage
collection with quarry sink he really
kind of makes that point so there's no
sense in which these are like
centralized together and there's anyone
watching what's happening
it's just however they happen to be
laced together on the heap and that's
how you get garbage collection of all
these things well step async doesn't
follow that model at all step async has
this
centralized view of everything that's
happened within that machine it all gets
pulled together in a central place where
it can keep track of these are all the
threads these are all the channels and
then it really intercepts all the
Quarian calls that you make and
registers them in that central place and
then that central place decides kind of
spoon feeds out then you know what to
actually allow to run next and this
picture is kind of horribly confusing
but the main point is there is the
central place that's that's tracking it
all which is not at all what you want
like in general like so I think like
core I think is I'm sure doing the right
things in this regard but there are some
context in which it's useful to be able
to have more visibility have more
control and so if those are things that
are useful to you then this is a
possibility for you so this is an
example that's a bit different than the
others that I showed so what this is
doing it's again one of these
asynchronous functions called
accumulator this is actually taking in
an input channel or an output channel
which I hadn't shown in the previous
example but you can do that and what
this does is it reads all the values
from the input channel expects them to
be numeric values and it adds it keeps
an accumulation of all the sum of all
the values that have come in so it reads
the value in adds it to the the old sum
and writes that sum out on the output
channel and then sits there in a loop
doing that so if you ignore whatever
magic core async is doing you can think
of there's like a value on the stack
somewhere that's kind of accumulating
this state and the point here is there's
some state inside of this function that
isn't like strictly part of the core
async state so it's not like a thread or
a channel or something like that it's
just some other state of the machine the
actual virtual machine so if we run code
like this and again flip some switches
here so I set this action history to
true and I've alluded some included some
code here that makes a little messy but
if you run this in a step machine then
you can do things like this so here I'm
showing I write in a value of 1 which
causes a value of 1 to come out on the
output channel and then on the input
channel I put in a value of 2 so I get 3
which is the sum of 1 and 2 and I write
in 3 which is the sum of the previous
value 3 and 3 so that's just like what
you would expect but then again if all
that code is deterministic and the only
source of non determinism is the query
sink calls and if it's purely functional
there's not side effects that are bad to
rerun then you can do things like this
you can take that step machine at this
point after we've written in 1 2 and 3
and so we've got a value on a stack
somewhere saying there's a 6 and as the
current state of the machine we can ask
it to go backwards so we kind of say go
back to the previous state like go back
a couple of steps it turns out to be 2
based on the way that like when you put
something in there's like multiple steps
to actually putting something into a
channel but anyway we go back two steps
and then ask for the machine that
returns us a new machine we can ask for
that machine to kind of stop doing
whatever it's doing which in this case
you can kind of think of it as going
backwards and then what we can do is we
can write in a new value so write in a
value of 4 and so we get out of value of
7 now so that's significant because it's
taken back a value that was written in
before and gotten that code back to a
state that it was in before and now put
in the 4 and you get out 7 which means
it's now it's it's back in the state
that it was so it's as if we just did 1
2 and then for here the 3 has kind of
disappeared so this is really just an
experimental feature in the code I just
kind of did it because I could see that
I could I don't know that is any real
practical value to it but it was it was
kind of fun to do and if you think about
it you can figure out what it's actually
doing kind of see behind the curtain
here it's not really stepping the
Machine backwards rather it's making a
new machine and stepping forwards from
the beginning of time until you know
just leaving off the last couple of
steps which gives you a new machine it's
back in the state that the old machine
was in just before you did the last two
things
so that's what I have prepared you can
see the code up on github it's code that
we're currently using in the production
code on one of our projects so there's
what we're using is a thin slice of the
functionality that I show today so
there's a thin slice which I'm confident
in and I think it's solid but as taken
as a whole it's really proof of concept
code so it needs some kind of care and
feeding but like I said we are using it
in production code and the application
we're using it in it's not for debugging
at all
but what we're using it for is we're
using Amazon simple workflow to model
distributed workflow executions and the
basic model with that is that you have
these concurrent workflow executions
that are running and then they produce
and they consume these events so it's a
model that matches closely to core async
and so on this project what we do is we
represent the logic of those workflow
executions in core async so what you can
think of is you know as the vision plays
out you can say you know we'll write you
know say a dozen workflows that all work
have to work together so we write those
as straight-up core async code and we
can run those in a single JVM and kind
of get all the kinks worked out and
they're just talking to each other as
just plain old Cori sink but then using
step async and using some additional
machinery to kind of bridge the gap
between the distributed world and core
async once we're happy that our lor
logic is sound we can then kind of
promote those workflow those core async
threads up into these workflow
executions that are now running in a
distributed fashion out on Amazon
cluster under Amazon simple workflow and
so step async is critical in that role
because what happens is there might be a
workflow running on a given machine so
it's receiving events and it's making
decisions producing output events and
for whatever reason just you can imagine
now we need to move that to another
machine and so when we go to move that
to another machine the ability for us to
programmatically like
access the state of that machine and
programmatically like recreate the state
of that machine on another host it's
what enables us to kind of maintain this
illusion that these things are just
running in process when they're actually
kind of being time sliced across hosts
so that's all I have prepared thank you
and ya have time for questions
don't thank you
No so the question was can you just
drive it with data not with channels now
you have to use the query sync API or
the API that I've shown I'm not I guess
I'm not sure what you're asking so just
inject that in like say this is my
current state I'm in this state now run
from there I see so certainly for the
parts that are being explicitly tracked
by the step machine that is the channels
the threads the listeners I mean there's
certainly no reason you can't inject
those things in the problem is there are
actual there's actual code somewhere
that has go blocks that's in the current
state and that state is not represented
inside of the step machine but it's
relying on there being actual go block
somewhere that the cusps that the user
wrote that are in a certain state and
without stepping through all the states
to get them in the right place then I
don't see a path with the waders right
now to get to make that happen
okay okay
right well I didn't show it I did this
really crude UI which just kind of
renders these things I had a kind of a
pseudo spec for like a real AI that
would show you our UI that would show
you these are the threads these are the
channels here's how they're
interconnected in order to kind of
interactively explore it to see the
current state and then obviously on the
monitoring side like you said you can
see you can like emit information saying
you know what state all these different
things are in I haven't done work beyond
that though don't know if that answers
your question yeah
oh is that a statement or a question
oh yeah yeah
there wasn't that example that I showed
that I think is what you're talking
about where we're using test check to
kind of run through the different paths
to find a deadlock but are you saying do
like analysis to determine when there
will be deadlock I say it's where you
try to reason about it and get it into a
certain state maybe yeah no I have not
that sounds interesting and I guess rich
to one of your points about you wouldn't
want the non deter the yeah the non
determinism in quarry sink like I
totally see that for let's say most use
cases are almost all use cases I think
there are use cases where that is what
you want like that's what we want for
that simple workflow example that I
described
we the performance of it is absolutely
irrelevant it's can we control the
execution and is it deterministic and so
that's production code where that's how
we want it to behave and when I
published the synopsis of this talk I
was contacted by an outfit that's doing
the same thing for simulation so the
reason quarry sink is the fundamental
abstraction or the the means of
describing their simulation and they
want to go to control that from the
outside to say hey let's play it a
little bit further let's go back let's
go forward kind of thing and so again
they're it's not the raw performance of
how fast it goes but it's expressing
their logic with those primitives and
then running it in a controlled
deterministic fashion ya know
it seems like it wouldn't be that hard
I mean clojurescript is already running
in this kind of serialize fashion but I
guess that's a total like tangential
issue I haven't done anything with
closure script it doesn't strike me on
the surface as being something that's
hard for any reason I just haven't done
anything with it yeah
so the question is if you do the step
all and all the threads are waiting on a
timeout what happens what happens is if
you say step all and it sees that all of
them are waiting on a timeout that's
just as if they're waiting on a regular
channel and it says oh they're all
waiting and there's no input for them
and there's no but that will ever come
to them unless someone programmatically
calls that thing to make the timeouts
fire yeah absolutely
oh very good
I see
Scot that's not the way I would say it
so what we're doing is so we need some
logic to run for workflow and that's
represented in quarry sink code so we
get a call for a workflow we fire up one
of these machines we get it to the state
we want and then we give it some input
say this is new input that's arrived I
want you to tell me what to do
I think of it like the matrix so you got
people and people know how to interact
with each other their brains they take
an input they produce output but the
matrix they wired up these people's
brains and they just like stimulate it
with stuff and then kind of got
something out I guess they got heat out
but so I think of it like that you've
got this court this asynchronous machine
which is like a person it can take input
and it does things but we're putting it
in this matrix where we just wire it up
and we say here's your reality here's
the events have happened to you and
ignore whatever you do but now here's
the new event drop in the new event and
whatever it does it decides oh this is
what up this is my output this is an
action I would like to take and then we
take that we register that with simple
workflow saying this is what this
workflow should do now and one of the
things we do since we have programmatic
control of it we can do things like ask
that machine when it's done running
which is a problem with Cori saying if
you have some Cori sync code and you
call into it like just a that's one of
those functions F that does Cori sync
things how do you know when it's done
you don't write unless you do something
explicit to allow it to communicate out
to you if you just fire it up and it
runs you don't know if it's ever
produced its last event that it's going
to produce so this gives us I mean you
can solve that problem you can have like
an output channel and it writes to the
output Channel and closes it when it's
done and you see the closed so you know
it's done but this allows us to just
more generally like fire up the code and
let it run for a while and it doesn't
have to do anything explicit to tell us
when it's done we can programmatically
look at it and see that it's done
I think I still have time if there's
more you had a question
no but you yeah obviously you could do
that yeah
I'm not familiar with mechanism you
describe it's entirely possible that if
I knew about it I should have would have
used it I will say that recently I got
to see Rich's talk from Euro closure
where just talk about the implementation
of channels and it's a great talk and it
would have been good for me to watch
that before I did this because it
obviously has you know very precise
thinking about some of the invariants
that that need to hold while you're
while you're doing this but if you're
interested in this kind of thing I
definitely recommend that I think we're
out of time is that right all right I
think we're out of time I'm happy to
take questions down here afterwards
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>