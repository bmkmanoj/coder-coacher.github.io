<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Experimental Creative Writing with the Vectorized Word&quot; by Allison Parrish | Coder Coacher - Coaching Coders</title><meta content="&quot;Experimental Creative Writing with the Vectorized Word&quot; by Allison Parrish - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Experimental Creative Writing with the Vectorized Word&quot; by Allison Parrish</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/L3D0JEA1Jdc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone
gonna start so this talk is experimental
Creative Writing with the vectorized
word I'm Alison Parrish I am a poet
programmer and game designer I'm a
member of the full-time faculty at New
York University's interactive
telecommunications program where among
other things I teach a class called
reading Alec reading and writing
electronic text which is both an
introduction to the Python programming
language and an introduction to computer
generated poetry so in this talk I'm
going to talk about some poetry
experiments I have been doing over the
past couple of years with vector
representations of language so I'm going
to introduce the concept of vector
representations to those of you aren't
familiar and then I'll talk about vector
representations of semantics and then
vector representations of phonetics and
I'm also going to read some of my poetry
that I have generated with these
techniques because I loved to read
poetry and I loved the sound of my own
voice and you are all trapped here with
me so the word poetry brings different
things to mine for different people but
for me poetry is just language that
calls attention to its own aesthetic
properties and so and that's beyond what
it means in a conventional sense in the
same way that a Rothko painting or
objection Pollock painting or whatever
doesn't depict anything the kind of
poetry that I write doesn't mean
anything it's about how it makes your
brain feel and that's all really
pretentious but what I'm trying to
prepare you if I'm trying to prepare you
for what what you're about to on what
you're about to see an experience so I
want to start with this quote from amiri
baraka as an American poet it's from an
essay that he was writing about the
typewriter about text interfaces this
was like pre personal computer but um so
he writes a typewriter why should it
only make use the tips of the fingers as
contact points of flowing
multi-directional creativity if I
invented a word placing machine an
expression scribe or if you will then I
would have a kind of instrument into
which I could step and sit or sprawl or
hang and use not only my fingers to make
words express feelings but elbows feet
head
behind and all the sounds I wanted
screams grunts taps itches a typewriter
is corny this quote is proposing that we
think about text composition differently
that instead of literally typing text in
letter by letter we should be able to
use our bodies to use other logics than
literal logics
so what amiri baraka is proposing is
pretty sophisticated we might have to
start with something a little bit
simpler so if you're not familiar with
the theremin it's kind of I think the
simplest possible electronic musical
instrument invented by this fellow Leon
theremin who had named it after himself
which i think is amazing
so I there are two proximity sensors
essentially one of which controls the
pitch of the waveform and the other
controls the volume so my question is
what is the text composition equivalent
of the theremin what are the simplest
possible gestures that can meaningfully
be used to compose text now it's simple
to make an interface with a
straightforward mapping between
gesturing and sound it's simple to make
a sound interface like this because
sound is continuous right with the
theremin you can make your hand
continually get closer to control the
pitch and the volume right but it's
harder to make a textual interface from
gesture because the units of language
like words and letters and sentences and
so forth as we generally conceptualize
them are discrete not continuous right
so for example you can model you can
multiply an audio waveform by 0.75 and
you get another audio waveform you get a
slightly attenuated audio waveform but
what does it mean to multiply say the
word deciduous by 0.75 it's not clear
what that operation should do right so
in order to make a text theremin or a
amiri baraka vien expression scriber
i think you need a reliable and well
motivated way to represent words
continuously okay fortunately over the
past 50 years or so natural language and
machine learning
searchers have devised just such a
representation word vectors and I am
pretty excited about word vectors so I'm
going to take a few minutes to explain
to you how they work so the following
will be review summary summary meetable
remedial material for some of you but I
want to make sure that everyone's kind
of on the same ground from a technical
standpoint so first and definitions a
sequence of numbers it's used to
identify a point in space is called a
vector and if you have a whole bunch of
vectors that all belong to the same data
set we can call that a vector space so a
familiar example of a vector space is
color colors are often represented in
computers as vectors of three dimensions
red green and blue right colors also
have names so in a sense color words
also have three dimensions they're a
very simple and limited form of word
vectors will talk about more
sophisticated ones in a second but I
think they're a useful way to introduce
more sophisticated examples so these are
entries from the xkcd color survey which
is a database that basically relates
several hundred color names two points
in red green blue space it's just a red
green and blue point for each one of
these colors so with this database we
can answer questions like which color
words name similar colors what's the
most likely color name for an
arbitrarily chosen set of values for red
green and blue given the names of two
colors what's the name of those colors
average right we can find those answers
just by looking at the vectors so for
example I wrote some pipe I'm just
showing you like screenshots of a Python
your oven jupiter notebook here so you
can load this into a Python program and
then just load it as a dictionary and
you can get back the red green and blue
values for these colors and then you can
make some functions that perform basic
vector arithmetic like subtracting one
vector for another or adding one vector
to another to add two vectors you just
add their component dimensions and to
subtract them you just subtract their
component dimensions and you can do
something like find the average or the
mean of the list of vectors by just
adding all of those vectors together and
then dividing by the number of vectors
in the list right so they they kind of
operate like any other number
in some senses this function finds the
closest color name for any arbitrary
point in RGB space so you give it a
three tuple of three and then it uses
euclidean distance to find the closest
vector in the vector space and there are
more sophisticated ways of finding the
nearest neighbors to a particular point
in a space but this works for our
purposes for for these examples
so just to test this out like the very
very basics of doing of doing text
composition with vectors is just seeing
like give it a word and then see what
the closest words to that are so here in
the top thing it's getting the closest
words to red predictably the closest
word to red is red but then you get all
of the other words that are sort of
similar to that fire engine red bright
red tomato red the second example on
here just picks an arbitrary red green
blue couple and is giving us the colors
words that are most similar to that so
one fifty sixty one fifty the closest
word to that is warm purple which is
what you'd expect if you know how to
read red green blue colors work so the
magical part of representing words as
vectors is the vector operations seem to
operate on language the same way that
they operate on numbers so for example
you can do sort of weird arithmetic like
this if you subtract red from purple and
then find the neighbor the closest
vector in the vector space to that
resulting value you get blue cobalt blue
royal blue a darkish blue true blue etc
if you add blue to green again if you
know how RGB color works then you get
cyan so blue plus green equals sign and
that reasoning comes purely from the
values in the vector space you can also
average black and white if you take if
you add black to light and divide that
vector by two the closest resulting
vector is medium gray so this is this is
the first little poem of the
presentation starting with roses are red
violets are blue this snippet of code
randomly selects the color nearest to
the color in the last line and then
prints out the same line again
gun on so you end up with roses are red
violets are blue roses your tomato red
violets your electric blue and then at
the end it's roses or pumpkin violets or
darkside huh roses are Bert orange
violets are sea blue these are all
colors just sort of in like the the
bloom of colors surrounding red and
bloom so the examples those previous
examples are fairly simple from a
mathematical perspective but they they
feel magical nevertheless they're
demonstrating that it's possible to use
math to reason about how people use
language so with these tools in hand we
can start doing we can use our
vectorized knowledge of language towards
academic ends so here's some really bad
digital humanities with with Bram
Stoker's Dracula so what i'm doing here
is calculating the average color in the
book simply by tokenizing the text
looking at each token seeing if it
corresponds to a color in the xkcd color
database and then just adding those
vectors up dividing by the number of
colors that's the average color right
there 147 113 100 and what you'd expect
if you just add a whole bunch of colors
together however this is what you get
when you average the colors of Charlotte
Perkins Gilman's classic the Yellow
Wallpaper which is about basically about
the color yellow so we're kind of on the
right track all right like this is
giving us results that seem pretty good
so all of those examples are interesting
and they work intuitively because of the
fact that colors that we think of as
similar are closer to each other in our
GP vector space and we can kind of think
of those words that are similar in their
position in the vector spaces being more
or less synonymous or functionally
identical so you might be able to use
this data for example if you are writing
a search engine you could be pretty sure
that if somebody's searching for move
trousers on your shopping site it's
probably okay to show them search
results for dusty rose trousers dusty
rose proud trousers brownish pink
trousers and so forth and that makes
sense and that's all well and good for
color words which you know we
intuitively understand is consisting of
these you know continuous values red
green and blue but what about arbitrary
words is it possible to create a vector
space
for all English words that has the same
closer in spaces closer in meaning
property to answer that we have to back
up a bit and ask the question what does
meaning mean no one really knows but one
theory that's popular among
computational linguists and computer
scientists and other people who make
search engines is the distributional
hypothesis which is that linguistic
items with similar distributions have
similar meanings or a word is
characterized by the company it keeps
it's the way that JR Firth put it so
look at these sentences to demonstrate
the distributional hypothesis the words
cold warm hot and cool must be related
in some way they must be close in
meaning because they occur in these
similar context like they all occur
before a word that indicates a time and
after this adverb really so just that's
just like a basic demonstration of this
principle we can kind of into it that
cold warm hot and cool are all words
that mean something similar because they
occur in these similar contexts so how
do we turn this inside the
distributional hypothesis insight into a
system for creating general-purpose
vectors that capture the meanings of
words maybe some of you can see where
I'm going with this what if we made a
really really big spreadsheet that had
one column for every context for every
word in a given source text and we use
this very very short text short source
text to illustrate it was the best of
times it was the worst of times so
here's a spreadsheet that has the rows
are all of the words in the text all of
the unique type of words and then the
columns are all one word of context
before and after right and the number in
the in the cells indicates how many
times each of those words is found in
that context I'm so for example the
vector of the word is is 0 0 0 0 1 0 0 0
1 0 and there are ten possible contexts
in this source text which means that
it's a 10 dimensional vector and edit
all of the math that we did on three
dimensions before works exactly the same
on 10
dimensions and that's difficult to
visualize in your head but the math is
basically the same so what we actually
get here is that the vector for the
words best and worst they're vectors are
actually the same they don't they're
they're exactly on top of each others
there's no distance between them which
is telling us that they're related and
meaning right and of course we usually
think of best and worst as being
antonyms but they're also both words
that are you know superlative adjectives
that are expressing an emotion about
something right so they are actually
related in meaning now we could do this
with like really huge source texts like
gigabytes and gigabytes of source text
and we would end up with not ten
contexts but thousands of contexts or
millions of contexts and it turns out
that's really difficult to work with but
most of those dimensions are the columns
in the spreadsheet don't really contain
useful information and they can either
be like smushed together or eliminated
using dimensional reduction or matrix
factorization and so you end up with the
ability to represent the meaning of a
word with somewhere between 50 and 300
dimensions and various scientists
including the lovely people at Stanford
have made available pre-trained vectors
one group of vectors is called glove the
global vectors forward representation
you can just go onto that website and
download like a four gigabyte file that
has you know 50 million words in it and
then a 300 dimensional vector that
represents that words meaning which is
really cool and then word Tyvek and
facebook's fast text which is more
recent or basically implementations of
the same idea so this is what one of
those numbers looks like this is this is
the meaning of cheese right here
there's even more numbers after this
I've been working on a way to visualize
these that I think is a little bit more
intuitive so what's so this is a quick
prototype that draws vectors associated
with words by plotting each the value of
each dimension around the perimeter of a
circle that controls how far away each
one of the little amoeba pseudopods is
from the center and so you can kind of
see here intuitively that all of those
numbers have similar shapes and all of
the months have similar shapes which
means that they have similar vector
representations except for the word May
you'll notice May doesn't quite look
like the other months that because
that's because the word May occurs in
other contexts as an auxiliary verb
right like you may go to the store now
or something like that so more fun with
Dracula this time with actual word
vectors and you could use these
techniques on any text I'm just using
the pre-trained Club vectors for this so
for example we can take the vector for
the word basketball and then ask what is
the word in Dracula closest to
basketball if if the people in Dracula
are playing sports what sport are
they're playing and it turns out that
it's tennis which is kind of interesting
right and you get that for free just by
finding the vector that is closest to
another pre-existing vector if you
average night and day and Dracula then
you actually get the vector for evening
and morning which is kind of interesting
because evening and morning are kind of
halfway between night and day here are
the ten words closest to water and then
if you add the vector for frozen to the
vector for water you actually get ice
which is really weird
so with word vectors you are actually
kind of starting to have this continuous
representation of meaning with word
vectors you can represent a text as a
continuous waveform that's to me
analogous to an image or an audio file
and an image file each sample is a pixel
and an audio file each sample is a
number representing an amplitude with
word vectors a text is basically a
series of samples where each sample is a
word vector
and since the text is now a waveform
it's trivial to use all of these same
signal processing algorithms on language
and meaning that you would normally use
on images and audio so for example think
about blurring the meaning of attacks
think about resampling the meaning of a
text at a lower resolution think about
blending one text with another write
with a word vector representation you
can just run the same functions that
we'd use to do this on an image you can
do it on a text so I've been working on
this program that allows you to apply
signal processing algorithms I'm going
to start that animation yes to word
vectors in a text and then control the
parameters to the signal processing
algorithms in real time it's a prototype
of a framework I'm calling Viktor
tightrope for weird reasons that's
probably not the real name that I'm
gonna give to it and it's a lot like any
other interactive programming framework
like processing except instead of
manipulating a pixel buffer you're
manipulating a matrix of word vectors so
on every frame the framework finds the
word nearest each vector and the matrix
and displays it to the screen and then
you have an update function that just
updates that that matrix of word vectors
so in this in this example this is a
little demo of it moving from left to
right blends between the two texts
moving or moving the mouse from left to
right blends from between the two texts
moving from top to bottom applies a blur
effect to it and then you can scroll up
and down to resample the text so it's
shorter or so it has a fewer words or
more words and eventually I'm hoping to
add support for MIDI devices in OS c so
you can use whatever input you want for
it and it's not exactly what Amiri
Baraka was thinking of but i think it's
a pretty good start so in that previous
example the two texts that I was using
were an excerpt from Mary Shelley's
Frankenstein and chapter 1 of Genesis in
the King James version of the Bible so
using a previous version of this tool I
wrote a poem from these texts that's
crossfades from one to the other
progressively averaging the values of
the individual word vectors and then
just basically stopped to take a
screenshot
at particular points while I was cross
fading between the two I mean what you
get is a series of intermediary texts
that highlight the syntactic patterns
and semantic similarities in the
original and I call that piece
frankenstein genesis and it was
published recently in a poetry magazine
called vetch and i would like to read it
for you now if that's okay so there's
the first source text there's the second
source text there's going to be a number
that pops up that shows the proportion
of the meaning of the second text to the
first text so here's 100 percent
Frankenstein 0% Genesis hateful day when
I received life I exclaimed in agony a
cursed creator why did you form a
monster so hideous that even you turn
from me and disgust
God in pity made man beautiful and
alluring after his own image but my form
is a filthy type of yours more horrid
even from the very resemblance Satan had
his companions fellow Devils to admire
and encourage him but I am solitary and
abhorred you know here's that same
source text with 12% of the Bible added
on hateful day when I received life I
exclaimed in agony a cursed creator
occasionally did you form a monster so
hideous that even we turned on me from
disgust God in pity made man beautiful
Dikembe alluring after his own image but
my administrate is a filthy type of
yours more horrid even from the very
resemblance Satan had his companions
fellow Devils to admire and encourage
him but I admitted solitary and abhorred
1/4 Genesis hateful day when I received
incumbency you exclaimed yikes agony
accursed creator greedily did you form a
monster your mileage may vary hideous
that even we turn from me on disgust
commie into pity made masseuse beautiful
Dikembe alluring a for his own image but
its instantiate is a filthy type of
yours more horrid especially saw the
very resemblance Satan had his shipmates
fellow Devils to admire and encourage
him but I admitted solitary and abhorred
37% hateful Comino harmlessly Freya
received dreamfall geez exclaimed the
agony a cursive complainer greedily did
you form a monster your mileage may vary
yes although immediately we smote from
radagast on disgust commie girls pity
maid masseuse beautiful the alluring a
for his own image but his savor as a
filthy type of ain't more horrid
especially saw the very resemblance
Satan had his kith fellow Devils to
admire and encourage as Guardians but
Freya admitted solitaire and aboard
halfway unlikable the beginning
disbelievers received op sack heaven
exclaimed the agony a cur said wrongdoer
greedily was you form a monster which
ageless which painlessly the smote of
this deep bewilderment Bodhisattva
spirit trifle misremembered masseuse
beautiful the easygoing of his thralls
image but dryly let is be filthy
keyloggers of your light horrid gangly
saw the inky resemblance Satan had
helluva cthe near Allah thought AB
blitzed the smite from encouraged as
Guardians but disbelievers admitted
solitary light scorned more Genesis
unlikable the beginning disbelievers
created kargh heaven exclaimed the agony
in groan wrongdoer gala free was you
form nor smudge which deathless was
immediately this mote of the deep and
the spirit especially atoned moved
delightful the leering of the waters
yikes
Marduk backhanded let his be filthy
Malik of is light bashful God saw the
bluish resemblance
Satan was helluva moma unbelievers split
the admire from insurable Galactus and
disbelievers labelled solitary light
ceteris I'm gonna skip to one-eighth
from the end unlikable the beginning
commie created the heaven and the earth
hyaluronic the earth was some such form
unsere void and darkness was unabashedly
the smite of the deep and the spirit
golly God moved prudish the Cour of the
waters and commie said let wherever be
light and there was light
hyaluronic God saw the light possibility
thyself was good Evette deity divided
the ultraviolet from every darkness and
commi called fallopian my day and here's
just the Bible in the beginning God
created the heaven and the earth and the
earth was out for it was without form
and void and darkness was upon the face
of the deep and the Spirit of God moved
upon the face of the waters and God said
let there be light and there was light
and God saw the light that it was good
and God divided the light from the
darkness and God called the light day
now poetry isn't just about word choice
and meaning it's also about sound and
the voice speaking poetry a lot is very
important to me so I've been working
recently on a way to create a continuous
vector space for the phonetics of words
we've just been looking at vector spaces
for the meaning of words this is for the
phonetics of words how they sound how
they feel in your mouth and how they
feel in your ears so the idea is that
you find a word take its phonetic
transcription and then turn that
transcription into a vector of numbers
so I want to be able to create like a
visualization like this where you can
see that the words octopus and
apocalypse sound similar but they sound
different from inky and kinky which are
related in the meaning or related in
sound right
so octopus apocalypse sound right there
sound similar to each other
inky and kinky sound different now
unfortunately there's no equivalent of
the distributional hypothesis in
phonetics and there's no as far as I
know no large corpus of utterances rated
by sound similarity so I couldn't take
the easy way out like the word defect
people did and just train a big neural
network to predict phoneme context and
then grab the embeddings and call it a
day I have to do like work um so that's
I should not be bad-mouthing Google
computer science researchers I'm sorry
Google I love you um so there is however
this very helpful thing which is called
the CMU pronouncing dictionary which is
a database of English words and their
pronunciations it's available online for
free it was funded by DARPA and so
Americans who tax money have been has
been paying for this
the sounds are given in a phonetic
transcription that's known as ARPA bet
but you can see it gives the word and
then also the phonemes that go along
with that word right so my process for
turning those phonetic transcriptions
into vector embeddings was to basically
hand engineer features that reprimanding
phonetic features of the phonemes
themselves and then in order to capture
the fact that some sounds have acoustic
and articulatory similarity is below the
phoneme level I needed to be able to
tell that for example the sound be like
BA and P both are made with your lips
right that makes them sound more similar
than say BA and ha right the huh sound
isn't made with your lips it's made with
your um back of your mouth what's that
called linguist anybody in there yeah
the throat this where it's made so and
then I used I basically just like
combined each of those features into a
sequence of interleaved by grams and
each one of those diagrams of phonetic
features became a feature in the in the
analysis so you end up with a big really
sparse vector of stuff like this because
most words there were like nine hundred
and fifty different features that were
part of this analysis and then I use
principal components analysis to reduce
that down to fifty so in in these vector
embeddings you basically have an English
word and then a fifty dimensional number
that represents how that word sounds the
sound of that word and once you have
these vectors just as the way that you
can do semantic operations with vector
arithmetic excuse me on the on the
semantic vectors on glove or whatever
you can do phonetic operations with
vector arithmetic with these phonetic
with these phonetic vectors so for
example you can add the two vectors for
two words together so add the vector for
one forward a to word B and then divide
them by two if you find the closest word
in that vector space you're essentially
getting the average
how those words sound the word that is
closest to the average of those two of
the sound of those two words so for
example halfway between paper and
plastic is peptic halfway between kitten
and puppy is committee a half way
between birthday and anniversary is
perversity and half way between
artificial and intelligence is
ostentatious she makes sense so building
on that another thing that you can do is
you can basically use a single word to
tint the sound of another text so if you
take an entire text and then add a the
the phonetic vector for a word to that
text every word in that text then you
end up transforming it into another text
that has like phonetic similarities with
the word that you added so to
demonstrate this these are two shapes I
don't know if there's any psychologists
in the room but these two shapes are
famous in studies of synesthesia because
they're known cross-linguistically to
have consistent sound symbolism so
basically whoever you are wherever
you're from if I show you these two
shapes and then ask which one is Kiki
and which one is booba you're going to
say that those sharp pointy one is Kiki
and the round blobby one is booba that's
like the same across cultures across
different ages whatever so Kiki is this
sharp angular word and booba is this
round bulbous word right so what I
wanted to be able to do or what I did as
an experiment is this is Robert Frost
the road not taken if you did your high
school education in the US and you're
probably familiar with this text on
famous American poem what I want to do
is add Kiki to this text and then see
what the resulting text sound like and
see if I managed to make like a sharper
more angular version of this poem so
here's the original I'm not going to
read it because well actually I have
time I've been going through this
stur than I thought so let's let's have
some Robert Frost just so you have like
so this will feel like a an experiment
we'll have a control group here um two
roads diverged in a yellow wood and
sorry I could not travel both and be one
traveler long I stood and looked down
one as far as I could to where it bent
in the undergrowth then took the other
as just as fair and having perhaps the
better claim because it was grassy and
wanted wear though as for that the
passing there had worn them really about
the same and both that morning equally
lay in leaves no step had trodden black
oh I kept the first for another day yet
knowing how way leads on to way I
doubted if I should ever come back I
shall be telling this with a sigh
somewhere ages and ages hence two roads
diverged in a wood and I
I took the one less traveled by and that
has made all the difference so if you
take this text and you add key key to it
dickheads kooky roads diverged in a
yellow wood key and Sarty I go key pecan
Keeble booth and P one traveler long I
stood and loci down one as far as I go
key tucky Waikiki ich beak in the
undergrowth then ku pecked the other as
cheeky as fee Chara and having perhaps
the best way though as for peak the
peaking geeky hockey warned them Kili
kabuki the safekeeping and booth peak
morning kiki lay and teves no techy
hockey Teegarden black EO like hockey
the thirsty for another gay Faqih Tiki
owned Hal a Teague's on to tuck e way I
Tiki if I should keep her come back e
Ikey she P lucky Keith with he a psyche
squeaky cos I in Ches I hence cooky
roads diverged in a wood key and I I COO
pack the one lucky key will be and peak
has pig all the difference
this is this is Plus booba chubu roads
barboursville and a yellow wood and bari
i koba knob traveled both and bobby one
Basler dow mice Toba and jukebox bowed
one as far as i Koba to bowel it Banton
the Bogart Babu bow cook the bother as
Babu is fair and having perhaps the
Bobbitt claim bogus it's Ababa Baratheon
Wamba bowel though as for baggage the
Babu Babu hob warned them Bodley Abood
the same and both baggage booming
equally lay and Bob's Nome Babette hob
Baden blob oh I Bobcat the first four
bother doy Bob at knowing Baja way Bob's
on to way I bow if I should ever kabob
Bob I shall Bobby Babu Boggis with a say
boo Bobby Bobby's Bobby's fence chubu
roads barboursville in the wood and I I
bow cooked the one Babu traveled bah and
baggage has by Babu all the balance
so this technique for finding phonetic
similarity vectors works for individual
words but it also works for stretches of
text of arbitrary size I have a book
coming out this is my first full length
book of poems which I'm very happy about
from counter path that's called
articulations it's coming out in January
and it's based on a random walk through
the phonetic similarity space of all
lines of poetry from Project Gutenberg
which is a online database of English
text in the public domain
they have text in other languages too
but it's mainly English text so in a
random walk what's happening is an item
is selected at random from the space and
then paired with the next closest item
and that closeness is determined by
their phonetic similarity and then that
item is paired with the next closest
item excluding any line that's already
been in the text and so forth and I just
generated like 100 pages of text using
this technique at each step the line of
poetry corresponding to each vector is
added to the output and that is the
composition so with the remaining time
in the comp in the presentation I'm
gonna read a quick excerpt from this
book so again these are lines of poetry
from Project Gutenberg arranged in a
random walking phonetic order will she
ever ever ever hither come how every
pleasure
every good combined to every pleasure
every pain converts to pleasure every
pain and tawny copper shoots her Asscher
veins when the pan her vision cots
heavens what a sight her vision caught
that I should be her virgin and her
slave over her sides we dash on either
side she boughs her head then she bowed
her head and oft as if her head she
bowed she bowed her head and died to God
in her he said and died had then God
heard her had he sent her hair her hand
her body till she died she leaned her
head upon her hand she laid her hand
upon her brow then she laid her hand
upon her breast her hand upon her breast
upon her breast her hands and her hair
hands upon her breast scarce drawn her
breath her
upon her breasts less red the dart hands
crossed on her breast her head bowed the
her hands clasped on her breasts all
clasped her hands been clasped her hands
and gently clasped her hand she clasped
her hands and bent her head and wept she
raised her white head and clasped her
wrinkled hands thrice with her hand her
naked breast she knocks announced her
rank 12 rings were on her hand her eyes
her lips her hair flung down praising
her eyes her lips her nose her hair no
vision of her hands her lips her eyes
her eyes were heavens own Azure her eyes
a bashful Azure and her hair a vision
that our happier eyes have seen somehow
from that hour I had a new vision looms
high and fronts our vision yet our
heaven or cast our vision high and far
for her our eyes our heaven felt fever
on his Azure arteries hearts ease and
every lovely flower and every leaf and
every flower that every flower and every
leaf that every flower and every tree
and every flower and Verity every flower
and every fruit the redolent breath
flowers of every form in color every
flower is a lover of mine every flower
is a rose of every flower scent when
every flower when every flower on every
Hill on never on never I'll see him
arise I'll see around on every hand
beneath around on every hand look around
on either hand breathe around his hand
in hand my brow and Helens as we hand in
hand by heart as well as head and hand
he rocked as has not where to lay his
head he had not where to lay his head
had he dared had he gone to his help but
he had got Heidi into his head that he
has still his head upon his body he had
bowed his head and sorrow at his birth
that of his good presence he had no idea
not knowing that he had befriended that
he had wounded been that he had been as
he had done deed he had done the deed
that he had done thus by the deeds that
he has done by the ill deeds he had done
we held by the game and hailed the team
he kissed the child and by the hand led
and taken kindly by the hand the dualist
in his class that he should take the
damsel by the hand she took him by the
hand and said come love she took him by
the hand and said come in with me and
took him by the hand and took them by
the hand took them friendly by the hand
took me by the hand by the
and by this the hand takes his son by
the hand he takes her by the hand he
took her by the hand she took her by the
hand to take her by the hand by either
hand one took the other briskly by the
hand he took the children by the hand he
took the maiden by the hand I took the
dead man by the hand
I took the dead man by his hand he took
my hand in his and held it thus her
little hand in his he took in his little
house he learned it all he told his
little tale of woe of his little
children to and loved the love of life
love and life
Oh love and life love unto life life
unto love to life and light light life
and long life come life and light like a
thing of life and light the daylight
comes like a stifled sigh the Sun like
eyes whose light and life and like a
child like a child that's it thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>