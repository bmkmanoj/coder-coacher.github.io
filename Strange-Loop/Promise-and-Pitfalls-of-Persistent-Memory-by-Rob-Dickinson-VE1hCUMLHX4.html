<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Promise and Pitfalls of Persistent Memory&quot; by Rob Dickinson | Coder Coacher - Coaching Coders</title><meta content="&quot;Promise and Pitfalls of Persistent Memory&quot; by Rob Dickinson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Promise and Pitfalls of Persistent Memory&quot; by Rob Dickinson</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VE1hCUMLHX4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right strange loop it's a pleasure
to be here with you today
my name is Rob Dickinson I work at Intel
and I'm here to talk to you about
persistent memory I've been I've been
working on I'm a software person and
I've been working on persistent memory
technology not from the hardware side
but from the software side how do we
take existing software and use this how
do we write new software to use this
stuff and it's been really interesting
we've certainly found some things that
work and we've certainly found some
things that we wouldn't recommend and so
that's kind of the point of this talk
today Oh anybody anybody here looked at
persistent memory before heard about
persistent memory Wow a lot of hands
great
this will beat this would be a good talk
so as I said I'm a software person I'm a
database person and this is a fairly
short talk that we're gonna have here
today so I wanted to throw out my
contact information if you have any
questions after this we may not have
time to get to a lot of questions but
happy to happy to talk to you after this
and as you can see I'm wearing my my
Intel shirt here and so we're not a
sponsor here but if you want to grab me
as I'm walking around the rest of the
show too
I'm I'm easy to find so please stop me
and and ask me any questions after this
as well so life of a software person in
Intel is pretty much life as a software
person anywhere we're out to build cool
stuff the the fun part about it though
is that we we get early access to real
hardware we get early access to samples
and preview hardware and then there's a
big emphasis in Intel on going out in
the community and sharing our results we
don't tend to sell our code we were a
hardware company so we like to go out
and share best practices forums like
this we're also big contributors to open
source both Linux kernel and our own
projects so I'm I'm not one of these
crazy people
the bunny suit this is actually a real
intel employee that for some
inadequately explained reason decided to
do an ultramarathon in his clean room
suit I guess he just can't see this he
didn't get enough time in the clean room
suit at work I guess I don't know
they're they're a different breed we are
gonna talk about hardware today we're
going to talk about CPUs and CPU caches
and memory and storage and a few other
things but we're gonna keep this pretty
high level really looking at this from
the software side what's what's the
impact to us as programmers and people
trying to deliver systems for this
technology so we're not gonna go down
into the deep low-level details about
how this media is built and how its
fabricated and all the magic things they
do in the garage to make that happen the
other thing I want to mention is a lot
of what we're going to talk about today
is not really Intel specific
non-volatile memory storage class memory
or persistent memory whatever moniker
that's that's being sold under a lot of
these technologies are very similar and
in fact most of the most if not all of
the programming and kind of systems
design things that we're going to talk
about today apply equally to any of
these kinds of memories so not just
Intel specific but if you're using envy
dims from HP or Samsung for example a
lot of these things are going to apply
so what is persistent memory thankfully
a lot of people here put their hands up
so we don't have to do too much
background but for those who don't it's
the stuff on the right it's not not the
stuff on on the left Intel's not really
getting into banana based storage but
this is just kind of a funny picture
that we like to use to show this stuff
this really is the future I really feel
like this is the future if you think
about when you when you first learned
programming you probably learned
machine code and your programming model
was I'm just working with registers or
I'm just working with memory addresses
this is kind of getting almost all the
way back to that what if you didn't have
to page data in and out what if you
didn't have all these spinning disks or
SSDs out living on a bus somewhere what
if it was just your CPU your code and
your memory it's kind of any kind of a
neat way to think about it
Intel's specific offering around
persistent memory is what we call 3d
crosspoint technology and that's
illustrated by the picture on the right
as I said I'm not the guy in the bunny
suit so we're not going to get into the
physics of how this stuff is built but
it really is amazing if you are a
materials person or a semiconductor
person this is really a fascinating
topic on which I'm not qualified to
speak as the name suggests the biggest
feature of this memory is that it's
persistent and what we mean by
persistent is that it holds its contents
across a power cycle or when the power
is off and obviously that's a that's a
big difference from dynamic memory which
constantly has to be refreshed even
while the system is running it forgets
its data so quickly this is large
capacity memory it's three terabytes per
socket in the initial generation so six
terabytes for a to socket system and
pricing has not been announced yet for
the dim factored version of this but the
general guideline that we're giving is
that it's it's cheaper than DRAM so
that's the price point that we're trying
to hit this persistent memory revolution
is already here we're already using this
3d crosspoint technology in our obtain
SSDs these are already shipping now and
the the dim form factor which is the the
banana weight not the banana the other
one that we'll be shipping next year
so in our next-generation platforms next
year so the biggest point that I want to
try
to get across today is that even though
we're calling this memory this is the
persistent memory is not memory it's not
DRAM and it's not storage it's not SSD
either it is very tempting when you
first come into thinking about
persistent memory it's very tempting to
want to equate persistent memory exactly
to being DRAM or exactly to being
storage in fact I've read a number of
articles over the last year where people
said something like you know once this
once this persistent memory comes out
yeah it may take a year or so for it to
really ramp up into production for
everybody to be switched over for it but
then that's all that will use that's all
that we'll have inside our computers
will be persistent memory and as
somebody selling persistent memory that
certainly sounds great but it's not very
realistic
the fact of the matter is all of these
technologies have different operating
characteristics they have different cost
characteristics and they have different
wear characteristics and all of these
things go to persistent memory really
emerging as a new tier of memory so it's
it's it's completely logical and I
couldn't help but thinking like this for
a while even though people are telling
me not to it's really comfortable to try
to equate this directly to oh I used to
do this in DRAM now I'll just do it in
persistent memory instead or I used to
do this in SSD and I'll just use my
persistent memory like an SSD that's
certainly comfortable from a thought
model perspective but when you really
start to dig into the details you start
to see that the differences between
these so in terms of capacity persistent
memories is in the terabytes of capacity
obviously there's there's choices here
around what sizes of dims you use and
also how many dims you use what ratio
you use between regular DRAM gems and
persistent memory dims that's up to you
and that's also up to the the platform
OEMs to set the the bounds on that power
safe
we already talked about the access model
of the media is one of the important
most important ways in which these are
different so our SSDs that we know and
love in fact all of our all of our
storage technologies pretty much are all
block based and the blocks are usually
4k on memory on the other side here
memory is byte addressable and we can
flip individual bytes if we want to in
memory but the most efficient way to
access memory isn't is with whole cache
lines so even though it's byte
addressable if you're going to do 4
highest for highest read and write
performance
you're gonna step that up to reading or
writing entire cache lines persistent
memory is kind of in the middle so the
media itself has a very very small
native block size and that's an ECC
block that's a regular server memory
construct that we've just applied to
persistent memory and so you're not
reading in 4k you're not reading and
writing in 4k blocks your the the media
itself is fundamentally reading and
writing in 256 byte blocks
that's an ECC block and how many cache
lines is that that's 4 cache lines so
this memory is addressable in a byte
addressable way you can flip individual
bytes but the most efficient way just
just says with memory the most efficient
way to use it is in terms of a cache
line and then on the media that cache
line is going to be magnified out to a
nice EC block the general latency is
another place where persistent memory
falls in the middle between DRAM and
storage so we're all used to storage
being relatively slow taking tens of
microseconds to do something with a
storage based solution and memory is
extremely fast it's at the other end of
the scale nanoseconds you know 50 60 70
nanoseconds persistent memory is going
to be in the
so it's latency is not as fast as DRAM
and if anybody tells you that it's
exactly the same as DRAM there they're
probably upselling you to something else
I don't know but but in general that the
latency of persistent memory is is not
not as good as DRAM and that's one of
the that's one of the most important
things so what that means is generally
speaking if you take something that's
really really well optimized to run
entirely in DRAM and you run it on
persistent memory it's going to be
slower right now it's not going to be an
order of magnitude slower but it will be
slower it will be slow it will be slower
enough that you will care about that
difference from a design perspective so
again just trying to reinforce the idea
that this doesn't map exactly that we're
exposing an extra tier with this
persistent memory another thing that I
think is really exciting about this
technology which we're actually going to
come back to this point later but I want
to bring it up now so I can reinforce it
is we're leveling and endurance so I'm
sure everybody knows that when it comes
to using SSDs and NAND memory this is
one of the biggest limitations right
NAND wears out and it wears out
relatively quickly
and so you need a lot of sophisticated
wear leveling techniques to be moving
your data around so that you don't you
don't exceed the endurance of the device
3d crosspoint memory and persistent
memory in general does not have the wear
leveling and endurance problems that
that we've come to know and love with
with SSDs this is a a fundamental part
of how the media itself is constructed
and and it is very very different from
NAND in this way or yeah 4 min and then
the last one here is cost and as I said
we're not we're not releasing
cost and final costs and performance
numbers right now we're doing that
closer to product launch but in general
persistent memory is going to be cheaper
than DRAM and more expensive than
storage
so again occupies a third tier that's in
the middle between memory and storage
the main driver behind the rise of
persistent memory is performance as is
probably obvious but this slide kind of
gives an idea of the journey that we've
taken to get here and and kind of what's
at stake in this revolution this
persistent memory revolution that that
we're just getting to the point where
there's a lot of these commercial
products are starting to come out but
this has really been an over 10 year
process to get to this point not just by
Intel but by a number of companies that
that we've been working with and a
number of independent efforts and and
the left-hand side is where we started
so the left-hand side is a really good
NAND SSD so this is not a commodity SSD
like you may have in your laptop it's
not a SATA SSD this is an nvme connected
enterprise SSD and you can see for for
all the investment in this technology
the the media itself is still relatively
slow so the the big blue bar here to do
a 4k read from a a NAND SSD device
that's the amount of time you're gonna
spend in the media itself and this is
one of the reasons why SSD solutions
today are already pretty well optimized
it doesn't really make sense to try to
optimize your driver much more or the
operating system stack much more that's
all down here in the noise most the time
is spent in the media itself the 3d
crosspoint media and the in the
persistent memory media is significantly
faster in terms of how the media itself
works the switching frequency of the
media is much much higher than with NAND
much closer to DRAM so that's what oh
that's what the the middle bar
represents there so the middle bar is
you take in persistent memory and you're
using it in an SSD form factor and now
the amount of time that spent on the
media goes down really dramatically
right the time on the media goes down so
dramatically that now the time in the
operation stat on the inning in the OS
stack itself in the driver in the file
system in the kernel are tremendously
significant so that's those are the new
bottlenecks to remove so by taking this
persistent memory and delivering it
alongside our banana in our dim form
factor we've done a bunch of things
we've removed the bus entirely we don't
have to go out over our and we don't
have to talk to an nvme device that's
connected to the bus that's a relatively
long operation we've removed the file
system because we've bypassed it we're
going to talk about this more but we've
bypassed the file system we've bypassed
even the kernel this is direct load
store access to persistent media from
your application with with as many
software layers in the middle as you
want to have in the middle but the far
right-hand side of this this is this is
the this is the potential for the the
kinds of performance that we can see out
of this now obviously this is a this is
a little bit of an apples to oranges
comparison if you can if you can read
the captions down here but the the two
bars on the left are for 4k reads
because that's what SSD devices do they
work in 4k blocks but as we saw in the
last slide persistent memory doesn't
work in large blocks that works in cache
lines
it's byte addressable so the far
right-hand side the latency bar there is
the read for a 64-bit
Ashlin and it is a little bit of apples
to oranges but that option was never
available to us before I mean you can
you can read a 64-bit cache line from a
4k block you can treat a 4k block like
it's byte addressable and just throw
away the parts that you aren't
addressing we can do that it just makes
us terrible programmers no nobody would
do that in in their right mind but with
persistent memory actually treating
things actually touching your data
structures directly the way that you
would touch them in DRAM without
assuming that your paging them out
through the kernel through the page
cache through your file system this we
think is the the programming model that
holds the most promise you can see at
the far right the little orange bar
there is the read of a full 4k block
just to make a full apples to apples
comparison you can see it's still
smaller than the fastest Intel obtained
SSDs that we have now alright so in
talking about programming for this stuff
there's a little bit of a little bit of
background I want to set here the first
is as I already mentioned this has been
a huge process that's been going on for
many many years one of the first things
that happened was this was the the
formation of a new programming standard
for this so this is administered through
the through Snee ax and pretty much
everybody that you would have wanted to
collaborate on persistent memory has
been collaborating on this I think you
know Intel is in a pretty unique
position to just just the the companies
that we work with and the customers that
we work with we have a lot of
opportunities to bring people together
and that's really what we've seen as our
role in in doing this is to help bring
this community together because these
these changes that we're talking about
are so fundamental they could never
really be done in a strictly proprietary
way we're now
just talking about changes to the
hardware we're not just talking about
changes to BIOS and firmware we're
talking about changes to the operating
systems we're talking about new drivers
we're talking about new software stacks
we're talking about new ways of thinking
about our applications these are not
things that you can just do quickly and
easily so standardization is an
important part of this I have the link
here to the to this Nia programming
model please do check it out and what
you're going to find there is it's kind
of refreshingly boring in a way because
we've always had this capability to
memory map files and move data around
and memory map files have been around
for 25 30 years now fairly old technique
and for people that aren't familiar with
this technique you may not have used it
but basically what we're doing with a
memory map file is we're telling our
operating system treat this file as a
region of memory so I'm gonna take I'm
going to take a file that lives in
storage down here and I'm gonna tell the
kernel do a mapping and bring that file
into memory and let's treat it as though
it was memory resident and that's what
the the cache in the middle represents
so the kernel is going to bring that
file bring that bring that data into
into kernel cached memory and then that
memory will then be exposed up into the
applications virtual address space the
application thinks this is just memory
that I can manipulate I'm going to flip
this byte I'm going to flip that byte
I'm going to flip that byte right the
operating system is going to turn around
it's going to take those changes and
then it's going to push those changes
back out to the physical device in the
4k blocks that the physical device knows
how to work with right so this technique
has been around forever
every operating system supports this the
reason why you may not have used this
technique a lot in the past is because
it's very inefficient right because
we're treating this four K block device
as though it were a byte addressable
device and that means that you've got
this magnification now that when you're
flipping bits or flipping bytes in your
data structures that's getting written
back out as four K blocks so not very
efficient but a very good technique so
when when the Sneed group first started
to think about what's what's the what's
the general way to expose persistent
memory in a way that operating systems
will find it fairly easy to deal with
this seemed to be the best choice for
doing that now there's a lot of things
in this nia programming model because
it's designed to cover a bunch of
different cases and a bunch of different
scenarios and be very flexible but but I
like this diagram as as being one
picture that that brings it all together
what we have on the left is our
traditional application stack which you
all know and love already
you have standard file API is like the
POSIX API that's going to talk to a file
system file systems going to talk to a
driver drivers going to talk to storage
and that's all a block oriented protocol
and we're going to come back to this
idea but we kind of take our file
systems and our drivers a little bit for
granted nowadays most of what in fact
most of what a driver actually does is
things that we don't even think about it
all which is handling device specific
errors that come up which are incredibly
specific and hard to test and an error
prone to make a bad pun but anyway but
so we know you know we know that what's
on the left-hand side here is very
bust and and we've got a whole lot of
years invested in making that good and
for persistent memory to try to beat
that is a very tough bar to set we've
got thirty years of very very smart
people optimizing the stack on the left
and now we're saying hey let's disrupt
it and the only way that works is if you
can deliver something that's
significantly better but we think that
we think that this is it so as I said
when I first started you'll you'll see
the the bottom there says NV dim it
doesn't say persistent memory and it
doesn't say Intel this is the schema
programming model you can take this
programming model today you can take an
NV dim device from HPE for example these
are already on the market you can you
can order one of these today you can
throw it in your server and you can
start programming with persistent memory
so NV dim is the the generic term for
for persistent memory stepping up to the
application you can see that the first
thing you're going to see from your
application is if your application uses
a standard file API then you can
continue to use a standard file API what
we're going to rely on is we're going to
lie on a persistent memory aware file
system to help with some of the
optimizations there the persistent the
the the specific optimization that we're
really looking for is to bypass the
kernels page cache and there is a
specific standard beyond sneer for doing
this it's called Dax
Dax it stands for direct access and this
is a very exciting standard it
originally came out of Linux it was
something that that Intel helped broker
with the different Linux vendors and and
then we brought this to Microsoft and
originally Microsoft in kind of old
Microsoft fashion said oh yeah that
looks really good will
look at it and we'll get back to you
with what our version of it would look
like and then amazingly after that
Microsoft came back and said you know
what we're gonna use the same API we're
gonna use the same standard it already
exists in fact we're gonna call it Dax
and it's it's amazing it's amazing so we
actually have one standard for direct
access that works across both Windows
and Linux and this is this is I think
one of the the biggest one of the
biggest standardization benefits that
that that really we've been able to
broker in a long time so the persistent
memory where file system this Dax file
system that's the one that can do this
magic it's smart enough to know that I
can skip the page cache and I can go
direct it's really as simple as that so
whenever you hear if you hear Dax think
skipping the page cache and if you hear
sneer just think memory map file and
you're you're pretty much often running
on the so I mentioned Dax is already
available on Windows it's also already
been integrated into X FS and ext and
the the major Linux file systems as well
so that's our so that's our file system
so if we've got an application and it
uses a standard file API you can take
that you can run it on persistent memory
you don't have to change your
application another thing I didn't
mention earlier but I'll mention it
quickly now is this persistent memory
can be divided up it can be partitioned
so we're typically not saying I've got a
dim and I'm gonna give the whole dim to
my application in a device way now you
can do that certainly there's lots of
databases that want to work with raw
devices you can do that but what's what
you're frequently going to do is
persistent memories you're gonna carve
it up you're gonna carve it up into
different regions
or different partitions so on my MD dim
here my applications actually using both
sides of this so my applications using a
standard file API kind of in the center
line here and that's going to a region
of persistent memory that is being
exposed as a block device so that's a a
legal thing to do in fact an encourage
thing to do if that's the if that's the
the value proposition that you're
looking for and that's that's absolutely
intentional right we want there to be
easy ways to get started without having
to make significant tree architectures
to your software to get to get benefits
out of that so the standard standard
file API and Dax allowing us to bypass
the page cache has has a lot of
potential to improve applications
without making any changes at all the
far right hand side of this diagram is
an application that has been evolved to
take advantage of this load store access
and notice it could be the same
application right this is an idea we'll
come back to but within a save
application I could say yeah I've got
some standard file stuff and I'm just
gonna use that and then I'll have
another part of my application that's
where I have an optimization opportunity
that's where I'll try out load store
access for the first time so we're still
relying on the file system because it is
memory map files right so we're still
relying on the file system to do things
like security checks authentication
checks permissioning things like that
but once that file is mapped once we
have those MMU mappings in place that
memory that that's our persistent memory
becomes part of the applications virtual
address space the application now has
direct load store access to that media
so once once this memory mapping is done
you have direct load store access that
means you're not going through the file
system anymore you're not going through
the kernel you have direct access to
that media and this is what gives you
the fastest possible performance
one of the things that we have to do is
we have to flush so when we talk about
when we talk about persistent memory
let's not forget that there's there's a
whole lot of CPU caches between us and
our persistent media and so this is one
of the big challenges is that you have
to actually flush out to durable media
so we've got our l1 l2 l3 but we have to
flush all the way down to the right
pending controller and then down into
the dim so what does that mean that
means that the hardware is not providing
transactional consistency right which is
just like all storage systems right if
you're manipulating a data structure you
need to have transactions to protect
against that so just a few quick notes
on adapting persistent memory or
adapting software persistent memory the
most important thing to get out of this
talk today is that persistent memory is
not exactly like DRAM it's not exactly
like SSD it is a third-tier think of it
as a third-tier think of it as combining
this with the memory and storage you
already have think about using this
inside your existing architectures in a
hybrid way and you will be in a much
better place than if you're thinking
about I'll just take everything I'm
doing in DRAM or everything I'm doing an
SSD and put it into this new medium this
memory is going to be used in different
ways but different applications we
already showed this some applications
may use it in a file oriented way some
may use it in a load store oriented way
some applications may do both at the
same time there's also remote DMA
capabilities persistent memory are at
least in the Intel implementation is an
RDM a target so being able to remotely
replicate directly to persistent memory
is also a very interesting application
pattern the catch is that this exposes
applications to some classic problems
that before we could always count on the
kernel or the file system to do on our
behalf in fact we're so accustomed that
we don't even really think about those
problems because the kernel in the file
system they
Carolyn Voris so some interesting cases
that we've tried so we've been working
really hard to look at different
patterns look at different strategies do
benchmarking and and see what what are
the what are the different kinds of
patterns that we would recommend and
these are just a few but you'll start to
get you'll start to get a sense of the
possibilities here while acceleration is
one of the most obvious ones if you have
a database or a queueing system that has
a right ahead log taking that right
ahead log and putting that log on
persistent memory persistent memory is
very very good at flushing
irregularly-shaped blocks of data right
you're not going to have to write a
whole four K block you're not gonna have
to marshal it into sequential append
only writes going out take your take
your wall and let it do its thing now
remember on the earlier slide we talked
about we're leveling and endurance right
so this is where that's really important
when SSDs first came out one of the
things that people wanted to do with
their SSDs was was say hey I've got this
SSD and it's really fast it's really
great so the best place I could put that
in is I could use it for my transaction
log because if I speed up my transaction
log I speed up the performance of the
whole system right and what was the main
thing that held people back in doing
that it was concerns over the endurance
of the devices and that's a real concern
because you're saying I'm just gonna
write the hell out of the device with my
wall all the time so that endurance
factor when it comes to wall
acceleration is really really important
and I think this is this is a very
important lesson to take away from from
the rise of SSDs and and thankfully
that's not history is not repeating
itself again so though to the the high
the high endurance that persistent
memory has is really important when
you're talking about a right
head log kind of case persistent cashes
is another really really great pattern
for this everybody's used to caches
right we don't have to talk about what
that is but one of the one of the
biggest problems with caches is
refilling them that can be a very long
process and you can have a lot of
latency outliers when your cache is
being filled that you wouldn't see in a
normal steady-state situation but with
persistent memory you keep your cache in
persistent memory now that's going to be
a little bit slower than it would be if
it was in DRAM right but you never have
to reload it so you get a very
interesting trade-off there for an
in-memory database it's kind of taking
the cache idea to the next level when
you talk about an in-memory database
like Redis for example what are the
things that hold us back in terms of
using it the things that hold us back
are we can't get enough DRAM in our
system because we want bigger data sets
than then we have DRAM in our system
persistent memory helps with that
because the the capacity of the memory
is so much larger it's also being done
at lower cost so you can actually afford
to get that to get that data into memory
it doesn't have any of the reloading
that we talked about with persistent
caching so something like Redis the
first thing it has to do is stream all
its data back from disk when it starts
up if you're talking about a Redis
server that has terabytes of memory that
can take hours to reload that from SSDs
with persistent memory you can bring up
Redis you can do a little consistency
check if you want to you're off and
running very very powerful the last two
hybrid and converged hybrid means that
we're going to create data structures
that live in multiple tiers so maybe
part of that will live in DRAM part of
it will live in persistent memory then
as that data cools it'll be moved off to
SSD or even spinning rust again opening
up opening up additional tiers converge
compute and storage is bringing more
data
sir to the CPU so that we can do more
with it because you have that direct
random access to your data but as we
said there are some classic problems
that come back and the reason for that
is that we're bypassing the kernel
whereby pet we're bypassing the file
system and so the trade-off then is that
there's more that the application is
going to have to do to take care of that
so just some quick examples position
independence every time you run your
program it's going to be in a different
area of memory right the persistent
memory doesn't change so how do you deal
with pointers that may point to a
volatile structure or may point to a
persistent structure or even a volatile
structure that has pointers to a
persistent structure that's a very tough
problem to solve
memory allocation and garbage collection
you can't just take your regular regular
memory allocator and use it it's not
appropriate and by the way if you get a
leak in persistent memory how long does
it stay leaked for all time it's not
like you get a leak and then you just
get to restart the system and then
everything's great again inerrant
pointer that goes wrong and does a bunch
of stuff in persistent memory it's it's
bad transactions and locking these are
other other options there's the
hardware's not going to protect against
torn updates you you have control over
flushes you have control over when
things get durable that means you need
some kind of transaction protection if
you grab a lock and your system dies
that lock doesn't automatically get
reset because again it's persistent
right
these are inverted inverted behaviors
from what we're what we're used to
dealing with error detection handling is
maybe the biggest one right
most of what drivers do and a lot of
what file systems do is hiding errors
and we we as programmers are just not
even really used to dealing with these
kinds of errors so the kinds of errors
we're talking about here is not oh I got
a bug or oh I did something wrong
oh I've gotten off by one the error that
we're talking about here is that the
media is actually damaged there's
actually poison on the media there's
corruption on the media you're now
loading that corruption directly into
your virtual address space there's no
there's no poison help for this like you
would get out of a normal driver and a
normal file system that's that's
something that that the applications
going to have to deal with so our call
to action today most important thing
consider persistent memory as its own
tier if you do that that's going to get
you on the road to thinking about this
in the right way
look for those breakout opportunities in
your architecture to hybridize don't
think oh I'm going to throw out the
whole thing and start over look for that
look for the places where you can apply
either existing file system API or
you've got something that you can use
load store access in a limited way and
get your feet wet and go forward Intel
is sponsoring a number of libraries that
are laid to this Snee assess you can you
can memory map this stuff and your
terabytes of memory start here have fun
right that's what's Nia's going to tell
you Intel's not the only only group
that's making libraries for this but we
think we have first mover advantage our
libraries called env ml it's on github
it's all friendly licensed you can steal
our code you can use you can use it
however you want
even in your own projects and the the
link here is on github we do all of our
development on github this is all open
source and it's all developed in the
open we don't like get to a stable point
and then and then release stuff we're
actually making all of our mistakes in
the public eye and the last one you can
start with this today you can start with
emulation you can use D Ram as emulated
persistent memory you can try out these
techniques try out these ideas but
please as you make your final design and
tuning decisions validate and tune on
the real platform again these it's not
just like DRAM it's not just like SSD so
you can do a lot of prototyping
pretending that you're
is persistent memory but in the end
you're going to want to validate those
assumptions on real hardware thank you
very much appreciate it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>