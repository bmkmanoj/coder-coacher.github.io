<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;The Holy Grail of Systems Analysis: from What to Where to Why&quot; by Daniel Spoonhower | Coder Coacher - Coaching Coders</title><meta content="&quot;The Holy Grail of Systems Analysis: from What to Where to Why&quot; by Daniel Spoonhower - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;The Holy Grail of Systems Analysis: from What to Where to Why&quot; by Daniel Spoonhower</b></h2><h5 class="post__date">2017-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3u-td4vZYHU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks everybody for coming my name is
spoons my parents still call me dan but
nobody of that I hang out with her work
with still does really I'm an engineer
in a past life I was a researcher I love
to geek out about programming languages
and garbage collection but today I'm
gonna talk about running in monitoring
production software systems which is
also something I'm excited about where I
work now at light step we've built a
tool for doing distributed monitoring so
I spend a lot of time thinking about
this the tool itself is also a
distributed system so I also care about
it from that point of view and I also
participate in a thing called open
tracing that I'll mention a couple times
during the talk so what I'm gonna talk
about I think is sort of the essence of
monitoring or I guess if I follow
charity lead from this morning
observability but I'm sure I can keep
the two of them straight really I'm just
really time to go you know understand
for our software systems what's going
wrong where's the problem and why is
that happening
and I think anyone who is responsible
for production systems cares about these
questions and they want to find answers
to them quickly and with confidence and
with as little investment as possible
and I think is a real opportunity for us
to to better do better in terms of the
tooling we have but it requires us to
think a little bit more broadly about
first kind of the existing tools and the
way we've kind of maybe segmented them
and then secondly about the way that we
collect and analyze the data that we
have and so what I would kind of hope
that you all take with you today is just
maybe some questions about how we're
doing that and I'm going to show some
examples of how I think we could do
better in the future
I think we're here because we like
building systems and we like helping
others to build systems that's why I'm
here that's hard building systems is
hard if it was easy we probably wouldn't
be here and I think one of the big
challenges and running distributed
systems is remediating problems when
something's gone wrong
downtime broken deployments those can
mean a loss of revenue loss of users
they can damage your brand so the faster
you can fix a problem the fast food your
users can get back to whatever they were
doing and you can get back to sleep if
you want to learn
talk yesterday he said a lot about
different kinds of things they can go
wrong and how you can do science to
understand those vulnerabilities and
address them before they become outages
but of course some things will still go
wrong in production so I think the way I
think about it the purpose of monitoring
is to tell stories I don't think
anyone's gonna argue with that
probably some argument about the way
that monitoring tools do that maybe
that's why some people came this weekend
to argue about that but I think if a
monitoring tool doesn't tell a clear
story it's not really working and
ideally that story takes you to why the
problem is happening and it takes you to
a solution to that problem but there's a
number of factors that have led to our
tools becoming less effective recently
some of these might sound a little bit
familiar but I think for monitoring it
means something a little bit different
maybe micro-services are here I don't
think anyone's arguing with that anymore
it's great I'm excited about them
there's a lot of cool things about micro
services but they broke our tools in
that each transaction is single still a
single story but now not just one
storyteller about a whole set of them I
think this is kind of a big deal bigger
than the move to VMs or the move to the
cloud I think of a lot more like the
transition between desktop software
where we took something out of a shrink
wrap and install the DVD or CD or
floppies or something and transitioning
from that to a client-server
architecture the tools that we had that
worked for desktop software just didn't
work for client-server I think the shift
to micro services is sort of similar
categorical shift and I think there's a
similar kind of categorical breakage in
the tools that we're using in the
workflows that we have so back in the
day applications look like this they
were extremely symmetric you know they
were square they were one square and
maybe that box has broken into a bunch
of different libraries or packages but
it all fit together in a single process
and okay maybe it wasn't one process
maybe it was like three but I guess the
idea is like everyone in your org
but the idea with micro services that's
not true anymore right like I don't even
know all the names of the services
anymore there's too many of them to keep
track of they're changing all the time
um that's great
now that those things can be deployed
independently and scaled independently
not so great that they're being
monitored independently so it used to be
that tracing kind of old school used to
being just like following a pathway of
execution through a single process and
that was pretty easy we just walked up
the stack that's why we have print stack
trace but when you're doing that in a
distributed system it means that you
really need to follow that path across
process boundaries and that's that's not
as easy but really doing that is kind of
table stakes for monitoring and
distributed systems if you're not doing
that it's gonna be really hard to
understand what's happening I think if
you see folks moving towards
microservices even just starting down
that path like I'm not talking Netflix
style things just like ten services
maybe and you don't have some kind of
tracing solution it's gonna be pretty
painful and again not even like
complicated things like you know P 99.9
latency or anything like that just like
something's on fire what's going on I
think tracing is really important for
that kind of thing there's another
problem that's kind of come up maybe not
quite so recently but you know there's a
lot of things happening and I would like
to imagine in my simple single-core
brain that you know requests are atomic
and C realized but they're not of course
there's a lot of for performance and
other good reasons a lot of things
happening at the same time that's not
quite true either there's a lot of
asynchronous things happening you know
requests get interrupted they perform
i/o maybe using some sort of framework
like node or you're using go routines a
lot of stuff happening process blow off
a request will start it'll pause for
some reason other things will run for a
while it will come back and so on in a
distributed system you've got all of
that and it's worse you've got all that
in every process and now you've got
single transactions spanning multiple
processes so and even that picture
itself is kind of a oversimplification
because everything if you look kind of
closely is still a linear thing
generally and the whole part of the
reason you're doing this is so that you
can fork off parts of requests be the
parent process of processing in parallel
and then join the
it's back together as you go back up the
stack I sort of ran out of room on the
slide to do that though anyway
the goal is like connect that light
stuff back in and isolate one of those
requests you can actually understand
what's happening and try to address the
problem so yeah and you know we can do
that but I think I'll have a chance to
say a little bit more later about maybe
why that isn't quite enough so anyway
that's sort of where we are I think
those problems are not going away so now
the question is sort of like how are we
going to deal with them um and that's
where tools come in and I think like I
said tools are going to kind of try to
address these three questions and that's
how I'm gonna try to restructure the
rest of the talk okay so what's going on
answering these questions actually not
so bad today we've got some pretty good
tools for doing this so we can use
metrics tools for measuring symptoms and
symptoms are a pretty good way of
modeling what's going on so you know you
can make a metric that associates with
different end user experience that could
be latency it could be errors a few
other things and then you can set as
leis or goals around those metrics and
you know set an alert or notification
when you miss miss your goal and you
know I should say I brought end user
here and user might be an actual human
being if you're a platform your end user
might be either an app or another
application that's built on top of you
but the point is that you can model
pretty easily what your immediate
clients are seeing using metrics so how
do we explain anomalies with metrics so
you know we might have something looks
like this I just grabbed some random
thing from one of our internal
dashboards so I don't know if this was
error rate or something like that but
you know something's up about 12:30
better figure out what's going on
nothing was pushed out then so let's dig
a little bit deeper
luckily we use tags as most metric
system support to you know a tag based
upon things like customer service
what VM is running on things like that
and then we can do a group buy of the
data to actually break that apart and
drill down and see what's happening so
that's cool so now you know you can see
I think that
you know most of that bump actually can
be attributed to this this one component
you know we can look at the tag that
that thing represents and understand
what's up with that customer or service
or VM or whatever so that's great we've
gotten somewhere but this kind of a big
problem charity attention this this
morning we say cardinality sometimes I
feel like this is a pretty big word to
talk about a pretty simple thing but
just the idea is like some Tech's have a
lot of values a lot of different values
and when that happens it becomes really
expensive to collect that data really
expensive to manage it and then you know
when I say a lot I mean you know more
than 100 and it's you know not in common
if you have a customer idea to have
obviously hundreds or hundreds of
thousands or millions of values for
these tags so yeah that can be a problem
and means that like maybe this is not
quite so useful but that's sort of one
problem the second one I want to say is
this is not a diagnosis like we have a
smoking gun here but so we can sort of
narrow a little bit we don't know you
know why that customers misbehaving what
that service is doing or what's wrong
with that VM right so it's some level
it's neither scalable nor actually
delivering what we really want here
another problem with metrics is you
start to get dashboards that look a
little bit like this and that can be a
little bit overwhelming having lots of
metrics can seem great but I think it's
important not to confuse having lots of
answers to the what question not to
confuse that with quite answer to the
question of why something has gone wrong
we used to have the giant dashboards
like this and teams I worked on at
Google they were really cool and they
look pretty impressive but the thing is
unless you were an expert in the system
that was being described like the
dashboard was not that helpful right so
no I just you know so I should admit I
still use dashboards not everybody's
perfect this is some random metrics that
I grabbed from our service and you know
we had an incident here it wasn't
terrible but it wasn't great anyone can
help me out with the root cause here
like upper left lower right anything
like that yeah so you can see
correlation pretty easy in something
like this but it's really hard to
determine causation and that's really
what you want to do in distributed
systems you have lots of
interconnections between these things
it's quite common for a lot
metrics to blow up at the same time um I
don't really remember what this is about
either like this dashboard has changed a
lot these services changed a lot and
that's kind of Lisa the next problem
which is that you know the number of
metrics if you talk about your business
in terms of what matters here customers
like that should stay relatively small
you know of course the feature changes
you might care about a few more but as
as you develop more and more micro
services within your system you'll sort
of be tempted to create more and more
metrics and that's a problem for a bunch
of reasons one that's just a lot of
maintenance like that dashboard is
outdated it's no good
it can get costly from a storage point
of view even before we get to talking
about tags there's just if the data that
you have is scaling both with the number
of customers and the number of micro
services or the number of transactions
like that's not that's not great for
your business and and there's only gonna
be helpful if you actually define
metrics for all of the root causes which
you didn't write there's some Rica's
that you haven't seen yet and so it's
hard to anticipate what those are gonna
be and you're not going to be able to
use metrics to find those so in the end
metrics are not really the answer to
root cause analysis they're useful but
but they're not the end of it the end of
the story okay
so chapter 2 where is the problem in my
system and in this and actually most of
this talk I'm gonna talk about a lot
about latency but this could apply to
errors or other kinds of things that you
might measure as well so okay
figuring out where our problem is this
is sort of classic distributed tracing
how many people here do distributed
chasing like doing this review tracing
I've heard of this remediate racing okay
cool awesome um I guess that's why
you're here maybe so at Google we had
this tool called dapper and it was it
was pretty good for this kind of thing
so you get paged you'd have some metric
that said and here's your light and see
what's up you log in and you get dapper
up it tell you it'd tell you where the
latency came from and and you do a kind
of critical path analysis and then you
kind of go from there so I want to do a
little bit of illustration so I have a
kind of side project with one of my
co-founders we call donut zone
which is I'd donuts as a service
solution we're doing some blitzscaling
right now kind of experiencing some
hyper growth but we're micro service
oriented so we're pretty pretty ready to
deal with that growth but we are having
some latency problems and and we used
open tracing so it turns out that we can
you know integrate with a bunch of
different tracing vendors pretty easily
so let me just give you a picture of
what our architecture looks like through
three different clients there's a client
used to order Donuts we have a couple
administrative clients that we use for
either a restocking the ingredients or
cleaning out some of the equipment those
all talk to an API service API service
then fans out to some other backends so
we use brain pal for payments and then
we have a topper service in a friar
service they're also doing a lot of the
heavy lifting because we're we all
computer scientists and you know we want
to do it right you know we use mutexes
around the resources that are here so
you know we don't want to be cleaning
the fire we're frying doughnuts you
don't want to be restocking the shelves
while you're trying to grab the toppings
like it's gonna make a mess so we've got
a couple of neat X's in the system and
wherever you have you taxes you're gonna
have queues and so I mean this is all
just a toy but you know what I want you
to think about is like there's queues
everywhere you know whether that's me
Tech's a database table and network link
a processor these are all sort of
resources where you can have some
contention and I mean this is tiny at
some level but I hope it's kind of real
enough that you can squint and
suspend your sense of disbelief enough
to kind of believe it so okay
a little hungry I'm gonna order some
doughnuts here so this is kind of part
one of the demo um you guys are gonna
have a chance to participate in a bit
but let me kind of mess around with
Donna zone first here okay so it's on
its own here we go see what we got going
on for donuts we don't have an internet
connection though this is gonna be a
little bit of a problem for my demo
sorry I should've checked this give me
one second
okay so I'm gonna order some donuts here
all right super all right let's do that
so chocolate done it's here grab a
couple chocolate doughnuts there's a
little thing on here it kind of shows
you how it's doing and then I'm going to
flip over here to my tracing system and
check out how chocolate doughnuts are
doing here so let's see so I got some
examples yeah these ones took a couple
of seconds that's pretty long for a
doughnut oh yeah of course thank you
how's that better okay so this is a
trace describing what's going on here so
you can kind of read this a little bit
like a Gantt chart so time's going from
left to right and then kind of down the
stack as we go so you can see each of
these different bars here of course once
the different operations the top one
it's a the browser's kind of view of the
world then we hit some back ends in the
dapper terminology these are called
spans but a span is really just a timed
event at some level um anyway the idea
is there we can kind of look and see
okay there's some time kind of spent on
the client but then like this pink part
here this is really this is really the
problem at least in this case take a
look at some of these other ones and see
what's going on yeah just the fire
really
we signed a spec up a second in the fire
here so you know right away we've gone
from you know user visible experience is
not great - we know what component the
system we need to be looking at and
that's great so we've made some progress
and you know this is like a tiny little
example like in the real world these
things can have hundreds or thousands of
spans and stead of being able to
automate finding of the critical path is
really helpful and really like gets you
going in terms of finding a solution
okay so yeah so this was great but sort
of a gap here which is like this is
where deaf were kind of stopped and
where most other facing systems stop it
great to understand why that thing was
slow and it would be great to do that in
the kind of general and automated way
and this is sort of the the last part
but I want to do kind of a brief aside
on on tracing and why it's hard
so really tracing is just logging at
some level we kind of call it tracing
it's special in some way but like in
monitoring generally there's sort of two
things there's statistics and there's
events and events maybe includes tracing
and logging but I think when people say
tracing or at least distributed tracing
really they mean there's some kind of
maybe sampling strategy that's it's a
little bit smart maybe and then that
they can see some kind of causality or
structure on what's happening not just
timestamps so that problem with logs you
know I had this diagram here for for one
transaction oh sorry you guys can see
this here we go sorry yeah so chasing is
excuse me one transaction put a lot of
logs for that so I guess you don't want
to put your developers in a position
where they don't want to log like
logging is a valuable thing but if
you're logging a lot you know for every
micro service on every transaction that
can get pretty expensive or even just
kind of infeasible from a from a sort of
the throughput front of you so really
this is the idea of the problem that
Deborah was solving at Google is they
wanted to do logging from for web search
requests but even that the public
numbers for Google or something like two
billion requests a second so there's
just no way to log all that data so the
solution was basically to throw out
99.9% of it it turns out that like point
oh one percent of two billion is still
pretty big you can still find whatever
signals you're looking for in there but
you have to do that in a kind of smart
way and that you want to make sure you
capture the whole transaction so that
it's still useful but if you take no
returns actions like I said and just do
it naively it can be pretty expensive so
um let's talk for a minute about
sampling there's really kind of three
points that you can do sampling one is
you can do it in the process to limit
the impact on CPU
you can also do it in process too like
for the disable network costs and then
once you've sent over the network maybe
you buffer it somewhere and do some more
sampling tip to limit the cost to turtle
storage in the end the one that really
matters is just the last one like this
is really the only reason that that we
did any sampling it at Google only for
the most high throughput services did we
ever see any impact on the CPU and
really that's really about throughput
not latency like there's a way generally
to move all of this kind of
instrumentation out of the critical path
itself and not have really any impact on
latency and the throughput was really
only a small percent and that was fine
for everyone except for like the biggest
biggest parts of web search
so sampling in the process is really not
necessary I think that's really
important to understand because if
you're not simply in the process you can
make much better decisions about what
you choose to keep or not keep of course
you don't want to write it all to disk
that's crazy and there's some tricks to
making this work but there's a big gap
between tracing systems that do a lot of
this what I think of as kind of more not
use sampling and what what where they
really should be I think so ok this
gives us back to the last part of the
story understanding why so I had this
diagram before I was trying to isolate
distributed transactions it's not really
true like they're not really independent
so you know yeah and that I'm the rest
of the demo I think the reason I need
your help is to show that so I have this
hypothesis that most latency issues are
actually due to contention for shared
resources and that means when I'm
looking at a trace I see that there's a
slow span that request is actually not
the problem the problem is some other
requests that's happening at the same
time and I tried to frame this in a kind
of stark way like maybe most but I think
it's probably most and I think the
reason I wanted to do that is there's
kind of a blind spot in our tools and I
think that also bias is the way we kind
of think about the problem like tools
are pretty bad at understanding what's
happening in other transaction at the
same time I think that we kind of
internalized that in a way that makes us
think that it's maybe not that important
yeah so I had this thing before about
independent tracing but really the red
pill reality that we really live within
is that these things are all
interdependent so okay so you know I
have this don't as a service thing that
I'm running I need to understand what's
going on so here's what I did I assigned
what I call resource identifiers to all
of the important resources this is
really just I took a small wrapper
around the new text librarian go it's
like 50 lines in the actual application
changes I'll show you in a second but
it's just really like one or two lines
we then tagged all the spans with that
resource ID whenever they try to grab
that resource or use it and then three
is sort of the magic part where we
figure out how there's some interference
between these things and capture that
and then you know if we've done that
then we can answer the why question
pretty directly so like I said the codes
it's really not that interesting so this
is like you know drastic
oversimplification of how you might
write some like thread safe code and go
of course we want to defer that unlock
or something like that maybe do
something between unlock and lock and
unlock but really the only change I had
to make here was one to wrap the lock
itself
I was explicit about the tracer itself
that actually can be done with a
singleton but I just wanted to be clear
and then you know when I lock actually
to pass the context and so we can figure
out how to associate this with a request
so it's pretty easy smart like myself
like I said it's like 50 lines it's not
that interesting it's just about
generating a random number really and
that's it right so this is how it's
gonna kind of go so I'll give you a
little preview of what it's gonna look
like and what's gonna happen behind the
scenes so this is one of these traces
right and we've got this long file span
that's the problem right this long one
that's kind of in the middle here and we
have a resource ID that is going to be
associated with that this sort of random
number and then what we're gonna do when
we notice this this happens we're gonna
go and find all of the other spans that
are contending for that same resource
and then we're gonna go up the stack on
all of those things and aggregate the
data from those so that'll give us a big
picture about everything that's
happening the time
not just like what's happening at a low
level but from a very high level like a
user point of view about what's going on
um and then we'll use it to figure out
what's going on and I think the real
beauty of this is the tracing system
doesn't really care about the kind of
resource or how it works like really the
assumptions that tracing system is
making is there so you can a unique ID
for this thing and it can find other
spans with that unique ID and that's
that's really it so you should be able
to apply the same thing to you know a
database network connections machine
resources whatever it is it really
doesn't matter so okay let's get back to
doughnuts this is where I need your help
so this is the weird part where speaker
asks you to get out your phone or laptop
and I want everyone to go to the
doughnut dot zone and then I'm going to
divide everyone into two groups here so
everyone on the right hand side we're
gonna call you Group A sorry middle
thing has to be like awkwardly divided
but I'd like you all to kind of order
just like miscellaneous doughnuts I'll
show you what that looks like in a
second I want everyone on this side
that's your left to order lots of
sprinkles on it's like go crazy like you
haven't eaten
you didn't eat lunch today you really
need some wrinkles doughnuts okay and so
let's let's just see what this all looks
like for a second here how are we doing
in terms of actually before I let me
just restock a little bit here because
we're a little bit low on cinnamon cool
okay so
okay is everybody ready it may be are
you doing okay
well it's over here not doing it do it
right now just you guys miscellaneous
everyone over here lots of sprinkles
okay ready three two one go and see how
we're doing here
all right that's probably good thanks
okay can take a break not too many times
it's not good for you okay let's see
what's going on here okay yeah so that's
really lots of slow things happening
let's take a look at what's going on
here let's see yeah okay cool eleven
seconds that's not good it's not a good
user experience so let's see here so
this was actually a cinnamon done it in
the end not even the sprinkles won but
it's still really slow and it turns out
like when the example I did before that
the fryer is really the contention here
what we can do is then we don't like
drill down on that and then take a look
at what's going on over here try and
zoom and this is a really janky like
graph layout library that we kind of
hacked into this thing oh sorry but okay
so here's the things like this is
basically showing kind of aggregate call
graph going back up so fry donuts at the
bottom here that's the thing that's
what's grabbing the resource that always
gets called from make donut make done it
gets called from a bunch of places you
know based upon whether it's cinnamon or
chocolate or whatever but what we're
seeing here is that even though this is
a cinnamon doughnut that it's contending
for a lock that 84 or sorry 80% 70% of
the load on that lock actually comes
from sprinkles right that's that was
this half of the room and the thing
that's kind of cool about this is like
we didn't mess anything about tags or
cardinality or anything we have to make
any decisions I need this kind of stuff
ahead of time right like like we've
identified some you know important
information for these things but the
fact that you know there could have been
a hundred different 200 a thousand
different values for this thing it
wouldn't really matter we're just doing
this stuff sort of in real-time and
aggregating to sort of based on what was
important here
ready okay so let's go on to number two
let me remember to flip this back okay
so group a I want you all to keep doing
the same thing just casually keep
boarding up I mean not right now you can
donate too many notes I could for you
when I say go just keep doing the thing
you were doing but Group E I want you to
go down to the bottom there's this
little restock button here and I want
you to click on restock and then I want
you to restock a bunch of all the things
right that makes sense okay so everybody
ready I think we can do that now ready
so three two one go or to rent a bonus
restock lots of stuff okay it's probably
good you can take a break like I said
you've been working hard
eat and donuts okay so let's see what's
going on here okay things still pretty
slow let's grab a couple of these what's
going on so these are again looking at
making donuts here and let's see yeah Oh
a little bit of Network issues here
something like that it's one still fryer
let's take a look at this one
yeah fire is still pretty bad on this
one but if I look at the toppings here
let's just imagine for a second that the
fire was actually not the problem in
this case but it was the toppings in
this case what we actually have is not
just one call graph but to call graphs
right because there's not any shared
code path between these two things it's
just two resources right and this is a
thing I've seen it come up a bunch of
times where these could you view all to
two different teams at a company right
and not we didn't realize that you're
working on the same shared resource but
what this is saying here is like you
know even though you know I'm looking at
something it's about making donuts 65%
of the load sorry it's super sensitive
not doing it for a fact
65% of the load on this thing comes from
the restocking team right so this is the
kind of thing that's meant to show is
where you know you might have you know a
new release of a service that's coming
out over here puts a lot of load in this
resource not your service is really slow
and you don't even understand why
nothing on your side changed the idea of
something like this is allowing to go
right to that thing and find that answer
and be able to go to the other team and
say hey what's going on
like you got to stop doing this or we
got a we got a provision this resource
differently all right so I got one last
demo I need I need kind of a volunteer
for this one does anyone one person want
to volunteer ok I would like you to
clean the fryer the very generous donors
here so at the bottom of the thing
there's a button for clean the fire I
want everyone else to do what you're
doing before so you guys are ordering
Donuts you guys are restocking my one
new friend is cleaning the fire
everybody to do that right now just keep
cleaning the fire - it's pretty it's
been pretty dirty there's a lot of
donuts going on success success disaster
okay that's good thanks alright so let's
see what's going on here okay
yeah not looking good pretty slow got
some 11 second ones here 10 seconds 20
seconds it's not good he got a lot of
queuing going in here and a lot of work
to do inside yeah once I get back home
okay so let's look at some of these
things and see what's going on here
so I mean not surprisingly like 20
seconds waiting on that fryer mutex and
the thing that's kind of crazy here is
like this if I can make it bigger oh
yeah okay sorry it's hard to talk and
scroll at the same time um so this is
one person responsible for 88% of the
load on this resource and this is
actually another thing that can happen
as well like you can have one you know a
bad piece of code you can have one bad
customer and doing kind of a generic
like look at the whole world and look at
just p99 or something like that like
peanut and I might share all this
problem but you're never going to like
to see that one small one small user one
small service something like that that's
causing the problem if you have all the
data you can do this kind of thing so
again I think it's really kind of cool
exciting thing that we should be able to
do with the tracing assistance that we
have I guess I should say like been my
co-founder and I totally hacked this up
so none of this stuff actually works
this is all just like a janky demo that
we made but the theory of it's really I
think they're that like we have the data
to do this there's no reason we couldn't
build systems like this so I just wanted
to kind of scroll to the right screen
and wrap up here so a couple things I
think I'd like you to take away one you
know think hard
latency when you have latency woes
latency problems about resource
contention I think it's more common than
we might might believe I think that
there's an opportunity to do really sort
of game-changing tooling around based on
distributed tracing that can
this kind of real-time contention
analysis and you know if we can do that
we can really cut down the time to
remediate these problems to root cause
these bottlenecks so we can do that
super super fast by using tools to help
us do that right I think this is the
kind of thing where we should be
developing systems that help us be
smarter that help us work better and I
think we really really can do that so
the very last thing there's a couple
links on here if you're excited about
tracing I encourage you to visit open
tracing that IO up in tracing is an open
standard for getting tracing data out of
applications all the code that I wrote
for this doughnut thing is all just
using open tracing and then I like
talking about this stuff if you like
talking about this stuff I'm gonna go to
the bridge tap house after Adam stock
later tonight if you want to meet up
there and grab a drink or something to
eat
hit me up on slack and we can try and
coordinate and it'll be fun
Thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>