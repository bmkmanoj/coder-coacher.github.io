<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Managing Containers at Scale with CoreOS and Kubernetes&quot; by Kelsey Hightower | Coder Coacher - Coaching Coders</title><meta content="&quot;Managing Containers at Scale with CoreOS and Kubernetes&quot; by Kelsey Hightower - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Managing Containers at Scale with CoreOS and Kubernetes&quot; by Kelsey Hightower</b></h2><h5 class="post__date">2015-09-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pozC9rBvAIs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I don't see a MC here so I'm going to be
my own MC if it gets weird I speak in
the third person just go along with it's
totally natural your next speaker is
Kelsey hi Tara from core OS he'll be
speaking about containers at scale with
kubernetes it probably won't really be
at scale to probably four vm's in this
talk but just go with it so with that
let's give Kelsey a bit warm applause
all right so again managing containers
at scale with kubernetes and core OS the
idea here is what I want you to keep in
mind during the duration of this talk is
how would you design your infrastructure
if you could never login all right let's
say we're starting over from scratch
throw away all the legacy all the
production problems if you work in the
enterprise all those issues how would
you do it right if we could repeat this
process knowing what we know now so one
thing is abstraction we have all these
runtimes varying degrees of VM some were
compiled some have tons of dependencies
so the first step that we would do is we
would try to provide some abstraction
right what do we call an application
what we don't want to do is treat a
collection of runtimes and treat them
all special deal with the deployment
processes because we really don't care
what we really want to do is just have a
single way to describe an application so
one good use of that is container live
you raised your hand that you're aware
of docker so docker is unique take on
containers mainly because it comes with
an image format so many people have been
using containers for a while
but when dr. hit the scene is when we
start to get this image that you can
share around so if you're familiar with
a typical Linux distribution you've
always had this idea of a package and
you can name it you can version it and
get shared with other people and that's
what we get with containers and the
thing we're talking about here are
application containers right so we're
not trying to shove SSH not Chevy more
than one process the goal here is to
keep one application per container and
then we just focus on our application
and the dependencies so depending on
your language your dependencies can be
as big as a runtime some dynamic
libraries or if you're writing something
like going it could be a statically
linked by an area that that's all you
have the other thing is the run
time environment so ideally you want
things like C group so you can contain
and restrict CPU and memory utilization
and you also want to give your processes
their own view of the world via
namespaces so we'll see how we have our
own IP space at Mount space using
kubernetes and docker underneath the
hood when I set this up and here's what
I mean by a process so when a lot of
people talk about containers there can
be some confusion about what goes in
your container so really quick we'll be
using this demo application called hello
world and hello world will connect to
Redis and do a few things for Fred us
but its overall goal if they respond to
hello world it has a health check
endpoint so when it launches I can be
sure that it's healthy and that help
check endpoint will ping Redis to make
sure its dependency is available online
and we'll see this a little bit later
and in this case it's this golang in
order to start to package this up the
first thing you would normally do is
build this right so I'm on my Mac and
I'll compile it so I'm going to end up
with is a static binary so if I run this
hello world app and need permission I'm
going to run this application and it's
just a normal process listen on port 80
and we'll test things out just to make
sure it works right says hello world and
if we get the version from it
we're version version 1 at O which is
great and if I were to hit the health
check in point help Z is my endpoint I
get this error right so I get an on 200
because Redis isn't running this is what
I want and I need this abstraction to
make it easy for things like kubernetes
to be able to tell if my app is up what
I don't want to do is go create a bunch
of Nagios scripts to reverse engineer
the app the app should tell me it's
healthy by hitting this health endpoint
so whatever it needs to be healthy if I
hit this endpoint it should do so right
now it's complaining about Redis so what
we'll do is we'll stand up read its
really quick or the server so now that
registers running we try the health
endpoint and we get a 200 and no error
back so this is our process that we
essentially want to put inside of our
container so since it's a normal process
the next thing we need to do today dr.
primarily works only on Linux so what we
need to do is get this binary ready for
Linux so we'll just do go us equal
Linux so can get a cross-compiled here
so we'll build this and now we'll end up
with a statically linked binary that we
can just shove in the container so the
nice thing about my dependency is being
encapsulated this way I end up with this
you know slightly big 2.4 or 6.4 Meg's
but if you look at my docker file it's
very tiny
right I'm not importing an operating
system to build layers on top I'm just
going to copy in my binary that's all I
need so in order to build this if you've
never dealt with docker before we can
call a doctor build and normally we give
it a tag and we usually name it after
our repository so I'm going to store
this one on goals compute registry so
they have a container registry and then
I give it a user name kubernetes up and
running and I'll name the binary hello
world and you probably want to do some
semantic versioning right so that way we
can keep track of what container we're
running and we hit build what happens
here it uploads our binary tip docker
doctor does this build process and what
we end up is an image so now we have
this image if I were to do doctor
list or doc your images we see that we
have this image and it's almost the
exact same size as the binary that I put
in it so now I have this application
that just works as a normal process one
app per process and I'll have this image
that I can push to a registry and then
I'm ready to start using a system like
kubernetes to deploy it so this is what
I mean by a container
I'm only encapsulating so I can actually
use other tools to manage the particular
application
no RPMs no devs no tarballs so
kubernetes comes into play so now that
we have these containers we need a way
of managing it right so if you start off
with containers normally you point to
one single host and you say hey run
HelloWorld for me if I add a second host
it gets a bit challenging how do you
know what to run where how many copies
need to run where that's where it may
get confusing
so in kubernetes world what we really
want is a collection of machines now we
don't name these machines any fancy
names we don't care what their IP
addresses are really we just want their
CPU and their memory resources so all
we're concerned about so effectively
kubernetes will take this list of
machines and treat them as one log
cool machine right and that's where some
of the distribute computing concepts
come in in this talk we won't have time
to go deep into the details but we'll
touch on some of them as we go
ideally on these machines we need a
couple things we need docker so we can
create the containers but more
importantly we need the kubernetes
agents the couplet is responsible for
owning the server imagine taking a
server and being able to slice up those
resources containers networking and
mount and we abstract that by having the
couplet live there and we can give it
manifest atella what to do now how we
get it a manifest well before we get
manifests we have this concept of pods
so what I built early was a container
one app but my container also needs a
helper process Redis how do I deploy
those at the same time if you're
familiar with scheduling if I were to
schedule both of those there's a chance
that one can land on one machine and one
could land on another machine that will
be problematic then I will have to glue
them back together
so in kubernetes we do have this concept
of a pod we can logically construct an
application from one or more containers
so I can create a pod for threatened and
my hello world app and have them join
and be in the exact same namespace and
it can address each other over localhost
they do share ports so there can be
chances for port conflicts but it's much
rare much less rare than just having
every one share an IP address once we
have our pods we can give them to
schedulers so you as a user you as a
application developer you can design a
pod manifest and submit it to the
scheduler and the schedulers job is to
score the systems in a cluster and
choose the best fit and based on the
best fit it'll run your pod everything
your pod is an atomic transaction to the
scheduler so if you need hello world and
Redis it goes at the exact same time and
the scheduler is pluggable so if you
don't want to use the one that comes
with kubernetes you're free to stop it
and run your own scheduler watch the end
point for unbound pods and you have the
access to all the view of the machines
you can make your own scheduling
decision the next concept is a
replication controller so if you deploy
a pod you'll be scheduled but it goes
down there is no babysitter process to
care about keeping that running and
positive great for one-shot jobs right
you have a batch process you want to
just run a job until completion
use a
if you have a wrong long-running service
like a web app you want to use a
replication controller and replication
control will basically take a pod
template and step out as many copies as
you specify so in this case we say we
want three so we can go from one to
three and what happens is the
replication controller talks to the
central API the central API is the
coordination set for everything else so
the replication controller watches the
API server and looks for unbound pods so
they cooperate they never talk to each
other replication controller also
handles things like self-healing so
since we declared that we want three in
a cluster if one of the machines
underneath were to go away we can
reschedule the pod so we don't move it
like a virtual machine we just
reschedule it using the same template
that we launched the other unit now once
you have all of these pods running
around in your cluster since we do not
interact with the local machines IP each
pod has its own IP address if your part
were to move or be rescheduled it gets a
new IP address so the next challenge
then is how do you actually find the
applications running inside your cluster
and this is where service discovery
comes in and we'll see this during the
demos and what the service is the key
concept in Cabernets is this concept of
labels so you can draw out what a
service means at runtime so when we
launch our pod we can say it's the hello
app and then we can have a set of
services which will give us a virtual IP
or virtual port and if we were to hit
that virtual IP on any of the machines
it will do a lookup in the API server to
find all of the back-end pods that can
fulfill any request all right so that's
what we see that's the last slide right
so I'm going to try to do the rest of
this live demo gods please anyone that's
religious please give me a prayer and we
hope this works so even though the
machines aren't important this is kind
of a glimpse of my cluster I have four
machines some amount of memory and some
CPU but we won't care about that too
much and we won't refer to it if I
forget my commands I do have my cheat
sheet and I will use it so don't judge
me
all right so the first thing we want to
do is now that we have our container
image we want to start to deploy it
so everything in kubernetes is
declarative there are some commands that
will allow you to do imperative things
like hey run this one container but
that's not what we really want to do we
want to have this concept of here's my
declaration we take that declaration we
submit to the API server the API server
is completely stateless it stores all
this data and NCD now the fact that is
stored in NCD is where we get the idea
of consensus we go through raft and we
make sure that the entire cluster view
sees the exact same set of data and all
the other components rely on this fact
it be true as long as that's true we can
continue to work this way so here we
start with a bunch of replication
controllers here's what the v1
replication controller looks like so
here's a definition say we want to use
API version 1 we want a replication
controller now everything has labels
even replication controllers you can
have mini replication controls in a
cluster this replication controllers
name is hello world v1 and it has the
specification right now I'm saying I
want one copy of this pod template now
this selector says I as a babysitter
will scan the cluster looking for PA's
that have this label set if I find the
total amount I will do nothing if I see
zero and I want one I will create one
how will I create one I will create one
from this template this template says
create a pod give it three labels app
equals hello world track equals stable
version v1 and here's the containers
that should go on the pot right I want
my hello world and then I have a
liveliness probe it's the same help
check that I hid earlier on my machine
and that help check will be responsible
for testing all of its dependencies I
don't care what it is if your
application developer if you follow a
pattern like that just make every system
it's like easy
I don't know your dependencies are you
do I hit that end point you handle it
you tell me that they're all healthy and
in this case it would hit Redis which
will run inside the exact same pod right
so now we have this pod definition we
can store it we can check it in I can
use coop CTO to create the pod so coach
ETL create RC hello world
the
so what happened here and I'll let you
guys see what my events look like what's
happening here is that we're scheduling
out the pod so we create this
replication controller it's a bit a bit
noisy here so up here we create a
replication controller and then what
happens is really quick the scheduler
picks it up here's the schedules log and
it's successfully signed one of the paws
created to the template to one of my
notes here it says it got assigned to no
to one so once I do that deployment so
you get all these audit logs if anything
goes wrong if I were to do get pods
we'll see that we have one pot running
right if I were to delete this pot coop
CTO delete pods now if I delete this pod
the replication controller will get
really upset at the cluster and create
another one all right with a different
name because the specification is that
you want run running at all times and we
don't care where it once now we're to
look at this pot coop CTO described pods
if I were to describe this pot we'll see
that it gets its own IP address right so
we see that there's two containers here
right if and hello world and we also
notice that we get our own IP address so
this gives the ability to have our
applications decouple from the machine
the machine is no longer important the
app is unfortunately this is a private
IP so you with networking if I hit that
I get nothing
it doesn't route anywhere right and
we'll see how we solve this later so now
that I have that replication controller
online there's things you can do with
the replication controller now since the
API server has a global view of
everything if I want to communicate with
that pot I can actually use commands to
do so so let's get our pods again if I
want to actually talk to that pot even
though I don't have a routable IP I can
use port forwarding from the API server
to redirect me to wherever that pause
running and give me a way to communicate
so let's try that really quick so coop
CTO we want to do a port forward and
what we want to do with our port forward
is we want to take this pod
right and we want to map port 10,000 to
port 80 yeah
show youth up all right - P okay so what
happens here it will talk to the API
server locate the pod and establish the
link between the two and start
forwarding traffic from my port 10000 to
port 80 inside that container now since
there's multiple containers we can
choose to proxy any port that we want so
if this all works what we should be able
to do is we should be able to do curl
one two seven zero zero that one port
10,000 let me get hello world back right
so you do not need to log on to the
system to troubleshoot these things
unless the API servers down right so now
that we have that in place we can cut
that out and the next thing you may want
to do is run ad hoc commands right so in
my HelloWorld container there is no OS
there is no runtime I can't run any
commands in there but the Redis
container does it actually has bash it
has a user land so what I can do there
is I can run command so cou CTO get pods
COO CTL is zipped one of these pods so
will take this pod and if I want it to
we can talk to a specific container with
dash V and we'll say the reddest
container and what I want to do is call
command you name - a right so that goes
finds the pod and gives me a way to
interact with the pod by running any ad
hoc command that I want so this just
shows me that it's running on core OS
with kernel version 4.2 the thing about
containers they show the kernel with a
host and if you need to troubleshoot
logs you can also do that so if I wanted
to look at the logs I can also do the
same thing so I can also use coop CTO
run the laws command and I get a dump of
the log so this is this the Redis logs
from that particular container so you
have all these troubleshooting abilities
because we have all the data we need to
troubleshoot the cluster in a central
location right now so there we have our
application deployed we we basically
have solved a problem of automated
deployments but the biggest issue for
most people is not automating things
most people have tools that automate
their data
the biggest problem for most people in a
NOAA is resource utilization you ask
people to say it's entirely automated we
have thousands of servers and say what
is your utilization like well under 10%
that is a serious waste of cash right if
you're in Amazon ec2 you make them a ton
of money they mean I think they may just
stop selling books because of this
problem so one way I Ella straight this
up people it's like hey resource
utilization is not a problem so okay
cool let's let's play a game
how many people have ever played Tetris
cool the rest of you you've probably had
a very sad childhood but Tetris is I
think a good way to illustrate resource
scheduling but what I want you to do is
when you see the blocks think of them as
different shaped workloads and we're
going to play a game the first game
we're going to play is one without a
resource scheduler so people say man my
environment is dope it's fully automated
and it's fast okay it's fast so we'll go
to level 9 it's fully automated and then
they say we can do one-click deploys
like that's pretty impressive
how many people here can do one-click
deploys like that guy you guys should
meet these guys at lunch but here's what
one click deploy looks like because it's
very theatrical when they show you it's
like all right you ready run one-click
deploy so you hit one click and then you
do nothing else right and you let your
system as you scripted it out attempt to
utilize your data center efficiently and
this is what you end up with
it's totally automated though which is
dope
but you just wasted all your resources
most you don't even know right so
there's buy more machines so one thing
we could do is we can actually play the
game with our eyes open now I don't
expect a human to do this so don't go to
work
delete your scripts and like start angry
deploying things right don't do that you
don't need to do that there's computers
that can do that for you so here's what
kubernetes and other schedulers like
missiles do especially when they have
resources in mind so given a number of
predicates if we include CPU and memory
we can actually play the game a little
better so one of the things that
kubernetes does is it's going to be a
little slower mainly because it has to
examine multiple dimensions to make a
scheduling decision any paint on your
what your requirements are it could take
longer especially as the number of nodes
increases right but overall what you end
up with is in the simple case you end up
with the strategy of bin packing so even
though most people look at it for the
first time like why is kubernetes
putting all the things on specific
servers this makes no sense I'm confused
by the decisions that it's making the
truth is what is trying to do is trying
to even out and defrag the cluster so
what we want to do is utilize resources
try to make a scheduling decision as
fast as we can we leave room and when
the right piece shows up we put in the
right place and that's the idea of bin
packing there's other scheduling
algorithms that you can do with
kubernetes most people look at that and
say I can't use that in my
infrastructure no way like I'm not what
a green field versus brown field so if
you have an existing infrastructure it
is a bit tougher right you have existing
applications and you're not starting
from Ground Zero but if you move some of
your workloads over so in kubernetes
when you add nodes to the cluster you
can actually say I only want to allocate
half of the resources so your existing
deployment tools can continue to work in
their space and that's your stuff down
at the bottom and then kubernetes can
start filling in the blanks because it
can actually start to utilize data
coming real-time from the machines and
it will still implement in strategies
and over time as you move more and more
workloads over to kubernetes you start
to defrag the cluster right so these
things can actually be bolted on and run
side by side by adding your nodes and
given them a limit view of resources
that they commit to the scheduler now
some of you work at the
enterprise how many people work in an
enterprise right and they won't let you
think or do any innovation without an
initiative there might be hope right so
if you're in the enterprise and all your
stuff written in Java and you have to
deploy to Oracle okay
so those are kind of stacked against you
right so most people say look I have
this situation what can you do for me
and I have to be honest with them
there is nothing crazy could do for you
if you have that particular set up all
right so that's resource scheduling it
is actually important the goal is that
you need to understand how your
infrastructure is operating and these
resource schedulers are designed to do
this so it's not about just automated
deployment it's also about effectively
using the resources that you commit to
the cluster all right so the next thing
we want to do is now that we have a
conscious around resources let's look at
what Cooper nase what tools kubernetes
gives us to handle this and view this so
one thing going to have in my cluster
right now is the idea of a quota
so everything in kubernetes is declared
so here's my strange loop quota set and
here I'm saying I can use 12 CPUs in 48
gigs around now one thing about a
cluster that you also want to avoid
taking tiny workloads on expensive nodes
right if you just want to run your IRC
bouncer and you want like one mega Ram I
cannot have you out taking up resources
from this particular pool because if you
notice down here I'm limiting this to
120 part so if I get 120 IRC bouncers
I'm going to waste a ton of memory and a
ton of CPU because the scheduler is
going to say based on that predicate I
have no more room in this particular
namespace so we want to reject things
so in kubernetes we also have this
concept of limits so limits here would
be if anyone gives me a workload that's
smaller than this I will reject it if
you give me a workload that's higher
than that at the top I'll reject it if
you give me nothing
I will actually decorate the request an
API and store it with the minimal set so
that way we have some resources to make
the scheduler happy and you may have
noticed it because we didn't see it
before but if I were to get pie
Coop's et al get pods we'll see from
this part if I were to describe it
Coop's et al describe pods we'll see
that it was decorated when I deployed so
notice the memory and CPU limits there
and a nice thing about using SI groups
if one of those processes were to run
away only it will run out of memory and
be restarted and Cabernets will give you
logs to say that one process in the pod
was restarted for violating is mirroring
a CPU allocation the nice thing about
that it protects the rest of the system
so if you do resource allocations across
the board the scheduler will actually do
a good job of efficiently scheduling
things everywhere and you won't have
these weird issues where you have this
one runaway JBoss process that takes
down the entire server you can't login
protect it with C groups and you can
avoid some of those things all right so
that's a tool that it gives us so once
we have that tool it's easier to start
scaling out so now the scheduler will
make sure that we allocate enough memory
and CPU for each process so now that we
have one pod running and we remember we
have this template so we do coups ETL
scale the replication controller called
hello world v1 and now what we can do is
change the definition we can say you
know we want replicas to be fine so what
we'll do now we'll go to the API server
and we'll turn the knob on the
definition of replicas so we go from 1
to 5 the replication control is going to
observe this change by doing a watch on
its endpoint and see that there's a
change and then it's going to examine
the cluster and ask for pods realize
that there are 4 missing from the
declaration and add them so it's fully
declarative doesn't matter what hosts
are in or out of the cluster at the time
this is made so now they're all up and
running and I know everything is healthy
because kubernetes will not add the
pause to my view until they pass their
liveliness check and this is why that
help Z endpoint is very important so you
don't chase things around so now that
everything is scaled out we have a
problem still the problem we have is
that they all have private IPS how do we
deal with these things well you can
build load balancers that talk to the
API server
pull out a query to say hey give me all
of the pods that have these sets of
labels or come from this replication
controller but one thing we can do in
Cooper names we can utilize a service so
what we can do is create a service so
here's the hello world service and what
this service says here is that it has
its own name and labels but the
specifications what's important here
what it wants to do is find any pod that
has labels app equals hello world if you
have that you will show up in its
back-end and then when it finds it I
want to allocate a new port I'm doing
this because in this demo I won't be
putting a load balancer in front I'll
just be round-robin across the node so I
can get a high port on my nodes and this
is pretty critical for people like an
Amazon if you have traditional load
balancing gear they may only be able to
address the IPS on their network and may
not have routes into the actual
container IP space and here we're going
to proxy from no port 36000 to port 80
in the container remember we can have
multiple ports available in that
containers we need to be specific so
what I do now is I can create the
service so we'll create the hello
service and once you create this service
gives me this morning depending on the
number of nodes you have now all of them
are now intercepting traffic on port
36,000 for this service across the board
if you were to delete this they remove
it now how this works is Coronas has a
binary called the proxy that runs on
every single node it also watches the
API server for commands and endpoints
like this it observes them in the store
and then it goes and creates an IP table
rules that says anything that shows
about port 36,000 redirect to me once it
gets to me I'll do a lookup in the API
server for all the pods that have the
labels in my selector if we were to
describe this or coop CTL get services
so SBC for short what we can do is
describe this so Chris describe we want
to describe services and hello world
alright so notice the number of
endpoints that are located so we have
three plus two more right and these are
all the things that return from the
label query so what happens if we go to
scale this so if we go from five to ten
we go to the routine of creating
templates unbound pause and then they
get scheduled they find a home and then
what happens is once they become healthy
they show up as back in tier so we see
three plus four more the others are out
of you until they pass their liveliness
check what's alive in this check is past
what we'll see is they all show up right
and this is near real time because
they're all watching the API server so
we'll scale back that back down to five
so once we have this we have port 36,000
I'm free to hit any of the host and
service discovery the proxy will take
over so if I do g-cloud there's just two
G list my little shortcut here it'll
list all my nails in my cluster I'll
grab any one of these public ip's it
doesn't matter the pods running on the
host or not it will always redirect me
to where I need to be so I take this and
what should work is kernel this IP
36,000 out of the world right so we'll
just leave this running so let's just
get the version now all right so let's
do what you do curl and then we'll sleep
for a moment the ultimate hack and we'll
be done right and we'll bump that up a
little bit so the customer is now
hitting our one that L service of hello
world it's healthy we have this proxy in
front so now we are free to scale this
out if we were using a load balancer we
would just put all the nodes in the back
end and let the load bouncer figure out
health of the actual node or we can
actually do some integrations where we
integrate load balancers directly to the
API server to get the same query so once
we have this model the next thing we can
do is leverage a pattern for deployment
called the canary pattern how many
people have heard of the canary pattern
great
so craze makes it really easy to
implement
canary pattern since we're using labels
we can carve out a set of things that we
proxy to so right now our service is not
really that restrict or that strict it
says if you just have app equals hello
world I will send you traffic so that
means we can create another replication
controller and we'll call this one
canary it looks the same so I will
review all of it the only difference
here is that it will spin up the image
to dotto has a different set of labels
so instead of track equals stable track
Eagles production and this is so both
replication controllers don't fight each
other right you don't want them spinning
up and screwing up the cluster so what
we do is we give them something separate
so that they can view the containers
that they're responsible for and here
I'm going to have a specification of one
so if I curate this notice one thing
that it also meets the criteria for the
current hello world service at equals
hello world so if I deploy this what
should happen create - F replication
control is hello world canary if I
create this we go to the same song and
dance it will be created we look at the
number of pods get pods we see that the
canary is now running right oh refresh
itself letting me know that two of two
pods are ready and if we look and see
what the customer receives do you see
the canary here so two dot o get stuck
into our traffic right because the
service is boxing to both when it does
this label cream it also matches now
some people will say well what if we
have a specific customer that only wants
to see the new stuff and not the old
again using the labels we can also carve
out another set all we have to do is be
a little bit more strict in the new set
so we look at our services here hello
world canary will be a little bit more
strict will allocate a different port so
we can have a different path and then
what we'll do is say you know what to
meet the criteria in our selector you
must also have a label called track
equals canary you can add labels to pods
on the fly
even at run time and that way you can
have the services pick up those pods if
you're doing in troubleshooting or
you're trying to expand so if we create
this coop sitio create - F service
a world canary what we end up with is
now a new service portal or a service
port for this so I'll stop this
and now those customers are free to hit
port 36 thousand won and what they
should see is two that oh no matter what
because that's the only thing being
returned from that particular label
query right and the way those work get
pots so we see now with no queries we
get all the pots if you do coop CTO get
get SVC we see the label queries that
are actually being made behind the
scenes by our services let's try that
again
so if we were to do get pots COO CTO get
pots - L and I'm just going to copy the
selector so show me all the posit have
hello world we get them all if we do the
same thing that we use a different set
of label queries or labels in our query
you get different output well there's
nothing called PP equals hello world so
no match but if you add app there and we
only get one result and that's also how
the whole system works right so once you
have those things in place the next
thing you want to do is you're happy
with the canary let's go back to running
our normal app so the Canaries out there
there are no issues what's the next
thing you want to do once you have a
canary out there you want to roll the
cluster so the version that matches the
canary so what we'll do is we'll stage
up the v-2 of our hello will service so
hello world v2 so in v2 what we do is we
have a different version label so that
way this replication controller does not
conflict with the other and then we also
update the specification we want the
image to be too and I preserve my
replication set now there's a way to
actually patch the current replication
controller and just say you want to
update an image that will just keep it
declarative and submit a new controller
to the cluster so what we'll do here as
we'll do a rolling update so this is
what the
mercies and kubernetes tries it's best
to not drop any traffic by a giving time
for the proxy the finisher requests
removing it and then adding something
new it's healthy then adding it so
hopefully we don't see any drops in this
so what we want to do now is our rolling
update great to my history so what I'm
saying here is cout CTL I want to run
the rolling update command every 5 every
half second I want to kill the pot and
let the other one increment up by one so
we'll just start to tune the knobs down
one up one until we roll the entire
cluster to meet the new specification
dash F points me to the specification I
would like to be new and then at the end
hello world - b1 is the name of the
existing replication controller I wanted
to replace so when I hit enter here
what's happening is we go through this
loop and we start to dial up and down
both of the replication controllers what
we see here in our note as we see things
being killed things being scheduled
things being started so if you want to
do monitoring in kubernetes you can
actually process all of these events
into an event system or you can actually
grab logs directly from the containers
themselves but all this data is
available to you so we know that the
cluster is working we take a view and
see what the customer is looking at we
see that we're starting to get to know a
lot more often our requests we're almost
at 50/50 here we're still not dropping
anything which is great we have one more
to go so on the old controller we still
have one replicas in place on the new
control we now have four and over time
as these things become healthy we should
have the entire cluster roll to two
dotto so there's still one scraggly out
there let's see if we're done yet so
we're down to zero and one five and what
we've asked is the crust to replace it
so hello world v1 should be deleted as
the final step at the end of the loop
and if this works we have to Daddo
across the cluster and with that our to
end my presentation thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>