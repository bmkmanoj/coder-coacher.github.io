<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Practical data synchronization with CRDTs&quot; by Dmitry Ivanov | Coder Coacher - Coaching Coders</title><meta content="&quot;Practical data synchronization with CRDTs&quot; by Dmitry Ivanov - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Practical data synchronization with CRDTs&quot; by Dmitry Ivanov</b></h2><h5 class="post__date">2016-09-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/veeWamWy8dk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone thanks for joining really
excited to be here and talk about zir
disease it's still a trendy topic in the
industry nowadays so first of all our
short disclaimer I don't claim myself to
be like hardcore academia guy or
distributed systems experts I'd rather
want to show you that some recent
academia achievements actually can bring
a lot of value it can help to solve
real-world problems so I work at TomTom
in Amsterdam on a project called naath
cloud which is a cloud-based service for
storing and seamlessly synchronizing
personal data of our users navigational
data but not limited to that
and of course not cloud as probably most
of the services seems to be available
scaleable we have to be legally
compliant and secure because personal
location-based data can be quite
sensitive of course and developers and
not cloud projects our full-stack
developers so we hack on back-end
services on client libraries and SDKs
and also an infrastructure so this
really helps us to understand how
developers that integrate with the
platform experience our product or
platform on different layers like REST
API is client libraries and such so
speaking of the development stack that
we are using
we're scholarship at least for for the
back-end services and yet we'll do like
typed functional programming and we also
leverage some open source distributed
services and our flat and the client
library site we do have support for
different platforms about mobile
platforms and these client libraries
they basically do lots of heavy lifting
for our clients and for developers that
integrate with us so they handle
accession management connection state
monitoring background syncing and etc
so I want to argue that this is the nut
cloud nature but actually it's also
applies to probably most of the data
synchronization systems so of course
nowadays like users have lots of devices
personal devices and they all are meant
to be connected to the Internet but the
problem is that connection can be
unstable you go to the parking garage
you lose your connection but users
expect their applications still continue
working right
of course we also have problems with
limited bandwidth or network data plans
for example for in TomTom case some of
the devices personal navigation devices
that we ship to the market they are
equipped with built-in SIM cards and
TomTom pace for the data plan on these
scene cars so all the data that that is
used on these plans is expected to be
you know used efficiently and again yeah
offline offline readiness is really
expected from the applications that
build on top of this platform and the
data that is distributed across multiple
devices owned by the user can be
modified and modified concurrently and
these changes they can be potentially
conflicting and we platform bottom has
to handle that and of course there is
also no guarantee in and updates order
and our users they don't expect us to
their data that will be really
unfortunate and of course when users
make some changes whether it's offline
changes online changes they expect that
the data will converge to the expected
value so how to deal with this nature of
course you can probably write lots of
logic on the server on the client
libraries to handle the cases especially
H cases that's how we were also starting
with enough cloud in 2013 the first data
types that we were thinking we were
covering all the edge cases that can
occur when you are offline and you back
online and cetera et cetera it's not a
teaching and as Linus Torres once said
good developers they actually try to
solve their problems on the data
structure level because data structure
usually data structures they usually
provide good abstractions and underneath
they can they can really help to solve
the problems efficiently so here enter
CD T so as probably some of you know C
or D keys are family of data types and
these data types they have their own
algebra the R stands for replicated so
here DTS are meant to be distributed
across multiple knows multiple would say
clients horribly multiple devices and
cured it is they guarantee that the data
will be in conflict three states
sociology itself handles conflict
resolution for you but how does it do
that the key ingredient in CRD T's is a
merge operation or nutria share also
called join operation so merge operation
is a binary operation on to Shirdi T's
that yields another share DT and the key
thing in here is that the merge has to
be commutative associative and at
important so how does it help well as we
know probably in distributed systems
it's really hard to guarantee on the
order of updates but that's not problem
because merge operation is commutative
and associative we can receive updates
in any order it's also uninfluenced
delivery so we have to rely on let's say
at least once delivery but that's not
problem for for Co duties because Mercia
is a idempotent action so what does it
bring in practice the thing is that CRD
T's is not about servers and clusters
usage only actually when you use CLE T
is also on the client side you know
client libraries you can get additional
benefits like local updates
when you just can modify your data
without having any connection but you
know that later when it gets connected
and you push that data and propagate
that the data will converge to the
expected
and rich this is really beneficial to
decouple the data modification flow from
the communication flow let's see let's
see some examples so the first one is G
counter is a grow only counter so it has
only one operation incremental counter
really simple so let's say you have a
website and you want to count number of
hits on this website and of course the
website has to be available highly
available so you have more than one
machine in the fleet and probably the
name approach would be to just have a
local counter on each machine counting
the hits and then upon gossiping between
the servers they have to exchange the
data and merge it how do you merge the
data well probably we can think we can
just take two counters from two machines
and then you know just just sum them and
that's it unfortunately that does not
work because the addition operation is
not right important so if you upon
gossiping exchange messages twice
between the nodes and you know you you
over count so G counter solves that
problem a little bit differently it
actually uses the map underneath so it's
a map or from replica ID to the replica
counter and then the each replica can
increment only its own counter and upon
gossiping when they exchange the data
actually when they merge it they have to
take the max of corresponding elements
for each element in the map and to count
the sum of elements you just need to
simply take a sum of all elements all
the values in the map why does it work
well because the max function is in fact
he aleady compliant merge compliant it's
commutative
it's associative and aside important
another example is G set grow only set
so it's a set where you can insert new
elements you can remove again so naive
approach to have a single underlying set
on all the machines and then upon
gossiping just when two values have been
has to be merged just take a union of
that right will it work well
in fact it will so that's how G set
works it has just one single set
underneath and then the merge function
is simply union of two values together
and the total value is just the value of
the underlying set but why does it work
well the union function unlike addition
function is actually Theology compliance
so it's commutative associative and
idempotent good so let's see how we can
leverage the oddities in our case for
data synchronization enough cloud so one
of the types of data that we
synchronizes favorite locations so let's
say you drive somewhere you like this
place you mark the marketers favorite
location and it gets magically the
distributors replicated across all your
devices and to the cloud we also
synchronize different types of other
types of data like boys tracks and
cetera but it's probably the simplest
one so what could be the naive approach
in that case well we can just have like
an array of favorite locations on the
node and then we just well being offline
we modified adding an element let's say
and then when we get the connection we
just push the data to the server and
then overwrite the value on the server
we've lost writing strategy
unfortunately that's not going to work
and the problem with that approach is
that again we haven't stable connections
and when you the actual update time can
be drastically different from the time
when you send to the server and again
can be different from the time when the
server receives that because of network
latency and such and of course clocks
are unreliable all these problems lead
to a nasty nasty thing which is and
stale update may update mail may win and
this is definitely what not what user
expect so what let's try see realities
here is in fact really nicely match the
data synchronization platform nature
because we have unstable connection
that's fine because we're completely
decouple the mutation of the data from
the connectivity so we can easily update
the data while being offline is fine
limited bandwidth so this talk mainly
about the type C OTT which is called
state-based reality they're also
operation based charities so instead
based realities in the original ones at
least it's it's expected that the whole
state has been transferred to pond
gossiping and it's not really Network
efficient right so what we did we
actually extended the algebra of here
the cheese with a deformation to be to
be able to calculate the exact amount of
data that needs to be transferred over
the wire to another replica of course we
also get with varieties things like
offline addition and then concurrent
changes with potential conflicts also
handled ordering is not guarantee it's
fine because we have merges it important
we don't lose any data because we always
merge we never overwrite and then the
data conversions to expected volume will
it work
so in fact yes it will because we have
the same data everywhere on the servers
on the clients and even in our datastore
datastore ok so we actually use the
react from basha which is masterless
distributed key-value store inspired by
amazon dynamic paper so it has different
approach for conflict read approaches
for conflict resolution one is law
straight wins which probably not
something that you want to use another
one is based on siblings so siblings are
conflicted values that are created when
you write back to the database stay
avalue and in that case it's the client
responsibility your responsibility to
solve the conflict upon the next read
and sometimes it's considered to be not
a little bit client friendly of course
but in our case it's fine because we do
store just your duties in a database so
for us it's just a matter of calling the
merge operations on to sibling values
from the database and that's it really
simple so let's have a look how we can
implement a co DG set that can allow us
to synchronize seamlessly synchronized
favorite locations across our devices so
what do we want we want this set to
support addition and removal operations
so G said it was really work for us in
that case we want to optimize this set
for mutations and the footprint of
the set it should be more less compact
because of the network data restrictions
so the first one you can have a look
from the you can look at the set from
the literature's to face set it does
support additional removal operations
and it's modelled it can be modeled as
2g sets one for added elements another
one for removed elements like a Tom
stones and in this example if we want to
remove an element we simply need need to
take that element from from the add set
well we still keep it there and put that
also to the remove set how do we merge
these two replicas well underneath of
the two phase sets throughout two G sets
and we know that G sets are merge about
they are their shared keys so we just
need to merge corresponding G sets and
that's it and to calculate it look up in
that case we simply need two final
elements in the add set which are not in
the remove set it's a deadly simple
there are some code examples but I will
share later a github link to this to
this repository the facet didn't work
for us because once you remove an
element it's removed forever right
you cannot riad that again and the
elements are immutable
of course one might argue that you can
model the modification of an element as
a sequence of delete and then add again
operation and that probably will work
but the problem with that is that it
will quickly inflate the remove set and
we will see later how it can bite us so
another example of here that you set is
lust right wins element set which also
supports additional removals but this
time it actually adds a timestamp to
each element and again can be modeled as
2g sets one for edit one for removed
elements and it does support reading of
the element to riad an element to the
last rate when set you just need to
increase the timestamp so again an
example each element is a pair of
logical clock and the volume and again
the merge is deadly simple we just need
to merge corresponding G sets and to
calculate a lookup
we simply need to find out all the
elements in the ad set which have the
higher time stamps than their
corresponding values in the ad in the
remove that's right and here are some
code examples to merges everywhere is
the same we just take a union of G sets
it still has to this problem of
immutable elements so you have to quit
you will quickly inflate remove set so
we looked at another one which is or set
or it stands for observed and remove set
and the same it also supports additional
remote operations this time a unique tag
is associated with each element and to
riad an element to the in in the or set
you have to simply introduce that
element again with the different new
unique tag that's it again the merge of
these two replicas is simply merging to
G underlying G sets and to calculate a
lookup in that case we need to find all
elements in the ad set which have unique
tags which are not in the remove set and
there's some code for that cool the same
problem still the elements are mutable
will quickly incite the remove set so we
decided that we can somehow extend the
or set with this mutation functionality
and we called it observed updated remove
set or shortly our set one is intended
so each element in that case also has an
identifier this identifier is immutable
so if you want to modify an element and
a set you have to preserve this
identifier and then we have a timestamp
which has to be changed every time you
mutate the element and this case
actually were decoupling we have kind of
identity which is immutable based on the
unique identifier you go ID for example
and then the value which is mutable and
can be mutated so underneath actually
how it's implemented is implemented as a
single underlying set not G sets just
for the sake of optimization so we use
this discriminator remove boolean flag
to actually distinguish removed and no
most elements and then we also have a
metadata like ID and timestamp so here's
again an example each element in
replicas is free or fulfilled so ID
timestamp and the value and an optional
removed flag so the merchant here is a
little bit more involved only a little
bit so we actually have to take a union
of two replicas then group elements by
IDs and then in each group we need to
find a winner
so how do we find a winner well we have
a timestamp so we can just quickly pick
the element with the highest time stamp
simple right and how do we calculate the
lookup in that case we simply need to
filter the filter out the elements which
are removed that's it and there's some
code examples so let's let's try to
apply that to our favorite location API
so exactly we model our each element of
our favorite location as like a state
element which has this additional
metadata attached to it plus some data
so instead of favorite here can be sort
of any data right which is sort of
business domain related and one might
argue so you mentioned that we take the
winner by timestamp what happens if x x
type that's a good question so compare
function and the merge function has to
be deterministic so in that case if we
have a time x x we have to continue
comparing the elements metadata and then
compare the removed flag so then we have
we can have like an add or delete bias
for example dependent on our business
logic and then we have to compare the
the rest of the attributes going into
like business domain specific attributes
like name location and etc it's
important to have compare function and
merge function deterministic so okay now
we have these C or D key for for storing
favourites everywhere in our application
and our server in our database
everywhere it's the same technique we
just merge the data that comes from
another replica and when we also modify
the data which is awesome
to a local store it's pretty simple
model to reason about pretty simple
model in unified model to decode and for
example in that case the server is not
really that much different from a client
it just gets some data from the client
and then it needs to merge it with the
data that stored in our primary data
store
push it back that's it it's again pretty
easy to code so okay sociology is
everywhere sounds perfect
is there a trick or something right
there is always a trick so yeah let's
have a look at some considerations of
limitations that we observed and at
least in our use case so I think the
most common question that we get from
people who learn that we use here duties
in production is that so how do you deal
with garbage and that that's a problem
because here the cassettes tend to grow
because of Tom stones so every time you
delete an element it's it's it's still
stored in their underlying set as a Tom
stone and it's potentially an unbounded
growth and we all know that involved in
the data structure is like a killers of
distributed systems so we have to deal
with that sure
so ok we want to prune deleted elements
but the question is when well
intuitively we can think of probably all
nodes that hold this element that that
is about to be it has been removed and
we want to prune it they have to observe
this deleted operation right otherwise
we will have an element resurrected
again which something that we don't want
we can apply the time to leave strategy
for Tom stones so after some time we
just get rid of this Tom stone the
ticular that is that this dentally
strategy has to apply in the has to be
applied independently on all the nodes
and client on the server and this is
really important requirement to hold
another trick that we we saw is that we
as I already mentioned we extended the
algebra of theology with default raishin
with Delta operation and the reason we
do that is to optimize the bandwidth
consumption and when for example in that
case the client being aware the
what what the server has because the
client from time to time gets data from
the server it can calculate the diff
difference from the last observed value
from the server and then it sends two
elements to the server but server in our
case is not aware of the replicas state
of the client of particular client so
what does server has to respond so it
can respond the full merged set but it's
super inefficient because we will in
that case we transfer already two
elements that already have been observed
by the client so we can do better by
using a simple technique really simple
techniques called called scoped if so
instead server responds only with
element value that has one over the
value that client has sent and in that
case it only the the the modified be and
the client just simply merge that client
doesn't need to know whether it's a
scope D for normal D of client always
merges the value that it receives from a
server so it's really really a pack for
the client but it's optimized still
there is another interesting case which
is the other way round so we have these
subscription mechanism where the clients
can send the request to subscribe to
changes from the server and upon the
first bootstrapping phase server has to
send like the the the latest that
changes from that client has not
observed yet the problem is that as I
said server is not aware of the state of
the replic of the client and in the
knife case we can just send the whole
all values that the server has but again
it's really inefficient especially if
the client subscribes and jobs
connection is dropped then subscribe
again and cetera et cetera so what we
did instead we introduced some metadata
which is stored outside of CRD team so
it's a little bit of trick which
actually associated with every element
in the set on the server only which is a
cloak it's just a monotonically growing
growing cloak that is every time an
element is modified then this clock is
increased to the maximum value plus one
of the cloak of any element within the
collection
and the server upon any updates since
that updated data to the client with the
clock value so for the client is just a
matter of storing that clock locally and
then upon next connection when it wants
to subscribe it sends that clock vision
that clock server can calculate the the
elements that have been updated since
the last time that client has observed
their changes it's really efficient
technique and also allows us to really
reduce the data usage of the
subscription channel that we have so
let's talk a little bit about time there
is a trouble with time right so there is
no such thing as a reliable time
unfortunately I like this quote from
Janos bionaire from one of his
presentation arguing that actually when
we are tracking the time we're less
interested in absolute values of time
actually it's more about causality
causality and ordering of events that's
what important and in our case events is
just the wording of updates to a single
element within a set that's it time can
be sometimes just good enough it allows
us to deterministically order the events
the updates to the same element so of
course we can look at that from a
simpler point of view where we have just
ordering a place within a single node in
that case the timestamp field in our
metadata is just a logical clock that's
it absolute value is not important at
all it just should grow monotonically so
the only thing we have to do is to
ensure monotonicity is so when we
retrieve the timestamp from any provider
provider which is available on our
platform of El platform etc we just need
to ensure that it doesn't go backwards
that's it and sometimes it happens like
if we use like system current time
Milly's there's no guarantee that it
doesn't go backwards right so at least
we want to preserve that ordering of
updates from different nodes is is much
more complicated of course and being
company that has to deal with lots of
navigational devices that have GPS and
GPS clocks we might be sometimes a
little bit in more fortunate position so
of course the GPS clocks provides really
reliable time so we can just rely on
that
lots of our clients mobile clients don't
have that and in that case at least we
prefer to use server time to a client's
time so for example whenever a
connection is available we fetch the
server time cash and cash it and then
calculate the elapsed time since that
and this provides at least more reliable
time than then we can have otherwise
there is still an edge case this warning
that saying that if we have multiple
clients exactly at the same moment
modifying the same element within a
collection we can have a problem the
same applies when we have completely
have no reliable clock at all
unfortunately these edge cases may
happen still so we already seen that
picture were on all the layers
application server database everywhere
we have this merge operation one merge
to rule them all there is a trick with
that of course that the clients and the
server have to have exactly the same
merge behavior this which means that
given the same output input they should
produce the same results that's
crucially important because if it's not
the case if we have some rogue your
clients that that produce different
results and they can organize these nice
and list synchronization loops which are
not really funny to deal with yeah so
another thing that we also observed is
that we already seen the our structure
of our set element right so each element
has a metadata and a data associated
with that and when we compare elements
and when we merge them we actually need
both metadata and the data because we
always have to compare also them the
data if we have a tie otherwise so we
for like okay can we lift some
information about data to the metadata
level which allows which will allow us
to make data optional actually we can
because we can introduce that tag or
which can based for example can be based
for example in the hash of the value we
can introduce that to the metadata level
and in that case we can do all this here
DT operations merge operations compare
elements on the only metadata level and
have data lazily loaded and in that case
we can have a pretty flexible
synchronization strategy where you can
have eager
or lazy fish for example and this is
really beneficial for our clients so
sort of a summary what have we learned I
guess the point is kind of that the
academia sometimes might be not that
scary too pragmatic developers as it
might seem of course there is certain a
learning curve when you first encounter
CEO duty and you go to the wikipedia
page you read immediately that C or DT
is like a bounded semi lattice with a
joint operation which sometimes might
sound a bit cryptic but of course still
academia and recent researchers provide
really nice abstractions that can really
help you to tackle with with your
problems in a more efficient way than
just coming up with your own complex and
homegrown conflict resolution logic and
I think important point also that we
really need the better and simpler
abstractions to develop offline radio
applications like there are not that
many applications still that are really
can nicely handle the offline
synchronization scenarios and offline
access to the data and etc share duties
do provide a great value with that there
are some caveats as we saw things also
like even higher order abstractions like
for example last language like Chris
Michael John also could be a nice answer
to that problem there's a code available
on github with some samples and scholar
in Java for all the collections that
I've shown and yeah thank you very much
these are my two co offers Nami and
ideator
there are awesome and without them this
presentation would not be possible so
thank you
yeah there is a problem with these kind
of like platforms that provide so much
so so much abstraction so that's why for
example we don't use one any of these we
actually came up with like our own
implementation of co2 which is pretty
slim and it's just it's just a
collection in a and right it's just like
you you probably like when you
synchronize like a bunch of data in in a
collection like you have a list or or
whatever array and you just convert that
to share which is pretty natural in the
way of course the yeah I I see what you
mean like higher order frameworks that
that's really and the lots of stuff for
you and then there's really like far
away you know from from your business
logic and in the end this this is an
interesting question in the end so I
think in general the question about
adoption is is has the sort of adoption
problems have different sources it's
like one of them is that it's not all
data can be modeled in terms of CR DT so
you really
have to tackle the that problem that
you're trying to solve and of course the
collection case is pretty nice because
it's really nicely covered by CEO duties
but there are some data that the data
types that a little bit harder to you
know Express in terms of sheer duties
but yeah I think we need a little bit it
will be nice to have actually here the
key types be more like a slimmer maybe
part of even like a standard libraries
of different languages that really do
provide like simple operations like
merge and decent etc and then you can
build on top something harder something
more sophisticated but I don't know does
it answer your question question yeah so
of course like in our example for our
REST API exposes here DT compliant API
right so when you work with that when
our clients work with that they work
with your duty compliant API so it's a
little bit different from like a
traditional API and this is also kind of
a problem when you integrate with like
you have to build up this whole stack
right from the server to the client
that's that's that's a problem that that
I do acknowledge definitely so as I said
in the beginning we were fortunate to to
implement the whole stack from the
server to the client libraries so we had
much more control over that of course if
when you integrate with like let's say
third party service you don't have an
option right although we I had some
discussions at one of the previous
conferences with the mobile guy actually
even he contributed to our Java cle-cle
library to make it Android compliant we
had the discussion that can you leverage
here that you're only on client side for
example when you work with the third
party API that doesn't provide here that
you compliant here and then in fact
actually I think it's still still
beneficial because you can have you have
this nice abstraction that takes care of
concurrency nicely and you have local
updates and everything where you the
programming is easier even on the client
side only with that but yeah I think in
we still we still need to think how to
bridge the gap between like you know to
build the whole layer from the server to
the client and back yeah that's true
that's why I also mention things like
clasp that can can help with that
because basically they build up this the
the whole protocol from from the server
platform to the clients but it's it's
still under research
incompatible in which sense well I mean
we will always resolve conflicts
ultimately that's that's kind of like we
even wouldn't know yet so so if we look
at the the compare function actually it
has to be deterministic right so so if
we look at the yeah so if there is a tie
with timestamps we will compare the
remove slack and rest attributes we
consider that if there are like if there
updates with exactly that the same
timestamp for us does not really matter
which one wins of course it depends on
your business model of course maybe you
don't want that in that case mmm yeah
you have to think also you you can
implement a little bit more
sophisticated merge model right so you
can not just pick one element you can
actually merge them yeah
yet so the thing is that react itself
has Syria to implementation in place at
least react 2.0 since the 2.0 we didn't
use that because multiple reasons we
wanted to really have full control over
the logic that how our CDT set is
implemented and etc so for us it's just
we as I mentioned we leverage here sorry
react conflict resolution strategy where
it always keeps all conflicting values
and then we on the clients or in the our
application code on the server
take care about merging these together
right so I guess this with sequel
database it's a little bit tricky
because with sequel database I guess
we'll just overwrite that right so they
won't create these conflicting values so
I cannot answer that question in
regarding to sequel but the database is
yeah so if you're unfortunate to use
database with last write win strategy
then you have yeah you have to think
about it a bit maybe you have to
introduce some intermediate layer that
that will take care about this merging
before pushing that too but yeah that's
that's that's always a problem when you
basically switch from this always
merging strategy to like the straight
win strategy there is always an edge
case where you will you might lose some
data this is always like this is the
boundary where but things might happen
so
I gave you absolutely no no no no yeah
yeah yeah yeah yeah that's the point so
react with this allow multi-strategy
it's cold when upon next read when so
when you read from a key which have has
like two conflicting values siblings
then both siblings will be returned to
you and then you will have to have to
merge them that's that's how react works
and it wasn't designed specifically for
sure DT so in more traditional approach
it was just your client code
responsibility to merge them together
with your homegrown logic for conflict
resolution etc it's not it doesn't have
to be certain necessary reality it can
be anything right but yeah that's that's
how we got work so it always returns you
all the conflicting values it never
drops anything yeah that's the point
yeah yeah yeah yeah so we just see
realized to JSON and yeah it's it's just
like a binary data yeah I mean can be
anything brother barf or whatever
yeah yeah absolutely absolutely I mean
the bigger seriously the the the higher
payload Network payload that has to be
transferred overland that with the wire
like between the database on the server
etc so we put some boundaries on that
but it's still it's still tricky so of
course you do have to put boundaries but
the question is what to do next right so
I mentioned this time to like strategy
where it can gradually get rid of Tom
stones let's say but like in in in cases
of most of our lawful clients and the
users it's not a problem because like
problems start occurring when you have
like thousands of elements in a set
which for single user is for out in our
case at least it's it's it's probably
unlikely but of course when you have
like a better test users or developers
that integrate with our platform right
lots of integration test and they can
easily reach these limits and then in
the end state can be just rejected so
our platform prevent prefers to reject
this kind of updates rather than you
know just degraded performance that's
this kind of we're thinking about that
it's not it yeah it's it's it's not that
easy so of course yeah you can probably
do that you you have to apply the same
technique like on all the clients right
yeah this has some problems this has
some limitations basically because
because the thing is that so
there is no any coordination between
like updates on different clients right
so when they just like Victor elements
like you don't know which element will
be like they're not necessary in the
same state right replicas can divert it
can diverge and then if you start
dropping if they start dropping these
updates it's not necessary that it will
drop the same updates everywhere right
and this simply leads to like situation
when like different clients will just
chase each other and then just try to
resurrect it all over again so doesn't
really work probably it might work with
like one or two clients working with the
same with the same sheer that she set
replicas but then with the higher
distribution of clients I think it won't
work
and our questions yeah yeah so I'm not
really familiar with operation
transformation but from what I heard
it's more like the this unit is that we
we work with a state based theology so
there are quite different from the
operation based here that you were an
operation based transformation is
actually closer to operation based here
duties we're actually as far as I know
of course operate with like changes over
the wire right you don't transfer the
state of the data you you you sort of
say for example add this for example or
modify this so it's it's rather
operation transformation that's that's
why schools it I guess like that but
yeah the problem well I cannot say for
operation transformation but for example
operation base here this is there is a
problem with with the additional
requirements for the network
connectivity so when basically for state
based realities the order is not
important and you can just rely on on
any order of updates and etc where is we
have operation based theology you the
network have to be have to provide
certain guarantees for the for the
delivery of these for these operations
to the to the replicas and that's kind
of a problem and that kind of makes
designing of the systems a little bit
harder especially we like different
clients completely uncoordinated working
with the server also server
communication to the client has to be
designed in a little bit different way
another question
yeah
yeah exactly so it's it's lazy transfer
actually so the trick is is that we can
just now disseminate the reality sets of
metadata of the elements and we don't
need to transfer the data itself and
it's really beneficial because of course
we you know we can just have like
on-demand loading of the data when user
like you know presses the button and
says like I do really want this type of
data then it will be reloaded separately
from the server yeah yeah so actually
this metadata is kind of like a
composite identifier of the data which
is optional in this case so if if you
receive this metadata you merged you you
you actually see that this data is not
in your local storage and then based on
this composite metadata based ID you can
ask the server and server will always
respond with that date okay that's it
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>