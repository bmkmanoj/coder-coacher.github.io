<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Tuning Elasticsearch for English-Language Precision&quot; by Erin McKean | Coder Coacher - Coaching Coders</title><meta content="&quot;Tuning Elasticsearch for English-Language Precision&quot; by Erin McKean - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Tuning Elasticsearch for English-Language Precision&quot; by Erin McKean</b></h2><h5 class="post__date">2017-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kUS16DwY0xw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hi so I always like to show this
slide because it's not actually true but
I feel like it makes you feel justified
and showing up in this room so and also
I had a blue lanyard on but for the
purposes of this talk please feel free
to take pictures for I don't know later
ridicule purposes but hi I'm Erin and as
my day job I'm a developer advocate for
IBM with a San Francisco City team and
for the rest of the time I also I found
it and I run a site called word Netcom
which is a large and very friendly
online English dictionary
with an open API and I also run the
semicolon appreciation society and I
have lots of these stickers so please
hit me up later I'd be happy to give you
one and I am like oh my god why am I
here because this conference is pretty
intimidating and I am a community top
developer I like to say community taught
instead of self taught because it's the
generosity of everybody in the community
that helps me learn things so but I want
to say if you hear me getting something
wrong please let me know if possible
after the talk and if you won't do it
for me please do it for my future
audiences so I'm always happy to learn
where I can do something better do
something clearer and if especially if
it's wrong so thank you very much
so I'm here mostly because I love words
and I like to talk about words and I've
worked on dictionaries for 25 years and
I've been collecting words for even
longer and collecting words this
pointless unless you share them because
then you're just hoarding words and
nobody likes a hoarder and also for
those 25 years I've been working on how
best to share words I've put words into
all sorts of boxes I've put them into
literal file cabinets I have put them
into literal index card like library
card catalogue drawers printed books
I've put words into sgml if foot words
and XML relational databases like my
sequel no sequel databases such as Mongo
but my favorite place to put words so
far has been elasticsearch um this slide
is actually from a really great piece of
art by Joseph Casas which is called four
colors for words if you ever have a
chance to see it go see it it's awesome
but why elasticsearch so elasticsearch
is a distributed restful search and
analytics engine that's what they put on
their website it's open source it's
maintained by elastic code is anybody
here from elastic hey thank you guys and
yeah you're good being the guy that
tells me when I'm wrong that'll be
awesome so it schema free you come with
JSON documents it's free to use and
deploy anywhere there's hosted services
from elastic AWS even iBM has one I
think the number one reason to use
elastic search is because elastic search
doesn't presume to know your data better
than you know your data so if you are a
control freak with delusions of
omnipotence elastic searches for you so
yeah and so but when you're putting
words into any kind of place you have to
think about okay what do I mean by word
let's you know define our terms when I
say the word word you're probably
thinking about something like this it's
a nice simple word it's a prototypical
word and now I've actually satisfied the
technical talk requirement of including
at least one cat in every presentation
so but there are a lot of words that
aren't cat like words there are a lot of
words that strain native-speaker
intuition about what words are and here
are some examples these are also words I
don't care if you don't think that these
are words come fight me later these are
words and you can see some things are
just you know okay well maybe you might
say oh but that's a phrase or that's
just hyphenated or that's a prefix or
other that's an acronym or an
abbreviation and that thing at the
bottom that's what people are gonna
fight me on but from my point of view as
a dictionary editor these are things
that people look up which makes them
words and it might be fair to call these
things lexical items essentially these
convey meanings that are not 100% trans
parent from their parts so not to get to
humpty-dumpty about all of this but
we're NEX philosophy is that if someone
wants to look something up and we're
nick has data that shows how that word
is being used or who uses it or where
it's used or when it's used we want to
show the data that we have we don't want
to punish people's curiosity by being
all snooty and sniffy about what a word
is you want to know we want to tell you
and we definitely follow apostle's law
and that we are very liberal and what we
accept so when you're thinking about
okay what do people look up you really
have to be thinking about what do people
want users have different kinds of
searches with different goals sometimes
they want to know what a word means
sometimes they want to know how to use a
word so they want to see an example
sometimes they want confirmation of
spelling or pronunciation sometimes they
want to win an argument in a bar but all
of these things all of these different
kinds of answers are accessed the same
way by typing something word like into a
search box so ideally you would get an
exact match between what's being looked
up and what's returned but if people
knew everything about the words they
were looking up why would they be
looking it up and these questions are
often the questions that drive people to
lexical resources or you know let's all
be honest to Google in the first place
and so the number of people who just
automatically capitalized everything
they put in a search box
even with autocorrect turned off is not
trivial I like to call these people
crypto Germans they just have like a
very German thought process so okay
maybe you do have an exact match maybe
cat equals cat maybe they don't know
that dumpster is a trademark so they
don't capitalize it maybe they are
interested in the burning question as to
whether copyeditor is hyphenated or not
maybe they are avid readers of The New
Yorker and they want to know and so
maybe maybe they haven't maybe they
didn't grow up in a culture where they
were taught dictionary practices and so
they don't think that they need to look
up the simplest form of a verb and
instead they look up whatever
of the verb they happen to encounter so
these are all different ways that people
interact with this search box so what do
you get out of that box when you're
using elastic search so let's talk a
little quickly about how elastic search
works this is not actually a diagram
about how elastic search works I just
liked it but for the purposes of this
talk I don't really care about how
elastic search stores the data smarter
the people than me work on this they do
a great job I don't care about sharding
I don't care about distribution or
cluster management that guy cares about
it I'm pretty sure I'm also only talking
about elastic search 5.5 I know 5/6 is
out but I like to follow Alexander Pope
in my approach to software use which is
be not the first by which the newest
tried nor yet the last to lay the oldest
side and I'm also only talking about
English so let's get all those caveats
out of the way and so elastic search is
basically an inverted index if you have
ever used a book index then you've
probably used an inverted index so I
love this index I'm such a nerd that I
have favorite indexes you can say
indices if you want but this is English
we can use English plurals indexes this
is page 631 of the index to Hofstadter's
lata Waldoboro
there is no page 633 so he has indexed
the typos in the index yeah that's the
kind of stuff that dude does so in a
concordance if you've ever used a
concordance that's another kind of
inverted in index it's a list of all the
words used in a particular work this is
actually the background images in the
concordance of all the works of Tennyson
and so an inverted index is a list of
all the unique words that appear in any
document each word list of all the
documents in which it appears so the
inverted of index of elasticsearch is
not anything enormous ly different than
the kind of indexes that we've been
making non digitally for centuries so
this is where we get into trouble again
because we have to make sure that our
idea of what a unique word is an elastic
searches idea of what a unique word is
are the same but luckily elastic search
is very flexible and the way that you an
elastic search
come to a joint understanding about what
words art is through mapping and if you
think about it every kind of map is a
set of decisions about what's important
and what the user needs and an
elasticsearch map is the same way
so the decides with no pictures are the
actually important ones sorry so you can
set up a mapping for your document so
your mapping is applied to a type in
your index but I'm just going to hand
wave those away for a minute because
we're really gonna be talking mostly
about documents and so we can say here
you can see here that we have a mapping
that says okay we've got a property of
word and it's of type text and it has a
field a keyword which is a type keyword
and if those keywords are longer than
256 characters we're just going to
truncate them sorry you went on too long
we're done and so if you shove that
document there on the lower left which
is word e new e what's gonna happen to
that document well the field word of
type text is going to get analyzed but
the field keyword of type keyword is
gonna have an exact value so what do we
get when we shove that data in
elasticsearch and we haven't said
anything about how we want things
analyzed elasticsearch just shrugs and
says oh i think you're just gonna want
the standard analyzer you know standard
off-the-shelf and so we shove that
document in and then we make a query and
we say hey we want to match everywhere
where the word you know he is and so
we're gonna get that in our first result
yay but then we're also gonna get a
prime that doesn't look anything like UE
what is going on how many people now
want to know what a Nui means couple
okay it's the feeling of boredom and
lethargy that comes across you as you
endlessly refresh social media feeds so
it's a play on on me plus the e of
electronic yeah it's not great word but
for our purposes it does alright so
let's check the analysis so
elasticsearch has this great feature
you can say hey show me show me your
thought process behind what you just
showed me or how you're analyzing the
text that I've put in I'm just
anthropomorphizing elasticsearch all
over the place I really do think of it
as kind of a friendly librarian that
helps me do stuff but it has split that
string in our word field into two tokens
and one token is letter e and one is the
new e and the - who knows where the -
went and that shape that is why we got
to see a new e and E prime because the
token e matched in both cases so if when
you shove that date that document in it
which is called index time no analyzers
isn't specified it looks for an analyzer
in the settings of your index called
default if it doesn't find one of those
then it goes to elastic searches default
which is standard analyzer and that's
what you get this kind of output from so
what's an analyzer an analyzer is made
up of three parts 0 more character
filters exactly 1 there could be only
one tokenizer
and 0 more token filters and
tokenization splits a string into tokens
which is roughly analogous to words but
not exactly and the character filters
and the token filters help you normalize
the forms of those tokens so basically
given a token what do we think the most
natural most useful form of that token
should be so it's character filter a
character filter lets you add remove or
change characters kind of just what it
says on the box and there are some that
are built into elasticsearch they'll let
you strip out HTML you can do pattern
matching - can do things like convert
the ampersand and you can convert
characters that you don't want to lose
into something else using regular
expressions all character filters happen
before the string is split up into
tokens and you can have as many
character filters as you want and
they're applied in order as you set them
in your settings and tokenizer x' they
your text up into tokens and these
tokens may or may not conform to what
your expectation of a word should be as
we saw with anui it also tells you it
remembers and records the order the
position of each token and the offsets
of the original set of characters that
the token represents and again you can
only have one tokenizer no matter how
nicely you ask and then you have a token
filter which happens after the
tokenizing and it lets you add it remove
or change tokens so once things are
split up then you can shuffle them you
can shuffle them individually but you
can't actually change their positions or
the character offsets so when you've got
a standard analyzer you get the standard
tokenizer you also get the standard
token filter which does absolutely
nothing it's a placeholder in case they
need it in the future you get a
lowercase token filter so once your
string is split up into parts it just
makes them all lowercase and then you
get a stop token filter which is
disabled by default so stop words are
very common words in your language in
English they tend to be words like the
or and or but and you might not want to
include them in your token string for
mathematical reasons which may become
important later so that's what goes into
the standard analyzer the standard
tokenizer follows the Unicode text
segmentation algorithm which I'm sure
you've all picked up his light bedtime
reading and it has one property which is
your max length which means ok the
longest this token can be is is 255 and
so now we know a little bit more about
what was going on with the standard
analyzer so because it uses the Unicode
text segmentation it's going to break it
that - and it really doesn't do very
much else when you've got the standard
but because analyzers are made up of a
bunch of highly configurable parts you
can tweak your analysis to get the
results you need which is really fun
because as we can see here are some more
examples where the standard tokenizer
might have a different opinion about how
to make things tokens than you do secure
a bunch of things that people may or may
not look up they're all real I promise
you
an apostrophe went missing this swoons
here should have an initial apostrophe
but this is how the standard analyzer
will break these up so in the title the
talk I talked a little bit about
precision but honestly I'm really mostly
going to be talking about recall from
here on out so if you want precision
especially when you're dealing with
dictionary texts one cool thing that you
can do is remember that your keyword
field is never going to be analyzed it's
always going to be exact so if you have
a lot of control over your data like for
example you have about a million
dictionary entries and you know what the
possible variant forms of all those
dictionary entries are it's probably
better for you to add all those variants
to each dictionary entry as a
pre-processing step and do it ahead of
time and then do exact matches so first
it will look for the head word the bold
word at the beginning of the entry if
you were looking at in a print book and
then it can also check a bunch of other
fields that are all of type keyword and
get an exact match that will give you
something way more precise if you're a
user because what you're looking for is
a very precise piece of text a
dictionary entry that is associated with
one or more forms of a word so if all
you cared about this precision we're
done here you can go to another talk but
if you want to learn more about recall
we can stay here so when you're thinking
about recall I find it helpful to think
about having cascading options
so yeah let's first look for all the
data that we have that matches the
capital D dumpster and if we don't have
a lot of that or if we want to show a
variety of forms of how this word is
used let's go and see if we can find
lowercase dumpster plural dumpsters or
even dumpster diving let's give all the
possible - ization and pluralization
options for copy editor let's give a oh
you with and without the dots
let's ask you fold and get rid of
diacritical marks and for people who are
too lazy to type you know the diet
marks a resume which you know to be
frank is pretty much everybody at this
point so let's not punish people for
being curious let's try and show them a
good variety and array of data and
remember the crypto Germans people are
just gonna randomly capitalize stuff and
the way to get around this or to have
this work great in elasticsearch is
multi fields so as we saw earlier in our
first mapping we had the word field of
type text and we didn't specify in the
wit and analyzer in that mapping but we
could say that same text that same
string that we have in our document
let's analyze it in a whole bunch of
different ways let's let's change up our
whitespace analyzer let's use something
called shingles so that we can see if we
can match phrases better and let's
lowercase things and use stemming where
we will try and reduce possessives to
the singular form and forms of the verb
to the dictionary form of the verb and
when you add multi fields that means
that you can try and catch as many
matches for your data and your query as
possible so we'll see a little bit about
how this looks so if you were going to
write a dictionary analyzer and I'm sure
you're all raring to do that because
it's fun you might use a whitespace
tokenizer which is a built-in one which
is a little less breaky than the
standard tokenizer breaks at fewer
points and then you could add a bunch of
character filters like let's strip out
HTML let's change all of our quotes to
straight quotes let's take out some
punctuation let's get rid of single
quotes when they're surrounding a word
but not when there aren't contractions
let's take out punctuation at the end of
a line so that it doesn't show up in the
last token of the string now let's get
rid of random new lines so let's see how
this works so with the whitespace
tokenizer you'll see it doesn't break at
that - so copyeditors stay cool with
there - it doesn't break up things like
ctrl Z and because it doesn't have
anything to do with HTML it won't strip
out your HTML tags HTML tags are not the
profit
of the tokenizer but they are what goes
into the HTML strip character filter
which is built into elasticsearch and
you can use escape tags to say oh you
know what I want to keep the blink tag
everywhere I see it
let's not strip that out you can use a
mapping filter as a character filter and
make your own custom mappings so here
we're going to change every possible
quote 8 quotation mark and apostrophe
variant to just two so that we can make
life easier for ourselves down the road
so instead of having to figure out how
we're gonna strip all of those
extraneous single quotes we've collapsed
it down to one and then we'll only have
to replace that one and this is how we
do it so this is a pattern replace
filter otherwise known as a regular
expression filter so I am legally
obligated to give you the regular
expressions warning and in fact I need
to give you a double warning because it
took me an embarrassingly long amount of
time to know that the regular
expressions in elasticsearch are Java
regular expressions and not Perl regular
expressions and I've always written Perl
regular expressions you know you know
they were good enough for Jesus they're
good enough for me and so anyway so that
gave me a couple of weird bugs but yes
please understand that if you're using
regular expressions please do so
responsibly and not under the influence
of any mind-altering substances and
remember that an elasticsearch their
java regular expressions and there is a
really nice page that will show you the
differences between Perl and Java
regular expressions if you're fuzzy as I
was so we'll use more regular
expressions here this is going to strip
out the punctuation that's left after we
strip out all the other punctuation that
that this is gonna strip out periods at
the end of a line because remember we
don't want to strip out periods in the
middle of tokens because then we're
going to mess up things like IOU and MD
and other abbreviations that use periods
and then we'll also just take out new
lines and then this is a kind of grab
bag filter that's going to get rid of
all the other punctuation that we don't
think is a sale
feature of the words that we want to
look up so if this punctuation is in a
string of ADC characters it very rarely
has a word like meaning unlike the - so
that's a lot of character filters but
they all go into our dictionary analyzer
and so once we have all those character
filters which remember apply in order
then we'll go to tokenizing and then
we'll get our output and so there are
lots of other analyzers that are useful
and another one is the stemmer analyzer
and that will use the standard tokenizer
because we want to get rid of those
hyphens in this case it'll strip out
HTML it'll simplify the quotes and then
we're gonna have a bunch of token
filters that happen after the string is
split into tokens so we're gonna
lowercase everything and lower casing is
often a prerequisite for using a stemmer
filter that's built into elasticsearch
make it lowercase first the English
possessive stemmer just takes off the
apostrophe s on possessives the light
english stemmer does the bare minimum to
take forms of the verb and turn them
into simpler form so it kind of gets rid
of ings and edie's but it's not as
aggressive as some other summers if you
would like to know more about stem errs
a lot more there is so much more to know
about stem errs there are lots of
arguments online about them they're very
entertaining I would like to know more
about stuffers myself so like email me
or tweet me afterwards and like maybe
I'll make another talk about stammers
because it could be like a day and then
a ski folding just takes characters that
aren't in the basic a ski set like the
diacritical marks we saw earlier and
smooth them down into their basic ascii
representation if they have one so so
then when we run the stemmer analyzer
over a string like I hate skipping but I
love jumping the tokens that we get out
are all lowercase I hate and then
skipping turns and skip but on then I
lower case again but I love jumping
turns into jump and this will help us
make more matches down the road shinka
filters are really interesting
create like a kind of moving window
across your text that you set to a
minimum una maximum size and it outputs
tokens that fall in that window so for
example I want to catch a wave any ice
cream gives you these sets of moving
tokens though I want I want to because
we have a minimum shingle size of two so
the smallest number of tokens that can
go into this are two and the biggest is
three and we have output you know grams
to false so a unigram is an Engram of
size one so that's just a fancy way of
saying single words
so we've already output single words and
some of our other analyzers so we don't
need to bother to output them here yeah
it's query time so we have analyzed a
bunch of different fields in a bunch of
different ways and now we're going to
run a multi match query of the type most
fields across our original word field
word shingles are lowercase stubbed
field and the white space field which is
the dictionary analyzer and so what I've
tried to do with this query is the use
case that I care about is when a user
looks up a word and wants to see a bunch
of example subjects is that may include
the exact word they search for or might
also include closed variants of the word
that they search for because if you're
trying to figure out how to use the word
copy editor you might want to know how
to use it in a sentence but not
necessarily care whether it's hyphenated
or not and this is especially true of
forms of the verb most people want to
use an inflected form of the verb more
than they want to use the base form and
in this query your search term will be
analyzed but the same analyzer as you
used for each field so it'll be analyzed
multiple times in each kind of analyzer
that makes sense
so if here we're looking for just make
it clear we're looking for ice cream the
phrase ice cream or and so our first
match hey that's pretty good
it matches ice cream it also matches
capital ice cream which is good because
you know we don't really make that much
of a distinction in English between
shouty case and non
Jodee case and then also it will get you
the more stilted hyphenated ice cream
there at the end so but how did this
actually happen well what happens is
it's trying to match the tokens that are
output by analyzing the search term with
the tokens that are output by analyzing
the text and if there is a match you get
points and the more points the higher it
ranks and that is a very hand wavy
description of a lot of math that goes
on under the covers but I will get to
the math in a minute so you can see that
you get more points for a closer match
without the hyphen matches more often
than with the - now if you're like okay
one of these fields I care about a lot
more than another field you can boost
that field so let's say that you decided
that for every example in your examples
database you think it's a good example
for one term say the term ice cream and
you're gonna save that in a keyword
field that's not analyzed in addition to
the text of the example sentence so you
can say hey if someone looks at ice
cream without a - an ice cream without a
- is in the term field that's an exact
match and that's worth way more to me
than the other matches that I might get
by jumping through more hoops of
analysis so you can boost that field so
that it weighs more in the final
calculations and so then you get stuff
like this so in this we're looking for
ice cream with a - so now we get the you
know the thing that no human being has
ever said this is obviously some kind of
alien transmission with the - first and
then you get the all-caps ice cream and
then you get the lowercase ice cream so
these results are somewhat less
explaining right I'm not sure why all
caps came before lowercase to my native
speaker intuition - a linguist into a
and it seems like okay lower case would
be closer so this is where you get into
the math and I'm gonna hand away of
about the math but elasticsearch doesn't
hand away about the math because you can
use the explained query parameter and it
will walk you through the math in its
glorious tedious detail so my suspicion
is that the term frequency in side the
text that was originally analyzed is
somehow subtly different for these
different sentences but I haven't dug
into it very much because usually it
doesn't matter all that much but for
your analysis it might be really
important so I want you to know that
this exists and also you can do all
sorts of fiddly stuff with these numbers
you can turn them off you can mess with
them you can add multipliers there's a
lot of cool stuff you can do if for
instance the math parts of elasticsearch
are way more to your liking in the
language parts of elasticsearch so when
you're tuning elasticsearch when you're
trying to figure out the right match of
analyzer for your data and analyzer for
your queries and how many different
kinds of ways you might want to analyze
your data so that it gives your user the
best chance of finding it because
there's no point in saving data that no
one's ever gonna find or use what that
really involves is first you have to
know what your users are looking for and
what they care about if you don't
understand what they want it's really
hard to give it to them and also how are
they gonna look for it are they gonna
all-caps type no matter what you do are
they going to add extraneous search
terms because they think it's going to
make things more precise when if you've
got it set up in a different way it will
actually make things harder to find by
adding noise into your search then you
have to figure out how to change what
you've got to what they want
maybe you should pre process your data
and add more exact search fields or
maybe you should
laughs fields in your data if you have a
separate first name and last name field
but everybody who uses your phone number
lookup looks for first name last name
you might want to collapse those into
one field and then you have to have a
willingness to make trade offs so I
could tweak the punctuation splitter we
saw some examples like ctrl Z and the
name of the language that begins with a
glottal stop that's represented by a
exclamation point it's really hard to
say there very few words in the English
language that actually do have a plus
sign in the middle of them and there are
many more cases in the data where people
are just putting plus signs in for
decoration and it might be better for my
data to take out all those plus signs
and you know just kind of fiddle just
say oh well the people who are looking
up control Z they're just gonna get a
lot of examples where there may or may
not be a plus side and it's not gonna
get them an exact match but they're
probably gonna be ok with it so you have
a willingness to make trade-offs because
no analyzer is gonna be perfect for your
use case and then you really have to
enjoy fiddling around with this if you
have sat through this and you're like oh
my goodness this would lead me to
self-harm then perhaps elasticsearch is
not the tool for you you have to enjoy
spending time with the Cabana console
and playing around with different
analyzers and you know going down the
the dark and twisted paths of regular
expressions and really enjoy it I really
enjoy it it's a lot of fun for me and I
think that once you have the basics of
analyzers under your belt that you will
also have fun trying to mind-read your
users figure out what they want check
their old search logs you don't even
have to mind-read in some cases and tune
your data so that it fulfills their
needs so this has kind of been a little
bit I think of a basic introduction
especially for the guy from elastic
sorry to keep calling you out but this
is too good not to do but there are
plenty more resources about elastic
search and you should definitely seek
them out here is a photographable slide
of these resources the elastic online
dots aren't amazing they are very good
they are highly
detailed and they are clear and
easy-to-read elasticsearch the
definitive guide is a bit out of date
but it's still very useful especially
talking about the underlying concepts
and you know even though I said I didn't
care about sharding and all that stuff
the chapters on that are actually really
good too there's a book out from Manning
called relevant search which goes into
far more depth about tuning your data
and creating analyzers and all sorts of
cool things you can do with the math for
the underlying of tf-idf frequencies if
you really don't think you're gonna be
able to sleep tonight without
understanding Unicode text segmentation
there's a link for that a link to
explain the differences between Perl and
Java regular expressions and if you want
to know why linguist gets so vocal about
search engines you should read the
papers about the linguist search engine
and understand how much modern search
engines just drive linguist crazy
because they take out all the stuff we
care about you can't really kiss you
can't really search for hyphens um it
just shoves all the text down to like
the lowest common denominator and you
can't look for linguistic variation at
all but thank goodness we have twitter
twitter doesn't do anything we can do
all the sorts of stuff for Twitter um so
if you want to know about what how
linguist think about search engines
there's a lot of great documentation on
that it was a project out of Michigan
very interesting and if you have
questions or better yet if you have
answers please find me anytime let's
talk about elasticsearch I love to talk
about elasticsearch please also find me
to get your semicolon appreciation
sticker I have a lot of them to give
away you can appreciate the semicolon
from whatever distance you deem
appropriate
if you appreciate semicolons when
they're far away from you and not in
your code I'm totally okay with that if
you appreciate semicolons more when
they're in your code but not in your
prose also a-ok just appreciate the
semicolon however you like and also you
can there's my twitter handle the one
forward nick the one forward to KPI but
I especially want to thank all the
people the lovely people of Flickr who
make their images available through the
Creative Commons
this whole presentation is cc-by and csa
if you would like a copy of these slides
so that you can give a completely
different talk it about with all these
slides in the same order actually you
can change up the order because it's not
ND you know i'm happy to share the
slides with you just uh send me a
message and by ever whatever means you
can find me although not Facebook please
I don't really friend people on Facebook
so thank you very much I really
appreciate your kind attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>