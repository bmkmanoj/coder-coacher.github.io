<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Leveraging DSLs (Almost) for Free&quot; by Alvin Cheung | Coder Coacher - Coaching Coders</title><meta content="&quot;Leveraging DSLs (Almost) for Free&quot; by Alvin Cheung - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Leveraging DSLs (Almost) for Free&quot; by Alvin Cheung</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/shDoux5Dcyk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi my name is Alvin and I'm an assistant
professor in computer science and
engineering at the University of
Washington and today I would like to
tell you a story about how you can allow
your existing applications to leverage
new domain-specific languages without
the need to pull your hair out and so we
live in interesting times right indeed
we have a whole bunch of domain-specific
languages or dsls and they have kind of
permeated permeated across all these
different domains so from classical ones
such as parallel computing and
programming hardware all the way to
newer domains such as image processing
and even better analytics dsls have
helped developers to easily express
their computation a new trend however
has been developed where DSL is also
embedded within them optimization and by
that I mean the following if you can
express your program or your computation
using the DSL then it's guaranteed to be
you know execute efficiently consume for
your resources fewer energy whatever or
whatever that the DSL is designed to
provide now however face against all
these DSL are a bunch of existing
application or legacy applications so
these applications tend to be written in
a general-purpose programming language
or DSL that were designed in the past
decade or so and so forth now at this
point in time the million-dollar
question this room should be how can we
leverage how I'm Cal can we enable these
applications to automatically leverage
these new shiny dsls well so of course
one we can think about doing this is you
know we can always just rewrite the
existing program into using these new
dsls that are like new build on top of
new frameworks and whatsoever but now
let me tell you a little bit about how
these rewrites actually happen so I'm
sure we all have this experience where
you know our tag League or manager
basically give us a piece of code that
has been heavily optimized over the past
decade or so by multiple different teams
right so in theory these programs should
look really clean and modular and
everything now I see a bunch of smirking
faces like in the and in the front and
of course you're right in reality they
look more like this right so yeah we're
done
by our manager by this you know this
piece of spaghetti code and we're
basically told to pls rewrite using this
latest and greatest framework that we
just found online so at this point our
job is of course try figure out you know
what in the world is going on here
what is going on right now after banging
your head against the wall from your
days and perhaps months you might figure
out that you know this tiny piece of
code there it's just doing a simple
two-stage image processing pipeline this
is great right let's accept that on the
other hand you know the details about
how to implement caching and how the
implement pipelining is probably not of
concern for us because we are going to
probably rewrite them using you know
whatever CUDA provides as an example
right that's what we're trying to
achieve and likewise you might also
realize that this tiny piece of code
over there is just doing your data
partitioning followed by a relational
joint on two relations and you know
light and likewise the details about how
the hash table and hash functions are
implemented is perhaps not of interest
to us because we're just going to
replace that with the new shiny things
that are being provided by Hadoop or
spark or whatever new their framework
that you'd like so I can distill this
process into two steps so the first
first thing that you need to do is to
basically try to understand what is the
algorithmic detail that is happening in
these existing applications by that I
mean some sort of letting your
high-level specification right because
these are the functionalities that we
want to be able to retain across these
different rewrites and at the same time
they're a bunch of what we call
architecture specific optimizations or
linear framework specific optimizations
that we can probably live without
because we're going to basically scratch
those and then we target the code into
this new DSL that we are interested in
now wouldn't it be nice if there's a way
to automatically do this kind of reverse
engineering and we automatically
automatically do this kind of
retargeting to new dsls completely
automatically done for us as opposed to
banging our heads against the wall right
so at this point you might think hmm
that's all right I would just kind of
you know bite the bullet and then I'll
just do this whole rewriting you know
myself where were your team's assistant
right so you would think by doing this
once right then we should be done ah-ha
no why because what you consider as
new shiny ESL programs of today will
quickly become obsolete within months
right with when newt ESL's come in so at
this point you kinda like living in this
kind of like neo reincarnation hell
right because you have to keep doing
this over time and it's not fun is
tedious so and so forth and we don't
actually need to look too too far back
into history to figure out like you know
what these rewrites have actually turned
into right so i give you a bunch of
examples here they all involve multi bow
multiple years you know couple of
thousands of developer men year so and
so forth right
all these are major engineering efforts
so our vision is that we think that all
these rewriting should be done
automatically like we shouldn't be able
to precision needs to bang our heads
against the wall to figure out what is
going on and all these retargeting
should be automatically done by machines
now for now I'm going to focus
exclusively on rewriting you know
performance critical code fragments at
this point both because this is a new
project that we are starting so
therefore it's going to take us a few
years to achieve the ultimate goal and
also because by focusing on performance
critical stuff this is you know usually
complex code edit you know let me show
you guys a flashy demo and also get a
talking slot that strange loop okay
alright so you know option two right so
let's say I don't okay fine I I agree
review right so you know I'm not going
to do this do this whole thing and
manually why don't we just write a
compiler right after what that's what
these things are for we can write a
compiler that would take the existing
program and then translate it into
whatever DSL that we like so many
compilers are actually built using
something called syntax driven
compilation well in other words is just
a fancy term for basically pattern
recognition right so we write pattern
recognizers for example in this case I
can write a rule that we recognize these
kind of stylized for loops and being
able to convert them into a map
expression that you see here here on the
right all nice and clean very well but
what about if we have real world looking
code that looks like this right how do
we how many rules do we have to write
now I mean you can always keep writing
more rules and hopefully you know one of
them will kick and then you know our job
is done
but then when the next piece of code
come in then what so as a case in point
back in 2014 there was a paper published
and also a system for doing these kind
of transformation from sequential java
into Hadoop and spark very nice they use
this a rule-based technique and here is
the obligatory word count example that I
have to show you so you're in this case
you know on the left here we have like a
double for loop that is basically doing
word count and in this system in
particular does it by applying different
rules so we apply a bunch of rules here
don't worry there's no quiz afterwards
I'm not going to test you on this and
then you think you're done but
unfortunately we still have quite a few
rules to go and even more rules okay all
right come on okay finally at the end we
get our shiny MapReduce program doing
work town all right yay now don't get me
wrong here right so this is actually
fantastic technology and the guys behind
this as part of it's basically one of
the forefront researchers in programming
languages research now the problem here
is basically how many rules were I get
how do we develop these rules is
basically hinting at the fundamental
problem where this rule-based approach
is basically brittle right I mean so how
many rules do we have to write and when
do we know that we're done and these
rules are difficult to be proven to be
correct because these rules are
developed by humans and humans make
mistakes and they're also difficult to
maintain and you don't believe me I
would like to see your a facial
expression expression when you're
tackling gives you 100 of these routes
to maintain from now on right so we can
turn our heads around and say okay so is
there somewhere else is there something
else that we can do and we decided to
look at the problem as search you give
me a piece of input code we just search
over all possible programs so you can
write in the target PS out that we are
going to rewrite insert and then we
basically see if any one of them is
semantically equivalent to the input and
by the way since we're doing search why
don't we also search for a proof of the
chant of the translation so we both get
to target everyone and also a
justification is it that nice well this
all but at this point some of you might
also papers basically say that well this
is not going to work because if your
target language is something as big as
si
or Java Python you know the space of
possible programs it can break is really
really really big right if not infinite
however if we actually target dsls
instead we might actually have a chance
why because by being domain-specific the
language is likely going is likely going
to be smaller meaning that the space of
programs that we need to search is much
smaller than the actual your full-blown
general-purpose programming language and
the technical term for doing this is not
search is program synthesis so let me
just give you a one slide
comparison between traditional program
compilation and what we call synthesis
so suppose you want to optimize this
expression that adds for X for x you can
do it the old way using rules right so
in this case we start with the original
expression that we wanted to change and
then we keep writing rules that would
basically turn the crank until we reach
one that we like for example in this
case because it is the most efficient
one yeah
we've synthesis however the way that we
do that is we specify a grammar that
describes the space of programs that we
want to search and then we just go one
by one we just enumerate every one of
them and try see if any one of them is
actually semantically equivalent to the
original now you can do it in multiple
ways right so you can do this by using
unit testing or if you want to be fancy
you can run a model checker even a
theorem prover to check that they are
indeed semantically equivalent to each
other so that's the approach that we
take for this particular problem we are
given input source code and we're trying
to generate or retarget part of it into
a target DSL of our interest now as I
said right so we could search over to
see of possible programs that we can
that we can express using these dsls but
we can be even smarter by not searching
in that concrete syntax so how about we
first lift the semantics of these of
this DSL into a high level specification
language the nice way about the nice
thing about this is the fact that we now
not only basically reduce the search
space even further
we're not even searching the concrete
syntax of the target DSL we're just
searching over the semantics of the
target language in a sense
basically another way of making the
search space go even smaller so we're
gonna perform search in this high-level
specification language they will tell
you in just a second and in that sense
all that we are doing is basically given
an input piece of code we're going to
somehow infer a specification that is
expressed using this high-level language
that I'm going to tell you next and the
same thing also happens right so for
every candidate that we can write in
this specification language we're also
going to go through this validation
phase we don't want things to break so
the validation is go either go and say
that yes you got it this is semantically
equivalent on No please come please come
back and give me something else and if
all goes well we now have a shiny
candidate Express using this high-level
language and then we can do cogeneration
to get back the original concrete syntax
that we want and I claim that doing this
final code generation phase here is much
easier because we already have the
semantics of the language and
translating between the high-level
semantics and the low-level concrete
syntax is going to be much simpler than
doing it from this side to that so
that's I completely so we call this
approach verified lifting is lifting
because we are converting from the
target DSL into a high level language
and performing the search there instead
of the concrete syntax and it's verified
because we are going through this
validation phase that make sure that
anything that we can that we that we get
at the end of the pipeline is indeed
semantically equivalent so to apply this
technology right now we are building a
new framework called meta lifts to
enable other people to leverage this our
our technology to be able to convert
their programs into their own DSL so
man-lift has two components first of all
it has a specification language that
allow people to express disadvantage of
their dsls and it's also a compiler
generator so in the specification
language side we support the kind of neo
basic syntax that you can imagine for
example you can express things about
boolean expressions of arithmetic and we
also support a bunch of different data
structures as you can imagine and they
come in kind of like a functional form
that looks exactly like Haskell because
all we all that we are doing at this
point is just to specify the semantics
of the language we don't care about the
concrete syntax right now on the
compiler generator side the way that it
works is the following
we take input programs and we first go
through an extraction phase we try to
extract out code fragments that are
amenable to be retargeted into the new
framework or the new DSL that we are
trying to do and for each of those code
fragments we now do this search over the
high-level specification language and
right now our system actually is hooked
up to a theorem prover to try to do this
validation phase although we do have
plans about extending this to just run
unit testing for example and as I said
earlier if all goes well we can now go
through code generation where we would
get you know the target the target code
fragment being expressed using these
dsls of our interest so how can an end
user actually juice our compiler
generator to generate a compiler for
their own DSL well you basically give
three different inputs to our system
first of all you need to tell us what
kind of code fragments are you
interested in in converting are these
loops are these recursive functions what
is this DSL that you are trying to
convert to it's actually good for and
given that the next thing you need to
give us is the semantics of the DSL that
you are trying to lift to write what are
the operators what are the program
constructs optionally you can also tell
us what is the search space that you
want to search over to see of possible
programs at this DSL and finally it just
tells us how to actually generate the
concrete syntax from the DSL that you
have just defined using our semantic
language and then you're done let me
give you an example so suppose I want to
be able to generate SQL which is the
language for databases I assume most of
you have probably familiar with that if
not hate write and let's say I won't be
able to convert Java programs into SQL
okay so the way that I use this pipeline
is the following so I first define the
semantics of SQL which is basically just
a bunch of operators on relations really
using this syntax right so for example
in this case I'm defining the Select
operator and then the join operator
those of you who are familiar with
Haskell can basically see that this is
just a Haskell way of saying things
right
so given this synth semantics of this of
these operators
I'm going to tell the framework that I'm
targeting loops like just try them right
now maybe we can always fail and say
that okay I can't find you a valid
translation but might be we might be
able
seed right okay and then sit for the
actual cogeneration rule we basically
just define one specific cogeneration
rule for every one of these operators
that we have defined earlier so you can
see in this case for example I'm just
saying that you know the concrete syntax
of a select is select star from blah
blah blah right and you can see that
defining these rules is much simpler
because we are the ones who first define
the semantics of this language so we're
basically just telling the system for
each of these operators that we have
defined here is the here is the way to
generate actual executable code so at
this point you might ask have we
actually try doing this right does this
actually work well as a matter of fact
we actually do this we actually try
doing this by converting Java programs
into exactly SQL so as I said earlier we
define the operator defined the
relational algebra operators using our
high-level specification language that
you have seen before and now we just
take input code this is code that we
found online right in some public
publicly available application and then
we just tell the system please try to
extract loops from this thing and see if
you can convert any of them into SQL
statements and sure enough some of them
actually were convertible into SQL and
why are we doing this well because by
converting them into SQL we can actually
leverage the optimizer embedded within
databases and get multiple orders of
magnitude performance improvement again
right so this is kind of like where what
why I'm saying in the beginning the new
trends in DSL is kind of like embedding
optimization within it right so now if
by being able to express the computation
or reexpress the computation into the
target DSL we basically leverage the
benefits that the DSL runtime is able to
provide us in this case that the SL
turns out to be SQL so we have also done
this on other domains as well so you
know for example we have converted
Fortran code into a language called
halide which is a DSL for doing image
analytics on the GPU so in this case the
operators that we need to define is just
get and store on a race and you know
with that we do the same thing again
right so we extract loop echoes from the
input program we try to search over
possible programs that you can write
using get and store
if we secede we able to then generate
halide code by doing this code
generation process and in this case by
moving computations to the GPU we once
again able to get speed-up that you
might expect if you were to done this
completely manually for example and the
third instantiation instantiation of
this is we now actually generate
hardware as well
you can abstract away hardware as a DSL
I mean after all they have an ISA and
ISA you can describe the semantics of it
using our language so in this case this
this program post which has this funky
is a that consists of pairwise execution
of two things at the same time as a MUX
and so and so forth so if you express
the semantics of the hardware in this
case using our language and now you can
take a program that looks with a see
like looking syntax and actually
generate executable hardware that you
can now deploy on a programmable switch
cool and of course just to cut round up
the story we have also done this by
converting sequential Java into Hadoop
exactly the example that I'm showing you
earlier in this case the DSL called a
tube is very simple because if you have
get as you might have guessed it only
has two operators of map and reduce so
we've got just so you define the
semantics of this operator using our our
language and now we basically just go
out and look for loops that might be
amenable to translate it into Hadoop and
sure enough we find something that we
can convert and then now we can leverage
the optimizers that is execute that is
provided by a tube or spark to get speed
up so now I'm going to switch gears and
talk a little bit more exclusively about
this new the last instantiation called
Casper which is this compiler we
generated using verify lifting and meta
lifts for converting sequential Java
into Hadoop and of course you guys are
probably not believing me so I'm just
going to show you a demo of how this
actually works and by the way this is
completely over an open source right now
so you can actually go try it out if
you're interested so you can go to this
website I don't I don't know you can see
it right now is a Casper you dub pls yo
RG
so we have released this earlier and
then you can actually try the online
demo
in this case this is the word count
program that everybody just loves right
and because we just got to have it so
I'm just going to click translate and
then see what happens so what happens in
the in the meantime is this basically
going through this you know conversion
procedure right and trying to do the
searching over the space of possible
program so you can express using this
DSL using this high-level specification
that we have provided now in the
interest of time I'm not going to wait
for this to finish I'm actually going to
go back to my slides and we'll see what
happens towards the end but for now you
can trust me that it's actually running
in the background or actually on the
website going in the background maybe
both ok ok so how this Casper worked
right so again I'm showing you this is
kind of architecture diagram of our
compiler to chain which again consists
of three parts right so we first try to
in this case take in Java programs and
we try to extract right now in this case
loops loopy programs and we try and see
if we can convert any of them into a
tube or spark and as I've also told you
guys earlier you basically to give us
you know three different inputs and an
optional fourth one for this particular
instantiation you know the tire code the
target code fragments as I've said are
basically just loops what about the DSL
and what about the code generation rules
right from this DSL well in this case
since you know we're just try and
convert to Hadoop as I've said earlier
we just need to define two new operators
using our specification language namely
map and reduce and for code generation
is relatively simple as well because we
just basically take each of these
operators that we have defined and then
define the actual concrete syntax in
this case in Java that you can now run
your MapReduce job over now the missing
piece of the puzzle here with is the
following so map and reduce take in the
function right the reduce in the mapper
basically need to take a function here F
where does that come from we have
defined the semantics of our operators
but we have not defined you know these
are kind of like code fragments specific
every input code is going to require
different mapper and the different
reducer function right so where are we
going to get those well that's where the
search comes into play so we're now
going to ask this program synthesizer or
the search engine
so now search for the see of program so
you can express that you can plug in
into the mapper and reducer and know to
achieve what we wanted to do as a as
semantically equivalent to the original
input so for that purpose we define a
search based grammar which is optional
you do not have to provide it to matter
lift but if you want the search speed
going even more efficiently you can do
so so for example we can first define
the mapper functions to consist of only
no additions and subtractions and maybe
a little row and so and so forth and
just try it and see if you can find a
matching function if our synthesizer
ended up failing we can't find any
candidate that would pass all the test
cases well we can expand the grammar for
example by including division or
multiplication and what and another kind
of conditional constructs right the nice
thing about this is that all these
grammars can be generated on a per code
fragment basis for every input code
fragment that we have you can choose to
perform your lightweight program
analysis to configure you know what kind
of operators that is used writing your
workout program constructs out there and
just toss into the file and see if the
search engine can actually find
something useful and if not you can
always go back and express your grammar
in a slightly different way make it more
expressive such that you know two job
would go through okay so given this
grammar and given the input code how do
we actually do the search right so now
we have two input code we've someone
told us what the grammar is that grammar
might be programmatically generate as I
said earlier so how do we actually do
the search well in Casper we have a
multi-stage search process just to make
this whole process go efficiently first
there's a synthesis coordinator so the
job of the coordinator is to take the
input code and the search space Graham
and to generate a search based grammar
for that particular input code fragment
so given these two input code given
these two different inputs we're now
going to start in numerating different
start to enumerate different candidates
in this search space of programs again
in this search space we only have to
consider like you know what are the map
mapper functions that we can put in what
are the reducer functions that we can
put in right and for every candidate
that the search engine returns we're now
going to go through your first app and
the model checker process and in a
theorem proving procedure you can
basically
think of that estes validation so if all
goes well we have our shiny new you know
code fragment we expressed using
MapReduce and then we can go back and
tell the code generation to actually
generate the concrete syntax or if it
failed then we'll just reiterate the
same process again until either we ran
out of programs to search or we just
timed out either way this is how we do
search within Casper and then for more
details please come talk to me
afterwards so since we have already open
sources this compiler have we actually
evaluated it and the answer is yes so we
go online and we collected five
different open open source benchmark
Suites so these range from typical
MapReduce programs I can imagine writing
you know all the way to you know image
processing frameworks written in Java
but it's sequentially so out of those
sixty benchmarks that we were able to
collect we're able to translate 50 of
them there a bunch of those that we were
not able to translate for example they
have they call some external library
methods for which we have no
understanding about the semantics off
okay and there are also a bunch of
programs that uses program constructs
that we currently do not handle in a
high level of specification language but
these are more engineering that we think
with more engineering we think we should
be able to handle those benchmarks as
well now the second question that you
might ask is okay you can translate code
fragments but do they actually perform
well right because after all that is the
goal of doing all these translation
right it's not just for the sake of
translating but it's actually trying to
get some benefits out of it as compared
to the original so here's a plot that
shows the relative speed up against the
original sequential implementation of a
bunch of benchmarks that we have chosen
okay so you can see that across the
board we kind of got quite good
performance improvement as you might
expect
now again right so we are not the one
who is providing the efficient
implementation here it's basically due
to spark and map and and Hadoop right
you know what we enable what we enable
here is by just rewriting or retargeting
your original computation using these
new DSL so you can now leverage the
performance benefit provided by these
dsls without banging your head against
the wall now the gray mark the but gray
bars I have shown you here is just a
bunch of examples that we were able to
using the previous compiler there
alluded to earlier the one that is based
on rule-based approach we're not able to
evaluate against these other benchmarks
because that tool is not open source so
we cannot compare against them but for
the ones that we can all translate we
get you know close to 20 X speed-up by
rewriting into spark and then up to 31 X
speed-up for doing so now and just to
torture ourselves we also hired a bunch
of spark programmers these are expert
programmers with multiple years of
experience and asked them can you please
rewrite these benchmarks from sequential
Java into spark right and here is the
difference in terms of comparing the
performance so we can see that across
the board we are kind of on par you know
and similarly have different in similar
implementations that would have that is
that would have been written by a expert
spark programmer one particular case
that one point out is this particular
one through the histogram and the reason
why I will automatically generated
implementation the green bar here does
not perform as well as the expert is
because they were able to recognize that
there's a special data structure that
only spark provide and not her tube and
they leverage that and then they got you
know more speed-up than what we can
achieve because we're targeting both
Hadoop and spark so therefore was not
able to synthesize that particular
solution now another question that you
might ask is okay so it's this approach
you know practical right because you
know you can do this but do I have to
wait 10 years before getting anything
out right because if that's the case I
might as well just write it by hand
myself right well so turns out that's
the mean times of compilation there's
only about five minutes so this is based
on just doing search and the median time
here is just you know less than a minute
really we also have a bunch of strategy
to not only prune the search space but
also only select those that are that we
know is going to be efficient in terms
of performance and I don't have the time
to talk about the details here but
essentially they're just a bunch of
heuristics that talk about the cost
modeling and you know and by being able
to incorporate in a cost model during
the search we're able to not only find
candidates that are semantically
equivalent to the original but also ones
that we know are going to perform well
and if you don't care about that then
you basically get your first solution
you know in less than a minute
and here's just another another thing
that shows like you know the kind of
number of solutions that we're able to
prove using our cost model and again if
you're interested please come talk to me
afterwards and I'll be happy to show you
how that works so you know we think that
this is this is kind of really great
right but where is all these all these
things going so I think that right now
we're kind of living in the you know in
a Stone Age of programming if you will
where the way that people do programming
is either kind of bang your head against
the wall and you know always a timeless
pray for it and looking at like new spec
Abaco and trying to reason about things
so we think that in the future we should
be able to train computers to do these
coding tasks for ourselves right we
don't we shouldn't be able to shouldn't
be doing these kind of things you know
on our own we should just rely on
computers to do this right especially
with advances in machine learning and
program synthesis for example so what
I've read it to you today is one of
these techniques right one of them you
know being able to do search across a
space of possible programs and see how
you can do compilers
how you can do program completion that
way and in fact let me just go back to
the demo that I promised I'll show you
guys earlier and hopefully it did not
fail let me see aha okay so you can see
that we have can you guys see in the
back there
oh okay so you can see if you go back to
the oops so to the original right so you
know to the original program so again we
have this word count example that
everybody just loves we send it over to
our to our tool Casper and then what is
doing in the beginning is basically
trying to search for possible
semantically equivalent expression in
Map Reduce
you can see that we try we try setting
in a grammar and then you know it failed
because we know we were not able to come
up with a solution so therefore we
increase the expressivity by generating
another grammar and you know once again
it failed right so we you know printing
out some error code and then we generate
another grammar and finally we were able
to find a solution in this space of like
in this space of programs and now given
that when basically going through all
these steps again and then you know
after he has finally being able to
verify it right now we actually get the
program 31 because now we just generate
the concrete syntax of the MapReduce
program and we are on our way so you can
see at the end that we just do a bunch
of stuff and then it passed all the unit
test cases that we have so therefore we
think that should be good okay okay so
this is the program that we got that
that was generated and if you don't
believe me you can go online right now
and you know take a look right so this
is programmatically generate a program
so I believe it's not intended to be
consumed by humans okay so you can see
that it basically has all the all the
elements right it's basically operating
over over rdds and it's basically
generating emits and reduce and so and
so forth in fact you can also run it so
we have different data sets that you can
use and then let's say I just want
generate a bar chat and then you can see
the two versions running at the same
time and they're both running were
cameras I said and then the optimized
version that we generated complete in
six seconds while the sequential one is
still running okay so it's kind of like
what do we expect right because this
original one is just doing this in a
sequential fashion and this whole
process took maybe a few minutes it was
kind of like just way in between when I
was first kick this demo started all the
way up until right now and then you know
it's basically done by the time that we
get to this right cool
and this website actually has like other
Auto demos that you can run and you can
even write your Co and see if we can
actually generate and an equivalent
implementation and how to do it for you
okay so as I said right so I think that
you know in the future these all these
tools should be available to us that
uses machine learning and your program
synthesis and whatnot to help us write
programs more efficiently and today what
I've introduced is this technique of
verified lifting which is a new way to
use program synthesis fancy term for
search to do program translation without
writing any rules in particular I can
introduce to you this new system or
framework that we are currently actively
developing called meta lifts that allows
people to generate compilers for the
target dsls of interest I've just shown
you this one example that we have open
source for for translating sequential
java into hadoop this is built using our
technology is built on top of method
lift and we have also built another
compiler also open source as well cause
stain that translates Fortran into GPU
and we have a website and you're more
than welcome to go to it and to get more
details and also get pointers to these
compilers that we have open sourced and
we'd love to talk to you about other use
cases where you see this might be
beneficial and finally of course I'm not
able to accomplish all this work without
a whole team of great students and also
collaborators and of course as usual
they get all the cool things that they
are done and all the errors I take full
credit for and if that thank you very
much for having me here today I'm now
open to take questions thank you
yes please great okay so the great
question so just to repeat the question
is can we actually crowdsource writing
these rules right so the answer is yes
of course right so you know we've in
fact all these rules right if you look
at the rule based compilers they were
indeed developed by humans so it is
indeed possible to outsource all these
rules and being able to and so this is
humans help to write them but what we
also pitching here is we can also try to
use the machines right to learn these
rules you can think of it that way so I
think the short answer is both of them
equal equally equally plausible
as much as you know convenience is
concerned we might actually prefer the
machine driven way right because at that
point we don't even need humans to begin
with but you know that just kind of like
a vision that we have in mind sorry yes
question great ok so the question is
what is the complexity of the search
indeed is exponential in fact if I
remember correctly is actually doubly
exponential so you know but that but the
great thing about this is by putting in
different grammars you can actually
control how the search is actually done
right so if you give a simple grammar
and you restrict it and a in if you were
checking more then that basically means
the search space is smaller so therefore
the search would be we were able to do
the search quicker even though it still
typhoon exponential yes but if the
exponent is small enough we might
actually be able to complete and
actually there has been a decade-long of
research into program synthesis just to
speed up this process so when I first
mentioned it I was saying like you're
we're going to basically in New Marais
every possible programs well of course
we're not gonna just enumerate flying Li
every possible programs right so that
bunch of different techniques that
people have try happy to talk to you
about that afterwards if you interested
yes question please
okay okay so the question is we I
mentioned that there are a bunch of Jews
cases that we're not able to translate
can you give a flavor of what they
actually look like sure so some of them
involve things like crazy side effects
such as like you know printing sayings
writing files so those are not
expressible using our language because
currently we don't have monads even
though it's a school thing like so you
know we don't have Mona so therefore we
can't model the side effects and there
are other things such as calling some
external library that we just don't know
what it's doing so therefore just to be
on the correctness side we can't
translate those programs just for that
for that particular purpose but I would
argue that we probably don't need a
full-blown language for people to be
able to express all these things because
you know after all we're trying to
translate that to a particular DSL and
if that DSL is not good for say doing
like you know five operations then why
bother mourning that modeling that into
the into the into our specification
language so yes question please
okay great question there the question
is instead of starting from source coke
we could we have also start from
bytecode or other kind of representation
right so do the same kind of process the
answer is yes although if we go with a
Biko or assembly approach then you know
in terms of specifying what kind of code
fragments people want to be able to
translate into the dsl that might become
slightly more clumsy because for example
if we sin if we do a source code level
we can still say things like you know a
loop or a sequence of two assignment
statements but then if we do this at the
Assembly level you know some of these
high-level information is already lost
so therefore it'd be more difficult for
the end user to specify what kind of
code fragment do they think would be
amenable for this kind of particular
transformation but in terms of
technology I don't see any particular
problem in terms of applying this to
even my code as well in the back yes
please okay so the question is what is
the largest specification fragment that
we able to synthesize so it depends on
date on the language of choice so in the
SQL case for example we can translate
code around maybe 200 lines or so
there's a source code level well but
that might not be a good measure of how
successful or not successful we are
because it also depends on how
complicated that piece of code is right
and as I said earlier we can't try to
target these really complicated code
fragments just because they're complex
and it basically is a very challenging
thing for us to do before I ask you our
case for liking our 200 line of code
fragment it's basically doing everything
with you know join an aggregate a
selection and also a projection we able
to find something like that within
minutes okay awesome questions anything
at house yes in the back please
okay that's an awesome suggestion so the
question is can we actually translate
back into the original language not to a
DSL right just for example we can do
normalization and maybe we're gonna
suggest allistic changes so and so forth
that's a great use case we've never
thought about it
happy to talk to you about that
afterwards all right great so the
question is what happens if the input
code actually has box within them right
so I mean if we convert it doesn't that
mean we are also kind of like you know
precipitating that's precipitating the
same bug over to the new version right
and the answer is yes indeed right
because we don't know you know whether
the inferred specification is indeed
correct right so therefore we're just
going to blindly convert that into the
target DSL if we can indeed find one but
for example to go back to example so the
question being raised we were saying
basically what about like know for
example off by run errors it loops right
so chances are and we have actually seen
that in practice for those kind of loops
will not be able to translate into the
target language for example if you think
about like SQL again right if you have
an off by one if off by one error for
reading like a relation then that
basically means that you're not really
doing like a projection or selection on
the entire relation which is not which
is what sequel can do right if you want
to do everything except for one
particular tuple I'm not sure whether
that is actually expressible in sequel
so that's just a maybe a short way of
answering your question but yeah I happy
to talk about that as well afterwards
okay quick
okay yes another awesome so the
suggestion it's like you know we can
just go in Stack Overflow and can't do
like stylistic judging right so we have
run a beauty patch and in terms of like
you know who's writing the most
beautiful code right in deeds I mean
that's that's one possible use case
right in fact some of the things that we
also have in mind are things like you
know patching security loopholes so for
example we can imagine we implement
being able to re-implement your existing
applicant code right using fewer api
calls or a less publicly available api
kpi then you can kind of thing of like
you know minimizing the attack surface
if you will right because my original
api might have ten different things that
you can call but then my new one only
has five the question is can read
leverage right this five you know a
smaller api and still be able to come to
express the computation in the original
code right you can see different
benefits that's way too cool so i'm
still around you want to talk to me
afterwards and i think in the interest
of times i just closed this session and
thanks for coming again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>