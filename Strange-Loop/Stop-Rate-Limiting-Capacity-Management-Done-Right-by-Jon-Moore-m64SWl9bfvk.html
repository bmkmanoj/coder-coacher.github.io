<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Stop Rate Limiting! Capacity Management Done Right&quot; by Jon Moore | Coder Coacher - Coaching Coders</title><meta content="&quot;Stop Rate Limiting! Capacity Management Done Right&quot; by Jon Moore - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Stop Rate Limiting! Capacity Management Done Right&quot; by Jon Moore</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/m64SWl9bfvk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right hi everybody we've got 40
minutes I've got too much content I've
drunk too much caffeine so there's gonna
be like a punk rock concert are you
ready for 40 blistering minutes of
queueing theory I said are you ready for
40 blistering minutes of queueing theory
all right so am I so let's say that
you're going to adopt a services
oriented architecture some of them might
even be micro but it all starts with one
application or service right you pull
out some functionality you get it ready
for production you get it set up ready
to take some traffic and sure enough you
know you start to gather some traffic to
your service and then you start building
more services and they start attracting
more traffic themselves and before you
know it you have hundreds of services
and thousands of clients and you have to
make sense of this somehow and this is
usually where you bring in something
called an API management gateway now
there are really two things in API
management gateway tends to do one of
them is authentication and authorization
right we want to make sure that the
people calling the services are actually
allowed to do that that's not what I'm
going to be talking about today because
the other thing that they do is they
provide capacity management right and
one of the things that we want to be
careful about is we don't want to let
one client use too many too much of the
capacity of service and starve everybody
else out that's that's sort of like this
method of dividing pie right which is
like you know here here's a slice for
you and I'm going to take the rest of
the pie so we don't want to have this
going on now the way that many API
management gateways on the way that we
think about managing access to AP is
they often use rate limiting okay so
this is the idea that I'm going to say
hey you're allowed to use say 25
requests per second to my service so
what I'm hopefully going to convince you
of today is that this is not the best
way to do it and in particular there's a
couple of places where this doesn't work
out so it so that so the first one is in
cases where the origin service has been
overloaded okay so as so it turns out
rate limits don't work very well there
and then the second
is that sometimes rate limits can be too
aggressive so sometimes we end up
rejecting requests that we could have
served otherwise okay so that's we're
gonna be talking about today and
everybody ready for the queueing theory
okay it is not going to look like this
so don't worry this is all of the
queueing theory that we're going to do
today okay is this simple equation n
equals x times R this is known as
littles law people who work with me know
that I'm a big fan of littles law in
fact some folks I worked with have
coined Moore's second law which says
that if John Moore has an opportunity to
apply littles law he will so but this is
super useful I've actually used littles
law to debug performance problems with
pencil and paper and just sort of asking
another engineer how a service worked so
so this is cool stuff okay so let's
let's go through what this is so n is
the average number of concurrent
requests so if we took a snapshot of the
service this is the number of requests
they're currently in flight being
processed by this request processing
system X is the average transaction rate
okay so this is the rate at which
requests are arriving at the system and
then R is the average response time okay
which is how long it takes on average to
process one request it's important to
note littles law applies to averages
okay so this is about capacity it's not
about performance we all know we want to
look at things other than average
latency and when we're trying to manage
performance but anyway to put this into
a context that the hopefully folks are
familiar with how many people have been
on an amusement park ride that either
there's like this where you've seen a
ride that's like this right okay lots of
folks okay so so put this in littles law
terms okay and is the number of people
on the ride okay so if I took a snapshot
of the ride how many people would
actually be on the ride X is the rate at
which people are arriving wanting to
ride the ride and then ours is how long
the ride lasts ok pretty straightforward
so now you know of course well some of
us may operate amusement park rides I'm
not sure but but when we're looking at
computer systems right the question is
okay so so where what is the equivalent
of this okay so what are the equivalent
of the the flying elephants here so so
there are lots of
places in our applications where you can
think of a request being resident while
it's being processed okay and and these
are the things that really represent
capacity in the system right these are
these are the flying elephants these are
where the requests are sitting while
they're on your ride if you want to
think about it
okay everybody with me so far all right
so now we're gonna we're gonna actually
look at a couple of demos in and get at
some of these things so just to get a
squared away we're gonna come over here
I've written a couple of applications so
we're gonna start with an Origin service
and we're gonna spin this up over here
on a particular port and now the way of
this origin service is going to work is
it's gonna have a fixed size thread pool
okay so I'm gonna say I'm gonna give it
say I'm gonna give it seven worker
threads and then basically when a
request arrives we're gonna hand it to
one of the worker threads the working
thread is actually just gonna sleep
because this is a demo and it doesn't
actually have to do anything so that's
nice
but we're gonna have it sleep on average
for one second okay and so there's a
little spring boot application we're
going to spin this guy up all right now
let's let's generate some load so so
let's come over here and I have another
application so we're gonna spin that up
on another port so so I have this this
is really just a listener and a port so
I can scrape metrics from it so I can
show you what's going on and a nice
dashboard here so we're gonna give this
a client ID and we're also going to give
this a number of worker threads that are
going to be generating load however this
load generator is going to be configured
with a rate limit okay so we're going to
send five requests per second so so I
have a bunch of workers but but I use it
in this case I'm using a token bucket to
make sure that in aggregate they're only
sending five requests per second here
okay so this is a rate limited client
all right so there's remember this is
what we normally do to clients and then
we need to actually send that load
somewhere so we're going to send it to
our origin server I'm not going to a lot
of the rest of these you're not gonna
have to watch me type command line
for lots of talk because we're gonna
just modify them as we go and then we're
gonna start the load generator all right
so we're gonna spin up here and now
let's come and look at a graph on my
dashboard here and so this is really
this point is not super interesting
because we're not actually going to see
any problems we only have one client one
origin but I want to just sort of orient
everybody into the Dashwood's we're
gonna be looking at alright so down here
what we're doing is we have the origin
throughput so that so the origin is just
counting how many how many successful
requests this and response is it's being
able to generate and we're just scraping
that over time it's also measuring how
long it takes to to actually generate
those responses right and so if you
remember we configure this to have a one
second work time for the responses it
it's actually randomized it's sort of
like one second plus or minus some
random amount but but it averages out to
a thousand or to a thousand milliseconds
or one second up here on the client side
I've got two ways of looking at the
throughput so so we've got a green bar
which are the the rates of the HTTP 200
so this is the success rate we're seeing
and sure enough that's right at five
requests per second the blue line at the
bottom that's labeled good bit true good
put trailing 15 seconds that's the same
thing but it's averaged over the last 15
seconds so it sort of gives a longer
average so you can see sort of the trend
without sort of the jagged ups and downs
here all right so this all looks pretty
good okay so now let's let's let's let's
have a problem
so suppose let's stop the the load here
for a second let's come back over to our
origin service so you know it turns out
origin services they you know they have
problems sometimes and so let's take a
look at what happens if oh I don't know
my latency gets doubled okay so so now
got is the same thing it's just going to
take longer there's lots of reasons this
could happen right I could have
accidentally introduced you know a
performance bug into the system I could
have it may actually legitimately be
doing more work because there's more
things to you or it may have a
synchronous dependency on some other
downstream service that that's making
itself slow all right so let's spin that
up
sorry there's one more thing that I have
to do here which is important so I'm
actually going to make this origin
server not very self protective okay so
now what's going to happen we've got our
worker thread pool I've set shedload to
false so what that means is if a request
comes in and there's not a worker thread
available I'm just going to put it on a
queue okay and then when a worker thread
becomes available then I'm going to pull
it off the queue and process it alright
so so we're gonna spin that up and we're
also going to spit up the seam load
right still five requests per second and
let's come back over to our dashboard
and let's see what happens see if you
can see you can think about what might
happen here so we got some graphs here
I've seen an origin latency graph over
here on the bottom right that doesn't
look super promising here we've got the
latency seems to be climbing over time
and now instead of sort of getting
successful responses I'm actually all my
requests are timing out right now the
interesting thing is the origin still
thinks it's processing things you know
successfully which in fact it is it's
just that there's big this big queue
building up in front of it okay so we've
actually gotten to the point where the
origin latency has gotten so high we're
getting zero successful requests so I'm
gonna I'm gonna stop the I'm going to
stop the load here for a second and so
we can see the client a traffic fell off
there and if we wait a little bit
eventually what's going to happen is
which we're seeing now is is the origin
actually runs out of queued stuff to
work on and and it finishes and we're
not measuring latency anymore so so
what's the so what just happened okay so
so anybody who's been to a popular
amusement park knows what happens if you
have more people showing up on the ride
then the ride can actually have in terms
of throughput right and what happens
right a queue forms especially if you're
in Britain where we actually call things
queues for this in America there's a
but anyway so so now let's let's take a
look at this from a queueing theory
point of view all right so so I promised
queueing theory so here we go all right
so now we've got our ride we've got a
line forming in front of the line or in
front of the ride and what littles law
says is this whole thing is a request
processing system right and so littles
law applies to the system as a whole but
and this is the really powerful thing
about littles law it also applies to
each of the subsystems
so that means littles law applies to the
queue and littles law also applies to
the ride okay and so this is how you can
start to apply these things to
understand what's going on all right I'm
gonna simplify my my photos away and I'm
gonna put this back in terms of the
origin service here all right so now the
origin server had seven worker threads
right that was what we set it up okay so
that means that when we look at where
requests on the system there can only be
seven currently working being worked on
by the worker worker pool we set the
average response time to two seconds
that was what we configured it to do and
so so now we get to do simple arithmetic
and so that means that this is
processing things at three and a half
requests per second okay everybody with
me
7/2 three and a half okay so the worker
thread pool is able to pull through
three and a half requests per second and
so that means that actually that's the
rate at which things can drain out of
the queue as well right so that means
the queue also has a maximum throughput
of three and a half requests per second
because that's actually how requests get
out is by being pulled by the worker
pool threads but here's a problem our
client is injecting five requests per
second okay so so requests are entering
the queue faster than they're being
drained so how many things are going to
be in the queue well if you wait long
enough there's gonna be an infinite
number of things in the queue and so so
if I divide infinity by three and a half
I think that's infinity so we start to
get an infinite response time but now
actually we can take it up to the whole
system right so now the whole system as
a whole is what the client actually sees
so the interesting thing right is we
actually saw the origin server we
actually saw this right we actually saw
the response time growing without bound
on the latency and and yet the origin
server was still processing things for
the work of threads right it still had
throughput that it was still getting
through so number of requests present in
the entire system well that's the number
requests in the queue plus the number
that are in the worker threads right so
that's infinity plus seven which is
infinity the response time is the amount
of time you spend in the queue plus the
amount of time you spend being worked on
that's infinity plus two which is also
infinity and I'm not exactly sure what X
should be here but but it's not
something that that we really understand
so so so this is really this is the
thing right so so when we think about an
API management gateway right we could
put one of those in and we could say hey
I want you to limit that client to five
requests per second alright so because
the requests from clients going to pass
through my proxy was you're gonna decide
whether to accept it or not pass it on
to the origin and then the response is
going to come back through the proxy
right this is how most a piano
management gateways work so the clients
sending five requests per second right
and and that that's actually its quota
that's its rate limit and so the
properties like yeah go for it
that sounds good but the origin was
having problems okay so the werden is
not happy and in fact because the origin
was not able to protect itself and I'm
not gonna ask people to raise hands
about how many people actually actively
shed loads other services but it's
actually really hard to know what the
capacity of your service is to be able
to protect this so it's actually common
that you run into services that won't
protect themselves this way so we're
stacking up all these questions but but
that means that the number of open
connections at the proxy that's waiting
for all this stuff to happen on this
unhappy origin is also stacking up and
if you're not really careful that means
that your API management proxy can be
unhappy and you can get starved out of
resources at the proxy which could have
implications for other services that are
being protected by that gateway I have
actually seen this happen in production
it's not super fun so so let's not do
this so so what's the problem here the
problem is that
the rate limit doesn't actually push
back on the client when the origin is
having a problem okay and so let's look
at it as an alternative so we're gonna
apply littles law now to the combination
of the proxy and the origin and so we
want the client to send be able to send
five requests per second right that was
its quota and under normal operating
conditions we know the average latency
of the service is one second right that
was that's how we want things to work
so with littles law we said that okay
well that means on average there should
only be five requests in flight at any
given point in time so here's the first
idea the first idea is that actually
instead of setting a quota on a rate
limit let's actually limit the number of
concurrent requests right we'll apply a
little song we'll say we're gonna keep
you under five requests in flight okay
and it turns out that is actually really
easy bookkeeping right so a request
comes in from the client plus one pass
it on to the origin response comes back
from the origin minus one all right so
the bookkeeping is really easy for this
and so if a request comes in which would
be sort of the sixth request in flight
right one comes in but I've already got
five well then the proxy just says nope
right you know that's where I'm gonna
push back and say no you can't send that
request so now let's look what happens
if we're going to use that as a quota of
the five concurrent quest and and the
client actually tries to exceed the sort
of rate limit that it wants all right so
this is where we usually think in terms
of applying rate limits to clients so
I'm going to try to send nine requests
per second so what littles law tells us
if we go and do the math is that by
Counting how many requests are in flight
actually what the proxy is going to do
is actually just going to pass five
requests per second on to the origin and
it'll send knope's sort of four requests
per second right so so the important
thing to understand here is under normal
operating conditions for the service
limiting concurrent requests behaves the
same way from a client's perspective as
rate limiting does okay so so at least
we've cleared that bar right like we're
doing at least of that good but there's
something better okay so so now let's
look at what happens when the origin
latency spikes
okay and anime spiked because for a
variety of reasons it may be overloaded
for example so we need to make sure that
we limit we're going to end up limiting
the total number of concurrent requests
to under five and so in order to make
this true that means that we're going to
end up limiting the throughput we're
going to allow to the origin to just two
and a half requests per second okay
because in order to make that inequality
work right to keep them under the under
the quota and so what that means is the
client is actually going to receive back
pressure right they were sending five
requests per second the version is
having a problem now I'm only going to
let them send two and a half requests
per second three okay this is actually
what we want if you remember the the
latency on that organ service didn't
actually drain off until we actually
stopped sending so much load okay so
this is a problem right an overloaded
service you know it's not going to get
better unless you actually back off of
it in some cases
all right so the first thing that we
want to take away is that when we look
at managing capacity through your API we
can actually limit concurrency and not
request right because it actually
behaves a little better when we get into
these overload conditions all right but
now let's let's change the scenario a
little bit suppose our origin service
instead of just having seven worker
threads let's say it had 20 and so we
elevated the latency from from one
second to two second right so now what's
going to happen so we're going to send
the same load over to it and we'll pop
back over to our handy-dandy dashboard
and and what we're going to see here in
a couple of seconds is even though the
the origin latency is is in fact
elevated right we're up around two
seconds which is what we expect and
origin is still able to process actually
now it's processing five requests per
second okay
and in fact if we look up here the
client is having success here so wait a
minute so what just happened okay so so
we we looked at a situation where okay
we're going to limit concurrency my
origin service had elevate
latency but in one case that caused sort
of infinite cueing and in the other case
it was fine okay and and really what
we're getting at here is that throughput
is not the same as capacity okay and
it's often the case that we use these
terms interchangeably in our industry
but they're not the same littles law
actually these are two different
variables in littles law right and so
these are different and so what I did
right when I changed the number of
worker threads from 7 to 14
I added capacity right I added more
flying elephants to my ride more places
for those requests to sit and so I no
longer at this request rate actually
exceeded the capacity of the rod okay so
that's an important thing to look at but
but how do we tell right like there was
no way I mean we could see that because
we knew what the capacity was what about
our API management proxy right we don't
want any just because the elevated
latency in some cases we don't
necessarily want to push back because
maybe the origin actually had capacity
available for the same request rate even
at a higher latency because we don't
want to end up with sort of wasted PI
right it's like ok here here's your
slice here's your quota and you know
yeah I've got all this extra PI but you
can't have any that this seam is sort of
you know not not intuitive so what we
really need is a way to understand what
is actually the capacity of that origin
service and so you know a good question
is well T can we think of any other
cases where we're trying to figure out
capacity or throughput and we don't
necessarily have direct access to it but
maybe we can sort of run some
experiments and try to figure out what
it is well it turns out there is this is
the way TCP congestion avoidance works
right so so TCP congestion avoidance
works under a principle of additive
increase and multiplicative decrease
right so the idea with TCP is that I'm
going to slowly increase the rate at
which I'm sending data until I encounter
dropped packets and then I'll interpret
that as congestion and then I'm going to
back off okay and that's that's the way
flow control for TCP works so let's try
to apply the same thing to our proxy
setting
okay so when things are going well and
we have clients who are requesting the
sort of additional capacity well maybe
we can allow additional connections back
to the origin service right if things
are going well let's go ahead and try to
service all those requests but we're
going to limit this so we can increase
this by a constant factor per unit time
so in the demo I'm going to show you
it's going to be one connection per
second will sort of allow one additional
connection to the origin every second
you know if there's actually a demand
for additional connections and if things
are not going well then we're going to
back off a multiplicatively which is a
hard word to say but anyway what we're
gonna do is we're going to say hey our
estimate of the capacity of the back end
service we're actually going to set it
in our case to a fraction of its current
size and so for the demo we're going to
set it to 75% of what we had right so
we're gonna increase by one per second
and when we encounter sort of some
problems we're gonna back off to 75%
okay and we're gonna try to adaptively
find the capacity so so what is this you
know sort of indications that things are
not going well well these could be
explicit right so the origin service
could actually pass this error codes or
maybe set a warning header or some other
indication that hey you're like kind of
overloading me now
and so but but they may also be implicit
right so so one of the other things that
you may see is that you know my request
for the service our timing out right or
I can't connect to it or or even just
establishing connections or timing out
right those are all indicators that I
might be overloading a service all right
so so let's let's take a look at this
now okay so let's set up our origin
service okay so I'm going to set it back
down to the sort of capacities
constrained version that had seven
worker threads and and - sorry a two
second average response time and it's
not protecting itself right so this is
this is the same thing that we just saw
where the where the where the latency
went you know through the roof over time
all right now I'm going to spin up a
proxy over here so so what this is this
is a this is an engine X in sense that
the algorithm that I've just been
talking about implemented as a lua
extension okay so we're gonna spin that
guy up and then we're going to send the
same load that we were at now remember
when we sent it directly against the
origin service this is this knocked over
the origin service okay and so all I'm
gonna do is I'm just going to update the
target so that so that the traffic
actually goes through the proxy instead
okay so so let's see what's let's see
what's happening over here we now have
an additional thing to look at so so
we're also scraping some metrics from
the proxies so there are two lines here
the orange line at the top is is the
proxies current estimate of the capacity
of the back-end service so that's the
target number of connections and then
the green line are the total connections
that are actually being used right and
so as additional load comes through what
we're seeing is that we get this
familiar sort of sawtooth pattern right
so so we have a demand for additional
connections we keep exploring we keep
exploring we encounter some back
pressure and then we exponentially back
off okay so we're adaptively finding the
that capacity of the origin now down
here we're doing a good job of
protecting things right like the the
origin latency is not running away from
us you know it is going up and down
that's because the back pressure signal
that we're actually using here is an
implicit one where we're getting to the
point we're actually timing out we can
see some of those timeouts up here this
is the blue line up here on the client
so we're seeing some timeouts where the
cue is getting too long and the proxy
backs off okay based on that signal and
so based on that the actual throughput
we're allowed to get through is going to
vary with the current number of
connections right this is littles law
again right there's a relationship
between those things so this is working
okay this is working pretty well so
let's let me stop this load and let's go
back to the queueing theory alright so
what's going on here
so so in this situation we still had
seven worker threads at the origin we
had two seconds of average response time
and so that note we still have the same
three and a half requests per second
throughput they were able to get here
now if we want
actually sort of stay within that
maximum throughput that means that the
throughput that we actually want to see
for the whole system and the throughput
of the queue also have to be three and a
half requests per second or less right
that's that's the whole point that's
what we're trying to do we're trying to
stay within what that origin service is
able to do so
now in our case we had a socket timeout
of two and a half seconds on a client
side okay so in order to get good
responses from the origin service I need
the response time as a whole to be less
than two and a half seconds okay but
that means if I'm spending two seconds
on the worker thread I can only spend
five or half a second sitting in the
queue which means on average I have a
queue length of 1.75 requests sitting in
the queue and a given point in time on
average and so that means that that the
total sort of concurrent connections
that we want to see here has to be eight
point seven five or less and this we
actually left that I stopped the proxy
and this left off but it may be a little
too small to see but in a triumphant
proxy or a demo success the target Manas
actually stopped at nine concurrent
requests so we actually with this
algorithm we actually found what the
community protected was going to be the
capacity that we needed to limit this to
all right but that's not all there's
more so so I've shown you how to
adaptively determine the origin capacity
but of course you know we've so far
we've only been looking at one client
okay now now this has been my experience
of how capacity gets managed when you
have sort of you know in an enterprise
we have lots of services right I've got
client and it says hey I'd like to use
you know 50 concurrent requests actually
a lot of times they're not used to
asking for concurrent requests so we
have to sort of you know teach them
about littles law but I might say hey I
want to use 50 requests and so then we
go and we we actually talk to the people
that run the origin service and they say
okay well I actually have capacity for
200 concurrent requests 50 is less than
200 so okay you're good to go and we set
a quota
for a client a of a 50 requests right
then client becomes and they say I'd
also like to use 50 concurrent requests
and the origin says okay I've got
capacity of 200 and I gave 50 to those
folks so yeah I can give 50 to you too
so that's okay
and so I set a quota for be a 50
concurrent request and then we actually
see what happens in production okay and
one of things that I commonly see is
that Kline a uses 10 and client B uses
10 because they've asked for
conservative quotas that's actually a
you know sort of a standard and it's
just like when you build a bridge like
you know you build it to support like
way more trucks and then we'll actually
cross the bridge right and so what we
end up with is we end up with stranded
capacity right so so what ends up
happening is that you know we've got
client a and climb B you know or are
using this slice of pie but we've got
all this wasted pie and you know you may
get a theme I don't like wasting pie so
so now there's another scenario that
happens which is that client a may
actually come along and attempt to use
60 concurrent requests right and and and
this sometimes happens where a client a
may be running a service that's actually
gotten more popular it's starting to
take more traffic but actually forgot to
come and update their quota okay so so
if we actually just enforce the the
quota of 50 concurrent requests right
we're actually gonna say that proxies
gonna say well wait a minute 60 is
bigger than 50 so I'm gonna start you
know sending notes for the extra 10 that
you're trying to send me but but here's
a problem right which is that the origin
service right we're only trying to use
70 and I certainly have the capacity for
this okay so so this is the case where
sometimes the quotas are actually being
too aggressive right these are requests
we could have serve but we're actually
enforcing the quotas strictly and so
we're we're actually not serving
requests that we could have served right
so these are in some sense avoidable
errors all right so so now let's let's
try Ferriero what we're gonna do here so
so we have an idea because we can you
know part of the problem here was we
didn't realize that the that the origin
service had additional capacity okay but
we actually just saw a way that we can
adaptively figure out what that capacity
is so now the idea is okay well
rather than having a flat sort of
absolute number for a quota what I'm
actually going to do is I'm going to
give you your configured quota
percentage and I'll explain what this is
times my current target connections
right I'm going to have my target
connections with my estimate of the
number of concurrent requests I can have
and so then I'm going to hand those out
on the basis of sort of what your
configured percentages and so so that
means that now we have when a request
comes in from a client and we have to
decide whether to admit that request
we're going to do one of two things
right so first we're gonna say hey is
the number of concurrent requests you're
trying trying to use currently less than
sort of your fearer allocated configured
percentage of the total capacity that
that I think the origin service has and
if it is yeah hey you're sort of within
your quota from some point of view and
so I'm gonna let that in or I may also
let it in if the total number of
connections that are coming through is
less than the target right so I've
actually estimated that even though
you're going over your quota the the
origin has room to handle your request
right that was the scenario that we saw
there where client a just sort of just
went over and so we're gonna allow we're
gonna try to make that work
okay and so so now I'm out to show you
another scenario so in this scenario
I've what we've done is we've configured
quota a say so we've actually written
down hey we want to get five concurrent
requests to client a we give fifteen
concurrent requests to client B and so
that means that essentially client a is
entitled to 25% to the available
capacity client B is entitled to
three-quarters of the current capacity
all right so so now I have a video
because the laws of demos plus the
fallacies of distributed computing say
you should not run a network demo during
your talk so I actually took a video of
this because I couldn't actually run it
from one machine because there's a limit
to how many sort of connections you can
hear pin through the loopback interface
over a bridge docker a network interface
so all right so so here's what we're
going to do okay so I'm going to start
up the same proxy that we
right this is the one now this has been
configured with the a is entitled to 25%
and B is entitled to 75% we're gonna
come over here on our origins server
we're gonna spin that up again and we're
gonna give it a little more capacity
just so that we can sort of see this a
little more effectively so we're gonna
give it 40 worker threads and to make
the math easy because you know it's easy
for computers divided by two but maybe
not so much for folks in an audience or
a speaker for that matter to do it on
the fly so we're gonna have an average
latency at one second so we're gonna
spin up our origin right so so that
origin should have a capacity of 40
concurrent requests all right so now
let's come over we're gonna spin up some
load from client a now remember Cline a
is entitled just to 25% of the available
capacity so it should only be entitled
to like ten concurrent requests but
we're actually going to send 40 requests
per second right client a is going to be
sort of really aggressive and it's
actually going to try to use the full
maximum capacity of the origin service
and we're going to send that load
through the target here right at that
proxy so we're gonna spin that up now
we're going to come over to to client B
okay so client B is actually not going
to be aggressive it's actually going to
stay within its configured quote
actually is going to even stay within
its its actual quota so we're only gonna
send 12 requests per second was actually
configured to be 15 absolutely but it's
entitled to 75 percent right so it
should be entitled to 30 concurrent
requests and so we should be you know
well within what client B is allowed to
use here okay so so let's see what
happens so we come down here we're gonna
look at our proxy so this is actually
climbing its way while while we were
launching stuff it's been climbing its
way towards 40 concurrent requests right
it only adds one per second so we
actually have to wait for the traffic to
sort of catch up with us if we come up
here and look at client B we can
actually see client B is actually
already getting its 12 requests per
second which it was trying to do so
we're actually admitting the traffic
that we wanted to admit from client B
which was staying within its quota on
the orange line there that's sloping up
that's that trailing 15-second average
and so we have to average out the zeros
that that we were seeing there over here
on client a we have the same trailing
orange line and that's a that's showing
up a little uh looks to me like twenty
three twenty three concurrent
connections or so 22 and so what we're
seeing so so it's trying to use forty
it's allowed to use ten but there's all
this unused capacity we're actually
letting it use twenty two and we're
pushing back on the rest of it alright
if we look down here on the origin
throughput you know our throughput is
sort of varying again we have a back off
factor of twenty five percent right so
we're sending it down a seventy-five
percent so we actually sort of dropped
from when we back off we dropped from
forty requests per second down a thirty
requests per second when we do that and
then we grow back up so we sort of see
it up and down but but the throughput is
stabilized the origin latency is
stabilized so we've stayed within the
boundaries of the the origin capacity
and and we've sort of this this looks
great right like we you know we've got
client B's within its configured quota
they're getting everything that they're
asking for and we're sort of doing best
effort we're doing as much of client a
as as we can possibly serve based on the
origin capacity all right so now so one
last thing though is that let's go here
so so we can actually borrow quota when
it's available right so that that that's
really the takeaway here so now here's
the thing right okay so the reality is I
don't actually just have one API
management gateway right usually what
happens is I set this up so I have
multiple instances of these you know
possibly behind a load balancer
all right so traffic is going to come in
there it's going to get distributed
between these proxies and so the
question is okay how am I going to do
this across multiple instances right so
I've got you know quota a at five
requests per second well I've got two
instances does that mean I need to or
sorry five concurrent requests does that
mean I need to configure you know
divided by two and sort of configure
each proxy to just have two and a half
concurrent requests I'm not sure how I
can't have a concurrent request anyway
or do I have to have some centralized
bookkeeping or
where you know maybe there's a some kind
of shared cache across these things
where these things are updating accounts
of who's using what you know that could
be a scalability concern certainly it's
a single point of failure so so that's
not great and so here is the great thing
about the additive increase
multiplicative decrease thing that that
we just saw right so the the thing about
this property is that actually converges
to a fair allocation of available
capacity okay so so let's look at one
more demo all right so this is the same
scenario we just left off except that
the traffic is actually going through a
load balancer now to our single proxy so
that's the only difference here right
we've got our adaptive number of
connections and what I'm going to do is
I'm going to come over here I'm gonna
spin up a second proxy instance this is
configured exactly the same they're both
configured to say kleine a gets five
client B gets fifteen I'm not dividing
anything I'm just giving it the same
configuration and so let's go and see
what happens so so now there's another
second set Alliance that's starting to
show up on the proxy line here right
that's the second proxy is beginning to
probe its way and find additional
capacity at the origin and and again due
to the additive increase multiplicative
decrease what's happening here is that
the first proxy actually backs off
faster than the second proxy is is sort
of trying to gain additional capacity
and so what happens over time is that is
that these things these two rows are
going to start sort of finding an
equilibrium and going back and forth and
so essentially the two proxies have
evenly divided the amount of available
capacity for the origin and it's okay we
don't have to divide the actual quotas
because we're actually only working on
quota percentages anyway right so so it
turns out when we look back up here at
the client B it's the same thing it's
still able to get its 1200 watts per
second Cline a at this point it looks
like that stopped it
you know 23 so that's 35 out of the 40
which you know again because sometimes
we're backing off it's only 30 requests
per second when we back off that's
actually pretty good right so so we're
actually able to do this dynamic and
adaptive
sort of thing without central
coordination at the proxies which makes
this pretty cool I think whoops we don't
need to see that demo again well I mean
unless you want to but all right so so
that's what I have to say here so so
there's a few things that I want to I
want you to take away here so so the
first one is littles law is great so if
there's one thing you should take away
it's like you should write down littles
law okay and now all my co-workers
you're laughing at me but but seriously
it's a great way to understand sort of
the capacity of your request processing
system right you can break it down into
different subsystems think about where
requests are sitting think about okay
how many of how many of the connections
in that connection pool are being used
how many file descriptors are being used
right you can figure out where you're
starting to run out of capacity in terms
of API capacity management I think it's
actually a lot better to limit
concurrency rather than request rate and
that's because when you have an overload
condition at the origin limiting the
concurrency actually provides the back
pressure on the clients that you need in
order for the origin to be able to
recover in those cases I've also shown
how to adapt to the capacity of the
origin right the original service might
be in an elastic scaling group right so
it could be that actually it will just
add capacity as you send more traffic
through it right and in those cases you
know we want to take advantage and not
strand that capacity if it's available
and then you know the last thing is
we've shown a way where we can actually
borrow unused quota from other clients
to make fuller use of that capacity and
so you know what this all boils down to
is that we can still protect the origin
services which was the idea of rate
limiting we can keep the allocation of
capacity between clients fair but we can
sort of make us still a best-effort
approach to serving as many requests as
we can and so I am slightly over I guess
I need to drink one more cup of caffeine
but I'm happy to take questions after
this you can either find me here or I'll
be at the Comcast table between breaks
today so thanks
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>