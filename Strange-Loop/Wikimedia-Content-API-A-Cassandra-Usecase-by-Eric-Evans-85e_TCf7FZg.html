<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Wikimedia Content API: A Cassandra Use-case&quot; by Eric Evans | Coder Coacher - Coaching Coders</title><meta content="&quot;Wikimedia Content API: A Cassandra Use-case&quot; by Eric Evans - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Wikimedia Content API: A Cassandra Use-case&quot; by Eric Evans</b></h2><h5 class="post__date">2016-09-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/85e_TCf7FZg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you all for coming it's a la last
slot of the last day everyone's probably
ready to start heading for the airport
yeah my name is Eric Evans I am a
software engineer with the Wikimedia
Foundation full disclosure I am also a
member of the Apache Cassandra PMC and
I've been involved with the project
since well since about as long as it's
been at Apache you used to work on it
full time it used to be my job but that
is that is not the capacity that I'm
speaking here today since I've been at
the foundation at least in terms of
modern Cassandra I've had the best
opportunity to to kind of dog food it to
have an actual application actually use
case to develop against and to be
involved with cluster operations and so
it's it's been quite an eye-opener to be
honest and they'll be no blind advocacy
today I'm just going to kind of wear my
user hat and be honest and
straightforward about things so the
Wikimedia Foundation well Wikimedia is a
kind of umbrella a global movement for
which there are a number of constituent
projects all sort of in the domain of
freely available collaboratively edited
knowledge the foundation exists in
support of that of that movement and it
is our vision for a world in which every
single human can freely share in the sum
of all knowledge every month
Wikimedia properties serve out more than
16 billion page views the best
well-known of the projects wikipedia has
more than 38 million articles all by
itself in almost 300 languages and just
by itself just Wikipedia alone you've
got the number sixth ranked website
according to to traffic my understanding
is if you take the page rankings of all
the other Wikimedia properties and
combine them you have something like
number five but Wikipedia alone is
number six so what does it take to to
run a site of this size it may come as a
surprise to some but the software that's
sort of central to every one of those
projects is media wiki and media wiki is
PHP so it's a lamp stack the next Apache
MySQL PHP or Maria DB and h h vm if you
prefer but simple right straightforward
like a blast from the past
in 1989 except for the the most
comprehensive architecture diagram I
could come up with is this and even I
can tell this is you know really hand
wavy and high level and kind of glosses
over all sorts of things missing things
it's not my intention to explain that I
would be the wrong person to do it
anyway
a media wiki is crazy extensible and
there's really no limit to how you can
modify its behavior how you can extend
it and we run a lot of extensions and
that combined with the scale makes
things interesting one of the things
that makes things interesting is called
visual editor it's an add-on and it
forms the basis of the the use case I'm
presenting on today so a a wiki is
basically a you know an editable website
and websites serve HTML if we had to
edit HTML markup then this would be
really painful so we usually have some
kind of you know relaxed syntax markup
something that's that's the mirrors the
semantics that we're interested in but
that's easier to edit so this means we
need to convert from that format to HTML
and what we go back and edit we edit the
the wiki text as it as it's called in
media wiki and then we we save it and
then we rerender that to HTML what
visual editor provides is a lucy wig
editor what you see is what you get so
instead of you know mashing edit' and
getting a web form with the text area
that you enter this wiki text in i'm you
just stay on the HTML representation and
and you go to town you click anywhere in
the page and just start editing and
there's format or format
editor controls this uses html5 s
content editable so the web browser is
technically the editor and you are
editing HTML directly but it kind of
goes without saying that if we're going
to do this we need to be able to now
convert in the other direction because
we have 15 years of document history in
wiki text we have a sizable group of
editors who have you know workflows
based around wiki text it's a big ship
it would take a long time to steer even
if we did decide to just shift to HTML
directly and this makes things a little
bit complicated so the biggest
complication is is in performing diffs
right we want to be able to do diffs
between arbitrary revisions of the
document and see only the normative
changes just what the editor changed so
we can't normalize any portion of the
document outside of what was actually
edited edited when converting from HTML
and the way we do this is by passing
along all sorts of additional metadata
you know as you're parsing the document
from wiki text to HTML you kind of
figure out you know at the time what
would it take to preserve this
formatting and just pass along that
information in the form of additional
metadata so in this really contrived
example when you see a link that's
formatted with double brackets like this
in wiki text that is a a wiki link it's
different than normal link it says that
this document is not only on the same
wiki but in the same namespace and so
you want to maintain that distinction so
you can see here that we've passed along
an rdfa attribute rel to indicate that
with the value that indicates this is a
wiki link so that we know how to put
that back and if the text of the link
were the product of a template then you
know you want to execute this template
and transclude the output into the end
of the HTML but you wouldn't want to
take that output then and put it back
into the wiki text that would just be
wrong so here we're wrapping it in a
span with with rdfa attributes to
indicate that that that's the product of
a template then we have this data parsed
soy to attribute this private attribute
that we pass along as well that can
actually contain the contents of the
template and that private attribute the
data parser attribute can get really
verbose can get right down to like
character and byte offsets if need be
so to this end we have a stateless micro
service that we call par side it's
written in note GS its purpose is to
take a specific revision of wiki text
and to convert it to an editable version
an editable representation of HTML you
know all the metadata necessary that if
edited and pass back the parser that can
be converted into wiki text and if you
diff that wiki text to the previous wiki
text you'd get just the normative
changes so not just document semantics
but also syntax as well you might wonder
well why not just have MediaWiki you
know parse HTML like this rather than
have an external microservice and the
answer is it's kind of slow it's
computationally intensive the output is
larger so it would take longer to
transfer which also drives up page load
times even the Dom is is more complex to
the point where it actually takes the
browser longer to work out so for
anybody who's not editing the HTML this
would actually be kind of a regression
and visual editor is an add-on so that's
that's one good reason to have it as as
an external service the other is it lets
us scale this out horizontally
independent of everything else and the
the output the responses from par soit
are very cacheable so we place these
behind rest base the rest base is
another service from Wikimedia we use it
kind of in place of service discovery we
use it to to aggregate microservices
sort of the produces an API which is
composable and the result of an
arbitrary number of back-end requests
and responses and trent you know
transforms very flexible and it also
performs durable caching we have
pluggable backends for this but we use
Cassandra in production so it looks
something like this
parser is not the only service that we
aggregate base there are others usually
it's kind of a similar similar theme you
have some sort of expensive transform
you know just to solve a particular use
case the response is cacheable and we
put that behind rest base ok so that's
that's our use case for Cassandra let's
dive into that a little
so in our environment we we have one
cluster that spans two data centers we
run in Cassandra 2 2 6 we're using
Cassandra's network topology strategy to
distribute a total of 6 replicas 3 to
each data center within each data center
we have our hosts spread out across 3
rows that's kind of the unit we're
planning for for failures is to lose at
most a row so we have the the replicas
distributed across those rows as well
we're using fairly beefy hosts 16 core
32 32 threads 128 gigs of memory SSDs
we're running a 54 node Cassandra
cluster despite having 18 hosts and I'll
explain them how that works a little bit
a little bit later
Cassandra's default compression is lz4
we're using deflate we actually also see
better compressed sizes I think than
what most people do I'll also expound on
that a little bit as well and it is a
read heavy workload which you might
expect since I described it as a durable
cache but at 5 to 1 ratio maybe it's not
quite as read heavy as you might expect
the reasons for that are we also have a
very robust CDN and rest base is
typically only queried on a cache miss
from the CDN so the read read requests
are a bit lower than you might expect
and since we really don't know which
documents you know we don't know how to
anticipate which documents might be
edited with visual editor we prime the
cache on every single document edit so
the the write rate is perhaps higher
than what you might expect as well ok
everything in Cassandra and everything
else you might want to talk about kind
of pertains to the data model I mean you
have to get the data model right if you
expect performance characteristics to be
with you which you want them to be and
then that affects everything else so
just start with the data model we have
this primitive storage primitive in
rest-based called key revision value and
the key and key revision value for the
parser use-case is a combination
of the site domain you know the site
name and the document title because
documents are unique to a site and in
the recipes rest basis Cassandra storage
module that key ends up being the
partition key in Cassandra so that's
what determines distribution throughout
the cluster ultimately the revision and
queue revision value is the revision as
supplied by MediaWiki so this is a
monotonically increasing integer value
comes right out of MySQL database every
time an editor matches save a new ID is
created this maps to that that ID and
obviously you have more than one
revision for four documents this is a
many to one relationship with the key
but then we have this value called a
render which is kind of a revision of
the revision or a version of the
revision keyed by a timing UID many to
one relationship with that revision and
the reason for that is we're doing full
page renders so there's always some
material here that wasn't a part of the
actual wiki text document the canonical
document sidebar or footer headers or
templates you know if the document uses
a template you can imagine that template
being changed somewhere and every
document that uses it now needs to be
rerender to HTML so the render as shown
here is basically a point in time
representation of the HTML for a given
revision and then the value is just a
blob so in our case this is usually a
string storing the HTML for parsley
would look like this a CQL DDL or
visualize more directly again you have a
key made up of the domain and the title
and then an arbitrary number of
revisions and then each revision has an
arbitrary number of renders and this is
kind of how the data is laid out on disk
I mean Cassandra does store in sorted
order our data is going to be sorted
first by the revision then by the by the
render which is important when you're
talking about compression so compression
is a topic that we have spent a fair
amount of time on and will probably
spend some more still because what we're
storing is document history and there's
a lot of value in that
but space is finite so the more the
better use we can make of that space the
more history we can store than the more
use cases we might be able to enable
sort of the hallmark of any
general-purpose compression algorithm
and remember to remember we're using
deflate is the ability to look for
repetition within a stream and then
store back references to that as it
turns out our revisions have very very
little changes typically between between
1 and the next and since they're stored
sequentially there's a lot of repetition
there that could be found cassandra has
this parameter called chunk length that
basically determines 1 from its stream
as it's reading or writing data how much
of that to pass off to the the
compressor so we jac that wide open 64
kilobytes is is the default a lot of
people go down from there we actually go
up as high as 512 512 K in order to pick
up more of that repetition and it's
because of this because this works so
well for us that we have been looking
for compression algorithms that don't
have a fixed size window deflate has a
fixed sized window I think it's 31 K
from which that it can look for
repetition within that stream some
compression algorithms lzma brought Lee
for example will let you specify the
size of that window bratli is a really
good compression algorithm all by itself
it's kind of made to be a better better
deflate than deflate apples-to-apples it
will produce smaller compressed sizes
with lower computational cost so it's
pretty much a win no matter what but it
will also let you change that window
size and so larger window sizes and a
larger chunk chunk slowing still we have
a lot of potential to really do to get
some space savings here I wanted to have
more formal results for this but my
intentions got pulled elsewhere recent
more recently so there's I will have
these slides posted there's a link to
the to the ticket where we and the
preliminary work is done you can look at
that if you're if you're interested in
that another topic that we have spent a
fair amount of time on and will probably
continue to spend a fair amount of time
is compaction Cassandra has a log
structured storage system and compaction
is kind of the price you pay for having
log structure storage
it runs asynchronously in the background
the intention is to reorganize the data
to make it more optimal for reads at a
very minimum you want to reduce the
number of files you want to merge many
files into fewer files if possible when
doing that you'd like to reorganize the
data in such a way that the results will
be near each other and can be read from
fewer files on disk this is also your
opportunity to - garbage collecting
obsoleted data if you overwrite a value
in a log structure storage you're
basically just placing a newer value and
in masks the old one and a delete is
kind of a special special case of that
overwrite you you write a tombstone
which is a marker that says any previous
values should be considered deleted
compactions your opportunity to expunge
those and then there's also key TLS
which is basically putting a timer on a
value where you expect it to be you know
considered deleted once that timers up
so this is your opportunity you know as
you're merging files together to just
drop what's no longer relevant the other
important function of compaction when we
first set this cluster up there were
only two compaction strategies that were
available size teared and leveled size
tear just takes files of similar sizes
combines them to make fewer files
it's oblivious to your column
distribution but it doesn't use a lot of
i/o then you have leveled which is kind
of clever - clever it's inspired by
Google's level dB you take files of
fixed size fairly small sized files of
fixed size and you arrange them in
levels of non-overlapping ranges and
then each level has a sort of maximum
number of files they're allowed for that
level and each with each successive
level that number is exponentially
larger when a level has too many files
you merge it some of that data into the
next highest level so a simple point
query if you're just looking up a single
column
you the kind of the worst case for that
is is the number of levels in terms of
tables that you need to read from and
that tends to be a very small number so
this is very optimal on reads and you
know we're serving up pages to clients
with you know with the web browser on
the other end and latency matters and so
we thought you know this is this is
definitely for us turns out it's not
mostly because it's just way too much
disk i/o way too much compaction
throughput to make that algorithm work
you're compacting your data and you're
compacting it again and over and over
and over again your reach reading all of
that same data multiple times it's it
produces pretty much the most efficient
reads but it does it with pretty much
the least efficient use of the disk and
you know there's there's you know
allocations on the heap as part of the
compaction process so if you even if you
can manage the disk i/o you're pushing
throughput on the JVM just garbage
collector you're probably starting to
see some pauses it's just really hard to
to accept that much compaction
throughput but Along Came D tiered
compaction after the fact and this
really seemed like this would be our jam
because it's clever instead of combining
files of similar size like the upper one
size tiered what it does is it uses its
knowledge about the minimum maximum time
stamps in a file to combine them instead
by their date and time whether by when
they were written the idea being if your
data set is total ordered and if you
remember ours is you know its first by
revision then by timing UID newer values
were always sort of higher than later
than previous values if your data set is
total ordered then arranging things by
date ranges your data in order because
time and date are total order and it can
do this in pretty much the least amount
of disk i/o so this this definitely
seemed like the thing for us except for
it really wasn't without going into too
deep of an explanation about how they
tiered works
just say that it is so easy to defeat
those optimizations that that for all
intents and purposes they just don't
work any kind of out of order right is
enough to completely defeat those those
optimizations a delete isn't out of
order right an update is an out of order
write and read repair is an out of order
right and you kind of want you know you
kind of want your data you kind of want
read repair there to kind of counteract
any inconsistencies that might build up
over time so it did not work out nearly
as well as we'd hoped it doesn't work as
nearly as well on paper as it has it in
real life as it doesn't paper we're
still using date eared we're still
trying to figure out what the right
thing to do is with the optimizations
defeated you're pretty much kind of left
with with what size tier does so we made
you switch to that there's another out
of tree compaction strategy called time
window compaction strategy which isn't
sensitive to these out of order rights
and that that may be our saving grace
but yeah this is still kind of an
outstanding problem and it's actually
worse than just the the not really being
optimized well optimized for reads if
you remember you also need to to expunge
obsoleted data and compaction your
opportunity to do that there's this
problem called the overlapping tables
problem when compaction happens
automatically it's it's only ever
working on a subset of all of the the
tables on disk and so when it encounters
a tombstone it encounters any sort of
droppable data any data that's a
candidate for from not writing in the
output it has to take into consideration
the picture of you know like what what
exists in these other tables the ones
it's not compacting if there's the
potential for a value that precedes the
value that you're considering dropping
then dropping it would you know perhaps
resurrect a value that was supposed to
be deleted I can't do that so it takes a
it takes a look at the indexing for you
know the metadata for these tables not
being considered and you know if if
these files don't contain the same
partition key well then we know it's
safe we
go ahead and drop the data there's also
a maximum purgable timestamp and so we
can look at that and further optimize
whether or not we can maybe rule out
rule out on these tables but if we just
can't be absolutely sure that there
isn't a preceding value we have to keep
it and make it part of the the output of
the compaction and hope that a
subsequent compaction of the subset of
the tables will pick it up in our case
if you remember we're storing document
history all tied to one partition key
which means that over time our partition
keys are pretty much across all of the
tables and with with with timestamps not
really being ordered those don't do much
to help optimize things away and so we
have huge huge amounts of data that
should be candidates to get draw to be
dropped
and never does so this is another
problem that we're we haven't yet have a
solution for and and it's hurting us
moving on Cassandra is written in Java
it runs on the JVM it's a database
you know we're concerned about latency
so of course you know garbage collection
is the thing JVM garbage collection this
is actually a story with a fairly happy
ending when we ran concurrent mark-sweep
you know back in the day yeah we spend
an inordinate amount of time trying to
get this right
trying to get GC right never really
happy with the results always working on
it felt like then we found g1 GC and
most people will describe g1 as being
you know mostly tuning free maybe you
know you need to give it a bit more heap
than what you're used to give it some
working space to work it's magic we
found that to be mostly true we had one
one exception to that in g1 parlance
they call they call a humungous object
one where it takes up more than half
half or more of your region size so G 1
divides your heap into some number of
regions of some size and if you attempt
to allocate an object that's 1/2 that
size or larger then it it puts it in
what it marks as a as a as a humongous
region and puts it in a special area and
it doesn't put more than one
object / humongous region it treats them
in an exceptional managed manner so if
if you have too many of them then it can
kind of destroy the collectors
efficiency in our case you know it was
calculating the 4 Meg regions and we had
enough data enough objects that were 2
Meg's or larger that that actually did
create some inefficiency so the one
setting we overrode was to increase the
size of the the regions and that fixed
all of our problems and it's worked
beautifully since all right another
subject that has taken up a great deal
of our time is no density you know how
you have a data set of some size you
have a throughput you know how many you
know how do you size your cluster how
many nodes do you put you know do you do
you install a end of what size
unfortunately this is you know a little
bit more there's a little bit more to
this than what you'd like it to be you
know you'd kind of like it to just be
based on what on the hardware capacity
you have available to and then just you
know divided by your throughput or your
size but Cassandra just does not
vertically scale to modern hardware
nearly as well as you'd like it to and
some of the some of the things that will
cause you problems are for example
compaction those algorithms are you know
applied to the data set that you know of
a single node so the larger that data
set is you know if the efficacy of that
compaction strategy is poor and the
worse the problem becomes the more data
you have under a single node the more
you know the more share of the data that
a node has then the higher its share of
the throughput is as well and that's
more throughput on the GC which you know
could make turning it more of a
challenge and and bigger nodes also you
know you have the blast radius to
consider like what happens if it fails
you know and I need to bootstrap in a
new node it takes a lot longer to stream
you know 10 terabytes than it does to
stream one for example so you find
yourself you know not able to fully
utilize modern hardware you need a way
of deploying basically scaling the node
size down
wasting your hardware what we do is we
spin up multiple processes I wish we
were you know something more
sophisticated than that but it's not we
just we use puppet in our environment so
we have puppet write out multiple
configuration directories that reference
multiple data build a directory and we
write out multiple systemd units and we
just spin them up to make matters worse
we have a raid zero shared raid zero on
the host and all of the instances use
the same should parade zero so there
goes your blast radius again you know
one disk fails and the raid you know the
array is is gone and all three instances
go with it and that's actually happened
to us so don't do that it's not good
what I wish we'd have done instead is
used you know anyone of the norm you
know the standard you know commonplace
abstractions that people use for carving
up Hardware these days and a
virtualization containers or we could
have just even bought blades I think
would have would have made sense you
know if you're worried about you know
the number of machines and the size of
them they you know the data center
occupancy issue then you know maybe
maybe blades would have been a good
option multiple processes actually works
it works better than I kind of expected
it to but it you need to do it's a lot
of it was a lot of work to get it set up
and it's you know it's kind of different
and weird and it has you know it's
operationally it's more complex there
are definitely better ways okay so those
are the kind of the things that you know
we we spent focused most of our time on
and you know of course you know you
don't spend time working on things that
are working well for you so there's I
guess it's a little bit of a negative
meaning to most of that stuff I'll kind
of pull back a little bit and just you
know touch on what I think works and
what doesn't more broadly so the good
basically everything that you hear
talked about in Cassandra is you know
sort of core feature set you know all of
that stuff is awesome you know like the
the fault tolerance and availability you
get the way you can game you know you
your replication factor to trade away
consistency for availability all of that
is fantastic it's well instrumented you
know as good visibility so you know if
you if you're if you like to monitor if
you're monitoring that like I am it's
really awesome because you can kind of
get in and see see what's going on with
every aspect in every part of the system
it's ubiquitous at this point and a lot
of people are using it which helps you
know you go on Stack Overflow and you
find answers to questions you know that
you find out are a lot more common than
you thought there's software being
written for it it's got a big robust
community of you know friendly helpful
people if you need to get a question
answered or you just want to have lunch
with somebody and bounce ideas off of
them you know it's good to not be the
only one or to be the early adopter the
bad I would say number one for me is
usability and I know a lot of my you
know fellow Cassandra developers would
probably you know think this this is
being unfair and they would probably
point out rightfully so that usability
is way better than it was like a year
ago and way better still than it was two
years ago and they're right it is it's
way better and it's still horrible
they're still awful I would argue I
don't necessarily have like an idea of
how you would fix this but I think I
would argue that just the fact that you
have to deal with compaction at all that
you have to think about it you have to
know your data model and your access
patterns and then understand all of the
available compaction algorithms in order
to choose the right one and tune it
appropriately
it requires that of the operator is kind
of a you know it doesn't make for a good
usability story I don't think one of the
things I think really damages usability
is the fact that all of that nifty
monitoring and management functionality
comes via JMX
that's the kind of functionality you
kind of want to be accessible to dynamic
programming languages those are the
people and the tools they use
you know JMX kind of puts that at arm's
arm's length from from them here kind of
requires that you use Java and then it's
really unpleasant to do even from Java
and you know it's this this all sounds
like I'm being very critical I'm
probably the one most responsible for
the use of jam axe if we went all the
way back I truly think that was the
wrong move and then like I've already
pointed out it just doesn't vertically
scale as well as you would like it to
you so you're left you know doing hacky
things in order to you know to to make
to take full advantage of your hardware
beyond the band there's a few things
that are very very ugly I think the
process of upgrading from one version to
another is absolutely horrible I just
did one I did one from - 113 - 2 - 6 and
that should have been a very simple very
straightforward move it wasn't expected
to be a lot of change between those two
versions in fact 2 to 6 was released
specifically because there were big big
changes coming and it was like we didn't
want to wait until all the way to 3.0 so
it was you know kind of flushing out
some of the simpler more straightforward
stuff it should have been an easy move
and I waited till you know 2 to 6 so
that you know other suckers I mean other
people would shake out all the nasty
bugs there were a lot of people running
2 to 6 a lot of people who've upgraded 2
to 1 you know from the t1 series and I
tested it you know I tested it I would I
thought it was very carefully so you'd
think there'd be no room for things to
go wrong three things three critical
things went wrong and you know it the
new version had a new on disk format
file format and so the moment you start
to upgrade you know the compaction in
and new writes start to write the new
format and you can't go back because
it's not backward compatible
you know you need this stuff to work
really smoothly if you're if your data
format has changing like this I know
people I don't know if I would go so far
as to say you can't upgrade but I do
know people who do say that who say that
they would never you know consider an
upgrade of a production system Cassandra
that they would always set up a new
clock
stur double right back fill and then
decommission the old one which is really
sad that these are smart people to like
so I'm not sure I fully agree that
there's if it's impossible to upgrade
but I do see where they're coming from
and and that's pretty sad I think a lot
of this boils down to like quality
control between releases you know that
they upgrades are so bad and it was
because of that that the release process
was changed to something that's kind of
vaguely like Intel's tick tock you know
where Aude release means one thing and
even release meetings another and the
idea was to try to keep Trunk more or
less releasable to get new features out
and get people to test them while
maintaining a stable base and this was
done because what people used to do is
they at least to do like what I did they
waited till like the dot six of the
minor in December release version they
waited till about 6:00 when all of the
really bad things had blown up and hurt
other people before upgrading and so
this was like an idea of trying to solve
that problem so couldn't so every wanted
to just stand around waiting you know
like playing a game of chicken to see
who'd upgrade first and all this change
of the really release process did is
just obscure where that sweet spot is
when that when it finally become stable
like now nobody knows it's got like a
way of tricking people into running then
and if you think that I'm being overly
harsh there just search the mailing list
archives the confusion about what
version to run is just you know it's
pervasive like nobody knows what version
to run and everyone's very confused by
it really did you just made that worse
with that I maybe have a couple minutes
for questions yeah
we do run vinhos at 256 per
yep
yeah that questions come up before if
you could start over would you use a
completely different database I'd go
back to the you know the the good you
know it's like I don't really know what
else would have would have you know
we're all guilty of bias and I'm
probably pretty heavily biased or
whatever so I you know there's the
temptation when you're sitting down to a
value a to you know the problem and what
you need to to kind of you know come up
with a set of requirements that match
what you know you know what you know one
which you're familiar with so short of a
bias like that though you know based on
the requirements I still I'm not really
sure what else you what else you would
use I know we are kind of having it we
don't we do kind of have a close eye on
Cilla DB which is a Cassandra compatible
ish re-implementation and C++ they seem
to be doing good things so we're kind of
keeping an eye on them because most of
my I think most of my complaints boil
down as sort of a quality control kind
of like the release processes yeah it's
either the either boil down to kind of
you know the release process the quality
control and the testing which is largely
it's responders very difficult to test
because of its sida architecture and the
use of Singleton's either boils down to
that or they boil down to issues related
to the JVM both of which if you re
implemented we're hopeful about that
anything else going once
alright twice okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>