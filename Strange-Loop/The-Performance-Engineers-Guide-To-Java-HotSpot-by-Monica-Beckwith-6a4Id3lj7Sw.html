<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;The Performance Engineer's Guide To Java HotSpot&quot; by Monica Beckwith | Coder Coacher - Coaching Coders</title><meta content="&quot;The Performance Engineer's Guide To Java HotSpot&quot; by Monica Beckwith - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>&quot;The Performance Engineer's Guide To Java HotSpot&quot; by Monica Beckwith</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6a4Id3lj7Sw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody I'm Monica Beckwith I'm
gonna I'm a performance engineer I'm
going to talk about hotspot which is a
java virtual machine I'm going to talk
about the what makes up the virtual
machine and how from performance
standpoint how there are various
optimizations in the virtual machine
that we already are taking advantage of
or we can depending on the version of
Java that you're at so a little bit
about me like I said I'm a performance
engineer worked with the java virtual
machine for more than a decade
especially with hotspot which is like
sun oracles virtual machine we also have
worked on the open JDK hotspot which was
a similar and maybe this is on I was
getting a feedback I was wondering okay
so open JDK hotspot is also the same
thing so when when changes are made to
the hotspot virtual machine most of them
are propagated into the open JDK so I've
worked with Sun Oracle and before that I
worked with Advanced Micro Devices I was
working on their new Opteron 64-bit
processor and we were trying to do
optimizations with respect to static
compilers dynamic compilers so that's
how I got into java virtual machines at
Oracle and Sun I've worked not only with
the JVM but also with jet heuristics and
compilation garbage collection it's also
so my last title at Oracle was garbage
first garbage collector performance lead
and now I'm consultant I've been working
as a consultant for about three to four
years now I optimized managed runtime
applications and
one of the reasons I wanted to give this
talk is because it's really important as
a performance engineer to to understand
what you what are you trying to tune so
and one of the reasons I went into
performance engineering is because I
really like the VM the garbage
collection and I could see various ways
to be to improve certain things and with
this knowledge hopefully you will go
back if you're working with the virtual
machine you can go back and take these
optimizations to your place of work and
try to try them out if you haven't tried
the mod already so this is the agenda
since I'm going to talk about the
virtual machine I'm going to talk about
its components so there's the execution
engine and you know in the runtime so
I'm going to talk about those those are
those I call them the helpers because
that's what kind of helps the virtual
machine function the other part of it is
performance engineering so I'm going to
talk about the trifecta of performance
engineering what drives these
innovations into the virtual machine so
basically the footprint the throughput
and latency which is also known as
responsiveness and then finally who will
dive into these and try to see how these
like how the footprint throughput and
latency how that influences the virtual
machine itself so let's look at the
virtual machine in a simplified form we
have your java application and then it
runs on top of the virtual machine which
I call the java vm and then you have the
operating system and hardware it could
be like he could be running it on the
cloud or whatever so basically the gist
is that your virtual machine and your
java api conforms your GRE JRE java
runtime environment that's what most of
you guys download on your on your phones
or on your cloud system applications or
whatever so whenever you're downloading
something that's a GRE mostly many
people who work with
the development kit they download the
JDK so JD case of this + tools basically
some built-in tools like a console and
stuff like that ok so let's talk about
the helpers like us and helpers are the
runtime and the execution engine so the
execution engine itself
they're two components first is the heap
management which is known as garbage
collection and then the second is
compilation so we all know about the JIT
compiler so that's what I'm talking
about when I'm talking about the
compilation so let's talk briefly about
the heat management aspect of it so we
know heap management garbage collection
we all know is reclamation thank you
reclaiming back your heap but there is
also an aspect of to heap management
which is allocation taking control of
your allocation and how that is
important I'll talk about it when I'm
talking about the performance trifecta
okay and like I said when when you
talking about optimization you're
talking about compilation we straight
away think about static compilation most
of simpler optimizations in the dynamic
compiler come from the static
compilation world - so one of those is
like ahead of time from for compilation
so instead of doing just in time
compilation if we know there's a
particular library that's going to be
used often or some shared code then we
can compile it ahead of time and then
use it in when we are calling you know
compiling our application so we already
have pre compiled code so that's what
ahead of time complicate compilation
gives us for hotspot JVM Java 9 is the
first release that we'll have ahead of
time compilation and I think it's just
the base of the modular Java 9 and
there's a base and that particular
aspect is ahead of time compiled because
base is used in every application
so there are two other aspects two main
aspects I would say one is the profile
guided optimisation and then the other
one is adaptive optimization so for
hotspot virtual machine this is these
are the two backbones so from from the
get kill like maybe from Java one point
whatever you know we did a little bit of
you know we had tiered car we had
different compilation levels like server
and client and then there was to be able
to go to from one to the other they were
thresholds and as your application would
run they go from warm up to steady-state
you would cross these thresholds for
your hot code so what every application
consider is whatever is hot for your
application that's what the JVM find
that finds out because of the number of
invocations that you have and that's one
of the ways that it does profile guided
optimization it has been improved
further by tiered compilation and stuff
like that and all if I get chance we'll
talk more about that and similarly we
have been adaptive optimization such as
inlining such as D optimizations and
I'll talk about that as well but those
two make up the hotspot virtual machine
and then äôt just got added in it's
simplified form in java 9 so what about
runtime so we all know the most common
most so that's how it you know your code
is interpreted first at the very
beginning as soon as you start running
you start that startup that's what you
get you get simple interpreted code
eventually you know it'll go based on
like I said thresholds and stuff like
that it'll go into different look to
your levels and get compiled at
different efficiencies what else does
the runtime do it does class loading and
various other things thread management
synchronization I'm not going to cover
and more of these but when I'm talking
about optimizations I may talk about
synchronization and stuff like that but
if you want to read of course please go
ahead
then look at the OpenJDK java.net page
and that gives you a good overview of
the runtime of the hotspot vm so what
when we talk about runtime there's only
one goal
runtime L&amp;amp;E has one goal and that's
converting byte code to native code
right and so that's why interpreter is
how it starts and then the compilation
happens and then you have optimized code
spewed as a part of the JIT compilation
adaptive optimization but the goal is
byte code to native code that's simple
as that so you have your Java bytecode
converted to native code you execute the
native code on your hardware so now we
understand the execution engine we
understand the runtime so what does that
have to do with performance so what
drives innovations in performance
there's the three aspects of it the
responsiveness is the first one
footprint is the next one and throughput
so if I send a stimulus how soon am I
gonna get it back that's what responds
nerves is so you have your system you
send it a stimulus maybe two to a block
of it or to the entire system
right so how soon do you get it back and
that's a responsiveness either of the
entire system or components of that
system footprint is very simply it's how
much memory you're consuming you know
not just your heap but your process
itself how much memory is it consuming
and can i push fit in more can i compact
it you know those are the optimizations
that drive innovations in in the JVM
itself ok throughput how can I maximize
the usage of my system operations per
second I want to have the most pushed in
through my system I won the JVM to do
its best to optimize the code generated
code to its optimal sequence and and
executed I don't want the garbage
collection to
would be a pain you know and and I'll
talk more about that but the goal is
that my application should be doing its
work most of the time that's the goal
that's the throughput so now we know the
execution engine we know the runtime and
let's go and try to see how
responsiveness has driven innovations in
those two okay
responsiveness and garbage collection so
responsiveness has given remember and I
said about heap management and garbage
collection I said it was not about
reclamation or not only about
reclamation it was about allocations as
well so to be able to improve
responsiveness allocations have been
improved so for a long time now the
hotspot virtual machine has faster
allocation paths these are called
thread-local allocation buffers these
are loxley allocations where a thread
has its own local area where it can
allocate where you can allocate out of
so that way it doesn't have to have
contention issues and stuff like that
what other threads it only goes and
communicates when it needs another
buffer area so it's lock free of course
we have faster reclamation some of the
some of the GC algorithms have
deallocating in place so it's really
fast okay and I'll talk more about that
but just want to list all these things
and of course more work done
concurrently remember when I was talking
about you don't want your application
spending more time in GC basically
applications not doing any work and and
that's one of the aspects of open JDK
hot spot virtual machine
most of the GCS are have stock the world
pause so that means your application
threads come to a full stop and then GC
does its work and then GC would do its
mark sweep you know and compaction in
some cases I'll talk more about that so
what that means is your application is
not working your GC is doing its work
your application has stopped so
some of the GCS depending on how
sensitive they are to responsiveness
they may do some of the work
concurrently with the application that
means your application threats are
working and then your GC threats are
working with your application threads
okay so that's called doing work
concurrently
what about responsiveness and
compilation so of course I mentioned
about ahead of time compilation it helps
with improving startup time so that's
one of the things that have just been
added in its most basic form in Java 9
we have the adaptive optimizer and I'll
talk about it over the MIS entire talk
because there's lots of adaptive
optimizations done in the hotspot
compiler and then locking improvements
ok and I'll talk about locking
improvements as well sorry here shortly
the another place that's important is
the drop locking is important is with
respect to the runtime remember I said
interpreter so when we're talking about
about locking these the locking
instructions are generated in in the
interpreted code itself so so anytime
you have locking improvements it also
helps responsiveness you know for the
runtime of course and then class data
sharing the class data sharing is is a
particularly important thing because
somewhat sometimes you have shared code
that you want to be able to access so
it's kind of like a shared library or
something
that you want to be access and then
that's called class data sharing okay so
let's now talk more about the locking
improvements so over the years I think
maybe since Java 5 or so up until now to
Java 9 there's been a new improvement
added injera line so there we've seen
various flavors of locking improvements
okay so what are locks what kind of
locks do we usually target the first
kind of course is uncontained 'add so we
have a single thread t it's trying to
execute a synchronized method okay and
that's called an uncontained deadlock
because it's just a single thread that's
trying to execute the method
okay so uncontacted logs are also called
deflated locks also known as lightweight
locks so in that simplest form the
lightweight lock would only do a compare
and swap instruction it's called Cass
Cass instruction so this is at at your
hardware level so it executes this
instruction to store pointer to the lock
record in your object header so every
Java Java object has a header and a body
and I'll talk more about that when I go
into details about compressed oops and
stuff like that but so so so - what it
doesn't burn what we achieved by this is
saying that this is locked object and
here is where you'll find more
information on it so it's basically a
pointer now what if the there was
contention so basically a different
thread you comes in and it wants to
access the synchronize method which is
already locked by team remember so now
we have contention and contention is
very different so it's an inflated lock
and I'll talk more about what that means
it's also called heavyweight lock which
means it's not using casts right it's
not your lightweight lock and it does a
slower path now what is the slower path
it needs to so now since you have
contention we need to make sure that
we're taking care of these threads that
need access to our object so every
object has a monitor and then each
monitor has its weight set so as threads
are contending for my object they will
go and wait in the weight set once I'm
done with the work I need to notify and
then these objects these threads can
come in access that object so if I had
to draw something this is how it would
be my object monitor is right there I
have an entry queue and so you have the
monitor enter and monitor exit operation
and if somebody needs to wait they go
and wait and they wait queue
once they get a notify signal they come
back into the enter entry queue and then
then they can access the object monitor
so that's how an inflated lock works
that's usually done in the slowest path
and up until Java 9 what other
improvements are with respect to locks
so remember what I was talking about
deflated locks or you know lightweight
locks I was talking about the cast now
one if I knew that there was like there
was only single thread that was going to
ever access it so I can bias my lock to
that thread and that's called bias
locking you don't you only need to do
cast the first time and then your your
thread is known to be biased to the lock
so that's an improvement you don't need
to do it casts every time the second one
second improvement is called la
collision and I'll talk more about it
when I talk about escape analysis so
with escape analysis we try to find out
if an allocation escapes the method or
the thread if it does not then we don't
need the lock anymore so we can evade
the lock there are other ways to evade
the law called sir--they and I'll talk
more when I talk about I skip analysis
so let's move on to lock coarsening so
we've seen multiple synchronized
accesses to either method or or an
instance what if they were all for the
same object right so a JVM can identify
that and it says okay I don't need to
lock every time I don't do need to do a
casts every time I can just course in
these and just have to it once so that's
called lock worsening and then coming
back to contended lock so with with Java
9 what happens is inserting the snowpack
we can we can based uncertain based in
the path we took already and we know
that it's in fated lock we can do
something called a quick path it's
called quick enter where you can perform
certain things like for example
remember I mentioned about the wait
queue and when the notify is sent then
these guys can go and sit in the entry
queue what if we could just do that as
soon as we know it's an inflated lock
we know that it's somebody else is going
to get get to it so why not just
transfer them as soon as we know it's an
inflated lock so those kind of
optimizations have made it into the Java
nine so this is how an inflated lock
would look so once you know runtime
knows that it's a contention or an
inflated lock it can do any of those
things and it'll go to enter first and
then it'll go the slow path with Java 9
there is a quick enter which does these
kind of optimizations ahead of time
because we know it's an infinite lock we
know that we have reached here for a
reason so we can do certain things that
are necessary to be done so now let's
concentrate on footprint so usually why
I say you know based on the three legged
stool or footprint runtime footprint
responsiveness and throughput footprint
is the least important you know because
most of the optimizations are gone into
either improving throughput or
responsiveness but this particular talk
I wanted to talk more about footprint
because there's so many optimizations
with respect to string that have gone
into Java 8 and Java 9 that I wanted to
highlight here so let's first talk about
footprint and garbage collection so
compaction and allocation reducing
allocations so what is the order is
compaction so we have our heap space and
we have reclaimed a bunch of objects but
they may leave for example and it does
happen in concurrent mark-and-sweep up
garbage collector they may leave holes
because you have these live objects and
then free space live objects free space
and based on the size of the free space
an object can fit or may not fit and
even though you have space your your
object the new from
the object doesn't know where to go and
that's called fragmentation so in order
to to be able to come back to
fragmentation what we can do is we can
compact so we can move all these live
objects into the lower end of the heap
such that we have enough space to
promote objects into so that's called
compaction and most other GCS do that
they all of the young generation is
compacting and I'll talk more about the
generations and stuff like that which is
something to keep in mind the other
thing is reducing allocations and that's
one of the things is improvements to
string like I said reducing allocations
or deduplication again with respect to
strings so we'll talk more about that
here and then compressed headers so like
I said a Java object will have header
and body compressed headers are very
important with respect to with respect
to the the storage right so it's very
important with respect to the footprint
and we'll talk more about compressed
headers when I talk about compressed
hoops footprint and compilation we have
inlining inlining is one of the most
important benefits of dynamic
compilation because you have all these
remember the profile guided
optimizations you have all this
information so an adaptive optimizer can
take that into account and based on
various thresholds it can do a great job
of in lining and the inlining has been
started to the core by hotspot virtual
machine engineers so so it's one of the
best engines that performs really good
in lining and in lining you can do with
intrinsics as well so we'll talk more
about that if we get chance but let's
talk about escape analysis and D
optimization so escape analysis I have
an example actually I have a flow chart
with respect to escape analysis
what is the optimization so when I was
talking about - your compilation we are
going through the five different levels
and eventually we're getting to this
optimized level what happens to all this
code that's generated it stowed
somewhere in code cache it's taking up
space in code cache if your compiled at
level four which is a most the highest
level what about all the code that was
at level 2 or level 0 or whatever what
happens to that
well that means to be reclaimed that
means it's called D optimized so
basically in the code list gets D
optimized and then zombied and reclaimed
so that leaves more space in your code
cache so that's one of the optimization
that adaptive optimizers can do so it's
coming to escape analysis so in the
simplest form you need to just figure
out if it's able to escape if
allocations are able to escape the
method or the thread right so how do we
figure that out it's not stored to a
static field or a non-static field of an
external object so if it's an external
object and it's a non-static field there
it's still an external object so it's
it's it's escaping right what about
being returned from the method of course
it's escaping then what about passed as
a parameter to something else that's
escaping then so these are the things we
check for so remember I mentioned about
flocculation so that's that's a benefit
of escape analysis first of all if the
allocated object does not escape the
compiled method and is not passed into
the parameter then we can remove the
allocation itself and and and then
remove the lock as well so we can keep
instead of if stuff performing the
allocation we can keep the field value
in registers right so that's a beautiful
optimization because you're not taking
any space on the heap at all right this
other thing we can do too so what if it
was passed as a parameter we still know
that we can
form la collision so we can get rid of
the lock and then we can use some
compare instructions to do more
optimized path so escape analysis I
think was introduced in Java 7 but it
wasn't really functional and Java 8
onwards it's really fun it's really
doing a good job so if you have not
tried java age and escape analysis
please do go and try it because it's
it's not buggy as it was in Java 7
what about footprint and runtime ok so I
was talking about compress to go oops
all the time and I was talking about
compressed headers so what are
compressed tubes we have 64-bit hardware
I was talking about Opteron from AMD
that was like first 64-bit hardware for
x86 architecture what that helps with is
having 64-bit operating system 64-bit
applications and the java virtual
machine also has 64-bit pointers when
it's working on 64-bit OSS because you
can access more heap but because of that
everything gets take space right cut
outs instead of 32-bit pointers you have
64-bit pointers so what if you did not
need that much heap what if you needed
heap up until before Java 8 it was less
than 32 but after Java 8 it's less than
64 what if you could fit your your your
allocations everything into less than 64
gigs of heap then you can take make use
of this optimization called compressed
oops and class pointers okay and I'll
talk more about that soon what other
optimizations are with respect to
footprint and runtime code caches I
mentioned briefly about code caches so
with many I was talking about D
optimizations so all your made all your
compiled code goes and sits in code
cache okay and that's one of the
optimizations so basically you can reuse
it so you can you're it's saving
footprint because you want to keep this
code or
but it's also a cash it serves as a cash
so he can so it's optimized and classic
the sharing I already mentioned that so
let's look at compressed oops and
compressed class pointer so Java object
remember I said header and body right so
what's what's in the header so a Java
object header will have mark word which
is native size to basic based on your
native pointer size and a class pointer
so and and the class pointer is what we
compress when we use compressed ripped
optimization now if this was an array
object then it would also have a rail
length okay as a part of your header
with compressed oops and compress class
pointer optimisation what we are
achieving is is compression of course of
the object pointer and the class pointer
and it's done by using three bit shift
how can we do that okay let's talk about
the three bit shift first we can do that
because all Java objects are eight byte
aligned okay that's a fact all objects
so when you're when you're allocating it
on the heap their line eight bytes so
that means that those three bits are 0 0
0 so we can use those to get em up to 32
bits started to get gigabytes of
addressable space right to raise to 35
is equal to 32 gigs ok so this is what
it means if if you write it down so you
have the wide which means your
full-sized ordinary object pointer and
you can go to narrow which means
compressed by shifting it left and then
you have a field ops the offset to
access the exact field that you want so
so narrow oops is when you want to store
so you're compressing so you utilizing
more space and then why do piz when you
want to read so when you're reading and
you're reading it a proper 64 bit and
when when you're
during it you're storing only the with
three-shift
so you're you're compressing it
basically okay how does that help us
so if you have less than four gigs of
heap you are you're why do is narrow OOP
remember because it's to raise to thirty
thirty-two so you already get that you
already get that that's for free it
depends on the heap size what about if
your heap size was greater than 4 and
less than 28 days
that's called zero based and then you
need to left shifting you don't do it
for by yourself the the virtual machine
does it for you and you get that for
free also so so when you are 32 gigs
you're not gonna get that so you have to
keep that in mind it has to be less than
twenty eight gigs because of certain
calculations and other overheads into
internally so when people talk about
compressed
oopss up to 32 gigs actually it's only
up to 28 cakes so that's something to
keep in mind when you're when you when
you just need you need the compressor
optimization but you're going up to 32
you could try to reduce your heap to 28
cakes and you will get that for free now
what if you just couldn't reduce your
heap it has to be between 28 gigs and 32
gigs you will still get the optimization
but then you'll have to use the field
offset and basically the JVM is doing it
for you again but it is maybe a little
more little less performant than the
previous one so try to stay within 28
gigs if you can or at least try to
figure out if you could reduce your hip
up until 28 gigs with Java 8 what we
could do is shift the alignment the Java
Virtual Machine by itself could shift
the alignment to 16 bytes remember I
said it was 8 by the line the object say
by the line if we shift the alignment so
basically now instead of 3 bits you have
4 bits now that you can play with so if
you could shift the alignment change the
alignment to 16
you have four bits to play with so your
addressable space now is less than 64 so
but remember because the alignment
changed so we have more wasted space so
do your experiments if you need to go
above 32 but you can still be within 64
then try to do an experiment in Java 8
and try to see that or you could change
your lineman by itself that there's an
option to change object byte alignment
change the alignment and see if
compressed and enable compressed oops
and see if it works for you so it's the
same thing instead of 3 bit shift you
have 4-bit shift because of the
alignment has been changed okay so the
same table just rearranged so we have
object alignment we have offset
requirement and shift by so if your heap
is less than 4 gigs you get it for free
there's no shifting necessary nothing
between 4 and 28 you get you don't need
offset again it's called zero based so
you it shifts the JVM shifts it for
years for you and you get that it's not
as expensive as the others less than 32
you will still get it but you need
offset again the JVM will do it for you
but it could get expensive less than 64
on Java 8 the alignment will change so
it may get very expensive because you're
wasting space and then please go and
experiment and figure out if that's
affecting you in a in a bad way okay so
what compressed oops in Java 8 you also
get something called compress class
pointers I'm not going to talk about
prom gym Jen removal but I have an
article out there on info cube that you
can go and read about it but with Java 8
we also went and did something called
perm June removal so now you're whatever
used to be in the information now goes
and resides and something called meta
space ok one of the benefits of that was
that you could also do compressed you
can also compress
pointers remember when I spoke about the
hitter I mentioned about that class klas
s so so if I took this oh sorry I took
this from this particular presentation
by John masa meet sue and Colleen
Fillmore they did they presented the set
Java when John Muhammad sue was the
project lead for meta space for
permission removal so what it shows here
is when what is important for us to know
because I'm not going to talk about
function removal is now chromatin is
meta space and then we have the
compressed class space which is also a
part of meta space so in compressed
class space you're storing all those
compressed classes remember I said about
the object header the class those are
compressed and you're storing that there
so it saves your matter space which is
basically a native memory meta space is
outside the heap and it's taking it's a
part of your native memory so that's an
optimization that you get in Java 8 when
compressed oops are enabled ok something
to keep in mind if native space is of
concern ok so now moving on to
throughput and the execution engine in
run time
ok throughput in garbage collection so
to remember to stop the world events
that garbage collections all do so often
what if it was just single threaded that
would be really really bad especially
depending on how much work it has to do
right so there's an optimization which
you get it's called pedals
GC threads it's by default it's 5/8 of
your processing threads available to you
but you can increase it to as many
processors you can give it so if your
application threads are using 16
processors you can increase barrel GC
threads to 16 because that's to stop
their world time that's the time that
the GC spends by itself chugging along
and during the GC work so those are
called so those GC threads are are
multi-threaded and they do work stealing
so basically
the threat so this is basically a
queueing theory everything done right
and I think I may talk more about it for
some time but let's move on to something
else with respect to throughput and GC
so in order to maximize throughput one
of the things that hotspot does is and
it most of the hotspot GCS they do that
it's they have multi generations which
means there is the young generation
where most objects get allocated in two
and then they're aged in the young
generation and then they're promoted so
for your long-lived objects which may
and should actually form your live data
set that those will go get promoted into
the old generation and they'll stay in
the old generation most of the hotspot
GC algorithms are optimized for the old
generation most of the young generation
algorithms are similar actually all of
them are similar if it's a multi
generation a squat garbage collector all
of the young generation algorithm is is
the same okay and I'll talk more about
that too multi-phase marking I'll get
when I get to that I'll talk more about
that too and compaction there are two
ways now with respect with the g1 GC
with the introduction of G 1 G C now
compaction can does not have to be
monolithic so in the past old generation
compaction was monolithic which means
the entire generation was compacted so
usually if you have used federal GC in
the paths in in your past life then you
would have seen that the stuff there's
one stop the world pause that takes 23
seconds or something some absurd amount
like that
so with G 1 GC if if your application
has been optimized or if it has it's
been following warm up phase and steady
state G 1 GC will do incremental
compaction and it's adaptive in a way so
especially more adapted with Java 9
again depending on how much time we have
we'll talk about that which probably we
don't have any time so
probably not sorry but this these slide
decks are self-explanatory I hope and if
you get a chance please get them
whenever it's available I'll do that
so I'm gonna quickly go through this and
talk about intrinsic something intrinsic
you know people do use these kind of
optimizations without even knowing so
one of the things is when you have the
JIT compilation code you actually the
JVM will put in hand optimized code so
basic somebody went in there and
optimize it for example array copy
somebody went in there optimized system
dot array copy and and so basically like
your start you get benefits of your
static static compiler so when I read
copy inside a compiler is optimized it's
the same way just that it's hand
optimized for for that particular
sequence and intrinsics can be inlined
we already spoke about the compressed
troops in stuff like that sorry
so one thing I wanted to talk about with
respect to the string so as I mentioned
about strings and footprints and how I
don't cover this anywhere else so
strings and footprint the way we talk
about Java objects right we have the
instance and then we have the object
itself and there's a backing away
because string needs to be backing
Arabic a string is string right so up
until up until Java 9 the backing array
was a care array with Java 9 with string
density efforts this is now not care
anymore
it's a byte array so you're saving space
right there ok I don't have time to go
into that but if you get a chance and if
you want to look at string density
optimizations I have various talks on
those too and there may be some articles
out there there's a jdk enhancement
proposal Jeff go ahead and read that and
that's a very useful optimizations
because apparently strings make up like
about 29% of your Java heap like live
data set so you will benefit from this
optimizer
without even you knowing because there's
nothing that you have to do but just
move to jump 9 maybe other than doing
all the modular things you know I'm not
gonna go there but point is that these
optimizations have been made there's the
other thing about in turn strings also
so what if that was in turn so if you
use in turn strings then you get this
pool of strings here in your Java heap
and each backing array and the object
itself will reside in the pool of
strings and within turn strings you know
that if string one was that in turn is
equal to string 2 that in turn then
string 1 equals string 2 right so what
happens what is the optimization that
you get for free you get if string 1 and
string 2 where the set what equals then
you just point the instance just goes
directly to the same thing so you don't
have to allocate string 2 knowing that
they're equal okay
so similarly with respect to G 1 G C and
Java 8 this optimization is made in Java
8 you don't even need the strengths to
be interned what you need is just use G
1 G collector you used you and collector
you have those two strings and if
they're equal
it'll just directly point to the same
backing array so you got that for free
so but you need to be using G 1 G C
garbage first garbage collector and Java
8 okay I think I can still go on because
because I have a lot of slides like I
mentioned but if you guys need to leave
all you need to catch me later please go
ahead and Twitter at Twitter or write to
me you know at my company's email
address talking about generational GCS I
wanted to talk about the generational
principle so which is young generation
and old generation I was talking about
that young generation is Eden and
survivor spaces and the way it works is
very simple all of the young generation
is divided into Eden and survivors so
you have survivor 0 and survivor one
most allocations
happen in Eden and then all the live
objects after a collection are moved
into one of the survivor spaces and
that's kind of like a sieve where you
age these objects okay based on this age
threshold which is inbuilt into the
algorithm you and you can change it on
the commander and of course you you will
get these objects you that can get
promoted I think the default is like 15
at age 15 they get promoted but it may
change different GCL gotham's may have
changed their algorithms so after the
aging threshold is crossed you generate
it's also called promoted so if I had to
talk about the optimizations and
classify them based on allocations or
Reclamation's that's how I would do it
but I already spoke about those most of
them so let's look at some allocations
optimizations so I mentioned about
thread-local allocation buffers it's
based most of the allocation because
most of the allocation happens in Eden
so most of the allocation optimizations
happen in Eden right so your eatin space
is divided into this allocation buffers
and based on the number of threads you
have those allocation buffers they will
be sized based on the allocation
patterns of your thread and the number
per thread also depends on how fast a
thread is consuming them okay so in this
case I have five tea labs because I have
five threads once you Eden gets full
survivors happen there's an optimization
that is working with parallel GC for a
while now and this was based on the
experiment that I did with when I was
working with AB Tron so after on was not
only 64-bit but also it did Numa was a
new Numa architecture which is
non-uniform memory access and that's how
it looks so you have different
processing nodes and they each have
their memory controllers and each
controllers controlling
memory bank DRAM bank so if if there is
something happening if the process is
running on node 0 but it needs to access
a memory bank on node 3 it has to go one
hop and two hop so like that you know or
it could go from node 0 to node 1 to
node 3 so there's 2 hops away and then
if it was on the same one that's 0 hop
and then no 2 would be 1/2 away so that
means that you're accessing memory in a
non-uniform memory
previously all of the processing nodes
would go to a single memory controller
and to the DRAM bank so everybody has
uniform access to memory well with this
architecture it changed the way memory
could be accessed so again with Numa
allocator now the JVM can have eaten
space in sort of regions such that you
have area for node 0 you have area for
node 1 so if a thread was running on
node 0 it'll do its most of the
allocations locally right so the the
thing is that or the hope is that the
thread is not moved there's not too much
of context switching so that way you get
the Numa benefit so you're going on the
local hub there's no hop basically you
don't have to go to a remote thing but
what happens with all generations or
with survivors they are interleaved so
you go chunk wise so little chunk here a
little chunk here a little chunkier
little chunk here so the Eden is local
first it's based on the first search
principle whichever thread touches
whatever local area Eden space will be
allocated out of that area and then
others are interleaved so that's how
Numa allocator works it's only available
on parallel GC g 1 GC will get it to
when when there's time to do it
so there is an RF e 4 G 1 GC to get Numa
allocator as well but it hasn't been
implemented yet fully these are the
optimizations you can go ahead and
research that I was going to talk about
so like I said similar GC algorithms for
generational OpenJDK hunt span but the
only difference is in the old generation
algorithms okay so we have three
different kinds that I want to cover
here the first one is called parallel GC
it's also what I could like to call
throughput Maximizer so it has
multi-generational team of course young
and old generation it has multi-threaded
workers remember the parallel GC worker
threads count that I mentioned
compacting collector it's a monolithic
old generation compacting which means
the entire old generation will be
compacted faster Reclamation's were have
P labs which is promotion local so
similar to T labs we have when we are
promoting the garbage collection can
make use of so T labs are by threads by
allocating threads right your
application threads basically P labs are
by GC threads so your promotion lamps
are basically a GC threads are promoting
so these are P labs they're only
available in the old generation or maybe
in the survivors to I don't know for
sure sometimes it may be depending on
the GC algorithm the other one is CMS
it's deprecated in Java 9 I think the
difference is again in the old
generation algorithm so you still have
multi-generational and you have
multi-threaded workers especially for
the young generation you have some
multi-threaded workers for the old
generation where some of them are stop
the world ok
and it's multi-phase marking so it does
so like CMS's concurrent mark-sweep
there's no compact so there's no
compaction and that's why CMS is prone
to fragmentation CMS has other issues
which it goes to concurrent mode
failures and other issues so that's
right one of the things is like the GC
engineers don't want to support it
anymore so so what's happening now
Google and Twitter and others are like
doing these they're doing compactions
adding compacting algorithms to CMS so
because most of their workload depends
on the way CMS works because one of the
things is that it's it's does in space D
allocation so it does sweep really fast
right so
so that's what that's the CMS algorithm
that's why it's latency-sensitive
because there's no compaction you're not
compacting there's no stuff that well
pause with respect to that compaction so
youyou you're doing quick just
deallocating in space so sweep is pretty
much free with CMS g 1g c comes along
and one of the optimizations one of the
major thing about g on GC is
regionalised heap so you know your unit
of work with respect to garbage
collection is reduced previously
remember when I said all generation is
monolith to be collected so your unit of
work is like the entire old generation
you couldn't divide into chunks I mean
you could internally because you do the
pages and the pointers the card tables
and stuff like that but when you're
collecting you want to take a monolithic
old generation and you're going to
collect all of it with respect to G 1 GC
the unit of work is a region so
basically you can add your young
generation plus one of the old
generation region and that's all you
have to collect so that's why you can do
it incrementally so those are called
mixed collections so basically mixed
collections will have one or more
regions out of the old generation and
basically that's that's that's the way a
g1 GC works it's more the algorithm
adjusts itself you know based on your
past time gold your live data set and
and the size of course of your heap so
these three things helped the algorithm
tune itself in a way and with Java 9 it
is it has adaptive thresholds which
means that you're marking can start in
an adaptive fashion like it used to be
in CMS and that so if I had to do some
draw where I could do say throughput
Maximizer versus latency sensitive
sensitive GCS G 1 GC would be in the
middle because it takes care of you it's
takes character if your throughput as
well I think that's pretty much it
I was going to talk about what
compilation thresholds are which you can
just look and read from here and to your
compilation different levels of
compilation and yeah so the the adaptive
optimization reaches with respect to the
hot squat virtual machine is something
very important because we do profiling
of hot spots but we also do adaptive JIT
optimization so if I had to draw it it
means that I could go based on my tiered
compilation levels I could be doing this
for awhile right I could be gypped
optimizing myself and that's that's the
beauty of hot spot what's the machine
eventually I could just give up and go
back to introvert because I don't think
it needs to be optimized or the or the
method is not hot anymore right so you
want to go back to the interpreter so
that's adaptive optimization that comes
for free with hotspot JVM and dynamic
deoptimization
why do we need it we have dependency
invalidation classes getting unloaded
uncommon path misguided profile
information of course I think I should
just walk out because I know that's next
our next speaker may be here somewhere
I'm so sorry I took a lot of your time
and thank you guys thank you for staying</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>