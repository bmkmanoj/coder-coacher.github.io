<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;A Frontend Server, Front to Back&quot; by Zach Tellman | Coder Coacher - Coaching Coders</title><meta content="&quot;A Frontend Server, Front to Back&quot; by Zach Tellman - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;A Frontend Server, Front to Back&quot; by Zach Tellman</b></h2><h5 class="post__date">2016-09-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_1rh_s1WmRA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so Fitbit has monolith and I think that
most you can probably guess how he got
here about nine years ago our CEO made
the first commit and we've been building
on it ever since
and at every step that kind of was a
pragmatic decision right because we had
all of our operational stuff figured out
we had all the deploy all of the metrics
all the alerting it was all there and
for each incremental change we made it
didn't really make sense to reinvent
that from scratch so we kept building
and building and so now we have this
million lines of code that just kind of
sits there and we want to fix this we
want to make this better and I joined
about a year ago to help with that
effort and the project that I was
working on was sort of one of the
initial steps that you take when you're
breaking apart a monolith which is
writing a front-end server and I use the
term front-end server because in tiered
service sort of deployments anything
which is upstream of you which calls
into you as a front-end and anything
that you call into is a back-end and of
course that's relative you are a
back-end to your front-end but in the
sort of entire system there is one
server which is the front-end East right
and that is the front-end server and
you've also heard this call the Gateway
server maybe some other things but I
like front end so the front end server
exists so that when we start to cordon
off pieces of code and we start to kind
of pull them out into their own little
piece we can route traffic to it right
this allows us to start to kind of think
about these things a little bit more
modularly and the role of our server is
to route the traffic right make sure
that it goes to the right place the
right services receiving requests it's
to validate traffic make sure that the
traffic that the requests are
well-formed and this can mean that you
know they conform to the HTTP spec but
also that they are authenticated that
they aren't kind of pathological in some
way it also will shape or prioritize
traffic right if our back-end is sort of
under load is not able to handle all the
requests it's our job to decide which
requests go overboard first and so the
way to think about the server which is a
very small focused piece of code
is that it's fulcrum right it is
positioned very advantageous Li with
respect to the rest of our code in a way
that can sort of affect great change and
when building the server we really want
to have four properties sort of in order
of importance that'll be transparent
stable fast and extensible and by
transparent what I mean is that we can
see what's going on inside of it while
it's running in production that means
that we need to have a lot of metrics
it also means we need to have a small
number of metrics that we need to pay
attention to regularly and it also needs
to have a structure that allows us to
map the code onto these metrics right if
something is going wrong we need to be
able to look at the code and understand
or at least build a hypothesis is what's
going on quite quickly we want it to be
stable right if we deploy it and
everything seems to be okay that should
remain true even if we look away for a
bit we want it to be robust to
pathological requests right we want it
to reject malformed requests we wanted
to reject sort of malicious requests
like a slow loris attack where someone
sends us one byte every minute
we want it to also be robust to lots of
well-formed traffic right we want it to
be able to shed load if the back-end is
unable to deal with the sheer volume of
requests and still stay up itself we
want it to be fast right we want it to
add as little overhead as possible
and we want it to be predictably so if
something is fast 99% of the time but
slow the other 1% that is far worse than
being slightly slower on average and
where possible we want it to be able to
detect issues on the backend and route
around it right we want it to actually
make the system faster as a whole than
it would be on its own lastly we want it
to be extensible this is a project which
again is very advantageous Lee
positioned it allows us to do things we
couldn't do otherwise in other parts of
the system and so it should be a Commons
it should be something that other people
and the company feel that they have sort
of the right and ability to change to
sort of fix their problems and that
means it need to be small needs to be
easy to understand
it also means though that we need to
make sure that it is inflexible right it
is rigid with respect to the other
properties which outweigh this right
don't want it to be easy for them to
make it unstable or slow or opaque but
for everything else we want it to be
relatively flexible right it should be
something that sort of indicates how
it's meant to be changed and when we're
thinking about all these properties the
sort of key thing that we should keep in
mind is that designing systems we don't
just look at what sort of the day-to-day
traffic levels are and say we need
something that can handle this right we
need to actually think very carefully
about what is the utmost craziest
situation that we want to be able to
handle right and in that crazy situation
what do we expect it to do and what we
expect it to do if we go just over the
edge right does it fall over are we dead
are we going and just rejecting all the
traffic like what are these scenarios
and these are the questions that shape
the code not be you know we have X
number requests a second on average
every day and so one of the first
decisions that we made which is shaped
again by this sort of question of
extremes is to make it an async server
right this is sort of a bit of a
religious issue in the Java ecosystem
because you can do that with or you can
have a Freddy request you can have you
know this sort of more complex async
model which has sort of less costs
associated with each in-flight request
that is handling and we chose us here
not because it's the one true approach
we chose it because we don't control the
application boundaries either now or in
the future right we are providing a
service to the other parts of the
company and it's not our job to dictate
to them what they can and can't do right
how many requests in flight they
actually get to have and you know the
deck was a little bit stacked here
because when I joined they'd already
written a prototype using Eddie and they
hired me in fact because I have years of
experience using Nettie and so it's
again not necessarily the most objective
set of decisions that we made here but
it is I think a reasonable one right
most high-performance networking sort of
projects processes sort of frameworks
are all built on top of Nettie within
the sort of JVM ecosystem and so this
helps us in some ways right it makes us
fast because Nettie is a
high-performance networking layer and it
makes us stable because
able to handle greater extremes of
traffic
unfortunately it makes us quite a bit
less easy to understand right stack
traces are the foundation of the JVM
sort of ecosystem in terms of
understanding what's going on you have
the ability to stack dumps using J stack
you have a whole host of profiling tools
and all of these things sort of assume
that your threads mean something right
that they are a meaningful narrative
about what's going on inside your code
and the whole async callback mechanism
kind of just stomps that into nothing
and so that's a problem right maybe a
little bit less concerning is the fact
that the callback sort of model is
harder to reason about right someone
coming in from some other team who
hasn't internalized this sort of
approach will find this a little bit
difficult to reason about and so the
sort of first thing that we need to
think about is how do we get back the
sort of ease of understanding right the
the give back something that's sort of
like a stack trace and the approach that
we used was to create a finite state
machine and I want to be clear because
we use this sort of term to describe two
different things one of which is the
formal automata theory mechanism that
underlies regular expressions and then
there's another sort of much more
informal thing where we draw a bunch of
boxes and lines and use that as sort of
a broad spec and we're talking about the
latter and so this is a complete finite
state machine all right you start a
request and then we either succeed or we
fail this is complete this covers all
possible you know outcomes but again
what we want to do is capture sort of
the nuances right if something fails you
want to know why fails if something
succeeded we want to know how it
succeeded and so we can expand this out
a little bit right in the course of
handling requests we first call out to
an auth service which tells us whether
or not the request is allowed to be let
into the rest of our system if that's
exceeds when we make a call to the
back-end service if F succeeds then we
succeed and so in each of these cases
these can fail though right we may not
actually get a request we may not get a
successful outcome from or call the off
servers when I get a successful outcome
from the back-end service right and so
now we have more failures and more sort
of intermediate nodes on our way to
success
but then failure is sort of a broad term
right
maybe we actually want to know why we
failed how we failed so we can actually
start talking about like timeouts right
either the service of recalling didn't
actually respond or something else
happened we don't know what but you know
something that's sort of the catch-all
case and so you can keep on expanding
this out
you know and sort of further refining
and being further sort of granular in
terms of all the possible contingencies
and that's what we did and you know you
can't quite read this I think from here
but you can see here that along the left
there is this sort of box of the happy
path right all the things that we do on
our way towards success and hanging off
to the rider a bunch of red boxes which
represent all the ways that we can go
wrong and I'm just going to kind of step
through this very quickly because I
think you know it's worth sort of
understanding exactly how granular we
were so at first we have just a
connection waiting right for a request
to come in and then the request comes in
or doesn't because we time out or
because the connection closes before we
receive one and once we have a
connection we try to figure out which
machine within the off service should we
call we get a route if we're able to get
a route then we try to get a connection
to that particular machine through our
connection pool if we get that
connection then we go and actually send
the request once we've sent the requests
we wait for the response once we get the
response we check to see whether or not
it was authorized if it was and we
continue on to make a call to our
back-end service and we go through the
same sort of song and dance again right
we get a route to a particular machine
we get a connection to that machine and
then we send the request but here things
get a little bit tricky because we've
sent the request which is just the first
part right it's the thing that has a URI
has the headers we haven't said the body
yet and so now we have this sort of
issue where we need to first send the
body up to the back-end server right it
needs actually get the full request but
it has enough information to start
sending us a response and in fact some
api's are structured such that it will
take pieces chunks of the request and
start sending back chunks of the
response in line and so if we don't
allow these two things to happen
concurrently we can deadlock right we're
trying to send more of the body back to
up to the back
server and is trying to send more
response back to us if we were to order
these right upload the entire body
before we start to handle the response
things to go very badly and we again
don't know what the application might
want to do this is not something that we
do anywhere within Fitbit but we have to
allow it or years later someone's going
to be very confused so over here we have
the sort of sidelined upload of the body
and then back here we're waiting back
for the response once you get the
response we forward it back to the
client
we then forward the response body and
finally we're done right so easy and so
again this is a very informal thing this
has no ability to go and generate code
this is no real mapping into our code
base and so we have to sort of invent
one and the way that we did this is
using what we call a passport and a
passport basically allows us to mark
each time we advance within our state
machine right we stamp it each time with
the states and the time stamp and that's
literally what we do within the code we
have a thing that says we're marking
that we are sending the back-end request
body and then still later within a
callback we say we have sent the
back-end request body and so this is not
something which is sort of a provable
implementation of this right again this
is very informal but we have now this
sort of one-to-one mapping right
something that we can go and sort of
trace through the diagram that we
generated and sort of map it to what's
going on inside the code and there are
few other sort of abstractions that we
need to look at to sort of understand
all the pieces that they fit together we
have a thing called a router which
routes given a request it tells us which
machine we should actually send that
request to you'll notice here that
there's also a parameter called an event
loop this is sort of a nutty thing where
Neddie does parallelism by creating an
many single threaded event loops and we
want to make sure that our requests for
all the various sort of network traffic
does stays on that event loop and so we
have to provide this as an additional
piece of information we also have a
thing called a channel pool we call it a
channel pool because many calls
connections channels you see here that
we have an acquire call the acquire call
has given a service route and returns a
future that will yield an HTTP socket we
also have a release which gives a socket
back to the pool for
in HTTP socket is basically what you
would expect it is kind of like a TCP
connection except that it speaks at a
higher level sort of protocol we can get
to the next message off the connection
the message is sort of the first part of
that request everything but the body we
can forward the content for one socket
to another that is actually forwarding
the body and we can write an arbitrary
message into the socket and I want to
call out specifically here that we have
explicit mandatory parameters here that
give bounds to each of these methods it
says how long they're willing to take
and in the case of the Ford content how
much actual bandwidth we're willing to
use right these are not optional there's
no way to opt out of this you have to
define the boundaries of your system
because again that is what defines the
behavior of everything and so we have
this sort of life cycle right that uses
these sockets right we have a connection
that represents our incoming or client
connection we have the auth connection
we have the back-end connection we have
this thing that sort of threads them all
together and we call it a request state
machine and a cross state machine takes
this router and the channel pool in the
passport and then when we call a knit we
hand it the initial client connection it
just kind of goes from there right we
are no longer interacting with it it's
just sort of driving itself forward and
when it's finally done it tells request
passport that it is done and the request
passport is then responsible for sort of
generating all the metrics right this is
sort of a critical separation here
because I think that in a lot of code
which has sort of a rich set of metrics
which you want you end up in tree
leaving that with all your business
logic and oftentimes the metrics code is
much more verbose than your business
logic because you're trying to track a
bunch of different things that are in
flight and I think that this has sort of
problems for a lot of reasons we want to
be able to look at the business code and
not have to think about all of these
sort of side channel communications that
we're doing so also very hard to verify
that we're doing it right if we have
some sort of interval as we do where we
track for instance the amount of time
that elapsed between us trying to
acquire a connection to the back end
service and the between the time you
actually acquire it but we also want how
much time it allows between trying to
acquire and actually completing the
request these are both useful for
different reasons if we were to get this
if we were to measure the wrong interval
how would we know what is our baseline
right it's easy to get this wrong and
never realize it and so having this sort
of separation having something which has
a data representation of this state was
advanced to at this time and being able
to sort of reason about the intervals of
that just that pure data thing not this
sort of imperative generate this metric
right here in line with our business
object it's a really helpful thing both
in terms of simplicity of the code and
the correctness of our metrics so the
point of this talk is to kind of try to
give a slightly uncomfortably close look
at some of the aspects of this right not
because necessarily we think that what
we've done is amazing and you know
transcendent and you know you all should
take a look at this but because I think
that we don't talk about these things
enough right we don't actually have sort
of practical hands-on examples of you
know code that actually runs in
production and so I'm gonna show you a
bit of this code again with the caveat
that I don't think that this is anything
special and I think that there are
things which are trade-offs that I
wouldn't defend to you know the death
but here it is this is a function called
send back end request this is what
happens once we've actually received the
authentication we want to forward that
request and so it'll go and it will
acquire connection to the back end and
it will write it so first at the top
here we check to see that the client is
actually connected if it isn't we go and
we just clean everything up and get up
because what's the point if we are still
connected though we go and we mark that
we're going to get the route and we get
the route if there is no route then we
really can't do much and so we return
that we can't proceed and bail out but
otherwise we tell the past for what the
route is that we selected we then check
to see whether or not the request to the
client was keep alive so we know whether
or not to close the connection once we
actually have completed all this and
once you've done that we go in the
overwrite parts of the request because
there are things that are specific to
our connection to the client that we
don't want to transitively sort of carry
over to our back-end connection we don't
want upgrade headers and other sorts of
things to carry through there and we
don't want it to ever be anything but a
keepalive connection right we want to
reuse our connections that's how we get
sort of better performance then we
define a
back which will take a future
representing these sort of connection
that we're going to get to this server
if it succeeds then we go and we mark
that we've acquired the connection we
then start to write to that we send the
request and we register another callback
which marks the success of sending that
request if we are successful then we
mark again on the password that we have
and then we fire off these two different
parts of our state machine recall that
we are now both sending the body up to
the back-end service and receiving the
response from the back-end service and
so we do each of those in turn if we
failed to write then we have to return
an error and sort of bail out and then
finally if we failed to get a connection
at all we have to handle that and all
those sort of different ways we could
fail to get that having to find that
callback we now mark that we are about
to get the connection and then we
acquire it and add our callback and you
know the request state machine is a
number of those sorts of functions about
300 lines of stuff that looks roughly
like this and you know this code is not
I think the prettiest that you've seen
right there is a thing well publicized I
think within the node community called
callback hell and it roughly goes like
this we have a callback and we have to
handle the error case in the success
case but if we do succeed then we have
to handle some other nested thing which
you know has an error case in the
success case and inside that success
outcome we have another callback and you
know this can go arbitrarily deep right
you can have arbitrarily an arbitrary
number of closing curly and round braces
at the end of this big scope and it's
not nice it's hard to reason about it's
hard to know what kind of context you're
in when you're looking at the code
indentation is not really all that
helpful here and so there exists a dozen
or more libraries that try to make this
better
right you have these sorts of promise
Combinator's which rides some sort of
method like then which allows you to DNS
them allows you to linearize all these
callbacks and it makes it cleaner right
by having just a single success callback
and it treats the air as as sort of this
implicit thing where if anything air is
out it will just short-circuit and pass
that through to the bottom and this
makes it simpler undeniably but what it
also does is it sort of
conflates it munches together all the
possible errors it says either we
succeed or we failed for some reason
right and that runs could entirely
counter to our desire to understand
specifically why something failed and so
if we were to try to reproduce this we
would have something that more looks
like this right because there is an
inherent complexity here inherent branch
enos that we have to capture we can't
just sort of elide that we can't just
sort of you know cover over it because
it makes our code more complex and this
is not to say that we couldn't create
something that still looks like this
something that sort of makes us a little
bit cleaner makes us a little bit less
sort of you know callback hellish and I
stand before you as someone who's
actually written a number of
abstractions over sort of async things
within the closure ecosystem and so by
all rights I should be the one sort of
you know triumphantly showing you these
abstractions that I've made here that
make the code very beautiful but again
this is about 300 lines of callback code
and the abstractions if they're going to
make that cleaner they have to make it
really clean and they have to be really
really simple to understand because it's
just not that much code right you know
we don't want to build a tower of
abstraction to just you know make this
tiny little surface area of our code a
little bit easier to understand that's a
poor use of my time as an engineer's a
poor use of anyone's times trying to
understand the code because they see
this code and they're like okay I kind
of get what's going on but how does this
look under the covers and all of a
sudden there's this enormous new set of
kind of terms and concepts and
implementations that they have to
understand it so we decided to sort of
leave it as is write the request state
machine is you know about 300 lines of
callback e stuff and some other
supporting code I'm it's messy but
pragmatically so right we you know kind
of decided that that was a worthwhile
trade-off to make and it's informally
specified by a graphics file a dot file
which we use a generative image which
you know we try to keep sort of more or
less in sync with each other
and so thus far I've kind of talked to
you about how a single request threads
to our system right but that's really
not the entire story because we have
tons of these requests flowing through
us all the time all the time and we have
to understand how these things work
together right how they
sort of shape the system in tandem and
there is a thing called queueing theory
it's a branch of Statistics it is
largely concerned with sort of proving
formal properties about systems that do
not resemble the systems that we build
but there are some insights that can be
sort of gleaned from this and rather
than talk about queuing theory I want to
tell you about a pumpkin Carver at a
State Fair and in deference to my lack
of any sort of artistic ability we're
going to be pretty abstract here so the
pumpkin carvers are the X pumpkins are
the O's lines of the queues and here we
have a pumpkin Carver doing their work
someone is standing there with a pumpkin
in line they go and you know start
handling one of those pumpkins they
finish that pumpkin and then handle the
next right easy very reasonable to sort
of think about a problem arises though
when someone realizes that there is no
upper boundary on how big the pumpkins
can be and so someone goes and drives up
and they're caged forklift and patiently
waits in line for their turn so
so then you know the pumpkin Carver goes
and starts to carve it out and scoop out
all the innards and you know slowly
starts to make forward progress and
while this is happening people keep
showing up and the problem here is not
just the people who are waiting behind
while the pumpkin is being carved the
problem is that once the pumpkin is gone
there's still this enormous line right
there's still this after effect and this
after factor can last for quite some
time and how long it lasts depends on
how much sort of excess capacity is
right how much utilization we were
having because we have to pay down this
debt now right so if our pumpkin Carver
was quite busy if they were busy 90% of
the time however long that giant pumpkin
took to carve that line that sort of you
know excess latency that has been sort
of put into this queue will last 10
times longer which you know will
probably be the rest of the day right
people will show up and they won't know
why the line is so long they won't know
that there was a giant pumpkin here but
they will know that it's taking an awful
long time to get their pumpkin carved
and you'll see this in traffic jams
right even when the accident is sort of
cleared out it still takes a while for
the you know traffic to normalize and so
one way that we can mitigate this is we
can have many pumpkin carvers handling
the same queue in this case when a giant
pumpkin shows up one of these carvers is
going to have a really you know big job
to do but the other one is able to still
keep things going right they're still
able to make forward progress the line
is not just going to kind of grow
unbounded behind that single Carver and
I want to contrast this with a different
approach where we have sort of multiple
queues right and in this case everyone
has sort of like a 1 in n chance of you
know getting behind a giant pumpkin and
pour them but the other people won't but
this still means that some people are
going to have a really bad time right it
makes the sort of average case better
but it still isn't quite as optimal
right you will find that in every case
having many many people servicing one
queue will give you the best behavior
under adverse conditions unfortunately
that's not always possible right we
can't necessarily an unbounded number of
pumpkin carvers around this single line
and so you know in our County Fair we
try to get as much space as possible
they tell us that we actually
have two spaces available one at each
end of the State Fairgrounds so we set
up different Carver's there and we set
up a line somewhere towards the entrance
to try to make sure that we're getting
an even distribution across there and so
in this particular case we can assume
that you know we have taken to heart the
fact that we won't have many consumers
for a single queue and so all of our
Carver's are busy if one of them
actually finishes they radio in that
they are ready for a new pumpkin and you
then begin the long trek across the
fairgrounds to actually get our pumpkin
carved and this is a problem right
because we have the situation where we
have a pumpkin Carver who's not doing
any work and we have a person who wants
their pumpkin carved and yet nothing's
happening right there's something that
is sort of preventing us for doing this
and this is lost through point this is
loss efficiency right you're not making
good use of our resources here and so
this is maybe optimal for the customers
for the people with the pumpkins but
it's not optimal for us it's not optimal
in terms of you know our sort of you
know money-making endeavors and so we
might sort of compromise a little bit
right we might set up a couple of queues
so that we always have at least you know
one pumpkin on deck when we want to you
know do some more work right and so now
the guy with the radio at the entrance
is in the job of just sending a small
bounded number of pumpkins to each of
these pieces at the end of the State
Fair so if someone comes in they'll be
sent to one and then sent to one of the
pumpkin carvers and so I just want to
contrast this with a different scenario
where we have a pumpkin carving factory
and in this particular case all we're
concerned about is throughput all we're
concerned about is the overall
efficiency in terms of how we're using
our resources right our workers and in
that case we love queues right we love a
big ole like sort of you know collection
of pumpkins that we can work on and if
we get too many right if you know 10
giant pumpkins show up in a night we can
go and we can you know call people and
we can spin up new workers right all we
are concerned about here is are we
making good use of our pumpkin carving
resources but the State Fair or you know
carnival sort of situation here is more
nuance right we're trying to bow
these things because there are real
people waiting for us to do the work
right they're gonna get bored they're
gonna get pissed they're gonna leave bad
reviews on Yelp and so we need to
balance their need to get the pumpkins
carved quickly where if we were just
trying to optimize that we would just
have a thousand pumpkin Carver sitting
around all the time on the off chance
that a thousand people show up at once
with our needs to make efficient use of
our resources where we are trying to
make sure that you know no one's just
kind of sitting around twiddling their
thumbs and so it's harder and it's much
more contextual right there is no one
right answer here and so just to kind of
you know break down this metaphor a
little bit the different sort of
collections of these pumpkin carvers
there are backends right these are
Hardware cores and they're servicing
sort of a queue which is generated by a
web server you know handling all the
incoming requests the entry queue and
the guy with the radio who's directing
them that's us that's the front-end
server and this thing that is deciding
where we actually want to send our each
pumpkin right each request is our load
balancing algorithm and you might wonder
what is you know the giant pumpkin a
metaphor for right and you might think
oh it's just a really expensive request
right someone just uploads a gigabyte of
data or something like that but in
practice it's not that simple because we
from our position right where we are
sort of you know sending these pumpkins
into things into these places we don't
know the difference between a pumpkin
which is really difficult to carve and a
Carver who's having a really difficult
time right there's no difference from a
pumpkin which is really really hard to
carve and someone who is just like you
know talking on their phone the entire
time or someone who's taken a coffee
break right we don't have a position to
really differentiate between those two
and so when we have these load balancing
algorithms we need distribute the work
fairly and our goal is to send work
where it can be done the most quickly
right we are only in the business of
minimizing latency all the decisions at
optimized for throughput those who've
you made elsewhere those are sort of
properties of the infrastructure that
we've already built and a load balancing
algorithm is only concerned with where
do we send the work to get done quickly
and sometimes the work the nodes we send
this work to will be slow and we won't
know why right we the engineers might be
able to look into that but we the
algorithm that is going on again this
decision does not have that sort of
perspective
and so we just have to kind of deal with
this as it comes and so when we talk
about load balancing algorithms one of
the most common ones that were you know
all sort of familiar with is round robin
and round robin is very simple we have a
counter for each request that comes in
we increment it we then take the modulus
of that and the number of servers that
we have and if it's zero we send it to
the zeroth node if it's one we sum is
the first node and so on the problem
here is that round robin is in the
business of evenly distributing request
volume and so if one of our nodes is
slow we don't really care right we just
keep on sending it the same number
requested everyone is giving and we get
this backup right and again even once
that node recovers there's now this
backlog which is going too far out last
that sort of duration of you know
whatever issue was going through and so
round-robin is very simple use a single
piece of state it has predictable
request volume we can verify that it's
sort of doing what it should but it
amplifies issues within the system
whenever there's a slowdown it makes it
worse so an alternate approach we can
use is sort of the least connection
strategy or at least in flight requests
right which tracks how many requests
each node is currently handling and
sends the next request to whichever one
is handling the least so in this
particular case if we you know have two
on each of these nodes and one on the
last then we send the next request to
that one if all of them are handling an
equal number requests and we randomly
select one of them and so the nice
property here is that if one of our node
slows down then it just stops making
forward progress and so while the other
nodes around it our handling you know
requests are opening up capacity we will
go and we will just sort of route around
naturally right we'll send work to where
work is actually happening and so the
two or more requests which are sort of
waiting on this node which is having a
bad day there's sort of a lost cause
right we sent them already right we
don't get to unring that Bell but at the
very least we're minimizing the number
of requests which are going to sort of
suffer as a result of that and so least
in-flight is a significantly better
approach at the cost of some additional
sort of complexity right we have a
little bit more state now and it's also
a little bit harder to verify that it's
doing its job because we're no longer
looking at the requests per second we
have to
get what is he concurrent load on each
of these machines how many requests to
each of them have but it does you know
minimize the impact of slowdowns but
there's a problem here actually which is
that if one of our machines is in a bad
state where it's not slow but in fact
very quick to error out right just very
quickly returns a 503 I'm too busy then
this heuristic as described would go and
just start funneling all the traffic
there which amplifies again this sort of
problem we don't want to do that and so
we want to or have to really make this a
slightly more or maybe even if n't lis
more complicated we have we have to
track both the in-flight requests you
also have to track the historical
failure rate of each of the nodes and to
track that we use an exponentially
weighted moving average which sort of
biases our value of sort of success rate
to the last one or two minutes and we
have to make sure that no matter what's
going wrong no matter how bad any
particular node is doing we're always
have at least one in-flight request
because we have to know once recovers
right we can't just you know ban it for
life because I had a bad day and this
means that we have a predictable
concurrent load but only if there
weren't recent failures and so again
this makes it even harder for us to sort
of verify that it's doing what we expect
but at the end of the day it does what
we wanted to do it minimizes the impact
of those slowdowns and outages on our
back-end so to kind of concrete ly talk
about what you know the impact of this
thing would be we have an issue with our
deploys which I think many people do
especially in sort of the Java land
where you know when you restart the
server it's slow right the caches are
cold it has to JIT and the round robin
approach again is exactly the opposite
of what we want try because we keep on
throwing more and more traffic at it and
so what we were seeing with the round
robin approach which was being used by H
a proxy previously whenever he would do
deploys are sort of P 999 latency right
or 99.9 percent I'll was nine seconds we
get these big ole spikes and we would
see that about a thousand requests over
the course of deploy would actually
timeout would be greater than 10 seconds
when we moved to least in
he went down to about 1.5 seconds we had
you know give or take no timeouts and so
this is a significant change right this
is a meaningful impact on our customers
and of course this is exaggerated by the
fact that we're looking at the top end
of this distribution we're looking at
the median values this would not be
nearly as important but this sort of
brings me to a point about metrics in
general which is you improve what you
measure right this is sort of a truism I
probably have heard this before but like
if we're if we say that we're looking at
the median latency that's basically
saying that you know the 49% of our
customers can you know just you know do
whatever they want they suck we don't
care about them right as long as we are
going and giving the majority of our
customers a good experience like we just
don't really care and obviously that's
not true right but I actually much
prefer the converse version of this
because I think that this sort of
captures it a little bit more accurately
which is everything that you ignore will
get worse right and this is important
because it's not enough just to measure
something right we can have thousands of
metrics that doesn't mean that thousands
of things are consistently getting
better in our system what's consistently
gets better or at least what
consistently does not get worse is the
metrics that we actually look at
regularly the metrics that we examine
every time we do a deploy the metrics
that we look at every day just in case
right otherwise these things will slowly
inevitably get worse and so when we were
building our you know front-end server
we had to sort of decide what are our
key metrics right and it can't be
latency because latency is not a really
a factor of what we're doing is a factor
of whatever back-end service were
calling into and so the one that we
decided on was overhead overhead and I
when I showed this initial state machine
you might have noticed that some of the
edges were blue I've bolded them here to
make it a little more obvious the blue
transitions here are where we are
waiting for someone else to do something
right we have sent requests out into the
ether we are waiting for a response and
these are not intervals that we have any
real control over
conversely everything else we do right
if there's a connection pool sort of
back up because too many people are
trying to get a connection to a
particular back-end that's time that we
are adding right we that is overhead
that we are
leading to the request if you're taking
too long to compute something that is
overhead if we have GC pauses that's
overhead right and so we want to keep a
very close eye on this because this is a
proxy for a wide variety of things that
we can do wrong and so our key metrics
are our p 999 overhead which is about
0.4 milliseconds right now and
unfortunately it's not enough by itself
because we could have a very low
overhead because we have very few
requests right so we have to look at
that we have to make sure that we're
actually getting a reasonable volume of
traffic and we could also get a low
overhead because we are just airing out
very quickly so we have to look at our
error rate but these three things
together these are what we alert on
right these are what we are paying
attention to you these are what we've
thought very carefully about what the
thresholds should be we are not just
adding random thresholds to dozens or
hundreds of metrics it's just these
three now we have many many other
metrics that we you know keep track of
but these exist so that if one of these
key metrics looks weird we can
understand why right having high
overhead does not describe what's going
on in the system it just indicates that
there's something that we should be
paying attention to and so that's really
it you know the things that I want to
you know how do you take away from this
is that if you're building a system like
this articulate what your goals are and
put them in order of importance right as
I said you know before we had all these
things that we wanted and extensibility
right the ability of some random person
to come up and start contributing is
something that we really want to allow
for but it cannot trump all the other
things we care about right it cannot
trump the stability of the system we
don't want someone to be able to kind of
come in and do that and so we have to
think about these things and make our
trade-offs and judge our trade-offs on
the basis of our goals and when we're
building a system like this we have to
understand and describe what the
extremities are what are the things that
we are willing to do and what is our
expected behavior when we sort of go
over the edge and choose your metrics
carefully because once you put something
in production this is what guides the
evolution of your system right this is
what informs what you change what you
tweak what you try to improve and above
all what you allow to get worse and so
that's it thank you
so I believe we have some time for
questions yes yes so the question was
are we using the passport as a way to
sort of model the correctness of the
system unfortunately I had to cut out
quite a bit of this talk yesterday
because I realize it was gonna go away
over time so one of the things I didn't
really talk about was how he uses to
test right because the state machine is
basically a mechanism for defining
coverage right in a more meaningful way
of interest like how many code branches
are we dealing with and so what we do is
we basically set up a test harness where
we have all the back ends and have some
proxies and simulate Network failures
and sort of tweak them and make sure
that we are following the path that we
expect we also in every log expose that
log that sort of you know passport and
what the intervals were between each
state and so that gives us a very sort
of clear understanding of you know
what's fast what's slow you know what
exactly you know how did we get where we
got yes
so the question was why do we roll our
own rather than use something like a che
proxy so this is something that I think
I meant to sort of discuss we're talking
about the goals right we route we
validate and we shape the traffic so HT
proxy and nginx and other things like
that are great at routing right they're
less good at sort of capturing the
specifics of what it is that we need
what validation means for Fitbit what
shaping means for Fitbit and there is
some extensibility there that you know
exists you can do Lua scripting and
nginx and stuff like that but like at
the end of the day you really want it's
not a big piece of code and it makes
sense to have it be very very flexible
and very sort of custom fit to what you
need because it is again in such a sort
of powerful position within the system
and if you look at you know Google
Google has the Google web server Twitter
has a Twitter front-end Netflix has Zuul
each of them has sort of rolled their
own and then except in the case of
Netflix and none of them have open
sourced it because it's not a useful
thing to open-source right it's it's a
solution to their problem and will be
developed to fit their needs and isn't
sort of an attempt at a general form of
you know the problem because that's
quite complex whereas the sort of very
narrow needs of a particular company are
more tractable yes we do so those are
where our alert thresholds are there are
other sorts of metrics you know that are
for the surrounding systems and but you
know those sorts have captured all these
sort of bad things right if there's an
error then that will capture all the
sorts of things that you would normally
sort of bundle under like how high are
500 sort of you know or 5 xx responses
and you know late and seeing other
things are there so we kind of have
looked at it and it does seem to capture
all the things that we care about we
have run books which say if you see this
then here are the other sort of
ancillary metrics that you should look
at to kind of determine what's going on
here we don't have it quite as like a
flow chart but that's sort of the
eventual place we would like to be there
right it's a very sort of you know
checklist sort of you know here's what
could be going wrong sort of deal yes
ah okay so the question was is there any
reason that we would want to use an eddy
directly rather than a higher level
rapper he used a mentioned my rapper
fornetti which is written in closure and
so we don't use closure at Fitbit and so
that is the main reason that you
wouldn't use that there are other Javal
and sort of extensions on top of that
the short answer is it's just not a big
piece of code right if you're trying to
write tons of async logic it might make
sense to kind you might get more
leverage out of these sort of
higher-order abstractions that are there
but the parts of interact directly with
Neddy are not that many and so at the
end of the day having an additional
layer of indirection that makes it
harder to reason out what's going on
into the covers doesn't give you much
like it costs a lot more than it you
know sort of benefits you yes we the
question was what do we use for service
discovery we use the the zookeeper
server set mechanism which you know is
part of the Twitter sort of you know
ecosystem of things yes so the worst
failure that we've observed I believe
was and so this is again something that
I would have loved to have talked about
it we're going slightly over time here
if people want to start going to lunch
by the way it sits outside please I
won't be offended but uh so the worst
failure was when we actually were having
out of memory exceptions so nettie uses
off heap memory to make it more
efficient to sort of talk to the
networking stack and it uses slab
allocation which has reference counting
which you know can go wrong for all the
obvious reasons and so we were trying to
track that pretty well but we have we
had this one sort of case of sourcing
and production that we hadn't tested for
and the problem was that we were using
sort of out-of-the-box metrics using the
Dakota Hale metrics library which is
getting from JMX and there was a metric
it provided there that was called off
heap memory right so we're like oh we're
tracking this turns out that that
doesn't actually do what you would think
which is tell you how much coffee memory
there is and so that value actually
happens to be like a private field under
Java ni obits and you have to use
reflection to kind of make it accessible
and then you know track that so the
worst failure that we had was that after
a fixed amount of time the Machine
started all toppling over without a
memory exceptions and luckily we have
some redundancy and one of these
machines is able to handle roughly about
50,000 requests per second and so we
caught it before enough of them fell
over that it actually impacted
production but it was a very very new
thing and then we spent a number of very
confusing days try to figure out what
was going on and like examining the
memory dumps to kinda understand what's
going on and you know another part of
this that I wanted to kind of talk about
is that you should really validate your
metrics right like if you have this sort
of thing that's telling you something
and there's some way for you to get
external validation and it's telling you
the right thing like do that it's going
to you know be worthwhile I think yes
the question was as a server do circuit
braking or bulk heading we rely on the
service discovery to do that for us if a
server goes and takes us out of the
service of the server set then we
basically stop sending a traffic and we
rely on it or some sort of other
external mechanism to tell us that that
servers off limits alright I don't think
that there's anyone thank you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>