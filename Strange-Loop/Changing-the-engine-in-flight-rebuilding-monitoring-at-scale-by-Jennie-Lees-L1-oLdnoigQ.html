<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Changing the engine in flight: rebuilding monitoring at scale&quot; by Jennie Lees | Coder Coacher - Coaching Coders</title><meta content="&quot;Changing the engine in flight: rebuilding monitoring at scale&quot; by Jennie Lees - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Changing the engine in flight: rebuilding monitoring at scale&quot; by Jennie Lees</b></h2><h5 class="post__date">2016-09-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/L1-oLdnoigQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody welcome back how is the
food coma is it good
great all I'm going to talk about
something really really fun and exciting
to wake you guys up and that is
monitoring yeah okay cool
hands up if you monitor your software
okay some of you didn't put your hands
up that really scares me hopefully in
the next 3040 minutes will change your
mind and help you understand why it's
really really cool this isn't just about
monitoring so I've been in the situation
I'm sure you guys can relate where you
go to conference you hear about some
cool new technology your minds exploding
with buzzwords and you get back to your
office and back to your desk and you're
like I want to implement all of the
things right now and you look at the
code that you've already got and the
legacy system you're supporting because
somebody left the company and all of the
things you're trying to do already and
you're like well how do I get from here
to this glorious new future where all
I'm doing is buzzwords every single day
and this is really kind of the story of
how we rebuilt our monitoring systems
piece by piece to get to that glorious
future more or less I want to start out
by explaining a little bit of context as
to why I care about monitoring so much
so I work for Riot Games out of Los
Angeles
we've been office here too and I care a
lot about games I'm a gamer we make a
game conveniently it's called League of
Legends have you heard of the game some
people yeah cool that's awesome I love
it when people have heard of it a little
bit less explaining to do so it's a
popular game we recently announced we
have a hundred million players monthly
and as a game designer 100 million is is
great right
loads of people playing this thing that
I loved and made as a back-end engineer
I hear 100 million and I think oh that's
a lot of things that can go wrong and
they do right nothing's perfect so our
game is about seven years old and some
key characteristics of it it's a very
strategic game I like to think of it as
action strategy tactics all it all
rolled into one you grab for other
people mostly it's 5v5 you carve out an
hour of your time to play and then you
play this kind of big
Paper Scissors who's gonna play what who
are my friends playing who are they
playing you get into the game and you
spend an hour making decisions together
about what to do and because it's so
complex people take it really seriously
it's a sport at eSport but it's a sport
right people get paid to play it and I
kind of wish that sort of thing I have
been around when I was in college just
like just cuz there's professionals
playing it doesn't mean normal people
don't take you seriously too I take it
seriously and you've like spent time
trying to get better at the game get
your competitive ranking up which is
what it really really really sucks when
you lose and it's not your fault and I
don't mean your teammates are bad I mean
this I hate this screen something's gone
wrong with the server's right I can't
get online I'm in the middle of a game
I'm right up to the hype DUP and
something went wrong and I kind of think
of as my job to stop this screen
happening to help us fix it when it does
happen and stop it from ever happening
again the players don't like this either
they really don't like it
they make memes and YouTube video songs
about our service and like I've been in
the glorious position where someone's
made YouTube videos about a product I
worked on and they were fantastic right
they were like fan videos that's amazing
these are not the kind of videos I want
about things that I work on this is not
my legacy I'm reading too much Hamilton
so what happens behind the scenes when
this sort of stuff happens right so we
have a network operation center this is
NASA but it's basically the same thing
like I'm not allowed to show photos of
our NOC but basically the same thing and
that's a room full of people with 20
steps 24/7 around the globe and their
job is to like detect respond to
incidents escalate fix things what do
they need information they can't just
sit there and guess they're not psychic
I wish it were sometimes but they're not
and so they need the information and
that's where monitoring comes in we need
to enable the net operations folks to do
their job
and ideally do it with a minimal
disruption and many more like escalating
to the wrong people at 4:00 in the
morning which can happen
so the question I like to think my job
is answering everyday is how do we
reliably monitor stuff at scale and I'm
gonna break that down a little bit it
seems like a simple question right it's
kind of complex in places I highlight
the word reliably because we've been in
this wonderful position where if the
monitoring systems aren't completely
reliable why do you even have them
what's the point but you can't trust
them then you just resort to spamming
the forum refresh and hope no one's
complaining if they're like 80% of the
time available 100% real are reliable
that 80% I guess they could kind of work
but the real job is to try and get a
hundred percent believability
in the monitoring systems otherwise you
end up with things like the monetary
team getting paged first because the
folks receiving they alert don't realize
that they don't know if it's a real
alert or not it's like is your system
working or is it real so reliability
number one monitoring that's what this
talk is about what is monitoring I like
to think of it as three layers
you've got basic systems monitoring CPU
RAM disk space network usage the kind of
bread-and-butter of writing the metal
the hardware that you're running on and
if you're starting from scratch right
that's the first stuff you do the next
layer application monitoring and this is
more fun right is my application even
running maybe not if it is running how's
it doing what's it doing all right
there's some warning signs is a thing
starting to trend the wrong way or is it
behaving badly are there error rates are
there response times and so you can
think of this as like the skeleton
outside that up like response times
error logs what's going on less about
the interior logic because so that to me
that is the business intelligence
section and when you talk about a game
you get some really fun questions here
right who's playing what champion are
they winning too much are they losing
too much are people talking in chat
about lag too much can we detect things
like like Network glance from what
people are saying in game spoiler we can
it's pretty cool there's also things
like who's logging on from where who's
buying stuff is this normal is this
abnormal and as soon as you get into
those kinds of questions
you still going back to the network
operations you're like hey this is stuff
the NOC should probably care about
nobody's logging in and we're expecting
peak traffic and it's not there that's a
live issue that's just not something you
wait for an ECL job and a tableau report
does something I care about right now
and I trying to fix and so these lines
are kind of blurry and I'll get into a
little bit of how we deal with that
later
the third price scale so games companies
have different definitions of scale and
I come from the tech industry we're like
vastly varying definitions of scale so I
wanted to color this in a little bit to
us scale is having a bunch of data
sentence physical data centers bunch of
metal in those data centers a lot of
things running on that metal and then
just for monitoring alone a lot of
metrics hundred thousand individual data
points per second being sent obviously
this isn't where we started this is a
rough estimate of where we are today and
this is nothing in comparison to like
our business intelligence pipeline
itself this is just the standard
monitoring pipeline and again when I see
these kinds of numbers as an engineer I
get really scared especially when the
word this has to be reliable is
highlighted in the front of my vision
right these are some some pretty
annoying numbers try and keep up
and so it's kind of the story of how do
we get there so life without monitoring
is glorious right everything's fine you
call it see the file you don't know is
there it's perfect so maybe you start
out here you know start up with
something you're like guys don't need
monitoring it's fine they're gonna tell
us if it's broken and you you deal with
this for a while and then you think okay
what's the most basic stuff I can do I'm
getting a bit concerned about not having
anything
so you add network disk and CPU standard
stuff and then maybe you're like okay I
want a bit more than that because you
know that doesn't tell me much about
what's actually running maybe you add
stuff like port checks right it's like
responding on a specific port that to me
is like monitoring 101 if you don't have
that then you're not monitoring anything
really and the problem with stuff like
that in terms of delivering a game as an
online experience is
if it's not responding its des
something's already broken and it's too
late to do much about it apart from
reboot it and hope it works so you need
more complexity than that to try and get
ahead of the problem and anticipate
instance before they actually start
hurting people's competitive experience
so you Google monitoring how do I and
you find an off-the-shelf monitoring
agent we did this a few years back I'm
not going to talk about the specific one
we used because it's kind of irrelevant
to this story
and I don't to make it the bad guy it
was it does monitoring stuff and it does
it pretty well you end up with something
that you install on every single machine
and it runs a bunch of checks for you it
pushes it to a data store we have one in
every datacenter
it's a monitoring service so it has
things like alerting and dashboards and
visualization built in that's lovely we
realized that in the network operation
center we don't want to open a tab for
every data center especially as we add
more and more data centers so we add
something global on top of that just a
little cash keeps track of the alerts
that are currently going off give some
it dashboard here's everything that's a
problem right now and everybody's happy
right so happy
and there's unicorns and rainbows and
everything is lovely and nothing ever
goes wrong well not quite
so one of the issues with this system is
I need to know what's on the machine to
tell it what to check so we're very tied
to things like knowing what is on a
specific piece of hardware what role
that hardware has in our ecosystem and
as we move towards the future that's not
really a world we live in anymore let's
face it a few other drawbacks we have a
very diverse ecosystem our philosophy on
spec and software is do what you like
we'll figure it out so we have like
almost everybody so here every
programming language because I know
there's gonna be some you guys work on
that I've never even heard of but we
have a lot of different programming
languages in our spec and as a result
you end up having the monitoring team in
the middle
connecting the agent on the Box to the
thing that's running right you teach it
how to talk to Java apps you teach it
how to talk to databases you teach you
how to talk to very specific random
stuff that random teams build and that
monitoring team becomes a huge
bottleneck they have to be subject
matter experts in basically everything
and they have to keep up and they get
patience tough breaks because nobody
knows who it really belongs to they're
the only ones that really understand it
you end up with this blurry line but
with health and business intelligence
and the main thing that happens there is
you have lots of arguments because say
we write a database monitoring check and
part of that check is our the number of
transactions being processed in the
database a healthy number is it greater
than zero so that becomes business
intelligence because hey I kind of want
to know how many transactions with
processing an hour in the store or
whatever
what can I keep that for like a year can
I keep that for like five years
well no we're gonna keep it for a week
because we just care about alerts and
debugging and live response but I want
to keep it for a year so you get these
these horrible arguments and it's also
like it's a monitoring system so some
things that it doesn't do I think we
started today we use something like
Prometheus some of these things would
not be true but they're true for the one
we chose things like we stored it in the
data center level so you can't get a
global view without adding more stuff
things like predictive alerting trends
regressions stuff like that not built in
to have to add our own and we end up
having to like add them in the
connecting blue which is a little bit
awkward for us things like it's a
polling based system so every N seconds
we get a data point so we get the
samples what's the problem with samples
they hired a lot of things the height
spikes they're highly normally is they
tell you like useful things were like
memory and disk perhaps but they really
don't give you everything you need so we
try and solve these problems now having
said all this the thing worked pretty
well like we got monitoring that we
didn't have before we are no longer that
this is fine dog we have some
information and the network operations
folks are really happy because they
suddenly have all this visibility so
this approach wasn't bad it just wasn't
enough and so I happened to kind of
evolve this for us micro-services
I love this though talking with
buzzwords very microservices or as we
can call them to services just maybe a
little smaller so we started moving more
and more towards this micro service
ecosystem our engineering organization
grew we added more stacks more languages
and we end up in a case where one team
just can't own monitoring we don't want
monitoring to be their job we want it to
be everybody's job
this is sneakily a sort of culture talk
about how do you change that how you get
people on board with thinking about
something they don't really care about
it's tough it's very tough so there's
two things that we can do kind of hint
on the screen so we get people to change
their minds we get people to care about
monitoring and not just leave it for
some people to clean up after them and
we change the data store take data
pipeline to actually a data pipeline
rather than a monitoring pipeline we
look at things like the blurred lines we
look at things like how people want to
use this data and we say well maybe
monitoring isn't special after all maybe
we can just use the same stuff we use
for a big data which ones we do first
well one is pretty easy I think changing
out a technology it's not too hot
changing people's minds
that's impossible or nearly impossible
so we did that first just in case it
failed I guess and we basically took a
step back and said oK we've got a Mike
reservist ecosystem on our hands how we
make this thing a really healthy
productive micro service like
environment for developers how do we get
developers all speaking the same
language rather than being in a room or
shouting at top volume across each other
and there's three steps to that the
first one is interoperability contracts
so it seems kind of obvious when you
look at it you basically agree for
everyone to talk the same way to each
other I have a common language a common
way of exposing data we came up with a
speck called ambassador
the name so I'm gonna mention it and the
idea is if I'm a service I have to
expose these kinds of endpoints we
agreed on HTTP and JSON as our common
format for interchange format format for
api's I'm transferring data and some of
you might think Oh HTTP and JSON or I've
got a favorite thing like protobuf or G
RPC your soap that's better than that
why didn't you choose that this does
come around time and time again but
fundamentally I learned from this a
really interesting lesson which is it
doesn't matter what you agree on it
matters way way way more that you agree
on something and not everyone's gonna be
on board with it from day one but the
thing that you create my being on the
same page is so much more powerful than
having some rogue person off using
protobuf in a corner by themselves so we
have these endpoints expose JSON and the
idea is I just put them in my app and I
will be part of this ecosystem we make
this easier of course we create
frameworks for common languages that
have these things out of the box we
integrate with code 'hail metrics
libraries and various ports of that
library making it really really easy for
me to get metrics in my app from the
moment I start writing it and things
like the status endpoint give us that
for free as well it's effective port
check but it gives us a lot more
information all perfect right follow
quite one thing I learn from this
experience is it doesn't just matter
what you specify it matters what you
don't and the stuff around the edges so
you say HTTP JSON some services you use
HTTP I don't know which ones always go
to the wrong URL and they realize oh
it's HTTP and have to go back and change
it so that's kind of annoying but it is
what it is another interesting thing as
we say you have to expose these
endpoints we don't say where they have
to be so I could have whatever that I
like has the preceding URL path and that
leads to some fun moments when you're
debugging you're like how can I remember
where this one is now we do say you have
to give us a way to expose them in a
common way that we can find them and so
here's my little plug for swagger
I love swagger swagger is a great way of
turning basically JSON into web page and
then you can interact with API endpoints
and we use this pretty heavily so we
have a guarantee that if you're in our
microcircuits ecosystem I can go to your
swagger and find you and use you which
is great another interesting thing is
authentication authorization how do you
lock down things not standardized yet
working on it but it's one of those
things around the edges that we didn't
quite specify quickly enough so I have
to try remember does this one use an API
key this one uses SSL certificate this
one uses a token it's kind of fun all
right so we speak the same language
that's great but we've also moving
towards this world where I'm not just a
service on a box this box is not just a
database server well there's a poor
example it probably is this box does a
bunch of things you know like back in
how do I find out what's on that box how
do I find out what's running kind of
bread and butter of Micra service
ecosystems you guys pray familiar with
these two things but I want to mention
them service discovery so I love it we
use a protocol based on net fixes Eureka
and the idea is I register a bunch of
stuff as part of my metadata i specify
where is my Ambassador API so we can
find that easily and we have
standardized on how you name yourself
and how you identify your location that
makes it super easy so like interact
with stuff and find it and know what it
is there's one problem there so I need
to register with service discovery but I
need to find out where it is but if I
don't know where service discovery is
how can I discover service discovery to
register with service discovery we use
DNS we take a format of your location
turn that into a text record query tells
you where they are of course that means
that DNS has to work and you'd be
surprised that's not always the case
especially means standing up whole new
data sent a whole new shot so that's fun
so it's always gonna be some dependency
it's Turtles all the way down you just
get to choose which Turtles at the
bottom I guess the other thing that I
find really really helpful for metrics
especially its dynamic configuration we
have what's effectively a global key
value store that's all it is and we key
based on location
name and version put key values in get
them back out it is as simple as it
sounds we use git and Jenkins to push
stuff in because that gives us a lot of
cool stuff for free like auditing like
pull requests reverse is great why do I
care about this so much for monitoring
this is more like application stuff now
it allows us to turn things on and off
dynamically turn on and off alerts
change thresholds and if you're
debugging a live issue and you want more
metrics and you've kind of turned them
off because you don't wanna send a bunch
of stuff you never look at if you have
to redeploy your service to add new
stuff that redeploy might well fix the
issue itself because software is
software and sometimes turning off and
on again does actually do the trick so
you want to be able to like keep things
as close to the problem state as
possible while giving yourself more
debugging information and don't have a
configuration is one way to do that of
course if there's a problem with your
dynamic configuration code then you're
kind of stuck but that stuff's really
hard in the mel tested in our ecosystem
so I love this stuff so we have a bunch
of services we don't care where they are
because we have discovery we know that
we can talk to them a specific way
hopefully you guys have put these two
things together a little bit and the
next piece will come somewhat obvious I
have a box I have some agents on the box
and my agents talk to server discovery
and then talk to the metrics endpoint
now I could have skipped this side and
made things a little obfuscated and
simpler but we do local discovery on the
box why why don't we just use central
service discovery two reasons as a games
company we sometimes have the kind of
services somewhat ephemeral for example
a game server that lasts the moment
people are in the game and is torn down
when people end the game we don't
necessarily want those things to be
discoverable in service discovery we
want them to stay fairly isolated onto
the box they're on we also find that
viscus are very natural sharding and
scaling because if we have a central
discovery in a central metrics agent
then we need to figure out how to
partition the space of services to
straight for metrics this means we don't
have to solve that problem yet so that's
nice
and then once again the box we just look
at the symmetric standpoint there's also
a metrics metadata endpoint that defines
alerts as a service owner I control that
I could put what I like in there looks
like this
it's just JSON we learnt that if you say
hey developer you have to specify the
schema here and register with this
registry here and do all these things to
get your metrics they probably just
won't all they'll come to you and make
you do it for them and we don't want
that because that sucks
so just JSON works pretty well for us
and we changed our pipeline the only
thing we changed is the box and read we
keep the rest the same this is important
this is the changing the engine in fly
aspect because we don't want to retrain
a bunch of people on how to use stuff we
don't want to have a whole lot of new
untested code while we're running a live
service and people aren't going to add
their and batter their endpoints
overnight so we have to keep running the
old stuff while we're also running the
new stuff we overload the same protocol
that monitoring service is using so we
insert into the same data store
we kind of impersonate it and look like
the same kind of data and we realize
this isn't enough we need more global
services at this point so we build a
full-blow global cache index layer that
looks at everything in every datacenter
and we use grow fauna on top of that
degress dashboarding and give us much
more powerful tools while still keeping
90% of our stuff exactly the same as it
was I'm gradually showing developers how
you expose this metrics endpoint this
magic thing will happen you'll be in
this dashboard what does that look like
I already show this graph because I love
it when people have pictures and not
words and also because it kind of
illustrates the point this uses the same
pipeline you're still running today we
haven't finished changing the engine in
flight yet and you get the ability to
see hey this thing spiking I did this
change it's not spiking that's great so
it's not just incident response but it's
also health monitoring on an ongoing
basis did my stuff change
you get some interesting things like we
started shipping more bytes huh I didn't
realize that number was low
until I noticed that it went up after we
did some stuff now I realize that's low
so monitoring this ongoing thing of like
you don't always know what's healthy
looks like and having a lot of data and
a lot of dashboards gives just that
color and picture and it helps us give
us a tool to communicate between various
stakeholders right and network operation
center the service developers themselves
even product owners and management all
those people they love tools like graphs
they tell stories so we're not perfect
yet in fact we only change one thing to
a tick which is sad so we we deal with
the biggest problem ecosystem diversity
we get rid of a bottleneck dependency
and you know being on the monetary team
is kind of cool now cuz you're building
this platform that's a generic thing
that everyone's using you know what the
after for janitors trying to get
monitoring in the last minute but if
anything we make the second point here
worse because I'm a developer I could
put whatever I like in my metrics I
don't put all the stuff I care about and
that's both health stuff and like
business intelligence and logic II stuff
who's playing what games at what time
and I get really sad because I want to
keep that for years and I can't a good
example of when we ran into this is we
moved our entire game server data center
from the west coast to Chicago to make
sure that players on the East Coast had
better latency and we used a lot of the
metrics that we had around that to
capacity plan to see the effect of their
move but if we were thinking about
monitoring is something that's fairly
ephemeral it's just data that sends off
alerts and then we throw it away we
couldn't have done that so what happens
we've run into scaling issues my
favorite thing because we put more and
more things in the system we have to
scale it more and the monetary system
stopped popping out there like they
can't keep up with one service developer
launched a service without really
telling us and I think the service
launched about 500 metrics on every
single machine in every single data
center all at once and that to us was a
significant percentage of our traffic
and I'd like monitoring back-end started
going crazy it's like holy I've got
all this new stuff and I can't keep up
what's going on and
him no because he hadn't told us he
didn't have to he just had to expose
such metrics and it all worked so we had
a communication mismatch of hey if
you're gonna do this you should probably
tell us so we can get ready for it and
add more capacity but we didn't have
that and instead we had to scramble to
keep up we also end up writing these
tools around the data centers and again
we can't do things like prediction or
trends or alerting we have to try and
get the data out somehow we run my C
Paul scripts to get data and run
predictions on it it's pretty messy and
it ends up making a date making
application developers have to take on
some of that work they have to start
recording rates they have to start
recording spikes themselves and exposing
those into the metrics pipeline which
kind of sucks so that's where we go on
to the next step change the back end out
use the data back end not to monitoring
specific back end everything's gonna be
lovely we hope so we change the pipeline
again we keep the left-hand side the
same this time well mostly we get rid of
the data center level of storage thank
goodness
finally with a global store because this
is 2016 and we can do that now and we
use a bunch of things that are kind of
battle tested for scale for much more
scale than we need we use elastic search
for asturias kathcart to get stuff into
it our vent collectors very dumb it's
really just a buffering measure so that
if we have issues between data center
and global we can keep on but you keep
onto data without having to drop it on
the floor and we use HTTP to get stuff
in because that's our agreed-upon
protocol this works pretty well but of
course our metrics agent does after this
week learn how to speak HTTP we have a
little change there we get a whole bunch
of stuff from the switch to a big data
back end like dashboard in Cabana
Cortana alerting using elasticsearch
that we didn't previously have we do
have to retrain people now now if you
prefer NAR the same so that looks most
of the same but Cabana is a whole new
kettle of fish but we didn't have to
retrain them on everything at once
we just retrain them on this one thing
so how we doing Oh looking good oh not
quite there this is yellow doesn't quite
show up because we're using a database
the big data based solution right we can
do things like determine whether
something is a metric or not we can
change retention periods even access
policies we can provide a lot of tools
because the lastest search has a lot of
tools for interacting with the data and
doing things like prediction and trends
and stuff rates just built-in statistics
of there we get a global few but we are
still polling because we didn't change
the metrics ation out yet one piece at a
time and polling isn't perfect and as
everything else gets better the flaws
with what you already have become more
and more apparent right so we're still
polling what do we do we rewrite the
metrics agent - we sneekly can do this
at the same time because we have to
change it to HTTP anyway we start like
moving more pieces out why rather than
just update it why do we totally rewrite
it which you rewrite it and go because
we wanted to make it really really fast
really resilient go concurrency and
buffering give us some really really
good tools out of the box for doing this
we can move really fast with the code we
can make it super simple and it works
with our cross-platform diversity code
system we can also share the code
effectively so we are not writing
something that's just a tool for pushing
batches of metrics to our collector so
why does that need to be an Asian it
doesn't write it's basically a library
so we go back to our development
frameworks but we originally launched
and said hey guys use these you'll get
your API endpoints free now we put the
metric stuff in there to say hey use
these you can send your metrics however
whenever you like and we get rid of the
problem where you're like snapshotting a
metric inside your code and then the
agent comes along a minute later and
picks up a really stale piece of data
and then sends it because you're in
control so we move to push based metrics
and eventually we don't even need the
agent at all
so we kind of make ourselves obsolete so
there's no caveats yeah
first is that we haven't finished doing
this yet because when you have a system
that works the resistance to changing it
can be pretty high because it works why
do you care like why why should I as a
stakeholder care about funding a team of
engineers to go solve this problem so
you can end up with a wonderful hybrid
ecosystem we have some of the really old
version some of the second old version
and some of the new version all running
together and you have to work really
hard to duplicate it and that's kind of
where we're at now and that's our focus
is just deprecating stuff we have to
make sure that things that don't have
those API endpoints built in have a way
to talk in the new system so we build a
bunch of side cars we call them to wrap
around an existing service and provide
those API endpoints and health checks
built in one of the things that was
mentioned yesterday is that just because
you have metrics doesn't mean you have
the right metrics it's what you don't
measure that's normally get what the
thing you want to look at right you get
paged it's 4:00 in the morning some
expropriations and so that's why we
ended up with a system that made it
really easy for people to add new stuff
in and that has to be really important
you have to think about who is your
customer what are they going to need to
do and how do you make life as easy as
possible for them the second caveat is
but people have different levels of
standards and perfectionism and people
have different levels of caring about
stuff like metrics so if I care a lot
about metrics and I've put loads of
stuff in my app and loads of alerts and
all sorts of things I'm probably going
to get the page because my software is
complaining even though it's a piece of
dependency three hops down the line
that's actually broken and you end up
with this wonderful micro service fun
time where you're trying to figure out
what is actually broken just because
your service has some error going off
and the best way to deal with that is to
like look at your architecture look at
dependencies and see if you can at least
alert like with relevant information
about what's broken
not just my service is broken but my
service is broken because it can't talk
to this one because the
things not here but it's definitely
something that you have to work really
hard as a culture to align around let's
get ourselves all caring about this
stuff all having the same standard of
alerting and make sure that you know
just because your dependency has Alerts
doesn't mean that there are people who
are on call on on vacation this week so
you have to get on the same page about
standards of response times and SLA s
and all that stuff that is an engineer's
you kind of try and let someone else
handle for you but you do have to start
talking about it but fundamentally
changing this thing piece by piece
helped us focus on what's important to
us helping make sure that developers
started to changing their minds caring
about monitoring and making sure that we
didn't have some teen it was just doing
it for them and making sure the network
operation center had the tools to
respond to things and that means like my
life as an engineer working on
monitoring is a lot more fun than it
would've been two years ago we didn't
have any of this stuff and it gives us a
chance to hopefully the metric I care
about most is a number of YouTube videos
being created about my product and I
think it's going down I should get some
hard data on that fundamentally you
can't change everything overnight and
monitoring is awesome those are the two
things I wanted to say to do thank you
guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>