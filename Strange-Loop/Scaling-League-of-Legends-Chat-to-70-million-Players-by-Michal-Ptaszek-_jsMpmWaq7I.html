<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Scaling League of Legends Chat to 70 million Players&quot; by Michal Ptaszek | Coder Coacher - Coaching Coders</title><meta content="&quot;Scaling League of Legends Chat to 70 million Players&quot; by Michal Ptaszek - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Scaling League of Legends Chat to 70 million Players&quot; by Michal Ptaszek</b></h2><h5 class="post__date">2014-09-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_jsMpmWaq7I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone hi hello everyone my name
is me helped a shark and I'm a software
engineer at Riot Games today I would
like to talk about how we scale League
of Legends chat to nearly 70 million
players before we start let me ask you a
question how many of you have ever
played a multiplayer team game doesn't
have to be a video game any kind of
multiplayer game yeah so as you probably
are aware in any team game communication
is extremely important if you could
imagine a basketball game where players
couldn't communicate with each other
so things like no place could be called
on the floor no substitutions made or no
no passes called for it would be a
disaster League of Legends is also a
team-based game and communication is
crucial for us as well so today I would
like to start with the short
introduction to League of Legends I
would like to tell few words how you sat
for the game then focused on technology
we use to build chat then move forward
to lessons learned so what kind of
difficulties we faced when building chat
and how we solve them and finalized with
Q&amp;amp;A session so let's start with what is
League of Legends so League of Legends
is team-based game from the general of
MOBA it's a shortcut for multiplayer
online battle arena and typically in the
game you would see two teams competing
with each other two teams of five people
trying to to come to control a map and
achieve objectives destroying an enemy
base and also protecting their own base
every every person playing the game is
controlling a single champion I think
unit and in league of legends we have
over 100 chance right now and everything
is is based in a modern fantasy world so
you would see swords and shields and
also magic so where do we have chat in
all of this
so chat primarily acts as a messaging
service for the game so players are able
to talk to each other using private
one-to-one channels and also participate
in group chat discussions so they can
hook up together and start talking with
each other the second very important
thing that chat chats doing is acting as
a presence service that is it controls
your friends lists its remembers who you
friended in past and also displays their
presence status that is whether they are
online or offline and what is their
state are they playing the game or are
they afk or maybe also things like what
champion they are playing or for how
long they they've been playing and
finally chat is also used pretty
extensively by other services at Riot
so we expose a lot of restful api to
other services on a back-end things like
store talks to chat to verify
friendships between two players or we
have leagues that are trying to use our
social graph in order to seed new
players together so that we can like
together more often and can compete with
each other what we have observed over
time is also that whenever chat is not
working or it's not working as expected
player experience is getting
significantly worse and what is also
important to note is that chat not only
has to be up and running but it also has
to provide low and stable latency to the
players so we cannot
introduced lags in the communication
otherwise wouldn't be effective so here
are a few examples of how chat looks
like in the game so on the very left vs
left side you'd see or babble is so all
the players you friend it and their
current status and if you hover a mouse
over one of the entries you'd see at the
detailed information the presence so
he's the the icon of the player their
level they were custom message they put
and also a few other things and on the
very right side you'll see a one-to-one
chat window so everything is using chat
on a back end so to to set the context
for further discussions here is the
high-level overview of the scale we need
to operate that so we are very lucky to
enjoy 67 million unique players playing
League of Legends every month we can see
27 million players playing daily and
during the peak hours we observe as many
as seven and a half million players
logged in to the platform that
translates roughly to the numbers we see
on the chat side the number is slightly
hover on the chat side because of the
third-party clients connecting to our
services not using the game itself but
that translates roughly to 1 billion
events that each chat server each each
box has two out every day if you average
it to the second a number of operations
we have to process every second during
the day it's something around 11,000
messages so our goal was to to come up
with a very stable and scalable chat
service chat solution and over time we
identified three components that would
help us achieve it first of all we had
to choose a protocol the communication
layer between
clients and the servers and then we we
had to choose the server itself so the
implementation that the routing engine
and finally we had to choose datastore
something that would persist all the
information you would like to keep in
between player logins so let's start
with the protocol for that we've chosen
XMPP XMPP stands for extensible
messaging and presence protocol and it's
basically a communication protocol based
on XML and it's primarily used for
messaging for instant messaging for
present information for maintaining
contact lists as it is stated in its
name it's pretty extensible the
community created a lot of extensions to
the core of the protocol which are
called PSAPs XMPP enhancements proposals
and riot although we try to be as XMPP
compatible as possible it's not always
available to us sometimes when
developing new features or or trying to
be as performant as it's possible we
have to diverge from from the core of
the protocol and we keep our own
internal library of PSAPs called riot
XMPP enhancements proposals on top of
that XMPP is also widely adapted on the
market if you've ever used facebook chat
or Google Talk or whatsapp in the very
early days you used XMPP under the hood
for example even Skype has XMPP gateways
so if in theory you can connect from
your idiom or pigeon or whatever XMPP
client you want to Skype servers the
second component of our service is
server itself for that we chose each
other D at the very beginning and we
started from there at the very beginning
it offered us
very nice scalability and performance
with pretty much no changes to the code
or to the configuration nevertheless we
were extremely our that one day
considering the growth and our internal
requirements we would have to get our
hands dirty and start modifying it so as
a fun fact in jeopardy is written in
exotic language called Erlang you
probably heard of so a couple of words
on it so at first as first our language
a functional language it's it gives us a
much faster way to build prototypes and
it also imposes a declarative style of
programming it also built with
distribution concurrency and scalability
in mind the language itself has language
level constructs allowing you to spawn
processes control processes control
fault tolerance both locally in the
scope of a single server as well as
globally if you run multiple erelong VMs
in a cluster which in turn gives us more
time to focus on the actual application
code as opposed to fixing or coming up
with solutions for scalability problems
and finally which is super appealing to
me is an ability to perform hot code
reloading so whenever you find a bug in
your code that is deployed in production
you don't have to stop a service you
just need to upload the patched file the
patched module to run up to all your
services and we load it on the fly that
way we can eliminate downtime entirely
so when it comes to the architecture of
in jeopardy or of the original user body
and the job early we use now we were
aiming at building a decoupled systems
systems that share nothing in between
and that
applies to both internal and external
facing interfaces and servers it allows
us for better error isolation and
traceability and it also goes well with
a scale we need to operate that it's our
aspirational State we are not really
there yet
however we are slowly but surely
progressing in that direction more on
that in the next slides next because we
we have deployed a few hundred chat
servers all around the world and our
team consists only of three people doing
that all we need to make sure that our
service is able to self-heal we do not
have to to react whenever we have a
minor failure it will auto repair under
the hood so if you think about user
sessions so every time user connects to
a server we have his session living on
the backend whenever that user messes up
with something starts to break the
service or whatever that actions will
only affect his session context that the
error will not propagate to any other
users or any other services running on
that server and whenever that user
session crashes for some reason the
error will not explode and will not take
this whole service down instead that
error will be isolated to the session
only and will be garbage collected that
session will be garbage collected later
by our internal processes the next thing
that is quite opposite to to default
tolerance is that let it crash
philosophy which says that whenever we
have a major failure we do not try to
slowly recover from it instead we let
the system or the part of the system
crash and we can easily restart it to
the known stable state that way for
instance
we have a huge lag of pending queries on
our database drivers instead of waiting
for them to drain over time over a few
hours what we typically do is we restart
the database driver and all the new
queries will be processed in the real
time while the queries are queued up in
the past will be rescheduled for
processing later so that the way we
build the application it is possible to
reschedule that queries for later
processing so here's here's how our
services work or how they look like so
on the very left side you can see two
physical servers and on each of these
servers we run through OS processes one
is a jeopardy daemon and the second one
is react which we use for datastore
more on that later so that part the
server part is very easily scalable we
can add as many servers as we want and
we we increase capacity as we add them
so they run in a custard mode each
jeopardy is able to talk to any other in
jeopardy
servers in the cluster and the same the
same goes for react servers and on top
of that react servers use multi data
center replication to export our
persistent data out of the primary
cluster to the secondary react faster
that allows us to run costly ETL queries
on our social graph as well as perform
backups without interrupting life
service when it comes to implementation
as I mentioned earlier we had to hit to
focus a lot on performance and
scalability and fault-tolerance and
overtime
little by little we pretty much rewrote
the entire jeopardy code today if you
asked me to estimate how much is left
from the original open source
implementation I'll probably say from
five to ten percent so 90 to 95% of
koats hours and it's custom to to riot
requirements and so we overtime we
removed unwanted and unnecessary code we
optimized the protocol itself and the
existing code a lot and we wrote a lot
of tests to make sure we don't break
anything as we go forward and to give
you an example of protocol optimizations
XMPP allows you to have a symmetrical
friendships between users so if alice is
a friend of bob bob doesn't have to be a
friend of Alice for League of Legends
we only allow players to be mutual
friends so if alice is a friend of bob
bob is automatically a friend of alice
so XMPP in order to establish a full
fully functional friendship required us
to send something about like sixteen
messages back and forth between clients
and servers and that was hitting our
data stores pretty bad what we've done
instead we change the protocol a bit and
we we made it that way so whenever Alice
sends an invite to Bob a Bob replies the
friendship is already established we
don't have to shake hands once once more
the next thing we had to do was to
profile the entire code and look for
obvious bottlenecks things that do not
scale that sure stayed across multiple
processes or multiple nodes and try to
make it so that it's can scale linearly
on single server as well as in the
clustered environment so as an example
again here a MOOC is a multi-user chat
is an abbreviation for multi-user chat
and so every chat server can handle
hundreds of thousands of user
connections and for every user
connection we we have session process
every time a user wanted to send a
message or post his presence update to
one of the rooms she was a member of
that event had to go through a single
process called new router that new
router was basically a messenger that
was relaying all the messages to
particular group chats on a multi-core
system that was a disaster you can think
about of it like of a critical section
that spreads all over that few hundred
lines of code in your in your C++
application so it was a real bottleneck
to us and what we've done was we
paralyzed the routing and the whole
routing concept the whole look up for a
group chat room happens in the context
of user session so that now we are able
to use all available cores at any time
we want the other kind of optimization
we implemented for chat was every every
each other the server contains a copy of
so called session table so session table
you can think of it as of translation
table between jabber identifiers and
session handlers that session processes
and so whenever you want to send a
message to someone you know his jid
jabber identifier and you look up where
is his session in the cluster so in the
original in jeopardy whenever a user was
hosting presence update say Bob finished
his game and she went back to the game
lobby so he posted an update each other
he was writing down that presence
updates to the session table so
effectively even though his session
handler didn't change there was a right
to that table by by first checking in
the session exists
and checking that the president's
priority is correct and stuff like that
we were able to reduce the number of
distributed rights to that table by I
think 96 percent which was a huge win
for us we were able to log in more users
faster and we were able to also allow
users to change their presence updates
much more frequently and finally we had
to take a closer look at our long VM
itself and the OTP libraries it comes
with so we were aware that by changing
the the p.m. and the summer libraries we
are sacrificing the generic nature of
our loyalty P in favor of better
performance or better scalability so on
the application side level on the
libraries level we we try to we try to
get more visibility into what's going on
whenever we deployed a new code to
production whenever we had called reload
in the past the original I rank OTP
framework whenever you upload release
upgrade package to it it will only
output two messages first one will be
okay I'm beginning to migrate your
system from version 8 version B and the
second message would be it was
successful or it failed whenever we
perform a larger upgrade and we need to
change say hundred files getting a
message saying it failed it doesn't
really help us so we added a lot of
logging a lot of visibility points into
that process so that now we can keep
track of progress on every single file
level and we can also have an ability to
reload the code on multiple servers at
once in a transactional contexts the
other thing we we had to do for
optimizing our servers was to build in
the back functionality into the VM it's
so to keep the memory footprint low for
every session handler that lives on the
server side we had to inspect we had to
have an ability to inspect what that
process is allocating and what is
wasting memory on so for the Erlang VM
we added a functionality to inspect the
heaps of the processes so that you can
point to a session handler process and
ask what kind of terms that process
allocated has on its heap and how many
machine words each term consumes that
way we have a clear picture of memory
utilization now so what are the parts of
the session state we should take a
closer look at finally the data store
for for a chat with the number of
players right enjoys we knew that
scalability would be an issue from the
start we opted in for no SQL which as
you know sacrifices a seed benefits of
traditional SQL storages initially we
started with MySQL however over time we
were stumbling upon move people
performance issues stability issues and
flexibility issues when we were not able
to manipulate my SQL schema fast enough
to keep up with the changes we we made
in the code so we which was react react
is a distributed fault tolerant key
value storage
it's truly masterless so whenever we
have an issue in life service when one
of our servers dies there is no single
point of failure we can we we can afford
losing two of the servers on production
and still be able to operate without
losing any data without losing any
functionality and react implements
availability and partition tolerance in
the cap theorem
it's sacrificing strong consistency so
for that we had to spend a lot of time
on the client site on that client firm
from the reactant perspective on the
server side to chat servers and we
implemented each other D level CR DT
library see Rd T stands for commutative
replicated data types so that library
takes care of all the right conflicts
that happen in react it tries to
converge the objects to the stable
predictable state over time it proved to
be a huge success it allowed us to scale
linearly and it also gave us a
flexibility in terms of changing the the
object layouts and money and playing
around with features nevertheless it
required a huge paradigm shift if we
collect a lot of work on our end to
change our way of thinking and also how
we test our services and how we build
tools around it let's let's move to
lessons learned so first of all we
realized the key to success is to
understand what our system is actually
doing so monitoring is key to to see
whether your system is in a healthy
state whether it's about to crash or
whether is being attacked by some
malicious users so we've built over 500
real-time counters
raised histograms into each other leaked
out they are all collected every minute
and posted to either zabbix or graphite
and that we use for monitoring and for
every counter and every metric we
collect we have thresholds that define
regular or normal conditions as well as
abnormal conditions so whenever we any
of the counter values goes way off the
limits
it is knock is automatically notified
about that fact and we can react pretty
fast even long before the players can
realize there is a problem on the
service so a fun story about it is few
months back in time we released a very
buggy client update the client code
itself was pretty much broadcasting its
own presence every time it was receiving
a presence update from someone else so
you can imagine an infinite loop of
presence updates flying around the
network so you're wondering why our chat
servers are so hammered with the load
and by briefly looking at the graph that
graphite we were immediately able to
realize ok that additional traffic is
actually presence of our presence
updates and they started to show up
exactly at the time when we released the
new client update and after patching the
client which was relatively easy
we were almost immediately able to
verify that the fix worked and the
presence the the rate of presence
updates went back to the normal level
and another thing we found very useful
when building new features is to have an
ability to turn them on and off on the
fly without having to restart the
service itself so whenever we come up
with something new
we typically surrounded with enough
toggles and that way if a feature causes
any problems we can safely disable it
yet another thing we've built for our
chat servers is an ability to perform
partial deployments that is we can get a
new code and enable it only for selected
users for example we can provide a list
of user accounts that would have that
particular feature enabled or we can
express that only 1% of our player
population will see that feature in life
that allows us to test
potentially dangerous features on at the
lower scale and if they behave okay we
can then turn it on for all the players
another thing which I mentioned earlier
is ability to code reload on-the-fly so
a few times in the past we were
unfortunate to release code that wasn't
really well tested on the third-party
clients so it was working fine with the
official client however
adium or sigh or pigeon we're sending a
different type of events to the chat
servers and the our client and we're
that was causing problems so we had
either an option to patch the button
code perform the server restart and run
with a fix code or what we could do with
Erlang and with the way we built our
release upgrade system was we were able
to upload our changed modules to the
servers and apply them without having to
restart the whole chat that of course
translates to lower downtime for the
players the next thing is logging it's
pretty obvious but it's really important
as well to get it right so for that we
not only try to log all the abnormal
conditions like errors and warnings but
also try to make sure we keep track of
healthy state of the server so servers
report healthy state over time and that
way by briefly looking at the logs we
can immediately identify ok that server
is ok because it's logging users it's
accepting new connections
it's modifying friends lists for example
and more on top of that we build a
functionality to enable debug modes for
selected user sessions so if for example
you have a suspicious user or if you
have an experimental user a QA tester
who
testing his things on production servers
we can turn on the bug mode for that
particular session and that way even
though we have hundred thousand players
connected to that particular server only
that session is emitting megabytes of
logs including all the example traffic
all the internal events and all the
metrics related to that particular user
that way we are able that if you combine
that with the future tunnels and partial
deployments we can actually deploy the
feature to production servers and let it
be tested only by a few people that will
emit tons of logs for us so that we can
take it back to our development machines
and optimize it or change if it is buggy
then always load test your code so by
that we mean that we have an automatic
verification system of our latest
changes every time we change something
that very night we have a Jenkins job
that picks up the code builds an
artifact deploys it to one of our load
test environments and runs a battery of
load tests against it it's not only
spinning up clients are hitting the chat
servers it's also monitoring the health
of the server so it's pulling the
metrics from it it's analyzing them it's
creating that beautiful confluence page
with tables and all the counters metrics
and everything and also in the morning
we find an email summary in our mailbox
saying that this was successful or it
failed we also have an ability to
compare our changes to the previous
builds so that we can keep track of the
impact of our code changes whether it
was a disaster or whether improved
memory consumption by 50% and lastly
things will always fail I mean we have
no control over everything around us you
can think about it
that in a way that even though in theory
we can come up with a bug less code on
the server side there will be external
factors such as ISPs what happens if one
of the main routers at the isp side dies
and we lose hundred thousand players at
the sun at the same time or if the
hardware melts or if we have problems in
the network so we you have to be
prepared for that situations and you
have to make sure that your service is
able to handle losing half of your
logging players at once or is able to
handle losing five out of twenty chat
servers without degrading performance
also no code is is bagless so even if we
have a code that works in most of the
cases say a bug happens once every one
billion times at our scale it will
always happen on each chat servers once
a day so even unlikely events will
always happen over time so where are we
now
chat is doing great we can we can enjoy
over 99% of quality uptime our
aspirational State is to reach the
mythical nine nines of availability we
are slowly going there when it comes to
scale and performance we have a few
hundred servers physical servers
deployed in production and as I said
every chat server deals with at least
one billion events a day and that
happens with relatively low resource
utilization it's something in between 20
to 30 percent of available CPU and RAM
and chat is evolving it's not a complete
product what we are doing right now is
we are migrating data off MySQL
worldwide it's we are still in the
transitional state the second thing we
would like to do is to
make a league of Legends chat available
outside of the client outside of the
game so that players can enjoy the
friendships without having to log into
the game itself and finally on a
backhand side we are exploring
possibilities around reusing our social
graphs to make the whole experience
better
so analyze the connections between
players and understand how they impact
their enjoyment in the game itself
that's it thank you for listening and I
would love to hear some questions
sure so the question was can I describe
CR duties in more details
so the concept we use is instead of
storing object values say you have Alice
and her friends lists like it has Bob
Charlie and Eve we don't store and every
time we update that list like append by
appending new item to that list adding
new friend we don't store the list
itself the new value instead we we
maintain an operational log to that
object which will say add Bob so
whenever two updates happen in the same
time to that particular object we would
end up with two entries appended to the
object log you would have ad player one
and uh player two so next time we read
the object we would detect a conflict a
write conflict that object then we would
take that two objects and merged the
logs that the operation logs and apply
them in in any order because the order
doesn't matter that way we end up with
with a consistent State of Trance lists
so basically the concept is you do not
update the value used that is stored in
the datastore in place instead you build
a long log of operations to that object
and you apply that operations whenever
you read the object itself
so currently okay so the question was
whether the in-game messaging is similar
to outdoor game chat so currently the
in-game chat is using game servers
throughout the messages between players
however I think the plants are 2 micro
migrated off the game servers to the
chat servers themselves so that we can
use all the goodies we have today for a
local chat for the in-game chat as well
oh so that's that's totally different
things so that's totally out of scope of
chat and that is dealt with that that's
dealing with the game server itself
anywhere else pretty light right thank
you all</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>