<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;The hows and whys of a distributed SQL database&quot; by Alex Robinson | Coder Coacher - Coaching Coders</title><meta content="&quot;The hows and whys of a distributed SQL database&quot; by Alex Robinson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;The hows and whys of a distributed SQL database&quot; by Alex Robinson</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6OFeuNy39Qg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so thank you guys all for coming my name
is Alex and this is my first strange
loop so I'm super excited to be here
talking to you all today I am a software
engineer at a startup called cockroach
labs and what we're doing there is
working on building an open source
distributed sequel database and I know
the name is kind of silly
you know cockroach labs the database is
called cockroach TB the idea is just
that like cockroaches the database
replicates itself and is very hard to
kill so hopefully it doesn't make anyone
here a little too uncomfortable but
that's the name I'm not here today to
talk to you specifically about cockroach
though I'm a huge fan of databases and
I'm really interested in how this new
class of databases that's been coming
around the last five to ten years these
distributed sequel databases I'm
interested in how they fit into the
historical context of databases why
they're being built now as opposed to 20
years ago and what makes them different
from old databases or from previous
databases and so that is what we're
gonna take a look at today you know why
a distributed sequel database why would
anyone care about them why would you
devote years of your life to building
these things or millions of dollars to
building them and and how do they work
so specifically and how they work we'll
look at how data gets spread around the
system how it gets replicated in a
consistent way and then how transactions
get layered on top in this distributed
computing environment and so let's start
by looking at the history anytime
someone introduces a new system you
really need to ask why you know how is
this system gonna be better than the you
know the hundreds of systems that have
come before it in the history of
computing and why is it being built now
instead of previously you know what what
do you know now that developers before
you didn't that they weren't able to
build the system instead and so we'll
begin with the very first databases and
these were being built in the early
1960s at companies like IBM and General
Electric these databases were pretty
primitive because they were the first
ones of their kind yeah they didn't know
a whole lot about what they were doing
at the interfaces that they exposed to
programmers were we're very tough to use
so we rather than having like a table of
records like we're used to today or a
key-value interface like we're used to
today it was almost like they were
programming Morgenson in memory data
structure there were these series of
record
but your queries would have to iterate
over one by one and traverse links
between the records so you might look
now traverse from like an organization
record to iterate over all of the
employee records that are working at
that organization and the queries that
you wrote were really tightly coupled to
the storage format of the data on disk
and so if you ever wanted to change that
to optimize for different access
patterns you'd have to pull all the data
out of the system load it all back in
and then to top things off you have to
rewrite all of your queries as a results
so this is a huge pain they were really
hard to use or required a lot of effort
to use and so this started to change in
the early 70s there was a mathematician
by the name of Ted Cod at IBM who saw
all the programmers around him at IBM in
his office having to spend a ton of
their time rewriting these queries
optimizing the access patterns changing
the layout of data on disk and thought
there had to be a better way and being a
mathematician he was all about elegance
and so he created what's now known as
the relational model which powers all
the sequel databases you're familiar
with today in this relational model
wanted to get rid of this tight coupling
between queries and how data is stored
on disk so it relies on very simple data
structures you know the table that
you're probably familiar with and a
high-level query language which is now
known as sequel and the queries here are
very independent from the storage on
disk so the database software is free to
optimize things underneath the covers
without the programmer needing to worry
about it it can change how things are
stored without anyone ever having to
know so these systems were much more
developer friendly and as a note of
historical context they were always
designed to just run on one machine you
know in the 70s most organizations
didn't even have one computer let alone
multiple computers let alone have
multiple computers that they wanted to
devote to running a database so these
systems were always built for a single
machine the first commercial database
was released in 1978 by Oracle and it
just kind of exploded from there over
the next couple decades sequel continued
to mature to grow to gain popularity
more vendors started putting out sequel
databases there were acquisitions there
was lots of competition it was great but
in this time there was also a new kind
of database they got introduced that
didn't really take off you know it was
attempted but but didn't get the the
necessary traction
these were object oriented databases
the motivation for these systems was the
object-oriented programming languages
for growing in popularity in the 80s as
well C++
I believe Java and there is a bit of an
impedance mismatch when you're trying to
translate between the objects that your
program has in memory and the rows and
columns that sequel databases store so
they are they argue that these were
easier to use because you could
effectively just take an object in
memory and dump it in to the database as
a single object it was a lot like
today's document oriented Norse no
sequel databases but at the time there
wasn't enough reason to switch in order
to switch you had to give up things like
complex queries that sequel database is
allowed and all these different
object-oriented databases had pretty
just disparate api's there's no real
standard as there was with sequel so
these systems never quite took off the
first real big change in how databases
worked or how they were used came in the
early 2000s
during the web boom so suddenly there
were more and more companies that needed
to support more and more users you know
they had to build services to support
millions and millions of users and that
had to be up 24/7 and they just couldn't
scale their databases to support this
yeah you can only scale a sequel
database as big as you can buy a single
machine so you had company you know
these big web scale companies like
Google eBay Yahoo and Amazon all having
to deal with things like sharding their
database and what this really meant was
just that they get you know 10 machines
to put their data on and they break
their table up into chunks so the first
chunk would go on the first machine
second chunk would go on the second
machine and so on so this all had to be
done manually by a database
administrator and then for queries to
work they had to put this middleware
layer in front of all these shards to
translate application queries into
separate queries for the separate shards
this limited what programmers could do
because they couldn't really do things
across shards and anytime the data grew
to be too big to fit on their current
number of shards it was a huge
operational headache to try and recharge
to move all the data under different
machines so you could have more of them
to account for all the growth and so
this was worth it for the really
valuable services that these companies
were building back then you know like
Google famously sharted a my sequel set
up for their adwords product but they
realized it wasn't worth it to do this
for all little services that they were
and this might not be worth it for
smaller services like Google Calendar
due to all the effort involved and so
around the same time so the same
companies you know Google Facebook
Amazon all started building no sequel
databases and these no sequel databases
had the primary goal of being scalable
they wanted scalability at pretty much
any cost because they needed to build
bigger and bigger services and continue
scaling as they knew more and more users
would be coming online over time so
these no sequel systems had to give up a
lot of things in order to get the
scalability that they were after now
they gave up the relational model
obviously that's why they're called no
sequel which is kind of a silly way to
define a class of systems do what
they're lacking but they're called no
sequel databases and they also gave up
things like transactions and joins and
indexes for the most part that usually
didn't offer the same sorts of
consistency of data that the programmers
were used to with sequel systems and so
you know whereas Ted Cod and the 70s
took complexity out of users
applications and brought it into the
database to make life easier for
developers no sequel systems did the
reverse so they took the complexity that
had been built into the database to make
life easier for developers and just
pushed it back out into the application
saying like we can't handle this anymore
you're gonna have to deal with this
you're gonna have to work around the
databases insufficient capabilities and
so now we're in the 2010s and there's
this new class of systems coming around
sometimes called new sequel or you can
call it distributed sequel some people
call them elastic sequel whatever the
heck you want to call them they're
sequel databases that run on multiple
machines and why are these being built I
think it should be fairly clear at this
point based on the history but without
no sequel or without distributed sequel
databases you have to make a choice when
you're starting a new product are you
gonna pick a sequel database and limit
your future scalability or are you gonna
pick a no sequel database and take on an
extra development burden to work around
the lack of a solid foundation to build
on yeah because the no sequel systems
don't have transactions and indexes and
consistency in general and there have
been a lot of really successful services
built on top of no sequel databases
right so they're clearly not you know
completely useless and tons of great
services have been built on top of them
but it's a lot harder
and so Google who published a paper
about their distributed sequel database
which is called spanner back in 2012 put
in this choice quote about why they
thought it was worth it to build this
system and to have most of their
services switch over to using it and I
think it's more worthwhile to have their
developers occasionally deal with
performance spent the performance
problems than to always have to deal
with the lack of transactions you know
it saves Google money in the long run to
do this because save their developers
time and energy and so that's why these
systems are being built because we want
to try to eliminate that trade-off that
was inherent in picking a database five
to ten years ago so there's a whole
class of these systems appearing that
are distributed databases providing full
sequel semantics where you can go and
talk to any of the machines in the
cluster as if they're an individual node
but they'll do all the coordination
under the covers to work together as a
distributed system to process your
queries and so we're attempting to
combine the best of both worlds when
these are usually built as entirely new
systems because to build this
functionality into a previous database
you'd basically have to rip out the guts
and put in entirely new guts like an
entirely new engine at that point it's
almost like building a new system
anyways so the two primary systems I'm
going to be referencing a lot today just
because I'm most familiar with them our
Google spanner which was published back
in 2012 and had another paper published
earlier this year detailing how it's
changed in the years since
and then cockroach TB which is because I
work on it on a daily basis and all the
information is publicly available out
there on the internet due to it being
open source so that's why these systems
are being built but there's still the
question of how so what makes them
difference why couldn't they have been
built you know 20 years ago and and and
how are they getting this different set
of trade-offs so we're gonna
specifically focus on as I mentioned
earlier how that data is distributed how
remote copies of the same data are kept
in sync in a way that keeps it
consistent and how transactions work in
the distributed setting and for each of
these points I'm going to specifically
compare all the different types of
databases so how sequel databases to do
things how no sequel databases do things
and how these new sequel or distributed
sequel systems do things as a brief
disclaimer there are a ton of databases
out there that work in different ways so
there's a lot of sequel databases
there's a lot of no sequel databases I
can't cover every little exception to
every rule I'll do my best to speak in
generalizations that are is
as possible and point out exceptions
where there are some but there are too
many systems for me to know all of them
let alone to point out all the
differences today and as I said I'm
gonna be focusing mostly on spanner and
cockroach as a representative of new
sequel systems because they are fairly
representative but there are some
slightly different designs out there for
these new sequel systems as well so
let's dive into data distribution all
the data distribution means is just this
question of how we're distributing data
are amongst a cluster and for sequel
databases there's not a whole lot to
talk about because they're designed to
run on a single machine right so all the
data is on your one server and you might
have backup somewhere you might have
secondary replicas that are getting
copies of the data you know in a
streaming fashion but all the data is on
one machine or all the data is copied
onto a couple machines there is this
option as I mentioned to manually shard
data across multiple instances of a
database but that's not the database
software doing anything that's still
just the database storing all of its
data on one machine and the human who's
managing the system spreading the data
across multiple machines manually so
sequel doesn't really do data
distribution no sequel and new sequel
systems have to though because they're
running with this core assumption that
all your data is not going to fit on one
machine so you can't do it things the
sequel way so there's this core
assumption that means you're gonna have
to break up your data in some way and
divide it up amongst all the machines in
your cluster so no sequel you know
really pioneered some of these
techniques and new sequel has
effectively just learned from them
saying you know sequel do this well we
have no reason to change it let's keep
doing things in a similar way and so you
have to answer for all these systems how
you divide up the data into chunks and
then once you've divided it up how do
you find where those chunks are located
within the cluster because it's no use
to have your data broken up and spread
out if you don't know where each piece
of data actually is at any given time
there are two primary approaches to
doing this which we'll take a look at
individually hashing and order
preserving so this hashing approach to
data distribution just means that you're
gonna pick a hash function maybe
multiple hash functions and you run
these hash functions on the keys
assuming it's like a key value system so
you run the hash function on the key it
spits out a result and you
deterministically mask that map that
result onto one of the machines in the
cluster
you know maybe just by doing a modulus
on the number of machines or something
along those lines and this makes it
super easy to find any piece of data
because all you need to do is we run a
function on this key and you know
exactly which machine or which machines
it's going to be located on so reads and
writes can find the given machine they
want to talk to you really quickly the
problem here that makes this prohibitive
for new sequel databases is that you
can't do range scans because hash
functions don't maintain ordering of
your data so if you wanted to scan over
all the keys from A to B you can't do
that efficiently in a system that's
dividing up the data based on a hashing
a hash function so this approach is used
in databases like Amazon's DynamoDB or
in cassandra by default but it's not
used in any new sequel databases as far
as I'm aware because sequel databases
really rely on having ordered indexes in
order to access data efficiently so some
other no sequel systems like BigTable
and HBase as well as most new sequel
systems I know of use what's known as an
order preserving approach and this
approach means that you put all your
data lay it out from A to Z or from the
beginning of the alphabet to the end of
the alphabet in alphabetical order and
just split it up into chunks by you know
picking arbitrary dividing lines that
give you nicely sized chunks in the
middle so you might have a chunk from
the beginning of the alphabet through a
from A through F from F through Q and so
on and you just distribute these chunks
evenly across all the nodes in the
cluster this is really good for range
scans of course because we're keeping
the data in order you know all the data
between a and B is in one place
you might have to talk to multiple nodes
if you're scanning over a very large
range but the data is all going to be
all in order for you the downside of
this though is that you no longer have
this deterministic function from a given
key to where that key is located so you
have to add on some kind of range
indexing structure on top of your actual
data to track which ranges exist in the
cluster and where they're located at any
given time so what that looks like in
practice is that you divide your data up
into what we're gonna call ranges of
data so you have a range from the
beginning of the key space through lemon
from lemon through peach and so on that
each just holds the keys and values that
fall into that range of data you also
add on this range index on top as I
mentioned and that's just going to track
which ranges exist
and where they're located so anytime a
query comes in say you want to read the
key mango you go up to the top of the
range index and ask which range does
mango fit in in this case it's going to
be the middle one and that range index
will tell you which machines that middle
range is located on so you can then just
go send a request to that machine for
that key and it might seem like this is
gonna add a lot of overhead to all your
requests but in practice these ranges
don't move often or don't change often
so they can be really easily cached on
all the machines so you don't have to do
this extra lookup the most of the time
and finally you can do a range scan very
easily as well as I mentioned you just
look up where the start key is you look
up where the end key is and read all the
ranges that contain any of the data
between those points the last thing you
have to ask about this approach though
is when do you split up the data because
you can do a split pretty easily right
by just picking a point in the middle of
an existing range and dividing it in two
so you can divide up that last range you
know from P to the end of the alphabet
and split it into two if you get a few
more keys added but you have to decide
when to do this so you could wait until
the data got too big to fit on a single
machine or maybe you'd do it sooner and
practice most databases do this when it
gets to be about 64 megabytes or 128
megabytes somewhere on the order of 100
megabytes of scale and there's a
trade-off here because if you let your
ranges get too big then moving them
between machines is quite slow so if you
need to quickly move them to recover
data that from a node that crashed or to
rebalance how much data is on each
machine it's tough to do that with
ranges that are really big what if you
make your ranges really small then your
range index at the top gets to be huge
having millions or billions of these
ranges as an overhead into the indexing
and the caching of that index and so you
don't want to have tons of them either
and because this is a distributed system
you have to assume that failures are
gonna happen from time to time so you
might have or you have to have multiple
copies of your data such that if any
given one is on a machine that fails
you'll still have other copies of it
available somewhere and so usually this
will be some number that's three or
greater might be user configurable
you're gonna spread these around the
nodes in your cluster and you have to
have some kind of control process that's
doing rebalancing because what if a new
node comes up into your cluster you know
an administrator adds a new machine to
your data center
you don't want it to just be empty you
have to proactively move data onto it
and so this control system could live
outside the cluster it could live inside
the database software itself it doesn't
make a huge difference other than in
terms of how you operate it but it needs
to detect when there's this imbalance of
data
so here nodes 1 through 3 have way more
data than node 4 does so it makes sense
to move some copies of ranges over on to
node 4 so you'll move a copy of range 3
over there and node 3 can just wipe that
off its disk it doesn't need to have it
anymore so you can keep doing this until
the cluster gets approximately balanced
and you can do this both for the size of
the data the number of ranges in the
cluster or even the amount of load so if
some ranges are hotter than others and
receive way more QPS you could split
them up into two and spread those out
onto different machines in order to
balance the QPS on each machine in your
cluster which can be important in
practice there's also the opposite case
you have to deal with so if a machine
fails you're running with fewer copies
of data than you want right so you have
to up replicate these things create new
copies of them so that you get back to
the desired amount of replicas at all
times whenever possible so if node 2
fails here we need to make a new copy of
both of the ranges that it had on it and
then if the node never comes back we'll
still be ok and have enough copies so
that's how data gets distributed around
these clusters both in no sequel systems
and in these new distributed sequel
systems but once you do that as soon as
you have multiple copies of data in a
distributed system you really need to
ask yourself how are you keeping them in
sync or how are you dealing with
conflicts where one copy says one thing
and one copy says another thing because
it's bound to happen network requests
will be lost a machine will be down and
might not be up-to-date and you need to
have some way of you know deciding which
copy is correct at any given time and so
sequel databases since they store all
their data on one machine don't really
have to deal with this unless they're
running some form of primary secondary
or application but that is pretty common
these days now because if you have cold
backups you don't really expect them to
be kept up to date all the time but if
you're running primary secondary
replication there might be an
expectation that your secondaries are up
to date because the way primary
secondary replication works is just that
you have one copy of you
that's considered the primary where you
send all of your rights and oftentimes
all of your reads and the primary will
just take H right right into its own
disk and then at some point send those
rights off to the secondary so that they
can have the same copies of the data and
in theory it's a very simple concept
right the primary gets a write it writes
it to its disk and it passes it along to
the secondary or the multiple secondary
ins in practice there is a bit more
complication there are edge cases where
you're going to lose data or where
you're going to be reading stale data
and it's tough to really manage this as
an operator all you have to choose
between asynchronous or synchronous
replication where asynchronous just
means the primary will write a write to
disk and acknowledge it to the client
and in the background it will start
sending those rights off to secondaries
what that means the secondaries won't
necessarily have all the data the
primary has if you have to do a failover
you could be losing recent rights if
you're doing reads on the secondary you
could be missing out on a recent rights
if you do the opposite approach what
could be called synchronous replication
that would mean that you're waiting on
the secondary strike the data to disk
before telling clients that their rights
have succeeded this is going to
introduce extra network delay into
you're into all of your write requests
and it also introduces interesting
questions like what if the secondary
fails or what if the secondary gets slow
you know how long are you willing to
wait on that secondary to respond to a
request before you just give up on it
you know and either way either way you
do this failover is really tough to get
right because you have a lot of clients
in your data center presumably talking
to this database and they need to know
which one to talk to at any given time
so it's tough to fail them all over
without some straggling requests
potentially going to the wrong instance
and leading to a sort of split brain
moving on to no sequel no sequel systems
really do a whole ton of different
things to keep track of their copies of
data and most of them are what's known
as eventually consistent which is kind
of just a euphemism for not consistent
most of these different solutions for
maintaining copies of data can fail in
weird ways whether there are partitions
or not they could be dropping writes if
two writes come in for the same keys
around the same time one of them is just
going to clobber the other and it's not
monistic they could be serving up stale
reeds so if you do a read in an
opportune time you might be getting an
old piece of data in some situations you
might get an uncommitted piece of data
you know something that was never really
acknowledged to be committed the only
solution here that really can be called
eventually consistent and that if you
wait long enough the data will truly be
consistent with nothing lost is these
conflict-free replicated data types
these are some really cool algorithms
that have been talked about a past range
loops so I'd recommend checking out
those videos if you want to learn a
little more but they do in fact keep all
your data consistent if you wait long
enough if you let all the data from the
different machines eventually converge
we don't have enough time to really
drill into the details of how all these
different things work you could fill
multiple really interesting talks just
doing surveys of all the different
mechanisms no sequel databases use to
keep their data in sync just know that
typically you know they really don't
keep data consistent in any way you can
lose data get old data etc keeping
copies in sync new sequel databases
usually works about the same like in
similar ways between all the systems
which is a pretty serious change from no
sequel most new sequel databases or
distributed sequel databases use what's
known as a distributed consensus
algorithm and these consensus protocols
have been very heavily researched you
know there's a lot of academic papers
proposing different ones proposing
improvements and variations on the
existing protocols and they've been
proven through formal methods to be
correct too to keep your data consistent
and reach agreement on all the rights in
the system the most famous of these
protocols is paxos it was originally
written about about 30 years ago almost
but is notoriously hard to understand
and hard to implement so it there aren't
really many implementations of it you
know google wrote a paper on how many
years it took them to get one of these
implementations into a usable reliable
production ready state it's really not
easy raft was a new protocol written
about about five years ago by a PhD
student at Stanford that has really
taken off because it was designed from
the get-go to be correct but much more
understandable and easier to implement
so now you can go out i'm good.how up
and find open source implementations of
raft
you know just already written for you
that you can throw into your own system
and use to get reliable replication of
data
it's not necessarily easy to use but
it's available if you want to use it and
so this has made consistent systems much
more possible much easier to write than
they were ten or twenty years ago
because these protocols have become
simpler easier to write and more widely
available but the way that these
protocols typically work is that they
have some odd number of machines in a
cluster so three or five maybe seven and
commit just happens when a majority of
those have written the data to disk
there is more in the details that makes
them always correct but that's the high
level and so let's take a look at raft
in particular raft is what we use in
cockroach dB we share an open-source
implementation with lock with the lock
oriented database and at C D and it's a
pretty hard an implementation at this
points and graft is based around a
leader election so if you wanted to
describe raft in three sentences you'd
say you have a set of replicas that
elect a leader and that leader appends
incoming commands to a sequential log
and assigns an index to each command and
if you've been to past range loops
you've probably seen talks on how
sequential logs are these really
powerful constructs and distributed
systems so you have these rights being
appended to a sequential log and then
that log just gets replicated from the
leader to the followers and the commands
are considered committed once a majority
of the nodes in the system have written
into disk so in this example if a write
comes in for the key cherry the write
has to go through the leader because all
proposals and Raft have to go through
the current leader so the leader will
write it to its own disk in that
sequential log and replicate it to the
followers as soon as either of those
followers has written into disk we
suddenly have it written to disk on a
quorum of them right so we have two out
of three at this point the write is
considered committed even if all the
machines were to be suddenly turned off
at this point that write would be would
have been persisted and at this point
you know normally the follower will
acknowledge it to the leader and the
leader can acknowledge the write to the
client so you do have to wait this one
network hop before you can acknowledge
writes which is a bit different from
those sequel databases just have
everything in one place but notice here
that you didn't have to wait on that
third replica to write it to disk all
that these algorithms require typically
is a majority to be up to date so if one
node is running slow for some reason or
say you know it gets shut off for a few
minutes
or it's on the other side of the country
you don't have to wait for it which
which allows for some interesting
configurations where you can deploy you
know two replicas on the East Coast and
one on the west coast and those two that
are close together can operate and serve
these rights really quickly because they
don't need to wait for the one that's
far away but that's what happens when
things go well and of course you need to
ask yourself what happens when things
don't go quite so well yeah that's
always a question in distributed systems
because you're gonna have failures
you're gonna have partitions you're
gonna have all sorts of problems and so
if we have the same example where a
write comes in for the key cherry and
the leader dies before it's able to
replicate it by the definitions we've
we've been given that right has not been
committed because it's only on one hard
drive right so those two followers are
gonna elect a new leader while that old
one is down so we have the new leader
now the second replica has become the
leader and it doesn't have that key
because the two replicas that were
followers didn't know about it so they
didn't know that the leader is supposed
to have this key so for read then comes
in for that same key according to raft
according to the consensus protocol that
key was never written it's only on one
disk and the leader doesn't have it so
the leader is going to you know go to
its followers and say hey like do you
have this key no no I don't
the first replica will then have to be
asked to remove it because it was never
truly committed and on a leader like I'm
a leader change over any uncommitted
proposals just get dropped and so at
this point the key is just returned is
not found and that's like a surprisingly
thorough overview of raft I mean there
are more details of course but that is a
very understandable algorithm I'd
recommend reading the paper if you're
interested in this kind of stuff in
practice new sequel databases like
cockroach and spanner have to run one of
these consensus groups for each range of
data so in that example from earlier we
had those three ranges in three or four
nodes we'd have to have one group for
each of those ranges so three consensus
protocols running in a cluster and in
practice we have way more than three
ranges right
these were restricting them to be about
64 megabytes so if you have terabytes of
data in your system you could have
hundreds of thousands of these ranges so
you there are a lot of engineering
complications that come into play in
these systems it's tough to scale to
have this many ranges and to be able to
handle member changes when you're moving
between nodes to handle splitting them
consistently to deal with software
upgrades where some nodes are running
different versions of the software than
others but these are all engineering
complications that can be dealt with
through just hard diligent work assuming
you have the solid foundation of
research underneath you telling you that
that this replication is correct and so
finally let's take a look at
transactions so we have consistently
replicated data on a Ripper range basis
but sequel databases allow transactions
to touch any rows anywhere in the system
so you have to handle coordination
between these ranges between these
consensus groups and so these groups
might be on different machines all
throughout the cluster and when we're
talking about transactions we're talking
about good old old-school acid
transactions where you can have any
combination of reads and writes to touch
any of the data in your system that are
atomic consistent isolated and durable
the two more interesting facets here the
two tougher ones to provide are being
atomic and being isolated consistency is
really just about checking user-defined
constraints like uniqueness constraints
or foreign key relations and things like
that and durability is really just about
synchronizing your rights to disk
properly but the hard parts from an
architecture perspective are being
atomic and being isolated and so atomic
means that all you the rights that are
sent together as part of the same
transaction either commit or none of
them commit you have to get all or
nothing and being isolated means that
having concurrent transactions trying to
operate on the same data in the system
that they can't interfere with each
other so it's as if they all have their
own correct consistent view of the
database at all times and there are
multiple isolation levels we're just
going to focus on serializability today
because that's the isolation level at
which you don't get any anomalies you
don't get any incorrect data back out of
your database and the serializable level
is required by the sequel standard to be
truly compliant so first let's look at
sequel databases and you know a lot of
sequel databases are proprietary so I'm
focusing specifically on postcards here
the way that postcards does atomicity is
that all the mutations that are applied
as part of a transaction all of those
rights when they
written to disk they're written with the
transaction ID associated with them or
attached to them so if I write to the
key cherry it's gonna have this little
marker next to it saying it was written
as part of transaction foo and those
rights aren't considered to be visible
to reads necessarily because they're
still tagged with that marker and
transactions have to know which
transactions have committed or not when
you do want to go ahead and commit the
transaction you have to write it to this
commit log and that's where the
atomicity comes in because you can do a
single write to it to a disk block in an
atomic way and so if we can do this
single disk write to the commit log to
mark that transaction is completed then
we have in one fell swoop committed
every operation that happened as part of
the transaction and that one disk write
will either fully succeed or fully fail
that's guaranteed by the disk so that's
how you get atomicity of all these
rights spread out over the entire system
to be committed all at once you boil
them down to just this one describe our
isolation is provided in a couple
different ways which we'll look at
individually readwrite locks are the
easiest way or the simplest way to
provide isolation between transactions
this is how sequel databases did it for
decades after their creation and it's
exactly what you would think it's the
same as using a readwrite mutex in your
own program when you want to access the
row you take a read lock on it when you
want to write a row instead of read it
you have to take a write lock on it and
your read and write locks exclude each
other right because that's how readwrite
new taxes work and then you have to hold
these locks until you commit and release
the locks once you've committed your
transaction so the database has to do
some deadlock detection but this is
pretty simple to understand and pretty
simple to implements as well now there
are ways to reduce your isolation level
by releasing locks early around in the
transaction but then you'll have
potential anomalies appearing in the
behavior of the user sees the other
approach to doing isolation in sequel
databases is which is what's known as
multi-version concurrency control or
MVCC for short because that's quite a
mouthful and what this is is that
anytime you write or route a disk you
store a time stamp along with it and
that time stamp represents the time
stamp of the transaction that was
writing it I mean anytime you update the
row rather than overwriting the row on
disk
you have to just write a new row next to
the old
with a new time stamp so you're not
getting rid of the old data and then
anyone that wants to read a particular
row has to get the most recently written
version of that row from before the
reads own time stamp this has a few nice
properties and it allows access to
historical data you know because we're
not deleting and overwriting the rows
you can run a query you know ads of you
know 24 hours ago and get a consistent
view of what the state of your system
was then now this could help you track
down things like bugs in your
application that have started writing
funny data to your database you can pin
down when the issue started happening I
mean the other big win is that it allows
long-running read queries to not exclude
you know other transactional workloads
from happening because you're not
holding on to these write locks anymore
you're just running at an older
timestamp and not blocking anything else
from happening so MVCC by itself isn't
doesn't really provide isolation you
have to have conflict detection on top
of this in order to prevent transactions
from doing things that they shouldn't so
you have to be able to tell when a
transaction is trying to write or read
to a key that something else has also
been accessing recently so it's a little
tougher to implement postgrads
implemented a version of this I think
about a decade ago that it uses for
certain isolation levels now and so no
sequel systems don't have a whole lot to
talk about because they just as a rule
don't really offer transactions the best
thing you're going to get in most no
sequel systems is a single single key
transaction so you might be able to do a
read modify write loop I'm just a single
key but you won't be able to atomically
commit multiple keys at the same time or
you know write some keys on one part of
the system then do some reads and write
some keys somewhere else and have that
all happen as an atomic event it's just
something that they sacrificed in order
to achieve scalability at the time
because transactions imply coordination
and coordination is hard in distributed
systems right so it was a lot gonna be a
lot of engineering work for them to try
to do this and so you know this is kind
of all I can really say about no sequel
transactions finally distributed sequel
supposed to support the same semantics
as traditional sequel databases because
you can't replace a sequel database as a
drop-in replacement unless you have the
same semantics so we need full acid we
need serializable isolation and that is
takes a bit of work in a distribute
system but we can learn a lot from how
sequel databases do it so we can provide
atomicity in a similar way all rights
that come in as part of a transaction
can be tagged with the transaction ID as
usual and we just have to have this
transaction record somewhere in a
consensus group because rights to a
consensus group a raft group or a paxos
group are atomic they either do happen
or they don't so whereas a single node
sequel database can use a single disk
write to get atomicity we can use a
single consensus group write so it just
takes that flipping of the transaction
status from in progress to committed to
market is done and to mark all the
rights involved as done in isolation can
also be done somewhat similarly the
single node databases it's a bit harder
due to the network latency involved and
the fact that all the state isn't being
held on one machine and in the same
memory space but we can use similar
approaches I think most new sequel
systems that I'm aware of at least use
some form of MVCC spanner uses a
combination of MVCC with two-phase
commit and two-phase locking
they have to rely a lot on their very
special hardware clocks that are in the
Google Data Centers these days
cockroach DB uses pure MVCC with
conflict detection and it has to use a
combination of hardware and software
clocks to make this work reliably over
widely distributed networks so if you do
have replicas on both coasts of the US
for example but let's take a look at
this MVCC and conflict detection
approach to transactions so you'll start
by beginning a transaction and putting a
key to the database and you know that
key will be in some range and for
convenience and cockroach at least we
put the record for that transaction
tracking a status in the same range as
that first right so that they're
co-located if the entire transaction
happens to be happening in just that one
range so you'll start the transaction
and you'll write that the key and notice
that the key is tagged with the
transaction number which we're just
using one in this case and that's to
indicate that it might not be committed
yet if some other transaction comes
along and wants to read the key cherry
it's gonna have to check the transaction
status for that transaction to be aware
of whether or not it should be returning
that key the transaction can go off and
write keys and other ranges of the
system
other tables of the sequal system and so
on you know so it could write mango in
another range and then when it's done it
just has to go and do a single consensus
right to flip the status of the
transaction record so it's going to mark
it as being committed and at this point
the transaction is done any new
transaction that comes along and reads
those keys will know to just go check
the status of the transaction and it
will see that it's committed that does
add some extra work to the reads though
and so after a transaction finishes it
goes and proactively cleans up those
transaction ID pointers so that anything
that comes along later to do a read
doesn't have to go do that lookup and
once we've cleaned up the pointers we
can also just get rid of the transaction
record because it's not needed anymore
but again anytime you're talking about
one of these distributed systems you
have to ask about what happens when
things fail or when things conflict when
things don't go wrong in general so
let's take a look at one particular type
of conflict it's going to be a write
conflict so the transaction will start
out in the same way and it'll write a
key and put a transaction record for
that transaction into the same range
within another transaction is going to
come along and try to write to the same
key and so in this case the state to
detect that conflict has happened is on
the range itself so the machine that's
currently responsible for serving
queries for that range knows that
something else has written to that key
recently so we have a conflict here but
we've also detected it I mean so there
are there are multiple ways to try to
handle this you can make the transaction
wait on the first one that's already
running or you could do an abort of one
of the two transactions and force them
to restart and so if assuming we're
going with the abort method you have to
pick one of the two to abort and so this
can be done with a priority scheme so
transactions kind of a priority assigned
to them when they start it could be user
assigned it can be randomly generated
and you make the higher priority
transaction win for example so it's less
likely to be aborted by a lower priority
one in this case transaction two you
know has to check whether chance of the
transaction 1 is done though because
maybe transaction 1 is already done and
transaction 2 doesn't have to worry
about it
in this case it's not though so
transaction 2 is going to decide to push
or abort transaction one push would just
mean to make it have to start over at a
higher timestamp so transaction 2 has
aborted the conflicting transaction it
can update the right
to its own key we're intent just means
the transaction ID tagged on it and then
commit so at this point transaction two
is done and transaction 1 is gonna have
to restart it might be able to do this
restart internally to the database or it
might have to send that abort message
the abort error out to the client for
the client to retry so that's a quick
overview of how these conflicts work of
how these systems work more details can
be found on the cockroach labs blog in
the spanner academic papers other
descriptions of these systems online
because this description had to omit
things like the MVCC itself and other
types of conflicts
so to summarize anytime you're looking
at a new system whether it's a database
or some other kinds of thing you should
try to understand why it's being created
how does it fit into the historical
context of that type of system why is it
being built now why couldn't have been
built a while ago or why wasn't built
why wasn't it built a while ago and try
to get a better sense for what
trade-offs are involved in picking a
system so we've done that for for these
distributed sequel databases we've also
tried to understand how they work in
comparison to past databases so you know
how they distribute data which is you
know they do in a very similar way to
these no sequel systems how do they
replicate that data in a consistent way
which is done in a new way and different
from sequel or from no sequel it's
notably not new to academia though I
mean these consensus protocols have been
around for decades so it's taking this
existing work from academia and bringing
it into a system in a new novel way and
then how transactions work which has
been done by learning a lot from how
sequel databases work and then just
applying some engineering elbow grease
to make it work in a distributed setting
so thank you a ton for coming out and
listening today you know I'll be around
for questions afterwards I'd recommend
if you are interested in this there are
a lot of great academic papers out there
on these topics you can check stuff out
on the cockroach labs blog or on our
github repository since it is open
source with all of our design Doc's
but thank you for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>