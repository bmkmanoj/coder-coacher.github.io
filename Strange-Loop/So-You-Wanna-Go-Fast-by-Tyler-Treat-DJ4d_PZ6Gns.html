<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;So You Wanna Go Fast?&quot; by Tyler Treat | Coder Coacher - Coaching Coders</title><meta content="&quot;So You Wanna Go Fast?&quot; by Tyler Treat - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;So You Wanna Go Fast?&quot; by Tyler Treat</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DJ4d_PZ6Gns" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Tyler treat and I'm
recovering go hater now I just have
Stockholm Syndrome but no I've written a
lot about go I've spent quite a bit of
time at the language now or the last
several years I don't consider myself a
gofer but you know I just think of
myself as a programmer but go is
probably what I use the most these days
and I'm guessing most of you came to
this talk to learn about this one weird
trick I'm gonna show you to make a code
faster that's not what this talk is and
the better title for this talk really is
so you want to subvert go and that's
because we're gonna look at some things
that in many ways try to subvert the
language and the reality is goal really
is not a systems language I know that
word is subjective depending on who you
ask
but if you ask me it's not a systems
language and that being said doesn't
mean we can't build internet scale
systems with it
right and we've seen this with all of
the new projects that have popped up
many of them written and go and and it's
kind of become the lingua franca of
infrastructure if you will for better or
worse and so to get back to my
alternative title you want to subvert go
the reason it's better title is really
is to talk about how to write terrible
awful go code and it's really not talk
meant for a general audience of bill
programmers but more to talk about some
things that I think are interesting
things that I've learned and to also
kind of tell a story of how the language
has evolved over time and so I have no
doubt the go team will wholeheartedly
reject this talk that's that's okay
that's their prerogative but it's my
talk not theirs and so I want to talk
about writing performance optimized go
from my perspective and my own
experience and also to talk about
trade-offs because when we optimize for
performance we're usually trading
something off and return for that so as
I mentioned my name is Tyler I work at
apps era where I work on the open source
messaging system Nats
I have an interest in distributed
systems I have a blog brave new geek
calm where I write about some of these
things
and as I said I work on Nats which is an
open-source high-performance messaging
system hesitate to call it a message
queue because it's really not but there
are some similarities there but it has
sort of two guiding principles which is
simplicity and performance and
throughout this talk we'll touch on some
of the ideas and the principles that we
applied while working on that and some
of the things that we learned as well as
they relate to performance so why does
this talk matter well the compiler isn't
magic it doesn't just take in your
shitty go code and plop out this
pristine machine code it's very good at
what it does it does do a lot of
optimizations but it's not perfect it
can't read your mind and so we have to
be mindful of performance when it
matters and to be fair it doesn't always
matter but when it does we have to be
thinking about that and so the question
is what are we really talking about when
we we say performance what does that
mean how do we quantify it and you know
if you saw the the keynote this morning
about optimizing tail latency that's
really I think what we're interested in
is like how do we observe the behaviors
of our system and the problem is we're
usually looking in the wrong places most
of the time and we're missing a lot of
the problems that are happening and so
that's really what we're interested in I
think is like how do we optimize for
tail latency and that can be really hard
to do but when we reach a certain scale
it's often when things fall over so in a
previous job I worked for a company
called Rick Eva which builds software
as-a-service
kind of geared towards financial
reporting and one of the products that
we built was a spreadsheets application
and early on I was kind of involved with
building out the communication layer for
that and one of the larger open source
initiatives that came out of that work
was a library called go data structures
which as you can guess it's a library
written and go with a bunch of data
structures but with sort of an eye
towards performance and so we were
looking at like how do we minimize
garbage collection minimize heap
allocations minimize lock contention
etc and so Dustin Hyatt wrote majority
of that as he was building the
calculation engine for that Spreadsheets
products but again we'll talk about some
of the things that we learned while
working on that and some of the
techniques that we applied as well so
we're going to touch briefly on
measuring performance and go and then
we'll just dive into the stuff talk
about a few language features talk about
memory management and then wrap up and
talk about concurrency and some
multi-core techniques so I can't
emphasize it enough but don't just
blindly apply the optimizations that
we're going to talk about as with
anything it depends and there are
trade-offs involved and it depends when
those trade-offs makes sense and when
they don't and and so I tend to think of
optimization as kind of this two-step
process where you like measure and then
you optimize and you repeat that as
needed until you've met your acceptance
criteria and you know everybody so it
says premature optimization is the root
of all evil so it's it's good to have
data and go actually ships with a lot of
really nice tooling to help with that
measure step right the the profiling
tools how we can look at memory usage
CPU usage of our applications the
tracing we can look at the garbage
collector how that interacts with their
system as well as the runtime scheduler
like how that schedules go routines and
then kind of the other half - that is
benchmarking and as you know go ships
with facilities for that as well to do
kind of your code level micro benchmarks
and that's really nice a test for like
performance regressions but it's also
easy to get caught up in trying to just
shave every nano second off that you can
and so I think it's also important to
kind of balance that out with more of
the end and holistic approach and for
that I really like HDR histogram which
it's kind of a generic library but
basically it allows you to capture from
a very large range of values to cap
very fine-grain measurements like
latency for example and so I've built a
number of tools around that to help with
my own benchmarking and testing and for
example that's how I generate like these
latency distributions is with HDR
histogram so I've done a lot of
benchmarking and to start I was really
bad at it and I think the only way
really get good at anything has to be
really bad at it for a long time
and by no means am i good at it now but
I think I certainly improved at least in
terms of my methodology and my
understanding of these things and
benchmarking can be really tricky
business because often it is just that
right and people have a tendency to just
look at the data and ignore all of the
context and I think the problem is it
sounds kind of counterintuitive but you
can't just ignore that context and you
have to be careful about how you
interpret that data and and be aware of
like the bench marketing and things like
that and so it can be legitimately
useful though for for like quantifying
performance and measuring that and
looking for performance bottlenecks so
enough with that let's let's dive into
this stuff and I want to start by
talking about a few language features
that I think are sort of major selling
points of go or at least kind of major
features of go and when you used in
certain cases can lead to some
surprising performance results I think
so channels obviously are a big part of
go as well as go routines and
concurrency story and in channels in
particular early on at least we're kind
of pushed as like the way to do data
synchronization of like shared state and
you know this idea of share memory by
communicating and to like pass ownership
of data over channels as opposed to
using locks to mediate that at access I
think the problem with that is it misses
out on a lot of the nuance of like when
to use certain things when not to and I
think since then it's been a bit more
kind of pull back on that but you know
when you look at the data it's not real
surprising that just using a mutex is
quite a bit faster than like using a
channel to do state synchronization but
even just ignoring the data people we
understand locks we've been using those
for a long time and so you know it's
when you start to restructure your code
in a way that allows you to use channels
to do that you typically end up with
more complicated code in the end than
you otherwise would have so yeah
channels are great they have their place
I think they're great for like
coordinating go routines and synchronize
or uh orchestrating things but for
synchronization you know just using
mutex and similarly we can look at just
the the performance of channels for like
work assignment or work throughput and
at the end of the day the channel is
just a ring buffer with a lock and so
yeah there's some some tricks they do as
far as the handoff semantics and they'll
like copy between go routines stacks to
optimize for that but they're still lock
involved at the end of the day so you
can typically go faster just with like
the lock free version of that and to be
fair this isn't really that meaningful
of a benchmark I mean you're probably
not spending that much time in inside a
channel but there was Dmitriev ukhov had
an implementation of lock free channels
it never made it in and I think the go
team was looking for further
justification for the added complexity
that brings and I think that shows kind
of that fundamental trade-off between
like performance and complexity and when
does that trade-off make sense and when
does it not make sense and in this case
they decided that it didn't defer was
the target of a lot of criticism early
on I think because of the performance
overhead that it had and this I think
this was kind of a known issue or it
became pretty well known issue and the
question now is is it still slow a lot
of people deliberately avoid the use of
it and for the most part no it's really
not I mean it's to start off with the I
was there's a lot of overhead but then
in like go one two and one three it
really improved
dramatically and it kind of crept back
up a little bit but today it's kind of
back at where it was and go on three
which in this case this is like with a
mutex lock unlock it's like 40
nanoseconds of overhead so I still avoid
it and really tight loops but outside of
that context it's not really anything to
be thinking about interfaces are
interesting and go and obviously they're
a major part of the language and
especially empty interfaces which are
used I think is a workaround to the lack
of generics and go and it can lead to
some surprising performance implications
so I think in the format package there
is an interface called stringer and
let's take a tight binary which is the U
n 64 well I'll say we're on a 32-bit
machine so that takes up two words and
binary implements the stringer interface
so the way interfaces work and go is
they're actually two pointers there's a
pointer to an eye table which contains
the type that implements the interface
and then a list of the function the
function table and those are the
functions that on that type that
implement the interface and then the
second pointer points to a copy of the
data and so that's used as like the
first argument to each of those
functions and so there's some
indirection that has to happen right
when we call an interface method we have
to resolve that function and then we
have to resolve the data that's used in
that function and we can measure that
overhead and in this case it's like two
nanoseconds so you're probably wondering
so what but when we start to look at
some more realistic ish scenarios the
ramifications become a bit more clear so
let's look at an example we have an
interface sort of a lie face we have a
struct sortable that implements that
interface and then we have these two
slice types sort of life fates by number
and sortable by number which is a slice
of so this is a slice of the interface
a slice of destruct and then he's both
to implement the logic needed to sort
them so the Len the swap the less and
then we soared a hundred million of
these things and the results are kind of
surprising especially with earlier
versions of go the difference is pretty
staggering but even with newer versions
like 1/8 you know that's about 25
seconds to sort 100,000,000 structs and
100 seconds to sort 100 million
interfaces and it's sorting the same
thing it's just one is using the
interface and one is that and so we can
look at the profile for this and it's
kind of hard to read but this is the the
interfaces that we're sorting and we
spend about 69 seconds inside of sorts
sort and we spend about 12 seconds
inside of less and 37 seconds inside of
a number and that's what we would expect
right we're just sorting a bunch of
things so you would expect to spend most
of our time inside like the insertion
sort logic and so we look at the profile
for the struct version we spend about 18
seconds inside of sort and there's
something missing here that was in the
previous profile which is that call to
to number it's not happening here and
what's happening is in lining and so the
compiler is just taking that call and
putting it directly into the call site
they can't do that with the interface
version because of that dynamic dispatch
so we can verify that with the dash M
compiler flag of the GC flags and sure
enough is two calls to sortable that
number that are being in lined on line
84 and that's inside of less there's two
calls to number so the compilers is
taking those two calls and then moving
them directly into that less function so
we can disable inlining with the dash l
flag and kind of level the playing field
a bit so to speak and look at the
resulting profiles on the left is the
struct version on the right is the
interface version they look kind of the
same basically which is what we expect
there
calling through the number now and with
the interface or with a structure we
spend about 2.3 seconds inside of number
and with the interfaces we're still
spending over 42 seconds inside of a
number so there's still a lot of
overhead involved with that interface
call and we can look at the performance
of method indications and again plot
that across different releases of go and
obviously just calling a map calling a
method on a struck directly is the
fastest and it's basically constant and
then the interface adds a little bit of
overhead but it's also kind of constant
and then what's changed a lot is the
performance of like type conversions so
going from like using a type assert or a
type switch to convert from the
interface to the concrete type and then
calling a method on that and in go one
five that was when this conversion to
the concrete type pointer was inlined
directly into the runtime and in one
seven that was in the static single
assignment back-end the remaining type
conversions were in line so now just
doing the type conversion using one of
those two methods and then calling the
method on that is basically on par with
just calling a method directly on a
struct which i think is pretty
impressive and it's one more case I want
to look at with interfaces which is the
empty interface as I mentioned empty
interfaces are commonly used as a
workaround to lack of generics and go so
we have these two benchmarks benchmark
art I face benchmark art string and
functionally they're the same the only
difference is the interface version
calls this function that takes an empty
interface argument and the the string
version takes a string argument but
you'll notice that these print boolean's
are always false and so these print
lines never actually happen in either
case we run the benchmark and the
interface version is quite a bit slower
and we can look the profiles and see
what's going on and on the left is the
struck version and there's not much
going on right which is what we expect
benchmark artistry
is just calling our are extreme function
and that's pretty much it and then on
the right is the interface version
there's a whole bunch of stuff going on
and pretty much all of that stuff is
garbage collection it's all more or less
related to garbage collection and if you
look closely that's where the actual
call to our guy face happens so if we
zoom in on this what has to happen is
the runtime has to convert our string to
the empty interface and to do that it
has to allocate that empty interface and
so it does that by calling this runtime
conf T to e to convert from the concrete
type to the empty interface type that
calls runtime new object that calls
runtime malloc GC which does the actual
allocation that obviously causes all of
the garbage collection to happen so we
can just kind of verify that and look at
the assembly using the - s compiler flag
and sure enough there's that call to
runtime con 50 to e that happens before
our guy face gets called and then on the
right that's the assembly that go
assembly for string version and
obviously it's a lot more concise but
it's also missing that conversion call
and so this was an insight we gained
while working on go data structures
which is if performance really matters a
lot you're you're really better off
writing or generating type specific code
and so in our case we were trying to
write generic data structures or generic
algorithms and using empty interface to
do that and we noticed that we paid a
pretty big performance penalty and doing
that let's talk about memory management
for a bit so byte to string conversion
something you have to be kind of careful
with and go
especially since go I think tends to be
well suited to building like network
servers and so this tends to be a pretty
common operation so recently there are a
while ago I guess there was a
contribution made to Nats which was like
a small two line change
and ended up increasing the throughput
of the server by like two million
messages a second and all it did was
remove an unnecessary string allocation
and if we look at the actual change set
so what this code is doing is it's it's
reading some bytes off of a socket
converting that to a string and then
using that string to to do a lookup on
the map and what this change does is it
just in lines that string conversion
directly into the map lookup so
functionally it's like the same the only
difference is it's like getting rid of
this intermediate variable and yet
somehow that increased throughput by two
million messages a second so the
question is what's going on here and
what happened was there was an
optimization made for this specific case
because it's such a common operation and
doing that that lookup from like a byte
slice and doing a lookup in a map with
that key and we can make the observation
that when that that conversion is in
line in the lookup it doesn't escape
that lookup operation and so rather than
allocating that string what they do
instead is just allocate like a fake
string header and then use the
underlying bytes directly and so in
avoiding that string allocation we
reduce a lot of pressure on the garbage
collector that makes a really big
difference especially in the case of NAT
since that was on like a really critical
path for the server so I think that does
a pretty good job of showing how what
seems like a small change an
insignificant change can have a really
profound impact on performance if you're
not you know looking for that
specifically it also leads us into our
next topic which is memory allocation
and early on and go you could get some
really big wins by being a bit careful
about how you manage your memory and
basically just trying to subvert the
garbage collector and try and like reuse
memory and things like that using things
like slab allocation arena allocation
etc and I think and go one for maybe
sink top pool was introduced which
basically just retain
a pool of objects that it reuses between
garbage collections and what's
interesting is at least in my experience
sink that pool tends to outperform a lot
of these other low-level techniques like
the slab allocation the arena allocation
these are doing some low-level kind of
unsafe things and they're like retaining
large chunks of memory between garbage
collections in sync that pool those
objects still get garbage collected and
yet it still tends to outperform and so
it's kind of an interesting bit of
trivia but how is it that sink that pool
is so fast and the answer is the kind of
cheat a little bit and they use some
some runtime calls that aren't available
outside of the runtime to do this kind
of per CPU storage and so the way sink
top pool works is when you try to get an
object from that pool they just they pin
the current go routine to the processor
they disable preemption and then each
processor has its own local pool of
objects that they pull from and if that
local pool is empty they'll try to steal
from another processors pool before
allocating it as a last resort and in
doing that we avoid a lot of the
contention issues that we otherwise run
into and so last I checked there was an
open issue for providing some kind of
API that would allow you to kind of
write your own data structures using a
similar mechanism as sink pool and I
think the go team is trying to find the
right balance between you know an API
that's high level enough that it's not
like easy to shoot yourself in the foot
but it's low level enough that it's
actually useful so again one of those
kind of trade-offs or balancing acts
that you have to make and I'd be remiss
not to talk about concurrency since
obviously that's a big part of go so
we'll wrap up by talking about that and
some multi-core programming techniques
so I think the use of sink atomic
overall is kind of discouraged and go
and you know they say we generally don't
want sink atomic to be used at all
experience has shown us again and again
very very few people are capable of
writing correct code that uses atomic
operations
well hold my beer and writing lock free
code is hard right don't get me wrong
but really it's just a matter of like
replacing some mutexes with some
compare-and-swap operations right look
how hard can it really be so this is a
problem I explore it a little bit within
the problem of topic matching which is a
kind of a common problem in messaging
middleware which is we have a set of
subscribers we have a stream of messages
and we want to match those things up
efficiently there's a lot of ways we can
do this right we can use like an
inverted bitmap which is a you know
simple but pretty effective way of doing
that we could use a tree there's a
pretty good compromise between like
performance and space efficiency you
could put a lock around that and make it
concurrent but I wanted to solve this in
a way that would scale to a high level
of concurrency and a large number of
processors right hardware is shipping
with more and more CPUs these days and
go positions itself as a highly
concurrent language and so I figured
this would be a good fit for it so I
turn of the existing literature and sort
of a similar space which is concurrent
trees and it turns out we could take the
same idea and kind of put a twist on it
to solve this problem and so the way
this works is we just introduce these
like indirection nodes into our tree we
do our compare and swaps on those I
nodes to avoid putting it into an
inconsistent state and it scales quite a
bit better than the locker version and
this is just on like my quad core
MacBooks there's only four physical
cores but at least up to the four
physical cores of skills pretty well
after that there's like hyper threading
involved but it's also quite a bit
faster than the lock version and one of
the added benefits of this is we get
constant time snapshots of our tree
so with the lock version if you want to
take a snapshot of that tree we have to
acquire the lock copy the entire tree
and then release the lock whereas with
this concurrent version if you want to
take a snapshot of that tree we just
copy the root node of the tree to a new
generation since it's a persistent data
structure and so the way this works is
we assign a generation to each I knowed
we use the empty struct and go to denote
a generation we could use an integer but
then we'd have to deal with like integer
overflows and so we can just push that
problem off onto the heap instead and
then to make it change this tree we just
add a new node we copy the the parent I
node with the updated branch and then
the new generation and then we just do a
compare and swap on that parent but we
have to do two things atomically here we
have to compare that parent I know to
detect if the tree changed underneath us
so if somebody else came along and made
a change but we also have to compare the
generation of the root of the tree to
detect if a snapshot happened underneath
us and so we have to do these two things
atomically and kind of this this
generational compare and swap which it's
similar to like a double with compare
and swap which is doing a calves and the
two contiguous addresses but it's
different in that we're actually doing a
calves in the two non contiguous
addresses which isn't an operation
supported on any modern architecture
that that I know of at least but it
turns out we can solve this with just
your run-of-the-mill compare and swap
and kind of this two phase commit
fashion and so we do that but we still
run into data races or race conditions
and the go race detector it doesn't
protect you from doing dumb stuff so it
guards against data races but it doesn't
guard against logical races and so if
you're using things like sync atomic and
unsafe all bets are off and so what
happened was this this generational
compare and swap was always succeeding
despite the fact that snapshots were
happening on the tree the generations of
the nodes
changing but our generational this
compare-and-swap was always succeeding
and what happened was like I took an
assumption from Java which I had more
experience with at the time which is in
Java when you do new object equals new
object those things are always different
from each other because they're
different
keep allocated things I have different
pointers different addresses but if you
recall we're using the empty struct and
and go the empty structure needs zero
size value always occupies the same
address and literally the last sentence
of the go language spec reads that two
distinct zero size variables may have
the same address in memory I didn't read
the go language spec at least I didn't
scroll all the way down to the bottom to
see that and so the fix you know once we
know that it's pretty trivial right we
just add this unused field to our struct
and our stuff just magically starts
working and so that the side note here
is unsafe as the box suggests is in fact
unsafe but also if you read the go on
compatibility agreement
it states that packages that import
unsafe may depend on internal properties
of the go implantation we reserve the
right to make changes to the
implementation that may break such
programs so not only is unsafe unsafe
but if you're using it there's no
guarantee that your code will continue
to work if you upgrade to a newer
version of go and so it really is kind
of the olo package of go and you have to
be you have to be careful with it and
the other the key insight here is that
struct struct layout can make a really
big difference not just in terms of
performance but in terms of just the
functioning of your code and it's also
worth pointing out that the goal
language spec doesn't make any reference
to the word stack or heap and so yeah we
can get some big wins by being careful
about when we stack allocate versus heap
allocate but as far as language is
concerned there's no real guarantees
around that
but it's this idea of mechanical
sympathy which is having an
understanding of kind of the low-level
mechanics of how things actually work
can really help us and kind of the
classical example I think of mechanical
sympathy is false sharing and as Dustin
was working on NGO data structures this
is this is a ring buffer taken from Coe
data structures he noticed that this is
the performance of this ring buffer was
quite a bit worse without these padding
fields in between and the way this works
is we have this cue field in this DQ
field and they're being accessed
concurrently by different go routines
and so as one go routine accesses the
cue field the processor pulls that into
its it's cache and that gets evicted
from all of the other processor caches
and then vice versa so as the other girl
can access the DQ field you know it does
the same thing so they end up thrashing
that cache line and so by adding these
unused fields we basically push those
into their own cache lines and avoid
that contention and that has a material
effect on performance and then similarly
you know we can look at the scale
scalability of like the channel from go
and how that compares to the lock free
version of that and this is looking at
for like one or two cores all up to 16
cores and it scales quite a bit more
poorly than the lock free version and
the readwrite mutex from a sync package
also scales pretty poorly with CPU count
and this is kind of a known issue I
think and the reason for that is readers
all contend on this reader count field
when they try to acquire a read lock and
so what happens is you have these
readers as they go about access or
acquiring that read lock you know they
pull that into their cache that gets
evicted from the other caches and they
kind of go back and forth playing this
game of ping pong with that cache line
and so the way we fix that is you know
we can fix it the same way we solve any
horizontally scalable problem which is
we can partition and we can assign a
mutex to each processor
and that way as these readers go about
acquiring that read lock they don't have
that contention amongst amongst
themselves and then as a writer if I
want to acquire a write lock I have to
go through each of these acquire that
before I can proceed
so for writers is really expensive but
the whole point of this is for really
read heavy workloads across a large
number of cores so the question is how
do we actually build something like this
and go it's it's similar to sink top
pool and that has kind of that per CPU
mechanism but if you remember sink top
pool is using those those internal
runtime calls and we can't use those and
so the question is how do we create that
mapping from CPU to readwrite mutex we
can kind of hack it and we can drop down
into the assembly and and call at least
on AMD 64 architectures we can call the
CPUID instruction and so that gives us
the api cid of the processor that we're
running on which is basically a unique
ID of the processor that we're on and
then on linux at least we can read from
proxy PU info that gives us a list of
all of the processors that are available
to us and then from there we can create
that mapping from processor to mutex so
this is called a big reader lock by the
way this kind of distributed readwrite
mutex and as you can see it scales a lot
better in comparison to the readwrite
mutex from the sync package a crop so
looking at for like one all the way to
64 cores for for really read heavy
workloads it scales a lot better and so
this looks like in memory is we have
this contiguous array of mutexes one per
processor and we know each of these is
24 bytes wide and on a modern processor
the cache line size is 64 bytes wide and
so we still run into that false sharing
issue we saw before because two of these
fit in a single cache line and so we
fixed that the same way we we fixed it
before which as we add that padding the
cache line padding to
prevent that fall sharing problem that
basically pushes them into their own
cash lines we avoid that cash contention
and again that has a pretty material
impact on how this thing scales in
comparison to without that padding so I
haven't had a chance to play around much
with the new concurrent map that came
and go 1-9 recently but I didn't notice
through using empty interface throughout
the API so that's still happening I
guess generics so it's kind of an
unrelated side note you know it's I find
an interesting go-go positions itself is
a highly concurrent language but at
least in my opinion it really doesn't
bring a lot to the table as far as
actually making it easy to build highly
concurrent systems so I think there's
still a lot of work to be done as far as
providing better primitives better
abstractions for actually building these
things conclusions the standard library
provides general solutions and they're
generally what you should use seemingly
small idiomatic decisions can have
really profound impacts on performance
and the go tool chain has a lot of tools
for analyzing your code it's well worth
your time to learn those and to use
those and the go compiler and runtime
continue to improve and the corollary to
this is the performance profile of your
your code can change dramatically
between releases and so you should
always go back and verify optimizations
that you've made to make sure they're
still valid relying on assumptions can
be fatal read the go language spec read
the go on compatibility agreement and
really code is marginal I think in terms
of performance and what I mean by that
is really the big wins come from
architecture and so it's important to
get your architecture right first and
then you can start to look at more of
the code level micro optimizations
because I think that's where your return
on investment tends to be a lot smaller
but at the same time peeking behind the
curtains can pay dividends and it's this
idea of mechanical sympathy and I think
every engineer owes it to themselves to
at least have a working understanding of
like how the abstractions they use
actually work and above all optimize for
the right trade-off optimizing for
performance usually means trading
something else often return for that
like code maintainability for example
and there is no one right answer as far
as when that trade-off makes sense it
all depends and that's it thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>