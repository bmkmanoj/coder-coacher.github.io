<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;BayesDB: Query the Probable Implications of Data&quot; by Richard Tibbetts | Coder Coacher - Coaching Coders</title><meta content="&quot;BayesDB: Query the Probable Implications of Data&quot; by Richard Tibbetts - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>&quot;BayesDB: Query the Probable Implications of Data&quot; by Richard Tibbetts</b></h2><h5 class="post__date">2015-09-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7_m7JCLKmTY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning hi yes I'm Richard Tibbets
we're going to talk about Bayes DB brief
bio I spent most of the last 15 years
working on streaming SQL real-time
streaming systems which meant I spent a
lot of time looking at programming
languages and databases and thinking
about visual programming environments as
well as query languages and that's how I
ended up reconnecting with the problems
to computing group and in fact working
on Bayes DB and the Bayesian query
language probabilistic computing group
is an academic research group and so you
know just to acknowledge there's a this
is not work that I joined the project
less than two months ago this is work
that is largely accomplished you know
not by me I didn't write most of the
code I am just the person who wanted to
come here and talk about it because I'm
very excited about what we're doing
so query the probable implications of
data what does that actually mean or why
would we do that actually so why would
we want to query the probable
implications of data because it's like
hard to think about probability to think
about statistics to understand what your
data means everybody's getting excited
about big data but people are making a
lot of mistakes people doing science
people doing sociological research
people doing business healthcare they're
doing a lot of mistakes and they have a
lot of trouble feeling confident in the
results of their analysis when someone
tells them something because the
computer said so they have an inherent
willingness to believe that the computer
is probably right but when they try to
drill down any deeper than that you know
it's it's tough and so more often than
not people are willing to discount data
analysis in favor of their gut check
unless they really understand it and so
in order to get there we need to get to
a place where more and more people can
understand data analysis and can
understand what it means to infer
something from empirical data and
academic computer science broadly
considered well let's not throw rocks at
academic computer science let's just
throw rocks at databases because I'm
from the database community and systems
programming databases have done a lot
over the last 30 years they have mostly
made things faster and prettier and
consumed a lot more data
and better index and faster and faster
but in terms of the ability to ask
really new different questions of your
data
you know since business intelligence and
hypercube type stuff and not a lot of
adopted innovation coming out of the
database community for new kinds of
questions so everybody understands
databases so if I have a database that
contains a population so it's a contains
some samples and people so this is what
would be healthcare data maybe it has
combination of columns it could be
actually quite wide but it doesn't fit
on the slide then has some columns
related to demographic information
healthcare costs clinical indicators you
know just basic stuff that you would get
at any doctor visit but then also more
substantial indicators that someone's
been diagnosed with some issue or
something along those lines and so it's
it's a wide data set it's a large data
set potentially it tends to be a bit
sparse in some places so a lot of times
we don't know if you have heart disease
or not most people in the room don't
know if they have heart disease or not
and that's interesting so people know
what select does select is kind of the
go-to keyword in SQL so when we select
from data we pick out a subset of the
data so I start with a bunch of data and
I get less data that's good so what are
we talking about with querying the
implications of data well we can
introduce a new keyword we can have a
keyword called infer in fact we sorry I
should say we have introduced a new
keyword and by the way I'll get to it at
the end but this is actually software
that runs and you can run it there are
some caveats it's not ready to go
diagnose people with chronic heart
disease but it is is runnable it is
something we would like people to be
doing some work with and so what is
infer do so infer says I want these
records with this predicate but I
actually want you to generate fill in
missing data fill in stuff that I don't
know as long as you think you know it
with at least confidence 0.7 and a key
thing we haven't done here is to say
what the data model is or how the data
should be modeled or which model is no I
didn't say random forests I didn't say
scikit-learn
there's no R code there's like nothing
what we're focusing on with bql and with
this querying the implications of data
is to say what are the
primitives that make sense what are the
things that you can ask of a modelled
population a population that has a set
of models or model associated with it
and let's make sure that we get the
utterances right and then see what we
can do with that
so infer says find me patients where
their age is greater than 30 and what
you can see is that I was able to infer
for example the total costs for patient
133 I was able to infer the age for
patient 132 but I wasn't actually able
to infer whether patient 131 has heart
disease so I can leave that blank I can
do more things so I can actually look at
I can inspect the model I can say what
do we how do we think about this data
what's this sort of what's important so
in particular I can say what columns in
my population predict height so what's
the predictive probability and this
doesn't say that there's some linear
correlation it doesn't say that bigger
age is bigger height and bigger gender
is bigger height that doesn't even make
sense it just says that these are the
columns that are most useful if you
wanted to predict someone's height so
that's kind of that this is in somewhat
intuitively obvious for height you know
age weight gender although the fact that
chronic kidney disease had an impact on
height might be an interesting insight
and if you run this on some more
complicated data and you start to ask
you know which of these columns that
represent biomarkers or metallurgical
indicators are most correlated with
having a particular form of diabetes or
malnutrition then the predictive
probability becomes very interesting in
fact you can also turn this around and
say if I was doing a survey and for some
reason I needed to predict people's
height what questions should I ask so
you can also tell you what do you need
to know in the future to predict
something so there's this notion of
estimating dependent probability and
then finally there's a simulation so
what if scenarios to use a business
buzzword so we can not only infer
missing data and talk about the
predictive power but we can also
simulate new data and so in this case
I'm just making up new patients all
under age 10 and trying to get an
understanding of given my model what
would they look like what would patients
that that you know how often do they
have chronic
disease that sort of thing this is
helpful for filling in blind spots and
we'll get in to that a little bit later
but it's you know I may not have many
observations of patients under age ten
with chronic kidney disease but if I
have a rich enough model then I can
invent new observations and use those to
think about problems so what's Bayes DB
Bayes DB is the the Bayesian query
language the B DB
vql which is this SQL like language that
we've been looking at and so that gets
you model independent queries and that's
actually one of the most powerful things
that we have going here is the idea that
you can think about the question without
knowing about how the data is modeled or
what the architecture is this is similar
to how SQL databases allowed us to
separate asking the question from
thinking about the physical layout of
data on disk
you know once upon a time if you wanted
to summarize some data you had to know
you would use Perl or some cobalt scary
thing and you had to know how the
records were laid out how you wanted to
iterate over the records what tape drive
they were on all that jazz and databases
said no if you get your data into this
format you can then ask it pretty much
any question you want and we'll find a
way to answer that question hopefully
fast enough what we're doing with with
bql is to say if you can get your
question into this format if you can get
your model data into our system then you
can ask any question you want and we'll
give you an answer with a certain level
of confidence if you don't have any
models for it we'll get into that later
so we need to be able to answer people's
questions and so in addition to these
model independent queries we also have
you know that's front of the house we
have back of the house which is the meta
modeling language so that's how do we
represent models how do we think about
modeling data in this system it starts
with an automatic model builder so we
actually have the ability to you know
model data and model basically arbitrary
tabular data to the best of our
abilities and then layered on top of
that is the ability to plug in foreign
models algorithmic and statistical
models then we call that machine
assisted modeling so I'll start with
Bayes divi let's talk about just the bql
part so we're going to work with a data
set
the you've concerned concerned saddle
scientists which is this population of
satellite data so it's all the earth
satellites and we're gonna let's just
start with like one question that we
might want to explore so suppose there's
a satellite in geosynchronous orbit that
we know has a dry mass of 500 kilograms
what's it for who is running it that's
not an unreasonable question actually it
someone from the Air Force tell me that
they have better data and they're
interested but you know what can we do
with the public data so let's flip over
it's this strange loop so I need to give
a demo and that's so what does it mean
to ask that question so we're over here
we're in data science land we're using
ipython notebooks so we're gonna
simulate the country of operator can
people read that do I need to crank it
up higher bigger all right at some point
the plots like go to hell and for some
reason this thing has way too much
titlebar all right all right so we'll
try that the so what we do here is just
another simulate query so I've already
modeled the satellites data I've got a
I've grabbed a a completely modeled
database of satellites with a population
in it and so I'm just gonna create a
temp table where I simulate satellites
that look like my satellite and the
limit there is a little different than
sequel limit but it says basically make
me a thousand samples so I'm going from
a population to an actual table in this
case so then once I've created that
table of samples I can actually you know
do whatever I want with it I can run
selects against it or I can use my
little histogram plotting utility and I
pretty quickly fall out that you know
well this is almost certainly a
communication satellite it's probably in
the United States maybe because we
launched a lot of communication
satellites I don't know exactly why but
it could be from Russia China somebody
like that and so that's like a what-if
scenario like what if this happened
what else would I what else would be
true what else would have happened and
that's in that scenario and we do that
by having a lot of samples and then
being able to do this sort of histogram
type analysis where you can come up with
some amount of probability so you could
say for example there's about a 25%
chance it's a USA
and satellite so what if I wanted to do
that without BAE's DB let's just think
that one through a little bit so the the
normal way if you just hand this to
somebody and say well figure out what a
500 kilogram satellite in geosynchronous
orbit what's it for well the first thing
they'll do is be like well 500 is like a
number so and geosynchronous sounds
pretty canonical so it's I'm gonna look
at like widening my lens on the number
maybe like 400 to 600 so like my wife's
an astrophysicist and I actually don't
know how much satellites weigh or what
the variance is on that so it turns out
if you go 400 to 600 you find two Indian
satellites one for meteorology one for
communications so we could say it's a
50-50 chance that it's communications or
meteorology and it's probably from India
that kind of violates our intuition
because satellites are not generally
probably from India so I could be like
okay well maybe I was wrong 300 to 700
well now I get like well it's more
communications it's the only like 50
percent communications and you know this
is the kind of data exploration that
people are often doing to try to come to
a conclusion on these what-if scenarios
and it's like hard and you know you end
up either needing an enormous amount of
domain expertise like I'm sure there are
people who know the satellite stuff
backwards and forwards and they don't
even need to run the thing they just say
like oh yeah 500 kilogram satellites are
always communications unless they're
spies something like that so that's
that's the simulate what else can we do
we talked about doing these sort of
infer so I've got the same satellite
population and I can start doing things
like data cleansing on it or replacing
missing data so as an example I have
there's a lot of satellites where the
type of orbit is null and I can go ahead
and infer explicit against this and this
is actually a slightly different
formulation the language is more
complicated than just three key words so
with this formulation I'm actually
treating the confidence as data so I'm
predicting the type of orbit and I'm
actually also tracking the confidence in
that prediction and so that gets me back
a table where I have I filled in a bunch
of types of orbits and I can see the
range of confidences so sometimes I'm
you know 70% confident sometimes I'm
much less you know 25% confidence
something like that so that's
interesting and can start to help me
understand like what does my data tell
me like what
what's really implied by that idea that
I can predict some things with
confidence and some things without
confidence and so you can actually we
like these violin plots which tell us
this is looking at inferred confidence
versus the inferred orbit type so this
is like having chosen sun-synchronous
for an elliptical satellite what was my
range of confidences we're having chosen
and so what you can learn from that for
example is that it's easier for us to
predict low-earth orbit satellite orbit
types than it is to predict other orbit
types this is all really important
because remember that slide about you
know nobody trusts data analysis they
don't understand people aren't used to
getting confidence measures on the
answers they get from data scientists
like when you ask a data scientist to
answer a question they just they give
you their best answer and how would you
get a confidence it's a little tricky
like you don't want anyway it's it's
tricky to imagine how you do this with
humans we have some theories what we
could but what we're doing is saying
this is what it should look like when
you can do it and we'll show you a bit
about how we do it with our meta model
what else we do we can also identify
anomalies so another way that you can
run a prediction against some data is to
say okay but I have data and given the
data that I have what is the how likely
is that is that data so in this case
line wrapping what I'm doing is saying
an estimate query so this is about that
so sometimes I infer missing stuff and
sometimes I make an estimate about the
probability of something I really do
have and so what I'm doing here is
saying given period minutes that I have
how likely is that how likely would I
would I have predicted that value and so
I can identify the unlikely periods and
that can be that can be interesting in
particular when I look at some of these
unlikely periods these are all
geosynchronous orbit satellites the most
unlikely periods are all for geo
satellites okay and the most likely
periods it turns out with geosynchronous
satellites actually have a very stable
period you know you just know what that
is and so these are all basically wrong
and one way or another these you know so
it's 24 hours is kind of the right
answer and so in this case it's about
this was a unit error somebody put it in
four hours and in this case it was it
was a decimal points error because
it was a decimal points error at any
rate we're able to find things with
these you know 24 minute 24 minute
periods we're able to realize that that
some of this that some of the data is
wrong and sort of dig into how confident
should I be in the data that I'm
starting with so let's see jump back to
slides maybe there is
so we talked through the different kinds
of queries so that's that's like bql in
a nutshell it's the ability to explore
data by inferring missing values
simulating data estimating the
likelihood of data occurring and it
turns out that's actually most of the
rope you need or maybe all the Rope you
need to really understand and work with
a model two data set once you get into
the composability and what none of those
those capabilities but this would not be
interesting at all if we gave you the
ability to ask arbitrary questions of
data and we could not give you answers
so the other half of the puzzle is to
being able to model your data and give
you answers to all these wonderful
questions that you're able to ask and so
this machine assisted modeling because
people can ask infinite questions we
need a baseline answer and we need some
place for people to start as they work
on improving their modeling so what
would you do if you're trying to build a
general-purpose model for sort of
arbitrary data the the default
statistics 101 answer is correlation is
nice these are just two-dimensional data
sets XY which were you know they're
they're not there they're relatively
indicative of data sets that exist in
the real world but they were you know
relatively carefully chosen because what
we can show is that correlation
uniformly predicts that these are
there's no predictive power in this data
that the this the correlation is zero
between these data points because
they're they have this sort of evenness
or balance to them it turns out you know
as a human you'd like to you think you
can get some predictive power out of
that and that's because you're better at
recognizing the shape of the data or
you're better at thinking of the data in
terms of clustering or different ways of
grouping the data subsets of the
information that come out and coming up
with a different model for different
subs
the information and so we have the
system that goes beyond correlation to
do that same logic to come up with the
mutually predictive subsets of the
information which allows us for example
on little systems like this to you know
be able to recover not necessarily a
perfect picture of the shape but
actually a pretty good representation
you know and something a lot of what we
do is yes you could do better it's a
little bit like the old compiler versus
assembly fights yes you could do better
if you're a brilliant modeler or someone
who understands the domain perfectly and
we'll get to that in a minute but it's
kind of nice that the computer can do
stuff all by itself so what's going on
here what we've built is a probabilistic
meta model for inferring model structure
so we're gonna look at your model we
look at your samples your data and try
to identify looking at the columns in
the rows the right set of views to have
against this data and the right set of
categories to have against this data so
you think about this is just kind of
slicing your table so if I'd that table
of health care information I might turn
out that like some of the demographic
data is mutually predictive some of the
health informations mutually predictive
the demographic data is not super
predictive of the health information or
maybe it sort of is so it's trying to
figure out like what's the right way to
slice and dice this data and when the
data gets large there's this
combinatorial explosion even to finally
have a three by three matrix it's 205
possibilities it goes up pretty fast and
so that's why I'm here from the
probabilistic computing group because
the key insight to drive automated
modeling is the ability to really get
our arms around this problem and realize
that we don't need to solve it in any
way precisely or or you know with one
single answer but what we do need to do
is come up with a way for efficiently
getting to a good approximation in fact
what we're gonna do is efficiently get
to about 100 good approximations because
computers are cheap and actually we can
get there pretty quickly so this is an
example of running the metamodel process
which is basically we initialize an
ensemble of models and then we try to
adjust the cluster noisiness
so we look at each of these little
groups and say how good is the mutual
prediction within this group and if it's
or how would the mutual projection be
better if I kicked this element out and
put it somewhere else and then we just
iterate that for long enough you know
Markov Monte Carlo Markov chain they'll
be a little bit more on that later today
and what we eventually get to is you
know a pretty solid decomposition of
this so that was sorta that was building
a model for animals based on their
traits you know so looking at animals
and you probably can't see it on the
slide but the system was pretty it was
able to identify the traits that relate
to being a sea creature and cluster all
the sea creatures together and this is a
nice little video because it it it's a
human could sit there and do the
categorization it's a small and a set of
data and come up with and have the same
sort of disputes about what the exact
right categorization is and then
eventually end up with something looks
not dramatically dissimilar from from
what we get and what we do then is to do
that about a hundred times or enough
times so that we have an ensemble of
models that we can use and we can model
the data repeatedly and that's one of
the ways that we develop this notion of
confidence which is am I getting the
same answer from all the models so what
does an automatic model tell you it can
tell you this sort of pairwise
dependence probability and this is
actually a really interesting one
because it sort of tells you the
predictive power of your data this is a
heat map that we like people like
looking at what we've done is to
identify a dark square is a look it's
the same columns on the XY acts on the x
and y axis and a dark square indicates
that two columns are predictive of one
another and the cluster in the top left
corner in this instance is shows you
that basically countries tend to by
satellites from the same contractors
there's a consistency you can predict
what contractor a satellite comes from
if you know what country it's from or if
you know what contractors building it
etc and then in the the rest of the
picture you actually have you see
clusters around the physical
characteristics of the satellite so
we're trying to what we're effectively
doing is recovering is recovering the
orbital mechanics of this the details of
how that could work in fact let me flip
over the notebook
and what we can see is that you know so
there was that graph that I showed you
where we'd recovered mobile mechanics if
we look at just a baseline correlation
so it's not just my contrived data sets
that fail at correlation if I run
correlation on the satellites with orbit
we get nothing for orbital mechanics all
we get is that the countries are
correlated with the owners which is not
unreasonable but we end up missing out
on all those details of orbital stuff of
you know masses things like that we do I
think find out yeah we do know that like
launch Mass dry mass and power are
correlated that's the other thing that
correlation can pick up but as for the
details of how the orbits work it's it's
somewhat hopeless and a data scientist
would tell you well you just need to
pre-process your data hard enough and
you'll eventually get there but that's
the goal is to say what can we do
automatically with you know relatively
lightweight with minimal pre-processing
so how can you refine the model what can
you do to make the model better well one
thing you can do is tell us things we
shouldn't bother considering so a lot of
times we get data sets that have where
two columns are like a strict function
of one another height weight and height
weight ratio and you're like the system
ends up you know chewing up a certain
amount of CPU and assigning columns in
different places because it's in the
parts figuring out that height weight
and height weight ratio have like a nice
structured correlation so we just tell
us ahead of time that that's true or if
you tell us ahead of time that you know
I know my data is skewed in some way but
actually these two things are
uncorrelated it's just an artifact of
how I got my data so you can give us
assertions about dependence and
independence and this is the language
that we're looking at expanding like
what can people say here but you can
also realize that you know if you're
doing satellites there's like this guy
who spent some time figuring out how
satellites worked and so you can
actually take Kepler's laws and say okay
I actually have a physical model for how
this part of the data is supposed to
work and I can actually look at
extending the metamodel composing
together multiple models I'm combining
the default model on the bottom with a
foreign model of Kepler's laws I've also
tagged in a random forest model which is
just sort of an off-the-shelf model from
statistics or machine learning I guess
would be more accurate and so by doing
that I can actually get to something
that is much more predictive that it
does a better job predicting all those
mechanic pop parameters so if this is
something actually care about it makes
sense to take the time to implement
Kepler's laws and they're over here in
this tab and so then I end up with a a
modeled population where the model is
actually derived from this sort of
physical reality this stuff that I know
about physics and so what can I do with
that I can well I can basically do my
standard simulation trick so I'm going
to simulate Apogee and perigee from the
satellite so the nice thing about this
simulation is I'm going to be simulating
the orbital mechanics parameters from
using Kepler's laws but the inputs to
Kepler's laws are going to be generated
by the default model so I'm combining
multiple modeling technologies together
here and so in order to do that or when
I do that I get this plot that's it this
takes a minute to digest
I'm getting used to the process of like
looking at a graph slowly I spent too
long in business where you look at every
graph and expect something to go up into
the right so looking at this a little
bit more slowly the purple line down the
center is basically you know the
possible domain of answers to Kepler's
laws like that's what Kepler's law says
possible Apogee and perigee are given
that period you can't be outside of that
other than noise noise in your
observation of course there's always
noise in your observation the blue dots
which are clustered along that line
unsurprisingly are the blue dots are the
actual object observed satellites the
red dots are where we simulated things
and so you can kind of have two
reactions to this you can say like wow
your red dots are like all over the
place or I can see like yeah but like my
red dots are like not that badly all
over the place think they're kind of on
the line and that's kind of nice
and in fact you know notably the
computer does not know that these are
satellites or and it knows nothing about
Newtonian mechanics or any of that stuff
and we got you like partway there but
yeah I mean if you're unsatisfied with
the default model for orbital mechanics
that's great you should not use the
default model for overall mechanics you
should plug in the green dots which are
an orbital model simulation when we done
with the green dots it's a little bit
more sophisticated than just
implementing Kepler's laws we've
actually created a model that's trained
to identify the noise that's likely that
comes from the data set so actually
train your
mechanic model because we wanted to
represent the data we're actually going
to see not the idealized physical world
and so that's why the green got sort of
move off the line a bit but in theory
and I think in reality the green dots
move off the line about the same amount
that the blue dots do and everything
ends up copacetic there's different ways
we can look at that we can say for each
dimension you know how much error are we
getting in the red and this is always
fun so this is why one of the things we
like to do is model a data set and then
work with a domain expert to understand
like but what's the right answer and
then start to understand where do we
deviate from that or what's what's to be
learned from that process yeah and this
is a different way of looking at that
the two purple lines are different
eccentricities of the orbit and you also
can see here that the red dots do a
decent job filling in between the purple
lines there's actually a surprising
there are surprisingly eccentric actual
satellites in the database we ran out of
domain expert time before we actually
get to the bottom of that and we also
have a big cluster down here which is
we're obviously also a lot of the the
actual satellites are so that was a
story about how you can refine the model
so we talked through so we talked
through Bayes DB which is you have this
bql thing which is allows you to make
utterances about inference about
inference reasoning against datasets or
against puck I'm not supposed to say
datasets inference reason against
populations which is a population is a
model as a modeled it's modeled data so
you do inference against populations and
then you have this meta modeling
language is how you build these
populations how you get the model sorted
out how you train things up how you
integrate third party systems and
between those two things you know we
have a system that makes makes it
possible to query the probable
implications of your data and we think
makes this kind of inference based
reasoning or empirical inference much
more accessible both from people who
want to just understand what question
was asked and also people who want to
get their hands dirty doing more
modeling so we're here to talk about
future programming we're pretty clear
other where future what are we doing in
terms of probe
so there's some aspects of program which
are just like the query language this
and actually there's some aspects of
programming which is like the system's
infrastructure getting this stuff right
so Bayes DB we're is proudly open source
apache license as of september 22nd
you can pip install it it's a
combination of Python and C++ code you
will need boost and all that stuff you
can read the documentation but you know
you can pip install it and run Bayes DB
demo and you will actually see what I
showed you today - the orbital mechanics
that's on a branch in the repository
machine assisted modeling is something
we're actually still holding back on so
where we are right now is we've we've
released it you can do bql you can
download model populations and you can
interact with them we're still we're
running an alpha process for people who
want to actually model their data I'll
talk about that in a moment and we're
looking at building out that as a as a
service so that we can actually help
people model their data without having
to get on the phone with them but also
still not leave people sort of
unanchored trying to model their data in
isolation because we are still doing a
lot of work around assessing quality and
safety of these models and understanding
when do we do a good job when do we do a
bad job what do those words even mean
and in particular we're seeking
compelling datasets for collaboration
and ideally publications so if you have
data that's interesting and you're a
domain expert in that data and
especially if that data is something
that can be pumped can become public we
would love to talk but so future
programming so one thing about
programming we started out with a
command-line shell for this thing and
then as we talk to people who are
actually go main experts actually doing
data science with real data and
astrophysics and healthcare and things
like that they were like not fans of the
terminal you know read line command line
shell and they said everybody uses
notebooks you should use notebooks okay
so we recast the thing over the last
month is a notebook and it works great
in notebooks and you can do plotting and
all that and Python notebook tracks are
kind of nice they're good for giving
demos but it's still like not the best
interactive environment so one question
for this audience or for people who are
thinking about future programming is
like what's the interactive environment
supposed to be like for this and there's
a bunch of questions that are around
data visualization or on query languages
around usability you know when should
people be able to ask for you know the
machine to spend more time
doing inference on a particular subset
of the data or in a particular query how
does it make sense even ask that
question you know should we immediately
give people bad answers before the type
machine has had enough time to analyze
the data or should we wait until we've
reached some baseline quality before we
give people answers there's a whole
wealth of things around usability and
how to interact with this system out
there and then yeah as I mentioned we're
running this alpha if you go to the
website scroll to the bottom you can
apply to be alpha and you can bring your
own data to be modeled we'd like to be
working with people we are working with
people on you know healthcare of
bioinformatics Pharma astrophysics and
are in a range of other things but I'm
talking to people about global warming
and climate data talking to people about
other data sets because you know it
turns out an enormous fraction of
Natural Science is now being more and
more data-driven and the people who are
doing that science don't necessarily
have the best tools and so there's
opportunities for us in the computer
science or programming community to work
on giving them better tools so I could
take questions I think if that's what we
do yes
and this is okay yeah absolutely that's
good I love what you're doing why'd you
use sequel because nobody's learned data
log in a hundred years the because so
this is great question like I fought oh
oh sorry to repeat the question the
question was why did you use sequel
syntax or possibly just I object to
sequel syntax so let me answer why you
didn't use sequel syntax that's a better
question is like why didn't you do
something else think about or why don't
you change now because we did just
change from like a command-line shell to
fights on notebook so why don't you
change now so questions like what should
we change to you know the sequel is kind
of a lingua franca in this space like
everybody knows it even if they don't
like it and so that's it's that the sort
of pseudo English phrasing is helpful to
certain kinds of accessibility I'm happy
to hear that there's a different
language structure that's that's it's
you better accessibility but it's
something that people do feel like they
can understand it's something that
people were trying to get to use the
data feel like they understand and you
know I spent a while fighting about this
at stream base so we you know stream
base we had a visual environment for
building queries people really wanted a
query language they almost universally
wanted it to be SQL so that was like
another you know shove in the direction
of SQL there is a question about whether
we're trying to be remotely sequel 89
compliant or anything like that
it is nice that as you're in the middle
of modeling your populations you can go
back and forth between inference based
modeling and just regular sequel syntax
so you know the histograms and the group
buys and aggregations combining data
things like that so there's there's a
lot of benefits to it and you know it
wasn't the only serious suggestion that
we considered was something in the link
type space which is I think only a
difference of degree from sequel but I'm
happy to hear things in fact I guess
I'll ask you again and you can tell me
the answer
yes do a follow-up and all of them
eventually build sequel yes yes yep
right that is a lovely insight so the
statement was you know we have a golden
opportunity to make people learn
something new or to at least experiment
with something new because we're giving
them a whole bunch of new superpowers
and they'll they're willing to learn
terrible languages to get them they're
currently using sass so they have no
taste so we might as well try to give
them something of interest you are
absolutely correct and they're one of
the major reasons III justified coming
to this conference was because we are a
small research group that is able to
focus on certain things that we are very
good at the process of exploring the
programming language design space around
this it may not be one of those things
one of the reasons to open-source it is
to understand is to make it something
that other people can do research on
that other people can base things on and
so if there are people who do want to
have a different query formalism that
they like better who or want to suggest
one and in particular if they want to
like dig in and build one on top you
know pull requests appreciated it's on
get up anything else anybody else yes
you know a few hundred but it's actually
it's a so I've been told actually let me
know sorry repeat the question what what
how many dimensions does your model
scale up to and that there's like two
ways to answer that the the sort of the
intellectual answer is that's actually
the wrong way to think about the size of
your data and in particular a lot of the
question becomes well what what do you
know about the data how much can you
tell us ahead of time like if you
already understand your data in a
relatively sophisticated way then the
automatic modeling can just you know
fill in the constants for you and get it
done if you're coming in with data that
you know nothing about then there are
lots of places where you can find
yourself outside the the design envelope
for the automatic modeling and you know
hundreds of columns and hundreds of
columns is kind of the way you should be
thinking about it if you have something
with 30,000 columns which one of the
people I'm talking to does you should
ask yourself if you really have time
series data or what exactly you have but
we're also happy to we are very
interested in data sets that are outside
those parameters because from a research
perspective the most interesting thing
is stuff we can't model that we should
be able to other questions yes sure yeah
it depends a bit depends a bit on the
query like you know as you probably all
notice that I heard some of you noticed
that I didn't actually run them when I
was going here so if I go let's see if I
can get this going
you toolbar I mean relatively quickly
the goal is for they they should be
delivering interactive performance some
queries do not and one of the things
that we're actually working with right
now is demo itís one of things we're
actually working with right now is
understanding how do we signal to people
that the query they just asked for is it
is computationally expensive so like
simulating a lot of satellite it turns
out some of the simulations against
Kepler's laws are very expensive because
our implication of implementation of
Kepler's laws only solves them in one
way and we need like a closed form
solution the other way and we didn't
write one
so sometimes it's like what your model
is slow in one way and not the other or
something along those lines but for
example let's see so run the simulation
of creating the table that actually that
just ran and completed so whatever 15
seconds actually let me make that real a
few I give up
oh oh oh right speaking of things we
don't like about sequel yeah okay so so
we can we can do a lot of simulations
you know we can do very quickly some of
the inferences we can do quickly this
pairwise probability this pairwise
predictive probability is still scales
as like basically N squared there's an
opportunity to improve the performance
of that through some algorithmic
optimization that we haven't done so it
basically asks it Basques it runs a
query for every cell the heat map so
that takes a while like actually I can
how long does that one take we'll see if
that finishes before I'm done talking
other questions yes
yes yes okay
that's that is a very good question and
if there are no more questions class is
dismissed
you know that's actually so the answer
is the the the generative predict I'm
sorry oh I'm sorry
so the question was what is the
interface that allows those models to be
composed or how do we actually how do we
how do we drive composition from those
models and I'm going to a slightly off
the reservation and see if I don't get
myself in trouble no wait
oh wait I lost it
where is it it's told myself I should
have this open and uh I should I go like
that then I will see all right so
Kepler's laws we initialize a model
basically the two functions you need
implement in the current code base and
we're actually adjusting this right now
is simulate and log PDF so you just the
model just need to be able to simulate
new data points given partial records
are given conditions I should say and it
needs to be able to give the probability
distribution function for a given a
value how likely was it was it to have
seen this value and that is I think
enough rope we think enough rope there's
a math paper that says it's enough rope
and there's another speaker in a minute
who's who wrote the math paper so I will
appeal to Authority and be done so
that's so that was the answer to the
question of how do we how do we make
these things composable we have a pretty
thin interface that we think is adequate
there actually then from a database
perspective opportunities to violet
abstractions and make things go faster
so one of the reasons that the Kepler's
is slow is because it has to chew
through that interface like a lot of
times and you could probably come up
with a way to make that more efficient
anyway other questions okay great
oh you have a break in between okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>