<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Narrated Reality&quot; by Ross Goodwin | Coder Coacher - Coaching Coders</title><meta content="&quot;Narrated Reality&quot; by Ross Goodwin - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Narrated Reality&quot; by Ross Goodwin</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9E22CWmOpsk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Ross Goodwin thanks for having me
here it's awesome to be here this talk
is called narrated reality and I guess
I'll get to that in a second what that
means but first I want to play a quick
game
and this game is called actual English
word or invented gibberish so I'm going
to show you a word in the definition and
you're gonna have to tell me whether
it's a real word or a fake word and the
first one is bicker a tape so raise your
hand if you think this is a real word
few people okay think this is a fake
word more people
yeah it's indeed fake okay next one is a
bore hole real word okay like half maybe
a little more than half fake word a
little fewer it's also fake okay next
one is tautology real word like
everybody fake word a couple people
can't believe you guys are all
programmers it's indeed real and we have
the last one is logistic he thinks this
is a real word one person some people
fake word more people yeah I can't fool
you guys it's fake
or can i because you may be asking
yourself was that the real definition of
tautology and in fact it was not this is
the real definition in pathology the
saying of the same thing twice over in
different words the definition I showed
you was the study of the development of
moral principles in the mind and the
opinions of the process and the explicit
processes of people and despite the
etymology and word endings parts of
speech this is not a real definition so
where do these definitions come from
well they came from a baaad I made
called lexicon jure and this is a LS TM
recurrent neural network long short-term
memory but LST M stands for trained on
the Oxford English Dictionary to get a
definition for an invented word or an
alternate definition of
real-word all you need to do is tweet at
lexicon jur followed by your invented
word and the full definitions are posted
to lexicon Rambler calm so this is one
of my favorite definitions that came up
with was the one for love which is a
result of a person's or animals response
to a problem or difficulty she loved the
music of a new employee another version
of the model said the definition of love
was past tense of leave which I found
liked equally amusing so yeah it's it's
quite good at taking an arbitrary set of
characters and interpreting those
characters in the context of actual
words it's a character level SDM so it's
actually writing and understanding
language at the character level so
something weird happened with this bot
which is that I didn't set up the
replies correctly initially and Twitter
flagged as a spam bot and its privileges
got revoked temporarily
I don't usually feel this way about my
BOTS but that left me feeling raw in a
way that I wasn't expecting I know it's
not a person or anywhere near human
level intelligence but it might be
smarter than certain animals the way it
can break down an arbitrary string of
characters as I said you know if you end
a word and ology it'll oftentimes say
the science of capitalized words or
Webster's proper nouns anyway I felt
like Twitter had killed my pet and I
also felt conflicted you know should
anyone feel this way about a machine
what are the implications of that is
this the uncanny valley of language so
before I go any further into my
exploration with neural nets and
computational creativity I want to make
a personal confession which is that I
used to be a ghostwriter not that not
that kind of ghostwriter so how that all
started was I went to MIT as an
undergrad and heading to study physics
and then I met this gentleman Noam
Chomsky started off by having lunch with
him once my freshman year a friend of
mine was a senior who was asking famous
professors on campus if they'd have
lunch with her which is a great idea and
she asked professor Chomsky and he said
yes and I begged her to let me come
and I asked him a ton of questions and
the last one i asked him was can i work
for you and he said yes so for the next
two years I saw him periodically and we
talked about politics and language and I
worked on a project concerning the
foreign policy rhetoric of American
politicians and then I decided that I
wanted to be a political speech writer
so I got fairly far in that ambition I
took this picture on election day in
Chicago in 2008 in June 2008 I moved to
Chicago to intern on Barack Obama's
presidential campaign at the National
Headquarters mostly writing letters from
then-senator Obama to respond to voters
but I also ghostwrote op-eds for
prominent Republican endorsers including
lincoln Chafee who you might remember
from the Democratic primaries in this
past election cycle against my advisors
advice I took the fall 2008 semester off
of my senior year to work on the
campaign through Election Day and I'm
the transition team in Chicago and then
in DC and as a result my first job out
of college is at the White House which
was really awesome as a presidential
writer so I started out as a junior
writer writing birthday and wedding
cards and stuff like that and I got
promoted to writing presidential
proclamations which are like statements
of national days weeks and months of
things they're also used for Policy
action I think Trump just used one for
his travel ban but the ones I wrote were
for national observances most of them
are for national observances so
everything from Thanksgiving in African
American History Month the things like
safe boating week so as you can imagine
it was a really exciting job so I
decided after doing that for a little
while that I wanted like a real serious
policy job so I went to the Treasury
Department to work as a special
assistant for secretary Geithner because
I majored in economics and like I said I
wanted a more serious policy job this is
me in the background of a documentary
about the Treasury Department just
walking by so I started to learn how to
code at Treasury in order to automate
the paper turning tasks I had to handle
like making binders for the secretary in
depth sack and so I found that with a
push of a button I could print
hole-punch collate insert dividers etc
but I wanted to do more than make
binders and without any meaningful
speechwriting opportunities I left and
started freelancing as a ghostwriter
which was
different story entirely so I did this
all sorts of unsavory work I wrote
letters for a mining company to
government officials ostensibly from
concerned members of the local community
but actually all for me this is
something called astroturfing you can
look it up it's it's a very common
practice in American politics these days
so the letters would say something like
you know we want this mind your
government red tape
meaning safety inspections is killing
jobs and I did it
despite being morally reprehensible
because I needed the money and it paid
well because I found a way to cheat I
developed this technique I called
horizontal writing where I'd write in a
spreadsheet and write all the first
paragraphs of every letter then all the
second paragraphs and then all the third
paragraphs and groups and so on you know
in columns for each letter and then
every other day I'd run a simple Excel
macro to rearrange the cells randomly
and then edit them and handed it as a
new batch of letters so I was doing a
job that was supposed to take eight
hours a day in like two hours and I
spent the rest of my time working on
novelist still not finished that's
another story
so I turned this game or this technique
into a game with some friends so we
called it the diagonalization argument
inspired by Georg Cantor's 1891 proof of
the same name for the existence of
infinite sets that cannot be put into
one-to-one correspondence with a set of
natural numbers it's a really beautiful
mathematical proof so how this works is
each column is a chapter each player
fills the diagonal line of cells new
cells may only be written next to
already adjacently filled ones player
one writes the first paragraph or the
first chapter a player two writes the
second paragraph of the first chapter
and the first paragraph of the second
chapter player three writes the third
paragraph of the first chapter second
paragraph of the second chapter first
paragraph of the third and so on so it
can be done with any number of players
and I thought also have something really
new and innovative but it turns out that
people have been doing this for decades
literally mixing writing and algorithms
and this is an example Ramon Cano a
hundred thousand billion poems written
in 1961 is a poem that's designed to be
rearranged keno is part of the oulipo
which is the avada literature potentiel
or the workshop for potential
which is a group that continues to
explore the intersection of language
computation to this day and that's just
one of many examples of computational
creative writing that have emerged over
the you know since probably the 20s or
30s
so my last freelance writing assignment
I was asked to review all the Python
guides on the Internet
and I thought you know I might as well
try to learn Python to like make this
guy better and and one stood out it was
learned Python the hard way by Zed Shaw
and that was pretty much it for me I
basically I didn't even finish the
assignment I just started coding and
that was it
in 2014 I started NYU ITP which is the
interactive telecommunications program
it's kind of like art school for
engineers or engineering school for
artists and I wanted to continue my
creative work with computation and
writing so I did a bunch of projects my
first year but I want to sort of fast
forward to 2015 the first project in
this that brought me to this narrated
reality subject matter and that was this
project called word camera it was my
first machine learning project
it turns images into strange prose poems
and I was using this API called clarify
which returns a set of nouns from an
image using a convolutional neural net
concept net database to find related
words and then a template system to sort
of string those words together and
sentences and paragraphs and it was
really about redefining the photographic
experience so how this came to be was
that I received a grant and early 2015
from Google to produce a
computer-generated screenplay and I was
thinking a lot of a house about
screenplays and how they're written and
if you know anything about writing
screenplays you know that only things
that can be seen can be described you
never go into a viewer a character's
head so I thought you know how do you
generate a realistically descriptive
scene well you know maybe use photos in
the corpus so I got sidetracked with
this project I didn't end up making a
screenplay generator at that time but I
really you know fell in love with this
idea of narrating images because I think
that image captioning is sort of a
narrow view of what CN NS can do it's
it's useful as a benchmark but not very
interesting from a create
standpoint and I think that fixation
with captioning sort of constrains
creative thought around possibilities of
the technology it also implies
replacement of human function which
scares some people so I thought you know
why not just caption him it why just
capture him and why don't narrate them
why not try to write with a camera
instead of a pen and try to redefine the
photographic experience so the output
was a little choppy at first this is
what this would look like this is a
young picture of Vladimir Putin and now
thinking about computer-generated text
so that it's often like a Rorschach test
you can see it and whatever you want but
that's also okay Alison Parrish who's
one of my mentors she's actually
speaking here at this conference and
then I'd slod 3:10 about creative
applications of word divock has this
great this great comparison of space
probes and generative poetry programs
what she says is that both space probes
and generative poetry programs venture
into realms inhospitable to human
survival and send back to lemon tree
telling us what is found there for space
probes that realm us outer space for
gendered poetry programs that realm is
nonsense
humans generally shrink from nonsense
but a good poetic procedure can
demonstrate that nonsense is worth
engaging with there are infinite
undiscovered gems of language that lie
hidden within nonsenses borders so I
also feel like grounding generator texts
in reality by using a photograph as
input gives the text context and that
can often give it meaning at least so
far as I've observed in people's
reaction to this work so I decided to
put the this software inside of a
physical device so I made the series of
physical devices with raspberry PI's
instead of old film cameras again it was
about redefining the photographic
experience
I used thermal receipt printers to
output text and I wanted to build them
inside of old cameras because I wanted
to provide a reference that users could
immediately understand and communicated
this is a primitive form of something
you know this isn't the Canon 5d this is
the daguerreotype you know it's it's not
it's it's it's it's it's the first step
and sort of a and what could be a
lineage so it was just very simple
single button interface on the back
you press the button and a receipt comes
out here's another one that I made but
the receipt looked like this it was like
a Polaroid sort of and there'd be a
number at the top some generative
content based on the photo a URL where
you could see the rest of the generative
contents there was more of it they could
fit on like a reasonably sized receipt
and then an epitaph which would be a
paragraph from a novel that would that
was algorithmically matched to the
generative content so they basically you
know I did that because I thought that
text was kind of choppy at the time and
I didn't he could really stay on its own
and I wanted to drive people something
human written to go along with
generative output so I kept going with
with these for in 2015 and then at the
end of the year I got access to NYU's
supercomputer and I started training my
own deep learning models I trained a
number of LS TM recurrent neural
networks in various corpora lexicon drew
was one of the first ones but L strained
a bunch of models on poetry and prose
the output I got at first was was really
encouraging it was it was stuff like
this so I'll give you a second to read
that I hate reading stuff what's on the
screen
this is curated obviously but it's it's
you know indicative of the type of app
that I was getting just from training on
large sets of poetry
so I also trained image captioning
models using Andre Carpathia neural talk
to on Microsoft's common objects and
context data set this is you know some
output from one of the first ones and
you know based on the way L STM's work
where you have to actually seed them
with some starter text and then they
sort of extend that I thought you know
why not try to combine these two things
and make a new version of word camera so
what I did was I started seeding my
poetic language l STM's with image
captions so this is one one of the
earliest ones I did is from a really
early test it says a man is sitting at
the edge of the waters I should see him
begin to stand at the throat of the
graveyard and then it sort of descends
into more nonsensical text but I was
still really encouraging and I released
some models into Creative Commons in a
github repo called neural snap and and
some code under under an open-source
license and I made another physical
device this started this was what the
first prototype in my thesis project at
ITP so I started out as one device that
would use image captions location and
time to generate text and the concept
was was narrated reality which was it
was something that I thought might be
kind of like virtual or augmented
reality but more about experiencing
things as they happen and then later in
an Augmented format you know the the
goal of the project the sort of dream
idea was to imagine generating a novel
by taking a long walk in your city and I
got some advice from this this this guy
I enjoy he's the founder of the school
for poetic computation in New York and
he said it was just too much packed into
one device it was you know a little
confusing so I split it into three
devices a camera a compass and a clock
to narrate images location
in time respectively and for all three
devices I used one of these Nvidia
Jetson system-on-chip computers which is
like a Raspberry Pi with the GPU and a
data max O'Neill micro flash 40 printer
which is the same kind that police
officers used to print your traffic
tickets so the system was totally
autonomous no internet connection was
needed and I decided to train a library
of neural network models on it and put
them on SD cards so they could be used
interchangeably with each device so here
we have like tech news sci-fi prose
sci-fi film hip hop film chat which is
like a newest dialogue which we'll get
to in a minute fault music 2016
presidential candidates and robot God
which was a mix of every religious text
combined
so this is sort of how each device works
there'd be that gift did not start at
the beginning this is the clock so
there's there's a user interaction
that's funny these gifts usually start
at the beginning of their gift but it's
gonna start right in the middle for some
reason
so like I said this is the clock you
start by picking a model from the
library of models and then you basically
put the SD card into the device's SD
card slot like a video game cartridge
and there's a user interaction and then
you get the output in this case a
narration of the time and then for this
device I used a 19-15 punch clock made
by a company that was the predecessor to
IBM which i think was an interesting
like reference for the for the piece in
general so the output looked like this
the time was two minutes past midnight
the corridor was a long corridor in the
shadows and the whole thing was the same
as a wall and the door slanted from the
line of the wall the Sun was a dead wave
of appearance the moon had been dimpled
and a corner of the corner was silent
and the floor was like the sable pile of
a flower a cold road would be stronger
so you know despite the only thing
indicative of midnight being the scene
text which was a sort of natural
language rendition of the time the sort
of midnight miss trickles down through
the output and I really liked that about
this particular example this is the
camera I built using the same guts
displayed at the FIH Centre in Montreal
and here's some output from that this is
actually a picture of me standing in
front of the clock so very meta and I'm
using the full music model for the
output in this case this is trained on
folk lyrics the final device in the
series was the compass which was sort of
inspired by war driving is any everybody
familiar with war driving it's basically
you know driving around with the
computer in your car and you know
hacking into Wi-Fi networks but and this
was a friendlier version of war driving
I use the navigators compass from a b-17
Flying Fortress and the centre console
from a police car and it was designed to
be mounted in the car and you drive
around and it would generate tax based
on your location originally I wanted to
generate like plausible car accidents
but I thought that was a little bit too
dark but yeah that's that that's that
piece and I didn't get to test it at ITP
but idea to test it this year and
to that you know in a few slides but the
output looked like this so this is like
you know one I took it just just at my
UI TP and this is the the model is the
one of the screenplay models so here's
all three devices together for the first
and only time at my thesis show at ITP
they've never all been together again in
working state the fun thing about neural
nets you know I mentioned before that
that last sample was from a screenplay
model or a dialogue model from
screenplays when you train a continuous
dialogue you can actually ask the model
questions so this is one of the
questions I asked it only my favorite
responses it gave me which is where did
you grew up bunny I didn't have to do
that and the reason I was training on
screenplays to begin with was you know
going back to the original reason I had
received that grant from Google in 2015
which added up sparking word camera
which was to make the area generated
screenplay and I ended up doing that in
2016 with an LS TM and made this film
called Sun spring so I made some spring
with a film director named Oscar sharp
in April 2016 for the sci-fi London
48-hour filming contest
I met Oscar at NYU we were both in a
class called surveillance documentary
which was a joint class between ITP in
the film Department and he gathered an
incredibly talented group of actors
including Thomas Middleditch from
Silicon Valley on HBO
and as far as we know it's the first
film created from a computer-generated
screenplay anybody knows of a prior
example please let me know because we've
been saying that for a year and nobody
has challenged us on it the whole film
was written shot and scored and edited
in two days and you can watch it for
free on YouTube it's about 9 minutes
long and we had an advantage in the
contest because we had other teams to
spend their whole first day writing and
we didn't have to do that so you can
watch like I said you watch it on
YouTube but the ls camera at the action
description as well as the dialogue
which results in really surreal
sequences like this the line from the
screenplay was he pulls his eye from his
mouth so that's just that happens in the
middle of a scene and it's sort of
remnants are eminent of you know a lot
of surrealist cinema but I've never seen
anyone do this in a movie before and I
really liked this scene but I liked the
next scene more so in this scene the
line is he pulls the camera toward his
back so he physically pulls on the
camera but any angle changes that he's
holding nine and to me that's a really
indicative of the cycle of generation
and interpretation that this sort of
exercise lends itself to so while the
action of pulling on the camera was
dictated by the Machine the angle change
and him holding nothing was a human
interpretation informed by years of film
production experience which I thought
was really brilliant and and these
little gems can can come out in that
dialogue you have with the Machine so
for the sci-fi London 48-hour we also
made a short story for the flash fiction
contest you can read the whole thing at
that URL I'll just read you one quick
passage from it I'm the one who wanted
to be a millionaire says Mindy I know
you're a good man she said
Mindy mandible was a person who was
still alive it was a transient and
superior thought that was shameful it
was too heavy for her but there was no
way of standing beside her it was a
sense of astonishment and it was the
panic of a mind and that was a muscular
and almost familiar reason so like I
said if you curate the output from these
machines you get wonderful passages like
that and it doesn't take that much
curation on this way more recently I've
been working on a lot more projects with
deep learning this is one of them
it's called unasked questions
I can't claim credit for the design on
the back of the card this is just a
graphic designers mock-up of the project
but the idea is that I collected
questions from ask reddit for like a
year every question posted to ask reddit
and I have over a million and I trained
a neural network on those and now it
writes new questions in that style so
I'll read you a few of those what did
you think you were doing on the internet
but turned out not to be if you had to
choose between Trump's hands and final
death for you which would you choose and
why what are some myths that you would
like to share with North Korea what is
your favorite cheese distraction if you
could go back in time what would your
social media post be what is a product
that likely starts at a dangerous time
period why are Millennials so ashamed of
men and what are some really good names
for a straight guy so the idea behind
the card deck is that it would be an
infinite set of cards where every card
and every deck is unique still looking
for someone who can actually achieve
that with printing technology but I'm
hopeful about producing this project in
real life and not just as a graphic
designers rendition but that's in
progress another project I'm working on
right now that I've been working on
since late last year is I wore a
lavalier mic sort of like the one I'm
wearing right now but on my coat for two
months and recorded everything I said
and isolated it so that it didn't pick
up people around me and I had to
basically preface every conversation I
had for two months with I'm wearing a
wire but the idea is that I'm gonna run
speech-to-text on the output or on that
audio and then train a neural network on
the transcript and then delete the audio
and then have a bot that just talks like
me forever and put that in a mannequin
so that's that that's in progress a sort
of recently completed project
is this project word car so I've been
really fortunate to have gotten involved
with Google's artisan machine
intelligence program which is a fairly
new initiative from Google and they
sponsored this project it's called the
club word car and what we did was drive
a Cadillac from New York to New Orleans
and the car had a pan tilt zoom
surveillance camera on the back on the
rear trunk a GPS unit and a microphone
to pick a conversation and it was
basically the realization of that
compass project I mentioned before so it
was really cool and hopefully the book
is being published actually with with a
French publisher called John watt that
publishes computational creative writing
so I'm really excited about that and we
drove to New Orleans because I wanted to
meet this guy named Josh sniffing in
Biloxi Mississippi
Josh builds custom PC gaming cases and
he's helped me build a new set of board
cameras that would be a little bit more
professional looking and the ones I
build myself so this is one that is
almost finished it's inside of an 1885
large format camera and the idea is that
the entire computer is contained within
the camera rather than the previous ones
that I showed you that were either
driven by Wi-Fi you know or had the
prior version of the software on it this
has the newest version of my word camera
software which actually requires a
bigger GPU than the nvidia justin has so
yeah that's that's running inside the
camera and we made a follow up to some
spring this year called it's no game
also on YouTube starring David
Hasselhoff so for this for this movie
Oscar and I we curated the output more
and we also trained or I trained not
just one model but actually five I
trained one on and they're all trained
on subtitles
this wonderful website called sub scene
which is organ maintained by the deaf
community and it's it's a just a
treasure trove of every subtitle from
every show and movie you could possibly
think of so I got all the subtitles from
Baywatch and Knight Rider and trained a
model for David Hasselhoff we did a
Shakespeare model we did an Aaron Sorkin
model a Golden Age Hollywood model and a
generative ballet model which wasn't
even neural network it was a
context-free grammar designed to do
generative ballet instructions but watch
the movie it's on YouTube also like 9
minutes long also made for the sci-fi
the London 48 hour film challenge and
you can decide for yourself so I want to
close with a couple of demos I have here
a version of word camera this was on
display at Science Gallery in Dublin
Ireland for like three months so it's a
little beat up but this one the
computers inside the camera just next
five to my laptop via USB and I'll just
take a picture to everybody here and you
see it prints out on the back like that
there's an ASCII version of the image
first
okay so what it says is lights on the
ceiling we're already standing before
him and the sheets of water came into
the barn and the snow stood of the
street where the wind was blowing and
showed the tracks of the buildings
people in the background came to a party
a boy came in from the bedroom and
without music a sign on the side of a
building was being turned off a black
and brown shelf was still shut a large
design the table was locked it always
seemed to him that he had been to the
city office to carry the book on the
table and finally started to spend the
night in the corridor so that's been ok
one not exactly
not-not-not a you know as connected to
the image but the thing is there's a lot
of stuff in the background and can't
always control what the computers going
to want to talk about so I guess I'll
take one more adjust it myself so I can
show you the difference
Wow it didn't mention me at all let's
try this one more time
well there we go
something has gone wrong now there goes
now I'll just restart the script one
second folks
okay like I said this was on display for
three months I'll just read it off the
screen as it is showing up
now it's working
okay that was not a picture of me it was
picture of that but I'll to read it a
large black sign was standing beside him
and a car ran through the grass and the
point of the bushes and the hoods of the
barn that had been transported with the
front door all iDEN the bushes was a
cool basin and a snapping cardboard and
beans with colored cloth swaying in the
moonlit light of the trees a round
mirror on the wall was the size of a
door that had been given to a carpet and
a little patch of carpet that floated
above the counter with a long black
pinned small white tablet came to a stop
in the middle of the street and was
fixed to his shirt and a bright blue
breeze was clutched to the sky a wooden
table was filled with rain and dried
bottles so I got one last demo which is
for this little app I'm building called
dialog which is basically autocomplete
for creative writing should in the air
my screen
there we go
that is way to the joys of connecting
Linux laptops to projectors
okay there we go so um make this a
little so the idea behind this come on
come on here we go
nope let's switch I'm just completed
let's get a screen whatever the idea
behind this is that it's there's two
models that you can switch between one
is trained on all the paragraphs from
Project Gutenberg that mentioned art and
culture and one is trained on all the
paragraphs from Project Gutenberg that
mentioned science technology so I'll
start with the art one and I'll say art
is can't read that myself
so you guys tell me what it says artist
saying so I'll switch to the science
technology model
but let's see like the arts and culture
and see how it ends it
and there's this nice thing at the
bottom everything changed the lsdm
temperature so temperature if you're not
familiar is a parameter that controls
the riskiness of the models predictions
high temperature results are going to be
more variable but also more riddled with
errors low temperature results tend to
be more grounded but also more
repetitive and less interesting smaller
vocabulary
so let's travel temperature and see what
happens
so it when you turn the temperature like
that it oftentimes will make up words
make up new proper nouns insert words
that you don't express so it's really
fun tool to play with so that is all I
have and I'll just end with this deep
dream gif thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>