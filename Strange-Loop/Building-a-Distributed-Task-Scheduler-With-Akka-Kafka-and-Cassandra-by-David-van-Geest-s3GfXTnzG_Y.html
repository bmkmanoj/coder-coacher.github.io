<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Building a Distributed Task Scheduler With Akka, Kafka, and Cassandra&quot; by David van Geest | Coder Coacher - Coaching Coders</title><meta content="&quot;Building a Distributed Task Scheduler With Akka, Kafka, and Cassandra&quot; by David van Geest - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Building a Distributed Task Scheduler With Akka, Kafka, and Cassandra&quot; by David van Geest</b></h2><h5 class="post__date">2016-09-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s3GfXTnzG_Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone thanks for coming out today
my name is David and today we're talking
about distributed task scheduling at
Pedro Duty so first a little bit about
me I'm a software engineer on the core
team at Pedro Judy and it's composed of
these fine people on the right here the
team is mostly in Toronto but the
company has offices in San Francisco and
Seattle as well and on the core team we
support many different engineering teams
at Pedro Duty so this is mostly by
building shared libraries and
infrastructure and researching best
practices one thing I want to emphasize
is all the ideas I'm talking about today
we worked on together they're not my own
ideas I'm just a guy lucky or you know
maybe unlucky enough to be standing
right up here in particular Alexi who's
on the left there did much of the design
work for this project so kudos to him so
do we have any Pedro Duty customers in
the room let's see some hands ok awesome
so you know what we are but for those of
you who are not familiar we are a
software as a service company and we do
IT incident management so what that
means is that we aggregate data from
monitoring systems like New Relic or
Splunk or data dog and we create
incidents when something bad happens
based on rules that you set up so an
incident might be something like the
database server crashed so when an
incident is created we alert people who
need to know about it through push
notifications or phone calls or SMS and
these notifications usually go to
someone who's on call
so there's actually on-call scheduling
built into the system so long story
short our customers rely on us to let
them know when something is wrong with
their systems and this is a really big
responsibility and one that we take very
seriously so because we take this
responsibility so seriously we think a
lot about reliability and uptime
and the most important piece of this is
the notification pipeline so if
something is wrong in your system you
need to know about it and there's no
excuses so to achieve high availability
in the face of catastrophic failure our
infrastructure runs in three different
data centers and everything that we
build is expected to survive the failure
of one of those data centers so keep
this requirement in mind as we go
through the talk when I talk about a
data center failing this is a very real
thing and our business requirements
dictate that we handle it well so why
are we here today I'll introduce our
problem which is a distributed task
scheduler for use that page or duty I'll
talk briefly about what the old solution
was then I'll describe our new solution
and this is the one built with akka
Kafka and Cassandra after that we'll
spend a good portion of our time talking
about how the team used those three
technologies to solve some interesting
distributed systems problems finally
we'll talk about the results of our
project okay so the problem in terms of
the product here are some use cases that
we're trying to solve
so our system supports many types of
notifications if you're going on call
you might want more nning about that and
maybe you want it two days ahead of time
so you remember you know don't party too
hard and then when you're actually doing
your on call shift you need to be
notified that an incident happened so we
have very customizable rules for this
maybe you want a phone call immediately
but you want to push notification one
minute later to make sure that you're
actually you know awake and out of bed
another use case we have is retrying
notification delivery so we have
redundant SMS providers but what if
they're all down for some reason we need
to write read we need to schedule a
retry in that case so those are specific
examples but actually what we have is a
very generic problem we have these
arbitrary chunks of Scala code which
we'll call tasks and they need to run at
an arbitrary time and this code might do
anything it might be synchronous it
might be a synchronous might
to a database in might start your car
it's included in many different micro
services at the company and when I say
arbitrary time that means it could be
scheduled for one second from now or it
could be a scheduled one year from now
at the same time it has to be tolerant a
very dynamic infrastructure so like I
said we currently run in three data
centers but it could be for next year or
we might have to add or subtract ten
machines for one of our services and
some of those machines or data centers
might be down when a task is get old to
run or you know all of the machines in
micro service might be entirely
different VMs from when a task was
scheduled but spidel this we need our
tasks to run in order so ordering is a
complex thing let's define it a bit more
exactly for the purposes of the system
so we're gonna say that task 1 is only
guaranteed to execute before test 2 if
these three things are true number one
they have the same ordering ID that is a
developer said these things are linked
and I want their ordering to be defined
number two task 1 scheduled time is
before the scheduled time of task 2 that
makes sense
lastly the call to schedule task 1 was
completed before the call to schedule
task 2 and that last bit is important
and a bit tricky so let's have an
example here let's say that the we have
two tasks and their scheduled very close
together and they're scheduled for a
second from now so tasks to my execute
first if the scheduling call completes
before it does for task 1 but we have
some room to make things easier as well
so tasks only need to be executed within
second filler scheduled time now talking
about millisecond precision we can also
delay to task execution arbitrarily to
preserve our ordering also the tasks are
idempotent and this is a very important
point if you run them a second time
nothing bad is going to happen lastly
our tasks really fail but they do
sometimes so we have to be prepared for
that occasional instance ok maybe this
is just a simple
matter of programming you can just knock
it out in an afternoon well let's find
him so Pedro Doody has an existing
solution to this problem and it's been
around for a few years it works but
there's some room for improvement the
old solution was called work queue this
is a very simple version of how it
worked so the components are shown in
green on this slide now some of my
co-workers in the audience are probably
you know shaking their heads right now
because I have this all wrong and you
know but sorry I just joined the company
in January so don't really know how it
works anyway we have this Scala service
it's you know scheduling a task through
an API and the work Hugh writes that
tasks to a wide grow in Cassandra now a
wide row means that the queue is a row
in Cassandra and a tasks time is
actually the column we have another
component that's pulling Cassandra for
new tasks and executing them using a
thread fool this is pretty simple and it
works but what about when you have many
service instances trying to flow from
the same queue in other words how you
distribute the tasks to the available
instances without duplicating work so to
solve the problem of having multiple
service instances we introduce custom
partitioning logic into work queue and
the partitioning logic puts tasks into
different wide row queues and cassandra
and each service instance works a
partition which is basically hard-coded
it's not but we won't get into it
I know there's some logic to handle
service instance going down so each
partition actually has a primary a
secondary and a tertiary partition or an
instance that would work the queue and
these instances would be peeking at the
queues to make sure that a progress was
being made and they would also do health
checks on each other and if a service
instance fails the system would would
handle it but it's kind of a big deal
like people would be paged in the middle
of the night because the system is now a
entity in a degraded State so what's
changed from our previous diagram we
introduced some pretty complex custom
partitioning logic and now our work hue
library needs to be aware of how many
service instances there are and how many
queue partitions there are and what
happens when you need to add another one
things get kind of hairy so the old
solution works but it has some problems
that we wanted to address so like I was
talking about adding or removing a
service instance was quite a complicated
dance due to this custom partitioning
logic there's actually like seven
different steps usually like converge
chef and like restart a service and do
some deployments and I don't even know
it's not something you want to do at
3:00 a.m. when Amazon decides to kick
when your boxes the old solution was
also quite slow so the only way a task
was executed was by pulling it from
Cassandra so we're actually hammering
our Cassandra clusters with these very
slow reads and this frequent slow IO
actually influence decisions on where to
place our data centers physically so
they're all currently on the west coast
of the US
so that latency between our Cassandra
nodes is minimized this is in large part
due to work.you we also have a huge
reliance on this complex partitioning
code that was really maybe only
understood by the handful of people who
wrote it or have to deal with it often
it's not a good position to be in okay
enter the new hotness it's the same as
the old hotness but it's on fire early
early in this year the core team built a
new solution to the same problem and
very originally we named it scheduler
it's not actually on fire it's working
quite well and we're gonna find out how
so the new scheduler library is built in
Scala same as the old work queue and it
uses three different technologies to
tackle various challenges now I'm
assuming most of you have some ideas
with these things some idea of what
these things are but in short Kafka is a
distributed connect log and if you saw
David McNeil's talk yesterday that
yesterday about Amazon Kinesis a lot of
that is gonna apply to Kafka as well
even if the terminology is slightly
different cassandra is it distributed no
sequel column-oriented database based on
ideas from Google's famous BigTable
paper or also Amazon's dynamo and akka
is an actor system library for Scala or
Java so if you're not familiar with
actors they are computational entities
that run in parallel and communicate by
sending each other messages
okay definitions out of the way so this
is a very high-level diagram of the new
scheduler and how it integrates into our
services so again we have some service
that's written in Scala it's shown in
red here it includes scheduler as a
library shown in green it's actually two
separate libraries one for the client
and one for the implementation the
service the services logic schedules a
task by passing it into a scheduled task
method and the library intern C realises
the task and accuse it to Kafka now it's
important to note here we're just
serializing tasks and metadata we're not
actually serializing any sort of task
logic on the other end of the queue you
have scheduler itself which consumes
tasks as they are sent now it's always
gonna persist the task to Cassandra but
if a task is scheduled before a certain
time in the future is also going to
remain in memory at the same time
scheduler is also fetching tasks of
Cassandra on a regular basis similar to
the old work queue then this combination
of in memory tasks from Kafka and fetch
tasks from Cassandra is executed by
calling a task executor defined by the
encompassing service so you as the
library user need to define how the
tasks are executed and that's how we get
away with not serializing any task logic
now this looks somewhat similar to the
old version but a major change here is
having Kafka in the mix as well as
Cassandra so the purpose of this talk is
not to explain exactly how every part of
scheduler works instead I would like to
talk about some of the challenges
typically faced when building this type
of system and how we use those three
technologies to help us solve those
challenges
everybody knows distributed systems are
hard
they have many challenges I'm gonna
focus on some three main areas during
this talk so dynamic load which is
really a problem for anyone
datacenter outages and task ordering
we'll start with dynamic load so what
happens on Black Friday when everything
is happening all at once and everyone
systems are going down and that pager
duty we're sending out
much higher number of notifications than
normal specifically what happens when
you start scheduling more and more tasks
so there's a number of components I'll
need to keep up with the increased load
well let's start with Kafka since its
first in the pipeline so generally
speaking Kafka will scale horizontally
that is we can keep adding brokers to
our cluster to deal with the increased
number of tasks and this works because
for a queue which is call a topic in
Kafka lingo there are n possibly
replicated partitions so these topic
partitions are spread evenly across the
available brokers such that each broker
gets an even share of the traffic and
therefore an even share of a disk i/o
and etc and what's quite nice is that if
you add or subtract a broker from the
cluster Kafka can rebalance it for the
partition the partitions to ensure the
load is still even so let's take a look
at how this works
we'll start with two brokers and we have
one topic that topic is divided into six
partitions each partition is replicated
once from a leader to a follower all the
reads and writes are going to the
partition leader so in the case of a
write the leader forwards the write on
to its followers and this leader
follower stuff is important we're going
to see it again in the presentation so
in this example each broker is a leader
for three partitions and a follower for
three okay let's say we start exhausting
disk i/o in our two brokers so we got a
third to the cluster four partitions are
redistributed by Kafka this is something
you have to manually initiate but the
process itself is fully automatic so now
each broker is a leader for two
partitions and a follower for two each
broker only needs to deal with four
partitions instead of six okay again
we're gonna scale out now to six brokers
each broker is now only a leader for one
partition and a follower for one and as
you can see for this to continue scaling
we should have significantly more
partitions in our topic than expected
brokers this example is not going to
scale further than twelve brokers at
that point you have each broker is
either a leader or a follower for a
single partition
adding more brokers to the cluster is
not going to help but there is also a
cost to having a very large number of
partitions per topic it can affect
availability and end end latency so this
is not something you want to increase
beyond reason and you can do some
googling to learn more about that so
that takes care of skill and Kafka what
about the service itself well again we
can scale horizontally and this is
actually enabled by kafka so the
consumers of a given topic which in this
case our service instances are grouped
together and this is called a consumer
group the group's healthy consumers are
tracked by kafka
through a heartbeat mechanism the tapas
partitions are distributed evenly to the
healthy consumers and this is a dynamic
process so if consumers die or consumers
are added the partitions are reassigned
so here's an example with a topic that
has five partitions with two service
instances in the same consumer group one
service gets three partitions and the
other gets two okay now let's say our
two service instances can't keep up with
the workload so we had a third CAF guys
off gonna automatically reassign those
partitions
such that they are evenly distributed
across the three instances each instance
is now responsible for two partitions
that maximum instead of the previous
three this is called a consumer
rebalance and Kafka Kafka can do this
very quickly but typically you're
actually limited by how fast your
application can react to that consumer
rebalance so in our particular case for
scheduler it's actually about 30 seconds
again note here that the scaling is non
effective once your number of instances
equals your number of partitions your
instances for them to be useful have to
be working at least one partition so
this is another thing to keep in mind
when choosing when choosing the number
of partitions for a topic so our service
instances are receiving the tasks from
Kafka but now we have to persist them
somewhere so we're using Cassandra for
our task persistence and like Kafka it
will scale horizontally
now I'm kind of assuming that many of
you know about how Cassandra scales I'm
gonna go through this pretty quickly
it's not terribly interesting the key
for a Cassandra row is gonna be hashed
into a token in a known very large range
and the nodes in the cluster are each
assigned a range of those tokens so
therefore a given row will go on to a
known node in the cluster that needs to
add nodes the original nose become
responsible for a smaller range of
tokens so in this example we have four
cassandra nodes and to make the math
easy we're gonna say that the range of
tokens is 0 to 99 in reality this range
is going to be much much larger so that
range is evenly divided into 4 with each
node being responsible for the previous
25 tokens so for example node 1 is
responsible for 76 to 0 no 2 is
responsible for 1 to 25 so when we
insert a new row into the cluster
cassandra is gonna hash that row key to
get a token and the token dictates which
node the row will go on in this example
that token was calculated to be 80 so it
lives on node 1 now it doesn't take into
account replication a row is usually
replicated which means it's going to
live on multiple nodes in the cluster
it also doesn't take into account
Cassandra V nodes which is a more
complicated feature which I won't get
into right now okay so our 4 nodes are
having trouble keeping up we'll add a
fifth node to the cluster we actually
have to shift the tokens around so that
they are you still evenly distributed
each node is now responsible for 20
tokens instead 25 and this is an
oversimplification that you probably
wouldn't do in production but I think it
helps understand the concept I also
learned that it was very difficult
difficult to draw us and Google drawings
so I didn't want to make it any more
complicated ok so our setup can handle
changing load we can add machines to our
various clusters to scale horizontally
what happens when an entire data center
goes away you know what happens when
someone at Amazon trips over the
internet cable well we'll start with
Kafka again our setup is to have 6
brokers evenly split across three
different data center
so you have two brokers in each data
center now each partition has three
replicas one per data center again we
want to be very sure that we're not
going to lose any data in Kafka 0.10 you
can actually ensure one replica of our
data center through configuration but we
are still running Kafka 0.9 so we
actually do this through some custom
scripts so when we do it right we ensure
that it's replicated to at least two
data centers before the write return
success now normally if all three
replicas are in sync the write will
actually go to three data centers but if
one replicas falls out of sync we will
write to two brokers and still succeed
and that's what those configuration
settings mean on the slide so if a data
center fails a third of our partitions
will lose their leader the Kafka will
automatically change the leader to one
of the in sync followers and all the
reason the writes are going to go to
that new leader so here's an
illustration of how this works this is a
very similar example to our actual setup
the only changes then is the number of
partitions which makes the diagram a
little clearer so we have three data
centers two brokers in each one and
there is a single topic which has six
partitions so originally each broker is
a leader for a single partition and a
follower for two others okay and we just
lost datacenter 3 so Kafka is going to
shift our partition leadership
partitions 3 &amp;amp; 6 had a leader in
datacenter 3 which is now gone so Kafka
is gonna make broker 1 and broker for
take on leadership of partitions 3 &amp;amp; 6
and the bar the partition circled in red
have changed from being followers to
being leaders the only thing that the
other thing to note here is that now we
only have two in think replicas for each
partition 2 is the minimum number of
replicas in our configuration so rights
are still going to succeed okay so Kafka
makes it look easy let's look at
Cassandra what happens with Cassandra
when we have an entire data center go
down so we have configured Cassandra to
deal with this scenario
simply speaking we have five nodes
spread evenly across three data centers
and our replication factor is actually
five which means that all Cassandra
nodes are eventually going to get ripped
a replica of each row you want to be
very sure that we don't lose any data
and we do quorum rights which means that
at least three nodes must acknowledge
the right before returns success to the
application and three machines means
that we will be in at least two data
centers then we do quorum reads to
guarantee that we will get the latest
value now maybe some of you are thinking
that this sounds come slow synchronously
running two nodes and three different
data centers and it is somewhat slow but
we can get away with this because these
Cassandra rights are generally not user
facing so this diagram illustrates our
quorum right in the worst case scenario
where two of your rights are two nodes
in the same data center you were still
guaranteed to get a right to a node in a
second data center and again when you
hit the worst case scenario we would
just lost datacenter 1 along with the
two nodes that have the new updated
value we're still going to be ok so
we're gonna do a quorum read that means
that three nodes must respond and we
still have three nodes which will
respond in the remaining data centers
but nodes four and five have old values
so do we still get the new value and the
answer is yes and we're relying on
Cassandra's policy at last right wins
here the value on node three has a newer
timestamp than the values on nodes four
and five so it will actually be returned
and yes your clocks need to be well
synched for this to happen we do have
plenty of alerting around that and if
you really start diving into the corner
cases it can get hairy so you should
generally try to try to avoid actually
mutating your data so that takes care of
our data persistence now let's talk
about what happens to the service itself
when a third of the nodes go down well
as we have seen before
Kafka will detect that those consumers
are no longer healthy it will reassign
the topic partition
to the remaining healthy instances and
this works for three reasons number one
any service instance can work any task
number two the tasks are item potent so
if a task is halfway done when the DC
disappears it can be redone by another
machine without anything bad happening
and of course we don't run our servers
at max capacity
we're over provisioned such that we can
lose you know a third of our machines
and still deal with the load so let's
say we have three data centers and a
service instance in each one the topic
partitions are evenly spread out across
the service instances and notice that in
data center of three the service
instance is working partitioned three
well and we lost this and three so Kafka
will detect the failure of service
instance three that heartbeat mechanism
and it's going to reassign precision
partition three to service instance one
the system is going to continue to work
okay finally we need to talk about tasks
ordering this is a pretty tricky problem
in distributed systems but if you limit
the scope of your ordering you can get a
lot easier and maybe you won't need to
use vector clocks which would be great
so how did we limit the scope of our
ordering problem we have this concept of
this concept of logical queues for which
ordering is defined each task has an
attribute called ordering ID which
defines the logical queue that is in so
for example an ordering ID might be user
ID one two three which means it's in the
user ID one two three Q now remember
that a kafka topic is composed of many
partitions and we need to put the task
into a single partition so the partition
ID is just a hash of the ordering ID
modded with the number of partitions and
this takes care of the scheduling side
so here's an example we have four tasks
that have ordering IDs based on the user
ID associated with the tasks task one
may be an on-call notification and task
three might be an
call notification they're both going to
the same user and let's say they're
scheduled very close together you're
only on call for like a minute and it's
the best week ever so you want the on
call notification to go out before the
off call because otherwise the user
might be confused so ordering has to be
defined for those two tasks so the hash
of the ordering ID leaves the two tasks
to be put into the same Kafka partition
this means that they will be executed on
the same service instance or in a more
complicated case task 1 will be executed
then there's a consumer rebalance and
then task 2 is executed elsewhere the
important part though is though tasks 1
and test 2 will not order a task 1 and
task 3 will not be executed on different
machines at the same time so on the
execution side we have one service
instance executing a logical queue but
of course the number of logical queues
is very high if we try the ordering to
the users we could have millions of
logical queues and obviously we don't
want to have millions of service
instances so one service instance is
actually executing many logical queues
but to maintain ordering you know in a
logical queue if a task fails you need
to keep trying it into a succeeds and
you can't be executing any tasks after
it so how can we continue to execute all
of the logical queues except for the one
that has a stuck task so the answer in
our case was an actor system implemented
in akka you could certainly do it other
ways but we found that akka made the
needed concurrency to be simple each
service instance has this akka system
which is somewhat simplified and on this
diagram the ovals represent actors and
the arrows represent supervision so at
the top is the topic supervisor it
receives all the incoming kafka messages
for a given service instance now since a
service instance will be consuming
multiple partitions
there are partition supervisors for each
they are children of the topic
supervisor within each partition there
are multiple logical queues basement
tasks ordering Aggie
the partition executor supervises and
supervises an ordering executor for each
ordering ID now that's potentially a lot
right because we have like millions of
users and that could be a lot of
different logical cues but actors are
cheap
Anaka they have something like 300 bytes
per actor and overhead so we can
actually get away with having many of
these things the ordering executor has a
queue of tasks to execute it will take
the task from the head of the queue and
create a task executor for it a task
executor is actually a short-lived actor
it's not going to create a task executor
for the next task in the queue until the
first one finishes the task executor is
going to retry the task until it
succeeds or in some cases if it's
dropped by a human if it's never going
to succeed so if a task continually
fails the logical queue contained within
the ordering executor stops but the
other ordering executor is will continue
working through their own queues so the
ordering in the system is maintained but
only where it matters ok maybe you're
thinking enough theory does this thing
actually work well it does scheduler has
been in production since March if how is
everything that happens in page of
duties on-call handoff notification
service so if you use Peter duty and
you've received emails or sms's saying
that you're going on call or off call
scheduler is actually the piece of
software that that made that happen
and it does a fair bit of work so
millions of tasks are executed each
month and in addition to that it handles
some very interesting spikes so people
tend to change their on-call schedule on
the hour so we do large amounts of work
on the hour or the half-hour we're also
continuing to do development work on
this service which means multiple
rolling deployments per day scheduler
ensures of those deploys don't disrupt
the tasks that are being worked on
and maybe some of you are wondering how
we know all this data center outage is
off actually works well our answer to
that is failure Friday and this is
something we do every Friday we
purposely break our infrastructure in a
very controlled and planned way to
ensure that the system can handle the
failures so back in May we actually
simulated a data center outage we killed
all the machines in one data center and
made sure the system kept on working it
did then on July 19 one of our data
centers in Azure actually experienced
some some very severe network problems
which largely cut communication to our
other two data centers this is a risky
scenario where you have two pools of
machines that are still working but
they're not really talking to each other
well again the service performed as
expected but to the new scheduler
actually solved the problems from the
old work queue now if you remember
earlier in the presentation I said that
difficult manual steps were required for
infrastructure changes lots of chef's
work very easy to get wrong well now we
can actually add or remove service
instances by just killing or spinning up
a new box Kafka's consumer rebalancing
ensures that each box will get an equal
and out of work and no partition will be
left behind so I can actually type a
couple commands into slack and our
infrastructure automation will bring up
a new box it will join the correct
consumer group and it'll get to work I
also said that the old work you had low
throughput because it was frequently
accessing cassandra with the new
scheduler Cassandra access is down 65%
and this is because the new scheduler
pulls for tasks and a much longer
interval so it can do this because Kafka
is always feeding it new tasks this is
actually a graph of number of Cassandra
reads in production on our guinea pig
service we switched over to the new
scheduler at the red arrow and you can
see that the number of Cassandra reads
drops quite dramatically lastly I said
that the old work you had complex
partitioning logic the only a few people
in the company understood well in the
new scheduler the party
logic lives in Kafka it's off the shelf
there are great docks which are easy to
understand and there's many people
outside of our company working on it and
they understand it quite well this is a
much better situation to be in
so in short yes it did solve our
problems one last thing
scheduler is an open source project so
here's the github repo I will say that
using it has a bit more complicated than
just dropping it into SBT you need to
have Kafka running and Cassandra and the
docks you know they need some work yet
but we're working on it the code is out
there and you should give it a look if
this sort of stuff sounds interesting to
you and you're looking for new challenge
come talk to me or any one of the other
Pedro duty people here at the conference
we're always hiring and we love to chat
and now is a time for questions it's
actually like very hard to see you out
there any questions yeah go ahead yep go
ahead
sure so to repeat the question you're
asking those for cross data sent
replication what is our zoo keeper
topology look like is that accurate
okay so zoo keepers used by kafka is
that what you're talking about in this
case okay
so I'm not aware of anything special
that we're doing actually Kafka is using
zookeeper and it's doing its thing and
yeah other questions
I don't see any but yo though if you
have one in the back
yeah that's a good question so the
question is if some of these tasks are
side affecting you know they send out a
notification or something like that how
do you enforce the idempotency of the
task so the answer is that in some cases
this is application dependent but in
some cases the task is actually stored
somewhere so with the task has some kind
of state as stored in Cassandra and if
you run it a second time it's actually
going to you know see that the task is
already completed it's not gonna run it
again does that answer your question
okay anyone else okay
thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>