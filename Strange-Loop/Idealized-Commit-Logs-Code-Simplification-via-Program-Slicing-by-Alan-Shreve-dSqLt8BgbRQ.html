<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Idealized Commit Logs: Code Simplification via Program Slicing&quot; by Alan Shreve | Coder Coacher - Coaching Coders</title><meta content="&quot;Idealized Commit Logs: Code Simplification via Program Slicing&quot; by Alan Shreve - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Idealized Commit Logs: Code Simplification via Program Slicing&quot; by Alan Shreve</b></h2><h5 class="post__date">2016-09-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dSqLt8BgbRQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this is my first strange loop I'm very
excited and humbled and terrified we're
finally gonna do it we're gonna
understand software get excited my name
is Alan Shrieve most of the time I spend
doing networking and distributed systems
I build and rock by day but we're not
going to talk about any of that today
today we're gonna talk about code
simplification and idealized commit logs
and program slicing code is read much
more often than it is written this is a
hypothesis
there's I've looked there's actually no
academic study to back this up but I'm
just going to proceed like we all agree
with Raymond and and since we agree you
have to admit that as a professional
community we've kind of failed because
we don't really talk about reading code
we talk about writing code we have built
so many tools and they are all about
writing code not reading it even the
tool that you use to read code is called
an editor not a reader or a demystify er
or a viewer or any of those things
its purpose is for actually changing and
writing code and the other tools that we
have are focused on writing code as well
even even though there are some that you
might use to help you understand things
like debuggers or profilers they're
usually intended for people who are
already familiar with the code base and
we've written articles and and give
talks about how to write code how to do
it better or faster with the latest
tools but when it comes to reading code
we have almost nothing to say we are so
focused on writing code that whenever we
talk about reading or comprehensibility
of code it's always framed in the
context of writing it right pure
functions don't use global variables
when you write code right to interfaces
and all of this kind of misses the point
which is that most of the code you work
with is
code that you did not write everything
you learned about how to write readable
code doesn't matter when it's code that
you haven't written right when you open
up a new project and it's written by
someone who never read go to considered
harmful this is this is your problem and
maybe you believe erroneously that this
is not your problem that you live in a
you know beautiful utopian world where
you only work on code that you wrote I
don't I think many of you don't as well
this is a tool dedicated solely to
assessing and collaborating on code that
you did not write and even if you don't
do this I bet you still work on code
that you didn't write or have to work
with it because unless you're writing
Temple OS your project has dependencies
crack open your package JSON file or
pom.xml or whatever lock file you have
and marvel at all of the code that you
did not write code that has bugs in it
so there's a bug in your dependency and
what do you do oh you upgrade to a newer
version you try calling it with
different arguments you try using a
different library you file an issue you
complain about it on the internet and it
seems like there's a simpler option that
we're avoiding why don't you just fix
the bug I'm sure you deal with your
dependencies better than I do
I treat mine like magical black boxes I
don't want to know how the sausage is
made I don't want to open the box and
find out whether the superposition
collapses into a dead cat or not so time
and time again I will do ridiculous
contortions to avoid diving into the
internals of my dependencies and don't
worry if you do this too it's a
completely rational thing to do this is
a graph you might recognize from
economics it's the blue line is the
average cost to fix a bug and as you fix
more bugs your average cost drops until
eventually it gets worse because of
diminishing returns and the pink line is
the average cost average amount of time
value that you save from fixing
an additional bug notice that the for
the first bug for the first many bugs
the average cost to fix it is much
larger than the costs of time that
you'll make up why is that why did I
draw the graph this way because I just
made this graph up income you know the
program comprehension task is a critical
one because it is a sub task of
debugging modification in learning I
would go even further I would call it a
prerequisite if you do any of these
tasks without first understanding the
program and work on there be dragons so
in this graph that difference between
the cost of fixing a bug and the actual
value you receive from it is related to
the one-time cost required to understand
the software that you're working with
this is what we call a barrier to entry
it's the thing that stops me from fixing
bugs in my dependencies it's the thing
that discourages people from
contributing to large open source
projects it's the ramp up period when
you start a new job or a new project so
how do we deal with that cost how do we
deal with understanding software like
what is the state of the art in
understanding software software is
notoriously difficult to understand the
largest software projects are arguably
some of the most complex things that
humans have ever built
they have subtle interactions between
thousands or millions of components
sometimes running concurrently sometimes
each with their own state sometimes the
interactions are subtle and not obvious
so how do we solve this problem
well maybe documentation but let's be
honest your documentation is for your
users it's not for you and maybe maybe
you're lucky and you have architectural
documentation mine is out of date I bet
yours is too it's it's you know it's
some slide deck your CTO wrote two years
ago like frame
ethically before a meeting faced with
this kind of dark landscape of what you
can do to understand software like I I
despair and turn into this like pre
humanoid person wandering around with
like a stick with a pointy end like
poking at things right until they move
to try and understand what's going on
adding printf statements or stepping
through things in a debugger if you're
in a modern like software firm someone
might take you into a conference room
and sit you down and pull out a
whiteboard and they'll draw for you like
a fractal of nesting like matryoshka
dolls boxes with a mishmash of
interconnected lines that basically
could mean like pull down a terabyte of
data run the MapReduce job and put it
back into s3 or I don't know pull a
variable out of a hash map like that's
like those are those lines could be
equally the same thing I've drawn
diagrams like this myself they're
they're about as helpful as this one so
we end up resorting to our last best
hope which is to read the code and code
is a horrible serialization format for
the mental models of software
anecdotally we know this because of how
long it takes to build an understanding
a robust mental model from reading the
code it's it's kind of like it's kind of
like trying to understand how to bake a
cake by studying like chemical reactions
like you could do it given a long enough
amount of time it's just you're working
at the wrong layer of abstraction we
don't really work on code when you think
about software you're not thinking about
code you'll reason about it in lines of
code you reason about it in some sort of
abstract model in your head and when you
change a piece of software you don't
change the code you change the model in
your head and
you serialize it back out into code
source code is really just another
internal represent an intermediate
representation between your brain and
the compiler or between your brain and
the machine instructions this is one of
the authors of a paper we'll talk about
later and he says an expert computer
programmer encodes and processes
information semantically ignoring
programming language syntactic deals we
think about things as models but source
code is what we have it's it's something
that if you have a software project
you're guaranteed to have it and the
best part is it's guaranteed to be up to
date it's not wrong about how the
software actually works so what if we
had something more advanced than pointy
sticks to help us build mental models of
software from the code that we already
have so I was thinking about this and I
kind of was I was thinking along these
lines I came up with an idea and I was
thinking about small programs small
programs are actually pretty easy to
understand just by reading their code
it's when you turn them into production
systems when you turn them into larger
code bases you have to add things you
have to deal with edge cases and
workarounds you have to deal you have to
add additional features ones that aren't
used very often but someone needed them
or compatibility shims or air handling
or logging or monitoring like all of
these things that distract from the core
idea of what the code does what the
software is supposed to do so the idea
was can we take a larger program and can
we simplify it back down and remove all
of the noise to actually get to a
simpler version of the code that we can
then understand
this is real-world code this is the
function to parse a URL taken directly
from the ghost standard library I bet
you could understand it given enough
time like not just reading it on this
slide here like I don't expect you to do
that here's another function with the
same signature that parses a URL and I
thought I can explain it to you
let's read it the first thing you do is
you construct a URL structure you parse
the scheme out by calling the get scheme
function and assign it to the scheme
parameter and you save the rest of the
the string you normalize the scheme to
lowercase and then you split the
remainder on a question mark to get the
query string and then you split again on
the slash to pull out the host and
authority part and the path so you save
the rest the the path portion and you
parse the authority for the user and and
host like that's pretty understandable
for this function call for parsing HTTP
wo um those functions work the same
they're equivalent they yield the same
result and here's where it gets really
interesting those are more than just
equivalent functions I actually
constructed the second function out of
exact pieces from the first you can
actually see them I've tried to
highlight them here looks like it worked
but here's the really crazy part that
transformation was done automatically
here's the algorithm you write a test
case you run a code coverage report in
this case the test case by the way is a
test case that calls the parse function
with HT VW Google com and then you run a
code coverage report this is a tool that
we have that we use for determining
which parts of our code actually get
executed we usually don't do anything
with that
other than to say like oh we missed some
places we should write more test cases
but instead what if we took the coverage
report and then no we took all of the
statements that weren't matched by it
and we threw them all away the remaining
statements will be just the set of code
that we need to execute to actually pass
the original test case if you do that
you'll notice that the code you get out
looks pretty ugly it has things like
branches with nothing in them functions
that just return constants when they
used to do lots of important things so
what we can do is we can clean up the
ast we can do a number of passes over
the ast to actually try and fix up those
things to remove the dead branches to
simplify the control flow structures and
then write the code back out if you do
this there are two really important
properties that you get out of it the
code that you get out compiles and the
code passes the test case or test cases
that you use to compute that
transformation this is called a program
slice and the process of doing it is
called program slicing when I first came
up with this idea I foolishly believed
that I had invented something novel I
was so excited I told as many people as
I could I was just over the moon about
it until one day I went up to someone
and told him that he said oh cool
program slicing I learned about that in
my graduate compilers course and all of
my dreams of accepting Nobel prizes and
delivering Oscar speeches just
disappeared program slicing is not new
it is not even remotely new it is really
old it is nearly 40 years old at this
point and it was first described in 1979
by Mark Weiser
and since then there have been hundreds
of computer science academic research
papers written about program slicing
focused on all of the different possible
types of slices you can compute all of
the different algorithms that you can
use to
keep them all of the things that you
could do with the resulting program
slices the actual these are some of the
types of slicing there are many many
more than just these they're ten or
twenty or something like that
using the parlance that the program
slicing researchers use the slice that I
showed you before is an amorphous
dynamic backwards slice over all
variables the very first I I like I
thought this was really interesting so I
thought I wanted to like explain to you
a little bit about program slicing and
how it works especially like how it was
originally invented so the paper that
Weiser wrote when he originally proposed
slicing was about something called a
static slice it differs dramatically
from what I showed you earlier the
definition of a static slice is all
statements that may affect the value of
a variable V at some point P in a
program the main difference between the
slice that I showed you and Weiser
slices why's of static slices is that
they do not you do not execute the
program to compute them you do it solely
through static analysis and the two
properties that you preserve while doing
it are to preserve the behavior of the
program with respect to its output and
to remove as many unused statements as
possible but actually like computing the
minimal slice as the the pilots of the
community uses it is essentially
computationally prohibitively expensive
there are a number of algorithms you can
use to compute these static slices the
one originally proposed by Weiser was a
dataflow algorithm most people use
program dependence graphs I'll give you
a very very brief description of how you
do this with a program dependence graph
with just these two slides that I had
lifted from UPenn so
this is the the thing in the upper
left-hand corner is a program a very
tiny program that computes a sum and the
graph you see is the program dependence
graph so the program dependence graph is
made up of two graphs they're completely
separate but drawn concurrently here the
blue lines right are the control
dependences which is if you're at this
point in the program this node statement
essentially in the program
what other statements are dependent on
it and the other is data dependence
which is if I'm at a particular
statement in the program
what are I have variables that are
declared in that statement or
manipulated or reference and any other
lines that manipulate or define that
variable beforehand are dependent and so
the actual computation is like this
iterative computation where you
essentially propagate dependence
backwards through the graph by picking a
note first right so in this particular
example the node that they're using to
compute the slice is the final statement
the one that prints out I in the graph
it's labeled output i they've renamed it
for some reason over on the far right
hand side and you can see like the
actual edges that are bolded are the
ones that are that where we've
backwardly propagated the dependence
through the graph and it allows us to
remove these three nodes that compute
the sum because the only thing that we
actually said we were interested in was
the variable I
the other major type of slice was
proposed about ten years later it's
called a dynamic slice and the
difference here is that we're executing
the program the actual definition of a
dynamic slice is for a given program
input all of the statements that
actually affect the value of a variable
V at point P so like I said the main
difference is that we execute the
program we execute the program and we
record information about how it's
executing the value of variables which
way a control flow statement went while
the code is executing and what
information you record depends on the
algorithm that you use so the different
algorithms require that you essentially
pull out different information from the
runtime to actually determine this and
there are a couple of different
algorithms for doing this we're not
going to go over them so once you can
compute these slices what can you
actually do with them so there's a whole
bunch of literature about things that
you can do with slices some of them are
really interesting some of them are kind
of hilarious
so one of them is better context in
maintenance or debugging tasks right so
you get a pull request for one line
right and it changed one character which
if I'm remembering a code complete like
the three most possibly bugs in the
history of ever are all like one line
one letter changes that have like caused
billions of dollars in damage so what if
when you change those things you can
actually slice the program on that
particular line that particular variable
that changed and see all of the
statements right that might have been
affected one that Weiser proposes in his
original paper is automatic parallelism
which is this really amazing idea where
he said well what you do is you compute
a bunch of different slices from a
program and then you run them all
concurrently and pipe their output
to another process that merges the
output back together in the order that
the sequential program would have done
that one didn't catch on you could skip
tests this is a really great idea right
you have some sort of integration
testing suite a regression testing suite
that runs for hours but you changed one
line of code or one module like
something small like you don't need to
rerun all of the integration tests what
if you sliced the code and actually
determine oh these are the different
parts of the code that we know are
actually affected by this change so
we'll only run the integration test that
touched this particular piece of code
another one is is right reducing the
code base size so there are a lot of
static analysis tools especially in
research land that are devoted that that
have like these very computationally
expensive algorithms and the idea is
like well what if we sliced the code
down to a smaller piece of code and then
we run our algorithms on just that piece
of code and then are like exponentially
expensive algorithms like they run like
in time that humans can deal with my
favorite one though was was looking
through this I found a patent
application from this year by Amazon
which is this really fascinating concept
that I hadn't even thought which was the
idea that when you're shipping
JavaScript you know like you've written
like a big website like Amazon where you
have a giant amount of JavaScript that
you have to ship to your consumers so
you ship it to them and you add a whole
bunch of instrumentation to it to find
out what pieces of the JavaScript they
actually use right because the majority
people like they're all doing the same
thing they're all buying stuff right
like most people aren't like going in to
manage their account or other like sorts
of like tasks that the JavaScript needs
to perform so you actually find like the
majority of code is used by I don't know
like or the the majority of users only
use like 10 or 20% of the code or
something like that and you slice out
just that code right just the code that
needs to run and all of its dependencies
and that's what you ship and you write a
compiler module that basically goes
through the code and whenever it's about
to access a
piece of code that you sliced out it
tells it to wait and download that
dynamically right or if you have extra
bandwidth you ship it to them
optimistically and a client can like
compile the slices back together the
things that I'm interested in about it
though are for debugging and
understanding and learning and so one of
the questions you might want to know is
like does it work for those things
intuitively like I feel like it should
work but I would really like some
empirical evidence that it does work why
is there actually would argue that you
all use slices that every single person
who like reads code or debugs code uses
slices you just don't get them
explicitly but that good programmers
when we read code we're slicing in our
heads and and they actually like try to
do empirical research with a methodology
that I think is quite kind of
questionable to prove that you do
slicing mentally the introduction
section of nearly every paper about
program slicing begins with the sentence
slicing is beneficial for program
maintenance and debugging with like no
proof like just they like sight you know
Weiser's original thing that says like
oh yeah this is really good for
maintenance and debugging so there have
been actually like a couple people who
have done empirical tests very few most
of them were done I I couldn't find any
like more recent than 15 years ago these
are very old studies and more than that
like many of them like just weren't
specifically were like we got six
graduate students but you know we paid
them 20 bucks and here's like I don't
know it's not statistically significant
but it seemed like it worked there were
two studies that actually like produce
statistically significant results and
even then they weren't like pure wins
right like it says the debugging like
for the second Pro
Ram was significantly faster not the
first program though or what was the
other one like it improves like the task
of determining whether array accesses
were out of bounds and we like
statistically prove that slicing helps
that but only with second or third year
students not with first year students
have any of you used program slices like
anyone I want to talk to you afterwards
um I I haven't like actually talked to
anyone who's used program slices in
their day-to-day job there are very few
one of the reasons is that there are
very few tools to do it this is not like
a collection of them this is basically
all of them really serious there there
are a couple more but this is basically
it and they basically all work
exclusively on Java maybe C but that's
it because the slicing tool is for the
most part did like the the logic to
actually run them depends on integrating
with the compiler or the runtime if
you're doing dynamic slicing so like one
of the questions is like why are there
why don't these tools exist for like the
languages that we use day-to-day or like
why haven't you used them if you're
writing in Java or C all day part of the
reason is that they're like these like
most of these aren't maintained they're
not used very often writing them is
really hard right there's there's some
project progress here
one of the recent papers describing
program slicing is about a technique
called observational slicing which is
the idea that you can actually build
slices without any help from the
compiler basically by doing the same
thing that you would do for test case
minimization where you take a piece of
code and you remove stuff from it and
you compile it and you see did it
compile it compiled great will run it
did it give me the same output great
it's a reasonable slice and that
actually works and like parallelization
actually makes it work fast enough and
it lets you do things
like slicing without help from the
compiler or even doing things like
slicing across multiple languages
because most of the projects we work in
these days are composed across multiple
components written in multiple languages
so before we finish up I want to talk
about one more idea the idealized to
commit log when you're taking a slice of
a program you come out of it with a much
smaller piece of code that's equivalent
does the same thing that is hopefully
easier to understand it hopefully gets
you to the core structure of what the
program was doing for that particular
piece of input but all of the code that
you sliced away it was there for a
reason it is important it wasn't
important in that particular run of the
program but all of that air handling and
compatibility and performance tuning and
and monitoring all of that is important
eventually once you understand the core
of the piece of software then you can
actually add those pieces to your
understanding to your model so we
actually want a way I wanted a way to
actually take to go from this like
simplified structure this simple like
I've taken a slice and gotten to the
core structure of it and actually move
and acquire all of the additional pieces
right that we sliced away so I'm going
to propose to you another tool I call it
the idealized commit log the idea is you
take a slice you take a slice for a
particular test case well let me let me
start from the beginning say you run
code coverage for each of your test
cases individually and using that
information about what statements they
cover you hear this ticularly heuristic
aliy determine a reasonable ordering of
test cases
and then for each test you slice the
program with that test and then you do
it again with by adding another test to
the set so the first slice you compute
is with test zero the next one you do is
with Tesla zero and one the next one
with zero one and two and you difference
the slices this is what what it creates
is an idealized commit log and what I
mean by that it is that if you knew
ahead of time exactly the program that
you were trying to build and you wanted
to build it one piece at a time
this is the exact set of steps that you
would take on the code base if you
didn't make any mistakes so I actually
like built a little prototype tool that
I want to show you this is real-world
code again it is from the go standard
library it's a piece of code that is
devoted to scanning text tokenizing it
essentially you tell it like a
particular delimiter and ask it to like
give you the next token right so split
by lines right if you oh sorry yeah
how's that
all right we think we're at a good place
all right so you you might like give it
like a new line and ask it to like split
lines for you so I want to show you like
what this like actually looks like so
the first test case we have is down here
which is a thing that basically just
constructs a new scanner object and you
can see that the code that we get out
for it this is the entire contents of
the package the actual package is like
thousands of lines of code I don't know
it's not that much maybe a thousand
lines of code but this is like these
twenty or so lines are the only code
that's actually needed to construct the
scanner object here's our new test case
down here which we create a new scanner
object and we give it like internally
like a byte slice to work with and then
we tell it to scan but there's nothing
for it to scan right so we're sorry we
hand it some text to scan and here is
the implementation of the scan function
this is it the actual implementation is
about a hundred lines but to actually
like tokenize the the I'm moved it
already the actual like test from the
test case this is all you need and this
is actually like understandable it's not
like immediately understandable
like I can explain to you what's going
on you have like an end and you have
like a loop that you're doing
continuously to do tokenization that
kind of makes sense you have an end and
start variable which are basically going
to mark where in the buffer you're
trying to read from and you'll see if
end is greater than start well they're
both at zero to start with so the if
statement like never gets hit in our
first way through so actually what we do
the first thing that we're doing is down
here we're actually like reading data
from reading data into our buffer and
once we do that we like increment end
which is how much of the stuff that
we've read
at which point we go back up here and
now and is greater than start okay so we
actually like split the the text right
that we've actually that were working
with between start and end and it gives
us out the token that we're interested
in and the number of bytes to advance
and this is you can see advance down
here is basically just incrementing
start and then we assign token so that
there's another API that we can actually
call to pull the token out here's the
next test I'm not actually going to show
you the test I'm just going to for time
reasons but here's the next test and you
can see what actually is happening is
well you have some fixed size buffer as
your tokenizing it and like removing
stuff from the beginning eventually
you're gonna get to the point where you
can't add any more to the end of it so
here's code that actually like shifts
your the bytes that you have remaining
back to the beginning of the buffer that
you had but maybe it's completely too
small you can't get away with shifting
it like you have too much you're trying
to read more than you even have space
for at which point you have additional
code that's responsible for doubling the
size of the buffer so that you can
amortize the cost of growing the buffer
over time and that's that's pretty
reasonable code that's pretty easy to
understand maybe you're starting at the
beginning you don't have a buffer at all
right so here's another thing where oh
actually like we're gonna do this but we
didn't have any buffer so we have to
actually like pick a default size and
you'll see oh the default size constant
shows up and so on
see if I can get my mouse back so to
recap we need more tools to help us read
and understand code and one of those
tools is could potentially be program
slicing it's a tool that allows us to
reduce large programs into smaller more
conceptual pieces we can practically do
this with the tools that we already have
by combining by leveraging code coverage
tools and the algorithm for doing this
although like the particular prototype
that I wrote is forgo the actual
algorithm for doing this running code
coverage and pulling stuff out and then
rewriting the ast that's language
independent we can do that for any
language and the idealized commit log is
just an extension of this idea that we
can actually use slices and DIF then to
actually build a full understanding from
a large software project thank you
it looks like we have three minutes for
questions so talk about the heuristics
for determining testing order yeah I
haven't like settled on a particularly
fantastic design for that the idea is
basically you run the code coverage
report and you get out the set of lines
that each test case covers and then
you're looking for essentially a the of
the remaining tests that you haven't
decided to add to the set yet you're
looking for the Mac like the subset
intersection of all of the remaining
test cases that kind of satisfies the
most of the remaining test cases I think
that's the heuristic I ended up settling
on but I tried a couple other ones like
what's the next test case that adds the
fewest lines was another one that I
tried other questions
so to repeat it there are some tete
sorry the limiting factor is that
essentially like most there's a large
amount of software that doesn't have
tests is that like the gist of it or
awesome tests right yeah that's that's
definitely true if you don't have tests
then the particular like slicing
methodology that I demonstrated like
doesn't work right like you need tests
to do it which can either mean like one
maybe we have to write maybe this this
is not a tenable approach maybe we have
to work with static slicers or dynamic
slicers or like integrations with the
runtime as opposed to just relying on
test coverage or maybe the answer is
like if you want these benefits maybe
it's an incentive to write tests another
one as if you needed another one and
like some of the stuff that I actually
showed you like I actually ended up it
was kind of an interesting thing that I
discovered was that as I was writing as
I was working on this I like took pieces
of code and having the slicing tool
actually changed how I wrote tests
because I was trying to get the output
in a certain way so that it told a story
effectively I think we'll have time for
one more and after that if you have any
questions please find me on the hall
I have
cool so just to repeat aspect-oriented
programming basically can allow you to
take dynamic slices at runtime without
any additional tooling support cool I
didn't know that all right thank you
everyone really appreciate it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>