<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;An Introduction to Combinator Compilers and Graph Reduction Machines&quot; by David Graunke | Coder Coacher - Coaching Coders</title><meta content="&quot;An Introduction to Combinator Compilers and Graph Reduction Machines&quot; by David Graunke - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;An Introduction to Combinator Compilers and Graph Reduction Machines&quot; by David Graunke</b></h2><h5 class="post__date">2016-09-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GawiQQCn3bk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi there my name is David gronke it's
1250 and get started thank you so much
for for being here for joining me for
this talk and this is a pretty amazing
opportunity so I'm very grateful I also
want to say thanks to the strange loop
and the organizers and and to papers we
love Seattle for let me present a
version of this last week and yeah so
yeah my name is David gronke I'm a
partner in developer at a product
development studio called Thomas Street
and on the side I do side projects
sometimes weird side projects and this
is a side project that ended up growing
people were interested in it now I'm
talking about it it was really a project
that was sort of research project into
the early history of functional
programming languages and specifically
how they were implemented and the
techniques that were developed and be
talking about two complementary
techniques today called common one is
compilation to Combinator's the others
graph reduction these were two really
important areas of research in the 80s
in the 70s 80s in lesser extent 90s when
functional programming languages were
first finding their place in the world
and sort of the generation after after
your Lisp and an ml on those most famous
language that use these technologies is
called Miranda some of you might be
familiar with this generation was
eventually sort of surpassed by the
generation we're in now
where'd haskell is that is the dominant
form of functional appear lazy
functional programming and but I really
think it's worth looking at these
technologies I'm looking at these
techniques a because they provide a
window into the origins and motivations
of a functional programming MP because
they are actually a great tool for
understanding the state today which
built off this technology and in a lot
of ways as a response to it and so quick
poll who is familiar or roughly familiar
with Haskell syntax fantastic because
that's the what I use for my pseudocode
so that's great and okay so I'm sorry
talk about graph
production machines and a graph
reduction machine is a virtual machine
for a functional language that works by
iteratively modifying a graph data
structure in place
I'm also quickly to find Combinator
compilers before go back to graph
production a Combinator compiler is a
compiler that rewrites a functional
program into a version that only uses a
very reduced set of core functions a set
of sort of primitive functions that it
uses and combines in the poses to rebase
aliy implement your input program so
back to graph reduction like I said you
can think of graph reduction as a
virtual machine people talk about
machines when they talk about
implementations of functional
programming languages like the spineless
tagless G machine if you're familiar
with JHD internals a graph reduction
machine is a abstract virtual machine
and it's worth thinking about how we
normally think of virtual machines this
is a snippet of JVM bytecode that I
copied and pasted from Wikipedia and
normally we think of a virtual machine
you think of an analog of a physical
machine with similar inputs and similar
modes of operation where your input is a
vector or sequence of instructions from
some small instructions that relatively
small that you know you advance through
you advance a program counter maybe you
jump back but ultimately you're
following a path through the sequence of
instructions
all the while mutating some set of
registers or some remote memory and your
inputs are the sequence of instructions
this vector of instructions and your
output is the sort of final state of the
memory this remote memory that's being
modified by this code that's how we
typically think of virtual machines I'm
going to think about the JVM or v8 or
Smalltalk VMs under graph reduction in
our inputs rather than being a vector of
instructions are a graph data structure
that represents our program and the
execution consists of like I said
earlier repeatedly modifying this
structure and our final output is either
another graph that represents some data
structure that we can interpret or maybe
a primitive value like some number if
we're running a numeric program and the
the compelling property of graph
reduction as a model for from
for a virtual machine is that the model
of execution that it uses is actually
really similar to the semantics of the
high light high-level languages we're
trying to model
it's a VM that's based on lazy
evaluation currying and pure functions
but it's constructed from a really
simple set of primitives and has a
really simple data representation
basically the simplest graph you can
imagine in memory and what it shares
with with our high-level language is
like Haskell or or conceptually with the
lambda calculus is that it's based on
this notion of computing by rewriting
and computing by rewriting is something
that I think we're all even if we have
not thought about it in these terms it's
something we all do anytime you are
doing algebra by hand anytime you're
doing math by hand you're rewriting
expressions based on substitution rules
that you know we usually apply to it you
know on multiple lines but you can
imagine literally rewriting an
expression in place based on
substitution rules that you know and
that's how function application is
defined in a functional context this is
a snippet from the structures
interpretation of computer programs this
is from one of the earlier chapters this
is designed for novices people being
introduced to function application for
the first time and they define function
application as a evaluation of the body
of a procedure what the formal
parameters replaced by the corresponding
argument so we also express that when we
talk about the lambda calculus we use
this notation sometimes means the
application of the nut of this anonymous
function on the left with argue it with
formal parameter X and body e being
applied to the value Y you can replace
that expression with the expression on
the right which is e where every
instance of X has been replaced by Y
that's a pretty basic notion of function
application that hopefully corresponds
to how you think about functions and
this restated and so one look at an
example this is my pseudo Haskell
snippet we're defining a right fold
operation here and then we're calling
that right fold operation to define a
summing function this summing function
reduces over a list adds
starts at one in the list adds the last
moment to 0 and then precedes onwards
adding them all together to get our Sun
right this is a right fold I'm going to
it as a reduction it was an injection if
you're from Ruby and this is then some
function defined in terms of that
function rifled or folder another way we
could write that that second line that
is semantically functionally equivalent
is just by substituting that body in and
you know we could inline this ourselves
if we decide we don't want that
abstraction we've in lined the body of
folder of right fold but we've replaced
the values with the values we want to
use in the specific instance the value
of sum on the bottom half is equal to
the value is equivalent to the value of
sum on the top right and we can think of
the application of some in a similar way
this expression some based on our
definition that this expression where we
assign a value to total summing a list
of numbers we can think of our
definition of some and we can say this
based on that body of seven and so I
hope I hope it's makes sense that these
two expressions are equivalent I could
write the second one instead of the
first one and I get the same value right
and I could do that again and I can do
that again and again until I get to zero
at which point I can add all of those
right and that's conceptually how how I
think a function application in the
functional context it's a series of
substitutions well if you read the
introduction in the introductory
chapters of sicp the structured
interpretation computer programming
cuter programs this is also how they
describe recursion and function
application they have this great
visualization of a growing this big
expression that grows to the right as
you substitute in more and more values
and then gets reduced down at the end
that's a great intuitive way to think
about the application of functions graph
reduction is a concrete implementation
of that it takes that concept and uses
that pretty literally to execute
programs so if we think about that
program we just wrote where we are
defining a right fold we're defining a
son
we're defining a total we can sort of
conceptually step through and think
about how this would work under a graph
reduction machine the first step is
going to be replacing those bindings
with pointers this step is not shouldn't
be too weird this is kind of what you
would expect like a C compiler to do
right to turn variable bindings into
locations in memory turn them into
pointers but you can start to see we
start out the shape of a graph right we
have expressions that point to other
expressions when we want to evaluate the
second line the second assignment some
what we do under graph reduction is we
literally replace we take that body of
folder we substitute it there in that
graph and we template in our values and
we've created a new reduced form of sum
that is equal by definition but has a
more concrete implementation now and we
do that really by the key thing is that
under graph reduction we're doing that
by rewriting the input code essentially
which came in as a graph and the same
thing happens when we want to execute
this last line we rewrite that location
in memory that was previously pointing
to that expression some of the sequence
we replace it with this and then we do
that again where that pointer is now
pointing to some and again and again
until we have a fully reduced a fully
expanded form and we can perform that
addition and so the part to emphasize is
that when you're doing this in the graph
reduction machine and they got rep
reducing interpreter or a compiler that
is compiled to graph reducing code those
substitutions are are happening in a
pretty literal way we're really creating
new versions of the code where values
have been templated in and we then
execute that new version kind of compare
that to how you expect function
application to work normally where you
jump to some location in memory and you
step through this function and you refer
to an environment that gives you the
variables that apply to this specific
execution in this case every specific
execution gets its own version of the
code in a way and because we're
rewriting in place we get this important
property of lazy evaluation which is
that work is not duplicated
once it's performed when a given
expression is evaluated when total is
reduced to sum in this in this example
or where sum has already been reduced to
two this body of folder that work is not
gonna ever be performed again if we have
other things that point to sum which we
would expect you know given that we've
given to the binding we probably want to
reuse it if we've anything else that
points to that it's not going to do that
reducing templating work again it's
going to be as if we had written that in
line version to begin with and that's
something we can do because we have this
notion of referential transparency right
we're in a pure functional environment
which means that sum is mathematically
equivalent or I should say right fold
plus zero is mathematically equivalent
to this body of sum here we can freely
substitute those and make that change
without without changing the meaning of
the program and in fact we're sort of
elucidating the meaning of the program
by doing that until we get a final value
the other property that we get from that
is that you can potentially have
multiple reducers working in parallel
and that because you have this safe
substitution you can perform this atomic
substitution where the meaning of the
program doesn't change but it's in a
form that gets us closer to our our
normal form RN form of the program we
can have multiple set reducers working
at the same time we don't need a lock on
the program code code we don't need an
interpreter lock they can be freely
mutating this this graph data structure
and if something else gets there first
that's great it works been done for you
so an important part of graph reduction
in practice is is simplifying this graph
that we want to that we want to execute
because with our definition of a lambda
expression of a application of functions
this definition of the expression of a
application of a lambda expression and
there's some kind of unfortunate
properties for rewriting it's not quite
as simple as it is when you're doing it
by hand with sort of small toy examples
right you have to take this step where
let's say we want to perform this
templating step we pull in this body now
based on a definition of function
application we have to search for every
instance of op na before we can perform
this reduction right we're searching
this tree of expressions that's going to
be proportional to the size of the tree
right and that's not what we want to do
on every single on every single function
call right we don't wanna be traversing
an arbitrary tree just so we can make
that substitution and then move on to
the actual execution and there are a
couple solutions for this there's a
notion of directory strings which is a
sort of jargony word for an for an index
into the locations of bound variables
basically for every bound variable you
have an index of where it appears it's
one technique another technique that I
became uncas I won't say popular because
we're talking about functional
programming in the 80s but became
prominent in these in these types of
systems is a compilation to point free
expressions which I know there's a talk
later today by a marcia so you should go
to that if you wanna learn more who's in
the audience I think if you want to
learn more about this in detail and
especially learn about how to apply it
to your own work pretty much more a much
more practical than my talk and you
should go to that this afternoon and so
point free expression I'll I'll try to
go through it briefly it's an expression
where we avoid bound variables we avoid
introducing bound variables into our
code so this is a counter example we're
defining a silly little function f that
just performed subtraction and the
reversed version both with their own
bound variables to express this in a
point free form in a form that's
considered good idiomatic Haskell good
Matic functional programming we would
use a common Eider like flip so rather
than introducing new bound variables we
just use flip which is a function that's
previously defined flip is a function
that takes a function returns a function
that expects its arguments in reverse
order and then flip is to me the sort of
canonical useful
combinator for for constructing point
free expressions and this is something
that you do all the time in a school
code or in functional code something we
did earlier in the in example code we
redefined some as folder plus zero with
no bound variables but this this
technique to reduce to eliminate in this
case a bound variable eliminate the need
for a lambda abstraction effectively is
is key to the effective implementation
of graph reduction and such as it was
effective in 80s so point free
expression to recap is the use of a
Combinator to define a function without
specifying a bound variables a
Combinator I will say there are multiple
definitions of Combinator this is the
one that I find most in the in the
research I was reading a Combinator is a
function without free variables meaning
it operates only on its inputs it
doesn't rely on anything else that might
be in scope operates only on its inputs
that takes a function or set a function
and returns a function has been combined
or transformed in some way flip the
great example takes a function doesn't
take anything else doesn't rely on any
other bound free variables returns a new
version that has been modified and
there's this really interesting result
this is what originally got me into this
field was was the following the
following result which I found really
kind of baffling and that's that a you
can most importantly you can write any
program without using any bound
variables any program that you can
express in the lambda calculus you can
express purely using a small set of
combinators so you define a small set of
functions that you can then combine in
various complicated ways to create
programs that are equivalent to any
program the lambda calculus which means
any effectively calculable function
right and I think of that is sort of
like a basis from linear algebra these
are a small set of things that we can
combine in various forms to produce any
any instance in this set or the set here
is calculable functions there's a name
for one of the for a given basis that
you can use to compute any to construct
any calculable function it's called a
Combinator calculus this is probably the
most famous Combinator calculus and as
famous as Combinator calculate it's
called the skee calculus with a ski
basis of s ki calculus or basis and
consists of these three functions SK and
I written here with kind of Haskell
syntax and you can use these three
functions to write any program you want
any program that you can express in the
lambda calculus and any function any
program you write in your functional
programming language that presumably has
a straightforward mapping to the lambda
calculus you can express with just these
free functions in various combinations
which I found a little bit mind-blowing
is that these are a sufficient basis for
computation and the bottom function I
hopefully is kind of familiar it's an
identity function
the middle one K is maybe a little bit
less familiar but it's a constant
function it takes a value X and then
repeats back X no matter what it's a
it's a fixated bird you read tamaco
mockingbird no matter what why is it
always returns X s is a little bit more
complicated s I think of as a sort of
forking operation we have a value X you
pass it into F and G and then yuri
combine that with function application f
of X is applied to G of X and these are
sufficient for defining any function for
defining any program so here's an
example this is an expression above that
uses a bound variable to define this
doubling function that adds a number to
itself below is the representation in
ski Combinator's I'm also adding in some
an addition operator and so we don't
have to deal with church numerals which
would make this much much larger
much harder to step through and and I've
it's left associative but I put in
parentheses here to emphasize that we're
not adding s and I this is not s plus I
it's plus and iving past and s so we can
look at the application of this function
that we've now defined in this case
we're plying this expression to five
number five
once again we're not adding s and I plus
I and five we're all being passed into s
and we can look at the definition of a
ves and now we can perform this rewrite
we can apply this rule and we get this
new expression based on the definition
of s living sense and now we have
another rule we can apply we can apply
the different definition of I which
gives us five and now because this is
left associative we can get rid of those
parentheses and so we've produced we've
performed the same computation that we
would have with that bound variable but
with just these with just this
combination of functions S Plus and I
and you can do that with any function
that you can express in the lambda
calculus there's three examples one is
the one we looked at this doubling
function the second is at the flip
function that takes a function f returns
a new function that expects its
parameters reverse order and there's its
representation and smk combinators and
see if you notice a pattern with the
size of the expressions on the right and
finally we have church's successor
function and it's corresponding
representation sk i combinators on the
right so those are all pretty opaque
blobs of s's and k's right but if you
were to go through with a large notebook
and perform all these things by hands
you would get the same results as if you
had applied the lambda expressions on
the left hand side and you do that for
any lambda expression any arbitrary
large program expressed in lambda
calculus to by extension in a language
that we can easily map the lambda
calculus
like Haskell and and so you can create
these sort of bizarre giant blobs that
look kind of nothing like code that
we're used to even just expression the
lambda calculus already a little alien
right and we have these totally huge
opaque blobs but when we apply these
rules sure enough we get the result that
we expect and and there's a pretty
simple system for compiling from
expressions lambda calculus to
expressions of SK and I these are the
rules this doesn't include the logic for
recursing down and compiling the whole
expression that with these three rules
you can take any function that you can
express in the under calculus and create
one of these big blobs so this is three
lines effectively I have a version in
racket that's six lines and that
performs this work which I think that's
pretty cool that that we can create we
can map to this weird alien form with a
really simple mapping and that's one of
the one of the themes of graph reduction
and Combinator compilation is that we
can kind of take these big leaps in
terms of vastly simplifying our programs
or the rules required to define our
programs or the rules required to
execute our programs we can do this in a
very very simple way that you can write
a a program that captures the semantics
of the system in maybe 40 lines if you
have a pattern matching system a little
harder if you have these conditionals
but and these three definitions the
left-hand side are sufficient to capture
every possible program the lambda
calculus the lambda calculus grammar is
really simple right you have application
or you have a primitive term on the
right hand side this captures those
three cases with a special case for when
the primitive term is the value being
passed in and the reason this goes so
well with graph reduction is that it
really simplifies the work of graph
reduction that problem I talked about
earlier where we have to search through
this big tree to replace bound variables
is eliminated we no longer have bound
variables right all we have is these
sort of simple rewrites of these inputs
and applying a Combinator
reduction is always a local rewrite
we're never searching down into this
tree
we're always handling things that are
directly at hand so here's an example
i'ma stay on this page for a sec so the
this mapping between the expression on
the top and the negraph on the on the
bottom syncs in a little bit but we're
representing our our tiny little program
as a graph now consists of a set of
pairs right right hand side left hand
side might be a pointer to another pair
or it might be a primitive value we have
our first pair which consists of five
pointer to lit inter pair which on the
right hand side is plus the left hand is
the innermost pair s and I and it has
the shape because we're in a left
associative context how much sense does
that mapping make between those two
things
the idea with this with this graph
representation is kind of capturing that
our programs with Combinator calculus
are always going to consist solely of
pairs or primitive values because all we
have the only the only components of our
of our language that we have are these
primitive Combinator's plus may be
arithmetic whatever one a had and then
application of those to each other which
we represent with a pair so we have
these relatively simple graphs right
this is not an abstract abstract syntax
tree I mean it is in a way but it excuse
me it is in a way but it's much simpler
than any one any abstract syntax tree
that we're used to right and so we can
now look at a definition of s and think
about how we apply that as a graph
transformation which looks like this we
now have we previously had a sort of
lopsided graph we turn that into this
more symmetric graph which consists of a
pair at the top that in two pairs on
either side and now that we've done this
we can follow the same reduction rules
we did when we were doing this by hand
earlier reduce reduce I of 5 to 5 end up
with plus 5 5 and we do that by
performing series of small rewrites to
our graph the important thing is that in
the previous example those were
primitive values right but there could
be pointers to arbitrary sub expressions
and this is where the notion of lazy
evaluation and normal order excuse me a
normal order application are really
important a B and C can be arbitrary
large huge sub graphs they give me 99%
of our program but performing this
operation on them here consists of just
juggling pointers in a small local part
of the graph and we can do that because
we use normal order application which
means we don't execute operands before
they go into the function they're being
passed into write it's important part of
lazy evaluation the other way to think
about it is that we perform the leftmost
reduction first it's the definition of
normal order application and because we
do that we can perform a series of small
rewrites that when we add them all up
and do
iteratively and do it until they're no
more rates available we've computed
their entire program while only at a
given time touching a small part of the
graph which keeps the implementation
simple
it keeps the computational cost low
relative to replacing bound variables
and it lets us take advantage of this of
the semantics that we have in our
language normal order application lazy
evaluation to produce a simpler runtime
earlier I mentioned the notion of comm
you later calculus commenter calculi
calculus --is our sets of functions that
you can use to achieve this and there's
a bunch of different sets there's
probably gonna be infinitely many maybe
the smallest that I know of is SNK
probably someone has come up with a
version with just one Combinator like
the one instruction virtual machines and
what was that yeah so you can do with
one so I need one more above this which
is iota Combinator smk are actually a
complete basis because you can express I
as skk I is usually included because it
makes our programs a lot smaller you'd
noticed earlier that they grew bigger
and bigger and bigger if you look at the
length you might have noticed they were
growing I can remember what order it is
I think they grow exponentially
maybe it's quadratically but more than
linear which is not what we want with SK
and I and so a lot of times you add
additional Combinator's the handle
common cases B and C which are sort of
variants of s that cut down the size of
the generated code but providing special
cases for certain structures or patterns
in the program that you want to
represent with combinators one step
further is what's called the Turner
combinators and these are maybe the
canonical set of combinators that were
used in real world implementations in
the eighties and this consists of the
combinators we know plus y which is the
Y Combinator makes recursion a lot
briefer to express by having that
hard-coded in B and C B prime C prime s
prime which I just sort of further
complications of B and C NS that handle
it sort of common cases comments truck
choosing your code that you want to
represent as the application of a
Combinator there's also a notion of a
super Combinator and which also has
multiple definitions if you look it up
online the way it's used when you read
about implementations is it refers to
the generation of sort of bespoke
Combinator's so these are you know you
have these three s ki or a larger set
but your function your program might
have its own sort of common structures
that frequently appear or that you want
to represent so there's a notion of a
super Combinator which is a process by
which you come up with a bespoke custom
Combinator's just for your program as
part of the compilation process and that
was a somewhat popular technique so
there were there were implementations of
this technique this was a really active
area of research and 80s it was an
active area of development among
functional programming folks like I said
earlier the most popular version of this
is Miranda Miranda was developed by Dave
Turner a computer scientist eminent
computer scientist who also sort of
wrote a paper in 1979 that brought this
combination of combinators and and graph
reduction together and set basically ten
years of research into motion
Miranda was really popular for
relatively speaking was really it was
really popular for about ten years it
was a common education language it was
used by people who could convince their
bosses to use a functional programming
language and it was kind of a Haskell of
its day in a reduced form since the
community was smaller in general uh but
it was it was ultimately displaced by
Haskell and I'm not using I'm kinda
using Miranda syntax but I talk about it
as Haskell syntax Haskell is the is the
reference implementation of a functional
pure lazy language today right I should
say it's the reference language and GHC
is the reference implementation which do
not use graph reduction and I'll get to
that in a sec the spineless Tagus tree
machine is not a graph reducer despite
the G in the name
and it's ultimately because of
challenges mapping this model to stock
hardware which is sort of this classic
tragedy of functional programming where
we have this elegant small kernel that
is a beautiful way to represent our
programs it's elegant we can prove
things about it
but we run into challenges when we try
to apply it to stock hardware I'm going
to try to run it on our x86 CPUs on our
laptops and I will say that GHz has
overcome a lot of these challenges but
by 1989 there's a sort of sense after 10
years of really enthusiastic research
that there were some intractable
problems with with graph reduction and
that was that its representation of call
by need semantics of normal order
application didn't perform well on stock
hardware and it's interesting when you
read the literature there's not a lot of
formal analysis of it there's sort of
these descriptions of this shared
understanding that it's slow and isn't
working um but it was a really serious
problem and and posed a major Block in
research by the end of the 80s and there
was sort of two paths forward they were
proposed and there's a really great
paper from 1989 by Pollock you Jack
where he sort of lays out the last 20
years of research and proposes to pass
forward with respect to graph reduction
1 maybe the idealistic option is custom
hardware we respond to this problem of
not having CPUs that work for us by
building our own CPUs this has had this
met limited success in industry and
there's no shortage of masters and PhD
theses about creating FPGAs to run your
functional programs but they have yet to
break out of that world but it is it's
really interesting to look at there was
a processor that was I think actually
fabricated in some numbers in the 80s
called Norma and there's also a paper
from 2008 about a chip architecture
called the reducer on this was developed
between 2008 2010 which I find extremely
heartening that someone is working on
this in 2008 about 20 years after the
heyday
it doesn't make me wonder what what what
would happen a if if graph reduction had
it a day later in the history of
functional programming in an area where
FPGAs or cheaper we're ship
architectures are more sophisticated
where memory caching is much much more
sophisticated but the reducer on in
Norma both tackle this the fundamental
problem with graph reduction as
described by the authors of the reducer
on paper is that it's really memory
hungry specifically it requires moving
tons of memory back and forth because
we're shuffling this giant graph right
this giant arbitrary large blob in
general memory and the subtitle is
widening the von Neumann bottleneck
which refers to that that bandwidth
constraint and they do that with an FPGA
that can act when the reductions are
performed can reach into multiple sub
graphs at once and perform eight
remember correctly eight reduction steps
at once each accessing a different Bank
of memory it's a cool paper it's pretty
approachable and I'll talk about links
to these papers at the end the other
response to graph reduction that was
more successful was what's called
closure reduction closure reduction is a
cousin to graph reduction which could
have its own talk or or five and it's
what's used by GHD it's probably the
important thing and it's a technique for
mapping a call by need semantics the
semantics of our functional programming
languages to the call-by-value world
that we live in if you're familiar with
the GHD internals you might know that it
uses c and c functions as an
intermediary representation of haskell
functions c is maybe the preeminent
call-by-value language in our world when
we think about canonical execution of
call-by-value programs you're probably
thinking about the C stack and it pretty
successfully maps to that world and it
does that at the cost of greater
complexity the model of a haskell
function is much more complicated than
the SK and i combinators and there are
special cases there's more there's more
logic but it runs
it's successful and it's proved to be a
really successful compromise I do wonder
like I said earlier what today's chips
with today's FPGA is if Haskell do not
and ght particularly didn't already have
a dominant position
what graph reduction would look like so
if anyone was inspired here who loves
FPGAs you should work on something see
how it goes and tell me about it again
thank you I hope this is interesting I
hope this inspires people to look at
this sort of weird way of executing
programs that's my fundamental interest
is that it's really a weird way of
running programs you're mutating a graph
in place and rather than stepping
through instructions
so hopefully you find that weird
interesting to you okay thank you
okay five minutes for questions
yeah there is um I know of one paper
there's probably more what there's a lot
more research in that area in what's
called interaction Nets which are a
cousin of I think of as a cousin of
graph reduction and they originate from
proof nets in an informal logic but it's
sort of a I think of it as an
agent-based version of graph reduction
but there is research on GPU
implementations because you do have this
massive parallel reduction they could
potentially perform the challenge that
that paper described was that these are
large heterogeneous graphs of different
shapes and new shapes change changes
really frequently throughout the
execution of the program GPUs are
optimized for transforming one matrix
into another matrix of the same size and
so the work of mapping the graph onto
the matrix in between each reduction
overwhelmed the the speed benefits of
the performing the work in parallel
again it's another memory bus problem
really any other questions
I don't have the tied off the top of my
head it's from 1989 by Pollock you DAC I
meant to prepare a list of a list of
links I'm going to tweet a bunch of
links after this so this is my a really
dirty trick to get you to follow me on
Twitter and you don't to follow me just
look at my my Twitter feed in two hours
and there'll be a bunch of links there
there's a link in the on the website any
other questions alright thank you so
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>