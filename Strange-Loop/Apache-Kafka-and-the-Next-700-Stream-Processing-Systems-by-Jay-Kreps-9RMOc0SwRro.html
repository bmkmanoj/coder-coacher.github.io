<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Apache Kafka and the Next 700 Stream Processing Systems&quot; by Jay Kreps | Coder Coacher - Coaching Coders</title><meta content="&quot;Apache Kafka and the Next 700 Stream Processing Systems&quot; by Jay Kreps - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Apache Kafka and the Next 700 Stream Processing Systems&quot; by Jay Kreps</b></h2><h5 class="post__date">2015-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9RMOc0SwRro" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so thanks everyone for coming this talk
is going to be about Apache Kafka and
the next 700 stream processing systems I
don't know if it'll be 700 exactly but
hopefully around that my name is Jay
this is an area I've been working in for
roughly the last five years I'm one of
the original authors of Kafka and so now
I'll try and condense everything I know
about this area into you know 35 minutes
so the name the name comes from this old
1960s computer programming paper on a
system called I swim and what I liked
about this paper wasn't tried to break
down this language into kind of
constituent parts and see how you could
recombine those parts to get different
things and so that's a lot of what we've
tried to do with Kafka and I'll try and
explain in this talk some of the parts
that make up Kafka and how they can get
recombined for stream processing but
before I do that I should probably
mention you know what stream processing
is how many people have heard the the
phrase probably some most everybody okay
so everybody already knows you can
actually skip this section of the talk
then so my my definition of stream
processing is going to be a little
different I happen to think this is one
of the most exciting things going on in
technology right now I'm a little biased
because I'm I'm working in the area but
but I think it's really exciting you
know the way I think of it is there's
there's really three paradigms for
computer programming of course there's
lots of ways to slice up computer
programming by different types of
programming languages or whatever but
one that I think is actually a pretty
good way to slice up the pie is by how
you get your input and how you produce
your output so you know the systems that
most of us probably work on our request
response system so you get a request you
give a response people are probably also
familiar with batch computation and
stream processing is actually kind of a
third thing and and so I'll talk about
each of these real briefly so you know
request response this is you know HTTP a
rest service pretty much most of the
things people make you know somebody
sends you or a crash
which is really kind of one chunk of
data one input and you send back one
output one response and of course
there's lots of these requests and and
this is really the way you know most
systems are built at least the
interactive part of it and usually this
is synchronous like when I send you my
request I'm actually waiting for the
response but but still the back end of
most large companies is actually big
batch processes that kind of turnover
maybe once a day sometimes only once a
week and these are a little different
right they take all the inputs and they
kind of crunch on them and they produce
all the outputs all at once and I like
this picture I don't know what the guy
is doing I think he's just like
listening to see if his Hadoop job is
done yet and she's just like it never
finishes when you're listening like that
so it's kind of it's actually kind of
surprising that we still have all this
back stuff around but it's not that
surprising I probably most people's
first computer program was a batch
program right it was some little UNIX
program that read in a file and munched
it and spit something out at least mine
was and there's actually some advantages
to this type of computation it tends to
be really efficient or at least it can
be because you're allowed to kind of pre
organize all the data in the the order
you want you get really good data
locality so often batch computation is
really efficient in fact if you ever
want to do something really scary take
all the kind of input requests for some
kind of service-oriented architecture
and kind of try and rewrite your logic
just as a batch like Python script or
something and process them all and that
you'll actually find that that script
runs about a thousand times faster than
your big service-oriented architecture
most likely right why because you can
kind of pre organize all the data into
the right spot so if you do it that way
and this is why this stuff still exists
and between these two request response
and batch that's pretty much everything
but there is an alternative which I
think is you know getting more popular
and is out there which is stream
processing and this is kind of been at
the periphery there's certain systems
that work this way but it's kind of just
now starting to become mainstream and
the difference in stream processing as
opposed to these other two
instead of getting one input and
producing one output or instead of
getting all the inputs and producing all
the outputs now the program actually
kind of has the control so the program
kind of gets some inputs and then it
produces some outputs and how much is
some well that's up to the program so
you know it could for every input
produce an output and it could take all
the inputs and then produce a bunch of
outputs but it can also do everything in
between so so it's kind of a
generalization of these two extremes and
what's exciting about this is like a
request response like client-server
programming it runs forever but but it
also gives you kind of total control
over the trade-off between latency and
efficiency and it allows you to do a lot
of the kind of complicated analytical
things you might have in these big kind
of batch jobs but it allows you to do
them really quick and you know what it
doesn't mean what I just gave is my
definition a lot of people will have a
different definition in mind and and
what it doesn't mean is computation
which is you know transient or
approximate or lossy and a lot of
systems that have done stream processing
have been that way you know so so the
for a while you would see stream
processing systems can come the way they
worked is you would throw data at them
and they would try and compute something
and they might get the wrong answer and
if you know things came too fast maybe
everything would fall on the floor but
there's nothing inherent about that that
was just a weakness of those systems so
so I don't mean anything that's
transient approximate or lossy you can
absolutely make stream processing you
know get the exact right answer as you
would expect with the batch process and
it can compute you know the full set of
things that are computable the domain of
stream processing usually is
asynchronous stuff like I said it kind
of generalizes this full scope from
request response to batch processing but
it's kind of more useful for things to
do asynchronous by asynchronous I mean
decoupled and this kind of decoupled
work is actually really useful right
this is where you know all the kind of
back-end intelligence of a lot of
companies happen and in general I found
when building systems whatever you can
kind of take out of the synchronous part
of a request where maybe you have just a
few milliseconds to get your job done
and put into something a synchronous
usually the better it's usually more
efficient it's usually safer when it
fails because it doesn't take everything
else with it and you know a lot of
people may have seen some of these kind
of stream processing program things this
is some reactive X snippet I took off
the web and so you'll see a lot of these
types of api's designed around stream
processing and you know getting that
done but what I'm really talking about
is not so much this this is you know
like the kind of rx Java or these other
things these are really libraries for
doing computation in one process and and
they present really a user interface to
processing a stream of data and that's
very useful but what I actually am more
interested in is come stream processing
in the large and I think this is kind of
a trend you'll see in a lot of places
the scope that computer programmers are
interested in has kind of expanded so so
a lot of systems now are no longer
focused on just the internals of a
program kind of where you know
programming language would be they're
actually kind of zoomed out and they're
focused on computer programming at the
datacenter level so a good example of
that would be something like nice O's I
don't know people are familiar with me
so's but what it does is kind of simple
right it starts and stops processes but
by doing that kind of across a company
you can actually get quite a lot out of
it in the same way I'm actually
interested in stream processing in this
way so you know being able to represent
what's happening inside of a company as
a set of streams and it turns out I
think this is a pretty good metaphor for
you know what a company or organization
or large data system does right it has
streams of inputs that represent all the
kind of new things coming into the
company it has a set of processes that
run that respond to those inputs it has
some state about you know what it
currently knows and then it has outputs
hopefully one of those outputs is money
but not always right so if you think
about something like like retail I think
everybody's familiar with retail stores
you have sales you could represent kind
of a stream of continuous sales and a
big retail
you know company is going to have a
continuous stream of sales in different
locations probably all around the world
probably continuously you have a stream
of shipments you have a stream of maybe
price adjustments and inventory
adjustments you have analytics and fraud
and you need to reorder products and you
can actually map this pretty well to
streams and stream processing so sales
and shipments are maybe new inputs these
price adjustment processes and inventory
adjustment processes analytic processes
are probably stream processors that
react to this and do something now they
may not be implemented that way they may
be implemented as kind of weird services
or batch probably most likely in in real
stores batch processes but they they
could be implemented as kind of
real-time things it would be much faster
and it's actually kind of a better
metaphor for what's happening but the
kind of problem in the reason you
haven't seen as much stream processing
stuff today it is really there hasn't
been the infrastructure for it so you
know the world of kind of request
response things that's up there at the
top this is your kind of OLTP databases
you know rests frameworks it's actually
pretty well developed and you know it
targets you know getting a response
usually in a few milliseconds there's
pretty good infrastructure there and if
you come down to the bottom you know
kind of batch stuff you've got Hadoop
you've got data warehouse stuff like
tera data is actually quite advanced
it's really pretty good so there's a lot
of supporting infrastructure if you want
to do processing there but in the middle
there just hasn't been that much to help
people out right and so if you want to
build anything which is kind of slower
than a few milliseconds but faster than
a few hours you can are on your own
you're kind of inventing it a little bit
from scratch there has been stuff here
so there's kind of your enterprise
messaging systems complex event
processing or CEP there's kind of a
thing called an enterprise service bus
that hopefully not that many people told
of the OLTP world has had database
triggers and materialized views which
are their attempt at kind of a sinker
you know responding to events but none
of these are really good none of these
are really technologies you would want
to develop a large meaty piece of you
know important company infrastructure on
top of
and you know the result has been the
work that's done in this domain tends to
get pushed upwards into the kind of
request response layer or downwards into
the kind of batch systems and there's
kind of a vacuum in the middle and
that's really kind of what we've been
trying to address and the reason there
hasn't been the reason there hasn't been
much here is really there's there's a
number of hard problems that a stream
processing system has to address you
know the first is that since I said I
was talking about doing this in the
largest partitioning and scalability how
do you spread a program over many
machines and be able to elastically add
you know capacity to that or shrink it
down and how do you spread the data as
well
semantics and fault tolerance what does
it mean when one of these machines fails
what do you do there's this whole
problem of unifying streams of data with
tables of data so in the retail example
there were certain things were really
tables like what's our stock on hand and
there's certain things which are clearly
streams like the sales that are
occurring and how do you put those two
things together gracefully finally time
time is kind of the worst thing in
computer programming in general as
anyone who's tried to deal with gates
knows but it's particularly bad in
distributed systems where there isn't
the notions have done become many and
stream processing suffers from this even
more in the request/response world you
basically ignore time so if you have a
micro service architecture you query
lots of services they're all kind of at
now what is now mean I don't know but
you know it's roughly now right in the
batch world they actually just control
time by loading all the data at the
beginning of the day and then not
changing anything until the end of the
day when they do the load again right so
stream processing is going to have a
much harder time because it has to
continuously you know account for change
but it may actually need to catch up
with older data and finally reprocessing
so let's say I have a stream processing
system and I maybe count things that are
occurring if I change the logic in my
programming but I want to rerun that
programming and get new answers again so
okay so that was kind of an introduction
to the area the
now comes the patching kafka part so
Apache Kafka is a system I built with
some of my co-workers when we were all
at LinkedIn and we since left and we
started a company which is kind of in
this area but what it is is is really a
kind of messaging system or you know
stream database or something so you have
lots of producer processes that publish
streams of messages into Kafka Kafka is
a distributed system it runs on a bunch
of computers it kind of maintains all of
these streams of data it maintains them
in a fault-tolerant way so that you know
each piece of data is stored on multiple
machines and they can handle failures
and all that and then it allows
consumers to tap into those streams and
consumer and that's kind of the you know
what does it you how it does it is
actually really different from typical
messaging systems or other systems in
this space so internally what Kafka you
know stores is a log and not everybody
has seen this idea of a log it's
actually a very simple idea these little
rectangles here are meant to represent
you know messages or records and each
record I've kind of given a number like
0 1 2 3 4 5 and the next rights are
always appended at the end and different
readers could could be reading they
always read from left to right and so
you could think of this as being kind of
like a formalization of the log files
you get out of your applications so like
an Apache log right but this is a little
bit crisper so the contents they may not
be plain text it may actually be some
binary data I don't know what the
contacts of one of these records are
I've given each line a formal number and
I'm actually allowing readers to kind of
subscribe to it so you can think of it
more like a commit log in a database but
but it's not very different from a from
a Apache block and it turns out that
this data structure is very closely
related to the problem of you know
consensus or having you know many
distributed things agree on an answer so
most people would you know use a log as
the implementation of something like
raft or multi pack so these types of
algorithms are really kind of
maintaining a log that's probably the
simplest way to think about what they do
and they show up in databases so you
know inside of a distributed database
you will often
find some type of log of changes it's
the core set of what data was modified
and not all databases make the log part
explicit but certain many of them do so
I know when the big distributed
databases at Yahoo does this the the
database at LinkedIn were where I was
did this and actually is is now starting
to use Kafka for that distributed commit
log I know the big database of Twitter
does so it's it's not uncommon to have
an explicit log system that is recording
these changes and since this is to talk
about stream processing I would argue
that this type of log is actually the
kind of physical manifestation of a
stream so I talked about streams of data
what does that mean well I think
formally defined it's going to be a
sequence of records and the only
difference I would add is you're
probably going to partition this up into
multiple log so you have some notion of
parallelism right if everything is
totally ordered then you everybody has
to coordinate you know to maintain that
order and since many things happen in
parallel in a large company you're going
to have many of these kind of
partitioned logs and so producers are
going to add messages consumers are
going to read them and and this is kind
of the core data structure that Kafka
maintains so if you understand this data
structure then you basically understand
everything there is to know about Kafka
it just maintains lobs it tries to do it
a large scale efficiently not lose data
you know handle faults all the kind of
hard stuff and so now we know what a
stream is stream processing you know in
my world view is really just
transforming some of these input logs
into output logs right we said log was
basically like a stream so the processor
is going to be your your code in some
sense and there can be a framework which
you run your code in which helps you
know helps make it easier to write the
program but it could also just be like a
you know Python program
there's no magic if you're transforming
one stream into another stream you're
doing stream processing and you can you
can pat yourself on the back
one thing I've kind of called out here
is there's like a little database
looking thing little orange database
look at the inside the block of your
code and that is state that that are you
know stream processing the easy problem
is if you give me one input and I just
give you an output then then it's not
very hard but if I have to maintain some
kind of skater that is you know account
or a join or something that's going to
stay
and some length of time it could be you
know you know the whole execution of the
job or it could just be you know counts
over a 5-minute window then I'm going to
have this skate with my job and I have
to make sure that that state is kind of
protected even if my code
you know dies and that that's going to
be one of the hard problems okay so you
know an important thing to understand in
this area is the concept of a change log
so I talked about logs and I talked
about streams but one of the the core
uses especially in data systems is
maintaining a log of changes so here
I've kind of drawn out a series of put
operations so mutations write updates to
key so it's put key value and you have a
bunch of you know modifications of the
same small set of keys and you can use
this to represent the notion of change
over time so the progression of the log
from left to right that's basically time
and these put operations that are
recorded in the log this is the set of
changes which represent the mutation in
the state of the database so anybody who
is interested in functional programming
and knows about kind of persistent data
structures this is exact same thing but
applied to data systems at a larger
scale and this it turns out is actually
you know very commonly how databases do
replication so like Oracle has you know
kind of a log shipping protocol and my
sequel has a log shipping protocol and
internally distributed databases do this
and you can replicate state off of this
and this is going to come into play and
stream processing as well so Kafka
happens to have a particular facility
for maintaining this type of mutations
to state which is we call log compaction
which allows you to take these type of
redundant updates and compact them down
by getting rid of the redundant once
over time and this is important because
it's going to turn into a way of
maintaining that type of persistent
state that I just talked about
okay so we've gone through most of the
ingredients that Kafka offers we've gone
through logs partitioned up fault
tolerance in those logs the final thing
that we have to talk about is how to you
know actually scale the stuff outside of
Kafka the the consumers of data so it's
not enough to be able to horizontally
scale the data in Kafka you have to be
able to scale
the processing of the data otherwise
there's really no point and the facility
that Kafka has for this is called groups
so it allows many processes to all kind
of join a group and know who are in that
group and so originally we did this with
zookeeper but in the in the newest
release it'll actually be kind of native
for Sylve of Kafka and what this allows
this group of consumers to do is
actually divide up all the logs and each
kind of process its own subset of logs
and and it does this dynamically so you
can kind of add new consumers and
they'll join a group and they'll start
doing work and if once some of them die
they're then Lulu you know come out of
the group and you know their work is
given to other people and this is
maintained dynamically and there's
multiple groups because Kafka allows
multiple readers for the same stream so
all these streams are multi reader so
those are kind of the ingredients that
Kafka provides how do those actually
come together and attack some of these
hard problems in stream processing I'm
going to give you a quick outline of
some of the ways that this happens the
actual mechanisms for doing stream
processing with Kafka the most popular
one there's kind of just in your code
I guess people you know consume data and
do stuff but there's a bunch of
frameworks which have emerged which kind
of help with this
so there's SPARC has a kind of spark
streaming which will do stream
processing and works well with Kafka
storm is a framework which works well
with Kafka Sam's as a framework which we
wrote which is actually you know built
to work natively with Kafka there's
another system flink and they all use
you know different subsets of these
facilities I've described and the thing
I'm going to focus on most is actually
Newark that'll just be coming out in the
next month
which is called Kafka streams and what
it is is actually not a whole framework
or distributed cluster you deploy your
code into it's actually just a library
for stream processing but it does the
same set of things that most of these
systems do so it'll kind of address
these harder problems that I've talked
about okay so the first hard problem was
partitioning and scalability in Kafka
this is done by basically dividing up
these logs and with this group group
management feature so this is really the
way you know Kafka streams or these
other stream processing systems can
divide up the work or
you know horizontally spread it over a
set of processes and change the
processes
what makes this hard is the fact that
these processes are allowed to maintain
state so if you were here for the last
talk I think it was on stateful services
in some sense each stream processor is
its own stateful service and it has to
solve those problems of you know what
happens when I come up somewhere else I
don't have my state how do I get it back
and the way we do this in Kafka is using
that changelog feature so Kafka allows
you to maintain change logs like logs of
updates so one of these processors which
is keeping local state it can keep local
state in like a rocks to be incidence or
a local key value store you can keep it
in memory you can actually keep it
however it wants it journals out these
changes to Kafka and that acts as a kind
of fault tolerant backup of everything
that's happened in ik the new you know
instance of that processor can always
restore off about if it needs to and
this allows Kafka to to represent both
the notion of a stream which I've drawn
on the left and the notion of the table
which I've drawn on the right and
putting these two things together is
actually really important because what
people want to do with streams of data
is usually join them on to existing
tables of data which usually comes in
the form of change logs and so unifying
this notion of a log of changes with an
instantiation of those changes like a
key value store is one of the key things
I think that these stream processing
systems have to address ok fault
tolerance so there's actually a bunch of
aspects of this and is probably too much
to get into and a talk like this but
Kafka helps to do this by detecting
failures via that group mechanism it
allows you to restore your kind of state
off these change logs and since the data
streams themselves are playable whatever
hasn't quite been processed yet can be
pulled back there's more work to do here
in Kafka like there's actually a
prototype of a more complete kind of
atomic transaction across writes so on
that will strengthen this but but a lot
of these systems that do stream
processing basically rely on that replay
mechanism to be able to get provide
their guarantees all right the final
hard problem here was time and this is
one that in our earlier days we didn't
fully understand
but it turns out that you really need to
be able to deal with late-arriving data
and so you really have to be able to do
counts and joins and all these have to
be updatable when new data arrives and
so it turns out that really the solution
to that time problem comes out of
maintaining counts as sort of mutable
tables it can keep being updated even
after that time has kind of passed on
the local clock okay so those were kind
of some of the hard problems it turns
out if you solve these problems you
actually do kind of achieve this
unification of batch processing and
stream processing and the way you do
that is pretty easy so if you have this
like Log abstraction you know a batch
process is one which kind of wakes up it
has some position in the log maybe it's
a you know time three here it processes
forward until it's time you know it
reaches the end then it goes back to
sleep maybe it wakes up again at the end
of the next day and it processes more
and then it goes back to sleep and it
wakes up again so batch processing is
actually really simple it's just you
know it's just a process which when it
reaches the end it shuts itself down and
it kind of happens on some schedule
screen processing is one that kind of
stays alive it keeps waiting for new
changes to arrive and so this log
concept kind of helps you unify batch
processing which happens on some kind of
regular schedule with stream processing
which happens continuously as data
arrives and that kind of brings us to
this reprocessing problem what do you do
when you change your code and you need
to recompute things it's actually really
simple you just go back to the beginning
of time and reprocess and the beginning
of time it may not be everything that
has happened because you may have
compacted that log down to just the kind
of most recent updates and so this
notion of time is in Kafka is called the
offset that's your position in the log
and zero is kind of their very beginning
so you know anytime you change your code
you can have reprocess in the stream
processing system by just rewinding to
the beginning and letting it rerun and
this can happen in parallel because
these topics are actually multi
subscriber okay so how does this
actually play out in the large we
started to put this stuff into practice
and we kind of called the idea a stream
data platform so up at the top here I
have those kind of requests response
systems these are applications or rest
services database and you can go capture
changes out of these into Kafka and have
the kind of set of streams of what's
happening in the company you can attach
to this stream processors which do
transformation on those streams so that
they're responding to updates are
happening in databases they're
responding to log events or things that
might be happening in applications and
they take those streams that they kind
of transform them into new streams and
publish them back and then usually
feeding off of them in this kind of
asynchronous domain is real-time
analytics alerting a lot of the kind of
intelligence
you know stuff that's happening quickly
so that top layer is really the kind of
request response area the middle layer
is your kind of asynchronous stream
processing area and finally you can feed
these same streams into Hadoop or a data
warehouse and so we we actually spent a
lot of time you know building this
system and getting it to scale and then
we actually put a lot of this into
practice at LinkedIn and then through
open source actually and a lot of
companies so Kafka's and thousands of
companies now and LinkedIn it was
actually I think as of you know a few
weeks ago is actually taking about 1.1
trillion
messages or records per day that flowed
through this kind of central message
broken or persisted and that that's the
feed of all the data into the offline
world it's the basis for all the kind of
stream processing and real-time
analytics or Nearline stuff and it's
also you know kind of like one of the
core you know data systems for storage
and other data systems that rely on this
as a kind of commit log and so if you're
interested in these ideas Kafka is an
open source project it's an apache
project you can read about it there we
have a pretty active blog about Kafka
and stream processing stuff a confluent
that Kafka streams work I talked about
there's actually a design document if
you google Kipp 28 you'll read all about
it
and I have stickers if you want them and
I think we're pretty much out of time so
if you--if you have any questions or you
want to sticker just come find me I'll
be around I'm the really tall guy thanks
so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>