<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;An Overview of Probabilistic Programming&quot; by Vikash K. Mansinghka | Coder Coacher - Coaching Coders</title><meta content="&quot;An Overview of Probabilistic Programming&quot; by Vikash K. Mansinghka - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;An Overview of Probabilistic Programming&quot; by Vikash K. Mansinghka</b></h2><h5 class="post__date">2015-09-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-8QMqSWU76Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm delighted to be here for those of
you who saw Richards talk you will
definitely recognize some of the slides
we were getting together yesterday to
kind of go over each other's
presentations and Richard kind of you
know very calmly asked me hey can I see
a few of your slides for today and I
didn't realize what was gonna happen so
for people who didn't entirely digest
satellites the first time we'll have a
little bit of review but before I get
started on the technical material it's
not a little bit of context about the
group so I run the probabilistic
computing project and we're a small
group three or four research engineers
three or four grad students a couple
visitors at MIT and we do a mix of very
basic research math theorems stuff like
that through to very very applied work
where we take the problems to computing
systems that we build and try to use
them to solve problems to teach us
something about the programming
formalisms
and computing formalisms that we're
trying to develop so we're Richard today
really focused on Bayes DB I'm just
gonna give a brief overview of Bayes DB
and then try to broaden the picture a
little bit and explain my view on what
probabilistic programming as a field is
really about and give you a couple other
examples specifically where Bayes DB you
can think of as a domain-specific proble
stick programming language for bayesian
data analysis I'll describe picture
which is a domain-specific problems to
programming language for computer vision
and I'll spend a little bit of time at
the end talking about venture script
which is kind of oh it's our first
attempt to build a general-purpose prob
allistic programming language and I
think one of the key questions there
will be what does that even mean then I
hope by the end of the talk
I'll have given a good sense of that
so let's get started
so right now I think I need to start
with an apology which is you know
there's a lot of excitement around big
data and inference I've a colleague of
mine sent me this one so aside from the
headline quite quite modest the real
promise of big data is it's changing the
whole way humans will solve problems
down near the bottom it tries to argue
that a pap iya fication can be traced to
Kant's work in the 18th century which
actually I think is narrowly I think
narrowly construed may be true but I
think it's really important for all of
us to be very critical of this because
underneath this kind of picture is what
I think of as the newcomers reality to
machine learning and statistical
inference so what I'm showing here is a
flowchart that is still distributed as
part of the documentation with
scikit-learn a popular Python package
for machine learning and kind of a
vector for new machine learning
techniques at a certain level of
maturity to make it out into the wild so
a typical path through this flowchart
goes something like this so so so the
purpose of the flowchart is to teach
people who aren't machine learning
experts how to choose which API methods
to call for a given problem so maybe you
start at the start node and then you say
ok how many data samples do you have
well you can follow a path which says
let's say of more than 50 samples are
you predicting a category ok
and if the answer is yes then they'll
say that you have labeled data you know
that it's data where you know the
correct category label ok if the answer
is no then they'll say ok well do you
know the number of categories that
you're hoping to divide the data into
you say no ok what if you have do you
have fewer than 10,000 samples the
answer is no then you end up at this
node called tough luck
and kind of the worst part about it is
they're not wrong that is if you accept
the conceptual map on statistical
inference that's embodied in this API
then this is the best they can do so I
grew up actually originally as a
programmer but then gosh I guess about
ten years ago now started doing academic
research in AI and cognitive science and
started to learn about machine learning
and this picture has been unchanged at
least for that long but really much much
longer than that it's been been
unchanged since you know the late
sixties early 70s and the pattern
recognition era just you know the number
of nodes and the names and the nodes are
different so that's that's one kind of
view on the difficulties associated with
machine learning or they're the hard
reality but actually for experts there's
an attendance set of just difficulties
as well so even when you win with
machine learning you construct systems
using a methodology that's sort of
predicated on your own inability to
understand the system that you've
produced right so you do machine
learning when you get you can give an
empirical specification of the task
you're trying to perform and typically
almost no other specification so you
succeed and what you have is a brittle
hard to maintain opaque system often in
production so some of the leading lights
in machine learning systems
infrastructure at Google wrote a paper
about it which I think is quite nice
where they call machine learning the
high interest credit card of technical
debt and they talk about the problem
where the more miraculous machine
learning seems for a v-0 ship the worse
it's going to be for v1 because the less
you'll have understood what's going on
and at the same time another difficulty
is that although you see you know papers
upon papers about new machine learning
methods at conferences like ICML and
nips and UA I and there's a real
kind of sense in the field that if
you're really trying to get something
done you you often will want to reach to
the tried-and-true pattern recognition
techniques from the 60s and that indeed
the academic literature has created an
illusion of progress on on methodology
now that's not to say that there hasn't
been substantive progress but there's a
real translation problem happening where
even within the field of machine
learning it's difficult to tell you know
somebody has a new method what problem
should it apply to when will it work is
it really solving the hard problem
that's you know driving the nice-looking
3% increase in accuracy or whatever in
the published paper so I want to start
with this messy picture because I think
you know I've seen a lot of sincere but
I think maybe optimistically simple
answers to this problem over the last 10
years which are all predicated on the
idea that the basic type of computation
going on here is pretty familiar we just
need to use good ideas from what we
understand about engineering and
software engineering and you know maybe
we can maybe we can make a difference
and I've come to believe that that's
really not the case but also at the same
time that there's a tremendous
opportunity both to increase the
capability of computing systems the
level of automation we can depend on and
to really push our notion of programming
and fundamentally interesting in new
conceptual directions so how could those
be sort of at the same time well really
I think what's happening and this is
maybe the most important message from
from my talk what I believe has been
happening or actually really over the
last 15 20 years has been this slow
collision between probability and
computer science and wherever that
collision has led to a little bit of
integration like statistical learning in
big data there's been a lot of
productive result but at planet but but
at bottom this collisions really
uncomfortable and the reason it's
uncomfortable is you know computer
science is sort of this like descendant
of
logic and the conceit of computing is
that we get to reorganize the machines
so that reorganize the world so that it
makes sense for our machines but the
places where probability started to
creep in initially robotics user
interfaces later on information
retrieval artificial intelligence are
all places where we can't reorganize the
world to match the incredibly
restrictive requirements of the core
concepts at the heart of computer
science so that's a really uncomfortable
kind of set of tension that the fields
been trying to navigate for a couple of
decades and I believe now there's enough
kind of examples on the ground that it's
possible to formalize a satisfying
integration of probability in computer
science where we get to have some of the
best properties of computer science as
an intellectual endeavor and an
engineering tool but the capabilities of
dealing with the messy ambiguous real
world that come from confronting the
incompleteness of our own knowledge
which is what probability is about and
so the problems to computing project is
really trying to take that attack that
problem as directly as possible so my
talk today is mostly about probabilistic
programming which is the software layer
of that integration but I want to
encourage you especially here in the
future of programming workshop to think
about the problem space that we're
looking at that we can only see very
dimly and blurry ly at that scale
because I think there are tremendous
opportunities to do things that are very
very new and we're gonna need a lot of
help from from this community to be
successful okay
so let's dive in a little bit more into
Bayes DB so to recap actually can you
raise your hand here if you were at
Richards talk earlier okay good all
right so so so Bayes DB is a proble
stick programming system that aims to
make it as easy to query really at the
implications of your data as sequel lets
you query the data so so what could that
mean well
the first thing we need to understand is
the difference between information and
data so without getting too
philosophical like here's a data table
of patients okay well at least it's a
table of measurements that I'm claiming
or about patients right for me to call
it data about patients I'm making some
modeling assumption
maybe I'm saying I'm gonna treat each
row as a random sample from some process
that's producing patients but there's
some act of modeling involved in me
promoting this piece of information and
calling it David okay but let's say
we've done that with sequel we can
directly express questions about the
contents of that data right so we can
say what are the values of the heart
disease total cost of care and age
measurements or variables or columns
filtered on age being greater than 30
but this has problems
first of all in some cases we're gonna
get back question marks because the
table we had happened to not record
values for those measurements in other
cases we're gonna miss rows entirely
because we don't have the age and if we
don't have the age we can't run the
predicate so the first inferential
primitive that bql the Bayesian query
language provides is infer and what
infer does kind of think of it by the
following procedural model this is too
slow to actually do but you know our
prototype currently doesn't do that much
better than that right which is before
you run the Select try to infer all the
missing values you know and check the
ones that have the requisite confidence
and then run the Select okay so it's
meant to be a way of kind of integrating
an inferential workflow into a normal
kind of database land workflow or which
you get our values and then you can
proceed as as usual and what we see here
is that you know there there's a patient
we get back one row 132 where we were
able to
for the Aged with the necessary
confidence and so they're included in
the and the result set and we also see
that we're not able to infer whether or
not patient 131 has heart disease so
this is I think essential it's
unfortunately not the case that either
most machine learning routines or
statistics routines or most
statisticians under duress will be
willing to say I don't know or no and
this is really partly our problem as
consumers of statistical inference like
we've trained people to not say I don't
know so you know a key property of the
design of bql is that at many specific
places the machine has to be able to say
I'm unable to give you an answer to some
confidence level that you requested and
that means we have to get used to
designing processes that are robust to
stuff either being filled in or not and
behaving differently as you tune
confidence up or down okay so you can
think of infer as the interface to
predictive modeling as it's typically
thought of another branch or aspect of
statistical inference is parameter
estimation so estimate here this
estimate query can be thought of as a
linguistic interface to parameter
estimation where the parameter we're
estimating is the well it's what we're
doing two things one we're estimating a
parameter which is whether or not two
variables depend on one another so
whether they're mutual information is
nonzero and we're quantifying our
uncertainty about that so we're saying
what's the probability that there's a
dependence so that's what this query
does
and then simulate gives us an interface
to a type of operation that hasn't made
it out into the general kind of applied
statistics vernacular which is you know
generating hypotheticals or what-if
solutions or you know for people who
know Bayesian statistics this is the
core operation and predictive checking
where the idea is you have somebody
claims they have some models how could
you possibly check whether those models
are any good without understanding the
math the modeling methodology how much
you trust them the provenance of the
data etc well one recipe from Bayesian
statistics says I don't care how you
came up with the model let me look at
what it produces in conditions which
correspond to cases where I know the
data and that's something you can do
kind of heuristic alee so simulate tries
to make it possible for you to kind of
interrogate whatever models are in there
without knowing what they are by
producing data that you can check and
look at on a case-by-case basis like if
you're an expert on satellites or on
patients you can kind of look at those
roads and go okay this one doesn't make
sense this one doesn't make sense
etcetera etcetera okay so this is kind
of the way that bql a particular type of
problems to programming language tries
to formalize Bayesian data analysis
assuming the models are given it takes
these three types of operation
predictive modeling parameter estimation
and predictive checking turns each one
into a keyword and then does a little
bit of compositional stuff from the
sequel world
okay so what's Bayes DB Bayes DB starts
with bql but it also has m ml which is
then which is a meta modeling language
so what does that mean well one thing
we're used to doing if we do statistical
inference is specifying models but if
we're trying to draw inference from data
then like the key fact we need to
consider is that the models not
with perfect certainty so we don't want
a language just for specifying models we
want a language for specifying possible
spaces of models so the system can then
quantify its uncertainty about those
given the data and then have that
reflected in bql queries all right look
if we just had a modeling language where
you could name individual models then
the outputs of Bayes DB would be way too
certain most of the time so the meta
modeling language lets you specify these
high-level tactics which Richard showed
you some examples of for those of you
who saw his talk I think of it as being
based on two core ideas the first is a
very general technique from
nonparametric Bayes for building
baseline models and quantifying
uncertainty about them and I'll talk a
little bit about that here so the
jargony term for people who are in that
world is it's a semi parametric
factorial mixture and the second key
idea and the modeling in the meta
modeling language is a foreign interface
and I want to spend a minute emphasizing
that here so you know I grew up in
programming and I think it's it's
telling that the culture of machine
learning probability and statistics
still basically doesn't have foreign
interfaces so when people think about
designing programming formalisms for
that field they think about kind of
hermetically sealed worlds and they
don't think about sort of the escape
hatches that are needed to make it
interoperate with incompatible views of
the world or even partially compatible
views of the world I mentioned that
because I think there's a lot of
opportunity to to make incremental
improvements on that kind of
encapsulation and this community can
potentially help a lot with that that's
something we focused a lot on at the pro
ballistic computing project at MIT and
actually I'd say the stand guys are
starting to think a little bit about
that as well but that's not common in in
probabilistic programming okay so what
are the capabilities that you can kind
of put together given the basic query
language well the first one is that you
can take new data and you can get a
quick estimate of what its predictive
value is
so you know that's because I upload a
CSV and then I just ask okay what are
the what's this map of the dependence
probabilities so in our project with the
Gates Foundation on malnutrition so far
this seems to be the most meaningful
application of the current prototype
because the program officer much to my
terror as a PI had Bayes DB installed on
his laptop and the reason he had it
installed is because people keep
pitching him saying hey we fund me to
gather more data and he wants to be able
make a decision about whether or not to
do that and he doesn't wanna have to
believe the English that the people
selling him the data right about their
data he wants to be able to look beyond
the the metadata and just something
about the implications of the data you
know what's the actual let's say I had
10x more data like this what would I
probably be able to conclude you know
what are the variables that this stuff
is informative of right so that's a kind
of task where if you had you know an
army of top-notch statisticians and
patients right you could imagine doing
that for every new CSV file that you got
or every you know database connection
you had access to but that's not
practical in most cases whereas if you
have a system like this he you know you
can pose questions about what are the
implications you care about in bql and
you can get model baseline models using
the meta modeling language and it gives
you a way of getting started and then
focusing your effort on only the cases
where there is residual uncertainty or
very high value okay the next capability
is machine assisted data quality control
and so that includes things like
redundancies and irrelevant variables
and outliers and also anomalies so I'm
not really not going to go through the
satellite's example in detail but in
Richards talk most of you've seen but
not all certainly on the website you can
read an example where we use bql to find
multi variate anomalies in the
satellites database you can do things
like you simulate to generate proxy data
that you can share in situations where
the original data might be sensitive now
this is a big area and there's a lot of
very fundamental both theoretical and
practical questions to be asked here
right like nobody's done the
differential
privacy analysis for this this this this
this family of approaches but kind of
intuitively you can get the sense that
you know if I have a database actually
this is an example that came up in our
DARPA project there's a telco company
that wanted to have outsourced data
scientists help them with marketing but
they didn't want to share their customer
data so they wanted to know two
questions one what variables can we
possibly export that might be
informative about the endpoints we care
about but aren't informative about the
sensitive variables and second can we
generate proxy data for just those
variables that preserves the statistical
characteristics so someone else can
build a predictive model but that
doesn't reveal any actual field from the
database so there's a lot of quality
control that has to be done to even
begin to think about that that sort of
problem but I think it's interesting
that at least we can spec it we can we
can capture the spec very clearly in bql
and give a zeroth-order implementation
in in FML there's combining existing
models with automatically built models
machine assisted model checking which I
referred to earlier and something that
I'm very excited about which is the
ability to do ad hoc inferential queries
so maybe the best way I can put that is
this I remember in high school
getting into the kinds of you know
impassioned arguments that teenagers get
into about you know the world and having
this like niggling voice telling me you
know probably like the statistic
abstract the statistical abstract of the
US has some relevant information for
this thing I'm but I'm arguing about but
I have no idea and I've no idea how to
apply it so I think the most interesting
possibility for kind of the
human-machine interaction around around
Bayes DB is to start letting people
interactively ask empirical questions
and get answers that they can kind of
intuitively digest and do that in an ad
hoc fashion as opposed to being in a
setting where to do that first you have
to like spec a statistics team to go do
something and my hope is that they'll be
types of questions we're all trying to
answer that we can get good
sirs to you now that we couldn't before
because we can iterate a lot faster
because the machine is faster than the
than the statistics team okay so I think
yeah I think what I'll do is I'll just
say a little bit about the model
building engine and then we'll go on to
picture and I and I won't go through
satellites in detail so so to do all of
this it's may be easy to imagine trying
to do this using textbook statistics
right like maybe we're gonna fit linear
regressions and logistic regressions to
implore generalized linear models to
implement infer or you know maybe we're
gonna use correlations to detect
predictive relationships and I think
it's an important feature of BQ Elle's
design that you could in principle try
to implement a ballot oh we haven't yet
because I don't think it would work very
well but it's actually an important
intellectual exercise because we haven't
currently quantified how badly that fits
right but to get a qualitative sense of
where that fails you can look at these
data sets and these are on Wikipedia and
I love them because you know they all
have zero correlation between x and y
even though there's evidently
information about between x and y
actually in all but one case and so you
know on the bottom here what I'm showing
is the output of simulate if you build a
model for each of these tables
separately using the default baseline
model builder and then your unstimulated
to generate a synthetic data set and I
think it's important to take a look at
this for a minute and to get a sense of
like qualitatively what does the
automatic model are doing right and and
what is it doing wrong right so like
clearly it's lumpier somehow on the
bottom then it done it than it is on the
top right so underneath the
semi-parametric factorial mixture is a
kind of variable resolution
discretization of the space and what
you're seeing there are the artifacts
that come from that seeping through
given the finite amount of computation
finite number of models you built and
finite amount
initial data so what that means is this
gets worse the more deterministic the
truth gets right so like a sine wave or
like a linear function with no noise is
the worst case and what worst means is
in those cases this thing will blow up
into some very big lookup table that
overestimates the noise but in the messy
real world cases where you have scatter
plots that have funny shapes when you
look at them you're actually in a very
different regime from the standpoint of
understanding modeling quality and I
mentioned that here because another
major area where this field needs to now
draw start drawing on ideas about
testing and quality from engineering
after appropriately twisting them around
to handle the problem stick setting is
how do we quality control this stuff how
can we come up with automated testing
frameworks and Diagnostics that let you
you know figure out of all the variables
which ones do you simulate and validate
into what tolerance that you have
confidence and some other queries that
you've run this is not a question that
we've looked at seriously yet okay so
let me give you a brief metaphor for how
to think about what it is that the
automatic that the baseline modelers is
doing so let's say you had a 3 column 4
row data table so each of those panels
on the top is showing a different way of
breaking up that data table into data
subsets where all the cells that are the
same color are going to be modeled by a
single parameter like intuitively made
we're going to say everything with a
single color is irrelevant there's some
mutual information between those cells
so one possibility is that every
variable has a simple parametric
explanation or a single model so all the
data is relevant you know mutually
relevant that's the one on the on the
far left the far right is the situation
where every cell is idiosyncratic noise
generated by some demonic adversary and
then it's probably best not to think
about where healthcare data lies
on this spectrum so when you think about
what a statistician is doing when you
ask them to do a data analysis a key
portion is qualitatively exploring a
vanishingly small subset of these
possibilities guided by their own
intuition and they're at their expert
judgment and what the default model
builder does is it attempts to emulate
that process by running a Markov chain
where each step each transition and the
Markov chain can be thought of as this
like noisy statistical test about you
know as Richard said which data fits
with which other bits of data and it's
rigged so that if you just keep
iterating it eventually that process
converges to a probability distribution
over that massive space so we're not
trying to find a single best model we're
just trying to explore possibility as
narrowing down on the probable ones
using a set of rules which tell us that
certain key invariants of reasoning are
respected like the more you compute in
probability the better you get but of
course it's not the case that your
conclusions might not get better or
worse locally if you compute for an hour
more right look we can stare at a
question I guess I have an idea and then
we stare at it more now we're confused
right you know so like those sorts of
dynamics will show up but then if we
have enough models and we've done enough
computation to feel confident in our
answers you know then maybe we can draw
inferences at some again relative level
of confidence so crucially for Bayes DB
it's not the case that you're never done
doing analysis right it's always
possible that if you ran analysis for
ten more years it might resolve some
predictive relationship which looked
like noise you know with a faster
analysis and that's really important
because you know if you think about the
situation where you might want to
integrate so let me let me unpack that a
little bit so here I'm showing a plot
which again for those of you who were
here in Richards talk he showed but this
is from an for those who of you who
weren't this is from an analysis of
satellites so table where each row is a
satellite the columns are attributes of
satellites
and here I'm showing a pudgy on the
x-axis and period in minutes on the
y-axis and the blue dots are actual
satellites the dashed purple curves are
the theoretical lines that correspond to
Kepler's laws at varying eccentricities
the red dots are the outputs of simulate
and the green dots are outputs of a
probabilistic model wrapped around
Kepler's laws okay and the reason why I
think it's sort of important to sort of
stare at this stare at the slide for a
little bit is that I think one of the
undercurrents the cultural undercurrents
in this whole big data world is this
idea that like data is everything and
theories and models are kind of things
to be suspicious of and I feel like this
plot helps me understand where that's
useful and where the limitations are
because neither the green nor the red
dots are correct in all cases so let me
unpack that you know the red dots do
cluster around the blue except for that
kind of patch down in between the purple
line and the legend right but there is
some spread of red dots so you know the
probability density of those weird way
out there blue dots isn't like zero but
if you believe Kepler's laws it is zero
right and under the green you know it's
very very very low so what you're seeing
there is that there are some satellites
where there are errors in in in data
entry or satellites with engines and
Kepler's laws can't know about either of
those okay but if you look if you zoom
in to any of the regions where there are
blue dots locally Kepler's laws are a
lot more accurate if you're in a regime
where they apply and that's the one way
I like to think about that is you know
Bayes DB was doing statistics for five
minutes and
blur was you know resting on centuries
of you know the the ants you know the
analytic geometry roughly so it's not
the case that Bayes DB would necessarily
ever get to a model in the current meta
modeling language doesn't have Kepler's
laws in its hypothesis space right so
it's not the case that if you let Bayes
DB crank for a millennium or something
you know it would pop out Kepler's laws
but at least conceptually we can now
study the programming design problem of
incrementally increasing the
expressiveness of the space of models
that the automatic model builder can
consider and you know the human machine
interaction of model quality and try to
try to see you know how far can we get
and where should we have humans build
models based on deep domain expertise
and and detailed theories and where
should we be empirical okay so that's
Bayes DB and I'll make one comment about
in response actually to a question from
Richards talk about a Y sequel I think
this is very important for this audience
we've really struggled with the
questions of what affordances to create
with probable stick programming
languages and what false affordances to
suppress so we don't know what the right
answers are there and so I think you
know you're gonna see actually in the
other two languages that I present
syntax is and kind of like a kind of
surface structure that I hope offends
all of your tastes because it offends
mine but it's chosen intentionally as a
research tactic so you know in the case
of Bayes DB we're trying to explore the
question of can we come up with a
notation that qualitative researchers
can feel that they roughly understand so
that there's a type of qualitative
programming they can do where they give
Bayes DB qualitative instructions and
they get out a quantitative model that
they can ask and answer questions and
there'll be an analogous kind of choices
that we're making for the other
languages but for people in this room
I'd really love to hear suggestions
about other odd
that would want similar capabilities
packaged in a fundamentally different
way okay so let's spend a little bit of
time popping up a little bit and looking
at probable stick programming overall so
what is probabilistic programming is a
field
well one view of it is nobody knows
there have been no overview papers
published on the field yet there have
been three workshops at a couple of
machine learning conferences and a
couple at PR conferences and I think the
you know but a colleague of mine got the
first book contract for an edited volume
in the area about a month ago right so
we should expect anything I tell you
about this you know is is certainly I
don't have that much confidence and it
you know in a certain level or certainly
it's not that objective it's it's at
best quite personal but my view having
been involved in the field since before
we had the term is that there's three
thrusts of work in probabilistic
programming the first is maybe the one
that that has gotten the most has
carried the farthest which is the idea
that we can use ideas from programming
languages to specify models and then we
can build systems that automate or
simplify aspects of inference and Stan
is like the prototypical example of this
it takes a class of model I think of
Stan is actually a great analog of the
early like pre Fortran languages for
computational fluid dynamics all right
you know it's like there's a domain
class of problems that's really
important and you need a fast engine for
solving them and it's pretty declarative
already and you need a language and then
you need some sort of like a solver or
inference engine there are there are
other examples most of what I'm focusing
on today is actually the second aspect
of prolific programming languages or
promising programming which is the use
of complex computation to describe
models and queries so the contrast is
most probability is designed to be stuff
you can do on pencil and paper quite
declaratively but what if we were taking
computation seriously in this in this
arena so you could specify much more
complex objects
so Bayes DB is the example of taking
kind of systems ideas from databases and
programming ideas and integrating them
with probability picture is doing the
same thing for computer graphics and
then the last aspect of probabilistic
programming which is the one that I
think is the hardest it's the least well
understood and the hardest to digest but
ultimately the most important is that
the core of sort of how probability in
computer science intersect so the
question is what can we can we use the
core concepts from theoretical computer
science and programming to give us
better formal tools for representing and
manipulating the objects in probability
so the venture platform in the venture
script language is an attempt to do that
directly and some of the unusual
characteristics of the languages I'm
talking about today relative to other
problems tick programming systems you
might see if you Google on the internet
basically flow from that bottom that
that bottom bullet point where we think
there's a fundamental relationship
between ideas and turing universal
computation and probability and
information theory and then that leads
to a whole bunch of other stuff when you
take it seriously and integrate it with
system software and stuff like that ok
so what I'm showing here is kind of a
design map for most machine learning
software so on the right you have some
executable that you run and certainly
you know you get that or the process
that's running from writing some source
code in a language that looks familiar
but the computations that you're
specifying are these funny estimation
algorithms you can think of random
forest as a classic example and then
there's a whole bunch of work in these
boxes on the left here which are really
all done in math often they're done only
in the head of the designer sometimes
they're done explicitly on a whiteboard
where some were there these theoretical
constructs like probabilistic models and
inference algorithms that are invoked in
this human discourse
that ultimately cashes out in estimation
algorithms and then code that that runs
but almost all of the intellectual
action in designing systems that use
probability is in these math boxes and
the dashed arrows and what shows up in
the code it's kind of like only the very
end of that that process so in response
to that well actually let me give a
little more texture so what you could
think about for modeling assumptions
what you see there are like probability
distributions I can use a Gaussian model
you know or maybe in the case of Bayes
DB some fancy semi-parametric mixture
for inference strategies you'll see
terms like Gibbs sampling or
message-passing which are often called
algorithms but are really sort of
templates or problem-solving strategies
that have not yet been formalized except
in specific specializations estimation
algorithms are things like expectations
of functions or optimization so you know
one example might be you know you you
might take a random forests approach
which you implement in MATLAB to solving
a classification problem another example
might be you might say you're gonna
solve classification by building a model
that's logistic regression with Gaussian
priors on the weights and your
estimation is going to be posterior mean
by a simple Monte Carlo and you're gonna
use approximate samples from hamiltonian
monte carlo to generate it so this is
like what stan is sort of trying to lead
you to do for people who've seen that
and then the inference strategy is this
hamiltonian monte carlo thing which
isn't a piece of code it's not an
algorithm it's sort of this general
strategy that's used to go from the
model and the data to an estimation
algorithm okay so probablistic
programming is attempting to sort of
understand one sort of the the on ramp
to probe elastic programming is the
corpus of examples of stuff on the top
here we're trying to sort of formalize
them and
and andraia fie all of them in code or
at least all the ones that we think need
to be and I think this is very pressing
because right now the situation is that
all of the hard intellectual work that
goes into specifying and performing
these computations isn't captured
anywhere and when isn't isn't formally
captured anywhere and when it is
captured is is done so in a notation
that has well-known limitations so the
the most popular approach in
probablistic programming is to say let's
simplify the picture let's only focus on
model-based computations so you might
have a language for density functions or
these things called generative processes
and the goal is to get an executable and
and we're gonna treat this as a
declarative programming problem the
modeling assumptions or this declarative
thing we're gonna ignore the inference
strategy and the estimation or use
canned answers to those and hope that
there's like a sufficiently smart
compiler or runtime system that's gonna
solve the declarative problem
automatically and you know produce
something that we can run and that's one
approach and I think in some domains
it's likely to be successful Bayes DB
takes a slightly different approach
so in Bayes DB you specify data analysis
workflows and ad hoc queries which are
both declarative and procedural so to
the person who said probability
distributions aren't a great fit with
the declarative aspect of sequel I
completely agree and also asterisk
there's a very interesting set of
intellectual issues there but crucially
the query workflows article are both
declarative and procedural then they're
these models which are used to answer
them to produce the actual stuff that
runs and then you get those models by
specifying modeling tactics right so
this is sort of a different cut on
formalizing some of the work that you
might do on the top using ideas from
programming
okay so now let me give you another
example from a totally different domain
so computer graphics is one of the
really interesting sort of applied
success stories of computing we're now
very used to the idea that we can go
from data structures that are described
by programs like scene descriptions on
the Left this is a bunch of povery code
I think two images on the right and the
way we do that is by you know feeding
these things into software that
approximates solutions to the luminance
equation on a pixel-by-pixel basis you
know which tells us what color the pixel
should have and in the same way that a
modern physics simulation running at a
National Laboratory is inspired by
Newton's laws but is really quite a
different beast right a modern graphics
engine is inspired by the luminance
equation but incorporates a lot of other
heuristics and knowledge to make it
feasible to apply in a complex way so
given all of this knowledge it's natural
to ask could we do computer vision by
somehow inverting this process right
like we know a lot about how to
synthesize images so to somehow surely
that that should be relevant for how we
interpret images and in fact this is a
very old proposal you know which has
been it's a heart of a lot of research
and Hume the computational study of
human vision and machine vision it
actually goes back to Helmholtz who said
that the process of visual perception is
unconscious inference is a process of
unconscious inference about the causes
of the light that hits our eyes but
unfortunately despite its kind of
elegance
nobody really solves vision problems in
practice this way the gold standard is
to use statistical pipelines which are
constructed to model the end-to-end
inverse function and they're built using
tons and tons of training data so here
I'm showing a schematic of a
convolutional neural net which is one
approach that's been successful on
several benchmarks over the last five or
ten years
so it's been generating a lot of useful
activity there have been some attempts
to take this graphics idea seriously I'm
not going to dwell on them what I am
going to talk to you about is how we can
use probabilistic programming to take
the idea literally and see how far that
that gets us so a few years ago we used
we used venture or a general-purpose
probablistic programming language to
write probabilistic programs that are
described by the following template we
have some scene generator which produces
scenes and then an approximate renderer
which renders those and then this like
stochastic comparator that kind of looks
at the rendering and squints and
compares that to the picture and then an
inference process that's trying to
explore how must the renderer have run
given that the thing that squinted was
sort of relatively satisfied about the
match with the real input image and
there's some math for that that's not
important
but sort of the problems we could solve
with that generation of technology with
a were you know this is a typical
example so you know can we infer a 3d
model of a roadway from images from a
vehicle mounted camera on a
frame-by-frame basis so no temporal
continuity and impose that over the over
the roadway and we did okay
but this is a very very simple problem
in the sense that the dimensionality of
the hypothesis space is fairly low now
one interesting property we got from
taking a proble stick programming
approach is that we were able to get
measures of uncertainty so I'm showing
you this so you can get a sense of what
the analog of the confidence notion
looks like for computer vision so on the
left here there's a frame where the
assumptions of the model namely that
there's one roadway are violated and on
the bottom I'm showing you know an image
where how green it is is how much
evidence it thinks there is for a route
for a lane at the given pixel on the
right there's a frame where the
assumptions are satisfied and so one
thing you see from this simple
probabilistic program is that on the
Left is pretty broad uncertainty about
the interpretation
whereas on the right there is some older
uncertainty but it mostly falls into two
modes and actually if you look at the
second mode there's a visual artifact
along the center of the roadway that we
think is probably from where that that
is where that second marker aligns so
this is interesting because by taking
but basically using machine assistance
to deal with all the messy probability
we're able to do a more complete
probablistic formulation of this problem
than people would have done in computer
vision typically by hand and I produced
something that has some utility for the
robotic setting for example where you
really want to know if your lane finder
is confused you know that and that's a
that's an output of intrinsic interest
from the map of probabilistic
programming languages here's what
picture looks like so there's a picture
model which is specified in picture
which at this stage is currently an
embedded language and julia where you
specify a 3d scene generator and a
renderer and some stochastic constraints
I'll give you examples and then that
gets fed into some machinery which adds
on generic inference strategies that's
what's this mhm and slice stuff is and
bottom-up proposals so these are like
fast approximate recognizers that try to
you know do as much of the inference
problem as possible in one in one swoop
and then to that that that's ultimately
run and Julia and it's written so that
you can extend the inference engine by
writing inference code in Julia that
interoperates with the general-purpose
set of tactics there's a good place a
good way to think about it for people
who are familiar with this and an
interactive theorem provers is that the
notion of a tactics language has a
really fundamental role to play in
probabilistic programming okay so I'm
not going to spend too much time on the
architecture diagram for picture I'll
just show you some examples so here's a
problem where what we'd like to do is
infer 3d models of faces so we have a
scene model for faces which is described
by that graph in the bottom left it's
sort of a general statistical model that
has some geometry for different face
parts and simple motions of texture
and if you were to generate meshes from
that and render it you'd get the faces
on the right so it can generate things
with varying rotation and lighting and
so on and what we're gonna do is we're
gonna use a stochastic comparator that
compares the real world to the rendering
through the kind of blurry goggles of a
neural network and we're doing that
because we don't know how to model the
details of appearance so we want
something the place where we're saying
we have to appeal to machine learning is
in the appearance model rather the
inverse appearance model where we don't
know how to how to deal with it so we're
just gonna use the off-the-shelf neural
net but for the rest of it we're saying
no we know what faces are there there
are things you can generate in graphics
software and so here's an example of
that inference process running so on the
left is the input image and on the right
are the sequence of reconstructed
renderings that you get as the inference
algorithm explores the space of scenes
so it's going from the image on the left
to a 3d model and then rendering it
you're seeing that on the right so I'll
show that again so you can see it
initially starts with a face that looks
nothing like the target image there's a
lot of big changes in the beginning
that's when these bottom-up recognizers
are firing and then it's like fine-tuned
over the course of the process and for
people who are familiar with mental
rotation and some of the results in
psychology on face perception we're
actually just beginning to test out
picture as the basis for modeling of the
psychophysics of human face perception
so you know and for people who are
interested in that stuff I would love to
talk about it offline okay so here's the
picture code for the face model there's
this julia code that's calling into some
libraries and and the random choices are
in red I'm not going to spend too much
time on this you can check out the paper
but but the core pointed you to run this
program over and over what you get is a
mesh a rendering and then a comparison
between that rendering and some data and
then this is the code for the inference
program
we're what we're doing is we're first
constructing bottom-up proposals so
basically running the renderer forward a
hundred thousand times and then like
just using statistics to try to learn a
map backwards and then loading up the
real image
initializing with that and then doing a
bunch of fine-tuning using a proble
stick inference algorithm and this lets
you solve applied tasks that that look
like this so you know maybe we can go
from an input image on the left to a 3d
model but then we can rerender it and
novel poses or in different lighting so
we can this is like simulate except in
the context of computer vision right
where the variables were conditioning on
our lighting and rotation and you know
if you like you could think about
instead of Kepler's laws we're using a
graphics engine and a neural net right
and we've done a bunch of other examples
like human body pose estimation yeah
that's that's Hussein bolts and this is
an interesting one where we're starting
to explore open categories with out of
open shape classes so like objects that
are radially symmetric where our scene
generator uses lathe the lathe primitive
from computer graphics plus a
statistical model of the curve that's
being lathe so for picture we're just at
the beginning of launching on the effort
to build a serious open-source version
of this so we have this prototype that
sort of showed the computer vision
community this might be possible and we
now have some interest and and some
support actually from a couple of
companies to do it but that's a big
project and we're just at the very the
very early days of it and hopefully we
can improve the language by then um so
please help us out ok so let me spend
the last five minutes talking about
venture script
okay so I've shown an example of dsls
for modeling for data analysis Richard
mentioned the meta modeling language
I've shown a language for computer
vision we can ask what would
general-purpose probablistic programming
look like the deepest answers we don't
know but we have been building some
languages whose design intent is to
maximize our expressive possibilities
and our ability to incremental optimize
after writing programs rather than kind
of the time to productivity on some
narrow problem domain and those
languages are also built with the Lisp
aesthetic under the scenes so they're
meant to be reprogrammable and I'm happy
to talk about that offline inventor
script the key idea is that you specify
models and a whole bunch of other
declarative information and there's a
language for inference tactics and
estimation that's very rich that is in
fact the same language so the language
for models the language for declarative
statements about models and the language
for inference tactics in those models
are all the same language and it's all
probabilistic and I'll just try to give
you a flavor of one one thing that I
think starts to clarify when you have
that sort of a formalism so here's a
schematic problem where the shows up
actually in training large neural nets
where there's an expensive experiment
you can perform maybe you know train a
neural net on some subset of YouTube
with a bunch of parameters for like how
the training is going to go and you can
measure how well that neural net then
does on some tasks you're going to put
it to and your promise how do you adjust
those those knobs you know maybe there's
50 knobs or something like that
so on the top what I'm showing is a
schematic version of that problem where
it's actually unfortunately hard to see
on the slide but there's a blue curve
that looks like this okay that's like
the true function and the red samples
are from a model of the true function so
the way that people solve this with
neural nets is this technique called
Bayesian optimization where what they
say is we're going to build a model of
the function we're trying to optimize
and then trade off between running the
real function to get information
tightening our model to use that
information optimising in our model to
then figure out where we should look and
optimizing either to do what's shown on
the on the right which is like okay find
a point that's probably high value so
that green circle is in a place where
the red lines mostly agree right which
means it's certain that the value is
sort of relatively high but on the and
on the left the green is being put right
in the center we're on the top you can
see there's a lot of fuzz right so
that's a case where it's saying all
right I'm gonna I'm gonna choose to
gather more data by placing a probe
point in a part of the function that's
very uncertain so the right is like
saying well based on my experiments so
far I think I'll probably get something
if I use the following parameter
settings and the left is like saying
well based on everything I know so far I
have no idea what those parameters will
do so so let me try those and the the
sequential decision problem of executing
those trade-offs and building the models
and during the generalization involves a
bunch of math and here I'm showing about
a third of the math and the math is very
very messy because it's modeling a pron
approximation upon app optimization and
then approximate optimization and
there's you know models of models and
inference and models of models going on
so it's like very kind of messy but I
remember being very influenced by sort
of the success of Lisp in capturing some
of the the sort of quite elegant
modularity that's at the heart of early
AI search procedures so on the top I'm
showing a figure from Peter Norvig
book on AI programming where he gives
this template for tree search or the
idea is that by plugging in different
ways of like checking whether you're
done or expanding successors or whatever
you can span a very broad class of tree
search processes and they're all
understood as specializations of a
single template where you just pop in
higher-order procedures so maybe you
could do depth-first search or
breadth-first or best-first just by
plugging in little little functions that
are different so we can do the same
thing for Bayesian optimization so
been Zinberg master's thesis master
student who just graduated from the
group recently did this work where he
made a higher order probabilistic
procedure called optimize and I'm not
going to go into all the details but
basically you specify things that are
the analog of the things in research but
for that optimization problem like how
are you going to update parameters of
your model of the function you're
optimizing what model class are you
gonna use how are you going to optimize
the function once you know it how are
you gonna decide you're done optimizing
etc etc and by varying those you can
span a really broad class of algorithms
from reinforcement learning and other
areas of AI and applied AI that at first
blush look like piles of math that have
related Greek symbols but sort of hard
to understand I mean and experts would
all understand that there's this
modularity but it hasn't been formalized
in a form that the machine can
understand or that that novices can so
I'll just close by showing you a little
animation of that happening so here is a
function in blue the red samples are the
model of the function so we've seen no
data so the models maximally uncertain
everywhere and the green circle is the
point we're gonna probe so once we probe
that point we now have a black dot there
you can do this a few times and you can
see that as the probes are made our
uncertainty about the function shrinks
but what that optimization process does
I play it again is you'll notice that
the probes either tend to fall near
points that are known to be good or at
points where we have no idea and sort of
that's that kind of trade-off being
executed and it takes about a page of
code Inventure script to write that
program okay so let me wrap up
so what I've tried to argue kind of
tactically is that it's possible to
build useful probablistic dsls for
problem areas that I think are
interesting and substantive and that
then invite further research questions
in that domain like Bayes DB for data
analysis and picture for computer vision
and I we're really looking for help from
this community for both of those efforts
but the thing I really want to leave you
with is the following sense which is I
think it's gonna be tempting in the next
few years to take a really tactical
problem driven view of what
probabilistic programming is because
they're people trying to do data science
and machine learning and that's a useful
set of tools and people want to be more
productive and that's a great thing to
do but let's not forget that one of the
real powers of programming languages is
they let us think thoughts we were
unable to think before we had access to
the linguistic technology and so the the
approach we're trying to take at the
problems to computing project is design
languages that expand our range of
problems that are sensible to pose and I
think that's a really important activity
because we're at just the beginning of a
serious integration of computation and
probability and it would be a shame to
kind of narrow our vision before we got
started and we'll need your help for
that as well so thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>