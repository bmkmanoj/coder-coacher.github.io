<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;GPGPU Programming with Metal&quot; by Janie Clayton | Coder Coacher - Coaching Coders</title><meta content="&quot;GPGPU Programming with Metal&quot; by Janie Clayton - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;GPGPU Programming with Metal&quot; by Janie Clayton</b></h2><h5 class="post__date">2017-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kc4qG42z7GA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everybody welcome to my talk this
afternoon I'm gonna be talking about
general-purpose GPU programming in metal
my name is Jani Clayton and my twitter
handle is red-green coder so just
there's a little bit of background about
myself I took the last year off from my
job career etc to write a book and I
wrote the book the metal programming
guide metal was a framework that Apple
introduced in 2014 for graphics and
general purpose GPU programming which is
the topic that I'm gonna be speaking
about today prior to writing that book I
was the co-author on several iOS and
Swift programming books I'm currently
the co-host of the array winter-like
podcast again my twitter handle is Red
Queen coder and I also have a technical
and personal blog at Red Queen coder
comm so kind of the purpose of this talk
is to explain what general purpose GPU
programming is what you can get out of
it why you should care about it and kind
of how to implement it so first off I
want to kind of jump jump in the Wayback
Machine I want to kind of give you a
brief history of GPUs GPU programming
frameworks kind of how we got to this
point in time and just the general
history of what GPUs are and why we why
we care about them and why we have them
in our devices so way back in the stone
Age's you know back before in 1996
computers didn't really have a lot of
specialized chips for graphics there
were a couple of our cade machines that
you had things like the PlayStation I
believe to was one of the first consoles
that had a GPU in it but for a really
long time the only chip that you had in
computers was a central processing unit
and back when GPUs were first introduced
they weren't really that powerful and
there wasn't really a lot that they
could do so if you were integrating them
into your flow when you were creating a
video game or creating something as very
graphics intensive you couldn't really
send a lot of work to it and that kind
of informed how different frameworks at
the time really interacted with and
dealt with the graphics processing unit
and in addition to that every single
chip manufacturer kind of had their own
driver and they had their own way of
doing things so it
you wanted to create a game that was
going to be able to work on multiple
different devices you had to adopt a lot
of different standards in order for it
to work on all of these different
devices and that changed around 1997
John Carmack is a well-known graphics
and game programmer who worked at id
Software at id Software was the creator
of quake and Doom and when quake came
out John Carmack went in and ported all
of the graphics over to the GPU because
you could get a lot more performance out
of the GPU and it gave a much better
experience of the game while he was
going in there at the time there were
two different standards there was OpenGL
and SPI and he went and converted
everything to OpenGL first and he really
liked working with it he thought it was
really intuitive he got a lot of work
done he got a little really great
performance out of OpenGL and he got to
the point where he had to port
everything over to SPI and he was just
like okay this is garbage this is
horrible I hate working with this this
is a pain in the neck I'm just gonna
make a unilateral decision that anybody
who wants to have quake on their
platform is going to have to adopt
OpenGL and so since quake was a fairly
large and popular game all of the
different platforms really wanted to be
able to use quake and so they all
adopted conformance with OpenGL so since
all of these platforms already had
support for OpenGL it kind of became a
de facto standard for the last 25 years
that if you wanted to do something in
graphics you can do things in OpenGL and
it would you can write once and read
everywhere so OpenGL is an API that was
introduced in 1992 and it again like it
really became prominent in 1997 when
John Carmack adopted it for quake but in
1999 Nvidia released its GeForce 256 GPU
and that kind of popularized the term
graphics processing unit and that was
one of the first real performant GPUs
that started to become available in a
lot of different computers when it was
originally introduced it had a fixed
function pipeline which meant that there
was a bunch of state that you had to set
on your GPU you couldn't really get a
lot of specialized
like graphics effects from this pipeline
because everything was hard-coded and in
2004 after GPUs became a lot more
powerful they added the ability to do a
programmable pipeline where you can go
in and you can create specialized
lighting functions and specialized
mapping functions and there were things
that you could do to create really
awesome cool effects that weren't really
possible before 2004 and around that
same time in 2003 OpenGL ES which stands
for embedded systems was introduced
which is basically what you use if
you're going to be doing anything any
mobile development on like an iPhone and
Android I believe OpenGL ES what is used
in consoles so like this has kind of
been the paradigm that we've had for the
last 20 years of doing graphics
programming is that everybody does it in
OpenGL but over the last five years as
GPUs have become a lot more performance
people in the research community become
very interested in them we kind we came
out of an AI winter back in 2012 with
the creation of convolutional neural
networks and Richard scientists figured
out that the types of work that a GPU
could perform would be ideal for going
in and training neural networks and they
were cheaper than going and building
dedicated like server farms and so they
became like a cheap way that research
scientists had of going in and like
training neural networks and it became a
really big thing in the last couple of
years to be able to program a GPU to do
things other than just graphics and
because OpenGL came out like 25 years
ago it wasn't really created with the
intention that you would be able to do
anything with it other than graphics so
the entire pipeline of OpenGL is
basically just streamlined for graphics
up until 2014 you couldn't do any
general-purpose GPU programming on the
iPhone because there wasn't a framework
that enabled you to do that so since
we're now getting to the point where
GPUs are being used for things other
than graphics there been a couple of new
next-generation frameworks and the two
biggest ones are metal and Vulcan and
then another one is DirectX 12
Volken is a is was created by the
Khronos group which are the people that
own opengl and i believe they've just
recently added some general-purpose GPU
programming functionality to it but for
the most part these are these are
considered to be graphics programming
frameworks and the the general-purpose
stuff is just kind of like an add-on
that's been created in the last couple
of years and it hasn't really been a
focus of a lot of attention from people
who are really interested in GPU
programming until recently so one reason
i'm here to talk to you about this is
because there's been a lot of progress
made in the last couple of years on this
topic and i feel like a lot of people
aren't really aware of the fact that
this stuff exists so i've been talking a
lot about you know GPUs and graphics
program graphic programming every
programming units a lot so at this point
you're probably wondering like you know
what is a GPU why are you telling me
about this why is this special and why
should I care and return answer that
need to explain a little bit about what
actually constitutes GPU so when you
have like a GPU in its CPU all of these
are just they're computer chips and
they're covered with different types of
circuits and one specific type of
circuit that exists for these chips is
called an algorithm logic unit an
algorithm logic unit is electric circuit
that performs mathematical operations on
binary numbers now CPUs have some
algorithm logic units but a cpus of a
generalist it has to do a lot of
different types of things it needs to
cache data it needs to be able to do
conditional logic and it has to be able
to perform a lot of different stuff so
what a GPU is a GPU is a very
specialized chip that has maxed out all
of its available real estate on these
algorithm logic units and algorithm
logic units are really cool because they
allow you to do parallel processing so
like one thing it's anybody who's
working with computers knows that you
have like the multi-threading so you'll
have like 16 18 like like 36 threads
going at any given point in time but not
all of these threads can be executed at
the same time if you have a dual-core
processor you can only perform two
actions at the
time if you have a quad-core processor
you can only do four actions at the same
time one really cool advantage that you
get out of using a GPU is that each one
of these algorithm logic units counts as
its own core so if you have like 1,000
56 different algorithm logic units on a
GPU that means you can do one thousand
fifty six separate processes all at the
same time so that's kind of like you
have a difference between parallelism
and concurrency so with concurrency like
you can you can have multiple things
going on at the same time but only one
operation can be performed at any given
time with parallelism you can have
multiple things happening all at the
same time and this really adds a lot of
performance to what you're able to do
with the GPU so it's really in your
advantage to be able to move as much
work over to the GPU as you possibly can
so now at this point you're probably
wondering what is it the GPU can
actually do I've been to parallel
programming talks talking about GPU
programming where people will come up
and get really excited and say like Oh
GPU programming is totally like you know
the future of computing it's gonna
change the world and everybody's gonna
be doing this in like ten years but then
they never actually tell you what you
can do with the GPU you're like well can
I put my you know user interface stuff
on your like you know like what is it
that what does is work that I can you
know take and I can move over to the
graphics processing unit so right now
there's some common tasks that you can
do with the GPU the first one of course
is namely graphics if you look up GPU
programming generally speaking like 90%
of the information out there is going to
be about graphics because that's been
our predominant paradigm for the last 25
years was that you do graphics on GPU
and in the last five years the research
community has become very interested in
GPUs for specifically deep learning and
neural network training one thing I did
want to mention is metal is a framework
that works on the iPhone and it allows
you to actually like implement a neural
network but neural networking has two
different components to it that have to
be done separately so that the iPhone
can actually take a neural network and
run it on the phone but it can't train
the data model that goes in and actually
informs what the
what how the neural network is set up
one of the more popular training
frameworks imagenet has 180 gigabytes
worth of images which is larger than
most most of the hard drives you have on
the iPhone and you probably wouldn't
really want to go in and train a neural
network on an iPhone anyway so just just
kind of keep that in mind that there's
two different tasks that the GPU is
capable of performing and one of them
can be done on the phone and the other
one has to be done on a much larger
system and I'll be getting into more
detail on that a little bit later and
basically like anything that requires a
lot of linear algebra or vector calculus
processing which again includes graphics
and sheep machine learning and machine
vision those are things that you can
offload to the GPU so the GPU can't
really take over your user interface
stuff and can't take over your network
calls it specifically can only do things
that require a lot of mathematical
computation so now I'm going to
transition into talking a little bit
more specifically about the metal
framework from Apple
like I said metal was introduced back in
2014 I had been really interested in
seeing if Apple was going to adopt
another standard OpenCL that would allow
you to do general-purpose GPU
programming and I was really excited
when metal came out because it was the
first time that you were able to do
general-purpose GPU programming on an
iPhone and just over the summer they
added they created metal to metal to
allows you to have an external GPU that
you can attach to your computer they
said it was for virtual reality but like
I'm pretty sure you could probably use
it for this data for this neural network
training and they've just they shown a
lot of interest in this framework every
single year they keep adding more and
more functionality to it and I really do
think that it is one of the foundational
future technologies that Apple is really
putting a lot of their resources behind
so the way that Apple has set up their
GPU programming framework is there a
couple of different types of work that
they specified that you can send to the
GPU and those are rendering MIT
at being blitz and compute rendering is
just your generic like going and doing
3d graphics so you would send vertices
and different 3d graphics information
through the render encoder the MIT map
encoder is one that goes in and it like
maximizes and minimizes images and
textures without forcing any of that
work to be done on the CPU the blick
component coder is a way of taking data
that exists in one type and then
converting it to another type again
without touching the CPU and the main
one that we're going to be talking about
that all of the different apple GPU
frameworks are built on is the compute
encoder so one thing I noticed when I
talk to people about metal is that
people don't really have a visceral
understanding of what metal actually is
like people think that pedal is a 3d
graphics programming framework and
that's not really the case metal is a
framework that allows you to bundle data
and format it and send to send to the
GPU to be processed and returned and to
do something with that formatted data so
all of the different objects encoders
buffers etc that exist in the metal
framework exist to encode data in such a
way that it can be sent to the GPU and
it can be processed the data has to be
encoded into a metal resource type you
have state that is encoded into into a
render encoder or command encoder that
is then set up to a command buffer and
the command buffers in cued by all of
these different queues and they manage
the work that is scheduled and sent to
the GPU and then once that data is been
processed it's brought back over to the
CPU side and it is rendered displayed to
the screen or it's used by the CPU in
some way one of the components that you
have on the command encoder that doesn't
exist underneath the other ones is your
ability to create threads and thread
groups and a thread correlates to a
single unit of work that is being
processed by the GPU you can organize
threads into one two or three
dimensional matrices called thread
groups so like if you had a row of data
that would be a one dimensional thread
group if you
like a row a set of row and columns like
in an image that would be a
two-dimensional thread group and then if
you had multiple slices of images for
doing something like convolutional
neural networking that would be a
3-dimensional thread group and because
all of the different GPUs have different
requirements and different capabilities
every single one of them has in
different maximum thread groups so you
can't just hard code that you're going
to have like 1,000 threads for each one
of your GPUs because that might work
fine on an iPhone X but it might not
work on an iPhone 5s and there are
methods built into metal to go in and
calculate whatever the maximum thread
group is so you don't have to worry
about that you can just go and call that
method so after you take all of these
different pieces of data and you encode
them and you send them over to the GPU
the GPU is then responsible for
processing it and you write programs on
the GPU in what is called the metal
shading language or MSL and you put it
together in metal functions so it's a
little bit confusing but in metal
instead of calling everything a shader
everything is called a function they can
be kind of used interchangeably but that
can be a little confusing if you're
going through they're talking about
metal functions it is like what do you
mean by metal function so there are
three types of metal functions there's
vertex fragment and kernel if you have
any familiarity at all with opengl
you'll recognize vertex and fragment
shaders because those are part of the
rendering pipeline for doing graphics
and the kernel shader is the one that
you need to use if you're going to be
doing general-purpose GPU programming so
the metal shading language is similar to
other shading languages like the OpenGL
shading language so if you have
familiarity with GLSL or you read books
about the OpenGL shading language like I
have a number of cookbooks that talk
about how to do different texturing and
different effects in GLSL those are
almost completely directly translatable
over to the metal shading language the
only thing that changes is how you
actually like pass data from the CPU
over to the GPU metal shading language
is based on C++ 14 which can be a little
bit confusing if you didn't deal with
C++ because apparently the Apple
programmers think that everybody knows
C++ which is kind of
I don't know so it's a small language it
doesn't include any of the C's
functionality for things like strings or
any other data types it is all
completely targeted towards mathematical
operations so there's a lot of
operations and data types for things
like vertices and matrices and doing
matrix transforms and any type of work
that would use if you're doing
animations deep learning etc and again
those are all based on mathematical
principles so like if you know linear
algebra and you understand how the math
around how animations work you should be
able to look in to the shading language
and go okay I kind of understand what
this means and I mentioned before that
the way that the CPU and the GPU
interact is a little bit different in
metal than it is in OpenGL so again data
is encoded in two buffers that are
accessible by both the CPU and the GPU
but the GPU when it receives this buffer
of data it just sees a giant like blob
of like X number of bytes so let's let's
pretend like you're sending some vertex
and color data over to the GPU the GPU
looks at this this buffer of data and it
sees 32 bytes and it goes okay I don't
know what this is this could be like you
know this could be two different the
float for vectors this could be you know
32 binary variables like I don't know
what I was supposed to do with this so
one of the important things that you
need to do as a programmer is you have
to make sure that you create a data
structure on both the CPU and the GPU
side that decodes for each of them what
all of this data means so that way when
the GPU gets this buffer of data it can
go okay I have a map here that tells me
that the first 16 bytes are color data
and the second 16 bytes are vertex data
now I know how I can take this and I can
apply it to all of the algorithms that I
have inside of me as a shader so one
framework that so that all the stuff
I've been talking about before that's
just kind of the the base functionality
of how the metal compute encoder and
command structure works there are a
number of other frameworks that have
been written on top
that that allow you to not have to do a
bunch of boilerplate code and be able to
implement more powerful things by
building on top of that structure and
one of those is the metal performance
shader framework how many people here
are familiar with OpenCV okay we got it
got a couple so metal performance
shaders were kind of apple's response to
open cv open Seavey's written in c++
it's been around forever it's really
clunky if you want to go and you want to
work with Swift you can't really work
very well with open CV because Swift it
doesn't have a lot of support for any
C++ libraries so the reason that they
included this and they created this is
because there are a lot of common
operations in neural networks and
machine vision and etc that like there's
not really any point and everybody
having to roll their own implementation
of it so like a Gaussian blur is going
to be a Gaussian blur whether you like
write it yourself or somebody else
writes it it's the same it's the same
equation that is not really a point in
making every single programmer who wants
to use a Gaussian blur go in and write
their own Gaussian blur function so you
don't have to go in you don't have to
reinvent the wheel you don't have to go
and write the same shader code over and
over again but more importantly because
this is such a common operation apple's
engineers went and they actually figured
out and optimized this Gaussian blur
functionality for every single GPU that
they have so they did a lot of work for
you that you don't have to do by going
in and tuning and it can go in and
figure out what GPU this is running on
and it'll give a different
implementation based on whether it's an
iPhone 5s and iPhone X or it's a Mac
which is a really like awesome thing to
be able to add so as of right now there
are three types of metal performance
shaders available there's shaders for
image processing neural network creation
and vectors and matrices and I'm only
going to be talking today about the
image processing and the neural network
ones because the vectors and matrices
are fairly self-explanatory so the type
of image processing that the metal
performance shaders give you is not the
same type that you would have for
something like Instagram filters there's
another framework in iOS called core
image that has a lot of really
photo effects that you can apply to
images but this one is very specifically
targeted at people who are working on
doing machine learning and machine
vision
so there are morphological operators
there are convolutional operators
threshold Excalibur to be able to
analyze and make determinations about
images that you're processing so I'm
going to talk a little bit about one of
the big you know cool buzzy type
technologies the last couple of years
and that's neural networks now when I
went and started writing this book like
I thought neural networks sounded really
cool but I really didn't have any kind
of understanding of what a neural
network actually was I knew that it was
something that was really cool if you
told people you were working on neural
network to like well that's really
awesome but like like there just wasn't
a lot of information out there for
beginner level people who were
interested in getting into neural
networks there's a lot of information
about how to build one in Python and how
to like he'll set something up but it
didn't really say about like what it was
or how it works so but the next part of
my talk is gonna be about trying to
explain a little overview about what it
what AM network actually is so we as
human beings we neural networks are an
attempt to try and mimic the way that we
as human beings think and make decisions
so when I'm going to talk a little bit
as an example of a problem that a lot of
us have probably faced in our career and
that is whether or not we want to
relocate to San Francisco so like when
you're making a decision about doing
something like relocating to San
Francisco there's a bunch of things that
you have to keep in mind like if you
know that your commute is going to be
like more than half an hour long how
important is that to you how is
important it to you to own a house or
you know live within a vibrant developer
community or earn $150,000 a year be
able to take your dog to work like these
are all different aspects of this
question that can be answered with a yes
or no like is your is your commute half
an hour no can you own a house no can
you take your work yes and these all
will change depending on like where
you're looking where you're located etc
and these can be put into a network
layer where it will go through and it'll
determine like each thing you like
yes-no yes-no etc and those values will
all be summed and then if this sum
reaches a certain critical point then
the function activates and then you go
yes I'm gonna relocate to San Francisco
and if it doesn't quite get to that item
out you go no I don't I don't really
don't think I am gonna really locate to
San Francisco and all of these inputs
these are all neurons and each one of
the neurons is connected to another
neuron until you finally some it down to
an output neuron which is the one at the
end
now since not everybody has moved
relocated to San Francisco and some
people you know like have and some
people haven't you might be kind of
thinking well like how does this work
then because shouldn't everybody have
the exact same outcome if this could be
a logical operator and that's the way
that that changes is you have weights
and biases so we as individuals we all
have different priorities we have
different things that we particularly
care about so because some for some
people like you know like earning a lot
of money isn't that important for some
people like owning a house is really
super important for other people it's
not because we all have different things
that are important to us all of these
different decisions that we make have
different weights have different biases
I talked her a little bit earlier about
training and neural network to go in and
perform functionality what the training
process does is it determines these
weights and biases so it'll look at a
series of images and it will make
determinations based on what it sees in
order to figure out whether something
works or doesn't so I'm sorry I have to
I have to bring in the infamous hot dog
not a hot dog the way something like
this would work is that the neural
network training process would be
presented with like five thousand images
of hot dogs and various configurations
as you can see in this particular figure
configuration there's a bunch of them
together in a plastic package so the
neural network would go through and it
would look at all of the different
images of hot dogs it would look at them
with buns without funds with condiments
in packages out of packages etc and
after looking at a certain number of
them would begin to learn okay you know
hot dogs have a specific shape hot dogs
have a specific color hot dogs have a
specific you know like form factor that
it understands and it knows what it
looks at the dog that the dog is not a
hot dog and one of the problems that you
can run into with this situation is if
you have like 5,000 images in a database
then you have like the the police lineup
quite like issue where the neural
network thinks that every single thing
that it's looking at has to have some
correlation inside of its like database
library and you'll get a lot of false
positives because it'll try to find the
closest thing that it can in that object
library so you need to be very careful
like one reason this was a good example
was because it's either a hot dog or
it's not a hot dog and so I just quickly
wanted to mention metal integrates with
a lot of other cocoa frameworks the
great Larry Wall said easy things should
be easy and hard things should be
possible so in metal if you want to
render a box to a screen that takes a
hundred lines of code if you want to go
in and render boxes cream using scene
kit you can do that in three lines of
code there are a lot of high-level
abstract frameworks like seeing kits I
mentioned a core image before core ml
vision and natural language processing
that are all built on metal but they
allow you to like dig a little bit
deeper into the frameworks if there's
something specific that you'd like to be
doing that but you don't want to have to
deal with all the boilerplate stuff and
you want to be able to easily implement
some aspect of these frameworks so just
some of the takeaways I have is like
you'll carry super computers in our
pockets like we can do amazing like
crazy things with these phones that are
like more powerful than the computers we
had 5-10 years ago and like we're mostly
using them for Twitter clients and do
like you know things where you you reach
out to to API so you pull down weather
information like we do so much more with
the phones that we carry around air
pockets but we just we haven't we don't
know what's possible yet and we haven't
gone and really dug into what's
available and one big issue that we have
right now is worrying about privacy when
you send all of your data up to a server
to be you know processed by a server you
don't know like it can be hacked all of
your information can be gone being able
to process that information and keep it
on the phone could be a selling point to
people that they get to own their data
and that they can have total privacy and
then then just one caveat was that this
works best if you have large data sets
so like if you just have
to like analyze like like ten pieces of
information is usually not worth it to
go and implement it in metal and it but
just again like it's really important to
think about like what it is that you're
trying to provide your customers and
know that privacy especially right now
is such an important thing but it would
be really awesome if we can start
thinking about how we can allow people
to hold onto their own data</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>