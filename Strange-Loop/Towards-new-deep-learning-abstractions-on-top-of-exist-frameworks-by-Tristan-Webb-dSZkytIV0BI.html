<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Towards new deep learning abstractions on top of exist frameworks&quot; by Tristan Webb | Coder Coacher - Coaching Coders</title><meta content="&quot;Towards new deep learning abstractions on top of exist frameworks&quot; by Tristan Webb - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Towards new deep learning abstractions on top of exist frameworks&quot; by Tristan Webb</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dSZkytIV0BI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm Tristan I work for Intel nirvana
today I'm going to be talking to you
about Antonio Van de Graaff contender
Ivana graph
after that I work on and it's part of
our efforts to develop a universal deep
learning compiler and through that
identify a core set of abstractions that
should describe neural networks and
allow them to run efficiently on a wide
variety of devices nirvana graph is an
open source project so you can go to the
github repository under nirvana systems
and graph or you can go to our website
at nirvana sis calm to view the latest
documentation so I just wanted to thank
the organizers for inviting me to this
conference it's been great to see a deep
learning track added to Strange Loop so
if you're not familiar with deep
learning I'm not going to be going over
the the mathematics but a very simple
description of deep learning is it's
going to be the technology that you're
going to see that enables self-driving
cars and other examples would be alphago
a project from deep mind basically
mastering the game of go and able to
beat the top human players so a deep
learning has its name because it
basically describes deep neural networks
this is a very computationally intensive
task and the deep structure of these
networks allow you to train these
systems to represent multiple levels of
abstraction and give almost human
abilities of performance on things like
image recognition
so it's a very computationally intensive
task that basically involves training
with lots of data so having a very
performant back-end is very important so
here's the sort of background questions
for those of you who are familiar with
deep learning that might have so how is
the code that we've been used to express
a deep learning neural networks evolved
would have been the core features of
api's that we have seen over the years
for deep neural networks what exactly is
the interface between device hardware
and software frameworks and how exactly
is deep learning different than other
types of numerical software
so it was back in 2008 when I started my
PhD when I first touched on deep
learning and it was just a few years
earlier that Jeff Hampton released his
paper on deep belief networks that sort
of causes resurgence in the interest of
deep neural networks and at the time I
was basically programming and numpy or C
or very low-level things like CUDA and
this was a real headache because you
basically had to manually specify how to
do the sorts of algebra and calculus in
order to train through forward and back
propagation then in 2009 researchers
from Montreal released Theano
and Theano was was very useful to
researchers because it introduced this
notion of abstract operations and
abstract equations using a sort of
symbolic notation and it took away the
the pain point of writing your own
automatic differentiation algorithms
we've seen a lot of other popular
frameworks come up such as torch which
introduced the idea of modularity and
it's torch was written in the Lua
language and you can still use that now
with the PI torch if you're more of a
Python person cafe and cafe to which
were developed at the berkeley for
computational vision introduce the idea
of a core set of operations that would
be useful for doing computational vision
and then of course in 2015
tensorflow was released which really
solidified this idea of a computational
graph which we're all probably familiar
with now other frameworks of note are
Nirvana zone neon which is designed for
data science and sort of push the bar of
GPU kernel performance
there's Amazon's MX net which is for
training distributed networks and now we
sort of have frameworks on top of
frameworks one of the easiest ways to
get started with deep learning is to use
a front-end frameworks such as Kerris so
you can download that and literally get
started training
a recurrent neural network in just a few
lines of code of course there's a lot of
other frameworks that I haven't
mentioned here we saw deep learning dot
Scala exploring the functional paradigm
one of my own favorites is the grenade
library for Haskell which explores
dependently type neural networks and so
we have this huge ecosystem and as a
user we're basically forced to choose
one and learn that as well as there
being a lot of different types of
software new back-end devices are
appearing so a project that I've worked
on is the crest family of deep learning
chips from Intel so this is a GPU
accelerated that's going to appear in
the future from Intel and it's been
designed from the ground up for doing
deep learning it's very good at
achieving high performance in terms of
matrix multiply operations the sort of
core operations that you would be using
in a deep neural net and this chip uses
a novel data format we have a paper
appearing at nips of the nips conference
this year about this so I encourage you
to keep on the lookout for that so as a
user this is great you have a lot of
frameworks to choose from and but the
problem is that there's a huge amount of
engineering effort in order to map these
different frameworks onto different
devices the framework author is is
responsible for writing a back-end for
many different types of deep learning
hardware and the deep learning hardware
designers and the system software
programs programmers are responsible for
selecting a few popular frameworks so
this is a good way to limit in the
innovation by basically locking in an
ecosystem so the solution is the nirvana
graph project so what the nirvana graph
project consists of is a set of
intermediate representations to capture
the core abstractions for deep learning
and then it has a different set of
compiler backends or what I'll call
transformers
which takes this intermediate
representation and allows it for
execution on the device and there's a
set of front end connectors that can
plug in to new frameworks and existing
frameworks and then we have our
reference front end which is based on
our existing deep learning framework
neon so this is what the picture would
look like with Nirvana graph so what
we've done is we've taken that sort of a
combinatorial explosion of backends that
have to be written and instead we've
just created a intermediate
representation that can be targeted by
different connectors and then the set of
hardware transformers so we've
simplified the engineering effort many
times this is what a the intermediate
representation of the computational
graph would look like if you've seen
data flow graphs from tensor flow this
should look very familiar to you so it
consists of a data flow graph where in
this picture we're representing sources
by rectangles so the orange represents
internal data entering the system and
then trainable parameters are
represented in gray and then the
constants in the system are represented
in blue here so and this basically
describes a sort of sigmoid nonlinear
activation function for input where we
take external data and we take the dot
product with a trainable set of weights
then we add in a trainable set of bias
vectors then we do a numerical
operations negation exponentiation
and adding a constant and taking the
reciprocal so this is what it looks like
in the current Python front end that's
the same sort of idea
just in code one thing that you'll
notice is that we are passing in to the
the functions axes and these sort of
allow for static type checking or static
checking of different dimensions so that
we don't end up compiling a graph that
couldn't possibly run on the hardware so
Nirvana graph is just one more layer if
you're interested in running say a
reinforcement learning algorithm and you
want to run on a CPU then on top of the
CPU you would probably be using
optimized libraries such as mkl and then
on top of that a set of a library for
doing a tensor math and then on top of
that is nirvana graph and what this
allows is for a user of different
frameworks to basically optimize to
basically access this highly optimized
back-end so this is the sort of scope of
the current project now in the in the
ecosystem we are developing transformers
for the cresst hardware which I
mentioned CPU backends for training on
Xeon CPUs and GPU support
we're also targeting certain API is
exposed by libraries such as MX net and
tensorflow there's been a lot of other
efforts that are coming up in this space
so Microsoft and Facebook have recently
unveiled the the onyx format which
stands for open neural network exchange
and this is a way to sort of standardize
models across different frameworks so
that the different front ends can
produce a computational graph that can
be sterilized and shared between
different framework
we're currently working on expanding the
universe on graph API to develop a sort
of developer toolkit that would be
accessible to developers of frameworks
and our system architect Jason Knight
he's looking into other backends that
are along the same idea of being able to
run an efficient code on different types
of hardware such as haldi TVM and weld
so who are the unive users of nirvana
graph one of the main targets would be
framework developers so if you're a
framework developer and you've developed
a new framework that exposes some new
novel api basically you can plug into
nirvana graph and then be able to access
a highly performant code path for your
back-end it's also for developers of
hardware and system software for new
deep learning ships so by targeting
nirvana graph as your instruction set
you can commit once and plug into a
larger ecosystem and then people who are
interested in doing optimizations can
basically do optimizations at the IR
level and basically enable greater
performance across a large variety of
hardware backends and if you are
deploying large neural networks that
have been developed in a different
framework you can basically target
nirvana graph and use that in your stack
somewhere and so you can export models
from one framework and run it on other
frameworks
so now I'd like to go over the the
instruction the intermediate
representation and a little more detail
and talk a little bit more about the
front end connectors that we've
developed so if you go to the github
repo today you'll see a collection of
opps where basically we're trying to
strike a balance we don't want to have
an OP set that's too small right if it's
too small then we're not going to be
able to basically express enough
back-end primitives and enabled in order
to maintain performance on the other
hand if we try to represent too many ops
then we can have this problem of off
creep so most of the ops are centered
around tensor math things like
convolutions general matrix multiplies
we have support for unary and binary
element wise kernels and things like
folds and reductions another important
thing that we have is tensor
manipulation slicing and broadcasting of
tensors reshaping dim shuffles and we
have support in the graph for parallel
ops and sequential ops so basically the
ability to schedule computations to be
to be performed in parallel and then we
have a support for assignment and two
different memory buffers and other sorts
of data mutation well where we'd like to
go with our ir set is basically to allow
more control flow so to allow for some
sort of leaping and this can allow
certain recurrent neural nets to be
expressed more easily the ability to
iterate over select lacks ease and
greater support for folds and maps so
we'd like to give users the ability to
define their own sub computations and be
able to map that in
structures of the graph our goal here is
basically to enable people to avoid
writing low-level assembly code or CUDA
code and still get high performance in
support for new recurrent kernels and
layer types
I mentioned axis as being a sort of
novel innovation of n graph so axes are
basically a way to express tensor
dimension mana management so if you're
working with a particular front-end and
you're writing a back-end for that it's
basically a huge pain point because you
have to follow the convention and be
very aware of the convention that that
front-end uses so that's a real pain
point and it is basically difficult if
your back-end device uses a different
set of layout for the memory so we use
axes which is just a way to give
meaningful names the dimensions of a
tensors it's completely optional for
front ends that don't have this
supported and as I mentioned before this
allows the compiler to basically perform
static verification so you can tell that
your computational graph will in fact
you know you won't be having a tensor
dimension in this match at runtime the
current Python API exposes different
functions to work with the graph for
instance you can find all the variables
that contribute to an OP you can do
basic traversals and mutations so here
we're able to grab the the lists of ops
that contribute to another op and then
we have support for automatic
differentiation so given to ops in the
graph you can compute the derivative of
one op with respect to another
so our connectors were originally called
converters and when we started writing
we basically had to take the low level
output of frameworks such as tensorflow
and cafe and grab the the proto buffer
output of of these and then serialize it
in and then pattern match in order to
create one or more in graph ops of
course this was a real headache so now
that other frameworks are beginning to
expose API access to them we're
leveraging that so tensorflow
has xla which I said we're targeting and
as well we're using the API for MX net
so how does this compare to onyx which
is the open neural network exchange so
this is a new project that's announced
by Microsoft and Facebook and there's
basically an open design so you can go
now and contribute your ideas we are
participating in this open design
process so at the moment it just has
support for inference but it does allow
a way just to sort of serialized data
flow graphs with an initial set of deep
learning operators and to allow more
sharing across different frameworks so
the main difference between a format
like onyx is that it's not connected to
a whole compiler infrastructure so
there's no optimizers currently built in
and so that would be the main difference
I would say so if you go today to the
github repo you'll find the Python API
the implementation you'll have access to
the neon front-end and through that
you're able to run multi-layer
perceptrons convolutional nets recurrent
Nets gans their support for
serialization and if you use tensorflow
you may have come across tensor board so
this is a very cool project that allows
you to visualize and sort of expand and
collapse your
you're tensorflow Network and so we have
support for this in n graph and then we
have our tensorflow excel a connector
proof of concept the core n graph team
is currently working on porting the set
of the set of ops to a C++ API and we're
basically switching to C and C++ to get
better refactor ability and type
checking we and performance of course
we're working on developing a full
tensorflow excel a back-end and
integration with MX net and we have
plans to release external op or FFI
support which will allow you to write
custom user kernels basically in line
alright and now like to talk about the
backends so the graph transformation
architecture you may have noticed sounds
you know very similar to the LLVM
project and that's no accident it wasn't
in fact inspired by LLVM so what we do
with the intermediate representation is
this is our sort of core set of ops that
we're going to transform to do the
abstract tensor operations and then
we're compiling down to an executable
primitive so some example passes that
you might see in the optimizations would
be things like arithmetic simplification
taking for instance removing redundant
ops if you have a variable and you take
the exponent and then you take the
logarithm that of course can be
simplified to just that variable in the
optimizations we're also doing things
like pruning redundant dimensions doing
element layout at this stage we're able
to do things like storage planning and
start to think about how memory might be
laid out on the back
and at this stage were able to sort of
look into what parallelism opportunities
might be inherent in the computational
graph so you might be asking why don't
we use some existing compilers such as
LLVM or ICC so I would say that deep
learning is inherently different than
doing normal numerical computing and
that our operations are primarily based
on tensors so these other tools don't
really offer the right set of
abstractions that we want so tensors are
just another word for large
multi-dimensional arrays and deep
learning tensors are fairly regular at
this level and there are many
optimizations that we want to be
thinking about at the tensor level so we
do tricks like horizontal fusion in
order to simplify the amount of
computation that needs to be done when
we think about memory liveness we need
to think in terms of keeping tensors in
memory instead of registers and by
designing a system on top of these
compilers were basically still able to
reuse the code gen of those in the
Transformers so our proof of concept for
a C++ code gen is leveraging the
just-in-time compilation that is
existing in LLVM so then why would we
develop this in and why wouldn't you
just program directly in CUDA or cout
DNN which is a set of primitives for
deep neural networks or using something
like MK LD n and if you're executing on
the CPU well if you've ever looked at
these they basically offer a very low
level of abstraction it's not something
that you want to be programming in
you're basically confronted with a lot
of raw matrix multiplications and
convolutions you're sort of at the
memory management level of buffers if
you're a data scientist
you definitely don't want to use this
level of control when you're just trying
to write a model and there's many ways
that you can hurt performance if you
really don't know what you're doing when
you're working at this level you can
easily get memory layout wrong that's
wrong for your device the allocation may
not be optimal and doing things like
operating operation Fusion is very hard
to do so why would you be interested in
looking at neon versus say other popular
and frameworks such as tensorflow pi
torch or cafe well these are all very
good frameworks like if you're running a
production model tensorflow is a very
popular choice pi torch is very popular
among researchers cafe for computational
vision people
well we design neon to basically set the
bar for a performance so neon contains a
set of highly optimized GPU kernels
which basically set records for a GPU
performance a neon is is a framework
that's designed more for applied data
science so when we have our data
scientists do projects for customers
they're all developing and contributing
to neon and by having another framework
we're able to sort of use it as a
laboratory for deep learning
infrastructure so neon is where are
things like AXYZ originated we have
notions of containers were able to take
layers and add them to a container in
order to construct more complicated
networks and we have support for
dynamical networks and another thing
that we can do at the back-end level is
start to think about distributed
processes so our we have a distributed
process a transformer which is called
heater and the heater transformer works
by taking a computational graph
and inspecting it and following the
edges and the user will basically have
us specified in their script that they'd
like to take this part and distribute it
across different machines and so then
we're able to clone the graph and create
sub graphs which we call heater sub
graphs and split those and create
communication nodes so currently we're
working on adding heater support for MPI
and what this means is that the user
will basically have to modify their
script in order to add some MPI aware
the places where the heater should
basically distribute that using MPI so
at the moment we have this set of
transformers we have a GPU transformer
which is using the Nirvana GPU kernels
which executes saan CUDA then we are
working on CPU transformers using MK LD
n N and we have a reference numpy we
have the heater transformer and
transformers for Lake crest the common
optimizations that we've implemented our
operator fusion pattern matching
utilities memory sharing and layout so
the roadmap of things that we'd like to
accomplish at the rest of 2017 is
basically a seamless interrupt with
tensor flow to the XL a multi device
training support via heterogeneous
transformers we're working on then
allowing multi node training for data
centers and we have a set of performance
milestones we try to achieve the highest
performance for a reference set of
modern neural networks other the team is
also working on developing
Pemex nut transformers additional front
ends CNT K in a PI torch front end we'd
like to see more back ends of course
connect to all this as new novelty
learning hardware's developed and
brought to market and we'd also like to
focus on deployment operations so as
deep learning is seeing more sort of
edge devices things like weight
quantization where you switch to a lower
precision format is something that could
be built into n graph and weight pruning
which basically squeezes the amount of
the memory required for neural nets to
allow them to execute I'd just like to
take any questions at this point I
finished a little bit early so anyone
has any questions I'd be happy to answer
yes you had a question yes
big deal so remind me of big deal that's
a cloud-based solution
I don't think we have any uh I'm not
familiar specifically with big DL so we
have a cloud system which will basically
allows data scientists to upload their
model to a cloud and then we sort of
take care of executing it in the data
center yes
um so our paper describing the format is
coming out at nips in December we also
have Nirvana con which is taking place
at the same time in December so the the
hardware is designed for training right
yes at the moment of mkl DN n is its own
repository and so it's not tied to mkl I
believe it at the moment it's it's sort
of working on top of lower level blast
primitives so I'm not I don't recall
having to install mkl in order to run em
k LD n n okay if anyone else has any
other questions I'll be available in the
back so I want to thank you all</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>