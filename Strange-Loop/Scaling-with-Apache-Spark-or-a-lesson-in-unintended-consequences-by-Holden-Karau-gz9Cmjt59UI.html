<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Scaling with Apache Spark (or a lesson in unintended consequences)&quot; by Holden Karau | Coder Coacher - Coaching Coders</title><meta content="&quot;Scaling with Apache Spark (or a lesson in unintended consequences)&quot; by Holden Karau - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Scaling with Apache Spark (or a lesson in unintended consequences)&quot; by Holden Karau</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gz9Cmjt59UI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">for coming to not the last session but
but almost the last session I brought
Timbits but slightly more people than I
expected showed up which is it's lovely
but hopefully you don't all want Timbits
but if you do after the talk there there
right there cool so this is scaling with
apache spark or a lesson in unintended
consequences I'm gonna probably focus
more on the unintended consequences
because I think that's more fun when
things work it gets kind of boring or to
be fair it's not like I've had a lot of
experiences where everything's worked in
my career so maybe it is actually quite
exciting let me know if you are so I
also do want to throw it out there is a
Content warning this is not suitable for
viewers who believe marketing material
from vendors this may occasionally say
things like about how software doesn't
actually work and if you if you want to
be happy and believe that spark is
perfect this is probably not the talk
for you we're gonna perhaps shatter some
of those illusions so I'm holdin my
preferred pronouns are she or her it's
tattooed on my wrist in case you forget
despite that people still forget but
that's that's fine I'm a software
engineer at IBM spark Technology Center
in San Francisco I really like it they
pay me to work on open source software
it's pretty exciting if you're looking
to work on open source software and
willing to relocate to San Francisco
then come talk to me afterwards I am an
apache spark emitter which is really
exciting for me and probably not very
exciting for any of you but if you have
changes you want to make to PI spark or
you want to start contributing to PI
spark this is not that talk but please
come talk to me afterwards and I'm
always trying to get more people from
outside of UC Berkeley contributing to
the project so that we have if perhaps
not a diversity of people maybe maybe a
diversity of at least schools well I'll
take what I can get I'm a co-author of a
few spark books
I think high-performance part is
definitely the best spark book out there
I get much higher royalties on it than
any other spark book that's been
published it might not be very good for
learning spark but it buys me almost a
quarter of a cup of coffee in San
Francisco so that's exciting if you want
to follow me on Twitter it's just my
name SlideShare I didn't pick that
username just bite everywhere else but
whatever consistency is overrated and
yeah so just outside of work or perhaps
in addition to work I'm trans I'm queer
I'm Canadian which I know that one and I
consider myself a part of the leather
community actually so I'm the Canadian
one is I'm here on a work visa that
y'all are considering changing which is
just a wonderful time to be in America
and and I know we've we've heard this
before but please please have you know
support your co-workers I like to think
that when all of us work together we can
make the same shady software faster and
I get to go home and see my partner and
that's really exciting so we should we
should all work together we shouldn't
like fight with people on the internet
and you know we can we can be nice
people to each other and you can
remember we come from different
backgrounds right also I remember the
first time I met someone like me in
software I was really excited and happy
and so you know yeah whatever
um the other person who's presenting
with me today her name is boo she has
presented before the Texas State
Legislature they did not listen to her
she's she's lovely she has a lot of
experience barking at computers to make
them go faster and what else oh yeah
she's also she got a Twitter account on
this trip after I realized I've just
been slacking on that so please follow
boo on Twitter she tweets about happier
topics than I do
right oh my employer yes I work for IBM
we have this Lobby with a lot of green
in it green is the color of a passing
Travis build so if you're thinking about
buying software from IBM think of this
green lobby I'm sure it only cost us a
few million dollars and it
at least says quality software yeah my
boss told me I have to put the slide in
every one of my talks but he did not say
what I have to say about this slide the
fact that they've been doing this for
like six months means he does not listen
to these but that's fine so yeah I work
with a bunch of other people in San
Francisco
I'm co-located with a bunch of designers
in case you haven't figured it out yet
none of them look at my slides this is
not their fault this is all on me
they're lovely people they work on open
source design which is hard and so it's
great if you consider buying things from
IBM maybe you work at a bank please keep
doing that I like getting paid so yeah
cool yeah so what are we gonna talk
about I'm gonna talk about some of my
assumptions about who you all are you've
been laughing at my really bad jokes
though so that's just oh thank you so
much it's it's really sad when I'm just
doing this the stand-up slash is your
science routine and no one's laughing
we're gonna talk about sort of the core
abstractions behind spark we're gonna
talk about why spark ended up being the
way it is and I think really that's the
more interesting part and incidentally
along the way you might learn about how
do you spark more efficiently but
hopefully we'll learn about how to
design things better so you don't repeat
the same mistakes we did or if you do
you know what you're doing not that we
didn't yeah certainly not so you're nice
people if you mind pictures of cats this
is not the talk for you there are other
talks by people who do not like cats as
much as I do
although thankfully cats do seem to be
very popular it's strangely probably no
one's playing Pokemon go anymore
which just breaks my heart but it's
alright
and yeah so actually how many people
here actually use Apache spark okay
you see okay how many people do not use
spark I think I see some of the same
hands and I am very confused but that is
alright okay hopefully this will be good
for both of you for those of you that
don't use spark it's a general purpose
distributed system specifically though
it's focused on the kinds of problems
that we solve with data parallelism it
has a really nice API if you think
functional programming is good and you
probably do if you're here it even has a
Python API which is useful I work on the
Python API a lot
it's an Apache project with all of the
good and bad that entails this is being
recorded so we won't go into either of
those things it's faster than Hadoop
MapReduce and I just I just want to say
I love Hadoop MapReduce for setting the
bar down here it's I mean like it's
great like Hadoop MapReduce definitely
gave us the ability to solve problems we
couldn't before but performance wise it
left a lot of opportunities for SPARC to
improve on and other systems and so it's
very good normally people come to spark
when their problems get too big for a
single machine or they've decided they
want to find a new job and so they're
gonna start using spark so they can call
themselves a data scientist it has sort
of two core abstractions and we'll talk
about why they're both terrible so when
I say distributed system what I actually
mean is there's a hundred gnomes they
live inside of this thing it's called
AWS or sorry IBM cloud and when things
go wrong you can blame the gnomes
unfortunately sometimes SPARC is
responsible for coordinating what all
these different gnomes have to do and
sometimes SPARC really really just
decides it doesn't like one of the
gnomes so it gives that gnome all of the
work and then things take a predictable
turn when that gnome decides to go out
for drinks instead of writing software
and so yeah okay cool um right so why do
people come to SPARC all the cat
pictures are so big yeah um a lot of
people come to spark because they are
running a MapReduce job it's taking 16
hours and they're like
I can probably learn spark in 16 hours
um and you can that I have written
MapReduce jobs that took longer to run
than it did for me to learn spark um not
all MapReduce jobs the other one is you
know my data no longer fits in Excel and
those people thank you for buying
support contracts um you probably can't
read reviews pandas but more
realistically like the other way that we
come to it is I have a local problem
it's gotten too big to solve with the
techniques that I'm used to
oh crap oh crap I have to use
distributed systems yeah so um Paco does
a much better version of the history
because he actually knows all of these
people and and saw many of them
including taking them out for drinks
when they were told their projects were
cancelled before they started working on
the things that were spark but it came
out of UC Berkeley's amp love which is
really awesome it got rebooted into
something called The Rise Lab I don't
know they they change their name every
few years I guess in case the papers
aren't good enough to just keep that
ball rolling but the papers have been
really good and they still keep changing
the name so I don't I don't understand
their motivation it's really similar to
a lot of these other systems if you're
familiar with any of them this will give
you some context if you're not don't
worry no stress
so the sort of terms that I'm going to
use really frequently are are DD which
is sparks like generic distributed
collection it's called a resilient
distributed data set the resilient part
is important because in distributed
systems failure is a first-class citizen
it's really well I shouldn't say it's
really easy it's comparatively easy to
make a distributed system where we don't
care about failure but it turns out that
IBM cloud well we don't have failures
some of the other vendors have hardware
that fails more realistically like once
you start running on 100 machines your
probability of failure starts
approaching 1 over the course of any
reasonable length of time so you have to
be able to recover from failures SPARC
also has this thing called data frames
which often gets Python people excited I
don't see a lot of people excited so
either you've already gone down the
disillusionment of figuring out that
spark data frames aren't data frames or
you're not Python users it's fine either
way but it's essentially just like a
distributed data frame which really
doesn't have any of the operations you'd
expect on a data frame so all of the
concepts you want just with none of the
useful things and then data sets are
data frames with types
everyone loves types or at least some
people love types the spark developers
are Scala developers so they love types
and so data frames we took types out and
then we went wait that was a bad idea
and data sets are data frames with types
thrown back in this is the really fancy
diagram which definitely convinces you I
don't have designers but so that's
important to know that spark is a
collection of things it's got this sort
of core execution engine but we also
have this fancy machine learning library
very good for VC funding we have this
streaming library used to be pretty good
for VC funding now only sort of so so
the language api's are good for when we
actually want people to use our software
in addition to raising money now and
again so usage numbers though very
important monthly actives so Python and
I are definitely in there we have a
collection of graph tools that don't
work and we have this other machine
learning library that we just like to
pretend doesn't exist and community
packages is the polite way of saying
everyone who isn't us right ok so let's
talk about one of sparks core design
principles and that is laziness
now you're probably familiar with
laziness like you it seems like a lot of
people here have been exposed to Haskell
but sparks laziness is a little
different it's it's more so it's I like
to use the teenager approach you ask the
teenager to take up the trash you ask
the teenager to take out the trash you
actually finally go and check and then
the teenager takes out the trash right
and so spark does this you you ask it to
compute a bunch of data and then you say
well ok on top of that I want you to
compute this other data and some other
data and then that's fine sparks it's
like yeah yeah I got that I got that I
got that and only once we finally ask
spark to like save the results out
we'll be like oh yeah so about that
you're employed file just didn't exist
so I didn't do any of that but so now
normally when you when you think of
laziness or at least normally when I
think of laziness
I think of systems which are terrible to
debug and that's spark but on top of it
we actually reuse this laziness we do
smart things with it
so it's not it's not just you know
terrible things to debug we're able to
use some cool things about it we got
this really fancy graph okay no one's
impressed um but like it's it's you can
see this graph of the different things
that we've done in spark and we can see
that there's these different stage
boundaries and so in spark a stage
boundary is introduced where we have to
like shuffle our data so when when the
data can't be computed locally on a
single node anymore for each partition
and maybe we have to like combine data
by key we'd end up putting in a stage
bang boundary but you can see that a
whole bunch of things able to get sort
of collapsed into a single stage and if
you are coming from a MapReduce
background you're probably used to
spending a lot of time thinking about
how to combine everything into a single
map tasks or or trying to you know sort
of reduce the number of passes you're
doing over your data and in spark though
the nice thing is I can write sort of
more functional style programming
without having to spend a lot of time
manually combining my different map
tasks and sort of trying to shove them
together because the optimizer when we
finally force it to do the work goes in
and goes oh well I can stick these three
things together and this thing from over
here yeah yeah come on you can be
friends and then you know it gets a
little awkward if they use too much heap
space but otherwise you know everyone
gets along and then on data frames we
have something very similar except you
can't actually understand what it says
so you can have a similar graph that
shows you the execution once again SPARC
is combining a bunch of things for you
it just
instead of saying useful things says
whole stage code gen which means I put a
bunch of stuff here good luck so I am
going to have everyone's favorite word
count example as a licensed Big Data
instructor you're not allowed to give a
presentation with a word count it's why
it's in everything
one of these but so this this can
illustrate sparks really you know basic
laziness so the first line loads in some
data then we go ahead we tokenize it
with spaces because I'm in North America
I can pretend that's a reasonable
tokenizer and then I go ahead and I
compute the word and the number one pair
of very exciting and then I go ahead and
I tell spark how to combine these
results so I say for everything with the
same key please combine it together and
then finally okay yeah I should probably
save this output and so this is this is
the action which actually forces spark
to do the work until this point spark
has done nothing our input file called
SRC may not exist it might actually like
we might not even be on an HDFS cluster
spark wouldn't notice all of these
things could fail and be terribly wrong
and we would only find out here but but
the important part is right like this
this map task and the previous flat map
tasks can get combined together into a
single stage right so if i was working
in a system where this was going to be
done like a map and then do another map
i would actually want to explicitly have
to combine these Pitts bits together
myself
but spork can do this for me for free so
I don't have to think as hard now this
isn't so exciting when I've changed like
three operations together but when you
start to get those really complex graphs
like on the previous slide like the
automatic chaining is a lot nicer
so third there's some total downsides to
this though right the pipelining part
really awesome I got that for free
the complete lack of debug ability is
really frustrating for a lot of people
essentially like the fact that when I
call this I can get an error about my
input file not existing is really
something that takes getting used to is
a polite approach to it it also doesn't
help that support stack traces are
frequently around 400 lines long so even
once you do get the stack trace figuring
out what actually went wrong
is a little painful the other really
cool thing is spark uses this laziness
to to construct those graphs I was
showing you and those graphs are
actually used to handle sparks
resiliency
and this is a very different approach
than the MapReduce model the MapReduce
model says like I might lose some
computers but it's okay I'm just gonna
write it out I'm gonna write my work out
on persistent storage to a bunch of
different machines so if something goes
wrong I can go back and find it right
I'm not gonna lose all of my HDFS nodes
right if I do something really terribly
wrong has happened and we're probably
weren't going to be able to pay our
advertisers anyways it doesn't matter
but sparks says writing out to disk is
kind of slow
instead what I'll do is I'll be the lazy
teenager again if I lose my homework
I'll just pretend I've done it and then
once you finally ask for it I'll just do
the part that I forgot and looks like
it's not too bad an approach right like
just recomputing the missing piece is
often pretty cool
it can be a lot faster than writing your
results out to disk on a bunch of
different machines and so this laziness
gives us this implicit graph and we can
use this graph for resiliency um part of
it is we have to make all of our
collections immutable because if you
know I did this recompute and I allowed
you to mutate the previous state my
recompute would not necessarily make a
lot of sense but that's okay
immutability everyone loves immutability
definitely no one wants to mutate
collections it's fine it's fine
everything's okay I should have the dog
there okay so there is a problem though
because even though we are able to do
this really cool pipelining we're only
able to do pipelining up until this
action stage and part of this is like
unlike Heather's talk yesterday which I
don't know if y'all went to spark is
notably not a compiler it doesn't know
everything that's going to be happening
it's it's just a library it only sees up
until this action and so spark being
lazy will we'll be able to optimize up
until that point but it doesn't know if
you're gonna need any of this data a
second time around it can't see into the
future you don't have the Marty McFly
Back to the Future time machine okay two
people saw at that movie um but it's
okay right we can just tell spark
manually when we want to reuse our data
and so this is this is like a little bit
packy right like our
magic is starting to the man behind the
machine is starting to become visible
but it's not the end of the world right
we can just paper that bit over and just
tell spark I want to use this data again
and we can pretend everything's ok and
spark works right ok
some people so the other magical part is
the partitioning of our data right we
loaded this data in and I implicitly
just assumed that it was going to be
distributed on a bunch of machines and
most of the time that actually works in
spark if you're loading from something
like HDFS spark will do a really good
job of just like reading the data in and
the partitioning will end up being
pretty similar to what the partitioning
in HDFS was the only downside if you
start reading in what's called non
splittable data more gzipped files
because no one likes compression is then
spark just yeah it doesn't it's not able
to automatically split it for you for
technical reasons that aren't super
important so sometimes we have to help
spark out by explicitly partitioning our
data which destroys a bit of the magic
right I can't just pretend I have this
magical distributed collection I have to
tell SPARC how many pieces I want in my
magical collection but that's that's not
too bad right it's just like specifying
the parallelism that's ok right I think
things are gonna deteriorate some more
so you better be ok with that part um
so the the problem is once we get to
this like ok spark I want you to split
up my data some more for me
spark goes yeah sure I'll just I'll
split it up I'm gonna make this object
called a partition err and the partition
er will tell me where to send the
different pieces of data um
and spark decided to use deterministic
partitioning right normally when we hear
determinism in computer science we're
like yes I want my software to be
deterministic non deterministic
partitioning sounds scary let's not do
that the only problem is my petitioner
has to decide based entirely on my key
data and it turns out that you can have
this thing called key skew and the
really the really inconvenient part
about it is that's key skew happens in
the real world and not in my randomly
generated sample data
because I like picking uniform
distributions looks very nice but so the
problem is once we ask spark to start
partitioning our data things can start
to go downhill and this can start to
impact some of the operations that we're
going to be doing on our data so group
by key how many people have like a
visceral reaction when I say group by
key and spark at the same time okay
there's like five hands so most of you
haven't used that that's good never call
group by key and how many people have
the visceral reaction when I say sort by
key and spark three hands I'm really
sorry okay
let's talk about why those are terrible
so key skew comes to the anti rescue or
as I like to call it job security so
essentially it turns out most of our
data like real data has key skew humans
we cluster together in cities computers
really like clustering together in this
city called null it's their favorite
city they all really like to live there
group by key explodes it's not hard to
make her by he exploding spark honestly
like this is as far as design decisions
we made that were wrong the group by key
thing is just like one of those ones
where I'm like oh I really wish I had a
time machine because it would have been
like a one-character fix but that's
that's the story for two slides later
and the other one is like we can we can
just have really unbalanced partition
errs and in partitions and in that case
even more normal operations like sorting
by key can fail that normally happens
with no null is is the one which
normally makes that part hurt um so
here's an example of group by key in
spark so this is I've decided that I
really want to get out of tech at some
point in my life I'm not sure if
everyone else here has those feelings
but you know I love computers but one of
these days I would like to not wake up
periodically and really just hope
nothing's on fire you know want to wake
up and and feel optimistic about life
not wondering who who hates me today so
I've decided the best way to do that is
to open an artist
moustache wax shop in San Francisco
although actually because I'm thinking
about getting out of tech I'm not sure
if San Francisco is the place I should
open my artisinal moustache wax shop so
I can buy some information about where
mustaches are right marketers have a lot
of data it's kind of scary but I can buy
that data from them for about $5
alternatively if they won't sell it to
me I can buy information about third
wave coffee shops which is strongly
enough correlated with mustaches that we
can make this work right so let's say
I've got a bunch of Records so this is a
zip code and then probably your social
security number because I paid an extra
20 cents and then you know maybe your
mother's maiden name in case I wanted to
open a credit card and then so I group
this together by key and the problem is
it turns out there are just so many
hipsters in the Mission District in San
Francisco so 94110 is the district I
live in and there's just too many
handlebar mustaches my computer fails
and I can't send you these exciting
promotional offers about handlebar
moustache wax and this is this is very
sad I know you all feel sad when you
don't receive special promotional offers
and so don't worry the best minds of our
generation are working on this problem
only only slightly depressing so um what
do we do instead so group by key is
especially bad um so we can see here
essentially what's happened is we have
the zip code and then we have this this
is a list
I like Python syntax too much this is a
list of tuples and so essentially we
have created a single record of all of
the handlebar mustaches in San Francisco
predictably this message is too big to
fit inside of the to cake partition
limit that's just gonna happen so it's
okay though because I'm actually really
only interested in figuring out where to
open my moustache wax shop at this point
I could actually just go back to that
word count example and instead of doing
group by key I should actually probably
use reduce by key and then what happens
is spark will go and on each of the
partitions it'll you know compute the
number of people in each zip code and
then it'll shuffle the number of people
in each zip code and it can further
aggregate it
and then my data works and I can figure
out yes the two most popular places for
handlebar mustache is our San Francisco
and null and as soon as I can find a
Lisa null I'll open my shop there I
could get by key pretty similar you know
we also get free set maps I production
that's that's pretty much um but it's
okay we can make this same problem
explode just by sorting our data we
don't even have to do group by key and
this is this is the part which actually
makes me feel really bad about our
decision to enforce deterministic
partitioning because a group by key at
least I can probably tell you here's an
alternative please do this instead but
the alternatives assert by key is
sufficiently bad that I feel bad and it
takes a lot I've worked in software for
a long time so I'm willing to say close
won't fix to a lot of things um so once
again handlebar mustache wax data very
important but in this case I've decided
I want to like sort this data maybe I'm
gonna sort it by zip code so I can
figure out which people I should send
these mails to the problem is now
instead of not fitting in a single
record it just it just doesn't fit on
the machine right in this case the ten
records probably would fit on a machine
but if I was you know sorting my my data
for humans quite likely San Francisco
wouldn't fit on a single machine very
easily if I was doing this from my order
data would be trivial assuming my
handlebar moustache wax shop doesn't do
super well but you know this is this is
pretty bad right the sorting by keys but
should be a safe operation right if I
can at least sort by key I've made a
really not very good distributed system
for you um but it's okay what I can do
is I can tell you you just just add some
crap to the end of your key and
everything will be fine and so that it
it's it's like fine in the same sort of
sense that a rusty spoon is fine I can
get a tetanus shot later but it's
expensive and and this is a problem so
what happens is yeah we can do this
right I can manually add some crap to my
key to allow spark to be able to split
right so this deterministic partitioner
is able to introduce a split the problem
is I had to add some crap to vendor
Mikey now if I want to join this with
another thing I have to go back in and
throw that crap out now that's ok
because the deterministic partitioner
only applies during partitioning so if I
have a map stage
I'm not repartition my data in a map
stage I'm only doing it in a shuffle
stage so if I'm not currently
partitioning I can I can throw the crap
out it's fine
but on the other hand we really should
have seen this was gonna happen
given just that it happens like all the
time and and there actually is a
solution to this one of the solutions is
someone's been working on a
non-deterministic partition or support
for spark the reason why we chose
deterministic partitioning at the start
though is really reasonable right if I
know where my keys are and I want to
join with another thing where I know our
and keys are I can just do node to node
communication I don't have to do like
another shuffle and if I know where my
keys are and they're on the same machine
I might not even have to do any network
communication to join two data sets
together right that's what we want we
want co-located data where I don't have
to do network communication and I can
only really guarantee that with a
deterministic partitioner which is which
is why we chose it it just turns out
that like humans and computers and
pretty much anything I have data about
has enough key skew that I can't really
take advantage of this very often so
that was a great decision and it'll be
fixed in the future okay here's an
example with literally kilobytes of data
right we've got shuffle read of whoo 48
kilobytes and I'm running on one time so
we're just going to move ahead you can
see here I have a shuffle read of 1/4 of
that smaller numbers are good in this
case it's not revenue so we can see that
when we replace our group by key for
reduce by key we were happier the dog is
even happier all around Goodwin
right yeah so this is I already told you
all of these things but if you forget
there's a slide okay
arbitrary functions so this is a really
important principle of spark it means
that you aren't just limited to like
sequel or something right not that
there's anything wrong with sequel it's
a great language but you can write your
arbitrary Python functions you can write
your arbitrary Scala functions if you're
really out there you can write your
arbitrary c-sharp and JavaScript
functions and run them on your Big Data
if you're doing them on JavaScript I
know someone that really wants to talk
to you
but if you're not it's that's probably
more normal so the problem is our
optimizer with arbitrary functions
doesn't know what we're asking it to do
right you some of you may have been like
feeling a little bit cheated when I was
saying don't use group by key because
you can be like Holden like if you group
it by key and then you'll apply a
reduction to your group set like if I do
a group by key and then I compute the
sum right why doesn't spark just
pipeline that sum for me and the reason
is the sum is implemented in Scala or
Python code and spark has no idea that
you want to compute the sum it just
knows you want to compute something over
this list and it's gonna do that for you
but it doesn't know how to help you and
this is part of the motivation for spark
data frames and we're gonna we're gonna
move quickly now so we can get to that
right yes okay sadness we can we can
blind key skew with black boxes to make
spark even sadder this is bad word count
the second part of the word count
example that doesn't normally make it
into the big data talks we use group by
key and map values and spark has no idea
that we're asking it to compute the sum
so this just performs really poorly but
it's okay we have someone who's offered
to save us yeah okay available starting
in spark 1.3 which is been available for
a really long time now so please data
frames are really awesome you can use
arbitrary lambda expressions with data
frames but the nice thing about data
frames is you don't always have to use
arbitrary lambdas if you're gonna group
by key
and then compute the sum or you are
going to compute a bunch of different
aggregates you can actually do this in a
much more expressive syntax that spark
is able to understand and the optimizer
is able to help us it's not like
complete magic but let's look at some of
the magic um so here oh right the other
part that this is able to do for me is
push down filters to my data store level
and this is really awesome because I'm
super lazy and whenever I'm writing code
I don't normally think about like I've
got this filter oh I should probably
take that up and apply that to where I
was reading my data in so here I'm
reading an information about pandas and
I only want to know about the happy ones
because the sad ones make me sad so I
want to compute the fuzziness and so
I've got this filter expression and this
thing to compute the fuzziness now the
other thing to compute the fuzziness is
a Scala expression the thing to restrict
it to happy pandas is a special sparks
equal expression you can tell because we
put an extra equal sign in there it
makes it magical but what happens is
spark is actually able to understand
this expression I mean the syntax very
reminiscent of Perl we loved that dollar
sign but you know we're able to push
this down to the data store for you for
free and so that's pretty awesome yeah
cool i specify the types here because I
like types and type inference it turns
out it's hard and so we didn't bother
doing it in a lot of places but there's
probably only medium amounts of
excitement I can definitely tell those
the laughters gone down so we're gonna
go on to the really exciting one
yeah the talk is almost finished I
promise um so the other thing is we can
actually we we still have our arbitrary
lambda expressions right if writing in
sparks weird little sequel like DSL
doesn't feel very natural to you you
only have to use it in the places where
it's really important and the rest of
the time you can just write your fun
lambda expressions you can have all that
fun being a functional programmer and
just you know party I used to put this
example on and say in like you can do
things that are like difficult to do in
sequel and someone just pointed out to
me that I'm just really bad at sequel
so yeah okay um right yeah here's our
graph um you should trust us as much as
you trust graphs from vendors hmm
bigger numbers are bad smaller numbers
are good smallest number is thing I'm
telling you is good please buy six TL DR
right
whenever anyone tells you things work
really well you should ask them how
they're going to explode and these is a
non-exhaustive set of ways that data
frames are worse than rdd's if you're
doing machine learning on them even
though sparks machine learning exposes a
data frame API iterative algorithms
perform really terribly we actually just
hide everything from you and do it on
our DDS
long story ask me when I'm drunk the
pushdown thing that I told you was
amazing only works some of the time
probably not the most surprising thing
in the world it depends on your
different data sources and what they
support the the wonderful thing about
this is that we added a whole new set of
knobs for you to tune in your spark jobs
and they're just really well-documented
and very easy for you to figure out what
they should be set to and yeah the other
thing is like a lot of these default
numbers we picked are great for the word
count example kind of terrible for
production data so you get to go and ask
people on the user mailing list what did
you say like fubar - and they'll say 7
and then you set it to 8 and hope for
the best
um let's see what time it is okay um
yeah I'm gonna breeze through this part
we're gonna have spark on nan jvm
languages it's already there we support
Python some people made it work with
c-sharp coincidentally they work for
Microsoft largest surprise ever our
support actually that person has several
people work on it one of them is that
Microsoft really nice guy and
essentially this part of the story is
perhaps not super exciting to people but
it's essentially a study in how
expensive inter process communication
can be and it turns out pretty
offensive is the short version of all of
the slides I'm going to skip now okay um
and instead we're gonna focus on the
future the future is this thing called
Apache arrow has anyone heard about
Apache arrow yay is Wes in the room oh
yeah okay so if you have any questions
about Apache arrow go talk to Wes but
more seriously we're hoping to start
using Apache arrow more frequently for
interchange between these different
languages also for we're working with
GPUs and other fun things it is possibly
the future I really hope so
the alternatives are strings and strings
make me sad and yeah is that your logo
by the way I just like found something
on Twitter okay cool
so this is this is the Apache arrow logo
over a cat who is having a really good
time yeah okay
so for most of these things normally I
would end with patches welcome but that
would be a lie patches really aren't
welcome for any of these things it's
unfortunate but essentially even though
spark is open source these are really
core design decisions and we need to
work around a lot of these things but if
you just show up with code randomly on
the mailing list people will be like oh
no I don't want to do that so it's gonna
take a lot longer there's this process
called s pip for suggesting large
changes to spark if you have ideas about
how to do this consider writing one of
those at the same time you might want
someone to pay you so consider writing a
thesis proposal but they won't pay you a
lot um but UC Berkeley the alternative
option is you can just go and build
other systems that don't recreate our
mistakes I would prefer it if you fix
spark I make money in spark vaguely
somehow so yeah okay that's a depressing
end if anyone doesn't know spark and I
haven't scared you away from it Paco has
a wonderful introduction video more
importantly though oh no let's skip
testing that's not a big deal at all um
you should buy several copies of my
latest book high-performance spark that
quarter of a cup of coffee won't pay for
itself and honestly if you want to learn
spark learning spark or learning PI
spark probably better options I don't
get a lot of money I didn't even write
this one and I still think it's good
really great people okay yeah
cats love it no returns please ooh if
you want excuses to come go to fun
cities here's a bunch of places I'm
gonna go give talks at and really that's
pretty much it thank you all for staying</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>