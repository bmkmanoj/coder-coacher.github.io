<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Reasoning about performance (in the context of search)&quot; by Dan Luu | Coder Coacher - Coaching Coders</title><meta content="&quot;Reasoning about performance (in the context of search)&quot; by Dan Luu - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Reasoning about performance (in the context of search)&quot; by Dan Luu</b></h2><h5 class="post__date">2016-09-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/80LKF2qph6I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm gonna talk about two things today
one I'm gonna talk about some ways you
can think about a reasonable performance
and - I'm gonna talk about search
searches basicly just a case study
because when you're thinking about
performance it's often useful have a
concrete example an example will happen
to be searched and also in particular I
think I'm gonna talk about today is
interesting because something we use in
Bing in production but it's considered
obsolete for almost 20 years so I'll
talk about why we think it's useful even
though most people think it's a really
terrible idea okay what I say
performance by the way I don't
necessarily just mean speed oh this
could be powers me a lot over the
factors I'm going to talk about an
example where we talk about speed hablas
could actually be anything okay so why
don't we care about poor ones what
answer is we often don't care for most
applications right the application is
basically fast enough and where I got
princesa waste of time I think this is
true for most applications is an error
to spend a lot of time thing at
performance however for large
applications this is not true you have
two things that really matter for our
complications one large applications
cost a lot of money to run so I increase
in performance we can decrease that cost
substantially in this matters and second
and this is often not intuitive to
people but latency is for many
applications directly related to to
revenue so for large operations to be a
lot of money you care about performance
cuz will make you more money I'm talking
about why later okay so how can we think
of performance it turns out we can often
just do simple back the envelope math
like I'm talking about just arithmetic
and estimate the permit system before we
build it for the kinds of systems I work
on it often takes a year or two to build
the system so it's actually quite useful
tool to do this I understand this is not
a popular idea like here's a very
popular tweet working code attracts
people who want to code design documents
to try people who want to talk right I
get this idea like coding it feels like
real work right and deciding stuff
sitting the whiteboard having meetings
writing powerpoints like this is not
feel like real work however what I look
at systems that are built like I often
see two systems are they're actually
fundamentally trying to do the same
thing that were built by companies with
the same resources or one system is 10
times - a hard times faster in the other
system and so what I look at what
happened when I talked to the team they
built the fast system usually spent a
lot of time in the design phase for
something like I build which often takes
like it let's say a year and a half they
might have spent 3 months actually just
designing things before coding at all
all using the slower system they had
this attitude right like coding is like
a real work right we should start coding
right away the problem is if you start
coding right away you spend a year and a
half coding you build a system Furman is
not what you want it takes like a year
and have to iterate and build the next
version right I understand it's common
advice to say hey don't optimize too
early build this
then profile and like Sandown the
hotspots this fund mentally does not
work for very high performs you often
have to completely change the
architecture and you cannot do that
without repairing the system a mother
when I said 10 X 400 X I'm not making up
like a worst case example the worst
example much worse something I'm often
seen is that he won't build something
that's like mathematically infeasible it
cannot work and it often be like 2 or 3
years of 10 or 20 people right so
there's a like a lot of money you're
throwing away a few something like that
ok so before we talk of appearance talk
about scale just in the discussion as
people often mean different things when
they say scale I'm gonna be very
concrete since we're talking about
search let's imagine we're searching
possible purpose sizes of ten thousand
ten million and ten billion documents
and let's assume that each document is 5
K these numbers are totally arbitrary 5
k / doc you've done on the web a little
too small if you want email is a little
too big but you can scale this to
whatever corporate size you have and by
the way the actual specific problem I'm
talking about is you have a collection
of documents and you want to to answer
and queries that is crazy the form I
want this word and this word and this
word so maybe you want large and yellow
and dog you can oh we could also handle
or queries and cannot add nuts in this
is relatively straightforward but it
makes a problem more complicated so I'm
just gonna talk about this today okay
so let's say we're looking at 10,000
documents in each chuckling is 5k you're
gonna this is like maybe you know email
for most people or like one web forum
for like not really large web form so
you gonna go this is like a local search
problem professor it's like one person's
or one users documents this is like 99%
of all search problems right so if you
have 5k x penque okay x okay that's a
million right five times ten to fifty
it's 50 Meg's is it pretty small if you
want to put this in ramp you can put the
ceramic like anything right like today
you can go to Amazon and for $50 by
phone with 1 gig of ram right I think
it's like hundred bucks if you don't ads
but still pretty cheap right and so if
it fits in a ramp you can think about
running some naive algo and right you
can basic just scrap if you want
something more concrete you can think
about this as look at every single
document you have and for each document
look at every single term and since
we're having an queries we just ought to
make sure that all the terms we care
about are in the document if it is you
add into acute matches and then you have
your answer right so pretty
straightforward okay so ten thousand
documents we can basically just use grep
and that works fine what about 10
million documents so 10 million
documents you guys like this is like
roughly Wikipedia size we computed a
English language is like 4.9 million
documents and 50k times 10 million 50
gigs right so 50 gigs this will fit in
Ram maybe on a phone you can buy like a
really weird laptop that'll do this but
we play on a server because it's pretty
cheap today right for two thousand
dollars you can PI server with her
pickings around and this is a retail
price by the way for large companies
they buy a bunch of servers as much
cheaper than this so this works we can
try a grapple of them right the base
little work we have one problem the
problem is this this server let's keep
server has about 25 to section bandwidth
so if you have 50 gigs / 25 gigs per
second right oh that's 2 seconds during
this 2 seconds
reasoning all the bandwidth in the
machine so the machine is totally hosed
you can't do anything else you can't run
other queries in parallel you can't run
any other service right this implies it
takes 2 seconds to do query or 1/2 query
per second we're half QPS if we dedicate
the whole machine to this so it's 2
seconds of latency ok and it's half QPS
ok it actually depends a lot right for
many applications
two SEC's latency is fine right like I
know lots of people they have you know
some dev tools of the company maybe I've
logged search or something like that the
run a query takes a second or two I mean
obviously the faster is better right you
want it to be faster you can live with
all right you can deal with it I have
key PS is the same right imagine of a
company of like 10 20 Deb's and they're
just typing queries it's very hard to
the same 1/2 q PS query rate yeah no
automated queries just like people
typing between 10 Dems right they could
do it if that was a full-time job but it
turns out devs don't only query logs
right they have other things to do so
this is actually okay for a lot of
companies oh when it's not okay is when
you have a larger service because
latency is directly to money always have
been widely studied so Amazon Google
Microsoft they've all studied this and
they've all found the same thing
Amazon found that for every 100 seconds
of latency if they add sorry 400-mile
take this latency they add they lose
slightly more than 1% of revenue Google
at one point found if they add 500
months to complete in C or half a second
they lose 20% of their users this is
also true for smaller companies mummify
it looked at this this is a small
company maybe never heard of them they
found the same linear relationship they
found for every butter most of latency
they added this is on mobile they lost
one 5% of revenue for them that's only
$300,000 so it's much smaller but I say
only a $300,000 rate that'll still pay a
full-time salary like dev soy dev salary
full-time for a year right so even if a
dev working for a year can reduce your
latency by heart mo seconds this is
often worth it'll pay for itself okay so
here's a search query result this comes
back in 0.6 seconds right this is pretty
common most search results will come
back and half a second or less but this
search result this is basically you
typing anything at your computer it does
much of stuff goes over the internet it
goes to some servers that does a bunch
of stuff because back over you know it
comes back to the computer
right so it's doing all that and this is
like half a second ish right if you want
to do the budgeting for service like
and you want to look at the service that
is the actual computers that are serving
the query but actually have you listened
documents and return the documents that
matched you actually have a few
milliseconds to do this right so
something like this two seconds is
definitely not okay so what can we do
oh yeah the other thing is right half
QPS this is also not okay I think twenty
two thousand eight it may say rails app
they can handle a 202 PS issue right oh
that's fine for many services but as you
get bigger it's not gonna work for like
Google for being half ups is definitely
not gonna work on so can do we can use
an index this is a pretty old idea
people do this all the time so if you're
in EDX you can get widely varying
performance statistics from thirty to
thirty thousand QPS saying using index
is like saying using all of them how
fast you you get a fuse on all of them
what depends on the algorithm right so
for 30 PS you can see a couple of
references and slides later where people
get 32 QPS somehow in this talk we'll
talk about how to get thousands to tens
of thousands of QPS handling you know
search queries okay
so let's say for now that we have we can
using index of some sort handle 10
million documents on one machine and
that works ok somehow we're about 10
billion documents ok 10 billion
documents 5 k times 10 billion that's 50
terabytes it is possible to fit this
around a machine you can buy a server
that will do this it turns out this is
not very cost effective some mote
services today although instead by many
many cheaper servers so horizontal
scaling right this turns out to be
pretty easy to do for search like one
way you can do this is not the only way
but one way you can do this is you can
take your documents and split them up
onto different machines right so
different machines have different
documents when you want to do a query
you query all the machines all the
result is it's a union of the results
from all these machines so it's a really
straightforward so we said we had 10
billion documents and we said we could
somehow serve 10 million options per
machine right that's a thousand machines
if you do this this will work ok but
let's say you know since our Microsoft
these machines are in Redmond let's save
a customer in Dresden that customer does
a query it adds let's say on our new
through hello second-tier query right so
here we spend all this effort building
this indexed and the result back in
three milliseconds and then we spend 300
milliseconds query across the internet
right that's not so great since I'm
having one cluster somewhere maybe we
have ten clusters scattered all over the
world and so maybe like for any
particular user they only see like 30
milliseconds of latency but from going
back and forth back and forth across the
internet okay great but so one problem
with having 10,000 machines especially
10,000 cheap machines was come on
hardware is machines will fail all the
time like the filler 80 machines is
pretty high else you always have some
traction machine
they're dead and if let's say since we
splitting up hurting by documents let's
say the shirt that holds CNN icon goes
down right I'll usually become pretty
unhappy if they can't query things and
get CNN or Wikipedia back right okay so
maybe we add three times as many Sheen's
redundancy so then we have thirty
thousand machines thirty thousand
machines you have a couple problems if
you have thirty thousand machines one
problem is you have like a super systems
problem unless I find really fascinating
and you're saying but see I got script
let's talk little problem you have is
this constant on trim on your money to
run oh if you say hypothetically say a
machine costs thousand dollars per year
per machine like amortized cost
datacenters and buying machines thirty
thirty million dollars a year and by the
way this number is very low I think very
few companies can actually hit this cost
and is still thirty million dollars a
year at thirty billion dollars a year
let's say you can increase your formats
by 2x right you're in half as many
machines you're saving fifteen billion
dollars a year in fact let's say you can
increase performance by one percent
right you say three hundred thousand
dollars a year so what you can think
about this is you can have one developer
working full-time for an entire year and
all they do is save one percent off of
your performance or sorry increase
reports one percent right it still pays
let's still pays for itself
oh there's this conventional wisdom
right the Machine time is like way
cheaper than all their time so you
should make sure developers as per terms
as possible and not worry about
performance this is sometimes true it's
true if you have like a single rail
server right but it becomes untrue if
you have like a moderate-sized service
well then I call this moderate size
because large companies now often have
services that are not billions machines
at once so three thousand machines and
it's like more than one right but it's
much smaller than it could be all right
so that we've framed the discussion by
talking about scale let's talk about
search algorithms and so the problem
again is you have a bunch of documents
and while a hella queries of the form I
want this word and this word and this
word right okay so one thing you can do
is use something called a list
this is quite standard like volta fish
that use this lucy uses this I think
most publicly available or search
engines would probably talk about use
posting list pushing list is basically
as an index here's an index from the
1600s if you look at this index what do
you see oh you see a bunch of words or
letters a bunch of terms and for each
term there's a list of page numbers I
turn on pure song computers don't have
pages not list sense anyway so you way
you can think about how computer could
do this is maybe you use a hash map and
the hash map is a map from terms that is
the key is a term and it values a list
like maybe a list link of documents
right there probably operators you can
do this but this is like a cartoon view
of how posting lists might work all
right something else you can do is use a
bloom filter before we talk about that
let's talk about like oh sorry
yeah so good funnel this is a system we
have in pink they use the bloom filters
but before I talk about that let's talk
about like how you might like come up
with this idea so if your performance
are you in person you know anything
about search and someone tells you hey I
had this ad structure that fundamentally
is linked lists your product them funny
you say why didn't use like a vector in
our ready oh so what if we use in our
array so you can imagine using something
called an incidence matrix or one
dimension of the matrix is basically
every single term in the world the
dimension of the matrix is every single
document the world and for each entry if
that document contains that term you
have a one otherwise it's the zero
so how big is this and how fast would
this be well one question that is
important for this is how many terms you
think there are on the internet like
real question right like how many how
many terms you think they're on the
internet choose more than one
okay so and also we will start this
thousand or tens of thousands of ways
right so in each document we might only
have tens of millions of unique
documents so how many terms you think
there are in tens of millions of
documents turns out the answer is tens
of billions of terms per shard I think
people often find this surprising
because if you ask how many terms are
there on the internet people often say
well I think this may be like ten
billion right but turns out like if you
pick any number under ten billion it's
actually a high probably that numbers in
the internet so the numbers actually
greater than ten billion right people
often think of like things like you know
names like names of people place names
and words there's like a lot of those
right like hundreds thousands millions
but it turns out there's like a whole
bunch of other weird stuff they don't
really think about like for example are
people bought goods online by definition
there many possible keywords right
people what people put error codes
online right by definition they want
error codes to be unique people catalog
numbers online those are the weird stuff
like people that DNA unlikely people
will scan in DNA flick animal and stick
that online I'm gonna break it like
every eight or sixteen characters if
these documents have like 10 million
terms in them right and they're all
unique uh so one thing you might ask is
you would care about this like why do
you care about these weird documents
that like basically no one searches for
it right it turns out that even though
people never do these queries they do
these queries when you ask them which
search engine is better right if you ask
a person which search engine is the best
search engine what many people do is I
think of the weirdest words I could
possibly think of right so ballast is
like oh you know it's weird I put this
DNA up online can I search for AACC ztg
or whatever and then they're like oh
only one search engine returns this this
must be the best search engine right and
if you look at their search logs they
never do this query right but when they
evaluate search engines they actually do
do this query so we have to actually
handle this stuff even though it doesn't
really matter in any sense right like
what are the billing places like this oh
but it's all actually matters a lot if
you actually want people to user search
engine so anyway um if you have an array
sorry you give it a raid and you have
tens of millions of terms per shard
let's say 30 billion right imagine use
an incidence matrix right let's say you
have an int in each entry 30 billion
times an int that's like maybe 8 8 bytes
that's 2 or 40 gigs right and that's
just for one document that's not for the
whole index right so it's clearly not
gonna work one thing you can do is you
can use a bit vector so instead of
storing 64 bits maybe you saw one bit
for each document but that's like a
small constant factor improvement so the
thing you do is you can hash write most
documents do not contain both terms so
instead of having a unique entry for
each term you can hash this down to a
much smaller space that's one really
what a bloom filter is a little doom
filter you have heard this before
it is a set data structure and they
represent
set using a bit vector and a set of hash
functions so here we have the term large
all the bits in here that are read are
large in this diagram everything as
white is a zero everything that is any
color is a one and different colors
represent different terms so no way we
take the term large we hash it with
three different hash functions and we
hash the locations 5 7 and 12 in this
bit vector so if what insert this
determine this bit vector we cite
locations 5 Simone shelve so these are
once the number of 3 is arbitrary we'll
talk about how to get some number later
but for now let's just say we have 3 and
if we query this we take the bitwise the
end of these locations so we want 2 &amp;amp; 5
7 and 12 if we've inserted this document
or sorry inserted this term in this
document well set all three use these
bits so the query allsey that's one the
bitwise and will be 1 and we'll get a 1
now we can insert another term so this
all these gold bits are represent the
term dog so dog hashes locations 1 7 and
10 if we query this again we'll
definitely see that it is in the
document cuz we just set all three of
these bits the bitwise and of 3 how much
of one's is a 1 we also try carrying a
term that is not in the document we
could query the term cat so cat here
happens of app two locations 3 10 and 12
oh so if you do the bitwise and of these
you'll notice the location 3 is 0 and we
have to stop there right because we've
seen a 0 we know there's no possibility
even if we end in more more bits that
will get a 1 so we know the answer is
actually a 0 and will include the term
cat is not in the document um
one problem doing filters have is you
can possibly have false positives
imagine we query the term box in boxes
we have not inserted box into this
document a box happens to map to 1 5 and
10 because these bits were set by other
terms we will conclude the boxes in the
document even though it's not in the
document so that sounds bad right you
know what that happen how bad is this
like problem like what's the problem
with false positive so let's assume for
a second we have 10% bits I'd say this
is actually under our control but let's
say we control it such that we have 10%
bit density or let's say we hash two
instead of three locations are just one
location so if the if the document or
sorry if the term is in the document and
we hash the location we'll set that bit
right so they're probably a false
negative is actually 0 because we look
at this bit the biggest site would
include the term is in the document if
the term is not in the document what's
the probability while it falls positive
well since we did not set that for this
term in particular there's a 10% chance
if your temper 10% bit certainty other
that bit will be set just by chance
right so the 10% false positive rate now
if we have two two locations so that one
look
we have two independent locations who
can multiply probabilities right the
probabilities are point one times point
one that's 0.01 or one percent false
positive rate we have four three
locations it's the same idea point one
times point one times point yarn or
point one percent now one thing we have
to be careful of if we have two more
locations and we keep the bit vector the
same length will increase the bit
density right because we have this bit
vectors at some light and we're setting
more bits if we want to keep the bitna
to the same we have to increase the
length of bit vector you notice that
when we do this right we increase the
length of a vector by a linear amount
that was we're using a linear amount
more space we have an exponential
decrease in the probably false positive
right so you want an intuition for a
bloom filter works oh you get to give it
you're paying a linear costing an
exponential benefit right so sense it
work pretty well as things get larger
okay so that's your one document but we
said what are the index of multiple
documents right basically the entire
internet how do we do that for all
documents we can use multiple vim
filters so here's diagram that looks at
ten documents so each column is document
columns labeled a through J and as
before rows of these abstract things you
can have shoes that don't mean anything
in particular um one thing we could do I
was gonna do as we had before and each
document have independence of set of
hash functions and we can hash them
separately however in order to handle
queries in parallel will use the same
hash functions for all documents
understand mapping from terms to Rosalee
the same for all documents and this will
go into a query instead of doing a
bitwise and of one bit at a time for
each document will do the bitwise and of
entire block at a time here a block for
illustrative purposes is ten documents
so if we want to do that query which of
these documents
contains the terms large and dog we only
care about the rows that is the bits
that are associated with these terms if
the rows are relevant so I ended up
doing this operation on the right where
I take the end of these five rows one
five seven ten and twelve because these
are associated with these two terms so
in this diagram the part that is not
grayed out on the right is the part of
the query we've done and the part of the
left is the result that we have so far
if we look at Row one well that's just
Row one right Row one and Row 5 in all
this big F goes away if you end on one
and zero you get a zero if we end in row
seven row seven has been offset but
their bit doesn't get reset right
because we're still ending in one in a
zero so we saw was zero if we end in the
next row row ten you know spit I goes
away if we end in the last row and we
don't set any bits because again you
can't send a bit using an end you can
only clear a bit so we can clear the
document and J has a terms large and dog
and none of the other ten documents in
this block have the turns large and dog
and again each document here is a column
we could also do this query on some sort
of block sunblock that doesn't contain
or where no other documents contain both
of these terms so we do this as before
we want to do this operation Row one is
Row one and notice we end in Row one and
Row 5 we get all zeros so this point we
can actually stop right so as before in
the cat example when we see all zeros we
know that none of the documents in this
block can possibly contain these terms
we don't have to go any further okay so
I said we talked about for once somehow
so how do we do that so we actually have
enough information at this point
almost jessimae the performance so we
need to know is how many memory access
refugee two per block so the cost model
here is doing a bitwise and this is very
cheap it's so cheap that it's basically
free compared to memory access we also
never access disk so we don't do
anything more expensive than remember
access so you can think of like memory
access is the only thing we have to pay
for so we want to know how many America
says we have to do per block
the other thing we want to know is how
many blocks we have so you look at a
modern machine today like an x86 machine
it access memory and 504 big chunks so
you know here we have blocks of 10 just
because you can actually see that but in
bed funnel itself in the end of them we
basically operate on blocks at
hyper-girl bits at a time ok that's
straightforward right because you have
10 million documents in the Wikipedia
example 10 million divided by 512 that's
like roughly 20,000 to a fourth a 20,000
blocks
well then the question of how many
memorizes do we have to do per block I
wouldn't we can do is what works is the
math of this I'm not going to do this
today but it's actually a
straightforward application of
probability it is possibly a closed form
formula for this on something else we
can do if you don't like math I can run
a simulation I saw some 20 lines of code
so the simulation is if you have 20% bit
density and you have a like a set of
terms that appears in every thousand
documents how many memory access do you
have to do per block oh and you have a
set of terms sorry when you do when you
do the query that's 14 rows and by the
way didn't Bing when we do queries we
often end up with 2,200 roasts this is
actually quite common
oh this made me sound strange right
because we said earlier maybe we can map
a sorry for any term we might have to
three different things right so while we
have a query of like a hundred terms
that's sort of weird right that's like
33 or sorry 100 rows it's like 33 terms
so we do a lot of term rewriting let's
say someone queries for the term large
yellow dog not as a phrase but the words
large and yellow and dog but maybe like
they want to know the names of large
yellow dogs right so also we write the
terms - like actual large yellow dogs
like golden retriever or maybe they're
thinking of like Old Yeller but they
didn't remember that right also we also
rewrite this term to other this query to
other terms they might be thinking of so
as
even for simple queries that are four or
five words well we write this and like
roll it through the complicated queries
if you go into this graph this is again
a histogram how many memory accesses we
have to do per block
you know since bimodal so on the right
you have this this set of rows we have
to view 14 memory accesses our set of
blocks and unless you have this smeared
out thing other that's you know some
funny thing let's talk about the thing
on the right first so on the right these
are the blocks were at least one
document in the block contained all the
terms so this corresponds to the first
of the two examples we saw earlier where
we do all the row intersections all the
ends because we never see all 0 so we
don't stop right on the left this
corresponds to the second example we saw
earlier where none of that can't contain
all the terms so as a result at some
what we're doing this with very high
probability will see all 0 somewhere it
gets up early and will stop after some
number Mary memory accesses if you just
look at the average of left this turns
out to be about four point six memory
access per block on the right itself of
c14 if you average both of these
together you get like roughly five a
little bit more but let's just say five
okay so what's the expected performance
from what I said this should be enough
to cope with the performance we expect
again
so we said for this example the wiki pea
size example one machine we have 10
million documents we have 512 bits per
block 20,000 blocks I decided for each
block we have to do five memory accesses
or five transfers per blocks that's a
hundred thousand transfers right okay if
you have 25 because second on this
machine this machine is transferring
things in five bit blocks so it can do
three hundred million transfer for a
second you divide these out right three
or 90 million five hundred thousand this
is roughly 3100 so we expect to get 300
QPS out of the system so how close does
the story forints so this is actually we
actually get if you count for a number
of other factors are really close to
this what I new calculation like this
I'm actually pretty happy to get the
factor of two if I just calculate what
I've shown you here today
there's no turns out for our system
there's a lot of small factors that will
change things by five or ten percent
just interest a time when I talk about
those they're also a couple large
factors change things by a lot I will
discuss these though so one thing is we
also do rankings so the other one
prescribed today all this does is match
it tells you is this document you know
basically in this corpus or not we're
sorry is this term in this document or
not yes or no it turns out this is not
great for web search right like I think
AltaVista basically did this and people
are not that happy with it so we also
apply a relative system ranking of them
that is not that cheap and
we also ingest new documents all the
time although Vista was also I think a
batch algorithm people basically haven't
unmet for 10 or 15 years because when
new news happens right people or do
people do they merely go to Google or
Bing and they take a search result and I
want to know like where do I go clear
about this right so we're always
investing view documents in fact for a
bit photo for the outcome we talked
about today we ingest new documents that
have much higher rate than we handle
queries at I want talk about how to
estimate that but it is the thing you
can estimate and you'll get a result
that is actually pretty close to the
actual rate we ask we ingest documents
that by the way this is actually one
nice thing about this article if you
look at this data structure what is it
right it's basically an array with some
bits we do a query this is a kind of
annoying cause it's like a global thing
write the query wants to talk to every
single document must to see what what is
this term is document yes or no we
insert a document this only talks to one
column right so we can do this at a very
high rate and because we're just setting
bits this is actually easy to do
multi-threaded right there we actually
worry about locks or anything like that
that's not strictly true
but like we can do this in a way such
that it's locked free let me put it that
way okay we don't see one major
optimization that increases performance
a lot we can use my record will bloom
filters so in the example we looked at
today we said that each document has its
own bloom filter this is okay and it
works fine right we simply get like
almost 4,000 QPS which is like not
horrible right but it means that if we
do a query for a term that only appears
in one document we still have to look at
at least one bit per document in fact we
said we look at on average five bits per
document there's no particular reason we
should have to do that and we can in
fact look at a long number of bits per
document instead of a linear number bits
per document if we use her record bloom
filters I just also possible which is
the master list I'm just going to do it
because it's relatively complicated we
don't have time for it there are some
completing issues if we want to achieve
the porns are that we can get a system
there's actually a whole bunch of stuff
you have to do in addition there's one
really really major factor that will
cause just does not work at all if you
build a system as I'm describing to you
today so if you just build the system
and then you look at some random block
and look at 16 rows in the block or yeah
16 rows in the block you'll get
something like this you notice like
column B and column D basically all
these bits are set and column a and
column C are very few bits are set in
fact column a no bits are set what's
happening here is that different
documents have different numbers of
terms so if we want to size the number
of rows so it worked for something it
won't work something else let's imagine
it for example we size enter a row so
that it works with tweets and maybe
hypothetically let's say that's 15 rows
and then this funny DNA document comes
along right it wants it has ten billion
terms it wants to set thirty million
bits so we hash thirty million bits into
50 bits right this doesn't work that
great so we set every single bit which
means that every single
we do it'll return true even though no
one ever queries any DNA that's actually
in this document right so that's
terrible
something we can do to fix that is we
can size this matrix up so it can handle
these weird documents like 10 million
terms so maybe we size that up maybe we
have 70 million rows right that's fine
we won't get any false positives that
way but then we have tweets the sweet
still exists right so maybe a tweet
wants to set 16 bits
I said 16 bits out of 70 million right
so we said we wanted to use a bloom
filter inside of an array and the reason
I want to do this is because it's
relatively dense representation so it's
much more efficient than array
well then we have these columns it said
like 16 out of you know 70 million bits
right that's not that good
so what we do is we sure that as we slip
the index by the number of unique terms
per document and unlike most system for
you only shard when you spill over to
our machine we actually always run in a
charted configuration even running on a
single machine and we have to do this if
you want to get both correct results and
have relatively high space efficiency
okay
so conclusions before I talk about real
conclusions let's talk about some
takeaways you might have that are not
correct one takeaway might be whether
the search is the search is simple right
because I described this all up in him I
said hey we use in thing we use the
production and I spread this to you in
like 40 minutes right so it's not that
complicated it turns out there's a
couple of things this took like multiple
years to implement so it's non-trivial
implement this in a way that is actually
really fast it requires a lot like a lot
of detail work a second if you look at
the code we have in Bing this is less
than 1% far actually far less than 1%
than all the code we have so just
actually a pretty complicated problem
and this is only one tiny part of how
did you search another glues you might
have is the bloom filters have better
than posting list the answer for this
question like which is better is it
depends on our argument for why posting
let's type in bloom filters you can see
the Zobel at L on 1998 as a classic
paper since I like 400 times before I
posting list are better than bloom
filters blink deltas are better in some
cases but when that's true it's actually
relatively subtle like we believe that
for us it's true in the case we have we
have a very right heavy workload that's
easy to set bits it's easy to adjust and
very fast and we also well anyway for
reasons that may be sort of complicated
we believe that we really good
performance compared to the other
indices we use in bing um but it's not
always the case it actually heavily
depends on the kind of queries you get
another conclusion might come away with
is you can easily reason about all types
of performance so here I cheated right I
picked a problem that where I knew we
could recent report ones and get a good
estimate and then you know whatever we
got a good estimate right
but turns out this is not always the
case so some cases where you can get a
good estimate or when you're trying to
estimate the basically average
throughput of the system this is fairly
straightforward to do and like this is
what we did today
you could often do background bloodbath
priest you know pretty simple way in 23
minutes get an estimate informs the
system and then that's gonna take 2 or 3
years to build so that's one thing
that's like easy to do something that is
slightly hard to do but still possible
if you want to figure what the latency
of an unloaded or sorry according to be
on an unloaded system this is also
something you can do and it's a little
bit harder but it is still possible to
do with just basically arithmetic
something that is very hard to do if
you'd like to estimate the latency of a
query on a loaded system as load changes
oh this is almost impossible this is in
fact so hard that when hardware
engineers do this actually build a
simulator to submit the system because
further problems is considered
mathematically intractable to do this
with this kind of math in software you
don't have to build a simulator right
but you can build a prototype Oster
simulates the memory access patterns
just access questions you'll see and
it's what people often do if they want
to guess like sort of at a latency that
is the latency of let's say one the
worst on where it can be thousand
queries
okay how about actual conclusions one
you can often reasonable performance and
it's relatively straightforward right
which is hit arithmetic we just did
multiplication in division we didn't
even add right and multi-engine and
these are the same right so we just
multiplied and we got a pretty accurate
estimate from porn so a system they took
both years to build yeah thanks all
these people help they're all great if
if you want to know more about the
system you can go a bit fall under org
we have some design Doc's reporting up
online we're talking about sort of what
we're doing today I can also see some
source code it get down become slash
people's epiphan all I'm told that like
you have to increase yourself or
something
so I'm Dan Lew I write a blog at family
comm Ralph and talk about this kind of
stuff that is off stock up performance
data structures and trade-offs between
different algorithms all right thanks
for your time
oh yeah I'm not gonna take questions cuz
I feel like I don't know it like whole
people how some people feel like I have
to stay for questions even though even
if I'm interested I sorta don't like
that so I'm having a question of the
hallway or about taking questions on the
change okay great thanks your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>