<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Particle physics, 10,000 times faster&quot; by Jim Pivarski | Coder Coacher - Coaching Coders</title><meta content="&quot;Particle physics, 10,000 times faster&quot; by Jim Pivarski - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Particle physics, 10,000 times faster&quot; by Jim Pivarski</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jvt4v2LTGK0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so I'm here to talk about particle
physics and accelerating it to use a pun
okay so so let me begin with by setting
sort of the context particle physics is
what I call the most industrial field of
academia sort of this tension between
academia and industry it's it's sort of
in both because the goals are academic
you know it's to explore strange new
phenomena seek out new particles and new
interactions but the scale is industrial
so we billion-dollar Hardware planning
on decade old timescales millions of
lines of code it's it's a big deal it
has to work it's big data this summer
CERN just passed a 200 petabyte
milestone but it's not that big because
so this is a picture that I show my
colleagues when they are thinking that
we're sitting on the the world's largest
data set as we traditionally have been
that this 200 petabyte milestone which
is you know the history of particle
physics at CERN up to the present is for
some web services to trucks you know it
would be a road trip for them to take it
to some data center but on third-hand it
will be getting bigger so there's this
high luminosity LHC project coming up
that we all know is coming this is today
and this is how the data is going to
scale we will be getting into the
exabyte range and we have to somehow
figure out how to do that without buying
orders of magnitude more computers
that's not on the table so another
aspect of this is that our software
developed outside the big data ecosystem
so on the left are all the software
tools that we're very familiar with and
we work with a lot and on the right is
this this big data ecosystem and I'm
including in that scientific Python and
the machine learning and stuff all that
and
really it's not outside but before
because a lot all these packages on the
left they have these long histories that
go back some of them into the Fortran
era most of them are don't know if we
can say certainly a lot of them are are
decades old and they need to be
maintained but this big data thing
happened while we were building our
experiment actually let me put in a
little bit of context the the experiment
that I work on is has just it has been a
collaboration for 25 years it has
existed and people have been planning
how we're going to do this cuz we're
going to do this experiment for longer
than say Amazon and Google have existed
so this big data thing happened while we
were building our experiment we'd like
to take advantage of it but you know we
can't be faulted for not not having
known about it so now when we're trying
to combine these two worlds the
obstacles are not just accidental it's
not just artifacts of Technology choice
such as C++ in the particle physics
world and Java in the Hadoop spark world
which are rather hard to to fit together
but that's just an accident
there are also essential qualities that
the current big data systems don't offer
this is something that I've found
through through these investigations
however it's not I'm not going to cast
in a negative light this is definitely a
very positive thing because you know
it's an opportunity on both sides of the
divide it's like to alien civilizations
that evolved in different planets we
have a lot to learn from each other we
have different kinds of problems mostly
and we in isolation develops different
solutions and we can see what what
crosses what we can both use so then the
question what is unique about the
particle physics data I've just said
that it's not the size because hundreds
of petabytes are are not unheard of I'm
going to argue that it's the object
complexity
so for instance this picture would
represent one row in our data table
right all that stuff that's going on in
there those are all columns in some sort
of some sort of table and definitely put
quotes around that because our data are
not stored in in database tables they
are stored in files and this this has
some downsides naturally we don't
benefit from indexing query planning
high-level query languages all those
things that in some fields are being
second nature every data pole is a
custom C++ program written by the data
analysts themselves accessing lists of
files taking months while the user the
data analyst chases down failures and
stragglers
but even if even let's just say that we
took all that data and we did put it
into some kind of database systems some
relational or no SQL whatever the things
that are on the market now I've been
looking at this I'm realizing the first
thing that we we do if we had all our
data and this is we'd have to pull it
out again because it just doesn't fit
it's not the right shape so to speak why
is that
well our data has a unusually complex
structure it's deeply nested and
cross-linked now the the nesting
nowadays is not such a problem drill
parquet and arrow our Big Data projects
that not only can handle nested data but
can explode them into columns for spat
for fast sparse access like we need you
might want only just a couple columns
you want to be able to read just those
columns and not the whole thing we can
do the big data world can do that with
hierarchical data now we've been doing
it for longer but that's okay well
because we needed it actually I should
say that was driven by need also we have
cross links or pointers
now you could support this with a graph
database it wouldn't be exactly how
graph databases are usually used and I
think that this is even less of a of a
driver of difference here's where the
here's where the difference really is
the operations that we perform make
intensive use of that structure in ways
that would be really hard to use
traditional databases for so we
frequently need to search through sub
lists under constraints optimizing
paring iterating over combinatoric s--
all within one row of the data table
even the simplest particle physics
search criteria if you were to put the
data it data into a traditional database
would you require also to explodes and
tags and joins in SQL and I'll show that
to give a sense of the problem I'll walk
through the steps in analysis so we
start at the beginning we build this
detector looks like that it's got all
these channels and then you put a
particle collision in the middle of it
and you get pictures like the one on the
right the particles collide in the
center and then a whole bunch of new
particles come out and the new particles
go through the layers of the detector
activating them at various points and
then you read it all out and wires to to
make this block so can you see where the
particles went here
let's show of hands if you do and ok
some hands how about now a little easier
to see okay so we the first thing we do
is we draw lines through them I mean a
software to fine to them and then fit
them and they're gonna coarse be more
than one and they can get fairly complex
so the raw data could have been
described as a sparsely filled table
like each one of those points I can't
walk over there each one of those points
is on where it could have been off and
more than that there's some position
measurement involved in that too but but
really you could take each one of those
millions of channels that come out and
make each one of those millions of
channels a separate column in some
database and and there would just be a
number associated with it and most of
them would be blank but the
reconstructed data where those treant
you know how many tracks you have where
they go that's an arbitrary length list
which doesn't fit the the table model
very well at all
more than that tracks are not the end
from tracks to particles let me just
explain what this picture is that you're
seeing on the left is where the
collision actually happened and you see
a whole bunch of particles coming out of
that and those are measured tracks and
then I have a mouseka and then offset
from that you see some other particles
coming out of here and that's because
one of the particles that that started
in the collision walked over here and
then decayed into these things some
three millimeters away and this sort of
thing is is the majority of what we're
interested in where you were for the
most part not interested in all of those
really really long-lived particles that
live for like nanoseconds and and make
tracks through our detector we're
interested in the short-lived ones that
decay three millimeters away and usually
we're interested in the ones that decay
microns away you know you wouldn't even
see them offset so tracks have a
structured association with each other
and those associations are not certain
we have to carry the flexibility through
to the final analysis there are a lot of
them too this is just this is a typical
picture when you when you run two beams
to each other you can get
tens of collisions and then they all
spray out lots of stuff and you have to
undo all of that so let's walk through a
an analysis starting from the from the
tracks and and see where how how the
operations are complex
so suppose we're we there's a particle
called the Higgs that will decay into
twos Z bosons and each of those decays
into two electrons or two muons so
you've got this kind of a an equation so
you've got to look at all those tracks
and you've got to figure out which ones
are the electrons and which ones are the
muons and which are everything else so
you've got all these different kinds of
sub detectors that measure different
aspects of those tracks and that's all
collected into that picture and you can
use that information against each other
to figure out which ones of the muons
which ones the electrons and then when
you've identified them you group them
and compute the mass of the progenitor
that that would have decayed to those
electrons or those two muons or the quad
all four of them and then you can plot
that quantity and if if there is such a
particle the mass values that you
compute will pile up at some value and
then all of them is the
misidentifications that you made pile up
elsewhere so this is a typical analysis
and this is a very simple clean analysis
this is this is it gets it gets worse
from here now having our data as these
objects where there's there's an
arbitrary length list of them it's much
easier to do that than with a flat table
like in this example here to make a flat
table you have to choose that you say
that you're only interested in two of
them and if there aren't two you have to
put in na s and if there are three then
you lost one it's just much easier to
write algorithms over objects modern SQL
can represent this kind of data so
here's how you might create a table with
this kind of structure but then if you
wanted to do that Pig sir
which I claim is easy you have to do a
whole lot of work and I would argue that
this is in no way easier than just
writing a nested for loop we you know as
the data analyst we'd really rather
write that that nested for loop because
this is this is crazy
so I started developing a language to
try to help this you don't have to throw
the the SQL type idea the query query
language idea out but you just need to
extend it more so that we can do more
with it you can add first-class
functions to something like SQL so last
year I started developing a language
that would do this and it was a
declarative query language with a
functional object-oriented syntax this
is what the the Higgs analysis would
look like in it you know you can take
the tracks in your event and dot filter
say we want to filter on and then you
can take distinct pairs and dot filter
those pairs and compute masses on them
this this is the kind of thing that
you'd want to be able to do and this
coming from the on the one side the C++
world this looks very oddly sleek and
then coming from the SQL world this is
looking oddly program program like so
it's somewhere in the middle to try to
help to do both now this language is
great at developing it has opened up a
lot of ideas and I won't be talking
about it for the rest of this talk
because well first of all why is it
great by sri krithik the the language on
problem we have some nice features we
can automatically vectorize some
calculations across objects that was
nice we can do some a hundred percent
compile time error checking you know
this is when it's very SQL like we can
do these things but when I started this
I didn't see any other projects in this
direction and that's and that's why I
was developing my own but now in the
past year I'm starting to see
these things popping up so in spark SQL
3.0 they're introducing a transform
keyword that will allow you to do to put
in you add the it's a functor for SQL so
it's it's coming from SQL type SQL
syntax but it's this kind of idea also
Michael in US region say be after this
talk I recommend you go over there and
see his his talk on data fun which is
exactly this it's a query language with
where you can put in functions meanwhile
we discovered that FEM tickle its
internal data representation which was
sort of part of this this whole big
complex project whose orders of
magnitude faster than scanning with our
current methods so we're realizing oh
this is the key issue it's not about
making a new language it's about data
representation and so maybe we should
maybe we should be barking up this tree
right now we should apply the the new
data representation on its own without
introducing a new language and see see
what that gets us so let me show you
those orders of magnitude let's say that
you want to do something kind of simple
you want to loop over the events and
inside of that you want to loop over the
tracks and for each Trek you want to
fill a histogram if you were to do its
simply if you were to do it the most
straightforward way you have some
flattened track momentum array and you
just write a minimal loop over it you
can do that in 250 megahertz
you know with the O 3 flag on obviously
and as our current framework in the same
units 0.0 and 8 its enormous the
difference and is that just a mistake
well no if you slowly add in the various
things that our framework is doing you
can account for all those order met
orders of magnitude it's all there
to be fair it's because our framework
was not designed to do that our
frameworks was not designed to run over
all of the the petabytes of data and
make one plot it was designed to extract
a subset of events and a subset of
attributes for the physicists to analyze
locally and locally can mean a bunch of
different things so so one would never
use the current framework to do this
because it's too slow but then it's slow
because no users have asked for being
able to do this so the current jobs to
to to select these subsets and bring
them down can take weeks or months and
one analyst said 1.5 years was his data
collection phase and then you did as
analysis afterward the data analyst has
to manage sets of files and chase down
failed jobs this is part of what adds to
the time repeating the process is so
time-consuming that analysis groups
hedged their bets by requesting more
data than they're sure that they'll need
because you know you don't want to be
stuck in a situation where your local
subset doesn't have something and then
you have to do this all again so the
process is slower and the download takes
longer go to one write this is like in
the optimization point so really we're
talking about a change in behavior
between one optimization point which is
big download work locally and another
optimization point which is small
operations on a shared resource so for
this new style of analysis workflow to
compete and new to us the responses have
to be rapid enough for end-user analysis
they have to be able to make their plot
make a decision make another plot make a
decision and the interface has to allow
for algorithms on nested objects and
this is why we can't just use an
existing tool because they don't do that
unless and if I'm wrong tell me
afterwards so now we're trying to
develop this kind of a system or find
such a system
and what was the key idea with this sort
of the takeaway from the four orders of
magnitude thing was to leave the data in
columns I know this is a common thing to
do in databases but it's just not
usually done with objects so leaving the
data in column so now we actually we've
been still like I said earlier we've
been storing data as exploded column
since the 90s because it was necessary
in order to have this sparse read and
we've been doing this in a method that's
similar but different from Apache
parkade because of course we're aliens
we haven't I've never spoken to them but
we also one thing that we should be that
would be new is we also shouldn't be
spending time materializing them as
objects so to show what that looks like
suppose that you have data like this
there's a list of Lists now a way to
store this kind of hierarchically nested
data as columns you could do it like
this and let's let's focus on this for a
little bit because this is understanding
how this works is kind of key you take
the data and you just put it all in one
array flattened across and that's that
array is what you're going to be able to
run over very quickly by vectorizing
calculations on it then you have to
you've just thrown away the the nested
list structure to keep the nested lists
you have to make offset arrays or
alternatively counts of how big each
list and sub list is
yeah so these are these are offsets and
they are cumulative sums of what the
counts would be so you can just stare at
that and see you know works for a couple
examples now the user wants to be able
to write something like this this is the
absolutely the most natural way to walk
over this if you're going to be
computing some some Higgs mass or
something now we shouldn't here's the
thing we shouldn't create the lists and
sub lists because that's a performance
hit instead we should take that code
instead of bringing the data to where
the code is we should take the code to
where the data is we take that code and
instead execute array lookups that look
up the values in the array so in a sense
we sort of partially compiled this
against where we know that the data is
actually located and in fact for this
particular example because it's a for
loop of a for loop and you completely
exhaust them you could just do a single
loop over the inner stuff and you know
how to do that by nesting these offset
arrays now that data representation is
not anything new the data representation
is how Apache arrow does it and the code
transformation can be automated so let
me show you some measurements in a real
system I'm using Python as a stepping
stone toward them to code by
transforming Python object references we
can turn it into nothing but arrays for
number crunching and then we can take
number which is Python to LLVM compiler
which only knows about arrays and
numbers and is only particularly good at
optimizing code like that but now we've
given a code that's all just to raise in
numbers and it can go to town so we'll
give it something which is actually
computing a mass it's a function for
loop for loop and we're computing a mass
and notice here that the first one is a
loop over all muons and the inner one is
a loop over not the same muon
so we're doing distinct pairs which is
something
would naturally want to do then we
translate this into array references we
pass it to some function that will first
do the code transformation Python asked
to Python asked and then give that
Python asked to number and let it
actually compile it and now we have
astronomical speed ups for this Python
code relative to C++ code in our current
framework C++ code is down here at just
a little bit more than a megahertz and
then the this Python code is up at tens
of megahertz in fact these are four
different algorithms the massive pairs
is the is the slowest algorithm and if
you dig into why is it slow it's because
we're computing cosine and cosh we're
actually at the level where that matters
we've never been at the level where that
matters so we'll start putting in
look-up tables for for trig functions
yeah that's great couldn't be happier so
general code transformation is hard for
all types of data that would be it would
be an enormous project so let's instead
focus on a minimal set of type
generators
let's first sit back and think what is
the least you can get away with and this
is what I came up with primitives you
need fixed with numbers boolean's
characters you need arbitrary length
lists of another type and this is the
hard part the lists unions and records
you know what records are their
structures they're the product types the
unions of the some types and actually
for the particle physics data they don't
really even need the unions I just
wanted to have it be a complete type
system if you have product types really
not have some types other common types
strings and such you can construct them
from these so the code transformation
only really needs to know about these
these these basic kinds of types and
anything else that you do on top of that
well it's it's after the code
transformation
I have a project where the github
repository where we're doing this these
Co transformations and I would like
people to look at it if they're
interested in get together on that a
relationship of this to Apache arrow the
the way that the primitives lists sparks
unions and records were expressed is a
subset of the arrow specification so
we're doing it the same way that arrow
does it so in principle you ought to be
able to take some Python code with
arbitrary nested loops and and function
calls if you compile those also and make
them run fast on arrow data frames and
that can be very interesting to people
who are using spark and pandas and drill
and Impala and HBase and all these are
the things that are supposed to start
using arrow in the near future see
currently people are most of the uses of
arrow that I've seen have been for flats
tables where yeah you can do pandas
operations it's easy if you want to do
something complex on structured data it
can represent the structured data but
you can't write the code so it'll run
fast well this will help you write the
code so it'll run fast
so anyone interested in that oh oh yeah
oh okay
well I want to talk to you it's the last
thought another way another thing that
we should do about leaving the data and
columns we can also manage the data in
columns so one one of the reasons that
users copy data is to enrich it with
derived features so here let's say that
we've got a collection of lists of
electrons so first event second event
third event and then these are all the
columns you have an offset and then some
data and you want to add Z bosons to
that because you do a search for Z
bosons and then the first event you
decide that the that this electron and
this electron together make a Z boson
the second event you say that these two
do and in the third event you say Oh our
to Z bosons this one in that one
you would like to be able to add that to
the data set and make a new data set
that has that and it would even be nice
to be able to keep the original data set
around you'd like to be able to have two
versions of this data set share most of
the underlying data currently with files
and such we can't do that you have to
make a new file but if we manage the
data as columns if we have if they're
addressed individually if they're in
some object store or something then we
can then users can change the structure
of the data by adding new columns
without copying and then we can just say
that this larger set of columns is is
another data set and the smaller set of
columns the original one so my last
slide conclusions I hope it was
interesting to learn about data issues
in particle physics but I'm really
interested in hearing back from you and
maybe especially you do you have any
suggestions or do you think these
suggestions for me like stuff I haven't
heard of or do you think that these
tools could be useful in your work like
we're doing these things and you'd like
to benefit from them too if you think it
would help you but it needs to be mature
more mature are you interested in
collaborating this is exactly what I
want to do this is why I came here is
because how mighty for one thing get you
excited about this and then also see if
if we can work together so thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>