<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;All In With Determinism for Performance and Testing in Distributed Systems&quot; by John Hugg | Coder Coacher - Coaching Coders</title><meta content="&quot;All In With Determinism for Performance and Testing in Distributed Systems&quot; by John Hugg - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;All In With Determinism for Performance and Testing in Distributed Systems&quot; by John Hugg</b></h2><h5 class="post__date">2015-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gJRj3vJL4wE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks everybody for coming my name is
John hug I work on the vote
project in boston and i'm here to talk
about what we've done in terms of
leveraging determinism in distributed
systems for performance and testing and
i'll give my usual disclaimer that there
are some slides in this talk that could
be 40 minute talks by themselves I can't
go into some of the things I can't go
into tons of detail with here but I'm
happy to talk afterward or contact me
via email I'd love to talk about this
stuff it's it's a lot of fun so getting
started you need a replicated setup
you've got a constantly updating
database server and you want to have two
or more constantly updating database
server what are your options so number
one you could run a primary server and a
replica server pair and you could send a
changelog from the primary to the
replica either synchronous or
asynchronous this is really common you
could allow rights to two different
servers or multiple servers do conflict
detection merge all the rights maybe
last fri twins come up with some
strategy for how you're going to merge
all these rights but this these are not
the choices we make there's sort of the
the kind of existing situations we're
looking for something that's materially
better than these so active active in
theory using determinism if I've got my
state a and i applied deterministic
operation x i end up with state B if
state a is on 11 replica I've got
another replica of state a down here if
I apply deterministic operation ax down
here to this other copy of state a I on
up with a second copy of State be moving
this one step forward if I've got state
A+ deterministic operation x + y + Z i
end up with state C and if i do that on
my second copy of state a i end up with
state C and I can extrapolate this out
as far as I want so in practice what you
could end up with is a client that sends
operations a B and C could be multiple
clients but just simple here into a
coordination system which says alright
the order is going to be a B and C for
these deterministic operations and it's
going to send a B and C ordered two
replicas first replica it's going to
send
a B and C ordered the second replica
those replicas are going to in parallel
perform those deterministic operations
and I'm going to feel confident at the
end that the replicas are going to have
the same state so this is an active
active all rights are done to all
replicas of the data but I don't have to
worry about conflict detection last
right wins kind of stuff and even more
simple connect for good Plinko bad so
the ABC thing here is a logical log
right I do the same things in the same
order to two copies the same state this
order is the important thing the order
of operations is a log I can replicate
this log over the network for active
active inter-cluster replication i can
also write this log to disk so that if
something goes wrong and i want to get
back to where it was i can replay it
later in deterministic order and get the
same state I had before I crashed I can
even send this log over the network to
another data center maybe across the
country replay the log there and end up
with the same state volte be does this
both of me does this both BB does this
except we're don't do this anymore and
I'll kind of cover why in a little bit
but a little detour on how to get away
with this you need to be aware of
sources of non-determinism in your
operations the most obvious ones are
random numbers and wall clock time if
you use in your operations random
numbers in wall clock time you're not
going to be deterministic so same thing
with procedure code if you're running an
operation that's got some Java code or
so Scala code or something and it's it's
using random numbers in wall clock time
you got to be aware that that could be
determinist in non deterministic as well
sometimes record order can be non
deterministic if I've got some twins in
my birthday set and I say delete the
newest person from my birthday set then
I don't know maybe in one case the
record order i'm going to elite person 3
but over here i'm going to leave person
for because their birthdays are the same
it may be a non-deterministic change
external systems is a big one so if i'm
in the middle of my procedure i do it
big
operation may be in the middle I want to
go find out what the weather is and
store that in my system well if I have
to go to an external system to get the
weather maybe when i'm replaying my log
10 minutes later the weather has changed
i'm going to get a different answer
maybe the weather service is going to be
down and i'm going to get an error
message all these things introduce non
determinism if you need to talk to
another system you need to do that at
the client side or outside of the scope
of the deterministic operation that
you're logging and this is something
that applies if you're using like spark
streaming or some of these other non
deterministic no side effect kind of
processing systems there's some
interesting things we've run into it
volt non-user source of non-determinism
obviously bad memory is one of them if
you're trying to do these things with
without ECC memory you're probably going
to have a bad time interestingly there
are libraries out there that for
security purposes randomize the orders
of things that they do they're trying to
avoid hackers like figuring out that
you're doing the same thing over and
over where memory lives but these also
break your non determinism there a lot
of times people complain when someone
introduces like this the security people
say yeah the data people space people
say boo so something to be aware of so
if we've got these sources of
non-determinism how do we guarantee that
it's not going to mess up your app how
do we protect you from from the user
accidentally introducing non determinism
well one of the things we've done at
bolt is we've made sure that our sequel
is as not as deterministic as possible
so the sequel planner understands what
is deterministic and what is not we've
made a hundred percent of our DML that
is our rights equal or inserts or
updates and our deletes deterministic
there's no way for you to write a
determinate a non-deterministic DML
statement interestingly there are ways
to write non-deterministic read
statements for performance reasons for
memory size reasons there are a couple
of reasons why it's nice to be able to
do this from time to time and we've
decided not to make all sequel
deterministic however we do know that if
you've got a transaction that's mixing a
read and write we will slowed we will
plan the read in a deterministic way so
that you can't take a non-deterministic
read and feed
into a right so we've made our plan are
aware of this stuff so that it can make
the right trade-offs to make it very
very difficult to run a
non-deterministic transaction there are
sometimes as a result of the first
bullet point where we can't actually
guarantee it's not a terminus tick and
we will warn you when you load that
procedure code we also offer
deterministic api's we will give you a
seeded random number generator that has
that has seeds that are going to be
deterministic will give you the logical
time that your transaction is actually
executing as opposed to the wall clock
time which you know you shouldn't trust
your system wall clock time anyway we do
this other weird trick to to avoid
divergence so see I've got this
procedure foo and this is fairly typical
for a procedure I've got some logic some
read sequels some logics and write
sequels and logics and read sequels some
logics and rights equal turner response
the right sequel is the only thing that
can modify state so what we do is we
actually hash up what the rights equal
you asked us to do was as well as the
parameters any parameters you fed into
that right sequel and it just comes up
with a nice little uh either either you
know you can hash it all up and you get
a single 64-bit number you can hash up
as a as an array of numbers they're all
very lot smaller to communicate that
over a network than a lot of things and
so when we run these things the client
talks to the coordinator it says I'd
like to do this deterministic operation
the coordinator says all right well I'm
going to do it in this order now it's
your turn I'm going to send it to my
replicas and perform that operation the
replicas perform the operation but they
compute the response what you asked for
they make any changes to the state that
you asked for but they also compute this
hash of all the changes to state that
you make that information is pushed back
to the coordinator and the coordinator
can actually confirm that these hashes
are the same these two things did the
same deterministic DML with the same
parameters to the same state in the same
order so if we can guarantee all this
then we can actually we get some we can
take advantage of determinism without
being super afraid we are a little bit
afraid so we've got some belton
checks volt DB is capable of like a lot
of systems of sort of a copy on write
transactional snapshot that's cluster
wide so two replicas are performing a
snapshot that's at the same time then we
can basically take a hash of that as
we're computing it and if the hash is
different we know that there did the
replicas are different if the hash is
the same we know that the same this is
an out-of-band check it doesn't check
lively it checks every time you take a
snapshot which is every so often we
could do more as I hinted before we do
let you write nondeterministic reads
because they can in times be faster and
use less memory we could make that
different decision we could not allow
non deterministic reads it's kind of a
constant trade-off between how often
people get into trouble and how fast
people want to run there's also things
we can do that we haven't done we
haven't done a lot of a like deep code
inspection in people's Java stored
procedures looking for sources of
non-determinism there's a few more kinds
of protections that we could take that
we're talking about and some of the
things we started work on but we some of
this is just engineering effort so
performance why do we do this and
performance is really the biggest reason
one side trip here just to come across
how deterministic operations in our case
these are these are procedural
procedures so if say my procedures make
a pizza and the parameters are I want a
large pizza with ham and olives what
that expands to is a whole lot of logic
and so one thing is that in the case
where you're doing intensive processing
then then oftentimes the the invocation
of the deterministic operation is much
smaller to send over the network than
the actual intensive prop is processing
in some cases it's smaller than actually
explaining all the data mutations as
well it's a very compact representation
and of of kind of what you're trying to
do so we've got a deterministic logical
log for synchronous replication why do
we do this this is the performance why
slide here we can replicate faster and
when I say faster I should say lower
latency replication we can do both
things simultaneously on two replicas or
on
replicas rather than doing them one and
then the other or doing them on a
primary replica and sending binary
changelogs to other replicas what this
means is that our latency when you add
replication is not substantially change
it changes a little bit because you have
different you know you can have
different worst-case performance and
things but but for the most part if we
have a millisecond latency without
replication you had replication it's
like one point something so this is a
huge advantage if you're trying to build
safe systems that are also fast the
trade-off between synchronous
replication and low latency is almost
gone we can persist faster so by logging
logging this deterministic operation
that is what we can do is we can start
logging when we decide what the order is
most systems do the work and then log
all the data that's changed to disk so
they do the work that takes some amount
of time and then they love the work that
takes some amount of time then they wait
for an F sync to disk because that's
something that's important to do if you
really care and all those things take
time one after another much like the
replication by doing by doing this
logical log we can do them at the same
time it becomes a race I start blogging
I take a batch commit to disk where I F
sink maybe you know ten to a hundred
transactions at once and I'm also doing
those transactions in memory so if my
disk whether I've got a controller or an
SSD is capable of F sinking at one
millisecond F sinking every millisecond
then a lot of times I'm not adding a
substantial amount of latency to my
operation so that's a huge benefit and
as I said before sort of with the pizza
thing this gets us bounded size
operations too so if the client
describes to us a workload over a
Gigabit Ethernet connection or a set of
clients describe a workload over a
Gigabit Ethernet connection logging that
to disk is not going to be bigger than a
Gigabit Ethernet connection which is
actually pretty easy to log sending that
over a network is not going to be bigger
than another Gigabit Ethernet connection
whereas when you get into binary logs
sometimes the binary log might be
smaller than the logical representation
sometimes it might be much much much
much bigger so this is where I'd like
sent up a foghorn kind of because this
is the key performance slide but that
would be rude what
what so per node we're doing 100 to 500
thousand acid transactions per second
there's a lot of caveats and things here
but you know we have customers we're
running at these rates so it's not you
know the caveats are not made-up caveats
they're they're caveats people live
within the real world millions of sequel
statements per second now like just like
what the pizza example the actual work
you're doing in the transaction is not
super correlated with how big the actual
the number of transactions are all the
coordination overhead all the network
overhead is per transaction the actual
work done in the procedure can scale
pretty well so i can add more logic add
more sequel it often doesn't slow down
my system at all it certainly depends on
the sequel we've measured linear scale
to 30 nodes that number is not large
from like a distributed system
standpoint when we're running millions
of sequel statements per second it's
pretty large for a lot of the kind of
operational workloads we're working on
and it's not that 30 is a number that
like we think if bombs after that we
just haven't really tested we've tested
bigger clusters than that but we haven't
really set up the benchmarking
environments to really verify that the
linear scaling goes past that also as I
said before synchronous replication and
synchronous dis persistence we have
customers who are getting a millisecond
or so average latency on synchronous
disk writes with long tail that's really
not looking terrible this is sort of a
boring key value diversion key value
crud is is pretty trivially
deterministic key value crud also the
binary and the logical there's no real
difference between that it from a
distributed standpoint determinism key
value is not that interesting so what
are some of the trade-offs and
compromises we're making by doing this
because clearly there are some right
everything's got a trade-off number one
is it's more engineering work we do a
lot of work on testing determinism on
enforcing determinism we do a lot of
documentation on explaining how to our
customers how to take advantage of these
things what things are possible what
things are not you know that that
certainly takes effort another big issue
running mixed versions when you rely
heavily on determinism is really scary
especially when you're running things
like sequel and the scariest part about
this is that say
we have version 5.6 and someone finds a
sequel bug where we have null handling
is wrong for some random thing like an
acid bug it's a just a random sequel bug
if we fix that in 5.7 then you no longer
get deterministic operation when you run
the same query on 5.6 and 5.7 this is
something we're working on but right now
we don't support doing in a rolling
upgrade between major versions of the
software we only support minor versions
and we're very careful when we fix
sequel bugs because they break our
rolling up great capability that's a big
trade-off not that we can't answer it
eventually it just goes back to the more
work thing trade-off number 3 the non
determinism safety checks right now if
you trip them what we do is we snapshot
you're divergent replicas and we shut
down the cluster so I can say that we
have not had someone do this in
production in over a year and and even
in the case where they in it even in
production it's not not most of our
customers never hit this in production
this is a thing where people hit this
while they're testing people hit this
during sort of the kicking the tires
phase of volte be it's a big trade-off
but it's something that very often once
you understand what what is allowed and
what's not people hit people don't hit
this in production with their systems it
is something we'd like to sort of
address it's a difficult thing to
address based on some other constraints
that I could talk about offline of
people are interested I think the
important thing about that though is
that your data is safe and both copies
that if diverged are safe but your
availability goes away briefly testing
so moving on to the second half of this
talk how do we leverage all this
determinism to test so testing any
distributed system like bolt we've got
to test acid guarantees we make we have
to a sequel correctness integrations
with things like Kafka and Hadoop
performance testing is really important
because it's a big selling point of our
software operational procedures like
restart upgrade all the different
clients we support we're going to focus
on on the acid testing the guarantees
that we make for this second part of the
talk and we're going to really focus on
sort of one particular way we do
to review acid briefly the properties
for databases atomic means that either
all my rights happen or they don't
there's no half my rights complete
consistent is weird this is not the
concede that's from cap this is a
different kind of consistent it
typically means that all my constraints
and my schema have been enforced if I
say something is unique it's actually
unique if I say this foreign key joins
this foreign key than it does isolated
means there's no interference between
concurrent operations and durable means
that once an operation commits the
concurrency control is going to say no
nevermind actually I'm kidding it didn't
commit isolation levels this is sort of
a break out of the I in acid I'm only
going to focus on serializable right now
all we support is serializable because
because the way we use determinism all
we actually can support is serializable
we have to do one thing after another
deterministically so we this is the sort
of guarantee that we make it just
happens to be the strongest one in the
best one sort of a coincidence but what
we say basically is any pair of
operations must have a possible logical
ordering you must be able to say that
one thing happened before another
there's no overlap allowed it's possible
there more than one ordering if things
don't don't conflict at all then you
could probably order those ops any way
you want but there's at least one and
I'm not going to worry about the others
Google these it's really interesting a
lot of the databases ship by default
with much weaker isolation levels last
year at strange loop will from
foundation to be who's sitting over in
the back here gave a really cool talk on
simulation using deterministic
stimulation to test distributed systems
it's on YouTube watch it if you're
interested it's really cool talk but
it's about how they out the foundation
to be team used a deterministic
simulation a system called flow really
cool to to test their software we went a
different way also leveraging
determinism we have a lot of respect for
what they did especially the the fact
that with simulation you can kind of
exhaust testing all the different states
and your state machines and compressing
time is a huge thing that you get from
some of these simulators
you're able to test many many many more
longer periods of testing compressed
into much lower because you have to use
real clocks we have a lot more states in
both DB than a system that does
transactional key value storage we I
kind of get to some of the things that
we've sort of added to our coordination
model but this is one of those things
where I can talk kind of offline but it
would be a lot of work for us to do sort
of exhaustive simulations and and the
simulation you know I can't find bugs
that are outside of the core state
machines so if you're not if the
simulation tests your core coordination
logic you can verify that that court
coordination logic is valid for all
possible state transitions but outside
of that you've got to do other tests so
for the time being we believe that we
get more value out of the engineering
person hours spent the testing the way
we're testing and we're very confident
in the result one of the side notes of
why we're confident is that the
serializable stuff is actually a little
bit simpler to test but but talking a
little bit here about how we actually
test these things so how do we test acid
we do have sort of a state machine
fuzzing simulation that we built this
says this sort of fuzz's messages
between different state machines this is
a thing that we built when we were
developing our current concurrency
protocol or consistency actual engine
that maintains consistency in volt that
distributes transactions we built this
thing and it was super super useful for
the initial development it hasn't found
a bug in years we could expand it we
could do more work there make it closer
to some of the stuff that foundation
newbie is doing but that's a sort of a
different topic unit tests obviously
smoke tests that sort of try one
particular weird thing maybe one thing a
customer found these are things that all
developers have to run through before
any code is committed to to any branch
pretty much but what we really have here
that we get the most value out for
testing acid is a self checking workload
so how do we leverage the internal
checking that bolt does we talked about
how it does it does hashes it does check
sums of the right operations you're
doing and it verifies that I'm making
the
same operations to two replicas at the
same time in the same order can we
leverage this so if you read a value in
sequel we don't put that in the hash but
if then I write that into the database
that information goes in the hash so all
if I want to verify that I'm read
something correctly all I have to do is
write it so we have this testing
workload sort of when in doubt right
into this right into state everything
written to state is going to get self
checked by the system so our plan is to
build a nefarious app and our current
app is called transaction ID self check
to the first version of this thing
self-check one was written to test that
volt could guarantee unique transaction
ID ease for each when you when you asked
our API give me the transaction ID and
that that transaction ID would always be
the same across replicas without
coordinating itself checked because it
relies on this determinism checks that
we built into the product and two is the
version of it that once I saw one of our
engineers needing built this transaction
ID self-check thing we said well we can
expand that and use it to test a lot
more things this is sort of a brilliant
idea and so too is the second version
that's more nefarious and it's never we
don't have a 32 has just been added to
over the years so what is an app that
test for determinism look like I'm going
to do acid again I'm going to go through
the four properties and how we test
these things but I'm not going to do in
any particular order because it's not
the right AAA is not the most
interesting one to do first so I for
isolation say i have this transaction
right i read a value x i increment X and
then I read X again for verification if
i run this concurrently right i have a
client just flood the system with these
things then if any of them are stepping
on each other's toes it's possible that
i could read the second time of the
wrong value of x maybe X is an X plus 1
the second time I read it right so so
this is a system that could certainly
doesn't guarantee that it's going to
tell defined isolation bugs but it's
capable of detecting isolation bugs
using our software for atomic I can do
similar kinds of things
I've gotta read value X increment X
maybe I want to abort and roll back here
right increment X read value X maybe I
want to abort and roll back there read
value X for verification so this is an
expansion of the isolated procedure but
it rolls back and one of the interesting
things here is that X should never be
odd that's like a new constraint that I
can check because this thing always has
two increments in it either completely
rolls back or it completely happens if X
is ever odd then I know I've failed a
constraint d for durable academically
you can argue about whether the d in
acid it has anything to do with disks i
personally don't think that that's what
they were talking about but everyone
thinks d means disks so i'm going to
talk about disks build the data verifier
that can inspect the state and give a
thumbs-up or a thumbs-down at any time
we have that build a committed to Patek
and tuples may be the wrong word
committed transaction checker to verify
that constraints are met on recovery and
I've got another slide that sort of
talks about how we built that the last
one is consistent and as I said this is
sort of make sure your values your
unique make sure that your foreign keys
Australian we don't even support root
foreign key enforced in volt DB this is
mostly tested for us through unit tests
and through sequel the other things that
we do for sequel there's a whole nother
talk on how we test sequel so I'm going
to sort of punt on consistent for the
rest of the talk so we started for
isolation with this transaction over
here begin read value acts increment X
and it might find bugs but it might not
your kind of crossing your fingers and
hoping for a race condition so how can
we make this workload nastier so that we
can have we're almost certainly going to
have a race condition so here's a simple
thing we could do instead of doing it
once do it ten times right make the
transaction take longer am I making it
take longer the odds that you're going
to interfere go up the other thing is
you get a neat little constraint here
just like the other one had to be odd in
this case you need needs now your value
of x needs to be multiple of ten but
maybe we want to do something else we
could set X equal to the hash of X right
so this is weird where may
be you could have two things that sort
of by chance get lucky and cancel each
other's out if X is hash of X maybe you
don't and that's sort of an example it's
simple but we can get more and more
crazy here why not have two values x and
y set y equal the previous value of x
said x6 hash of x verify that both are
true all of these things are sort of
moving in the direction of building a
transaction that's more likely to find
vaults in your system so the actual
thing we run which I'm going to describe
a simplified version of next is even
more complicated than the simplified
version i'm going to say we've added to
it over the years as we sort of found
new we've extended our coordination
system new things and and it it's a very
very nasty transaction but let's take
sort of the first version when we first
rolled out transaction ID self-check to
here's my schema for this table T I've
got a client ID so what I've got is I've
got thousands of clients lot logical
clients and each of them is is
asynchronously dumping work into the
system each client has its own partition
its own little world in the database
that has an ID so the clients don't
trample over each other their operations
do because it's dumping asynchronous
work at us but it's many many clients
all separate and parallel so for a given
client ID I've got row IDs so I can have
several rows for a given client I've got
a transaction ID in a time stamp and I'm
gonna get to those a little bit next a
previous transaction ID you can probably
see where that's going a hash of all of
the rows at the time of insertion that
were that belong to this client and a
counter that we can increment because
counting and distributed systems is
apparently hard so the procedure
simplified pseudocode version looks a
little bit like this and i kept the
schema in the corner pass in a CID pass
in a row ID and the real ideas is
increasing the first thing I wanna do is
I'm going to select all of the rows for
this CID and I'm going to order them by
the row ID descending so the newest row
before this procedure is called is at
the top
the transaction ID in the time stamps
are sort of things i get from vult I'm
verifying by taking those and putting
them into the database by writing them
that they're deterministic across
replicas that was the original point of
this app the original app just had that
one line transaction ID getzville unique
ID dumped into the table previous
transaction ID is the the the most
recent transaction in order stored into
the systems this is another case of read
it and then write it the all hash is
basically hashed the result of that
sequel statement at the top into some
64-bit integer we're going to write that
into the database so that's going to
find phantom reads and other things like
that if I've got too many rows if I've
got not enough Rose if I've got
different views from somebody from two
other people it's not necessarily that
more than a right check it checks that
they all see exactly the same state in
the same order across all replicas the
count is just a counter so whatever the
the counter is in the last row I
increment it by one and I write it into
the system I insert that new row and if
it's greater than 10 I don't know if 10
is actually the real number but if it's
greater than some fixed number delete
the lowest row and note that you should
always be deleting one row you shouldn't
be you know defensively when I rights
equal I would always say well delete all
the rows past 10 and I would assume that
that's just one in this case we actually
bet you know better have 0 or 1 row 0 it
start up and then one rose for the rest
of time to make this a little beefier oh
sorry I so I can run this concurrently
is important right from the client I
need to send many of these at once it's
not one and then the other per client
each client just floods the system with
these things before I get the response
for one I keep sending others so
procedure P way I can beef it up I can
add us should roll back so this contest
its atomicity I can also add sort of
constraints one one simple way to check
data I can ensure that all the count
values are consecutive so I can just do
that i can write some sequel i can query
this i can look at this table i can
ensure that all the count values are
consecutive and that basically gives me
an idea that that my data model is more
consistent and i can add a roll back at
the end or at the minute middle it
shouldn't affect
anything constraints on this so just
from this the things I can check for
there's a maximum number of rows per
client all rows for a client have
consecutive row IDs encounters time
stamps follow row ID order they're
always monotonically increasing if the
row a is the row right after row B then
the wrote a transaction ID equals the
row where's the Robie transaction ID is
the row a previous transaction ID I know
if I got that right but you clearly you
should know who know what I mean and
then also test that the volt EV api's
return the same value across replicas
and replays so I say that the real
version is a lot more complex than this
some of the things that we've done we
run this kind of procedure on all
permutations of partition data on
replicated data mixing between
partitioner replicating data we do this
in transactions that are that our
partition so they go to one particular
machine and run through like a pipeline
we do this to global transactions that
include data from more than one table in
different parts of the system we add
huge blobs to the road this was
something that we did one thing we
notice is that the first version of this
the data sizes are pretty small and so
when we fail the system and we recover
it it would recover almost instantly
which is not interesting so we add
really really big blobs these rows
sometimes that's a tunable we can do it
with different sizes we had tables that
all they have is just 20 gigabytes of
data and all these things what they do
is they slow down your recovery and they
make it more likely that you're going to
hit race conditions we've added mayhem
threads if they say naming is hard in
computer science that that send ad hoc
sequel these are not transaction big
procedures that have logic in them these
are just single sequel statements that
read something and write something and
we have verifiers for the mayhem threads
that what they're doing makes sense so
that schema gets extended quite a bit
with lots of other information so that
we can do cross table operations we can
have values in that schema that get
modified by the ad hoc sequel statements
we've added joins and dimension tables
this found bugs this was added after I
stopped working on this project
sure what the bugs were but I'm sure
they were gross and horrible so this is
all on github this transaction ID
self-check to code is in volte be test
test apps you know a little bit buried
our repo but it's all permissive lee
licensed all the things we actually run
everyday are in here along with 34 other
workloads that we run not all of them
every day but frequently some of them to
design to test different things this is
sort of our go-to but do we have a lot
of different sort of workloads that we
do different things environmental tweaks
we certainly run one of the things about
having this nice workload that we sort
of can run we run it on different
operating systems different java
versions different vm hosts different
clouds we have lots of different
hardware we've heard of a closet of
weirdness at volt and we make sure that
we run it on lots of different Hardware
lots of different physical networking
devices is important because we're
running on top of a system because we've
got control of everything we can use
lots of fault injection tools to inject
faults in to this system so this is more
than just you know yeah we can kill
servers or kill multiple servers yeah we
can kill the whole thing and bring it
back this is things like we can inject
weird latency or we can just slow down
latency we can inject disk faults disp
latency into the system and all these
things help us find find bugs with with
fold and certainly again mixing these
things right you don't just inject
latency you inject latency and you fail
notes and you don't just fail one node
you fail several nodes so I talked about
the the committed to pool checker what
we do to do this each client and there
can be thousands of clients knows the
last cent transaction and the last
acknowledged transaction so it's like
the Oracle here when we're running with
synchronous durability the checker makes
sure that anything that was confirmed of
the client actually got written so its
startup it sort of just goes in this
mode where says I'm checking it makes
queries against the system it basically
looks will give me all the rows for my
from I CID here's my cid it comes back
and says yeah all the ones that i
expected to be here here maybe some
additional ones are here that you didn't
tell me about that's fine in terms of
acid but at least all the ones you've
committed to me committed are there when
running eight with ace
synchronous dis persistence and we fail
the whole cluster either you know all at
once or by just killing one too many
nodes we have to verify that few one of
our two bounds either fewer than a
specific number of transactions are
missing and or a specific window of time
is missing for this so one of the
catches here is there's lots and lots of
clients and so for all of them the total
missing transactions has to be whatever
you've configured your volt to run hat I
think by default it's like 200 or
something but the total has to be less
than that number so we have to sum all
of the missing transactions across all
the clients that one we can at least do
definitively the roughly specific window
of time we just do a best effort because
suddenly we're dealing with wall clocks
if the customer says that I want to lose
no more than 200 milliseconds of data we
do our best to make sure that we're in
that 200 millisecond ballpark it's not
you know given wall clocks aren't
terrible we're not going to be like 400
milliseconds off but if we miss one or
two that would be hard to tell with that
but typically if you're running a
synchronous and you've got 200
milliseconds missing one or two
transactions is kind of the point and
this is a lot of work this took a long
time and it's it's been very valuable
for us to trust our system a big
advantage we get from this anyone can
extend this so i was on a project where
we changed the coordination for bulk
congestion right we did we wanted to
make something sort of specific hooks
for bulk congestion so that we could go
a lot faster and we use this now for all
of our CSV loading or Kafka loading all
kinds of things and it makes things
faster we basically added threads that
use bulk ingestion into transaction ID
self-check to and then we added other
threads and made sure that other
processes were intertwined with that
those rights so we have those read all
the values and write those values to
other places from the bulk loading the
nice thing is this found bugs during
development and it found bugs
immediately so it's like we added a
transaction ID self-check to we ran it
failed immediately we fixed a bug we ran
it again it failed immediately somewhere
else tremendously tremendously helpful
it's gotten to the point where if you
have bugs it's very very good at finding
them and we do all this for all of our
features that affect transactional
guarantees
I'm going to kind of go quickly through
this slide we're rolling out multi data
center active active support and
transaction ID self-check too has been
running for more than a week on this
data so also the last bit I want to
cover before I'm done is a what happens
when you find a bug there are two
classes of bugs there's things that
happen fast and then there are things
that happen slower randomly for the
things that happen fast typically it's
pretty easy to identify in fix we have a
lot of context here we can tell you why
things deterministically fail a lot of
times we have a illogical log on
distances here are the operations we did
in this order which leads to a
reproducer for the for the other things
the things that happen slower randomly a
lot of times we're really glad that the
system found those but they are really
hard to debug and this is something
where we could build better tools to get
better at debugging those things we have
built a lot of better tools a lot of the
work in this kind of system goes towards
how do we find those bugs wear something
somebody commits something and then
maybe somebody commits something else
later and then maybe a few days later it
fails on when you're running in kvm with
transparent huge pages on right those
kind of things are just killer bugs so
we're working on better tools to kind of
figure those things out but at the same
time we're really grateful when the
system finds those things because it's
this really terrifying when customers
find them so I can say in conclusion
sort of hopefully and I would do this
again if I had the chance I think that
we made a lot of really good decisions
but there are clear trade-offs that we
made hopefully you've explained some of
the trade-offs in terms of testing in
terms of performance and I'm happy to
answer any questions about anything else
thank you very much I appreciate you
coming
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>