<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;GC Tuning Confessions Of A Performance Engineer&quot; by Monica Beckwith | Coder Coacher - Coaching Coders</title><meta content="&quot;GC Tuning Confessions Of A Performance Engineer&quot; by Monica Beckwith - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;GC Tuning Confessions Of A Performance Engineer&quot; by Monica Beckwith</b></h2><h5 class="post__date">2015-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Qli_Jttro6Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm Monica Beckwith and I wanted to
wanted to start early because I tend to
hurry up with my slides and because I
always have more slides than I should
have
so it's hard to follow up that person
earlier on stage she was awesome kay
and I hope I'm at least 50% like her if
you missed a slot and miss the
presentation you should watch it today
I'm going to talk about my confessions
my G C tuning confessions I'm a
performance engineer GC tuning is what I
most my most recent line of work so like
I said I'm a JVM GC performance
engineering I've worked right now I'm an
independent consultant I worked at AMD
Sun Oracle and basically most of my work
was on hotspot JVM I've done JVM
heuristics JIT compiler GCSE I worked
with mostly server class compilers so
server class GCS and most recently
worked with the g1 GC you I was the
performance lead for g1 GC at Oracle and
I want to thank you all for being here
for us to fall and thanks to strange
loop for having me here
so so what I'm going to talk about today
is a little bit of a bad performance
engineering it's primarily because
that's something that that's what I do
and I want to understand I want to kind
of broaden the knowledge and performance
engineering and then I'm gonna dive into
the garbage collectors talk more about
the trade-offs and GC algorithms and
though open JDK hotspot GCS and some key
topics and then I'm going to summarize
everything that I just said so
performance engineering it's ideal to
have have performance requirements and
plan and test development happen
alongside the design because you once
you get your requirements you're
starting the design phase it's always
good to have you know think about the
non-functional requirements from the
get-go so what what that helps with is
when your dream performance analysis
you have the design and the
implementation and of course your plans
and everything already so a basic a
performance engineer will help ensure
that the system is designed and
implemented according to the
requirements so when I talk about
performance requirements what do I mean
those are your SLA is the non-functional
requirements your throughput your Layton
sees and your response time metrics so
when I and my response time metrics I
mean not just the average or the
percentiles I also mean capturing and
everything which would include your
worst case and I'm going to provide an
example why I'm saying that and in case
any you know I'm just gonna define what
percentile means it's basically if when
you men you talk about X percentile it
means the value that it's the value
below which X percent of your
observations are found so for example
median is 50th percentile right so here
we have four systems and I'm capturing
all the GC events and I'm sure for this
particular example we had a hot like the
heap was really huge so our goal was
five seconds not to the worst case past
time goal was five seconds not to exceed
past time five seconds past time so when
we're capturing metrics even average
looks really beautiful rain hits three
hundred and twenty three milliseconds at
the at four system for moving on to 99
percentile it's still below five seconds
it's one point four one zero four one
seconds but we missed out if he wouldn't
have captured the maximum the worst case
we would have missed out on the twenty
point six nine nine seconds pause that
system for observed all the other
systems behaved with system for had
issues and then when when I went and dug
up the log and figured out I found out
that there were five full GCS and ten
evacuation failures I'll talk more about
that briefly but something to keep in
mind is capturing the right metrics I
always say you know get the logs there
is no harm in enabling pin GC details
for in GC time stamps or date stamps
whichever one you prefer but please
please keep that enabled in production
because that'll help us debug the
problem so quickly talking about
performance analysis monitoring
analyzing and profiling helps tuning and
there have been big part of of
performance analysis so monitoring your
monitoring system under load you're
talking about utilization you're talking
about the lots and not in utilization
would incorporate the entire system so
you have CPUs IO kernel and actually
memory bandwidth as well so and of
course Java heap because we talked to be
talking about Java right so what about
analysis analyze whatever you just
capturing up there right you're
monitoring those things analyze them so
your time spent in GC you know in GC at
times your CPU logs your memory and
application logs everything please
analyze them and then move on to
profiling when we talk about profiling
we're different with people who have
experience in different fields for
example a JVM person they think about
profiling as application profiling
somebody else like a GC engineer would
think about profiling with respect to
the Java heat profiling right but when
I'm talking about profiling is a
performance engineer I mean profiling
the application the system the memory
the system memory and also the Java heap
so let's talk about JPM performance
engineering quickly it includes the
study analysis and tuning of JIT
compiler the GC as well as sometimes the
JDK and there are so many optimizations
made in into the JIT that went into the
JDK because of JVM performance engineers
and GC performance engineer would
basically monitor the JVM you have tools
like visual VM helps you monitor the JVM
then you can monitor the garbage
collector you have a plug in the visual
GC plugin for online and then you have
the GC logs for offline right and then
what I do is I will develop scripts to
to parse these logs and then I will
visualize them so I will use GC history
geoffery charts but everybody can do
that there are specialist tools out
there their log analyzers out there so
if it serves your purpose use it and try
to dive into you know the analysis from
GC perspective so Asuma now with all
this knowledge I want to go ahead and
provide insight into garbage collectors
and hotspot openjdk hotspot so before
before that I want to share a fun fact
GC cannot eliminate to your memory leaks
your application has memory leaks GC can
help you identify them but it does not
eliminate it for you you got to do the
work and now so what what would be an
ideal GC maximize throughput yes
minimize latency yes and of course the
footprint right you don't want too much
you know taking up too much footprint so
yes but is that the reality no any
software optimization process or
engineer would tell you that you have to
trade off you can at most do to pick two
out of the three but not all three and a
healthy performance engineering exercise
can help you meet so so when you're
trading off you're also setting your
goals that realistic expectations set
realistic goals and then your
performance engineering exercise will
help you meet or exceed your goals
so with these trade-offs in mind we'd go
move on to hotspot open open JDK hotspot
GCS so I've been working as a consultant
for two years almost and before that I
was working with with customers I have
realized that most open JDK hotspot
customers would like to increase the
Java heap but they fear full garbage
collections the first thing they tell me
is that yes it will be awesome if you
can increase the heap of course this
compressed tubes people think about that
too when they're making the jump from 26
to what 36 or whatever but but most of
them are talking about they're fearing
full garbage collections so so with that
in mind when people when we're talking
about different GCS keep
keep the full garbage collections in
mind and why why they are kind of a
fail-safe for many GCS okay so with
OpenJDK hotspot they're refinements made
keeping the throughput and the latency
drivers in mind okay
so talking about the throughput
Maximizer so throughput has given us two
parallelism so there's parallel GC
threads the collection threads there is
peril concurrent threads - so previously
concurrent threads would be just one to
income because they don't want to
overload the application thread so now
they're parallel concurrent threads -
and the biggest of all are the
generational GCS so in hotspot including
they're very experiments done at Sun and
Oracle to figure out do we really need
to move to generational GCS and the
answer is yes pretty much because a
single generation doesn't tend to scale
that well and also you have performance
check and throughput performance
challenges for single generational GCS
and and now fun fact is that all open
JDK hotspot GCS are generational because
of all the experimentation done to
figure out that generation GC is the way
to go
what what what does that mean so most
and why is the reason why generational
GCS were introduced most objects die
young and you have fast path allocation
into the young generation for most for
most of the objects so with that you try
to age them age the objects so that
shortly the objects die and may be
transient long-lived and actually your
live data sets the static by data set
you can promote them into the old
generation again promotion is also fast
back there is something called
allocation buffers local allocation
buffers for GC and that's where the
promotions happen - for most objects
okay
now talking about from the latency
sensitivity part latency has this driven
algorithms - no compaction our partial
compaction right so time is of essence
if you can push off
compassion just because you have free
space why not do that just use up all
the free space and for promotions so
don't come back till you need to that's
that's the latency driver driver for
OpenJDK hotspot also cut currency right
we don't want to do stop the world
pauses so we're moving towards
concurrency so for example in CMS the
mostly concurrent mark-and-sweep
there's the of course the concurrent is
in the name right so there's the
concurrent algorithm there of Porgy one
has concurrent marking as well so when
I'm talking about this something I
wanted to highlight is all GC is in
OpenJDK hotspot fallback to a fully
compacting stop the world garbage
collection it's called the full GC so
for CMS you can tune it to void it but
you cannot I'm tuned in to postpone it
sorry you cannot avoid it
g1 we can avoid it and I will talk about
it in detail so let's move on to the GC
algorithms just for fun I'm gonna start
with the serial collector very simple
this is actually the minimum most lowest
footprint least complex so it's and also
the default for client in up OpenJDK
hotspot so what we see here are java
application threads they come to a stop
you have a single threaded young
collection that happens and then the
java application threads resume you can
have multiple young collections and once
your young generation get I mean your
old generation gets full then you have
you stop your java application threads
and into an old collection again this is
single threaded and then you resume your
work so that's the that's actually the
lowest footprint like I mentioned so
numb so if you optimizing for footprint
this is probably what you would start
with through the throughput collector
it's similar to the serial collector and
actually in fact the old generation
collection for throughput collector up
until I think JDK is six we used to be
single threaded just like the serial GC
it
was the cgc own collection actually and
then jdk 5 update 6 introduced something
called panel alone so now all their
federal GC threads are employed for all
collection as well CMS the series is
very interesting it's similar
the young collections are similar to
what you would observe for the trooper
GC but once the marketing threshold is
crossed
you have initial mark threads which are
stopped the world because you want to
mark the route and after that there are
some concurrent CMS threads in blue and
these do the concurrent work for the
marking and and then the whole
concurrent it's not a chunk you know it
happens concurrently but you could have
young GC threads
young stop the world pauses happen in
between them once that's done you'll
have a remark thread and previously this
used to initial mark and remark used to
be single threaded but now they are
multi threaded as shown here and the
remark kind of adds as as identifying
all the remaining roots and kind of
completing your object graph and then
finally you have concurrent sweep and
that's how a CMS collector works ok then
let's move on I'm not gonna talk about
g1 just yet because I want to kind of
combine all that and dump it on you
later because because this is the easier
part and if I kind of jump around it's
kind of it's may get confusing so let's
talk about the key concepts in the in
these GC algorithms promotion failures
in the throughput collector so a simple
Java heap this is how it looks for the
throughput as well as the same as
collector most allocations happen in
Aden you age them in survivors and your
tenure which is promotion into the old
generation okay so this is the old
generation of the throughput collector
this is after a few promotions have
already happened and then let's
about what happens to the the first
object comes along it gets it takes up
space in the doll generation second one
comes along goes in their third and now
you have a full old generation right now
so now at this point the petal markings
mark sweep and compaction will happen
for the entire heap and that's called an
old generation collection in throughput
collector were part promotion failures
and concurrent failures in the CMS
collector so this is CMS so generation
after a couple of CMS cycles so what you
see here are those those uncolored
spaces are free spaces and they're
maintained by free lists an object comes
along to the object promoted object
comes along and it doesn't fit in the
first space but it can fit in the second
one so it takes that space up the free
list is updated accordingly next object
comes along it fits in the first one
leaves a sliver of free space the free
list is updated accordingly
third one comes along cannot fit in the
first free space second third or fourth
so what have you observed here we have
observed promotion failure and and
multiple such failures would eventually
lead to conclude mode failures what are
concurrent mod failures when you CMS the
collector cannot keep up with your
promotion rate basically you're your own
generation is getting filled up faster
then your CMS cycle can catch up so
you're not freeing enough space
basically and what can cause those
marking threshold is too high
keeps too small or high application
mutation rate when I talk about high
application mutation I basically mean
the application behavior could have
spikes or maybe your allocation rate is
too high now let's move on to g1 and the
first topic I want to cover is the
incremental compaction in g1 and before
that I want to show you
g1 him g1 heap is made of regions
similar sized regions the entire heap is
contiguous
but the generation ng1 the young and the
old generation the generations are not
contiguous so basically you could have
an Eden region up there or over there it
doesn't have to be next to each other
except for humongous region shown there
I'm going to skip talking about him
mongers but I'll bring it back later so
let's look at this configuration without
the humongous object and let's talk
about what happens during a young
collection during a young collection all
your young regions are put in the
collection set so they will be collected
and they will be moved so the survivors
are down there and the newly promoted
objects are in that up there in the old
marked as old so so now with this
particular region now we have something
called a threshold that has been
exceeded so what that threshold and
similar to what what the threshold is in
CMS except for it applies to different
things a CMS marking threshold applies
only to the old generation for g1 the
marking threshold and placed it into a
heap of course because CMS only applies
to the old generation g1 is that takes
care of the entire heap so once that
marking threshold is crossed g1 can
start something called incremental
collections or even they also known as
mixed cc's so that's the heap occupancy
threshold that I was talking about the
marking threshold so after the threshold
is crossed a concurrent cycle is
initiated during the concurrent cycle
there is a cleanup phase and when that
cleanup phase is is going on if there
was any any if there were any regions
that were completely full of garbage
they will be collected right then it
doesn't have to wait till the pause to
collect them so that's gone because it
was full full of garbage now are the
concurrent marking phase is complete and
g1 has a knowledge about all its life
in the old generation so so the now we
can begin a mixed collection now a mixed
collection has all the young regions and
a few candidate or regions based on
based on certain economic this is called
GC organ Amex and these are some
thresholds combined with how much of
because G 1 the whole principle is most
garbage first right so how much is the
liveness threshold per region how much
are you okay with fragmentation wise and
all those things and I'll talk more
about that but let's look at a mixed
collection so so all they eat didn't all
the young regions and a few candidate
old regions will be chosen and then
they're evacuated and all the live
objects are moved into new regions so
why now that we know about all these
things what is a promotion failure in g1
so promotion failure evacuation failure
to space overflow to space evacuate
exhausted they're all the same thing
basically g1 could not find any region
to promote or or copy objects to and
what what it has observed is evacuation
failure it's expensive because
eventually you will you know if you see
too many of these evacuation package and
failures the full GC the fail the
fallback failsafe full GC will follow so
but one thing one thing is that
everybody should know that this can be
avoided okay so and how here you go for
something very simple when most of the
people that I worked with with who are
trying g1 they're bringing in their CMS
command line CMS remember same as people
who have worked with CMS for so many
years now their command line is so
geared toward CMS because and and as
I've explained before CMS and g1 are
kind of different right so so please
don't bring such heavily loaded command
line options to g1 try simple try with
just having a past time goal and your
heap you know max
initial heap settings now and first
thing you should do is try after you've
done that of course try increasing your
heap size but if you cannot do that
there are other things we can check we
can check for them are marking threshold
right now the marking threshold is a
fixed value at 45% but there there's a
CR to make it more adaptive CMS has
adaptive marking threshold and people
are actually moving towards making it
more fixed so well you know but that's a
good thing anything that makes GC more
adaptive is a good thing and that's so
that's the marking threshold for you and
you can increase the concurrent threat
so so as I told you when the marking
threshold is crossed there is a
concurrent marking cycle that happens
and and the cycle has to complete so
that we know all the live objects and
and the mixed collection can start now
if you concurred mark in cycle is taking
a long time what you can do is help it
get done faster by increasing and
concurrent threads but know that these
are concurrent threads working with your
application so you're taking away
resources from you for your application
there's also humongous allocation I'll
talk about that in the next couple of
slides and and finally I want to talk
about g1 reserved % so sometimes your
choose your your your family evacuation
failure could be that you're running out
of the two space the survivor space so
in in for g1 there's something called a
false ceiling and that's called as g1
reserved percent so it's a false ceiling
for the survivor space and it's a new
default it's ten percent so try
increasing that and it'll cap g1 will
cap it to 50% so you cannot if you
interested 90% it's gonna cap at 50 and
with that what that will help is that
give you more space to to age your
objects so not you know prematurely
promoting those objects okay now
fragmentation in g1 this is very
interesting because we spoke about
fragmentation and in CMS and how you can
tune to postpone it and all those things
and people work around fragmentation and
CMS so much you know people try to do
allocate similar sized chunks so once
space gets freed up you plugging it in
again with the similar sized objects
chunks but anyway so moving on moving to
g1 there is some fragmentation but
fragmentation is is tunable in g1 so
what what do I mean by that so G 1 has
heap waste percent so it's kind of
designed to absorb fragmentation you
don't want and the whole principle of g1
is to collect most garbage first you
don't want to spend your time collecting
live objects and moving them around
that's expensive
so those expensive regions can be left
out by default it's 5% of your total
Jaheim so basically what what you're
trying to do is that some dead objects
are allowed to remain and occupy your
own generation so you don't have much
wiggle room when you're trying to tune
it you don't have regular because there
this is occupied space and you're not
you're not doing anything with it but at
the same time you're aware of that and
you're ok with it so and if you're not
guess what you can adjust the defaults
right you can change it to to not do not
being 5% to lower than 5% so that's
that's fragmentation there's another
type of fragmentation in in g1
but you know in general G sees what if
your objects are bigger than that can
fit your fast path allocation right so
medium sized objects and something
called our large size object and
something called very very large sized
object which are known as humongous
objects so g1 has a special case for
Humungousaur checked first of all what
order humongous objects humongous
objects are objects then span 50% or
more of your g1 heap size region size so
remember we were talking when I showed
you that drawing the g1 region even has
similar sized regions and this the size
of that region is is determined at JVM
initialization it can range from one man
to 32 Meg's it's kind of your heap
divided by 2 K so so flow say for say
for example your heap is that the region
has said is
at two Meg's and you have one Meg array
but remember it's not one Meg it's one
Meg plus the head of science right so
it's one Meg plus so that would be
humongous 2g1
so what G one is gonna do is take that
and allocate it out of old generation
into some regions called humongous
regions now taking the case of the two
Meg some region science and if you had
to mega array now what would happen is
that G one would have to allocate
another region to accommodate that too
much Meg Plus remember so now G even has
to have to him contiguous regions so
humongous regions remember I was talking
about regions don't need to be
contiguous in Generation except for
humongous regions which are contiguous
based on the size of your Humungousaur
so the idea is that humongous region in
humongous objects should be short-lived
and very few in number but sometimes
that's not the case so evacuation
failures happen and why again it happens
because you have long lived among this
object and and and why happens is that
humongous objects will add to the old
generation occupancy because it's
allocated out of the old generation so
so it won't trigger the marking
threshold the marking cycle will start
the mixed collections will happen but if
they're long-lived guess what the
collection the concurrent phase is not
able to take them out during the cleanup
and mixed GCS don't do anything because
there's no point if among these objects
are live you don't want to keep on
moving this heavy this big chunky object
around right because you're not doing
and there's no reclamation so - now
you're stopped so you're stuck with this
marketing cycle that you have trigger
until the mix GC is not reclaiming much
because those are triggered due to your
mungus allocations so that is the reason
why effectors and failures happen
sometimes
alright talking about g1 GC logs the
reason I wanted to cover this here is
because there's some low-hanging fruits
that we can all quickly get you know
tune you
you see long looks scary but it's it's
has so much information I'm not gonna
cover everything I'm just gonna like I
said low-hanging fruit
II didn't sighs so there are this tag
that you can see that gives you the
agents eyes the survivor size and the
heap size what you can do is take all
those and parse all those and plot them
and that would give you your occupancy
right your region occupancy your
generation occupancy in a total heat map
keep in see simple what what it means
it's basically occupancy before GC the
Eden size before GC occupancy after GC
Eden size after GC same thing is true
with survivor but survivor it talks
about the size not not the occupancy the
total size of the survivor regions and
then there's of course the heap and heap
occupancy before occupancy after and
then the heap size before and after so
when you plot them you can see how the
occupancy changes for your application
it's a very good thing I use that myself
all the time I'm sorry I don't have any
graph to show what I do but but that's
what I do another thing I look at is R
is a scaling remember I was talking
about parallel threads are you so in
this particular case my I had eight
panel threads but my scaling is only six
point six so then I will go investigate
what was what was going on so so how you
do that is basically take the x tag and
look at your users in real times and
that's how you would do the scaling
another low-hanging fruit reference
processing I almost always say enable it
because because I've observed that
reference processing times would be
higher so that's the tag that tag it
will if you see higher at times there
and go check your remark pause and
remark pause will tell you exactly how
much time you spend in reference
processing so if you see these pauses
being high and or increasing I've seen
that too guess what and if reference
processing is the major contributor
please use this one barrel ref proc
enabled another thing when I was talking
about plotting
we need to understand the difference
between GC overhead and elapsed time now
GC overhead is an indicator of
throughput elapsed time as an indicator
of latency and I'll talk and this is
what I mean by that so the frequency
after overhead is the frequency of stop
the world pauses and the more frequent
these events happen guess what it's
gonna negatively impact your your
application throughput alright what
about the last time it's the total it's
an amount of time that it takes to
execute to stop the world pauses okay so
the higher the GC labs time the lower
the application responsiveness because
guess what you were you were spending
time doing GC induced latencies right so
we're thirty minutes into it I have ten
more minutes I'm gonna summarize
everything that I just said so what have
you learned most allocations fast path
they get allocated into the T labs they
get allocated into Eden your Eden is
full then you start young collection
other words keep allocating once you've
aged the objects appropriately then you
promote them otherwise keep aging and
and once you promote them promotion is
also fast path again with local
allocation buffers into the old
generation now for CMS remember we we
just promote into the fitting free space
it's like quick like that right so we
just promote it directly to the free
free space in the free list so what
about young generations and collections
if you've noticed most all three
collectors actually young generations
pretty simple and it's pretty
straightforward and it's the same for
the three server collectors that I
discussed you achieve reclamation we're
compaction and copying of live objects
and there are lots of options you
probably you know people have doing
presentations on this for about ten
years plus you size the Eden and
survivor space you and actually you know
what many G C's have their own adaptive
sizing in organ Amish for young
generation because it's so much studied
and so much feedback has been gathered
on the undos that there's so many too
to normals I pretty much everybody can
look up and get what about all
generations now things get trickier old
generation is different for all three
collectors that I discussed all three
server collectors for example for
parallel it's it's reclaimed in its
entirety and compacted as well so but
luckily the compaction costs are
distributed you know you have to the
worker GC working that takes care of it
and luckily the compaction cost depends
a lot on the make of the live data so
how are how is the application getting
the live data how is it making up near
the cache or whatever right instance at
every compaction you're actually moving
this light data right and there are no
tuning options other than the generation
size adjustments and then of course the
age so don't so the one of the
recommendations I think I have is as
well is don't promote prematurely right
no so what about CMS it's similar right
but at the same time what happens with
CMS is that we so when we talk about
garbage you know people talk about
garbage collectors there was it was a
thing oh we only care about the live we
don't care about the garbage but guess
what CMS does care about the garbage
because remember the garb the garbage
objects that just got de-allocated in
space now that that has made a hole in
your old generation and it can only be
filled by objects smaller in size than
that but not by objects bigger in size
than that so yes garbage matters for CMS
right when you when you're employing CMS
make sure
keep that in mind of course you can tune
the marking threshold it's adaptive but
you can tune it many people do it
doesn't do compaction but it has a
fallback GC if the full compacting GC
unfortunately the GC is single threaded
so it's that remember the serial GC that
I've mentioned it's actually the same
thing the same old generation GC single
threaded compaction GC what about G one
so g1 does incremental compaction it has
been concurrent marking and I like I
mentioned fragmentation is not unto
noble in g1 so we can tune fragmentation
away so to speak
unlike unluckily like I mentioned you
can have evacuation failures and we have
discussed how to kind of avoid them and
what would happen when you keep but when
you keep getting evacuation failures
eventually you'll fall back to the full
compacting GC again the full compacting
GC here is single threaded just like in
serial GC and but it's tunable and g1 GC
has multiple tuning options you know
it's it you can get inundated with the
tuning options for G 1 GC
but it does work when you have the right
to Ning's and so let's talk about the
tuna bones
so for throughput collector the goal is
to promote objects after you have aged
them appropriately and then the tunable
is like you can find those any on many
presentations they are the new ratio the
new size survival ratio genuine
thresholds and this is basically all
related to aging the objects
appropriately and the generation sizing
so some things to remember what
throughput collector is that you know if
you have steady application you don't
really need adaptive size policy so you
can tune you know you know your
application better you can tune it
manually and you would but turn that off
for sure if you couldn't go manual
tuning turn that switch off so whatever
overflow you have will get promoted into
those so if you're certain if your
survivor sizes are not sized
appropriately all the overflow gets
promoted into the old generation so try
avoiding that if you look at if you find
overflow in your logs try to add try to
size the generations properly for
long-lived transient data try to provide
larger survivor spaces because you know
it's and you have long live data and try
to age them as much or you have the
threshold aging thresholds too you can
change in most cases young generation
sizing has the most effect on throughput
of course because you
don't really have much to deal with in
no generation except for the size right
and keep keep that keep something in
mind this is a throughput collector you
don't want to have GC overhead more than
5% because I you know you this is your
collector for and that you've chosen for
throughput so try when you're tuning it
keep that 5% number in mind what about
the CMS collector like the same thing
but we have something extra here the old
generation marking threshold and the
concurrent threads remember to reduce
the concurrent marking time you have
something called a concurrent thread so
you use that to to reduce that time and
like I said the marking threshold
everything else is similar so premiere
premature promotions in CMS are very
expensive they will die and you will
have these holes and now you're stuck
with this even if you have the free
space you're stuck with the fragmented
heap so and and then of course I
mentioned about the concurrent thread so
you can reduce the same as cycle
generation buying that comp GC thread
option it will increase the concurrent
overhead you can also manually tune
remember it's adaptive but you can tune
their marketing threshold use these two
options and one thing I mentioned that
before this is CMS collector that's an
old generation collector so the
threshold is expressed as a percentage
of the old generation only not the
entire heap g1g see totally different
get the organ out the GC organ ah mix I
should have mentioned and this is byte
when I say organ Amex this is a special
word then hotspot 2 uses no open
shittaka hats but uses the word GC
economics I'm referring to that you can
you can google that so get those to work
for you and also know your default what
are the tuna bowls there are a lot many
tuna bones you of course the young
generation sizing but I don't want to
get to that what I want you to
understand that is your tunable is that
you should work with the pastime goal
the heap size the max and min
nursery and concurrent and parallel
threads you know how many multi-threaded
threats you can deploy for your GC work
right and then of course there's the
marking threshold for the old generation
collection the mixed collections
basically and then you can also have a
bunch of mixed collections
once your we have item to file all the
live objects and that helps you keep
your pastime more predictable and lower
if you distribute your collection work
there's a lightness threshold per region
so if you consider a particular region
expensive if you can adjust what you
consider expensive so anything over the
liveness - I should say liveness
threshold is 90% okay so anything over
90% well that will not be a good
candidate for collection because it's
considered too expensive
of course the garbage toleration
threshold that I mentioned they keep
waist percent and then you have the max
number of old regions to be collected
per collection so a mixed collection
cycle could have more than one mixed
collection mixed collections and you can
select how many regions you want to
incorporate per collection in the mixed
collection cycle there's lots of -
nibbles here again no your defaults so
basically if I gave you an example
before about the g1 heap regions and how
it interacts with humongous objects so
if you know that you know that oh yeah
that object is humongous for my firt for
the g1 region that I have right now
again and when you when you fix
something you're meddling with the GC
ergonomics and adaptive and you when you
when you kind of try to to tune it and
cannot restrict it you are restricting
the adaptiveness of G 1 GC and when you
select aggressive pastime calls these
are not hard real-time there are the
soft real-time soft time golds and then
what happens is you're increasing that
GC overhead it's now doing you're
basically sacrificing your throughput
totally spend time taming your mixed GCS
almost done yeah this is the last one so
you have the marking cycle so adjust the
marking threshold for your live data you
have the lightness threshold as I
mentioned the garbage toleration
threshold and he can distribute the
mixed cheese over a mixed cycle so you
can have about the default is about 8 so
you can change that and you
you know to your advantage I wanted to
talk about this book this book helps
everybody
you know for people like me who want to
go refer certain things this is very
helpful people like everybody who wants
to just learn about java performance
skin and of course therefore this book
as well there is a companion book coming
out I have written a few chapters in g1
and that companion book as well and then
the blogs and I have my blogs I update
alright I write a lot of articles and I
like to try to incorporate everything on
and that info cue website as well as I
put my presentations up here and of
course don't hesitate to mail the GC use
if you're using garbage collectors or if
you're developing garbage collectors you
can use a dev GC dev alias there are
lots of people that will jump in to help
and my favourite blog up there is John
masses blog really helpful and really
really knowledgeable person that's all I
have for you today
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>