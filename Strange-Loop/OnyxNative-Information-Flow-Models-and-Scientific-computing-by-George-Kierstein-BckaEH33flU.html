<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Onyx-Native, Information Flow Models and Scientific computing&quot; by George Kierstein | Coder Coacher - Coaching Coders</title><meta content="&quot;Onyx-Native, Information Flow Models and Scientific computing&quot; by George Kierstein - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>&quot;Onyx-Native, Information Flow Models and Scientific computing&quot; by George Kierstein</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BckaEH33flU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this work was all derived from a project
that I did at the National centers for
environmental information which is the
world storehouse of climate data and one
of the biggest research centers in the
world other than that in my professional
career I've focused on computational
pipelines and I no longer am at NCI I'm
actually bootstrapping a privacy focused
media hosting company named hexie
so since this talk was derived from work
I'm gonna present it in reverse order so
as to give a context for the work and
I'm gonna start off presenting a
character to scientific computing in
general and after that I will move on to
some of the properties of these systems
have I'll segue into information flow
models and onyx platform and the work we
did for it in order to leverage that
into bootstrap native code and non
invasive leary architect a system that
is almost a black box at that point I
will conclude with an overview of onyx
native as a platform and take questions
so let's get started so I'm gonna
present the human context of scientific
computing because most people who work
in industry don't really have the same
methodology or the same environment of
course obviously they're solving non
code problems ideally you'd like to
state that to be true for business but
it's not exactly the same thing because
developer culture and computing in the
industry have long sense focused on the
actual methodology of how to code
properly how to make reproducible
systems and so forth scientists of
course are worried about science and it
is not uncommon to have grad students
who help you and they're not really
coders necessarily and there isn't to
date yet a very sort of systematic
approach to how to build these computing
systems so they're very widely and they
have lots of interesting problems one of
the interesting aspects it's a little
bit different than a lot of business
computing pipelines as the human
timescale leads to a lot of
collaboration and some certain level of
kind of stability because the
collaborations done over long timescales
you know science takes time and in the
course of doing so there's a lot of sort
of human stability in that because it's
not always trading computer code
necessarily and when you do this stuff
moves pretty slowly there's also a high
degree of specialized domain knowledge
and they don't necessarily have a great
deal of development practices in fact
one great story I worked with a
scientist who is the world's expert of a
certain thing in the climate data realm
he was in his 80s and he had
legitimately never heard of source
control before he was very excited and
he was totally willing to learn but this
was a shock to him you know the one
could actually leverage these tools to
make his life easier so that leads to a
lot of problems because there's often
little overlap between these deployment
maintenance and authorships sort of
roles within development of these kinds
of computing systems so I'm just going
to jump to the bad and the ugly of
scientific computing
I would hope everyone understands all of
the good parts of it and what wonderful
things it does for Humanity when you
kind of have the situation you end up
writing systems organically over long
periods of time and multiple people
pitch in that means it is typically on
architected you know organic systems
don't necessarily have an architectural
bent in the climate data realm
particularly you have a situation where
you know this system I will introduce
later that's actually kind of the reason
that this came about was over has been
in production for over 15 years some of
it really literally has modems to talk
to devices and it is a mess of all kinds
of different things that other people
have tried to wedge in there over time
and in fact what goes into this product
has changed over time as well so you
kind of get this situation where the
systems tend to be polyglot they're
opaque and poorly documented that's
really problematic when you're tasked
with rehabilitating one which is the
situation that I found myself and when I
was working at NCI and is the origin of
this project which is the ASA system so
the ASA system I'm going to kind of
blazed through certain the some of these
slides because there's a lot of
information that's sort of irrelevant in
a very real sense I wanted to put in the
men's so that you could get a very
concrete understanding of some of the
types of complexity that is very common
in lots of different computing systems
especially ones that actually kind of
serve as de-facto production systems you
know there's a lot of variety in the
scientific community and scientific
computing so in some sense I'm not
necessarily talking about like pure
research code but there is a lot of
computing that is done now that is
transcended that and there's a lot of
expectations that these things are wrong
in sort of principled modern ways and
it's very difficult when you've got a 15
year old project to present those
affordances without doing something I
call code archaeology or a complete
rewrite where ostensibly you just chip
out the worst parts make sure it still
produces the results you had before and
you call it a day you know leave kick
the can down the road for the next
person hopefully a decade from now
so ace the system itself has been
running for 15 years it was on an AIX
box that was finally gonna be retired I
mean the operating system had been
passed its end-of-life date by eight
years by the time they started saying we
have no choice we have to rehabilitate
this but the complexity has been so
daunting and it is a system in
production so you know it's a very
careful process and the pros and cons of
which are completely right expensive
time-consuming and as we know dubious
outcomes sometimes or some other
approach and so I am try tried to find
another approach and
is the system that started it so it the
ASA system clearly has a lot of stuff
going on
it actually takes measurement takes
measurement data from a variety of
instrument platform usually via modem
there's low resolution hilar solution
data there's multiple types of QC that
go on in this process so it's got a fair
amount of complexity so it's a
three-stage workflow and ironically most
of NCES systems and a lot of scientific
computing systems have a similar kind of
simplistic kind of single stage here
single stage there for how the system's
actually constructed so there's this
initial ingest stage which does some
level of automated QC there's an actual
manual QC process to vet these results
before they go into the archive and then
there's some publishing step so one of
which is the automated weather
observation system there at airports
they collect all this data another one
is these this whole other system called
a sauce which are actual an instrument
platform that also collect all these
data and they're at locations typically
near some of these other ones and they
have lots of high resolution data
they're all pulled in via various modem
you know kind of techniques and there's
also ingest of all this other data that
is then incorporated into this final
core data set that a lot of other data
sets in the building actually also
depend on so not only are you in at
least within the climate data realm but
I think this is typical of a lot of
other scientific computing scenarios you
end up having you know not only any
problems changing the fundamental
project itself but you have to be very
careful because there is a much larger
system at play that has dependencies
that are also opaque to you at the time
this is including how many are up there
are six other data sets before it's
finally done there's levels of QC
and how do you go about trying to
determine how bad is it right so when
it's a polyglot system and this system
has actually got Java C++ C Fortran and
is wired together with bash so we're
looking at just the Fortran kind of part
of it here and we tried to come up with
a way to predict how long and make an
estimation for say a total rewrite for
instance but in a polyglot system
especially common in scientific
computing realms this really hard to do
because things like cyclomatic
complexity don't necessarily translate
all of the complexity across languages
you know how do you compare the
complexity of a Java project that has an
object oriented architecture with one
that's purely functional or just Fortran
one program Fortran so all we could do
ostensibly was kind of just do lines of
code at least that is literally kind of
comparable but again it has all these
same problems so I'm just pointing this
out mainly to give you the scope of how
many bits and pieces are actually in
this system that's the initial ingest
part the publishing part here's the
publishing phase there's twelve thousand
six hundred five lines bash in the
system and it's all wired together like
so and actually one of the interesting
and most kind of challenging parts of
the project initially was to ensure that
there was actual documentation so one of
the problems that's endemic everywhere
with documentation is that it's all kind
of institutional knowledge but it's a
particularly bad when you are in a
situation where a sensibly the person
who wrote it might no longer have like
worked there and you know might not even
be around anymore to ask questions from
so we took six months to produce this
diagram and ostensibly if I had to give
any advice don't get started or promise
anything until you force somebody to
write down exactly what it does
especially when no one really does
the main person who created this diagram
literally has been semi-retired for
almost a decade only to keep him there
to fix this when something goes wrong
and that is not uncommon which is kind
of shocking if you you know hear all
these great talks about how full
automation and reliability engineering
and works and all the rest of this sort
of stuff
those are things scientific computing
have not even really heard of
necessarily and again with the sharp
division between you know the people
making it were scientists and the people
tasked with running it and ensuring that
it does actually work barely talking to
each other this reliability idea is
really problematic I think some of the
best models that I've heard of are the
integrated ones but that kind of assumes
that your research code that's super
interesting eventually actually has to
be in production and that's a whole
other kind of organizational and
organizational investment that's really
difficult to sort of argue for when
you're in the government so they're all
these on the left you'll see all of
these different pieces that ingest
different data sets some of them are
modems some of them are from some other
part of the building there's all of this
logic in the middle and you can see your
standard three phases of the workflow
from left to right so you start off
there you move to the middle some human
is looking at the data and then you gets
through some massive chain and publishes
so that system had some interesting
properties that were kind of unexpected
and or what we leveraged to build a
solution that could non-invasively re
architect and I'll talk about that part
and what I mean specifically by that
later but a lot of this came out of the
culture of research where lots of these
things don't change over time very much
for instance the data collection and the
data representation of a lot of this
data but is pretty historically stable I
mean the parts of the code that are the
oldest use Kermit's to pull these files
off of modems
and they format of that data has not
changed in 15 years and in fact you know
I kind of showed you slides quickly of
all of these instrument platform
networks that it's drawing data from but
they literally haven't changed much
either in fact finally I think next year
they're going to start to upgrade them
so that they're networked they're not
going to change the data format but
they're going to make sure they're
networked and that project to upgrade
these systems is expected to take almost
a decade
so the timescales of how things change
in this realm is actually kind of more
on the human level and that is pretty
handy because how that happens to tend
to work and as part of the organic
growth process is that there are very
stable intermediate file formats that
are well described that haven't changed
in any you know period of time or if so
they've been meticulously changed and
argued about the changing of it you know
for probably years before someone
finally adopts it and that kind of then
rolls out into the scientific community
so that's very handy and another thing
that's very interesting too because
again this was all done from a human
point of view these kind of workflows
all the way down and I'll get a little
bit more into detail with that in the
latter half of the talk but you end up
having a system out the next slide in
fact so yeah if you look back at the
original the system diagram and all
those blocks you know we have a
high-level three-phase workflow but
inside one of those blocks is a bunch of
little Fortran programs that consume a
single input file with a well described
file format some logic to decide what to
do next
so forth and so on so that is itself
workflow here's a nice diagram and that
is also very handy for the solution that
I came up with so I'm gonna segue now to
information flow models and I'm not
going to give it to an actual definition
because that isn't really necessary the
interesting parts of it that are
applicable from the general idea that
you are passing data at every level of
the system you know
not necessarily using strong api's with
structure and semantics you know it's
it's data and it's declarative kind of
go all the way down that lets you do
nice things that otherwise are really
hard which is to compose and create your
own DSL and abstractions on top of them
at any level of logical kind of
organization of the system that you
choose so to go back here here's a very
low lower-level one you know the highest
level one and they're both theoretically
describable by workflow system and if
you had one that had an information flow
model fundamentally well then that could
all be declarative which would be a very
clean kind of general-purpose way to
describe these systems and potentially
reason about them so my goal at this
project was to find a way to
rehabilitate this code without really
understanding it and in a way that would
be shorter cheaper more reliable and
provide things that theory architecture
of like the re-engineering code you know
kind of archival approach can't deliver
which is modern affordances of deployed
systems deployed computational systems
that maybe could offer reliability
engineering and at the very least great
things that are still like wishful
thinking in the scientific community
like centralized logging very stuff the
stuff that we take for granted at this
point in you know industry so the
approach that I took then was to say
okay well if everything can be described
as a workflow system it's already
theoretically nicely decomposed between
every program that's run because inside
that massive map are all of those
individual Fortran programs they're
actual single things you can run them in
independently and there isn't really any
strong connection between them and then
I wanted to leverage the information
flow model to be able to try to make it
declarative
and do it in a unified way so that that
same approach could be applied to any
level of abstraction within the system
so for example each individual task
could have a nice little wrapper API
that hopefully would be lightweight and
not really asked much from the
implementing developer that could take
one piece and give it a consistent
uniform way to communicate with the
larger system and allow it to have all
these nice new affordances that were not
present in the original system because
they didn't think about adding them so
it turns out that there is such a
workflow system that has a declarative
model that takes actual data as a way to
describe the system itself and it's onyx
so the main difference the main problem
we encountered here while there were
several but the main problem that we had
to address was that onyx well at the
time was purely closure based so while
you could certainly construct your own
you know data structures and use onyx
the functions that you had to run
ostensibly had to be closure functions
and in within the government as you
could see there were already many
languages involved and they're pretty
conservative so they were never going to
let us write anything in closure and
given the historical context you kind of
can't blame them it's at least an
understandable problem you know they've
spent years sort of figuring out that
pythons okay you know that now pythons
okay but ten years from now maybe
nobody's going to use Python exactly or
I'm sure there will people be using
Python but it's not the program wing
language du jour and now they'd have a
system that had another programming
language bolted on to it that was
something we wanted to avoid and they
weren't gonna let us anyway
so the onyx platform for those who
aren't really familiar with it have a
wet describes its workflow as a data
structure here's the
graph of instructions that if you can
see they kind of lay out a nice little
graph of what pipes data to what and
inside onyx everything is essentially a
map so you're being passed data
structures between every part of it the
way you'd describe a job is a data
structure everything at that point is
declarative so for example these tasks
that we saw in the graph of the work
flow are just another kind of data with
annotation about what function gets run
with what name so there's an abstract
kind of binding to the actual function
that executes the code at runtime
and when you submit a job again you just
construct one of these data structures
and then you pass it to the system and
say good luck with that the other prints
of the other property that was really
advantageous to us within the building
of onyx was that it can be easily
deployed as a single instance job
essentially the competitive the
competing kind of workflow systems tend
to expect you to originally set up a
cluster of some kind and has strong
semantics with respect to some
programming language about how you use
it how you invoke things how functions
are declared and all the rest of that
that is not declarative so that is a
different another investment that you'd
have to ask the ops team to buy into in
order to get this to work so that it
puts you in a difficult position unless
you can make a legitimate case which you
can and onyx is is that you don't need a
cluster to run this program it's
theoretically Java I mean it does it is
just a jar which worked as an argument
at that point and it fits our use case
really well so the main problems though
we encountered and the work we did was
to have to give it a Java API so that it
really was Java at least on the sort of
implementation level of the code that
we're going to be writing and so we
started writing onyx Java which
basically as a convention
a fairly simple interface that
ostensibly emulates closures to
interface and just produces the same
correct data structures fork for the
underlying system so that is there
really isn't any kind of Hart dependency
between us and the platform itself we
provided extra utilities so that you
could make more direct use of the actual
kind of closure native data structures
that are pervasive in the Onix platform
if you have used it before some of the
kind of single-use non plug-in style
kind of used to get data into the system
are all done with these core async
plugins which of course extremely do
require you to understand core async and
if you're trying to create a job API
that's not really a great thing to ask
someone to have to understand when
they're not even going to be writing
closure so that provided some we provide
some tools to simplify that and we added
some extra affordances so that you could
have a pure Java class in a closure
based workflow in onyx and that sort of
looks like this here's a simple example
class that just passes data through and
we have set interfaces and a base class
that's an Onix function that does all
the bootstrapping within a Onix workflow
to ensure that this class is called at
runtime properly here is an actual
example of a catalog being created that
has a that same actual Java function
essentially there it wasn't really that
complicated all you had to really do all
we really needed to do was be able to
ensure that the right arguments were
being passed around within Onix
so that by and write a simple
bootstrapper that could actually
understand what to do with them and find
that you know give you an instance of
the Java class at runtime because of
course census closures compiled down
they are actually Java classes at
time and the thing that was last and
most important essentially to make this
work on a legacy system that had a
numerous amounts of Fortran code was to
make sure that we could also in a
principled way bootstrap native code for
at run time and of course Java being
sort of around forever and one of its
initial survival selling points was that
you could run C code and C hasn't
changed much and it's still essentially
the lingua franca of every computing
system I mean almost anything can be
compiled into seek fátima code that
actually was pretty handy and and well
documented and works great so what we
did here was just extended this class to
give sort of you know native
bootstrapping support so that you could
specify the library as part of your
metadata and you don't break the
declarative model and here's the
standard you know j'ni implementation of
a very simple c function so in Java for
those who have never used J and I it's
actually pretty clean it's really nice
you just sort of add this keyword native
Java provides an extra tool that will
generate this header they have a baroque
kind of way of expressing class
hierarchies and assets but ostensibly
this is just straight C with you know
nice X turns and an implementation
that's fairly straightforward right here
is a disassociated getting out of string
there's a map that got passed in and
we're calling the code on the Java side
to disassociate from the map and pass it
back pass the map through which fits the
sort of standard onyx model that every
function that gets executed is a gets a
map segment and showed a map one or more
segments so that was great on the Java
level but it was important to kind of
finesse some of the facts that map
manipulation and
of code when it is another level of
abstraction up is really kind of tedious
boilerplate let's be honest so we wanted
to offer some affordances to make the
use of this pretty easy both in C and
C++ the C++ kind of started off first
and offers so when when native code
pieces loaded the libraries loaded and
the native content the JVM runtime
context for that library is squirreled
away and kept available so that you can
do nice things like use a transparent
like opaquely without even knowing
you're operating in a JVM context or get
at the context directly if you need to
do kind of more sophisticated Java based
kind of Interop from within native code
so and again C and C++ aren't that
different so there's pretty it was
pretty easy to be able to offer both
that includes a bunch of map affordances
here's the standard set that we could
come up with that didn't include things
that work had a dash in in them so
because that's not as easy to get
because of the way the actual underlying
native closure classes had to be
implemented which kind of collide with
how you do variable very variable
argument functions so you can't use
ellipses and things that kind of you
would typically do when you're kind of
writing a C++ function the teddy bear
that was very variable an argument list
yeah again here's the C one for the
native affordances and since most of the
programs were actually in Fortran you
I'm showing you some Fortran based
solutions here but the basic idea and
the thing that made this again not so
hard is that there really isn't a whole
lot of difference between the library
format of a Fortran module and the
standard c library format and that of
course is not surprising considering the
historical context
see came to try to fix some of fortran's
problems when Fortran was king and there
are now nice standard iso ways to do
data structure conversion in a
principled way for you somewhat
automatically and you can kind of see
that this is actual Fortran here and
it's not that unfamiliar in a sense you
know if anyone has done some C
programming you know you have a
subroutine you declare it so forth and
in this example here loosely you know
I'm showing how one could call back into
the actual JVM runtime directly from
Fortran without having to have any
understanding of the runtime context of
this call when you're writing Fortran
and that was a very important aspect of
trying to make it non invasive because
the last thing that we really wanted to
do was to force them to learn some other
baroque library in order to say replace
their file loading with our file loading
or ad logging or event you know emission
from inside native code here is an act
the implementation from the other way
top-down again same kind of difference
because of the model being that you
typically with Java you have this you
know we provided a sort of header in the
C level and you typically compile
everything once and link it into one big
library so linkage between seeing and
Fortran Fortran is again pretty
straightforward because their libraries
aren't that different and you here's an
example of you calling some test
function that lives inside a Fortran
module that is all wrapped up and
packaged nicely to get bootstrapped into
the conex runtime system and so now that
we have all these pieces we could
actually step out way from the fact that
we use Onix a little bit and be and
start to abstract a way to describe our
system in a way that's like less complex
it's not necessarily done in terms
purely of onyx instead we can start to
talk about it in terms of the problem
domain
and that looked something like this
so now you can see that there are
certain tasks and you know we've got a
task block and we can kind of specify
what language and type but all in our
own terms pass constructor arguments so
forth and kind of again it bears a
strong resemblance to cue onyx declare
it declarative data structures but
ostensibly it's got all of the extra
metadata you know put in in a way so
that the person who's going to be tasked
would say here's this system map you
know can break it down into pieces you
know you could do this all declaratively
store this and if you had more
sophistication you could potentially
generate these at runtime as well just
like you could in a normal onyx like
execution environment so some of the
things that were really quite great and
unexpected well of course they weren't
all index unexpected per se but some of
them are aspirational and it was very
fulfilling to see them actually work out
in practice which is that we had a much
quicker delivery time like our
development cycle was much faster than
normal and what I mean by normal is that
a typical project and this one in
particular was expected to take two to
four years to do you know this is a
here's a system will rehabilitate it
we've got to get it off of the ax box
today but you have four years to do it
you know here and no one would blink an
eye you know you can just count in years
and they're like okay because that's not
on realistic for from a historical
perspective from how long some of these
test systems really do take to
rehabilitate you know especially if you
are leery of touching anything you don't
know which is a good idea and you know
are also kind of stuck in a deployment
context that is you have very little
control over so your ability to use
novel tools is very restricted and again
the original authors might not have any
have left anything behind so and in a
system that's been in production for 15
years you have to be really careful
years is normal and we were able to do
it much less and most of the time for
this project actually took writing the
bootstrapping part of Onix itself so
greatly reduced time to delivery was
wonderful to see and one thing that was
unexpected was of course that the
department and the organization wanted
all kinds of other things bolted into it
now that it seemed pretty easy to bolt
stuff on like let's be able to tell that
in so that just becomes another sort of
simple piece a new Block in the met of
the actual top level DSL says it's
another task you know here's a service
and you can describe that declaratively
and you know mix and match these things
in a much more sort of flexible way than
prior to you know prior when somebody
would have to rewrite that and bash or
some such you know kind of approach and
I know it seems unlikely but it but the
just ajar kind of mantra really works
it's pretty interesting that at the
point we got it to with a full-blown
Java API then onyx as a system stop to
being so sort of risky to them because
even though it happens to be written in
closure it really is just a jar when you
deploy it and that argument worked
pretty well because everybody uses Java
and finally the repeatability of this
approach is really high they've already
started considering using this in other
contexts with other systems because as I
pointed out much earlier most of them
really are these simple three-phase
workflows with somebody in the middle
some stuff in the beginning some stuff
with yet and yes inside all of there is
lots of little programs and lots of
complexity but again that complexity
tends to be little programs with some
logic to decide what program to call
next handing the file all the like
one-off all the way down the chain so
there really was a high degree of
repeatability within this particular
problem domain that was good to see so
I'm gonna make some bold claims here
which first of all I think that this
approach is a special
well-suited to scientific computing
systems in general because most of them
really do have the same stereotyped
human scale kind of growth and they in
it scientists tend not to you know
because they're less focused on
deployment concerns and maintenance
concerns and development concerns in
general they've been focused on
communicating results over spans of
years you know tree whatever programs
they generate tend to you know kind of
live on and have a life of their own
people tinker with them but eventually
they kind of settle down to be the
workhorses of that part of the
scientific process you know a lot of
these file formats and you know the
libraries they use to calculate a
julienne date like those haven't changed
in over a decade and they're not going
to any time soon either hopefully you
know because they work and they're right
and not so correctness of results tends
to be what people are concerned about
and you know they're not in a rush so
they tend to have a very slow change
cycle and they also tend to end up there
for very well decomposed in a very
similar way to this system in fact most
of the ones that I looked at kind of had
these exact properties which leads me to
my like last bold claim which is that
this approach really I think is very
well-suited in general to legacy systems
which have the same really small set of
properties one of which is that they has
these human scale stability to a lot of
its internals and the other one that you
know somebody could theoretically figure
out the map of the system to be able to
describe these workflows in the first
place but I would argue that the first
one is problem is true generally it's I
kind of pointed out because of the
nature of the environment in which these
systems were developed but secondly they
tend to also have the second property
because of because they just wait what
sorry yeah they can have both properties
which is that they're nicely decomposed
as well you know once you get to the
human time scale problem you know you
have these stable file formats and this
stable kind of communication and they
tend to be very piecemeal because of the
organic growth over time so most legacy
systems
share the same properties thank you
anding questions oh and I wanted to also
thank my coworker on this project
who Ryan Burke Eimer who helped develop
some of the Onyx Java stuff and
inherited this project so some of the
you know origins of the claims I've made
you know were because he actually
inherited kind of taking over the
project officially and he actually
developed the DSL independently of me so
it was a really good test of whether or
not you know kind of this approach
really could ease some of the pain of
transition when the original author left
which worked out really well and he did
a great job thank you
that's a great question he asked how
would you evangelize such an approach in
other scientific communities and I mean
I think that the simplicity and the
human kind of scale you know aspects of
it make the strongest argument at the
end of the day you know they do want
everything that modern systems offer and
it's not like they're entirely ignorant
of those you know properties but again
because of being you know conservative
with respect to rebuilding you know
they're they're they're not sure there
is another alternative and so I think
that you know when you highlight than
the typical nature of these systems and
then point out how they could reason
about them even if they didn't
understand the details by this
declarative method you know that that
was really a big selling point of the
system itself and the fact that it
really does you know kind of boiled down
to a deployment strategy that they
understand and that's not in an in and
of itself not controversial was also a
big win
with when you were showing that
talk about each one of the boxes like a
separate Fortran is pretty isolated
like what of to two boxes are dependent
on each other
yes yes so this question was like how
does dependency sort of you know layout
work and they're the workflow itself can
you know basically force it to sort of
end up being linear with some of the
affordances that are built in so you
don't necessarily have to have true
parallel lives ability within and Onix
workflow if you unlike if you don't want
to and in that case that was a pretty
critical sort of thing to be able to
work with in a system that had a human
intervention I mean it in that sense
it's not Rosetta it doesn't resemble
traditional kind of computational work
slow systems running in production that
are fully automated you know there
really could be a week gap between you
know the initial ingest and someone
finally doing the human QA before it
could go didn't start the archive
yes both of these are open source
projects so onyx Java is living right
now as part of the onyx platform and
onyx onyx native oh I didn't put the
link up yes there they're both online I
can get you the link to onyx native
after the talk
that is a great question so you can more
or less find a way to fake running bash
as a process and wedge it in there which
doesn't give you all of the nice
affordances mind you but at least as
conceptually in principled with this
approach and one of the other facets of
this approach that really pays off to be
able to address stuff like that is that
because now every little logical action
task within this system is sort of
nicely compartmentalized that lets you
do targeted reengineering so yeah the
one of the TAT the larger long term
tasks was to rewrite all the bash and
you know in you know use this new nice
new API but if you couldn't you could
still use you could still execute the
bash and more or less make this work
properly within the workflow sure
yeah well my experience was really
Pleasant but I'm incredibly biased so um
let me put it that way but yeah there
are really good example packages and you
know the nicest thing that made it
really easy to kind of come up to speed
was the fact that in reality you don't
have to know closure at all to sort of
think logically and reason about how to
build a workflow and what to make it
look like and what dependencies are and
so forth and at the end of the day like
this catalog shows you know ostensibly
at least in the pure closure case and
you know in the onyx Java slash native
case you know your responsibility at the
end after sort of describing this data
structure is to write that single
function that gets executed at runtime
which actually makes it is pretty easy
to reason about and there are nice
examples to show you how this works out
in fact I'm actually copying one of the
simple Onix tutorial code projects that
has you know a fairly simple workflow
but
I'm sorry can't hear you
oh well all of that was added after the
fact you know we there was the raw API
that just added the affordances to Onix
itself and then once we have that we
built a DSL with our own kind of tasks
API that added all these extra
affordances that fit into the
organization you know that we were
actually working with the data it's your
question okay okay any other questions
okay
all right well thank you thank you very
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>