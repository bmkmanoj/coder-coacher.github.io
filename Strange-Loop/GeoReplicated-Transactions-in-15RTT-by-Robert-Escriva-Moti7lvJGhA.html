<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Geo-Replicated Transactions in 1.5RTT&quot; by Robert Escriva | Coder Coacher - Coaching Coders</title><meta content="&quot;Geo-Replicated Transactions in 1.5RTT&quot; by Robert Escriva - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Geo-Replicated Transactions in 1.5RTT&quot; by Robert Escriva</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Moti7lvJGhA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Robert I'm a software
engineer at Dropbox but today I'm not
actually here to talk to you about
anything to do with that I'm gonna tell
you about some work I did during my PhD
thesis at Cornell and we're going to
explore distributed systems really deep
dive I'm hoping to keep everything
approachable to someone who's vaguely
familiar with distribut systems if you
sort of lose me I'm hoping that you'll
sort of like give me in quizzical look
so that I can elaborate a little more to
bring you up to speed so let's start
zoom out 539 miles high in the sky we've
got three data centers and they're in
different areas and a storm can take out
one of them and now you only have two
data centers and how do you make your
system handle that kind of event if you
only have one data center and the storm
takes it out there's no force on earth
that's gonna save you but if you have
two or three you might be able to save
yourself if you lose one the problem is
though that these data centers are far
apart and speed of light sort of
enforces some latency on us when we're
dealing with them it's about 75 to 90
milliseconds between east and west
coasts if you're on the same coast it
can be as low as 20 milliseconds but
fundamentally there's some latency there
and you're not going to get around it
and when you're working in the white
area the latency between the data
centers is the dominating cost so you
main memory reference could be a hundred
nanoseconds going on a classical hard
disk if anybody here grew up with those
it's eight milliseconds round-trip
east-west could be a hundred
milliseconds you can see there's orders
of magnitude different so no matter what
you do vtune profiling on your code it's
not going to make a bit of difference in
the wake of a wide-area round-trip if
you can cut one wide area roundtrip or
even just one half of that roundtrip
you're going to save quite a bit on we
can see and there's some approaches that
deal with this already and I'll go
through them to sort of bring you up to
speed
there's primary backup and you'll see
this in systems like spanner cockroach
DB practically every system that uses
replication you you elect a primary and
then you have some backups that actually
follow that primary replicate from it
perhaps by log ship and perhaps by some
other mechanism and if a client is close
to the primary that client is happy that
client can talk to the
the primary with low latency get
immediate reads the rights can be
immediately acknowledged and in the
background the rights will propagate to
the backup but a client who's very far
away on the other coast he's going to
pay a long latency cost to get to that
primary so that client is going to pay
that latency cost it's unavoidable they
can't talk to the backups they're nearby
and when the primary fails you have to
have an operator throw a big red switch
to fail over to the backups if you're
lucky enough to have this automated and
not have a flap and fail less variously
you've done a good job most people I
know just have a big red switch to fail
the primary over to the backups but
that's not a free thing that that incurs
an availability hit if you say I'm gonna
wait five minutes for the primary to be
down before I failover automatically
that's five minutes that you lose if you
wait five minutes to page someone that's
five minutes that you lose and so
ultimately primary backup has this
problem of availability if you throw the
switch quicker you have to be sure that
your code is robust be able to actually
handle that failover consistently so
primary backup low latency in the
primary data center it's simple to
implement and reason about simple being
it's simple at the basic level once
you're trying to get into all the corner
cases of making sure a switch over is
really robust and fast it becomes more
complicated but you pay a high latency
outside the primary data center and you
will always have some period of downtime
between primaries because there can by
definition be only one primary at a time
there's also a ventral consistency this
was very popular I'd say it started
becoming popular maybe ten years ago and
it's become increasingly less popular
over time and that's a good trend in my
opinion and the reason why is you can
have rights and multiple data centers
you can write locally there'll be a very
fast write the data centers will swap
and reconcile in the background and
eventually they will consistently select
one version of the object and completely
drop the other right on the floor it's
as if it never happened so if you write
to an eventually consistently to storing
you know one can never read that right
did it really happen it's a problem with
eventual consistency especially when you
use last writer wins
the merge policy as most systems do all
is not lost rights are local in thus
fast and there are ways that you can
actually guarantee that you get some
degree of promise out of the system
there's something called CR DTS when
combined with causal plus consistency
you get a guarantee of emerge but data
can be lost even if a right is deemed
successful so if you lose the replicas
that hold the right after it's been
acknowledged no amount of eventual
consistency can save you there's also no
means of guaranteeing you'll read the
latest value even if you have the most
fancy CR DT in the world that completely
models your entire application as a CR
DT there's no way of saying I am looking
at the last version of this object I'm
looking at the most up-to-date copy if
you run an auction site like say eBay or
I don't know of any other auction site
you you're not not going to be able to
look at that CR DT and say this
represents the last state of the auction
so eventual consistency not really a
good solution in my book if you can do
better
spanner had this neat optimization they
called true time so they're still
primary backup writes can go to have to
go to the master but reads can go
anywhere so the client on the west coast
here can actually use the very accurate
clocks on the west coast to take a
snapshot in time and get low latency
access to the data on the west coast so
true time is fast for read-only
transactions the tradition of the right
path uses traditional two-phase locking
and two-phase commit just like you would
find in a database textbook if you
assume that each node in the database
tax book is actually backed by paxos but
you're going to incur two phase locking
traffic in the wide area a simple read X
right wide transaction in a
sub-optimally deployed version of
spanner can incur six or eight wide-area
messages which would be three or four
round trips before the transaction can
commit and that's just read X write it
to Y a more transit complex transaction
then that would incur even more round
trips then there's this trick called
one-shot transactions you'll see this
often used sort of like a stored
procedure
essentially you're replicating the code
and not its side-effects so in this case
you would replicate the actual script of
the transaction this is generally a
little bit more flexible for scheduling
you can schedule the transactions and
then run them later and the code can
come from any data center and you need
one round trip to get it into the system
that's doing the replication but
one-shot transactions are inherently not
actually fully general if you want to do
something like interleave application
code that does a read and then it makes
the decision based on that read perform
further action you're actually gonna
have to break that up into multiple
transactions and then it starts to look
like an optimistic concurrency control
system where you'll have spurious aborts
and so you do a lot of wasted work in
order to get that so I'm going to tell
you about a different point in this
design space called Casas I wanted a
primary list design the idea of primary
backup doesn't really appeal to me
because I don't want to take an
availability hit and since I was doing
this in the context of a PhD I get to
make my own constraints I wanted
serializable transactions I taught an
operating systems course a couple times
t8 a couple times and undergrads really
understand if you take a lock and you
hold a lock you basically can do
whatever you want in that lock and you
don't have to worry about other people
doing stuff to the same data structure
because they'll also hold the lock you
can use the same description for a
database if you type begin transaction
you run your transaction and call commit
and it commits you can act as if you had
the database the entire time the
performance may suffer horribly if
you're not very good with what you do
but you don't have to worry about safety
you don't have to reason about anything
else as long as what you do between
begin and commit is safe and then I
wanted an efficient commit I sort of
said a benchmark for myself I wanted to
get it as fast as possible and that's
what we're going to talk about today in
the talk essentially you can't do better
than to message delays because that's
just send it to the remote data center
get an acknowledgement that it was sent
so that I consider that the bare minimum
two-phase commit with obviously before
message delays if you can optimize it so
anything more than four message delays
is just a waste of time
so let's go for right in the middle
three and we'll we'll get there
so the contributions of this idea was a
new commit protocol so I'm going to sort
of gloss over everything transactionally
related except the commit protocol we're
going to walk through how that works and
see how you can use proven correct
consensus protocols like paxos to
actually build a bigger system so the
system looks sort of like this and each
data center you've got a key value store
you've got a transaction manager and
you've got the commit protocol component
sitting out over there on the side and
this talk is entirely about that commit
protocol component and how it interacts
with the other data centers there's a
whole other talk that could be given on
how you build a scalable key value store
whole other talk on how you build
scalable transaction manager the codes
written it's out there it's in the paper
that I wrote I'm not going to talk about
it today all you really need to think
about is a transaction is a log think
about my sequel spin log for example if
you could take the bin log from one my
sequel instance and feed it into the
commit manager here and then actually
apply that bin log whether based on the
outcome of the commit manager you can
build this system on top of my sequel
but essentially all the commit manager
does is take that log of transactions
and returning yes or no decision commit
or abort and so there's some assumptions
here every data center running the
commit protocol should have a full
replica of the data and a transactional
processing engine so I'm assuming that
if you have three data centers you have
three copies of your data this is sort
of realistic because if you lose one
data center or want to be able to lose
one data center you have to have another
data center with your data in order to
remain available during that loss I'm
going to also state that the transaction
manager sort of follows the traditional
two-phase commit prepare operation
semantics which is to say if it says the
transaction is prepared it can later
abide by a commit decision it won't ever
say oh sorry I told you I've prepared it
but I didn't actually prepare it I can't
commit so the basics of this protocol
are the transaction may commit if and
only if a quorum of the data centers can
commit that transaction I'm not going to
go into quorum replication here I'm
happy to talk afterwards about why it is
that you can actually say that a quorum
of
data centers committing a transaction is
sufficient it does impose a little bit
on what a transaction actually means
that's also a whole other talk just if a
majority of the data centers can commit
the transaction though it will commit
now once I like I said we're gonna
execute up to the prepare stage run the
commit protocol and then actually accept
the results that commit protocol and
make them persistent and data centers
that fail to execute a transaction will
actually synchronize behind the scenes
to apply transaction so if they missed
this can be done lazily it can be done
proactively and it can be actually done
using the commit protocol itself there's
details about that that I can point you
to in the paper so like I said today
we're only talking about this red box
and if we were to take those three data
centers and draw a timeline of the
activity of a single transaction in the
system we would have the initial
execution the transaction is going to
execute in one data center and once it
finishes executing at the bottom of that
first green box the commit protocol is
going to begin the initial data center
will ship the log for that transaction
to two other data centers to replay
those two other data centers will play
the transaction and then broadcast the
outcome of the transaction to others and
so by the second dotted corazon ttle
line every data center will have
observed the outcome from every other
data center knowing whether sort of
knowing whether a majority have
committed and then we need to somehow
achieve consensus on those outcomes and
okay so here we have the consensus stuff
it seems sort of unnecessary right we at
the second dye line everybody sort of
has observed the results why do we need
a consensus stuff
well they've observed but they haven't
learned here I'm making a distinction
that you won't necessarily find in the
literature learning is there observing
is a term that I'm using loosely
observing something means that I have
seen it and I know that whatever machine
generated that event has also seen it
but I don't know
anything about what other people may
observe whereas learning means that I
have observed the original event I have
learned the event and I have guarantees
that as long as my system stays live
everybody else will learn the same thing
so observing is enough to say what I
know learning is enough to say everybody
will eventually know this and the
consensus step gives us the learning so
back to this diagram if we want to count
message delays we've got one broadcast
to broadcast the initial log one
broadcast to tell everybody what the
outcome was within our data center and
then somehow we're going to figure out
consensus in one round or in one message
delay not a round-trip one message delay
and so three message delays equals one
and a half round trips so do that we're
going to take a detour through two
generalized Paxos and before generalized
axis we're actually going to talk about
regular paxos and we'll see how we can
use these protocols to achieve consensus
in that one hop so traditional Paxos
makes it possible to learn a value
there's a prop these different
properties and essentially these
properties intuitively correspond to
what you would want from a system that
gives you very strong guarantees about
learning a value in the face of failures
there's non-trivial 'ti if you learn
something it must have come from
somewhere it doesn't just pull values
out of the air stability you can learn
at most one value it doesn't really help
you if every proposed value is learned
because then there is no agreement it's
just everybody taking every other idea
and nobody knows which idea is actually
learned there's consistency which is
that you can't learn two different
values and so that goes hand-in-hand in
stability you don't want half your
cluster learning one value in half
learning the other and then there's
liveness which is a property it says
that if some value has been proposed I
will learn some other value possibly or
I will definitely learn a value and it
will possibly be some other value
there's no guarantee that it will be the
value that was proposed there's just a
assumption that we will make enough
forward progress to actually learn the
value and this makes a few more
assumptions than Paxos makes for
correctness
so with traditional Paxos we can just
take that out rhythm I just described
and say I'm going to run paxos instance
one I'm gonna run paksas instance two
and you can run n different instances of
PAC source and what you get is a
sequence of values you get a log so
Paxos is not log replication it is quite
literally choosing a singular value and
when you choose a singular value in
succession with indices you end up
getting a nice log so that's what
traditional Paxos does and I won't go
into the details of the actual protocol
because they're not really relevant here
generalized paxos takes that and says
let's add a whole bunch of more math to
that and that generalized paxos paper in
traditional Lamport fashion has an
abstract for engineers and an abstract
for mathematicians and it's very
disdainful of the engineers Lamport
likes to do a lot of stuff like that in
his papers
the idea here is that we want to make a
partially ordered set you can think of
that like a dag there isn't a total
sequence of values things are not
necessarily ordered with respect to one
another so you can implement traditional
Paxos on top of generalized Paxos but
you can also do much more and it has
this crucial property that a acceptor
for generalized Paxos can actually
accept a value and commit to accepting
that value without doing a round trip
through anywhere else I can do it
without any network communication
whatsoever and so that allows us to do a
stay on the fast path and get a one hop
agreement so if you look here we can see
at the top of the diagram there is the
traditional Paxos when a proposal comes
in to a node and that node is not the
leader that node must forward the
proposal to the leader and the leader
can then run Paxos to make that value
agreed upon so you have at least one
network hop when a proposal comes in to
someone who is not the leader and then
you have additional network hops for
that leader to agree on the proposal and
then you'll have additional network hops
to disseminate the outcome of that
learned value and this is what makes
spanner have so many different message
delays in the body of a critical
transaction because in addition to doing
the two phase locking where you have to
contact a remote server for acquiring
the lock that server is then running
so it's underneath the hood to actually
guarantee the durability of that lock
request journalized paxos on the other
hand allows any server to that's on the
fast path to immediately say I accept
that value and add it to my set of
accepted values and then just broadcast
its set of accepted values and the way
that works is there's some math that you
can do over that partially ordered set
or the set of partially ordered sets to
learn the value you essentially find
what intuitively corresponds to what's
in common between all three of the
acceptors in between it's a subset of
the acceptors so here if acceptor one
accepts the value a that value is not
actually learned by anyone because it's
not common into enough of the acceptors
sets
similarly if acceptor two accepts B it
can do that immediately with no extra
communication to anyone else but it's
not a learned part of the value an
acceptor 3 even if except say and B
we're not yet to the point where the
learner can learn anything it's only
when a suitable number and in this case
3 requires 3 can learner actually learn
the value a and B and a and B are now
ordered with respect to one another
there's no arrow between them indicating
order so the learner can learn them and
these values can be learned in any order
it gets a little more complicated if you
want to add a little more ordering to
the partially ordered set more
constraints here you can see that
accepted 2 has accepted D before C while
everyone else has accepted C before D
the learner can't learn anything because
there's no common graph and prop in
common containing C and D there's a
conflict so when there's a conflict like
this we do have to fall back to classic
Paxos so you can see if classic Paxos
were changes C and D to something
different the learner can learn that
value so using journalized Paxos for
constants we want to stay on that fast
path and it turns out that if we take
every outcome from a data center that
decision whether the data center can
commit or abort the transaction so or
can commit or wants to abort the
transaction that's just a command in a
partially ordered set
we're going to introduce no order across
them so like a and B in the previous
slide there is no order they can be
learned by anybody in any order the
outcomes are incomparable therefore the
PO sets will never have a conflict
therefore we will never fall back to the
classic Paxos after accepting an outcome
we can immediately broadcast a newly
accepted state once a node receives
enough of these broadcasts it can run
the algorithm for learning locally and
actually just get the set of commits and
aborts from the different data centers
then it's just a simple task of tallying
okay I've got to commit I've got to
commit I've got an abort and now I've
got another commit so now I've got a
total a majority of the data centers
want to commit I can commit my
transaction and so each data center can
independently calculate the tally
without communicating with a single
leader or other data center so this
third round where I said we're going to
magically perform consensus that
actually equates to a phase to be
broadcast for generalized Paxos so
there's a large number of loose ends
here that I've completely glossed over
garbage collection in any Paxos
algorithm any consensus algorithm raft
Zab packs which doesn't matter what
garbage collection is always a problem
but it becomes a bigger problem when
you're collecting a part of a partially
ordered set because what does it mean to
actually take away a prefix of a
partially ordered set there's no real
good definition for it there's no real
way to make a good definition for it
except to sort of move in a pox where
you add a new node that is totally
ordered above everything already there
and then throw away everything before
that node and then your serve degrading
back to classic Paxos so I'm going to
sidestep that issue entirely a
transaction that comes into the system
starts a new instance of generalized
paksas for the commit protocol and when
we can garbage collect the temporary
state for the transaction we can just
throw away every state bit of state for
generalized Paxos you're talking even 40
datacenters if you're one of the largest
companies on earth that's a couple
hundred commands
you're talking kilobytes megabytes of
disk space used to persist the state for
the generalized paxos garbage that's not
much garbage at all and it's
proportional to the number of active
transactions there's also the
possibility for deadlock so I'm hoping
that people in the audience were sort of
rolling their eyes when I rolled out
generalized paxos because it seems like
a very heavy-handed tool to deal with
transactions why not just make a custom
protocol that can commit things I
actually tried to do that and after a
month or two of sitting there running
hand-drawn protocols every single one of
them had a problem where if there was a
deadlock across data centers it would
not be able to make progress and you can
get into a deadlock situation very
easily imagine you have three data
centers each data center wants to read a
value and write it to X and they just so
happen to be reading and writing each
other's values those three transactions
in the wide area are going to need to be
ordered with respect to one another you
can't commit all three of them and so if
each one starts its initial execution in
a different data center that data center
is going to vote to commit the
transaction that originated there and
abort the others and now each one is
left with how sufficient thresholds for
committing and you can actually get into
a deadlock situation especially if you
have only two transactions they're doing
this so to sidestep that I want to
introduce the notion of a deadlock
induced abort so if the local data
center detects that there could be a
deadlock it can actually add to the
generalized pack so set a command that
says I would like to retract my commit
and change it into an abort and here's
where the power of the partially ordered
set shines through if you make that man
totally ordered with respect to
everything else in the system that will
be learned by everybody else in the same
order so there's no worry about one data
center seeing the retraction from an
abort from a commit and changing it into
an port and another data center not
seeing it and then committing the
transaction accident
everybody will tally in the same way
because the the abort due to deadlock
command is totally ordered with respect
to everything else if you topologically
sort the dag of course buying to the PO
set what you'll actually do is commit
commit up abort oh wait there's a commit
that moves to an abort and it doesn't
matter if you do abort commit commit or
commit bort commit no matter what order
you process the first three commands the
fourth command will be the one that
moves the tally from one column to the
other and at that point the the total
number of commits the imports will
always be the same on every machine and
it can say oh well we've actually
already got a threshold for commit we
will just ignore the deadlock induced
abort the transaction commit that's just
the same as aborting it due to deadlock
because now it can resolve the lock
contention get it out of the system and
keep making progress there's also a
performance problem the generalised
Paxos algorithm I believe is like twenty
pages of TL a-plus and it's very heavy
math and everything specified like there
exists some Q such or some value V such
that there exists some quorum Q such
that there also exists some quorum r
where q and r have this property of
relating to each other it's this huge
mess that the only way to really solve
it is to enumerate every possible value
that you could learn and see if they
passed the check and of course since you
want to learn the maximal value you
really do need to enumerate all of the
values I haven't found any algorithm
that does better than that programming
language literature they're all like oh
we never actually want to compute these
values fast we don't care if it takes
overnight to prove our theorems but when
you're working in a system that's
running real-time transactions if it
costs you 100 milliseconds of CPU time
to actually decide whether you've
learned anything just to be able to save
100 milliseconds of latency you're
making a horrible trade-off so I had to
spend quite a bit of time optimizing the
implementation here it turns out that
there's quite a bit of pre-computation
you can do before you ever run the
learning algorithm and then the core of
the learning algorithm can degrade into
a bunch
bit operations or counts on a raise so
you can actually get by doing quite a
bit of work without having to run actual
graph algorithms without actually having
to enumerate every single possible value
and so that that was a big hurdle that I
had to overcome there's also a lot of
Paxos that I didn't talk about here it
was sort of a personal challenge of mine
to see how many different ways I could
use paxos in the system differently
there are four to five different
implementations of paxos here this sort
of came up because someone at Cornell
mentioned to me that raft was easier
than Paxos and I thought Paxos was
rather elegant and easy to use so I have
five four or five I started out with
five and one of the optimizations
optimized way so much that it would be
almost slanderous to call it Paxos so
there's a client as leader optimization
where we simply select the ballot once
and the client will leave the
transaction forever so within each data
center that transaction manager that I
showed you earlier is actually a
replicated state machine for each
transaction the clients right ahead log
is log to five different servers and
those five servers maintain a replicated
state machine that log is durable one of
those servers can just drop off the face
of the earth the client can talk to the
others and keep pushing the log and
there's the insight here that the log of
the client and the trend the client
itself are fake shared if the client
just dies and stops issuing commands to
the log it doesn't matter that the log
can still accept commands because it's
only accepting them from the client so
we can make a whole bunch of
optimizations there there was grey and
lamport's Paxos commit essentially the
idea here is that those five different
servers are logging the commands from a
client but they're also doing
interactions with the key value store
and the rest of the system acquiring
locks and so on and they need to be able
to say yes I have done all of the
requisite work logged by the client and
that's not something that they can
easily do logging into the clients log
even if there were others who could
submit commands to that log instead what
they do is they each have a singular
value of Paxos they it's a saying
value it's not a multi-value thing like
I showed before it's a single sonata
algorithm as they call it and each one
will log their desire to commit or abort
and if a majority of the nodes that are
running a client's log vote commit then
it can proceed to the global commit
algorithm there's also the generalized
paxos implementation I described here
it's fully general as far as I know save
for garbage collections so you could rip
it out and use it for your own stuff
that has a problem when running the
wide-area which is if you were to lose
one machine that machine dropping off
the face of the earth then means that
data center appeared to drop off the
face of the earth I don't want a single
machine failure making it look like an
entire data center has failed so every
acceptor and generalized Paxos for the
data center level algorithm is actually
its own recursive Paxos state machine so
the PAC so state machine is actually
agreeing upon the commands and messages
to feed into the next PAC so state
machine so there are five servers
sitting there running Paxos to agree
upon the commands to feed to the
acceptor running Paxos and you can get
even more creative than that making a
generalized PACs of state machines so
that the messages can go to the
different state machines in different
orders and now you're left with a state
machine or a machine in a single data
center that appears to be this really
really robust machine but it's actually
five machines and no one machine can
take that system offline no one machine
being slow can cause that system to lag
there's no leader to have died that you
then have to wait to reassign in the
average case there's always pathological
cases where you can force an
availability hit but in the common case
you won't take an availability even if
you just go up and pull out the power
cord for a machine and finally there's a
system called replicant which is a
replicated state machine hosting service
so you can write just straight line code
C C++ anything that compiles into an elf
binary but it doesn't have to be that
you could talk over it talks over a pipe
so anything that can read and write over
a pipe would work
so you write single-threaded code and
then you make our pcs from your client
library and it will automatically run
the code on five different machines or
ten different machines and this is feed
it the our pcs in the order of the
clients calling so you can actually do
things like cluster membership I don't
know if anyone here has ever played with
zookeeper and looked at the recipes for
locking algorithms and then looked at
the changelog for the recipes for the
locking algorithms to see how many times
it's been revised in response to a race
condition it's really hard to write that
kind of code I don't want to have to
write that kind of code instead I write
code that says oh I am registering new
server s1 and it's in data center Us
East I can add that server I can now
assign it to these Paxos groups and I
can publish a new config and everybody
waiting on the condition variable for
that config will get woken up receive
the new config and be able to apply it
so let's look at how it actually
performs because there's always a
question is of is this thing real and
the answer is yes it's currently
approximately thirty two thousand lines
of code since I've started employment I
haven't actually made it grow much
there's another forty one thousand lines
of code I stole from hyper decks and
it's released under an open-source
license the codes not production-ready
but I did make sure that every right to
disk is actually durable like you would
expect I did make sure the failure paths
are implemented they just aren't really
well tested so I wouldn't deploy and
prod the to evaluate it I went and got a
bunch of AWS ec2 instances and I did
something peculiar here I picked them in
the same availability zone and then I
added an artificial RTT between them and
I did this because I wanted to minimize
the effect of network variance in the
same availability zone the variance is
going to be on the order of single-digit
milliseconds and I'm setting a 200
millisecond RTT which means that I can
be pretty sure that everything I'm
measuring is a result of the RTT of this
system and not a artifact of an
experiment that's just running in a
adversarial Network condition and then I
had one server that ran against the
deployment and that's
actually would use a different or could
use a different node every time there's
nothing then that forced it to do so but
it could actually send the transaction
to a different data center every time
and I believe it was configured to do it
at random
and so what I did is run the TPCC
benchmark which is sort of a if you were
doing a warehouse in sequel in the 90s
this is how you would build it kind of
benchmark there's ten hotkeys on which
everything contends and what it is I ran
that in just one datacenter I used just
one of the five nodes I had deployed and
ran the benchmark against that and I got
the line on the left then I took that
doubled it remember we're going to
execute twice within consus and added
300 milliseconds which is three or
one-and-a-half RTT in our simulator
environment and then I measured three
datacenter and five data center
performance and here you can see we fall
just short that's actually a predictable
event number one by doubling the CDF
you're actually making a long tale much
worse number two when you're replaying a
transaction you're actually playing the
operations in parallel that the first
play of the transaction is actually
pulling into the client reads and writes
and the client is issuing the rights
back so there's going to be higher
latency there what looks like an anomaly
toward the bottom is actually a result
of TPCC
requiring 1% of transactions to abort a
transaction that aborts instead of
calling commit just terminates in the
local data center there's also a whole
bunch of other workloads that look sort
of like the previous graph the only one
that's different and interesting is the
stock level workload so here you can see
the CDF again on the far left there's
the red line and that's running in one
data center on the far right we have the
the long dashed line and that's what we
would expect in the middle you can
actually see the three and five data
center cases and they're very close in
performance because the number of data
centers does not actually influence the
Lea and so you have a commit they are
significantly less than what we computed
the latency would be because the stock
level transaction is actually reading
about 200 different keys
so here you can see the effect of the
serialized execution versus parallel
reacts acuter noir replay so to
summarize today I've sort of shown you
what can be done for geo replicated
transactions you can make them with
lower latency not everybody necessarily
wants to pay a cost on right to go to a
remote data center but if you're willing
to pay that cost or are already paying
that cost you can actually get
masterless geo replicated transactions
Paxos is not a one-size-fits-all
algorithm I've tried to show many
different ways you could use it and I
think the biggest lesson I'd like you to
take away from this is if you start with
a careful specification of your fault
tolerance and availability requirements
it's going to sort of force you to think
about the different ways you can
navigate the design space if I were to
take away that requirement that I must
be available all the time no matter what
happens so long as a sufficient number
of servers don't die simultaneously
there's a lot more that could be done
you could have a much simpler algorithm
you could have a much simpler protocol
but you would be taking an availability
hit sometimes and that's up to you
whether you'd like that
and so I've about three minutes left I
don't want to take public questions but
I'm happy to take questions up at the
front and thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>