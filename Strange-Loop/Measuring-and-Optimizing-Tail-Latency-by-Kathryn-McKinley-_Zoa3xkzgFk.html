<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Measuring and Optimizing Tail Latency&quot; by Kathryn McKinley | Coder Coacher - Coaching Coders</title><meta content="&quot;Measuring and Optimizing Tail Latency&quot; by Kathryn McKinley - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Measuring and Optimizing Tail Latency&quot; by Kathryn McKinley</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_Zoa3xkzgFk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you it's super exciting to be here
good morning on this beautiful day so
I'm gonna talk to you about
understanding what's in the tale of
interactive web applications and once
you understand it how the different
things that can be responsible for tale
latency need to be optimized a little
differently so I'm gonna go over some of
those a bunch of the research today in
my talk is joint work with my
collaborators 'seeing and Steve
Blackburn at Australian National
University and my collaborators at
Microsoft my former employer MD hock
salmon o makki D u shanky and Ricardo
Bianchi
so hopefully you won't be using your
phone too much during this talk but when
you are using it you want it to be super
responsive and humid we're we're
impatient people and if that phone
doesn't respond to you quickly you're
really unhappy and you're someone happy
you stopped searching and the providers
who are trying to engage you with a good
user experience don't get as much of
your attention and you abandon it so we
really care about tail latency and
that's our top part op priority in
optimizing your your your your
experience so what's behind that servers
in big data centers with lots of
networking they're consuming lots of
power and they have lots of processors
and this data comes from the Lawrence
Berkeley Livermore labs and they collect
government data and company data about
data centers and this is just a snapshot
of What's in data centers in 2014 and
then based on their data they did a
forecast so the bottom is branded one
socket that means one Numa socket so
everything's shared together and then
the red is branded 2 plus sockets so
that means we've made a real shift so
that now most servers have at least two
Numa domains which are separate memory
and processor
but still a strong interconnect in all
using the same power and then we have a
little thin bar of unbranded socket and
what unbranded means is you're not
buying it from someplace like Intel or
AMD or or any other provider and what
you're doing is your custom building
something for your specific workload and
this is a huge trend right now in in
hardware with the end of Dennard scaling
and the practical implications on
Moore's Law what we're we're seeing in
hardware is more and more specialized
components but those as you can see from
this diagram those specialized
components they aren't taking over the
whole chip the chip manufacturers are
still helping us there instead and add
on too much to this chip and the TPU at
Google is an example of that and
Microsoft's FPGA for their networking is
another example of that so this is kind
of what's under there and what we're
trying to use to give you that great
experience and then once you build it
put something in the data center
you're not done paying for it yet
because it consumes energy and the the
2000 trend was we're going to consume
tons and tons of energy but companies
like Google and Microsoft and Facebook
and Intel were super motivated to lower
that cost and so you see a nice nice
actual engineering trend we have here
that the energy that that data centers
were consuming did not go up at the same
rate as the number of servers that that
as an industry as a herb the hardware
industry did a great job doing that and
then there's a bunch of lines that are
potential predictions which is we follow
the current trend and that's that top
dotted line not the exponential dotted
line but that the flat line and then
under that we have what's called better
which is better software better Hardware
design that can help lower those costs
and that's what I'm going to talk today
about
trying to build better software that
consumes less energy and uses these more
efficiently or bigger and what is bigger
mean that means more nuba domains on a
chip more sockets and then you can put
those together and then there are some
best practices that other people have
learned and we can share them and we can
do even better so we could like even
decrease the amount of energy consumed
in in us data centers and that would be
a great thing both for the environment
and for the bottom lines of the big
companies all right so here's some quick
facts about how much money that
represents so a very small data center
might cost about $500,000 and there's
still a lot of people building their own
data center for their workloads so
that's still happening and then there
are about 3 million data centers right
now in 2016 in the u.s. these are all
estimates from that same report 1.5
trillion US capital investments because
it's not just how many build we build
each year but how many there are and
that is a lot of money and so even if
you save 1% of that in improving
performance reducing the footprint
you can save 30 million dollars by doing
less work so lots more if you improve
the capacity of the server to handle
lower workload and not even and so you
don't even have to build a data center
that's a hopefully some of the solutions
I will show you today convince you so
that says efficiency is a great
important top priority to all these
companies in order to make money and
also to be a good global citizens but
these are things are in conflict and so
how can we get posts and that's what I
can focus on today so just going a
little deeper into what's in that server
architecture behind your interactive
service is usually your request goes to
a server and then that gets branched out
to a bunch of different workers and they
logically use parallel
and replication and each of these
servers in order to get reliability
scalability and throughput so the
aggregated farm South the request they
wait for workers to respond and then we
return a response to the user
since the overall response time is is
can be no better than the slowest server
that slowest server that tail latency
really matters to your user experience
and so but that and that's usually the
99th percentile latency or higher or
98th and I'm just going to colloquial
call that the tail latency all the
numbers today I'll I'll be giving you on
on tail latency will be 99 percentiles
all right so what are the underlying
characteristics of of interactive
services as my example today I'm going
to use the leucine and I have leucine is
an open source java serve search
enterprise search and it's widely used
Disney uses it so for its to to do
search on its local site so if you go to
Disneyland you want to see Mickey Mouse
you type Mickey Mouse in leucine is
telling you where Mickey Mouse's
all right so so even though it's not
Google search it's not being searched it
is a good workload and some of the
techniques I'll talk about today have
been incorporated in being and the the
workload in being while I was at
Microsoft matched this pretty closely
and and workload that's public about
Google at the tail at scale paper by
Dean and Barroso that that also shows
this kind of work life so what is this
kind of workload so if you look at the
percentage of requests on the y-axis and
then you look at the latency on the on
the x-axis you see a lot of requests are
really short and a few requests are
really long and you see that red line
that's called the
that's the cumulative distribution
function and you can see that that has
this certain shape and that shape
changes very slowly so even if your
underlying workload look like today or
this weekend football got like
exceedingly more popular even because we
mixed football and and and I'm politics
and what happened was this CDF doesn't
change that much even though the
underlying queries that are inside it
change a lot all right so the slowest
server that's over here on the side
though unfortunately is what dictates
the tail and and we also have very
bursty diurnal workloads
something interesting happens the
hurricane happens more people search
more people in a certain area search for
certain things all right and there's an
order of magnitude difference between
this average latency and the tail
latency and so if we're going to try to
bring down that tail we have to
understand it in more detail than this
and that's what we're going to do next
so the overview of where we're going now
is we have to have better tools in order
to diagnose what's in the tail and then
we're gonna reveal three sources of
things that cause tail latency noise
systems aren't perfect some network used
some operating system scheduler queues
something bad happens queueing you have
too much load you can't handle the
burstiness so so you've got to
over-provision somewhat or do additional
replication in order to handle too much
load or work if there's actually work in
those long requests you have to do
something else instead and we're going
to show a couple of things perils them
and also using faster processors and the
insight in this talk is because that
cumulative distribution function doesn't
change we can solve optimization
problems offline and then we can
why them online by just using functions
that we learned offline and then online
we can also detect if the shape of this
curve changes a lot and if it does that
we can offline resolve this harder
optimization problem and then use the
results online so I'm going to show you
several examples of that so if you don't
remember anything else from this talk
remember that cumulative distribution
function is your friend and you can use
it offline to improve tail latency
alright oh and the second insight that
you'll see reuse repeatedly over the
this talk is that long requests revealed
themselves while your request is
becoming a long one there are certain
points in time that you can recognize
because there's an order of magnitude
difference between a slow request and a
long request all the slow request should
be done by now this long request is
still executing and there's sufficient
time to react to that and improve that
long request alright so here's the
simplified life of a request so this is
a that you have some client application
goes through the network it goes to the
aggregator then the aggregated sends it
over the network then some worker OS and
VM process it and it goes down through
the application and then it has to come
back all right how do we understand that
the prior state of the art is in this
great talk from dig sites at Google and
so if you are a Turing Award winner you
can do this too all right so you hand
instrument the system that requests that
requires deep understanding and now that
we have more and more of what would are
called microservices which are
individual components all contributing
to overall response then that's more and
more system someone has to be an expert
in and has to understand how to
instrument properly you get a one
percent budget sample so what you have
to do is turn it on and off and you have
to profile it ahead of time so you know
how much time all this instrumentation
even introduced on the critical path of
your in
after services it's also not a good idea
is that slowing you down and then you
get offline cymatics and you have
insight and you improve the system so
here's an example of one of those
offline semantics submit offline
schematics and some of the difficulty
you see in in trying to figure out what
the critical paths are through here all
right so what we're gonna do is we're
gonna show you how to do an automated
instrumentation that also has 1% and
gives you continuous online profiling of
the cumulative distribution function and
also can break down what's what's in the
different components of all all those
requests still have to have our offline
semantics maybe you don't have to being
a Turing Award winner but maybe you do
because these systems are very
complicated to improve but I'm going to
show you some ways we can do it all
right so this work is how we do this
cycle level online profiling with the
minimum district stir being of the
system so our insight here was that
hardware is just generating signals and
those signals you can read out a
hardware performance counter that's one
way to see a signal but also memory
locations can communicate signals they
can communicate values or counters and
so there we go all those things I just
said and then and all of these are
things you can read from another thread
all right you don't have to have an
interrupt the common way of doing this
is instrument the code or have an
interrupt in order to read these signals
if you're running in the same thread in
a shared process you can just read those
out of shared locations and so that's
what we did so the other core has a
processor on it for example it reads
last level cache misses per per cycle it
has
really simple loop while true for
counter in the last level councils and
cycles I'm going to put into a buffer
and read those counters so I'm not going
to do much processing on that I'm going
to do the processing on it later offline
I'm just going to stick it in the buffer
and you can also control through through
cache partitioning mechanisms you can
also control how much memory this uses
and how much it interferes with the
shared caching so or you can actually do
it on the same core in in hyper thread
or simultaneous multi-threading cores
that are sharing resources but it
interferes more and so then you have to
be a little more careful and you have to
correct for the IPC that that that shim
are our profiler is causing you and so
you still just stick it in there but you
do the math later alright and this lets
you read memory locations or Hardware
contexts so this seems so easy why
hasn't somebody done this before because
that doesn't work quite perfectly so
here's a instructions per cycle
this machine issues for instructions per
cycle at the maximum and so we have some
numbers over here greater than 10 mean
that's a problem all right and this is
the log of the samples and and why does
that happen
so the problem is the way we're taking
the samples in that loop is not atomic
and you you read the counter you you
start the the the counter you read some
other ones and that looks pretty good if
those samples are taken at take the same
amount of time then you've got good
fidelity data but occasionally when
you're taking the samples something else
happens like you get a cache miss or you
get paged out and so that time at which
you're taking the sample changes and
then you can't compare the two samples
you don't have a correct sample so the
to detect that is actually not too
amazingly hard but you can do is you can
read out the cycles at the beginning of
each of two intervals and at the end and
if the the two times agrees cycles per
cycle agree then that's a good sample
because it took the same amount of time
to take that sample and the other great
thing about this technique is if it over
time it changes how long it takes it to
take a sample because someone else
starts interfering with you or you have
shared hardware this can adapt to that
change because it's not saying you this
is how long it should take it's saying
just compare the ones next to me did I
take the same amount of time if I did I
can I can do that so we're using the
clock as the down ground truth we
compute this ratio of cycles per cycle
and then we can simply get rid of bad
samples so now if we go back to our
problematic trace and we look at that
yellow now that bar that I put back on
there is cycles per cycle so you can see
that cycles per cycle corresponding to a
lot of the noise out on the side so when
we do the filtering and we make it just
consider the ones that our cycles per
cycle is is one percent of one then we
get rid of all the bad data that we were
seeing that we it's clearly wrong
because you can't issue more than for it
once but we and we also get rid of some
that were too short no do we know this
is perfect yet do we know it has perfect
fidelity no of course not there can be
other things happening but this is much
closer to what looks like ground truth
and so here's an example of how powerful
this approach is if you instead of using
in in or if you use interrupt driven
sampling right now and you take the IPC
of individual methods and lucern you get
what you would expect to get if you're
sampling something
that has a high signal-to-noise ratio
because you're sampling it too low you
get a flat it's all the same and even if
you go up to the maximum of a hundred
Hertz you get the same but if you sample
with shirmat ten megahertz you get a
much more fine-grained a knowledge about
what's going on in your program and here
we can identify method number four as
being a real problem so how and then
overheads are important if you're
reading it from the same core you're
always going to have a lot of overhead
if you read from another core and you
try to read at 30 cycles you can but you
get a factor of two overhead but if you
slow it down to reading about every
1,000 cycles you can basically make this
not interfere with that thread on the
other core so now that we have the right
mechanism to watch this let's configure
our our Shin thread to look at thread
IDs time stamps and program counters
things we can always read out of memory
so those are awesome but we need to also
look at request IDs okay so the software
you're using has to say a little bit
more about what it's doing in this case
it just needs to give you a request ID
so when we do a sample we know this
thread is working on a certain request
because the same thread will work on
multiple requests so that's all we need
to add to our our our program to get
this to work all right
and so now we can take that cumulative
distribution graph and now what I've
done is spread it write it down a little
differently and I have requests groups
from the slowest to the fastest and the
latency on the y-axis and now I just
have client license latency versus
average queuing time so you can see hey
all right long requests have if we
concentrate over there along the quests
have have some queuing time and they
also have some work time so now what
we're
to do is just take the 99 percentile and
look at those in more detail and we're
going to use the program counter to tell
us what where these what these requests
were doing when they when they incurred
that long latency so this is the tail
these are the longest requests and now
you've been waiting for 10 minutes and
we're finally at this part alright so
what we see is in many of the requests
there is a lot of queuing at the worker
and there's some blue there's some
Network trash and so what does that mean
that means my network controller like
maybe it's not getting scheduled
properly or maybe it just has too much
work so it's causing sub queuing so
network imperfections and you see a
little bit of orange and that reveals OS
imperfections this thread is ready to
run it's sitting around on the core
somewhere but somehow it's not running
but then you see lots and lots of green
some of the very longest requests are
mostly work and no matter what you do
about queueing and making your system
more perfect at getting rid of noise you
won't get rid of work in long requests
by doing that all right so we're gonna
talk about techniques a first a
technique that concentrates on noise and
random events and then we're going to
talk about techniques that don't have
noise in them like how you get rid how
you reduce the tail and do that
efficiently with work so the insight
here is long request to reveal
themselves so we can do something
specially about them and this is
regardless of the cause but if the cause
is noise alright we can replicate and
reissue the basic system architecture I
showed you already has replication in it
so if one server so probably another
ones not unless you're overloaded so so
you do need to take into account load
and so you can use the the CDF to tell
you both the cause
than the potential for reissuing a
request so let's say you have a fixed
issue time which was the state of the
art until a few weeks a few months ago
so you what you would do is say I want
to I have a budget to reissue 10% so
that's not a perfect solution because if
the reason that you experiencing slow
latencies is because you're overloaded
adding 10% is can it's not efficient
you're using more resources and it's
also not handling the things that
actually do more work it's just making
more work for the system and it can
cause some queueing more queuing
so maybe you want to move it over to 5%
reissued in order to mitigate those
extra costs but you still have some
budget at which your your your reissuing
and that has a cost all right so
recently some my former colleagues had a
really nice paper at spa about
probabilistic reissue because you have
different reasons for tail latency
let's move this line over so we can
reissue sooner but let's reissue
probabilistically and not always do the
reissue based on the budget and then by
adding this randomness they actually
have a beautiful proof that shows you
one reissue time with randomness is
equivalent to multiple reissue times
based on different features and so this
gives some beautiful results that are
copied straight out of the paper because
they didn't get me the slides in time so
so so that's so so on the far left we
have 99th percentile on the y-axis of
all three of these graphs the far left
is their single random random reissue
which is the bottom red line and then
the state of the art prior to that which
is this blue line which is the single
one time you always reissue at that
delay so if your budget is 1% for
example you agree issue you think
out the D from the cumulative
distribution function and you'd always
reissue everyone so in their approach
your reissue budget is one percent but
you get to do it sooner and you get to
do it randomly and you can see it has a
beautiful curve now latency versus
reissuing as a function of utilization
on the second graph shows you the
problem when the reason is over
committed you can when you start
reissuing even at a very small budget of
six percent or five percent then you're
adding too much load to the system and
you're just making things worse so you
really need to keep track of load and
then this is their best reissue result
for loot and these are leucine results
because I promised you I was going to
show you lots of stuff about the scene
and so here they definitely reduce the
tale by doing this reissue but they have
a cost all right so that's a good idea
and we people should use that for the
responsiveness but if it's work it's in
the tail which we just shot too often
work that doesn't speed up the tail
so what speeds up things
Carles home we have lots of perils among
these machines and so I'm going to show
you why throughput optimizations didn't
work for tail latency but we can use
parallelism on a single machine to
improve tail latency and then I'm going
to talk about another thing that's on
every machine today which is dynamic
voltage scaling to take to identify the
tail requests and improve those and then
a solution that's on your phone but not
yet in servers but could be it's a
symmetric multi-core where you combine a
big core that can execute responses
faster and little coarse and why do you
want little coarse won't that slow stuff
down well you're gonna slow down the
little request the shorter requests and
so you're gonna sacrifice average in
order to improve the tail and that get
you efficiency in capacity and I'll show
you that and then we'll be done with the
talk all right
so work
Carlson param has historically been used
in these systems for throughput we hand
at multiple requests it does each one
completely independently and that works
great so the idea for using peril some
for tail latency is it instance you
could have every request be parallel
alright so this is a fixed level of
parallelism every request comes in and
you peril sight why isn't that a good
idea well in the past we didn't use it
for throughput because peril sim has
overhead for the short requests it's
useless
and so those see overhead so that hurts
throughput in systems that have uniform
distributions of requests on all right
and that's not the tail so if we
judiciously use parallelism
after some delay only on things that are
going to be in the tail then we do pay a
little bit of overhead but we ramped
down the tail latency and and we shorten
queuing and what's you shortened those
tails you actually reduce queuing of the
overall system
so in our system in steady state you see
this little diagram you have a bunch of
little short requests running and then
you have some longer requests that has
revealed itself and you've incrementally
added parallelism to it all right so how
do we do that
well every D this just shows the
animation I'm not ready for this
all right so so if we do a thread that
is at fixed D okay so you just wait a
little bit and then you add a fixed
amount regardless of load or the number
of requests in the system then you get
the following graph so here's the
sequential time so if I say every
request is going to be 4-way perils them
at delay zero then I get a good graph at
very low latency so I improve the tail
but I can't have
enough load because I've added overhead
to the system and I've and and and that
overhead decreases the maximum amount of
requests per second so if I wait a
little while and then add my peril huh I
do better and if I only wait 20
milliseconds I can't I'm almost out
there at sequential time at high loads
but if I wait longer then I'm more
likely to apply to parallelism only to
the tail and if I wait even longer I can
push my request per second out even
further so but we'd like to have the
best of all those worlds that green line
so can we get to that green line yes so
what we do is we take those cumulative
distribution functions and then we
profile the parallelism and the perils
of efficiency on the longer requests and
so that gives us a target amount of
parallelism so that gives you in our
systems we saw and this was implemented
in being we saw saw about four was the
right number sometimes three would be
the right number of threads to add to
work and and most of these many of these
systems are very parallel and so then
what sorry
then what we do is offline we
exhaustively explore a schedule that
uses load and the amount of parallelism
to ask to minimize response time and
maximize Matt and and minimize the
response time of the tail as well as
maximize throughput and so we solve an
offline in optimization problem and that
gives us a little interval table and
what oops
and what that interval table tells us is
based on the number of requests in the
system
how much curls have to add and and and
that's our request is the amount of load
so let's say we have two jobs come into
our system at the same time because
there's no load on the system we
paralyze both of them and so
shortens their execution time and so
that reduces the probability of queueing
in the future and then we add this green
one the screen run really didn't need
parallelism but we give it to it anyway
because we're not doing anything else
and we're and we know it will will
complete quickly in this case all right
so now the blue request comes in
slightly later and now we're at three
and it says okay parallelism of one but
we don't have resources for it so even
though we started we might be slowing
down the green thread a little bit if
they do
Subterra ng of the resources or this
thread just a sits in a wait queue so we
can run just as soon as the other
threads stop running and so then we had
the purple thread come in and it it has
to wait until it gets to execute at all
but then the green thread leaves the
system and we go back to three and the
purple thread is at peril some one and
so it gets to start executing and as
soon as the this thread leaves then it
would execute then it could have more
parallelism and so that gives us this
beautiful graph that hugs the bottom of
the line I showed you before and lets
you both increase the the that's 99
percentile latency release the requests
per second and shorten the tail latency
and so you can use this in two ways you
could just say I'm going to use this
approach and buy fewer servers and that
saves you more money or with the
existing servers that you already have
you can reduce the tail latency and the
energy that this system consumes all
right so that's our judicious peril
'some and now we're going to try to see
how much of the rest of the stock we can
get through so now instead of doing
pearls oh maybe that's too much
change to the system maybe that your
requests don't paralyze very well what
you can also do is use the hardware
to help you so you can use dynamic
voltage scaling to focus and speed up
the tail so currently all requests run
just as fast as the server will let you
run but if what if instead we run the
short request at a lower frequency and
so they consume less energy and we only
run the long requests at the highest
frequency and so we control their tail
and we lower the total cost of ownership
because we lower the energy that this
system uses and then you and that's
available in service today so if you
built an asymmetric multi-core this can
work even better because if you have
micro architectural techniques at your
disposal you can do much better in terms
of the efficiency than devfs does dvss
is a good user but it has a certain
range the the microarchitecture
techniques lets you use a much bigger
range of performance energy power
trade-off alright so you've seen the
parallelism insights and so this is has
a very similar flavor to it and it's
called slow to fast so what we're going
to do first is when a request arrives
we're going to execute it on either at a
lower voltage and frequency or we're
going to execute it on a slow processor
and then the challenge is of course how
to pick de when to migrate and what if
in in the your power domain the fastest
speed is not available or in your
asymmetric multi-core domain the the
fastest core is not available so the
insight is to use the big core just
enough to get your to get your tail
target latency and then that determines
when to migrate but the other idea is
when you have competition for the
asymmetric multi-core then you want to
migrate
oldest first so unlike classical
scheduling for interrupts when
something's run a long time and you say
oh you're a bad job we're gonna schedule
the operating system is going to start
interfering you and giving you lower
priority we're gonna always give the
longest executing time the highest
priority and so if we become crowded on
the little course the oldest oldest
request gets to migrate first to the
biggest core so in steady state we've
created a system where the youngest are
on the slowest if you have a medium core
than the middle-aged or on the medium
core and then the oldest are on the
fastest core the opposite of real life
all right so then dynamically you're
have a controller design where you take
that target input and you take the
cumulative distribution function and you
take a notion of load and you feed that
in and you come back with the telling to
you express can you use basic controller
design we just used off-the-shelf tool
to design our controller and then we
modeled the space of the its this gives
you a linear controller so we modeled it
piecewise for for systems that had more
than once for more than one speed so the
state-of-the-art when we started this
work was the system called Pegasus from
Google and Stanford and what they had
done was noticed well a low load we can
usually make the tail latency so at low
load will slow all the course down so it
was an all the course went slower and
then as load increased they increased
the speed of all the course together so
our approach is doing a per core
improvement and we also explored a bunch
of different configurations but for the
purposes of this talk we're just going
to talk about a two of them and two and
in this framework and this framework you
can adjust so you can optimize tail
latency only and you can say I don't
care how much energy I use I'm going to
rein in tell latency and so
that uses a zero threshold in our system
so you run on the fastest court if it's
available and then the eet l is the
ideas I just spoke about where you give
that target latency and you try to use
that big core that fastest speed only as
much as you need it and this min does a
much more energy efficient solution so
here are the results of those three
systems so on the left hand graph again
we have 99th percentile latency in
requests per second across the bottom of
both graphs and then on the far right we
have energy that this system is
consuming and these are using devfs on a
Broadwell servers server level machine
and so you can't see Pegasus because
it's under energy efficient tail latency
in terms of the 99th percentile latency
but but it's showing you that both of
them can meet the tail latency target
that you sent for a wide range of
requests per second until you just get
to the exponential part of the curve
where you have too much load and note no
nothing you can do in the this system
but that you can lower that even further
by using it judiciously just for the
longer requests with with the the
reddish line on the bottom all right
but now go over on the requests per
second and normalize energy you pay for
that in energy you can't like if you
have a system that's not meeting its
tail latency goals and this is the only
way to do it say your goal it was a
hundred and fifty then then you could
consume more energy and you could get
the system to meet your goal if you had
a software hardware trade-off here
essentially but if you but Pegasus gives
you some energy but benefits and it's
meeting our target which was 200 in this
case but energy efficient tail latency
which is this dynamically doing it per
core gives you a gives you another 50
proximately 10 10% improvement in energy
which makes can make a big difference in
the long run now let's look at since we
don't have a server class asymmetric
multi-core which is a MP we what we did
for our methodology was use emulation
and if you're interested in the details
I'd be happy to talk about it afterwards
and so in this configuration I'm showing
the results you just saw with devfs a
first through the three systems are
still on there but I've added two more
systems and those two systems are in
green and those are the devfs with EE
sorry the the asymmetric multi-core just
optimizing tail latency or trying to do
tail latency and energy efficiency so if
we look at just tail latency on the
devfs versus amp we see that you can the
asymmetric multi-core pushes you out in
the requests per second which is great
because you can handle same capacity and
these machine these two systems are
configured so they burn the same they
have the same amount of hardware and
energy and power budgets so that they're
they're comparable systems and then if
you look at the requests per second and
they're normalized energy you can see
well under low load they burn the same
amount of energy but as the load gets
higher and higher the the the benefits
of the underlying architecture get more
and more exposed and you can see that on
the normalized energy so the more low do
you have the more energy you save and
then if we look at just the green lines
now we see that we can really push out
the requests per second with and do it
energy efficiently because we're staying
down here at this best energy level for
the whole time and so that concludes the
technical portion of my talk and so what
I've talked about today is is to
optimize anything you must understand it
in more detail then then you do now and
and we needed new tools in order to
diagnose the tale because of the
institutes in sutures situation where
the tail is often caught a rare event
and caused by things that you can't
capture and so we needed new mechanisms
to do that and then by separating out
the causes of causes of the tail which
were noise queuing and work we need
different techniques and I didn't talk
about anything today to do with queuing
but we the same mechanism that helps us
profile we've actually shown how you can
help diagnose queuing and do get rid of
a bunch of queuing in different ways and
then for noise I showed you results that
showed how to do replication so it uses
less resources and is a more efficient
and mechanism work of others and then I
also showed you so some work of my team
on how you could use judiciously apply
your resources to these long requests
when there's work in them
and get a lot of benefit from that so
what you should take away from today is
that cumulative distribution function is
a powerful tool for this offline
optimization and so you can use much
more aggressive techniques when you
characterize your system this way and
that tail efficiency is not equal to
average or throughput so some of the
assumptions that we have about
optimizing throughput throughput for
these systems need to be tempered with
with sacrificing a little bit of the
average in order to get the tail and
there also happened to be some queueing
benefits in there that aren't in your
standard network queueing class that you
need to become aware of
that that one of the things we're seeing
in the hardware is more and more
heterogeneity a lot of focus that the
architecture community has been building
about specialized speed up things for
certain workloads what I've talked about
today is software that can can exploit
the differences in the workload to match
the software to the hardware and but
still use more general-purpose
processors which hopefully there'll be
more techniques like this so that we
don't all need a hundred different
specialized components on our chip which
is too hard to manufacture or to
reconfigure within FPGA
every time we have a new workload both
of which have huge challenges for for
the engineering of that and I had a
great time and I thank you for your
attention and I'll take a few questions
I think I have two minutes for questions
so he let me repeat he just said even
when you do some of these things they'll
still be a few long requests even though
they're that 1% will will have a shorter
but there's still be some one-percenters
so what do you do about those there's
still some things in that 1% and so what
do you do about that well you can
iterate on this process well I think
that one of the things that people have
done to a good good effect is is-is-is
have enough redundancy in their system
or not give perfectly precise answers so
you don't wait for some of those longest
requests so that's one thing but they're
still there gobbing up your system so
you you have to have dynamic techniques
detecting them and doing something right
but the fewer of them the better
engineered your system these sense
systems are highly engineered so a lot
of people spend
I'm trying to get more and more stuff
out of there so hopefully they will
reduce it but you'll always have
occasionally in the blue shirt so we
used all of Wikipedia English I meant to
mention that on the when I described the
workload we used all English Wikipedia
documents which were like four gigabytes
of data or sorry yes filling up our and
a big machine filling up our memory and
we used thousands of queries from test
sets and then we did validation on front
we learned on 10,000 and then had a
2,000 queries that were our test ones
and so when I showed you the 200 queries
at the end those were the 200 in the
tail of one run when you do a different
run you get a different 200 and only a
few like only about 10% which are the
ones with the green always appear in the
ones with work always appear in the tail
that's a good question sorry I didn't
mention that sooner and so and then we
matched we we showed that the
instructions per cycle of leucine was
pretty much matching the reported
instructions per cycle for Google and
also for that we observed directly for
being so that it was a good match for
their index serve when you don't hit in
a cache so you can implement you can
implement the scheduler parts of the
parallelism and also of the running on a
faster core you can actually control
that at an application layer if you have
control over the entire machine for this
Apple for this service but if you but if
you have multiple services running on
the machine then you have to have some
cooperation from the operating system
good question oh sorry I didn't repeat
do you need to do can you do this at the
application level or do you need to do
this at the operating system level and
so you can do both but if you have the
machine to yourself you can do it all in
the the application layer and the Lucene
results were all done at the application
layer so I think I've run out of time I
really thank you for your attention
enjoy the rest of the conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>