<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Performance Engineering At MasterCard&quot; by Ted Boehm | Coder Coacher - Coaching Coders</title><meta content="&quot;Performance Engineering At MasterCard&quot; by Ted Boehm - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Performance Engineering At MasterCard&quot; by Ted Boehm</b></h2><h5 class="post__date">2015-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Fq97BvwoJbU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">who's ever heard of Albert Gonzalez not
Adrian Gonzalez but Albert Gonzales
anybody couple people yes yes so he so
Albert so so Albert gonzales was
basically the lead ring the ring leader
for the TJX in the heartland payment
fraud that happened a few years ago and
widely known and shown that that was a
hundred and seventy million cards that
were compromised because of him and
because of his teams that were dealing
with this fraud so there's people out
there do this and and also the targets
and Home Depot's I think a lot of us
heard of that a little later and locally
we've got a grocery chain called
schnooks that was targeted a couple
years ago and so it was really big here
in st. Louis because it was local it was
hit hit home and fraud has really taken
lots of different steps so before you
know years ago I think when I first came
in the big thing was sequel injection
right people were stealing stuff through
websites and we're one equals one
you know semicolon and you know select
star from credit card table and you know
it all returned back well luckily the
industry has gotten a lot better and a
lot of those things and we don't deal
with as much of that art or hacking URLs
and going to somebody else's account a
lot of that doesn't happen either that's
not out there but for the most part a
lot of that doesn't happen anymore so
now hackers have gotten smarter they get
into the POS device the point-of-sale
device room you swipe your card and so
not reading off a disk anymore or off
out of a database or out of a flat file
they're taking just memory because you
know because that's somewhat stored
somewhat in unencrypted so they're
sitting there taking all this memory
just blindly taking memory and then
after the fact they go through and go
huh what looks like a card number this
looks like a card number this looks like
an expiration date this looks like
magstripe data so they start taking that
so there's blindly taking stuff in some
cases and that's how they're starting to
get they're starting to get their their
cars and they can do that as we've seen
without anybody even noticing for weeks
or months or sometimes even years there
sitting there just taking cards all you
know over and over and over again so
about a year a little more than a year
ago
MasterCard announced this product that
we were starting called safety net and
it's a product where we can look at
transactions globally and try to find
these this fraud that's being attacked
either our processors or for merchants
or anybody else that's getting attacked
we can kind of see a global view of that
and be able to stop that before the
banks or the card holders even know that
it's happening so that's kind of a big
thing that was was announced a little
over a year ago and so I was brought in
to I was not a performance engineer but
I was brought in to take a look at the
environment and to try to make sure that
we can perform at a at a level that we
can do this all real time so as cards
are being swiped we can take a look at
those cards we can analyze what's going
on we can look at history traits of the
card or the region or whatever it might
be and we can make determinations and
stop the fraud before it even happens so
that's a big project that's been going
on with MasterCard and so one thing I
want to talk about is kind of what I've
learned over my past year working with
this and trying to get our our platform
to tens of thousands of transactions per
second all measured in tens of
milliseconds so that's it was a big
effort that went through to to get us
there so we talked about three guiding
principles I'm gonna finish up with
three good steps to get this through to
your managers on on things that you
might find so who in here has like had a
great idea this is a perfect this is a
performance issue that's in my
application and I need to fix it your
manager goes you know I don't got time
we don't we don't have to do that who's
had that great idea I've got an idea but
they don't want to they don't wanna work
on it right there's some reason they
don't want to do it well my last the
last three things all this should kind
of revolve around that but then my last
three steps are good guiding principles
on you know what how to do that how to
show your manager here's the problem
here's a solution what do we do next
okay so my three guiding principles here
that that I've kind of noticed as I've
been moving through this is the first
one is tears
second one is tools and the third one
one of my favorite strategic strategic
may not be in the dictionary but it was
coined by by a great theologian of Will
Ferrel and Saturday Night Live when he
was talking when he was imitating a
previous previous president of ours but
that president used it in his
administration quite a bit after after
that episode so strategic will be our
last one
so our first principle tears so a lot of
times you're forced into tears or layers
or whatever you want to call it in your
application you might have a web tear
you might have an application to your
you might have a database tear and a lot
of bigger companies might force you into
that right and so so we may already be
doing this to a little bit but there are
also times especially when this platform
was originally built which was you know
maybe eight years ago we we thought we
needed to kind of combine things we
still were kind of centralized let's put
everything together and make everything
you know work faster because usually
historically the slowest piece of our
application is what the i/o right
it's the i/o let's go go to disk don't
go in network don't do any of that right
it's slow slow slow don't do it and we
need to be fast fast fast
so kind of the original thought was to
kind of lump everything together or make
it kind of centralized so this is
obviously a an extreme approach but we
everything was on a physical server so
we had so our our platform has a bunch
of different tiers in it a bunch of
different pieces of things that happen
all in tens of milliseconds to say hey
this is fraud or this isn't fraud or
this is kind of a level of fraud that it
might be so you we can all make
decisions at that point and so this can
sometimes work really fast at low
transactions at low volume it can work
really really fast but when you try to
ramp that up and you really try to get
some volume going through this you're
gonna have bottle you have a lot of
bottlenecks right you might have one
tier that's a bottleneck so maybe my
rules engine is a bottleneck it's very
CPU intensive it can only do so much but
it's but other pieces of my application
are taking away CPU from my rules engine
so it can't ever live to the you know to
the levels that it can live at
because other people are even holding
him back and some of these things have
license fees and and different things
like that that we have to deal with and
so if I can't really utilize the license
that I've installed on my server then
I'm wasting money I'm wasting time and
money and everything else and then if we
need to grow the capacity if we need to
scale this at all it can be a headache
in a couple ways right I mean we've got
a reconfigure every single tear in our
environment we have to redeploy
everything we got to make sure
everything's running on this thing all
the time it's it didn't really work we
had a hard time scaling this years ago
and so we obviously went to more of a
tiered approach where we put all the
like processing because we distributed
like processing on separate physical
machines so we moved our rules engine to
its own server we move our database to
its own server we had you know an
analytics tier we had a management tier
that kind of dealt with how to move this
transaction through its life and this
was made it a lot easier to scale right
because if my rules engine was my you
know kind of when I needed to scale my
rules engine cuz it wasn't performing
like I wanted it to then it was easy I
just had add a rules engine everybody
else stay the same I'm my only licensing
came from a rules engine made it very
easy to scale so tiers a tiered approach
is when you really need to scale up and
you need to get a lot of transactions a
lot of throughput is a great approach so
our second principles tools and one of
the big tools in your performance
environment should be your test
environment right and we all know that
we want production are in production
tests or a production environment to
look exactly like our test environment
right not at all all right it never
happens it ever works I don't even want
it I mean I don't know how many servers
you might have in your environment but
if you have tens or hundreds of servers
in your environment in your production
environment you think about a hard that
is to manage I don't want to manage that
in my performance environment either
it's it's near impossible you can't get
the data right you can't get the servers
right it never ever ever works it never
equals each other you know and nor do
you probably ever want them to equal
each other it's always too much too much
money to do it and it takes way too much
time too much effort too much
configuration so we know that that's
really not realistic and most any
environment that you want so so a couple
of things that we did to try to fix this
is we created just a small sliver of
what our production environment might
look like so we try to deploy everything
to somewhat like physical machines we
tried to make the network the same we
tried to do as much as we could didn't
always work but we try to do as best as
we could through that and one of the
original testing methods that that was
that they're working on and was working
on right when I first started was just
an Endon testing approach so we had this
whole environment set up we could always
run transactions through it we can't I
said okay we're gonna run you know X TPS
500 or a thousand TPS through this
system and make sure it look as at least
as good as it looked before right and it
helped a little I mean it found the big
things that were glaring oh you really
screwed this up when you made this code
change but it didn't really tell us
everything so when I ran this into in
protein 2 in tests at whatever 2,000 TPS
let's say I could I could get it up to
then I would find that one of my tears
would break so let's say I'm gonna pick
on the rules engine the rules engine it
just couldn't handle any more it's very
CPU intensive it's it fell over at 2,000
TPS and we said okay well our we know
our environment can do 2,000 TPS because
our rules engine is kind of the limiting
factor in our environment and we then so
the management came and said well how do
I get to 4,000 TPS easy add another
rules engine right no problem just add
another rules engine but the problem
there turns out to be well what if my no
sequel database can only do 2500 TPS I
don't know that until I get that into
production right I mean I'll get my my
new rules engine in production and I'll
run it up to 2,500 I think what what the
heck's going on I can't get past 2500
well my no sequel database is now
falling over or whatever the other tier
is that it might be so we took this an
extra step and isolated our tiers so we
could determine exactly how much each
tier each physical piece of software
could do on each physical piece of
server and so we would create our test
driver and one of the first things we
tested because it was the
kind of the biggest question in our
environment is how much our messaging
tier how much could it really handle
because it was fairly central to to our
environment and so we just drove it as
hard as we could until it broke till it
just stopped working anymore and then we
added another tier if we in this case we
knew we use the messaging tier to drive
our rules engine tier because we knew it
could outperform the rules engine I
didn't I don't want to add I like I'm
kind of contradicting myself as I said
well isolate your tears but now I'm
adding tiers onto each other but the
benefits of this is this looks more like
production than if I created my own
version of a test driver to test another
tier and I knew that my messaging tier
was definitely gonna outperform I knew
exactly what it performed and would
potentially outperform my rules engine
tier which it did so we would push it
hard until it died and we said okay we
know exactly how much my rules engine
tier can do on this set of hardware and
like I said we would stress test these
so we would stress test them until they
broke and now break can mean a couple
things you could break something where
the application just quits working it
falls over it seg faults and no recovery
no nothing right or you can just push it
until it just stops working until it
just stops accepting you know additional
messages and it just doesn't go any
further so when you do that you got to
find the bottleneck you know that's the
big thing you got to find is where's
that bottleneck at is it CPU are we just
maxed out at CPU or we max out at i/o is
it just a code issue it was there's
something wrong with a code that's
causing the problem those are all the
things that we kind of looked at
whenever we got to that point where we
couldn't push a tear any further than
what it was plenty of commercial tools
to do this stress testing we do we used
both commercial tools and we built our
own to kind of simulate transactions
coming across the wire one thing we did
underestimate is when we built this
environment out and sometimes our test
drivers weren't strong enough to push
our push our clients or your tears
enough so we didn't have enough test
drivers really push so we kind of at the
end had to build up some new test
drivers and new
jeans to actually push the the volume
onto a single tear to make sure it until
it broke until we found that breaking
point so that was one lessons learning
that we had is we didn't have enough
test drivers one thing just a comment on
a code level change that we found we
were using a library from another client
and we were pushing it hard pushing it
hard and we can only get it up to
whatever TPS you know some rate of TPS
but my CPU was fairly low and we
couldn't figure out what it is couldn't
figure what it is but wouldn't push any
further we ended up finding that they
had a debugging setting we didn't we
didn't specifically set a logging
setting so if he didn't specifically
said it they set it to debug which seems
I don't know I wouldn't normally do that
so it was logging things but to nowhere
it was not going to disk so we weren't
seeing her just fill up go there's debug
messages it was going to its back into
Ray and logging all these debug messages
to an array and that array was blocking
and ended up just stopping our
application so that was definitely one
example we saw where the code of a
vendors library was causing us problems
another tool that I want to talk about
is tracking metrics the tools that you
can do to do this or everywhere there's
a lot of different ways to do this but I
think tracking metrics are an important
part of performance and it has to start
in a couple places first of all in your
application design at the beginning of
your thoughts of building this
application how big is it gonna be what
am I gonna do how am I gonna track this
information am I gonna for the database
might not write it to a file what am I
going to do with it
how am I gonna track it and then define
the important things that you want to
track right I want to track volume I
want to track into end time I want to
track each tears time I want to track
how much time it is take to get to i/o
these are important to me because I
think they're gonna be maybe problematic
now the important things can change
right they're gonna change throughout
the life of your application you know IO
is now not a problem it's it's SSD you
know that's not a problem anymore I'm
not worried about that I'm worried about
this over here now and so that's all
gonna change as you move through this
and then be able to toggle more detailed
information as you go and not you know I
need to put a change request in and I
need to bounce the server no no you need
to be able to do this
you know easily you know it production
run time something's happen and I want
to see what's happening you know I'm at
peak time where's my problems at I'm
going to turn on some be debugging or
some info level logging and I want to
see what's happening for you know five
or ten minutes or an hour without too
adversely affecting my platform all this
kind of adversely affects it a little
bit but the benefits of having this
tracking information as is immense
also your server your server tracking
that was a that was huge for us we came
in there was very little server tracking
that we had turned on we ended up
implementing some vendor code to do to
do CPU and i/o and piece that's all
different things like that and just kind
of track those and graph them and we
found so many problems in our in our
current platform because nobody looked
nobody even looked we could see two
servers that should be doing the exact
same workload and this one was you know
50% CPU and this was 20 or this one was
and I had a you know longer latency than
this one and we found lots of different
reasons for that but until nobody until
anybody looked at it he just went on
forever and we had bad performance
because of it and continual this should
be done continually all the time as I
said always we should always be running
this because as soon as you go oh I need
to know what happened because something
broke or something's about to break by
the time you turn it on and time you get
it all running and working it's the
problems gone away more than likely you
know now you've missed your opportunity
to grab the few little CPU metrics or
i/o metrics that you needed to really
find out what the problem was and
reporting and reviewing obviously
important that's kind of what I
mentioned about when we start tracking
the CPU and Linux stats and then the
blame game this is one thing I want to
mention is that how many of you guys
have gone out and you know you're
working with your application and you
find a problem you know it's not working
the way you want you go that's you know
what that is that looks and smells like
a network issue right that's a network
issue it's got to be a network issue hey
you know Joe you're my network guy I've
got an issue my applications having a
problem because the network what does
Joe say not me let me look
I'm good networks good network all looks
good this is an application problem and
you go back and you look and you think
oh my gosh is it you know I'm looking
it's gotta be network it's got to be
network he's gotta be network and unless
you have these metrics you know we got a
lot of pushback that same way not me
it's not me whoever it is not database
not network and so we had these metrics
and we said hey listen you know this
one's tracking a lower latency than this
one or higher latency than this one this
one you know I looked at its max IO and
it's 40 milliseconds higher than this
one oh you know the storage guy fine
goes oh we got a broken storage head you
know what's going on it's like yes
that's the problem so when you can have
this these metrics all the time and
you're always looking at them and you
can say yeah here's my problem here's my
metric in my on my server that says this
is the problem then it's a lot easier
for those other people if it's the
network to take a look yeah you're right
it is Network let me take a look a
little closer but the main thing with
the blame game is the first thing you
should do is blame yourself I think the
first thing you should do is blame your
application you know or if you're the
network I blame the network whatever it
is that's who you should blame you
should look at yourself and say is this
me let me let me make a hundred percent
sure that this is not my code and find
all the ways that I could figure that
out I've left it all up behind it is
definitely not my code this points
directly to you no network or database I
don't mean to pick on network guys all
day but maybe I will the you know always
look at yourself first because you're
gonna lose so much face right when you
when you blame the network I it's a
network problem are you kidding this is
a network problem and then two weeks
later you find out is an application
problem then all of a sudden the next
time you want to go to Joe the network
guy he's gonna probably not even look at
your email or answer your call at that
point so always look at yourself make
sure it's not you before you start
blaming anybody else so here's a finding
I wanted to show us this is our MQ tier
where the first time we started pushing
it so this is a every all of our CPUs on
the server and so we started pushing and
pushing and pushing and we got well past
what they thought we were gonna get to
so we said oh this is great I'm like we
got four times five times what they
thought we would get out of it we said
well this is great
you know what the remote results we got
we said let's go home and we're done
when we took a little little
if you can see that a little closer look
at it and the server was only 40%
utilized it had 60% idle on the CPU
uh-huh
we're pushing is not going any faster I
mean there's no way we can push it any
faster what's going on as you look a
little closer one of the CPUs was
totally maxed out 99.9% maxed on that
single CPU which made me a disco well if
something's going on there's stuff
definitely a problem with that CPU why
is it and as we look closer and we look
closer and we look closer
the affinity level on those on that
server was set up for the receives to go
through one one cue or one CPU and the
sins to go through a different CPU and
so those receives that we were getting
that we were pushing the test driver
that was pushing those those messages to
my messaging tier couldn't push to the
the MQ tier couldn't receive the
messages any faster it was done it was
maxed out he had one CPU he was maxing
out there was no way he was going to get
any more to the two mq that was sitting
behind the scenes he was min he wasn't
gonna give it to me I'm ready I'd like
some more messages but he couldn't do it
so we were able to change that and
spread that across spread those that
work across multiple CPUs and then we
jumped up again we've got ten times as
much than what every anybody thought we
might be able to do so we were like okay
we got through a CPU issue and things
look better but we're still only it's if
you look at the right side of this graph
we're only at 60% CPU what the heck
what's going on you know and we realized
now we're at a code issue we're at a
library issue with MQ MQ on that device
could only handle you know this rate you
know 80 some thousand messages per
second with 1q and so we started playing
with how many Q's how many Q managers
what if I put ten Q's or a hundred Q's
or two Q's didn't we've realized they
didn't really matter as long as it was
more than one Q two Q's on the same Q
manager to Q managers or a whole bunch
of Q managers didn't really matter then
we could finally push it up and get it
at least in the closer to the 90% raise
we could probably squeezed a little bit
more out of that but that was really to
the point like okay we feel pretty good
about where
that's a good number everything's
running pretty good dere so this is all
about we found a lot of different little
pieces hardware and then software issues
and we just had to drive through those
and get past those bottlenecks see so
I'll talk a little bit about garbage
collection
Monica Beckwith yesterday had a great
presentation on garbage collection so
I'll let her talk a little bit more
about that but one thing we did see
Java's low-latency are not friends right
there they don't go really well together
they don't there's a lot of pauses
there's a logic are a lot of garbage
collection the young garbage collection
stops the world's old you know full
garbage collection stops the world it's
all over the place I didn't want you
know even at 20 or 100 millisecond pause
was painful for us we wanted flat you
know that was our goal we wanted a flat
response time you know what no matter
what it was we wanted it flat and Java
hotspot really had an issue with you
know I'd get a five-second I get a you
know three millisecond and then I get a
hundred millisecond I was all over the
place and I we really couldn't handle
that and it wasn't the speed of Java
Java was fast it was when it had to
pause to do its garbage collection so we
had when I came in and I just started
looking at this we had nearly 10,000
major garbage collections in our
environment a day 10,000 fulls a day so
that was a big deal over six hours was
spent in full garbage collection of all
my processes I mean there's a lot of
processes maybe that wasn't a lot but it
seems like a lot of time six hours where
it's going everybody just wait everybody
hold on take a break I got a I got to
take out the garbage everybody has chill
out and so does a lot of time that's not
even cluding the miners that's just the
majors those are all you know things
they cause problems our environment they
cause transactions to timeout and for
MasterCard every transaction matters
every transaction we want to look at we
want to know what what it's like we want
to be able to see if it's fraudulent and
then move on and so when we drop any of
those on the floor that's a problem and
so obvious our goals to remove them our
initial goals we want to get rid of the
majors so we were using a lot of old
parallel GC and we said well we got to
get rid of that let's you see a mess
that'll be a lot better and we did and
it was a lot better I mean
it took away a lot of our major fools
and we want to limit - the limit the
miners as short as we can so make those
run really really really fast without
promoting everything into old which then
would causing a whole nother problem all
to improve our SLA we worked on this and
we tuned and we worked with our vendors
of the libraries we were using and they
said man you have got this tuned to the
best that you could tune we had enough
memory we had enough CPU we had
everything and it still didn't give us
what we wanted we could still couldn't
get you know that low you know tens of
milliseconds flat response time that we
wanted so we kind of gave up on Oracle
and hotspot and IBM's everybody else's
and we ended up buying zing Xul's and
came from her name earlier the keynote
this morning she talked about zing
versus hotspot and so if you look on the
right of this our test for hot spots
this is a max pause or max response time
during a test which are all related
exactly to pauses and you see some of
these I had to cut the graph off at 6
seconds because hotspot would have these
huge swaths you know that would go up
and down and all over the place and you
know so I'm this is you know a second
and so most of them most of my maxes for
a particular minute were you know 200
milliseconds that was a lot for us that
was that was way over what we wanted to
do and so we ended up ended up
implementing the product called zing and
as you see it's nice and flat you know
there's some humps and bumps but for the
most part it is nice and flat across the
architecture so we liked that a lot
better so that was a great tool for us
to use ok third principle strategic it's
your teacher so the big thing that
people stopped with when they do
performance testing I think is after
they get everything tuned and everything
working the way they want it they say
I'm done we're done
you know the management says and the
business says you did it you're right
you're done your projects done
performance project is complete we
should be able to move on well I think
we need to go a step further than that
and that's what we started to do because
we got a lot of questions from our
business about well I've got a new it's
a new business we want it you know I've
got some some new
work that we want to do it's you know
maybe it may bring on another 100 TPS or
a thousand TPS or you know they they
think big you know it's a million TPS
we're gonna be so big you don't ever
gonna do everything and and it's a and
it's they said well will the will the
platform handle that will the platform
handle that and we uh you know a lot of
it before we started modeling it I don't
know I think I think it can do another
5,000 or what if this server came an ER
what's happened over the last six months
I don't know I have new rules been
implemented they were more expensive
than the old rules you know we're using
more memory than we were using I don't
know we got to go look at everything
again so we stopped and we said how do
we how do we model this and what we did
was we took what was happening currently
in production and we said okay what's
the volumes
what's the CPU look like what's the i/o
look like where our barriers and we
created kind of an application in Excel
it was a little crude at the time but
we're we said okay each this transaction
on this tear takes this amount of CPU or
this amount of memory or this amount of
i/o and we were able to kind of
calculate that up into kind of a health
you know the current health of what we
thought production looked like and so
that was kind of our first swath that
what a model would look like we were so
we came to stop there we got to think
about this a little further so he said
let's start as we're doing our
performance testing in our Pete
environment and we do our in our tier
testing we can look at that a little
closer and say how is this looking
compared to what production is looking
like I'm running this test at a thousand
TPS does this look like production at a
thousand DPS if it doesn't maybe I
needed to adjust that to make it look
like that and then grow it and see how
hard I can push so a lot of these tiers
we thought from a production model look
great you know this one's running at
only 20% CPU and it's only you know it's
doing thousand TPS or whatever you know
it's we've got plenty of room to grow
but then we found you know where we can
only get 1,500 TPS like the debugging
example I had a minute ago you know we
look great up to a thousand it didn't
have any issues but you got to a certain
point you just fell over even though
there was plenty of CPU available so
that's when we took that performance
data and we intermixed it and we
compared it with what we're seeing in
production and we're able to come up
with a more refined version of what
that's supposed to look like and then
every month or so
we go look at productions look at
production volumes we look at those
grass that you know that are running all
the time all you know continually and we
look at the CPU and we say our model
says CPU should be 32% based on that
production volume
what's my CPU it's at 62% that
something's wrong my some models off
somebody broke something in the code a
configuration wrong you know iOS broken
some things you know awry let's go look
at it and figure it out so this is
something that you know if you're trying
to do this to make sure as you're doing
this testing you know make sure that
this is never done you know make sure
your management your business knows this
is something we got to keep continuing
down the road for and then one of the
best ways which we learned we've looked
at a hundred different ways to try to
show this to our to the executives and
how do how does this look and when do I
need a new server and we had all these
grass and bubble charts and all this
junk and and none of them none of them
looked right you couldn't really
understand and we had a lot of tears in
our environment and what its gonna look
like and this was the best one so I mean
this is something you know if you're
trying to do the same thing this might
work so this is basically our tier and
our dates going across and just a heat
map a simple heat map of what that's
what that looks like in the future and
this goes on you know you know a couple
years however long I want it to go I can
just push it down a couple years and see
what that's gonna look like and
obviously your eye pulls to the red
right that's what I'm trying to pull
your eye to where's the red coming so if
it was February of 15 and I could say
listen we were modeling this out we're
looking at something our rules tier
looks really bad in May and June we need
to do something about that and
MasterCard we can we don't have servers
just laying around we can go pick up and
and plug in somewhere because we're you
know we have to order things and get
them in and so it's like I need a few
months to get all that done and get it
in and have all the different people
plug all the wires and they need to
happen to implement something and then
and then you see like in July that's
when I implemented that server you know
or whatever it was that we implemented I
could easily in my model say I'm going
to add it some more capacity on this
date and then show what that looks like
and so it was easy when the business
came
today I need to do another thousand DPS
or 15 or TPS can you do that and I can
just plug those numbers in and say okay
well with 1,500 more TPS due to ever all
pieces of my environment and it would
show me some very interesting things at
times but this was the best way we were
able to visualize that with with the
team and then obviously the thing that
you have to keep moving with this is you
know create some cool dashboards you
know you gotta create some cool
dashboards they love them management
loves them and we can't get too far
without funding and we need funding
right we need money to do all this work
and if we don't have it you know we
can't do it and if the management's not
excited about it it's harder to get the
money but if they've got cool dashboard
to look at then they love it they eat
that stuff up you know they're like oh
that's cool look at the bar graphs in a
pie chart
what's that pie chart tell me I don't
know but it looks good you know no we
explain it to it we actually talked we
talked stuff like this through with them
we say listen this is what this is
telling you we want you to go look at
this we're giving you access to real
this data real time or every month or
whatever it is go look at it you know
this is what this is telling you this is
telling you this is bad or good or
indifferent whatever it is this is what
you should be looking at when that turns
red that's a problem you know and it was
a little more technical but it got them
more involved it got them understanding
what what we're doing and why we're
doing it you know and I think that
really helps you know along with the
heat map but this is just enough to kind
of get them involved in moving so my
final slide here is those steps for
success I meant I mentioned earlier so
these are the three things that I think
you have to do that we miss so many
times at the that that to get that
funding from our management to get them
is to explain to them what the problem
is and how to fix it so the first thing
obviously show the problem don't explain
them the problem don't sit in the
meeting and go listen here's what's
going on the CPU is too high and this
thing is not working that way and the
the flux capacitors you know broke in
and whatever it is because there it's
gonna go like this over them they don't
get that right they don't get that but
if you put it in a simple graph of some
sort I had a we had it where we were
getting full GCS and every full GC I
could pinpoint that full GC exactly to
the time when I had
timed out transaction all day long oh I
mean it was her it was beautiful
beautiful graph that was put together
and I was able to show that to him I go
see this is what's going on here it is
you know you don't want transactions
timed out here's the problem and I'm
showing it to you in a nice graph they
saw that and they understood it it made
sense to them but you can't just stop
there you have a solution before you
give them the problem give them the
solution all we have to do is implement
CMS or implement zing or implement
whatever to fix this issue okay so give
them what's what the what the solution
is how it's going to work what it's
going to do and then big point big point
follow-up results follow up with the
results after you've implemented the
changes it might be a week or a month or
six months however long it is in your
environment or whatever kind of change
it is follow up with the results rebuild
that graph or we show what you had this
is what we had in January member
remember all these timeouts we had we
had 10,000 timeouts a minute now we've
got three because of this one change I
put in Wow really that's another cool
dashboard I love that here's some more
money and they just start opening their
wallets and they start throwing it at
you when that happens
but yeah the I think I really do think
these are the big things these are
simple but I think and they take some
discipline but when you do these and you
follow up with that and you really
explain what's going on why it's going
on and the results of what you've done
then it goes long long long way that is
it for me for everything I've got n
least in slides I know how much time
we've got but for questions or comments
or throwing anything yes
yes so he was asked is it
forward-thinking is based on the load so
yeah so we took we would take just
organic growth that we see you know in
our environment as new cards are added
and then we also took input from our
business hey we've got this job that we
think is gonna sell in in February and
it's X amount more volume you know we've
got safety net the safety net volume
coming on it's X amount volume so we're
just able to push that down on down the
line yes
okay so the question is for the
dashboards that we built do we have an
executive level dashboard and middle
management last board and an engineering
dashboard that kind of thing yeah so yes
that's where we're heading where this is
all kind of brand-new getting all this
in and getting all this working but yes
right now we've got executive management
dashboards like this we've got low level
you know what's happening on this CPU at
this point you know type of metrics and
dashboards all across another tool that
I'll not paid by them or anything Splunk
has turned out to be a great tool and if
anybody's use that yet but that is a
really cool tool to pull all your logs
and all your metrics from lots of
different places and aggregate it and
visualize it it is a pretty cool tool
they have a free version that you can
use and start playing with but it's
pretty neat
any other questions yes
we separate the the performance team
who's who does the actual testing and
evaluation from the development and the
only read and we intermix um when we
talk right we you know it's not like
we're there's big walls between us
question was do we do we break the teams
up between the performance team and the
development or the engineering team and
we don't we do because if if you you
know if they wanted me to be a tech lead
on a on application project and that got
busy then I wouldn't spend any time
doing performance engineering they would
say that that's not yeah don't worry
about that not a big deal you know we
need this done because it's due next
month or next week or tomorrow or Monday
you know and don't worry about this for
the next couple days or next couple
months and then all of a sudden we're
not spending any time on it so we're
trying to we're trying to really specify
this team is doing performance
engineering they're looking at this
they're trying to break it every day
every month and figure what the problem
is because if not we're just gonna get
pulled in a development and then we're
just gonna quit doing it anything else
all right thanks a lot for coming
appreciate it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>