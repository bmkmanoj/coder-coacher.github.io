<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;The art of service discovery at scale&quot; by Nitesh Kant | Coder Coacher - Coaching Coders</title><meta content="&quot;The art of service discovery at scale&quot; by Nitesh Kant - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;The art of service discovery at scale&quot; by Nitesh Kant</b></h2><h5 class="post__date">2015-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/27ynM2tbNXM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right let's start distributed
systems are complex and if you have a
tunnel vision you may end up into a
wreck
hi I am Nitesh Conte and today I'm going
to dive deep into the details of service
discovery so that you have a better
understanding of your ecosystem and do
not develop tunnel visions I'm an
engineer in the edge platform or edge
gateway team inside Netflix and in the
perspective of this talk I ran service
discovery for Netflix for a year which
eventually led up into the redesign of
off the service discovery system called
Eureka and the version 2 of that system
which is under development right now all
right so what is service discovery
I'm blissfully assuming that each and
every one of you are a subscriber of
Netflix and you are pretty much aware of
this homepage well if you're not don't
tell me and if you are thanks for my
salary so whenever a user logs into
Netflix and hits this home page for
example hundreds of requests land into
the Netflix control plane and I wouldn't
have a job if it was actually serviced
by a single highly available infinitely
scalable machine thankfully it's a huge
data center thousands of machines and
hundreds of ecosystem and roots of micro
services that interact with each other
in interesting ways so if any all of
these services interact with each other
and the machines look somewhat like this
in a data center how do they know which
one to talk to well the problem has an
easy solution you send an administrator
who will go ahead and label your
machines as to which instance belongs to
its service and that becomes a service
discovery I wish I wish it was so easy
but it's not in reality there is every
service goes through a scale up and
scale down cycle based on the
that it receives which we call
auto-scaling so number of instances are
not constant
there are VMs crashes so one machine
which is available may not be available
in after a minute and in really complex
scenarios one instance a one machine
which was running one kind of service
after sometime it may be running
completely different kind of service so
the point being that it's an
ever-changing ecosystem static mappings
of machines to which service do they run
on do not work and service discovery is
a problem of cloud environments we do
not need n min straight errs to label
our machines what we need is a dynamic
mapping of service names and which
instances do they run on and if you have
this dynamic mapping and you put it on a
on a service on a set of machines and
make it available to every other machine
in your data center this is what you get
which is your service discovery and
since it's an ever-changing data anyone
can get put or delete data from it which
which makes sense to them so that's
pretty good in concept but how a service
discovery used so going back to the
example of how to deliver Netflix
typically those requests come in to a
service umbrella which we call add
services and typically each of these
request fans out into multiple different
service requests now let's assume that
one of this service request was actually
to get recommendations for a user and
since it's it's a redundant
recommendation service there are a lot
of instances of it and you need to know
which instance do you want to go to and
to in order to do that you basically
query service discovery and ask a simple
question which instances of
recommendation service are available now
what it turns out that it's a pretty
intriguing question it has two parts I
will take it up in two parts and the
first simple question is which of the
machines in the data center have the
recommendation service software deployed
on it
and since that was the premise of a
problem of having a service discovery
system we can look up into our dynamic
mapping registry as to what instances
are available for accommodation service
but what we didn't talk about is how do
we even get that data so any instance
that starts in our ecosystem in our data
center has two distinct States when it
starts up its sense and tells
recommended in a tale service discovery
system ok Here I am I'm a recommendation
service instance I am running on this IP
and port and eventually when it shuts
down it'll send another event saying
that I am NOT anymore recommendation
service wiped me out of your registry
and if if you consider these two events
arriving from any instance into in your
data center you basically answer that
question as to which instances are of
recommendation service our will are in
your data or available at registered as
a combination service that was a simpler
question the other part of the question
is which of these instances are actually
available now if you look at this
particular state transitions that you
basically have two events from an
instance there is actually no way of
telling whether the instance is
available now or not because how would
you distinguish between two instances
which of one of which have abruptly shut
down it has missed sending a shutdown
sync signal and the other who's alive
and is not yet seta sent a shutdown
signal and typically how people solve it
is is by having a constant set of
heartbeats which keep sending the
instances keep sending heartbeats to the
service discovery system at regular
intervals and since you know that you
get a heartbeat every 30 seconds for
example you know that the system is
alive but going back to that question do
we really think by sending these
heartbeats we are answering the question
that this instance is available now or
not let's see network partitions they
happen in different ways some people
claim that they don't happen in the data
center but it is a fact it always
happens and what can happen is that
let's example
let's take an example of a reading
service instance which has been called
by two instances of edsells and it may
so happen that the rating service
instance is completely available from
instance one of its service but it's but
the EDD service instance too cannot
really talk to it so if you look at it
here being available in a distributed
system is very subjective what is
available to one can be completely
unavailable to another so if you were to
make an a judgement on whether a node is
available or not you you have to make
local decisions if I am talking to a
particular instance it's my it's my view
of that particular node whether it is
available or not it's totally my
decision and if that is the case there
is no point in telling that service
discovery is guaranteeing availability
of any node to register to it because it
doesn't make sense it's always a local
decision so if that is the case why
should this discovery store node status
every service discovery would tell
whether the instance is up or down well
an interesting aspect of this is that
availability is doubtful but
unavailability can be trusted what do I
mean by that so let's say let's take an
example if you trust availability of a
node services query tells you that
instance X is available and the worst
thing that can happen when you actually
call it is that the instance is down and
what results is is Layton sees failures
on your calling service but what happens
if you trust unavailability if you are
saying that a node is not available
there is nothing really that you use
that you lose because you would not call
that instance and there in all cases
they are like plenty of other instances
that you can talk to one one case that
can happen that the services what if the
services cover e is wrongly telling you
that none of the instances are available
but that is a case that we would see in
the later in the presentation how to
make a service discovery that would not
tell you that and in an interesting out
of saying that unavailability can be
trusted is what you can do is - you can
override statuses of your nodes what do
I mean let's say that relating service
has two instances one and two and all
the other instances in your ecosystem
are talking to them and for some reason
you want to take out instance two out of
service you don't want anyone to talk to
that instance you go to your deployment
tool and say Delta service discovery I
want to take instance two out of service
and boom since everybody everybody is is
trusting your your judgment that the
status is unavailable you can take them
take that instance out but why would you
do that it turns out that is pretty
effective while debugging instances
because you can isolate an instance you
can replay a requester on that instance
in production environment which is
pretty useful you can also start nodes
in isolation which is also useful
because typically when nodes come up
they do not they are not ready enough to
serve traffic so you can take this
particular feature of service discovery
that if if your if everyone is trusting
unavailability they would not call that
node so slightly qualifying my earlier
statement of saying that service
discovery does not guarantee
availability of nodes and it only
controls visibility so you can control
by a service discovery which of the
nodes in your ecosystem you want other
nodes to be calling we are distributed
systems engineers and we love failures
we love talking about them because they
are more interesting so let's see let's
see how failures affect service
discovery so what if service discovery
is unavailable
none of the none of the nodes of
services gravity is unavailable is
available so recapping the fact that we
established that service discovery
controls visibility of nodes but does
not guarantee availability so an
innocent question all you lea loose is
the visibility of new nodes because you
already know about old nodes because you
were talking at some point to services
curry and then it went down how bad that
can be so I briefly touched upon a
scenario where in instance which was
once serving one kind of service let's
say user service and is now serving
metadata service so suddenly from under
your feet the instance has completely
changed you are thinking that you're
talking to user service now it is
actually metadata service and typically
if you are having if if you're having
services which are rest in nature you
would typically get a 404 initially
before 4 and there is no way to
determine that it is because the data is
changed underneath or the service kind
is changing it's very difficult to debug
such scenarios and another very
interesting use case that happens is
what happens if you are scaling up your
system the the load of your requests
have gone up and in order to match those
in order to serve those requests you
need more instances and so you are
scaling up and what happens if your
service discovery is unavailable at that
time new instances would not come up and
hence you would get into this situation
your website is down because you cannot
handle more requests because no more
instances are available so that innocent
question wasn't really innocent it has
pretty pretty bad outcomes so high
availability is a strong requirement for
service discovery and since we are
talking distributed systems we talked
about availability
how can we forget this theorem called
cap theorem so cap theorem basically
says that in distributed systems if you
have consistency
availability and network partitions you
can only choose two of them and since
network partitions are always there what
you can choose between is consistency
and availability a few slides back we
established a fact that service
discovery controls the visibility of
nodes but does not guarantee
a node availability so let's try to
analyze if we choose one of this
consistency or availability what would
be the impact of on on any service
discovery system so let's use
consistency first if you choose
consistency first the first time that it
can happen is when you are auto scaling
up and I I summarized as to what really
can go bad when this happens on the
other hand if you choose availability
the worst case that can happen is this
contrived case because there are other
things that can happen because you're
choosing availability which will lend
into you talking to instances which are
not around and that will cost latency
but that is not an artifact of you chose
choosing availability because that
happens in natural systems in all
distributed systems when that network
partitions occur so the worst case is
really this and what it turns out if
that is your worst case were choosing
availability and if you are talking
between nodes with the persistent
verified connection you do not get into
that situation what I mean by that is
let's say that there again the edge
service and recommendation service two
instances are talking and if you were to
model your interactions on a persistent
long-living connection and all that you
do as on connecting you basically verify
X service instance you basically say are
you really like service ok fine so now
my connection is trusted and established
and then I would send multiple requests
on it this is typically the case that
you do but with HTTP 1.1 you basically
are doing stateful interactions on a
persistent connection which is not what
this means typically if you are having
like a TCP or a WebSocket connection
you're basically having States on which
you are exchanging requests so I would
claim that in most cases choosing
availability over consistency for
service discovery is the correct thing
to do
and when we are talking about failures
and we are talking about services that
store data the other interesting aspect
is durability what is the guarantee that
I write a data to service discovery is
available let's say after after a
certain amount of time is it durable if
the service notes crashes so in order to
answer that question as to how service
discovery should behave in terms of
durability the first fact that I would
like to present is service discovery
data is ephemeral and reach incredible
what it means is that in a dynamic
environment usually the the lifetime of
instances is in the order of hours or
days so you're not storing data which is
a month long most probably the data that
you're looking at now is completely
useless after a day and the data that
you're storing is is really an instance
information of different instances and
an instance always know what it is so it
is always available to the instance so
it is regenerated so let's say I was
talking to an audience service discovery
and the node goes down what I do is that
I start talking to another service
discovery node and since my data is
already available to me I would be
replay that data to this node which is
an easy solution of saying that I do not
really need to store this data
persistently because it is ephemeral so
at this point I have gone through a lot
of facts and features about service
discovery there are many things that
came out of it that you are basically
having regenerate able data ephemeral
data node availability is out of scope
so at this point I would pose a question
and say can we create a coordination
free service discovery system what does
it mean is that primarily we are
choosing availability over consistency
and all the nodes in service discovery
can run in isolation they don't need to
talk about and talk to each other
because they don't need to do any
consensus
because we don't need consistency so
let's let's take a stab on trying to
design a coordination free service
discovery system the first step to do
that is to figure out as to how client
and servers interact and in previously
in the presentation we establish a fact
that a typical interaction between
client and server looks like somewhat
like this at startup you register
yourself then keep sending heartbeats
and when you are shutting down send
another signal that I am I'm shutting
down I am the more so this is a stateful
interaction because the state is bound
between a start and a shutdown and there
are multiple events in between so it
makes sense to model this interaction as
a stateful protocol what do I mean by a
stateful protocol is basically any
protocol which provides you connection
guarantees so basically you stablish a
connection and whenever you send
anything on it is always ordered TCP is
a great example of it and anything built
on top of TCP is ordered
so what real benefit it gives you is
that whatever change in whichever order
happened on the client is propagated in
the same order to the server on a
connection and it is reliable so you
don't have to worry about lossy messages
a benefit of that as I as I said that
there is causal ordering you can always
assume or you can always say that
whenever what data you are receiving on
that connection
is coming to you in causal order and the
natural life cycle of the connection can
be bound to the life cycle of the
instance because if I started an
instance and registered and on shutdown
I basically tear down the instance you
are binding your life cycle to the
connection and an interesting aspect of
using this particular kind of protocol
is that you can send data tips because
you are guaranteeing causal ordering and
the guy and the server who is receiving
it can apply the diffs in the same order
and get to the same result
very very deterministic result so if you
were to do this
if you were to design your interaction
as a connection-oriented stateful model
you are basically saying that the
interaction would look like this you
establish a connection you register you
send heartbeats and then send shut down
an interesting aspect here is that in
order to guarantee liveness of the
connection you have to exchange
heartbeats in both directions so that
you don't you're not working on a
connection which is only half open and
it happens a lot of times so if we
establish this as a client-server
interaction model let's look at the
bigger picture since we want to
redundancy in your service discovery you
not much you need multiple nodes and you
want to be serving data from any of
these nodes for any of the clients and
in in in realistic scenarios there will
be multiple instances talking to
different service discovery nodes and in
order to for you to be able to serve
data from any of the snowed you need to
replicate data from Allen let's just do
another and so you basically get into a
ring of nodes wherein all data on one
node is available on another because you
are replicating all the data so keeping
this picture in mind let's look at this
scenario because we are talking about
long live connections and it is
extremely immature to even think about
that these connections would always last
forever so they will happen that the
connections will break and and since
it's a stateful connection it's kind of
simple that if the connection breaks you
go ahead and establish a new connection
but really what is interesting over here
is that you can you can land into a
different service discovery node from
the one that we were talking before and
what arises out of it is the interesting
part of of having a replication scheme
is data conflicts so having a more
interesting case of let's say a server
started with a status of starting
publishing itself on a particular IP n
port and then after some time when it is
actually ready for for infor request for
a different request to come to earth
it is saying that I am now
and interestingly so what happened is
that after sending the status of
starting the connection broken down and
then it connected to another node which
with the status of app and if you look
at the bigger picture wherein all the
nodes and services cavity are having all
the data after instance it may so happen
that there X there exists a node which
is getting data from both of these nodes
so one service discovery node is telling
you that the status is starting and the
other is telling you that it is up now
how do you decide as to what is the real
status of the instance and that gets to
a point of how do you resolve conflict
in order to answer that question let's
first see as to how node 1 which
received this state is starting behave
when the connection is broken so a few
things to consider is that firstly we
should show tolerance to broken
connections because they happen all the
time and if you tend to evict that
instance as soon as the connection is
broken it'll cost chance for you because
people will be talking to that instance
and you and and you remember that every
instance trusts that unavailability of a
node that this is that the status is
down is basically meaning that I should
not connect to it so typically it is a
good practice to wait for a reconnect
optimistically you you you think that
the state that the instance is actually
up and the connection is just broken and
if it doesn't reconnect after a while
you basically Evek that instance from
your memory the the other point to
consider here is that at steady state a
client is maintaining a stateful
connection to a single discovery service
node because it makes no sense to talk
to three nodes and send the same data as
you know that it is replicated and you
can query any node to get it so what
that really means is that after a point
you would see that service discovery
node 3 would stop getting the data which
still because the service discovery node
one would evict it from its cache or
from its memory so what the point that I
wanted to make over here is that data
conflicts are resolved naturally for
service discovery so could it be that
your solution to resolve these conflicts
is to tolerate them temporarily what it
means is that you basically have
different versions of this data
pertaining to different nodes and each
of these nodes change only modify that
data so you do not really step on each
other's data and you are having three
copies in this particular situation now
when you have this three copies the the
obvious question comes in that if
someone asks me as to what that what the
data is as to how would I read that data
so let's let's say that our versions are
put in a queue and we read only from one
end and we keep reading that version
till it's gone and you know that it will
be updating that version and since we
establish that at steady state
eventually these this stale copies would
be evicted so it will and if you happen
to see will happen to read from a
version of the data which is actually
stale and it is evicted you move to the
next version in the cop in in your queue
and at steady-state you basically evict
all the different instances that were
stale and you end up with the data which
is correct which essentially means that
the instance that is registering itself
is talking to one node of service is
coming so some of you may be thinking
that wow that looks like a viable
strategy but practically what is the
time to converge okay so let's take the
worst case the worst case could be that
you are reading from an older oldest
copy of your data and the latest copy is
right at the end of your queue so you
have to evict all these copies in your
queue to get to the latest guy
and if you were to put a formula to that
time of convergence you'd say that
the time to evict stale copies on a
particular node plus the time to
replicate from that node should be the
time to converge in your to the
steady-state data to the actual data the
first part of it is actually constant
because you say that this is your
constant interval in which you will
heartbeat to your to your server node
and there would be an upper bound on the
number of heartbeats that you can miss
so let's say that we heartbeat on every
30 seconds and there are three
heartbeats that we can miss that means
that 90 seconds is the time to evict a
stale copy now if the other part of that
formula was time to replicate from the
owner node and this is theoretically
unbounded so now let's see whether we
can put bounds on this particular time
so taking taking from the same concept
of how we had the persistent connection
a stateful connection between a client
and server let's model replication - in
that model so you have a persistent
connection in which you share data and
in order to again guarantee lightness
you basically are saying that you're
exchanging heartbeats in both directions
so if you do that based on the same
thing that you were doing between client
and server you can somewhat bound this
replication time which again becomes
heartbeat interval for that replication
channel into the number of tolerated
heartbeats and this becomes somewhat
bounded somewhat because they would
still be cases in which you would not be
able to replicate in time and there
would be variable so even even though we
put this bounds and we determine that
this is a time to converge it may still
be large so let's study as to what is
the cost of that divergence if it
happens so the first fact there is that
instance data is hardly changing so you
don't change your eye piece you don't
change your ports what changes is status
and status also since the status is
inferred locally from the caller nodes
it
it doesn't really give you so much
benefit to change data that often so in
practical in in all in all environments
the instance data doesn't really change
much and even if that changed the impact
is low because what we said was service
discovery controls visibility of node
and does not guarantee availability and
local decisions are always taken by the
nodes who are calling them so so to me
it seems that it is a viable solution
viable design to have your service
discovery and if you have such a system
and if you design your interactions
based on that you would you would see
that interaction between a client and
server which is giving you data is
basically an ordered stream because you
open a connection you start sending a
data for registration and send multiple
different events on top of that
connection and since it is ordered it is
an ordered stream and if you take all
the connections or all the instances
that are available in your system and
all are saying that they are ordered
stream you all all that you have to do
to take service discovery data is to
merge these streams because you are
implicitly ordered you are guaranteed
that everyone who applies this
particular changes would result in the
same result so if you were to more if if
your data service discovery data is
merged ordered stream how would you do
lookups how would you know that if I
were to call a recommendation service
instance how many of them are available
and since it's a stream you are
basically saying that out of this huge
stream of instances that you have filter
everything that it doesn't belong to
recommendation service and what you get
is a set of recommendation service
instances that you are interested in and
since it's an ordered stream you can
send data tips you can optimize on the
data that you are transmitting between
the server and the client and since you
can only send tips and extend in this
particular data stream concept to
replication it's it's really a special
case of reeds because what you are
saying is that instead of giving me a
particular
of service instances give me all the non
duplicated instances that you have and
if you do that with with the instances
of services Calvino's of your choice you
basically get all the data that you need
for services coming so really it's it's
a question of embracing streams and
dealing with that alright so that's a
time check I have around 10 minutes so
in the in the later part of these slides
I would I would just elaborate a few
cases that we came around by my
practical experience deploying this
introduction the first very interesting
case is really weird Network partitions
happening in your system so let's
consider that you have three nodes of
service discovery and each of them is
replicating data and they're a bunch of
instances that are talking to node one
and the partition is such that all
service discovery nodes can talk to each
other but none of the nodes from outside
can talk to node one now what would
happen is that since node one doesn't
get any heartbeats it basically starts
evicting instances from its memory and
this like a Poisson because you are
replicating to all instances spreads
over all the instances in your ecosystem
and what you realize if you are unlucky
enough that one fine day you lost most
of your service instances because one
node of service discovery is in
partition that isn't really a great
thing a great place to be so there is a
concept called self-preservation that we
follow inside Netflix what it is based
upon is that all instances generally do
not vanish and if they do let local
decisions prevail because anyways you
are making local decisions to find
whether instance is available or not
so can you be optimistic and say that
whenever such a thing happens that
suddenly within a period of time you are
not getting any heartbeats so be
optimistic and think that it everything
has not really gone down your data
center has not
crashed but what it is that you are in a
bad state so what you can do is put
bounds on the number of instances that
you can tolerate missing heartbeats and
if you get more than that
missing heartbeats you basically say
I'll stop affecting you preserve your
data instead of evicting it from your
memory you preserve it so that you you
even though you you're you are giving
out stale data you are actually giving
out data and the clients that are
talking to the in doubt instance will
detect failure as they regularly do so
this this was an interesting outcome off
of like an outage that happened so I
thought it would be interesting to share
the other aspect of after going to
production what you realize is well you
thought that it is highly available but
there would be cases when the entire
cluster is down you can't help it so all
you can do is to analyze what would be
the impact of such a failure and if you
were to do it the the most fatal impact
that would happen if in the entire
service discovery is unavailable the new
nodes cannot get started because they
can't get data of the dependencies that
they want to talk to and a degraded form
would be that the existing nodes which
are already running add had some data
would hang on to that data to that cache
data even though it is still they can
tolerate it because that's how they do
naturally in in in in irregular
scenarios too so the critical part of it
is that the clients should be created
with an ability to cache data when there
is total unavailability of services
covering so many of you may be thinking
that I've talked about a lot of stuff
which is which sounds theoretical is
this all paper where well it is not if
you look at Netflix Eureka v2 all these
things that I talked about the version 2
of Eureka is based on this there the
basic concept of favoring availability
or inconsistency has been the case since
version 1 but all this stream handling
and and modeling it on streams is what
we have done in version 2
so if you were to have a takeaway from
this presentation I would say the first
be aware what happens in your ecosystem
don't let your service discovery become
your Achilles heel and the interesting
and the most important part to get from
it here is that service discovery
controls visibility of nodes but does
not guarantee availability so that
that's all I have and obviously I have
touched upon like the tip of the iceberg
as to how to design how to start
approaching designing services cover as
a as a highly available system but feel
free after the talk to chat with me
about the questions that you have and I
I possibly have questions have time to
take a couple of questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>