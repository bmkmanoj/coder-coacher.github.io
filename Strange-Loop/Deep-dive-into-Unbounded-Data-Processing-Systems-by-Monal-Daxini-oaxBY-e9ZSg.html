<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Deep dive into Unbounded Data Processing Systems&quot; by Monal Daxini | Coder Coacher - Coaching Coders</title><meta content="&quot;Deep dive into Unbounded Data Processing Systems&quot; by Monal Daxini - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Deep dive into Unbounded Data Processing Systems&quot; by Monal Daxini</b></h2><h5 class="post__date">2016-09-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oaxBY-e9ZSg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">how many of you are going
subscribers who nice how many of you
actually use some kind of stream
processing or even processing system in
production nice okay a lot of you guys
so thing let's get started so just like
any any specific domain of problems that
you're trying to solve it acquires a
certain terminology there are certain
patterns of all over time and the field
of dealing with unbounded data
processing is no different so I'll go
over some of the terminology some of the
concepts how to deal with unbounded data
processing you look into existing stream
engines I'll cover three a lot more out
there and then we look at what we are
doing at Netflix to handle these
challenges at scale so we'll start with
getting to know our terminology the
first thing that we observe is how we
describe the data right today we talk
about streaming to mean a couple
different things we mean it's an
infinite stream of events are flowing
into our system and we also use it to
refer to like an execution engine so for
example you may say i'm using streaming
data or i am doing streaming analytics
but at the same time you might be
referring to the kind of processing it's
doing underneath the hood maybe storm
maybe flank or you may be referring to
you know batching to me in spark and the
same thing with badge you may be using
it to me in a couple different things
you may be using it to mean a bounded
set of data or events that are flowing
into your system or you may use it to
mean the time type of execution engine
that's handling that data so you know
let's let's be precise about our
terminology because it's going to make a
difference of how we address certain
things when we are talking about it
let's use unbounded to mean an infinite
set of the
data or events that are flowing through
and bonded to just mean the finite set
and we can use streaming and batching to
mean whether at the core the engine is
streaming or a matching engine so we
could say streaming a streaming
execution engine can handle either
bounded data or it can handle and
bounded data set or and the likewise for
batching right so for example with
Hadoop you can actually not only do
real-time data by just segmenting it and
it's been done for quite a long period
of time it's a segment into an early
boundary and then you kind of deal with
one hour chunk of data at a time and you
could do the same thing that's streaming
you could take a finite set of data like
a file and make it look like a stream
and process it that way so whether it's
streaming or badge both those engines
can handle bound or unbound data once we
have the clear terminology so you may
ask how do I choose a certain processing
engine so the three factors you need to
consider when you want to choose one you
have to choose between the trade-offs of
latency cost and what kind of a cure is
here looking out of your system and you
should choose an engine that lets you
make these trades off but does not make
it for you so you need to have the
option open for a use case you can
choose between these three and make a
wise decision the next important aspect
is processing semantics of your events
are flowing through do you care that you
want to process every single event
that's flowing through a system if
you're doing metrics and you just want a
ballpark understanding of what's
happening maybe a point one percent of
one percent of drop really doesn't
matter but if you're doing like billing
and you're doing ad clicks and you want
to charge back to your customers the
accuracy really comes in a picture and
you don't mind paying the extra cost to
get that accuracy and
we have the notion of exactly one
semantics quite often this is
misunderstood as exactly once delivery
which is very different and very
impossible to achieve practical if you
think about it so what we mean by that
is let's say you let's take a simple
example let's say you want to add three
numbers together you had the first two
numbers and your system crashes and when
you recover back again you don't want to
add the first two numbers again so for
example if you're adding two plus two
plus two when you come back up from a
failure you want the answer to be six
not eight right so if you do at least
one semantics you could get eight if you
do at most once you could get for but if
you do exactly want every single time
you should get six provided your source
is repeatable and you're able to replay
from it so when it comes to accurately
processing your batch data it's
relatively easy because you know the
data set the data sets not changing and
all you are doing is wasting a little
bit of time because you're reprocessing
it let's say it took four hours to
process it you have to reprocess it
again you take that extra amount of time
but when it comes to an unbonded data
source things are a little different
it's because your data is moving forward
in time and you don't have an infinite
buffer to hold it forever you want
certain consistent guarantees between
failures so you cannot start over at
well our job all over again and start
from beginning and the reason you may be
doing the real-time processing
processing is because you want that
results early enough and not have to
wait for it if it crashes so you need a
way to capture consistent state so that
when it fails you can resume from a good
checkpoint and the other thing you need
is if you want to do some kind of
aggregations and groupings you have to
group your unlimited or unbounded data
into smaller chunks but there's no way
around it so you need good tools to
reason about how do you group that how
do you deal with late data how do you
deal with
you know errors in the system so there's
two really important concept when it
comes to unbounded data stream
processing the time the notion of time
associated with event is really
important as well broadly can think
about it as three different times
associated with an event even time is
when the event is actually created and a
timestamp that's associated with it
ingestion time is relative to your
stream processing engine when it enters
into the system your system assigns a
timestamp to it so that you know what
time you actually ingest said the event
into it and that condition time kind of
remains constant throughout your
processing and the last one is
processing time that's the time in your
computation operator or your engine
actually sees that so if you have a
digraph of operations that you're
connected together each one will see it
at a different time because everybody is
not getting it at the exact same time so
let's look at the simple example to
better understand those three concepts
so you have some kind of a IOT sensor
that's emitting events and you're
capturing those events and assigning
timestamps to it so that's the event
time and it makes it way through maybe a
queue or messaging buffer like Kafka and
it enters into your processing system
and there's an injection time I signed
with it there are different use cases
for it so for example you may just worry
about I just want to know how many
requests per second and getting into and
processing engine through my job graph
right so an injection time is a good way
to know that versus you want to know you
know what's happening on the sensor in
the last five minutes you know how many
sensors is it Emmett or how many events
did it Emmett then you want to go by the
event time otherwise you come to be
inaccurate so if there are ten events
that were being generated in a five
minute time span but it took eight
minutes deliver because of network
issues or delay then if you just say in
this five minutes what did I get you'll
get a wrong answer because you missed
the other events that came late so if
you do it by event time then you get
accurate results
then doing we talked about being able to
group and make your unbounded data set
bounded so you can do operations on it
and usually you don't need it if you're
just operating on an element by element
basis so for example let's see all
you're doing is filtering and you're not
doing anything else that you really
don't need windowing because you don't
need a group of things to work with but
if you do want to do a group of things
like you know aggregation or even a
group by your accounts and sum or
average then you need to bound your
unlimited stream otherwise you'll be
buffering that day said forever and run
out of resources to do any meaningful
computation on it so the ten of two
broad classification of Windows one is
aligned and underlined with aline
windows what we are saying is if you
take a view into your unbounded screen
then everything in that unbalanced
stream belongs to your window and you
can have multiple windows into that view
with on the line you're picking and
choosing out of it so like tumbling
windows sliding and does it be aligned
because you're taking fixed chunks of
your data in everything that's in that
junk you're using it for processing
versus something like session windowing
you're picking and choosing which events
fall into a certain window so let's look
at an example so in this use case we
have an unlimited stream of events
flowing through we have some that come
late in time some arrive right in time
so we have an event that occurring
between 12 and 1pm and even time
actually was between 11 and 12 and for
the second one it was between 12 and 1pm
so we see here that the event that we
picked out into that window and all of
the events that fall into 11 and 12 will
fall into that vendor right versus if
you look at something like sorry let me
go back to the sliding
first so we had the tumbling window
where we have clear boundaries between
the two windows and there's no overlap
that means an element is not included in
two windows at the same time that
overlap and if you have a gap between
the windows so for example here these
are one our windows and let's say the
second window started 1105 instead of 11
now you have a five minute gap there and
let's call hopping windows so you can
sometimes use it for sampling your data
set and you mean you don't want to
process the whole stream the next is
overlapping like sliding windows so for
example you can say every minute give me
a count of the requests that are coming
into my system right so you can of every
minute you're looking at the events and
you may recount the events that fall
across the boundaries lease this is the
example of underline windows the session
windows as you can see between the first
ten to eleven o'clock the events that
are in our input stream are actually
being grouped into multiple session
windows so it's not symmetric it's not
aligned with the input so that is called
in unaligned windows and you'll hear
this in different literature so it's
kind of good to keep in mind when you
hear these terms so let's look at
triggers now you've grouped your
unbounded data set when you figure out
you want to compute the results on it
right we just grouped it right now so
triggers hope you determine that when
exactly you want to compute results out
of it so it could be before you decide
that my window is complete so for
example if your window is an hour long
you don't want to wait for that whole
hour before you get the results you want
to get a quick peek into it so that for
operations you can make sure
everything's fine so you could say every
five minutes you know whatever you have
just compute the resulting show me so I
can see progress that's happening and so
that's kind of before window and then
you can say then it's the our boundary
give me results or omit the results or
you could say do it later so for example
you could get an event at 1205 and
you've already computed your window for
11
12 and you get one more extra event you
can roll that back into your some later
on so the triggers kind of determine
when you actually emit the results and
competition of a window so let's look
through a concrete example when you land
on our netflix home page you see a list
of movies on the bottom and we want to
compute in how many times somebody
clicked on a certain movie box art so it
makes it easier to think about it in in
four questions right so what is it that
you really want to compute in this case
we're interested in computing counts per
movie ID and where that means you know
what's your timeline or time-bound green
event time that you want to compute it
over and you're saying in every four
minutes I want to see how many people
clicked on this certain movie and I want
to trigger the results every two minutes
I can see the progress of what's
happening and make a dashboard out of it
right and last thing is if I'm getting
late events how do I deal with it so
let's have a partner downstream and I'm
sending them the sum and I send again
the last one hour I saw hundred clicks
on this one but because the mobile
device was off somebody's on the plane
they got off and then we got an event
really late and if you cared about
accuracy if you take that event and add
it back and say oh sorry it's 900 it's
actually harder than one that you saw
later on so the last question kind of
determines what do you do when you get
delete data and how do you refine your
results so there's an important notion
of water marks now we looked at grouping
a window but we could get events later
right so how do we determine that a
window is complete we need some kind of
notion of completeness even if you say
if you're going to add that event back
later they're still needs to be some
kind of notion of completeness so
watermarks are used for that they
usually lag behind at the event time and
in an ideal situation you'd be
processing an event as soon as you see
it in your system
and if you were to graph that it would
be like a straight a diagonal line as
you see in here but that's not really
the case in practice it varies so here's
an example we have one stream where the
events are in order and you have the
next stream on the bottom there where
the events are happening out of order on
the one on the top we said we are
emitting these water marks as the events
are flowing through our system and we
shall give you a limited watermark 20
that means we are sure that we are not
going to receive anything after 20 and
it's okay to do the computation up to
the elements that included 20 and these
numbers are time Sam syncytium like a
sequential timestamp and on the bottom
then it's out of order you can see that
you get elements element 20 before you
get 19 and your watermark says my window
is going to be up to 17 right so it
gives you kind of a way to describe what
the lateness is or completeness is about
your system so when you plot in practice
this situation you see that there's a
skew between the ideal and when you
actually actually generate the watermark
because your watermark will always be
lagging behind there's a lag of getting
that event when it generated into your
system do several Network reasons or the
device is offline so when you actually
look at the completeness of it it's
going to be skewed from the from the
ideal line and if you compute the
watermarks too slow then you'll be
holding a lot more data you memory
before you flush it out and move on with
your computation if you're too fast even
miss some events and you'll have to deal
with those events later so it's not a
perfect it's your heuristic watermark
that you build out and then you still
have to worry about how to deal with
delayed events so working with late data
in a real time stream processing system
and if you want accuracy it's one of the
quite challenging things to do
especially when you're emitting results
downstream you don't know if the
downstream sink is at important what
and of side effects it's having
submitted challenging to build accurate
systems when you have later driving data
so everything I talked about has evolved
over time this is a great paper to read
it's a wall from systems like millville
flume java and many other older systems
and the terminology has evolved over
time and this paper gives a nice
overview of how to not only have a
programming model but to reason about
these things and apache beam which is
incubating is kind of a programming
model and abstraction and they have a
runner which you can target your generic
api is under a certain engine like spark
or flame so let's quickly review what we
looked at red we looked at how do we add
time support or what kind of time
support do we need in terms of event I'm
injection time and processing time and
then going we look at war marks how do
we deal with state and checkpointing
checkpointing is really useful for
recovering from failures so every so
often the system could create a
checkpoint both for metadata about your
system so for example if you're
consuming from Kafka then you need to
remember your offset in your stream and
at the same time you may be operating
and creating counts you also want to
store those counts and that's the data
so when you check point both the
metadata and the data and you have a
failure you can recover from it and have
exactly one processing semantics okay
there you go so in addition to you those
basic functionality a lot of these
frameworks also offer transformation
functionality like map filter groups
joins and it also allows you to specify
a complex diagraph of your job and in
take care of computing it and it also
supports different events and sources
and sinks and some of them make it
easier to even launch jobs by
supporting some variation of sequel and
it's generally called stealing sequel
now that's not all you need in an
operation system to put it in production
you need lot more than that you need to
make sure the system's stable and
handles back pressure and there are
choke points in your system or a sink is
getting slower or your sources getting
slower your system shouldn't keel over
and die it needs to handle the back
pressure you need a way to manage your
job code that you're running in the life
cycle of it you may or may not have no
downtime requirements so you need to
take all those into consideration you
need a multi-tenant environment if
you're running a lot of these jobs and
the competition is varied you want to
maximize the resources so you want to
make it running a multi-tenant
environment you need auto scaling you
need dynamic rebalancing of workers and
you need a way to quickly push your coat
out into test and prod and run it
against a shadow data to see what's
happening so as important is the as
important as the programming model is
and the functionality of framework
provides it's equally important to pay
attention to the operational features
associated with that streaming engine
because that's what's going to decide
how stable your system is going to be or
not so let's look at what a runtime and
execution engines we have at our
disposal to deal with this problem at a
high level if you were to you know do
our back of the neck and design of it
you'd end up with something like this
you need something to buffer your events
so that in the event of your producer
going away in the event of failures in
your processing Krista you have a video
get back to your data so you don't lose
it and you can use systems like Kafka
guinnesses pub/sub you name a system
that works for you and then you have the
execution runtime with all the
operational features that we talked
about in multi tendency and then you
have the execution engine itself that
supports you know the notion of event
time enjoying and watermark and things
like that and the other important aspect
of this is if you have a stateful job
you also need
good way of dealing with state if it's a
large state you know how do you deal
with it ideally so what you found is if
you have a system that supports local
state that is backed off to remote state
that works the best because you're not
constantly going over the network and
looking up remote data it's available
locally when you need it and when it
fails you can always recover it back
from the backup store how many of you
are familiar with Kafka and how it works
they're quite a few so I'm going to skip
over what it does but in one line it
lets you have a partitioned set of
eaglets you partition your topic and you
can send events into those different
partitions and you can consume it as a
queue or if you have multiple groups you
can consume it as a pub sub topic so
let's look at Sansa and how it works
internally so Sansa has an interesting
philosophy this is related to point nine
they have a single threaded model for
actual processing but for fetching the
data they have a multi-threaded approach
so for example let's say you have four
partitions that you're processing in one
stands a job it launches four threads it
pre fetches the data and it holds it in
memory so the processing is much faster
the processing itself happens on a
single thread and it goes through three
steps it goes through process window and
commit and during the window you can set
how often you want to winter the trigger
so it's a pretty simple timer just think
about it as you started a java timer or
something in and it fired so that's your
trigger that we're talking about every
30 seconds it's going to give you a call
back and you can decide what you want to
do and in your comet it says okay you I
did the processing that you asked me to
do now come in it so in that situation
you could take a kafka offset that you
read from and you could store it in a
store so that next time when you resume
from a failure you know which offset to
start reading from so come it lets you
commit your position in the stream and
gives you an opportunity to check point
your state as well
and that's both the local state as well
as the metadata about where you're on
the screen so here we see that as the
state changes the local state let's say
let's come back to our account click
example you're accumulating the cons for
different movies and it's stored in a
local state and it flushes that state to
another Kafka topic which street is it
treats it as a changelog stream so it's
very much similar to the event sourcing
pattern there where every single change
is emitted into a changelog stream if
you fail it's going to read the whole
changelog stream again the problem with
this is if you have really large state
and you have a lot of updates on then
your change lock stream is going to be
really big and your recovery times are
going to be very large so you've seen
that that if you're you know upwards of
one gig to Gig it takes a while for it
to recover from its cold state and if
you want to hook together a job graph in
Sansa you have to you actually have to
connect them with topics in betweens you
have to create the CAF topics you have
to link them up for each job so it
doesn't take care of automatically
running a complex digraph for you you
have to manually hook it up so it looks
very much like I said architecture there
and there's a lot more work you have to
do they are working on some
announcements to make this easier but
point nine and have it so let's let's
see what was missing out of Sansa right
so we didn't have any kind of event I'm
support out of the box that it's
supported there's no way to do any
watermarking out of the box you have to
build all that functionality in it and
from a runtime perspective it's kind of
a single-threaded blocking model and it
does i oh so you could assign it one
core in a multi-tenant environment if
you're running you know like a darker or
mesa style environment but again when
you're doing I oh the resources are left
unused so they're changing this model
slightly and they're going to add
support for multiple threads so they can
leverage those resources better so it's
spark streaming how many familiar of its
proxy
okay he answer good so the idea behind
spark streaming is it takes the input
screen and it actually chops it and up
into smaller batches I said this micro
batching and batches below a second
below half a second have a lot more
overhead and we'll look at it why that's
the case in a second so chop set up into
smaller pieces and then it processes
each piece so it's not exactly like
windowing even think it kind of looks
like windowing but it's just micro
batching upfront and what your results
are also kind of results of those micro
batches so the spark is notion of our DD
so our duties are to say briefly their
collection of elements that are
distributed across a cluster and their
partition so every already can have
multiple partitions and you can do
operations on it in parallel and our
duties also when you do transformations
and our duties they also record the
lineage of it and the our duties are
immutable so once you create an RDD and
you apply transformations of it you get
new rd DS and this lineage is captured
so that when there's a failure they know
how to recompute it back because
everything is mutable and they just
applied functions over it so it's the
functional programming aspect of it
that's leveraged quite a bit there to
make it happen so in this case we see we
have our duties for each micro batch and
we have some operations on it and then
you could do some kind of enduring light
by grouping couple of batches but then
it has to happen around the batch
boundary right so if you say my micro
batch is one second then you can do your
windows and multiples of those batch
sizes you cannot have an arbitrary
window size it has to match your batch
size so it's kind of a limitation spark
Todaro they have experimental support
for even time it's not fully out there
yet and it's not the pliable in
production but you could take a look at
the initial support they have for event
time but currently they don't have it
that you could use it in production and
spark uses the notion of d stream which
is nothing but junked up r dds
so the streaming is still micro bashed
and it is called a discretized screen or
D screen so for efficiency what happens
here is if you see the four small
rectangles in a bigger rectangle these
are all individual rdd partitions and
I've kind of overlaid two concepts here
so let me explain to make it easier you
could have lets you get have map and
filter and flat map that you're sorry if
you let's say you have map and flat map
that you are doing it in your job a
spark can take a look at that and say
okay map and flat map I don't have to
cross Network boundaries to those
operations it's going to change those
operations together and launch it on one
single JVM and it's going to apply that
to a certain partitions of our DD so it
can do that operation in parallel across
all the partitions of an RD that are
spread across your whole cluster so
that's what it's doing here legs doing a
map a couple of maps and then it's doing
the radius by key at the unified spark
Toro has something called structured
streaming they just have more explicit
data types and have unified how you do
the API programming through data frames
and data sets and looks more like
sequels you can write one style of code
and you can apply to both screaming and
bats out of the world so they have a
planner is very much like like a
database where they have a planner it
creates logical plan and at the end of
the day everything gets converted into
our DD operation so this is an
abstraction that sits on top of our dd's
to make it simpler for you to write jobs
and this just shows how at how the
planner is incremental as well and it
needs to figure out how to match the
micro batching architecture as well so
when you actually deploy this in
production you have a few components in
place one you have the cluster manager
that you're launching this end it could
be
like marathon or may so so that you can
launch the driver program the driver
program kind of works like a coordinator
it's managing what's happening on the
worker nodes and the task that it gets
assigned so if you have a job graph it
needs to determine which jobs need to go
first and don't have a dependency so it
does kind of a bread for scan schedules
those jobs in production when they're
done it takes the results back feeds it
into the next set of jobs and it's could
use those jobs now it's parked because
it's micro batching every micro batch
let's say you're doing 500 millisecond
micro batches every 500 milliseconds the
number of events had received it's going
to put it on a batch and let's say you
have five different operations you're
doing on it it's going to schedule that
on the cluster and every scheduling and
and every micro batch results in a new
scheduling so you're paying a little bit
of overhead there although it's pretty
fast but if you lower your microbots
eyes you pay that overhead of scheduling
every time now they do have what's
called executor java process that's
running out on all these holes it's not
a complete cold start it does have the
JVM up and running but you're still
making that scheduling call over the
network and trying to get the job up and
running so we looked at it spark they
currently don't have full back pressure
support what happens is every click or
time click so let's go back to the micro
batching example of half a second so
every half a second in its scheduling q
it's going to schedule an event saying
okay I want you to schedule this job on
to the cluster and regardless of what's
happening to your processing and how
slow it is it's going to keep adding
that event into the scheduling queues
it's never going to stop so as your
scheduling delays build up let's say you
have a problem with your saying or
there's a node that's bad in the cloud
and it's taking longer to compute then
the back pressure builds up your
scheduling q keeps growing forever and
eventually we'll run hub memory right
and so we have this problem in
production for some of the jobs are
running so we
monitoring job that looks at scheduling
delays and kills the job and I sort of
jobs right it's it's not ideal so the
back pressure support we've worked with
the community to get the back pressure
support ahead it's not fully there yet
there I think couple more JIRA's left to
be completed there for the full-back
pressures of put to get in and the back
pressure support is more like a PID it's
like a feedback loop from control
systems so it's not completely autumn
automatic you still have to tweak a few
knobs to make it happen so flink is
another new stream processing engine
it's streaming at the core on lexpark
it's not batch based it's actually true
streaming based and it can handle both
streaming sorry I can handle book
bounded and I'm bound or data so let's
take a year so this would look pretty
much similar to the api's and
functionality we have on the spark side
right you have all the same kind of
operators available to do map and flat
map &amp;amp; co groups and updating state and
if link also takes care of automatically
managing the state that's associated
with each operator and it also takes
care of check pointing that state and
creating backups of that state so it
creates a point in time snapshot of your
data associated with each operator and
also what's happening to your input
stream and it stores all the metadata as
well so it actually offers exactly ones
processing semantics out-of-the-box
spark does that as well by the way with
our duties and the lineage it holds on
to so we see here on an example of how
pipelining is happening very similar to
the spark a PIV looked at so they're
pretty similar in that format the way
they get scheduled is kind of similar
instead of having executors you have
something called task managers here and
instead of the driver program you have
got something called the job manager of
it coordinates what's happening on these
nodes they have a notion of tasks lots
too similar to the tasks you saw in
sports streaming and this this one shows
the state that's being accumulated for
each operator which is stateful so this
goes into little more into detail about
how actually fling does its
checkpointing and so you have the job
manager what happens is the job manager
inserts barriers into the streams that
are flowing and these barriers actually
don't lock your processing that's
happening on the different nodes what it
does is it lets the processing on that
node align to the barriers so that when
barriers from two different streams and
it will wait for those to align once
they align it will trigger a checkpoint
and these checkpoints you could have a
lot of checkpoints in flight because
none of them actually block your
processing which is the best part and
when it when the two streams are aligned
with the barriers it creates a
checkpoint and sends an act back to the
job manager the job manager records that
checkpoint and also takes a snapshot of
the state of the operator so if the job
were to fail at any point in time it can
exactly recover a point in time where
you left off so it does that out of the
box it has a really nice support for
something something called save points
very similar to the database so when
your jobs running you can issue in
command and say three the save point for
me and then you can take that save point
and either start a new code provided
it's compatible or you can run another
job starting off that state so you can
do a graph of very much like get you can
take the state and then spawn off into
different branches and run your
processing through it so either you
could use it to do a/b testing or you
could launch a canary in prod and say is
my new job actually going to be able to
successfully start off where this job
leaves and so you can do all that kind
of testing without even touching your
existing job that's running so save
points are pretty powerful in letting
you do those kinds of things so we
talked about how do we do you know like
analysis
real-time analytics on these streaming
jobs and unbounded data the one thing
that I'd like you to kind of consider or
question is are there any applications
that you write today and you have to
write a lot lot of custom code for which
could have fitted into a streaming model
and when we look harder than quite a few
problems we look into where we could
actually apply this model so it's
another you could think of it as another
pattern or an architecture to build your
applications out of so let's see what we
are actually doing about this in our
production my team deals with three
large areas we have messaging as a
service which is where Kafka falls into
place via stream processing as a service
and we have something called a
management service that helps us manage
our infrastructure across all the
systems and automate the critical parts
of it and so we can scale without having
to scale people and our philosophy is to
create reusable functionality so people
can quickly build their applications on
top of us so we might have you know
libraries to do anomaly detection and we
may have libraries to do some kind of
specific processing that makes it easy
easy to use for others and we help
curate these common functionality across
different groups so let's see what scale
Netflix runs at the background picture
you see actually is a real store with
that name in Berlin which is really
surprised to see so in a netflix is
available pretty much globally today and
it's it's deevil's largest streaming
network and you have over 83 million
subscribers enjoying our shows on in a
thousand plus different kind of devices
and they're watching over 125 million
hours of video every day so they washed
a portion of it while we're sitting here
and watching this talk so this leads to
you about a trillion events that gets
processed in our pipeline every single
and about 1.4 petabytes worth of data
flows through it we have peaks of 16
billion messages a second and forty
three gigabytes a second a data volume
that's flowing through it and we do all
this with four lines of availability so
we have steadily grown the number of
events we have seen in relation to the
subscriber base that has grown and
you've had to evolve our system over
time we enter inherited an old system
that we replaced earlier this year which
is what we dig into a little bit more
deeper so this is the snapshot of our
management layer where the users can log
in and let's say they want to start
generating events so they create a new
data stream and what that does is it
automatically provisions a new kafka
topic based on the ballpark guesstimate
they provided us of what the traffic
could be and then the link of what the
sinks that be they can declaratively
specific water filter is kind of looks
like adjacent pad filter and when they
say provision we just provision for them
we launched it in a multi-tenant
environment running on top containers
and they're up and running so it's a
stream processing is a service but the
functionality is limited in what they
can provide and by default automatically
get a dashboard for each stream they
create and how fast the steams doing
what are the sinks doing are they
healthy we even have alerts based on if
there are issues with lags so you could
have a bad expat filter or Jason pad
filter that you specified and it's
taking too much of cpu processing and
then you start falling behind because
your source is producing a lot more data
and so we have alerts on that to notify
that there's lags and we can identify
problems faster so this is how it looks
like currently we have a set of fronting
kafka clusters and the reason we've
separated the fronting and the consumer
kafka classes for isolation and
availability the fan art is pretty high
for some of the topics so if you were to
li
on the same cluster it would impact our
cluster availability so we have
something called Kafka Kong we do this
every week we intentionally move traffic
from one cluster to the other we built
this automation when we went live first
and you do a bug in zookeeper and Kafka
we had an outage and our larger clusters
that were beyond 100 nodes didn't
recover at all so this lets us do a
quick failover to a new cluster without
losing much data and you've done it
several times in production since then
it's been a year so we've done it'll
just 30 40 times intentionally in one or
two times unintentionally so this is how
it looks like we have events flowing
through our pipeline fronting cosmic
cluster goes down okay and we replace it
and it's fully automated so this is how
the event flows through and we run it on
docker containers we have our own
runtime and we use my sequel we don't
have we don't have any messes or yarn we
just kept it simple you're going to be
moving into that so what we're doing now
in our in our next challenge is actually
be able to allow users to specify custom
code and we are choosing fling for it
because of the reasons I mentioned
earlier better state management better
tooling for reasoning about time and
you're going to look at beam later right
now we want to make sure Flinx running
in production and so very scalable and
stable so we are we looked at the router
use case this is how it gets deployed on
to our tightest runtime execution
environment there's a nice talk on q con
if you look at how our runtime is
actually built there's a separate team
that boost its runtime and you're going
to move through this here's a job they
deployed and I'll show you an
interesting part of it oops I think it
stuck
okay so when we did performance metrics
we found that the swing side of it is
saves us about seventeen percent of
costs compared to Samsa about forty
percent better cpu utilization twenty
percent better network utilization so
that's our initial numbers that we got
for a specific use case you may not be
able to reproduce because your use case
might be different your deployment might
be different and you may not be running
in the same Club sorry for going over
but i'll be in the netflix booth on
third floor so grab me if you have more
questions you want to chat more</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>