<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Predictive Load-Balancing: Unfair but Faster &amp; more Robust&quot; by Steve Gury | Coder Coacher - Coaching Coders</title><meta content="&quot;Predictive Load-Balancing: Unfair but Faster &amp; more Robust&quot; by Steve Gury - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>&quot;Predictive Load-Balancing: Unfair but Faster &amp; more Robust&quot; by Steve Gury</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6NdxUY1La2I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you for coming to my talk so the
title is
Lord Brahma Singh and friend faster and
more robust my name is steve bourie and
walking a Netflix in the edge group
which is the group that do all the
critical services at edge weather booth
located in the third floor and we also
hire in obviously so I will start this
presentation by asking what is Lord
Brahma Singh I just want to specify that
I'm talking about clients I don't know
anything and not all the way a lot
bronzer so what is not bouncing
that's censoring this simple question I
have a set of servers they are all
completely nautical and I want to ask a
question to one of them but which one do
I talk to and there was multiple way of
doing this so during this talk I will
use lots of live demos this is one of
them you can see that on the left you
have a client this is this blue circle
that sends a request to a server the
server is this green circle on the left
you see this number which basically
represent the number of requests that
the server is actually processing for
simplicity this is single thread server
so it means that when the server is
possessing a request if it receive
another one during that time between
queue the request and then need to
finish the first one before actually
computing the next one so when you have
one server like this there's basically
nothing to do with load balancing but
when you add multiple servers this is
where it's becoming interesting so
during those examples you will see on
the top the name of the load balancing
algorithm in that case random and some
details about the speed the incoming
rate of requests so the first one that
everybody knows is random which is
pretty easy you roll a dice you pick a
number and you select that server there
was nice property with random number
generator they are not biased to
specific one specific number so it means
that over time every server should
receive the same number
request another very common
load-balancing strategy is run Ruben
we've run Ruben basically the client is
taking a very simple piece of
information the last every tax - and the
next time you want to talk to another
server it pick the next one in the list
so you see you have this nice wave
pattern of requests coming from the
client to the server and responses
coming back from the server to the
client this one is very common and used
almost everywhere
the third not bouncing strategy that
people use also very common is called
least loaded or join the shortest queue
in that strategy the client need to have
some sort of data structure locally that
represent the number of outstanding
requests that you sent to the server so
basically every time you want to send a
request you take you you pick the list
loaded so the smallest number in that
array and then you increment the number
of requests that you send to the server
and when you receive the response you
just Nick Ramon that number so that's
pretty simple so this is further the
command tech common algorithm that
everybody is using but why do we care
I'm asking this question is there an
impact on the performance of the system
which bring back my memory of my first
job many years ago I was working a small
video game company and my first task was
to design death specific server so my
technique called me like okay like we
have some server that we need to do it
needs to handle under like five RPS
that's your first job you need to endure
it so I walked on the client and the
server and very quickly I managed to
have something that works
it was working in steady state very well
at 25 RPS I didn't really manage to have
better performance at that and I came I
came to the tech lead and told him I I
have the server to me ok fine but you
also need to be sure that the p99 is
below 5 seconds
oh I say sure but what's the P 99 yeah
okay let me tell you if we graph over
time the duration of every request you
will see like on this button militancy
plot you will see some requests are more
expensive than others sometimes some
requests are just more computationally
expensive or maybe you have some queuing
on the server side so they are not
taking all the same time if you draw a
line that basically split that graph
into two hour the p90 will be in that
case 90% of the point are below that
line and 10% of the point are above that
line that's what we call the p90 and
that's a very interesting measure to
know the teletón ZF of those requests
there is something else that we can
compute it's the East Oh Graham this
program is basically you count how many
requests we receive at one specific
latency and the aides of the bar is
basically the number of requests that
you receive and x-axis is the the the
latency of those requests so we could
keep track of the p99 on that histogram
so you see that it's 3.2 seconds so I
came to my technique that's like that's
fine we measure the p99 that's three
seconds the goal was to have something
below 5 seconds so we good it's a fine
yeah but I told you that the obvious
should be 5 not 0.5 it's right like we
just use ten times more server
yeah yeah let's see about that
and then we just like created ten times
more server and we increase the obvious
I was pretty happy about that and then I
realized that all of a sudden the
latency characteristic of my system I
have like ten in article servers behave
differently you can see on the latency
plot in the bottom like there is many
outliers the latency of those are off
even though they are completely not eco
like the system well was Wow on the
histogram you see that the the p99 is
now at eight seconds so at that time it
struck me this is why this vision
systems are odd it's difficult to reason
about them if you take one server one
client you understand very well how the
behavior of those seven counter or is
when you multiply the number of servers
you have something different now you
have a load balancing algorithm in place
and yeah all the prime of queueing so
why do we have this problem it's because
even though the random number direct or
guarantee that over time every server
will receive the same number of requests
it's actually possible that some server
will be picked multiple time and they
will receive three consecutive requests
when it happens you see that some server
will be more loaded you see that some
server have a queue of like eight nine
and some server basically doing the
thing with a cube zero or one or two so
that that's that's the reason so how can
we solve that problem there is two way
the first one is does not one you switch
to another advancing algorithm in that
case I used list loaded so the client is
basically trying to even out the load
toward all those servers and when you do
that you just need to wait a little bit
for the queue of servers to drain and
you see on the latency plot that
basically the system behave kind of like
a combination of small systems if I wait
a little bit
I need to add about act 10 to 20 seconds
that those bad data point is a
we see that the p99 should convert below
five seconds which was my goal
yes it does that's this not way but at
the time I was not not so this is what I
did any kind of walk the same ways so
you multiply the number of servers by
two and you have the same length of C
characteristic it's little bit more
expensive so load balancing implies the
world balancing and you have multiple
strategy to do it
the first one is ensuring that it shall
receive the same number of requests
that's basically what random and run
webbing is doing if I wait an hour and I
look at the number of requests that each
server is receiving they will all be the
roughly speaking the same the other
strategy is to guarantee that at any
point of time every server with having
the cue the same number of requests
that's basically the strategy of this
loaded the third strategy is to minimize
some utility function at the client-side
fraction for instance the latency and
that's basically the strategy that I
will describe in the next and
exclusively first light but there is a
couple of troubles that all road burns
mega reason are usually having the first
one is not all server are always the
same you may have at some point some
server that are a little bit slower or
some arrows let me try to demonstrate
that with one client talking to ten
servers using the run rubbing algorithm
we aren't steady-state when you start
like sending traffic to several
everything is fine of p19 are four
seconds everything is correct nothing
nothing to worry about if all of a
sudden I take the three first server and
that says something bad is happening to
them they be they become slow on the
client-side because you just want
you don't have any piece of information
you don't keep stats per servers you
will continue to send send traffic to
the server the same way as before you
see like one after the other it receive
a request actually the server are very
latent they're very slow so you
shouldn't do that when you do that you
see on the latency graph that you have
like points at very high latency a
better strategy would be to use list
loaded algorithm you you see that on the
client side all the data the data
structure that you have the slow server
we have a high number so document will
tend to favor all the lower several the
fast one as long as you have enough fast
ever to talk to when you switch to that
strategy you just send a very limited
number of requests to the slow server
and you don't impact your PID
p99 in that matter another trouble that
you have with load balancing algorithm
is a common one which is called a
thundering herd it happens when multiple
trends are focusing their traffic on a
subset of the cluster and I'll try to
demonstrate that that that one is pretty
difficult to to demonstrate so maybe
maybe not be that obvious so in that
case I have two clients sending traffic
to a series of six servers we are in
steady-state everything is fine I have a
p99 of five seconds let me try to
simulate a typical outage so let me
increase the traffic now all the servers
don't have enough resources to unsolder
traffic so what will happen is basically
they will start queuing messages the
queue will grow and then some areas with
fire and people will be page and people
came back and say oh those servers
already what do I do I start new servers
but those servers are slow and all of a
sudden all the concern all the traffic
to those new servers but those service
because I sold a
therefore all the requests and they will
came back yeah there they are planned so
that's actually the opposite of what you
should do you should slowly warm up
those servers and when they are warm
enough then you even have the load in
that case you see in the latency plot
that he had a couple of points that are
very high in time of latency those
points are basically all the data that
you sent to those reserve and if you
have already doing that
the third trouble is that we want to be
resilient to what in presence of
outliers what is an outlier of its
basically if you have you the server
that ran on the JVM you probably have
heard of the term G suppose that's
basically when the JVM is collecting
memory and usually it's unavailable for
hundreds of milliseconds up to sometimes
seconds so in that example we have this
client talking to 6 server we are in
steady-state p99 of 3.5 seconds
everything is fine and all of a sudden I
take two random server and I created G
suppose from the client point of view
because it doesn't keep track of
anything any stats on the server side
basically to continue send traffic to
every server one after the other and
what will happen on the G he posts all
right they will be the key Q those
requests and you see on the latency
point that when they came back alive
they basically drain the queue and I've
spent something about 1600 requests
because of that if I switch to list
loaded and I do the same thing on the
client side you have this data structure
that keep track of the number of
requests you sent and you will see that
those slow server that are basically in
G suppose they will receive two requests
and after that the server the crown just
sent all the traffic to the lower part
of the server in that case I only wasted
two for requests because of those
not 15 or 16 like in the previous case
the impact on the P 19 is drastically
reduced the first rubber is the the fact
that a large cluster of servers may
dilute the local state that U is kept by
client so what I what do I mean by
dilute let's take another example you
have one crime talking to five servers
and using the list loaded algorithm
we're in steady state so the Klan keep
track in this local data structure of
the number of ads animal craft if all of
a sudden I keep the same RPS but I
multiply the number of clients now I
have six clients so each client is
having this data structure internally
and there is no coordination between
crimes whatsoever so for some clients
you see that you have lots of zero which
means that next time you want to pick
server you randomly choose one of those
zeros so the value of of this data
structure is actually reduced when you
have only one client you almost have the
perfect view of the key of the server
when you have many claims you have a
dilated dilated view of those servers so
if I want to draw load bouncing matrix
of the the algorithm versus the
different problems that we see so an
even server several top slow or fast and
during earth is everybody rushing toward
the same set of server out layers is
being resilient store DC and large
cluster is when you have thousands of
gems are you resilient they all have
problems they manage to work well for
specific condition but they are rough
current so the question that I ask is is
there a load balancing algorithm that
works with all those problems and the
answer is I hope so
so I will present to you this new
technique that we walked on
we call that predictive load balancing
and the idea is pretty simple is to use
the latency as the measure of the load
that you send on servers so basically
when you do list loaded you count how
many requests you send to servers with
latency based on Bronson you sent how
many milliseconds of CPUs and two
servers and it really burns based on
that so let's take an example simple you
have two servers one at 50 milliseconds
the other one at 120 milliseconds on the
client side you want to send a request
so we'll pick the least loaded the load
of those two servers is 50 for the first
one and 120 for the second one so you
pick the first one the load of the first
one become 100 it's 2 times the latency
you another request is coming you want
to select the server that case 100 which
steals lower than 120 so you select the
first one again the load of the first
server becomes 150 again another request
is coming on the crown side you want to
send it in that case the second server
is actually the don't want at your favor
so you will send a request to the second
server and obviously when you receive a
response you just D criminal or um the
value of the latency so you see that by
doing this you favor the faster server
compared to the slowest one and it's
pretty simple to keep track on the
client side you just have to add the
latency and you count how many requests
you sent so because it's very it's very
fast server it kind of work on the the
prime of an even server if you have a
cluster of fast and slow server that
algorithm will favor the fast one but I
use like the word latency that's pretty
difficult to say that when you send tons
of requests to a server you never have
one latency you have an Instagram of
latency in that example this is actually
an
a gram of latency of an actual
production server at Netflix so you see
that most of the requests are answered
at about 70 milliseconds but your server
requests that are under at 20
milliseconds and some are 140 and some
above that so how can we extract a
number from that histogram there is one
thing that we know is if I take two
histograms from one can to two servers
it's roughly stable it's look like the
same we know that if 5% of the requests
take more than 100 milliseconds on
server Bay server be same thing will
happen once every so the value that we
collapse that histogram to toward is
actually the median the median is the
value that cut that graph into two
equivalent area that's a very stable
value and we want a stable value because
we don't want that eating an outlier
impact that that data if use an average
for instance and then you send a request
to a server and it turns out that it's
an expensive request you don't want to
change your expectation about that
server it's only when the server itself
becomes slow that you want to change
that night the other prime with the
median is recency is actually important
this is the latency plots from some
actual production server metrics and you
see that over time there is some action
that could happen that may make the
server actually slower and sometimes
significantly slower so you want to have
some measure of the latency this minion
that actually move quickly when the
server is becoming so
so we used a streaming media latency
basically it's kind of measuring the mid
the the median of a sliding window it's
little bit more complicated than that
but I thought we have time to explain
all the details that's a stable value
and that salsa episode recent data so
I'm running a simulation in the
background I'm having a client like
talking to different servers and this
red line that you see is basically this
median which is completed on the fly
what we want to have is the median
moving quickly when the server is
becoming slower so if I make the server
slower
I want the median to jump pretty quickly
that's basically what I have if again
take those servers
I make them slower the median is jumping
and we want some pretty stable value and
also jumping pretty quickly when the
server is becoming slower I talk to take
the server fast fast enough again it
should very quickly recover to the
previous area but again divulge the
details and there is many things that I
never we talked about one is like how to
estimate the latency of a new server
like I have a sets of servers I bring
one new server to the poor I never talk
to that server I have no idea what his
latency characteristic how do we
basically you talk to it what is the
latency estimation it off like let's say
that I bring a core server to the tool
that server needs one minute to be fast
I first estimate the latency but latency
is high maybe I will never talk to that
server again and what about fast server
that actually are fast because they are
dropping most of their requests so for
the first case we serve that program
with this simple state machine when you
bring a server to the poor if you never
talk to that server it became the best
server ever its latency zero
so obviously the next request that you
will send this to what that server and
after that immediately after that
request it becomes a war server so the
its latency is becoming almost infinity
and during this death phase between the
requests between sending the request and
receiving the response we have this some
sort of probation mode where we
basically don't send any more traffic to
that server and you receive the result
after that first response we have like
one latency data point and we just use
it like a regular server it's being part
of the pool like any other server so you
see that that technique basically it's
some sort of warmup mechanism when you
bring new server to the pool you will
send like a few requests to estimate the
latency and you if that's a very slow
you won't talk to that server again and
use that to basically warming up those
servers so that's useful but what if the
latency estimation is off especially
when you bring new server new core
server to the poor it's very freaking
that those about needs few seconds or
sometimes minute to actually be fast and
when you first estimate the median
latency if that median is - I
you will never talk to that server again
so that's why we decay the value when we
don't talk to that server for a long
time and to two cases could happen that
value decay then it's equal to the other
servers and then you talk to that server
again maybe it's becoming fast now then
you bring that over to the pool or it's
still slow and then delete some seals
grunting at and then will be killed
after after why the other annoying case
is the prime of a server but bad server
so to solve that prime there is two
thing that we do
one is we nee measure the latency of a
successful response
successful responses if a server sending
lots of air
but very quickly we don't measure those
those arrows only measure the successful
response and at the same time works a
measure the success rate which we use
for basically favoring or penalizing
servers so if one server is sending 20%
of arrows and all the other servers
actually at 100,000 you will we will
penalize that server by a 20-person
latency increase so if it's latency is
100 milliseconds but send 20% of arrows
then its latency became 120 a third
problem that we have is the prime of
keeping fresh statistics when you have
large cluster of clients and servers
when one current is connecting to all
sorrows your statistics for every seller
may be a little bit out of date that's
happened because we have too many
connections and a simple example let's
say that you have a cluster of 10,000
servers and you send like every client
is sending 1,000 obvious that's
basically one request every 10 seconds
per server so six requests every minute
that's not enough to have like accurate
statistics my server especially when we
know that a GC could happen at any time
or the server could have some background
thread running or some compaction that
could happen so you need to have like
more fresh statistic the way we do that
is by sizing the number of connections
based on the number of al-saleem
requests so you have 10,000 servers but
if you delete the average latency is 10
milliseconds it means you could do 100
RPS so to do 1,000 RPS you just need 10
servers you don't need 10,000 servers so
if every count is self selecting 10
server in this big pool everybody should
be happy and you should have like fresh
statistics that's a little bit more
complicated than that because many
clients may choose the same server and
you will have over audits
so to walk around that forum every
is basically refining its you all the
time affecting slow server the server
that I picked by multiple trends so you
see that it's our way of walking around
s prime of large cluster that dilute any
information that you have on the
client-side we do it by sub selecting a
view from the cluster from the current
point of view so all those statistics
are fine but the big frame is we need
one slow response to realize that the
service flow we would like to know that
the server a slow before you actually
reply to us that's kind of interesting
so that guy is agnya long nothing
related to the programming language was
not programmer it was a mathematician
that's the the founder of the queueing
theory and in its original paper we had
this concept of instant news traffic
which basically means at any point of
time a much load did you offer two
servers and we'll use that as our
measure of the constituents value of the
load so the idea is pretty simple
we used the elapsed time per request as
the value of load offer to a server so
if I T equal 10 I send a request at
equal 25 i esteem aid that the server
has consumed 15 milliseconds of latency
15 milliseconds of walk that could be
CPU or anything so that's my estimation
at that time 25 of the load consumed by
the server if at the same time I send
another request to the server at T equal
40 I estimate that the server has
consumed 30 milliseconds of CPU for the
first request and 15 milliseconds of CPU
for the second request and then I'd
compare that to the actual prediction
that I add so I presently predicted that
the latency of that sir should be 20
milliseconds
and I sent two requests to that set to
that server at at equal 40 I realized
that my prediction is actually a little
bit off and then I use this instance
duration I'd a new load of the server so
that's great for things like gee suppose
as we size the number of connections
based on the number of outstanding
requests it means that we basically need
between 1 and 2 median latency to
discover that one server is later so if
you median latency is 10 milliseconds
you will detect 2 GC pause in between 10
and 20 milliseconds and you don't need
to spend tons of requests and discover
that you have tons of time a lot you
will see that way sooner so you see from
the map matrix that we were fine with
the outliers so now that's where I do
other things I will use I will do it
very slowly so you can like understand
what's going on it may may take time to
convert and I hope that it will convert
so on the left side next to the client
will see the load which is what we
estimate for the load of the server the
median will be our estimation of the
median and that number will be the
number of requests that we sent to the
server so initially the load will be 0
because we never talk to those servers
will pick one server will have the
penalization so very high load and when
we go back we got back a response then
we'll have some data so the bitter serve
has been chosen the first one we see the
response we have some median latency
next one to be back should be the bottom
one yeah and we'll receive a response
okay now we have median latency in Star
Trek or normal day-to-day
loved ones in technique let's complexify
things a little bit let me increase the
RPS and also in
the number of servers so the system will
basically try to increase the number of
several talks to but it will also slowly
warm up those servers send only one
request wait for the response before
having adding any data and we find this
view you can see on the left side that
sometimes the number is growing very
quickly that's where when the instance
duration is taking over the predictive
value so it takes a little bit of time
in that case because the RPS is very low
but basically the idea is the system
will collect statistics per server it
will be finally it's you so it will try
to talk to every server and evict all
the slowest one and now if I turn those
three first server into slow ones what
do I want is basically that that client
should be able to recognize that those
are Oslo and those medium latency should
converge to a lower value and what will
happened on the client side is it will
try to favor the lowest part of the the
bottom five server the first one and if
now I increase the number of clients
this is where it's becoming a tricky so
I need to wait a little bit for all the
clients to collect statistics pass
errors and what should happen is they
will collect statistics they will
discover that the first free server are
slow and they will favor the bottom one
what could happen is some clans may talk
beauty Balkans may pick the same server
so that there would be a little bit
overloaded but they will realize that
because they will see that that median
is likely the worst from their point of
view and they will evict that server
which will make that server fast again
which will even out so if I wait again
command algorithm we can do it
the p99 that you see on the on the top
is 15 seconds should decrease to at
least an but if the thing is it makes it
needs a while to convert because yeah
again it's tena it's a way to convert
because of two things the RPS is very
low and I compute the p99
over 30 seconds window yeah 5.9 seconds
the beauty of that algorithm now if I
switch to some other algorithm like
random random doesn't have any
information doesn't keep track of
anything so all clients made overrode
specific servers could be the slow one
could be the first one and the p19 I
should jump you see it's already at 15
seconds what could happen also is yeah
ready toll data
the counts me overload one one specific
server by just like choosing it so
already 19 seconds for the p99
you could say yeah but that's because
random is the kind of the wall stargher
isn't from all of them if i switch to
list loaded which should be one of the
best let me wait a little bit to see if
it convert i can't really tell you that
it won't and it will stay at between 15
and 20 seconds which is definitely worse
than 5 seconds
it's definitely not decreasing do you
guys we think we should wait or I think
like there is no way like I waited a
long enough to know that it won't
converge but nothing is perfect that
algorithm works in most cases but not
all of them do this case is where the
latency is not a good proxy for resource
consumption when you do long polling or
in case like that the latency of the
request has nothing related to the state
of the server under the Prime that we
have and we discover that when we were
deploying code at Netflix is that slow
you have a very slow warmup of course
serums when you think about it the load
balancing algorithm is doing what it's
supposed to do just easy slow server off
to stop talking to them and focused on a
fast one so when you do a new version of
the code like you deploy new servers the
load balancing algorithm will talk to
those servers and say oh that flow so
I'm focusing on the fast one and stop
talking to those servers the latency
will decay and then again you will talk
to those servers the still slow you
focus on the fast one latency will be
key again you'll talk to the server oh
now they are fast and then you to bring
those servers into the pool but
sometimes it takes tens of minutes to do
that so deeply a new card takes more
time but again it takes more time but
you have no impact on the p99 one thing
that was really annoying for us is that
it defeat canary and arises so at
Netflix we have lots of automated
systems when we push card in production
we basically have a canary that ran in
parallel and we have machine learning
that look at all those stats and see if
some of them are a little bit off in
that case you bring to two clusters of
the one version of the code and the new
one and then you run that algorithm
basically the algorithm will detect that
oh that new version of
either faster or slower and if it's
faster receive older traffic if it's
lowered don't receive any traffic so
when you try to apply machine learning
on the stats you will see that they are
all over the place
nothing is you can cover it's not Apple
to Apple in one cases receive thousands
of RPS and your server ten obvious so
you can't really make meaningful
analysis of that so we had to shadow
traffic like split the traffic into and
analyze that the last thing that we
discover actually in production that was
annoying is it doesn't cope well with
error these guys as successes so for
that one we had a server that basically
when something bad was happening it was
replacing the response with some default
value and he was not telling anything to
the can so from the clan point of view
one particular favor was super fast
because it was returning default value
all the time so the way we serve that
prem was actually by writing good code
that's always good that in mind so
instead of returning a success when you
actually have an error we return an
error and we say ok it's an error but if
you want I have some default value that
you can use so in that case the lot
bronzefinger ISM knew that it was
actually an error and didn't basically
count that that value in the median the
other surprising thing is that the
request distribution may be temporally
uneven when you look at your cluster you
may have some server that are like
normally loaded and some server that are
not because they are cold or because you
just bring them and it may be a little
bit surprising to analyze you your your
cluster you will have data all over the
place in our case it was a little bit
annoying because we use a double yes and
we are
sustained that scale up automatically
and the the value that we use was the
load average so when we bring a new new
servers into the clusters the previous
one was maybe a little bit more loaded
and new one who actually underloaded but
when you look at the average
everything's inside even though the
distribution is a little bit different
so in slowly in situation like this you
need to be more careful about the data
that you have and you need to look at
person tiles actually instead of average
so in conclusion I want to say that
that's the only algorithm with four
tracks that's easy to say because that I
designed those tracks it's still
experimental there is only one
production server a service at Twitter
that use it so it's not like widely
deployed and your mileage may vary
obviously I'm sure that we scale is were
Nixon tweak tweaking but in most cases I
would say it should work pretty well
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>