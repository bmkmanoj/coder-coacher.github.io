<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Antics, drift, and chaos&quot; by Lorin Hochstein | Coder Coacher - Coaching Coders</title><meta content="&quot;Antics, drift, and chaos&quot; by Lorin Hochstein - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Antics, drift, and chaos&quot; by Lorin Hochstein</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SM2uXpmyJmA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hi I'm Lauren I work on the cast
Netflix and let's talk about antics
drift and chaos so I wanted to start off
by talking about my favorite outage that
happened at Netflix so there's this
large service that suffers from what we
call cold start basically when it starts
up initially it's not ready to take
production traffic yet because it runs
too slowly so in code paths have to get
executed so that it warms up properly so
that it can respond quickly enough and
one of the ways it gets warmed up is
that there are tests that execute when
the service boots up so one day there
was a test added to the test suite for
doing warm-up that had to do with some
downstream service now the tests that
run on on startup are divided into
categories there are unit tests and
there are functional tests and the
functional tests are distinguished in
the unit tests by this annotation that
gets put on and the distinction between
the way the unit tests and the
functional test run that the unit tests
run earlier in the lifecycle then some
other stuff happens and then the
functional tests run the problem was
that that annotation was left off on
this new test that was added it depended
on some code being executed and life
cycle that wasn't executed one of the
unit tests ran and it blew up and the
instances couldn't start up properly and
we had an outage so to sum this up this
test that was correct that ran is a unit
test led to another chop a Netflix so I
think the moral here is is pretty
obvious you know unit tests are
dangerous we have to be really careful
with these sorts of things we should
really prefer larger scale tests when
when we do development of course I don't
actually believe that the real moral
here is that complex systems act in
really weird ways right if you were
looking at at review that code is
correct that test is correct and yet it
still took down production so I'm here
today to talk about failure in
particular I'm here to talk about system
failure how systems fail often we call
these outages more generally we talk
about them as incidents and I'm going to
talk about three things in my talk today
the first part is the how how it is that
these systems tend to fail
second part is sort of why why it is
that our systems get into unsafe states
and finally the the what what we can do
about it I'm gonna start with it with
the cow part which I'm calling antics
and so this phrase complex systems
exhibit unexpected behavior it's not my
expression it comes from John Galt God
was a pediatrician who wrote a book in
the 70s called system antics about
complex systems and how they behave in
really weird ways and it is surprising
how much of that stuff that he wrote
about is relevant to us building
distributed systems today and so I'm
gonna be drawing a lot in my talk from
the work that Dahl did and his
observations on how systems behave in
weird ways so he has this thing called a
generalized uncertainty principle which
he summarized the systems display antics
hence the name of of antics that this
part of the talk I'm also gonna be
drawing a lot of my material from Sidney
Decker's Decker is a safety science
researcher his background is in aviation
I think he's a licensed commercial
aviation pilot and he's written a lot
about how it is the systems get into
into unsafe States and so we'll hear
more about Dekker a little bit later on
it in a talk all right let's talk about
air handling so when we build attribute
systems most of the time something is
going to be going wrong if the system is
large enough for runs for a long enough
period of time the network has all sorts
of transient problems right switches and
routers get congested and packets catch
up on the floor and so things timeout
and we need to be able to handle that or
you know a disk could go bad the whole
server will go bad so we need to be able
to handle errors that happen in our
system because they're just happening
all the time
and this is great paper a few years ago
by researchers at the University of
Toronto that we're looking at bad
failures that happened in open source
projects that are commonly used in
distributed systems so things like
Cassandra and Redis and Hadoop MapReduce
and HBase and one of the the
observations that they made looking at
people reporting you know big failures
in the field was that almost all the
what they call the catastrophic failures
so these are the failures that led to a
widespread outage or data loss those
were all due to some incorrect
seeing error handling or exception
handling logic right so we need to be
able to handle errors to build reliable
systems there's just there's just no way
around it the problem is as we try to
deal with these these errors these
problems we create new problems for
ourselves and that's really what gets us
into trouble so I'm going to take one
example of that so one example of a the
technique we'll use to deal with with
errors is retries right so you make a
call on a PC call to a service it times
out and so you try again commonly in on
a different server so consider this
scenario you have some kind of micro
service architecture one of your
services goes bad maybe it was under
provisioned and so it's running too hot
your service starts to become latent
right it starts to take more and more
time to respond to requests and so the
clients are calling it start to timeout
what happens the clients retry on
timeout but because that service is
under provisioned all the servers are
early then increases the load on them
the latency is then going to increase
right more clients retry because the
latency is increasing there's more
timeouts and now you have a retry storm
right so this pattern of failure is so
common that we have a name for it right
and this is because we are trying to fix
a transient problem so it all has this
idea that if you build failsafe systems
they're going to fail in non failsafe
ways and there's there's two great
papers recently to talk about this on
both out of Microsoft so the first one
that was about basically they're looking
at different failures and production
systems like Asher and Amazon and
Facebook I believe Google where the
system that was designed to do failure
recovery made the problem worse right so
the the outage was due to the failure
recovery system doing the wrong thing
and then there's another paper also out
of Microsoft about these things called
gray failures where the users think
something's wrong but your system can't
figure that out right so the system
doesn't know there's failures but there
actually are let's talk about support
systems or another way of saying this
why does
Netflix needs so many engineers so we
have over 1200 engineers working at
Netflix and I was once at a conference
and I was talking to a guy who worked in
telecom industry for for a number of
years and he asked when I told him how
many engineers worked here he's like
well why do you need so many engineers
at Netflix to stream video right 1200
that's a lot and so it goes to what girl
calls the operational fallacy and the
operational fallacy is that the the
thing that the system says it's doing is
not really what it spends most of its
time doing and a great example this
outside of tech is Harvard right so you
might think that Harvard is a university
but actually it's just a huge hedge fund
that has this little arm attached to it
that they use for you know to get new
investment and to maintain tax-free
status similarly this is really great
quote from Adrian Cockroft who used to
be software architecture Netflix he's
now an Amazon I mean he says Netflix is
a monitoring company that is an
interesting and unexpected by-product
also streams movies so let's talk or
another outage we had so there's some
non critical service in our architecture
that failed it happened to think to be
the AV service at this point in time so
a daily service goes down we can't run
AV test but people should still be able
to stream videos right so the service
that was calling into the AV service
there was more logging messages right
because it was trying to make a call to
this non-critical service there's a
failure at logs it this service sends
those messages to kafka now the problem
is that the thread that sends those
messages to the Kafka queue takes a lock
and that lock is also taken by the
application threads so as more and more
log messages got generated the
application threads were starved a time
right you have lock contention and then
the system throttled and we had an
outage right so the logging system took
down production and you know you don't
need law games to stream video but we do
it's a support system another example
from a different organization from
Amazon about five years ago they had an
EVs out there so that's a last
block storage that's their persistent
disk system Solon u.s. East one in one
of the availability zones UVs one out
and so there's multiple contributory
factors here but the one that I thought
was most interesting was that there is a
memory leak bug in the agent that
monitors the health of the vs service
right so this is a system that doesn't
provide UVs functionality but it makes
sure that EVs is working properly and
that system failed and that led to an
outage that's not what mitigation so
there's another Amazon outage and that's
three a few months ago right so
according to their you know they
published post-mortem they're billing
process was slow and they're trying to
debug this there was an engineer who was
trying to take some server offline to
help you bug this problem that engineer
entered some some command incorrectly
took too many servers offline and then
you know s3 had a problem so I have this
conjecture that I've been developing and
that is that that once your system gets
large enough and you you've you know
solved the simple problems that all your
incidents are going to be because
there's some support system that behaves
in some weird way or that you've trying
to fix some other issues minor issue and
in doing so you make the problem worse
so to recap this part of the talk the
antics part we have to use mechanism to
improve the ability right we have no
choice so things like error handling and
like support systems like mitigation but
those things can also cause a new
problems right and that's what's so hard
about building for liable systems all
right second part of the talk about
drift so we really like to talk about
root causes when there's an incident
occurs right like why did the system
fail right what was the problem and
Sydney Decker talks about this this
trying to search for the broken part
right like for us it's like where's the
bug in the code that caused the system
to fail where's the incorrect
configuration option that that led to
this problem and typically we'll say
well you know you know someone was
sloppy right someone was careless they
didn't you know do enough testing or
they didn't you know review their
changes well enough and we talked a lot
about blameless post-mortems in our
field because it's so tempting to not do
that right to actually find what the
person was responsible for the broken
part
and blame them for the problem and so we
talk about you know you should really we
need to be more careful right and if
we're more careful
than we will not fail in the future
it'll fail less often so Decker has an
alternate perspective he says that this
sort of hindsight broken part thing it's
not that useful for keeping us from
failing in the future and that we should
look at in a different kind of way and
he calls his theory drift into failure
and he has five different concepts in
this theory then I'm gonna talk about
four of them and how they relate to to
distributed systems so one of them is
this thing that he calls unruly
technology right that that we we build
our systems out of stuff that we really
don't fully understand how it's gonna
behave and I think this is really really
true in software right software is just
really hard to reason about it's hard
for us as human beings to figure out
what our software is gonna do and in
particular we can't build large-scale
models of how our system is going to
behave we can there's some interesting
work in small-scale models and model
checking and stuff like that but really
can't build large scale models of a
little predict things like how the
Netflix Service is going to behave peter
alvaro is a professor at university of
california santa cruz and has talked
here a strangelet previously has this
really great phrase that i love and he
says fault tolerance isn't composable
you don't get a fault-tolerant system by
taking you know fault tolerant pieces
and playing them together it just
doesn't work that way on the leg all
puts it is you can't figure out how a
complex system is going to fail in
general just by looking at how its put
together or more generally you know your
your systems gonna fail in in all sorts
of ways they you know you cannot
possibly even know and to really drive
this home there was this great paper at
the University of Washington where they
looked at three different distributed
systems that had been implemented using
formal methods right informally proven
correct and they still found bugs in
those systems and what's really
fascinating about that so if you're not
that noted formal methods you have some
formal specification of your code and
then you have an implementation and you
you prove that the implementations
consisted with a specification but this
code has to run on you know real
operating systems and so your system has
to interact with the OS right it's got
to interact with make networking calls
and right to disk and so on so you need
to have some sort of specification as to
how the OS primitives behave and all the
most of the bugs they found were in that
specification they wrote about the
operating system right so the formally
verified parts to actually work properly
didn't find any bugs in them but it's
these damn interfaces that we have to
deal with what we went to trouble write
interfaces are dangerous and we know
that that's why we don't rely only on
your tests but we can't get away from
that we can't get away from interfaces
the second concept he talks about is
scarcity and competition so this is the
idea that we're always resource
constrained in particular we're time
constrained right whatever we're doing
we don't have an arbitrary amount of
time to you know verify that everything
is correct right we have scheduled
pressure we work in competitive
environments and so we have to make
trade-offs in particular we're always
making these trade-offs between
efficiency how much work we wouldn't get
done and thoroughness how much how you
know complete we can do on any one given
task and this trade-off is sort of
endemic everywhere and this another
safety researcher Eric Holland ago
called this the Edible principle right
the efficiency thoroughness trade-off we
are always always making these
trade-offs every day between being
efficient and being thorough and so once
again this is a guy writing a
pediatrician in the 70s or I hate
writing around systems this is not
unique to us as software engineers right
this is part of being a human being is
that we create these temporary patches
we make trade-offs with inefficiency and
thrown at us and eventually those things
are going to bite us
so the third concept is called
decremental ISM so that's what they're
gonna see means that the drift systems
getting a dangerous States that happen
sort of slowly it's not like the system
is safe and then all of a sudden
somebody you know does something the
system is unsafe and then a failure
happens instead it happens it happens
incrementally over time so it's an
example when is it alright to push the
production so Netflix we make heavy use
of what are called canary deployments so
when we're ready to push the prod we
don't replace all the server's running
the code the old code with the new one
instead we replaced just a few we
created a smaller cluster with just you
know a very small number of nodes a
fraction of production traffic goes to
those nodes and we look to see if those
are behaving correctly right if there's
not too many air
the latencies are alright then we say
okay it's safe the canary past and we
actually have some automated tooling we
can automatically compare Canaries to
baselines and see if they behave
similarly and then once that's done we
can deploy the entire new cluster abroad
so there was one day where the canary
failed right so there's a problem of the
canary deployment based on the automated
canary analysis system but the engineers
looked at the change that was made and I
said you know what this is an innocuous
looking change sometimes the canary
fails for some transient reason we're
just going to push to production anyways
right
now
so so this is actually super unfair to
the engineers involved in this because
this is what actually happened in this
case but I really love that that
particular tweak in this case that
changed they were looking at was
innocuous it wasn't the source of the
problem the the bug and the code had
been introduced many builds before many
days before but it hadn't been noticed
because it only manifested when there
was a change in traffic patterns now
that new canary that got deployed that
change happened and that canary failed
but they didn't look into that that
particular reason for the failure just
said oh you know is it because of the
most recent change no it isn't okay it's
probably safe and so this is sort of a
general problem that we have right when
we have some sort of you know end-to-end
testing system and it's kind of flaky
and you know the tests don't pass every
time and so we look and we say okay you
know 95% passed those tests are flaky
it's probably okay and then we go
through with it and it usually is okay
right and this happens again and again
over time and you can see how over time
you can get sort of less thorough about
how you're checking things because
you're making those trade-offs and when
you make them it seems like it's safe
right you're there's no negative
feedback initially and diane von who is
a sociology researcher who looked at in
the wake of the Challenger explosion at
you know practice at NASA called this
behavior normalization of deviance we're
slowly over time within an organization
there you know how stricter throw they
are sort of degrades but it happens very
slowly and people inside don't notice
that change and so if you're coming in
from outside you would say wow I can't
believe you're doing this and I have to
admit I've been on many software
projects where a new person has come in
and I'm like oh you know our deployments
aren't fully automated here I feel
really bad about that or you know we're
not testing this part but you know day
to day we don't say anything right
and so this is part of you know us being
humans and building systems the last one
I want to talk about in terms of drift
is sensitive dependence on initial
conditions somebody will call this path
dependence or history right and so that
the thing is that that systems you don't
build large systems that have no where
they evolve over time from from small
smaller systems
so sold all made this observation right
he says that you you know if you ever
find some large complex system it didn't
didn't just get created that way it
evolved someone made a small system and
it worked and they grew that system and
that became a large system and that has
all sorts of consequences right because
people made decisions a long time ago
that are going to affect you now and
that you have no idea that they had no
idea the problem is that we can only
make local decisions right if you're
writing code and you're saying well how
should I respond to this particular
error condition what should I do
you're making a local decision and that
can have some impact you know somewhere
else in Tyler that you won't know in
general so as an example Netflix we have
this service called Evie cash this is an
in-memory you know Volvo cash it's used
very often for performance and so here's
a scenario there's three services to
talk to each other there's a playback
service to talk to the ural service
which talks to an AV cache cluster and
so the ural service is not a critical
service if it fails the playback service
has some fallback logic that'll execute
so one day there was a traffic spike
which is interesting cuz it's not that
common Netflix our traffic patterns are
pretty predictable people watch somewhat
predictably if when you aggravate them
in this case the traffic spike saturated
the network interface cards on TV cache
clusters and so they became latent and
started timeout now the DB cache client
code in the gyro service tried to hit EB
cache it failed and it was configured to
treat that failure as a cache miss as if
the data wasn't there so playback the
playback service can handle the real
service failing but it can't handle a
little service saying out the data is
just not there and then we had an outage
so you might say well why is it
configured that way and it so that's the
default right they didn't set it that
way the default configuration for the
attached client is to treat errors as
misses but it turns out that that's
actually the right thing to do in most
cases because the client is going to
retry on its own and what would happen
previously that wasn't always the
default and people didn't handle that
error case properly be cached and people
doing too many timeouts because the
client was doing timeouts they were
getting error those were due timeouts
they would interpret their own arrows DB
cache errors and so the e V cache team
changed that behavior was right for most
people but not for this case
they didn't know so violence again has
this notion of structural secrecy that
means that the information that you need
to know to make a decision is somewhere
else in the organization that you don't
have access to it so Netflix is a really
really open organization and we're
co-located right all the engineers are
on the same campus and we still have
this problem right as we grow that
sometimes you're going to need
information to make a decision and you
won't have access to it because it's
some other part of the organization that
you didn't know about so to recap the
drip part of this dog so we're kind of
in trouble here because of the way
software is and because the way humans
are there because the way we act when a
resource-constrained because we can only
make local decisions that have non-local
impact and because of history that we
can't control
so even don't even know about those can
all lead to systems getting into unsafe
states that eventually lead to failures
all right
so what can we do about this so we can
do some things so Netflix we have this
service called chaos monkey and so what
chaos monkey does is it runs in the
background and it automatically
terminates servers to run in production
right so here is a picture of the chaos
monkey dashboard you can see we're doing
about six terminations a minute of
servers at Netflix you can also see that
somewhere around 240 there's this
terminate label and it stops for a while
and that's enough cats will keep killing
itself and then coming back so so you
can opt out if cats monkey if you want
to it's off its opt out of a chaos
monkeys opted in in it you know nothing
happens if gas monkey just doesn't kill
things for a while so the reason for
chaos monkey is that we want to make the
wrong thing harder right when Netflix
moved from data centers to the cloud
they were running on on servers that
were much less reliable right there that
are commodity grade and so the engineers
had to write their services to be able
to tolerate those failures but we can't
impose how people design stuff in
Netflix we have like free oven
responsibility teams are free to do what
they want so what we can do is make the
wrong thing harder to create more pain
for engineers if they write their
services to be stateful to not have
retries and things like that and it was
successful right and Casa mochi works
it's on it you know doesn't cause many
outages anymore because people are
writing resilient services
in a while there's a snowflake server
and it goes down and we don't write
snowflake servers anymore and so what
we're trying to do it Netflix has kind
of generalized this approach to
something that we're calling chaos
engineering and the idea and chaos
engineering is that because we can't
build models that describe how our
systems are going to behave what we can
do is we can take an empirical approach
and we can experiment on our services on
our system to figure out what it does
right in particular what we're doing
Netflix is we're doing chaos engineering
because we want to figure out where our
vulnerabilities are well the system has
gotten into an unsafe State and to
identify those vulnerabilities before
they become outages and then and then
route them out and we do these
experiments in production and we do them
because sort of we have to write because
we don't have a perfect representation
of how production is right there's we
have a test environment it doesn't
really look like production and we can't
fully replicate all the data and all the
configuration and the traffic patterns
and everything and so scientists call
this from the external validity problem
right when you run an experiment the
only way to know for sure that it
generalizes to a population is to do it
on that population and that's why we do
drug testing on people and not just on
on animals or on petri dishes right in
feature dishes and so the only way to
get the full external validity is to do
these kinds of experiments directly in
production and so one of the risks that
we're trying to mitigate is we want to
make sure that Netflix is not vulnerable
to a non-critical service going down
well if that was not critical but
actually ends up being critical so we
have a lot of services so this is sort
of like a screenshot of the Netflix
market service architecture this
screenshot of a tool called this role
that was also written at Netflix and so
most of our services are not critical
right so there's a service that
remembers where you were if you stopped
watching a movie the bookmark service
and then you can resume from the
bookmark if that goes down and user has
to you know start from the beginning
it's annoying but it's not a critical
thing so we have this internal tool
called chat which we is called the chaos
automation platform which we use to
design and execute these these kinds of
experiments at Netflix so I'm going to
take our you know Eevee cache example
again and show you how if we had done an
experiment in advance we might have been
able to catch this one so the question
is are we vulnerable to the ebee cache
clusters timing out
so what we do with chap is we take the
girl service and we clone it twice to
create two smaller clusters a control
cluster and an experiment cluster then
what we do is we route a fraction of the
production traffic that would normally
go to the ill service to these control
clusters and experiment clusters finally
we we inject latency and the calls from
the experiment cluster to EB cache to
replicate the failure problem and then
we look at the difference right so we
look at the difference in the behavior
of those two clusters control and
experiment and we look at the behavior
of the people that got assigned to the
control group in the experiment group to
see if there are differences and if
there are no differences then
everything's great and if there are then
we look in and see if there's something
wrong so we have these general
principles on how to design a run caste
experiments so you can go read about
them in principles of payoffs at org I'm
just going to walk through them pretty
briefly here and and give examples from
from chap that I just showed so one when
you do these caste experiments want to
build some sort of hypothesis about how
your system is gonna behave when when
you're injecting things into it
and so you should really have some
steady-state behavior of the system and
you want to see if that steady-state has
been changed in some way so a Netflix
tart one of our big biggest you know
business metrics is what we call stream
starts per second or SPS this is the
number of people who hit the play button
and successfully consume a video right
it's actually a pretty predictable
diurnal pattern for us and so when we do
an experiment we look at SPS for the
control and experiment groups and if the
experiment group drops that they can
stream as much video we know there's
some kind of problem when we failed a
non-critical service the second one is
two very real-world events you're gonna
inject something into your system as
part of your experiment right that's the
treatment that you're doing so with chap
we're typically either failing in RPC
calls so we're injecting a failure where
we're adding latency to a call as you
saw in the example earlier so typically
you know right near a timeout if we
delay it up to the timeout or after the
timeout what happens third and I mention
this earlier we try to run our
experiment in production as much as we
can so we do all our experiments in
production and what we do is we route
production traffic from the chaplet to
the trap clusters only a fraction of it
but it's real proud traffic we automate
right and the challenge is that our
system is frequently undergoing change
right constantly and so if you do an
experiment
it might not hold tomorrow the year old
service was a non critical service it
happens today to be critical because of
changes to it so we integrate with our
deployment pipelines right so we have a
spinnaker deployment pipeline system and
we can run our Chop experiments here so
they run on a regular basis like a cron
job or you can have them as part of your
deploys finally we try to minimize the
blast radius right it is dangerous to do
stuff in production so what we do is we
we only route a small fraction of
traffic to limit the total amount of
traffic that could be impacted and we
have this auto stop functionality so if
we detect early on in the experiment
that the users are experiencing pain
then we just stop the experiment so
takeaways from this top one is systems
behave like in really weird ways right
like they're pathological but you can
use Kasich's parents to read out some of
those pathologies the second one is that
these systems get into unsafe States
because human beings are making
reasonable decisions right it's not just
people are being like sloppy or aren't
are stupid right these are reasonable
people making reasonable decisions
that's leading to the system's getting
into unsafe states and and so if you do
pay us like gas monkey or automate your
experiments you can write and send us
people so that the drift doesn't happen
as much right so if they start if they
start loosening up on their standards
but then it you run an experiment and
boom they see the problem then hopefully
they'll catch it and they won't slide
back as much and the last take waving at
the end with here is if you get nothing
else in this talk you should really read
these these are great books you should
read so the one on the left is systems
Bible by John gall the middle one is
drifted to failure by Sidney Decker the
last one happens to be book written by
by my teammates and I and we're giving
that book away for free at the Kaos
ethicist at the Netflix booth on the
third floor it's not just good yes
there's other things at Netflix
over 1200 engineers and so we're giving
this away so if you want to come talk to
me just come right after the talk and
I'm happy to talk thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>