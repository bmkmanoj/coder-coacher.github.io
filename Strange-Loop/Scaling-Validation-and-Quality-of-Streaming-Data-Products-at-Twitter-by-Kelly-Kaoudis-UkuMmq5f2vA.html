<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Scaling Validation and Quality of Streaming Data Products at Twitter&quot; by Kelly Kaoudis | Coder Coacher - Coaching Coders</title><meta content="&quot;Scaling Validation and Quality of Streaming Data Products at Twitter&quot; by Kelly Kaoudis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Scaling Validation and Quality of Streaming Data Products at Twitter&quot; by Kelly Kaoudis</b></h2><h5 class="post__date">2017-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UkuMmq5f2vA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Kelly and I'm a software engineer at
Twitter if you saw Lisa's talk that was
that was the architecture talk this is
the Twitter ops talk I guess today I'll
be talking about what I've learned about
debugging distributed systems and ends
with the quality engineering team in
data products formerly good nip at
Twitter when I first came to Twitter my
manager was one of the og unep engineers
who later transitioned the management
side and from him I learned about where
we are now as the data access pipeline
part of Twitter but also where we came
from is connect so asking if we wanted
to be Grand Central Station for social
data automatic WordPress discuss Twitter
you name it we adjusted events from it
converted them into a common easy to
purchase on format and stream them out
again to our customers Caniff was known
for the quality of our data we do our
best to carry that quality and attention
to detail forward eventually twitter saw
our value as a source and bought us but
some of the things unique to the Knik
culture and operational mindset lived on
especially in how we think about and
test our systems we don't just rely on
engineers rating feature and unit tests
and s-series keeping our infrastructure
going but we also have a team of testing
and ops folks that we call quality
engineering
q exists in its current form today
because we're one giant distributed
system bridging twitter and our data
customers I like knowing how things work
but also how they came to be the way
they are no system or engineering
culture is an island in time testing our
system and and started one gannett pad
just a few engineers and it was just
engineers running tests along with their
code and doing QA and it was pretty
bog-standard but we just had a couple of
apps then as we grew from getting up
into offering a wider variety of
products we added micro services when we
needed them and soon no one person or
team and the organization had the whole
picture of everything at a detailed
level in their head like we're not
Netflix but it still it still happens so
future teams could write play your tests
for their own services and interactions
with their immediate
but we didn't have the consistency
between products that we wanted and in
some cases we couldn't figure out wire
system produce certain outputs our
quality bar just wasn't where we wanted
it to be so the quality engineering team
if data products is Grand Central for
Twitter data makes the trains are on
time today I'm going to give you a
motivating example of one of our
products that QE tests talked about our
approach to testing the whole system
holistically covers some of the
challenges we faced and trying to come
up with testing strategies that are
tailored appropriately for the work that
data products does and the systems we
have and I'll cover some of the tools
we've developed in the process of trying
to write and enable feature teams to
write the right tests despite these
challenges to start with all define what
QA actually does so QE writes tests that
behave adversarially to determine how
our systems actually work and ends then
we convert them into guardrails that
confirm the system continues to keep
working in order to not disrupt the real
streaming customer experience and
production we have our feature teams
deploy everything to an environment with
just fake clients that we control and
then we run a suite of stress and
operational tests before good coast code
goes to production and that's that's our
staging environment we also have a
separate experimentation environment and
that's not our staging environment this
is basically like a mirror of production
anything deployed to staging is done so
with the expectation that it will end up
in prod unless there's something awful
we handle real Twitter data in real time
in our staging environment just like we
do in prod but because many of our
products are streaming instead of
request response and are not accessed
through a browser we have a set of
operating constraints that are
relatively unique for part of Twitter
our clients just kind of sit there and
consume data they're fairly passive
they're people making financial
decisions and doing you know writing
academic papers so we just want to make
sure they get all the tweets and all the
tweets have the right parts we also run
some of our fake clients against
production data products so we can
compare what we're seeing in those fake
clients and staging and production
during the Soka period before deploys
but we have the staging environment in
particular just for
modeling our system and and as it would
be in product or the deploy and seeing
what falls out the quality engineering
team handles the deploy and also live
interaction testing for the whole system
of multiple data products because of the
soak period and the comparisons we do
rattling the system and and and putting
new things in production are pretty
tightly coupled for us we use the data
from our fake clients to determine the
city just the steady state of the system
so does anybody remember the dine DNS
adage from November of last year okay
yeah well it was fairly big for us and
it took a pretty big chunk out of our
expected down time for the whole year we
operate in a mainly streaming micro
service based environment the QE team is
in general most interested in trying to
find causes for your classic distributed
systems problems like lag or consistency
of output of the whole system and
looking for unforeseen results from
interactions between parts of the system
that are supposed to be cooperating so
the Dino tidge we definitely couldn't
predict but you know if you're just
shipping a new feature to production
there's potentially going to be
something wrong so we have the staging
mirroring prod set up with a soak period
because it allows us to see what will
happen and pride before small
interaction bugs in particular get there
so tiny cuts if you have enough of them
still kind of compound is a pretty big
injury our customers are making
decisions that impact the way they do
business and writing academic papers and
all sorts of stuff with our data so the
kind of testing we do for data products
is at the application layer and
primarily from a customer's point of
view and then testing takes place in
both a gray area between services and
outside the system boundary in our fit
clients so it makes sense that isn't
usually the first thing at a lot of
people's minds feature teams kind of
tend to rely on it just working in the
background unless we find an issue
but the number of bugs that we've found
through this level of attention to
detail have made it worthwhile end to
end testing is not a replacement for
your garden-variety feature unit testing
and feature testing your engineer will
muck out their edge upstream and
stream dependencies in order to just
exercise the code they've written the
rule for us is a feature test doesn't go
off one box but this is this is live and
and system testing on the other hands
nothing is mocked all dependencies are
up and at them you know databases
networks streaming connections the works
because we're testing for a live
interaction issues in the whole actor
texture as a single entity
well bugs caught by other forms of
testing could definitely have customer
impact also our specific target with
this kind of testing is false and
interactions between disparate parts of
the system and fidelity between
different sources of the same data
finally Twitter does have a whole
organization devoted to making sure
developers have solid tooling like we
have somewhere between 1,500 and 2,000
engineers I've really lost count so
twenty or so of them can just work on CI
and work on our build tool pants but qyz
not that what Q is is different our work
is tailored just for data products the
Tauri observability team gives us tools
to explore the state of the system once
we do have data but a lot of that data
in terms of the services owned by data
products is produced through the efforts
of the QE team an example of products we
validate on a continuous basis its power
track I like to briefly describe power
track so that you can more easily
visualize what I'm talking about when I
say that QE is looking for interactions
between systems in the case of this
particular product the fire hose is what
we call the stream of all real-time
Twitter data there's so much of it per
second that to consume the whole thing
as a customer you have to make twenty
non redundant connections to us to get
all the tweets power track and just that
fire hose applies your custom rule set
to it to filter out just the tweets you
might not you know you might not be as
interested in and serves you just the
good stuff all in real time with as
little delay as we can manage well we're
getting ready to launch our most recent
version of pirate rack we had two
problems to solve first we needed to be
faithful to our other sources of Twitter
data especially the old version of power
track in the fire hose we needed to make
sure the new version wasn't
again compared to the old version
especially if we were going to ask
customers to switch soon and that if we
saw a tweet in the old version we also
saw it in the new version with all the
same components with this update to
power track we wanted to allow our
customers to filter with more finely
grained rules additional operators and
maybe even more reliability so this
isn't like you know your everyday push
to production this was kind of a big
change for us
now blending for an average level of
tweets per second isn't that hard
especially when we have decent city
state metrics but planning for meeting
our high-water mark on the other hand
can be a little bit more difficult so
that's our second problem most systems a
power track stream or a single firehose
partition is a pretty good example
should be able to handle a certain
number of tweets per second comfortably
as input without any human intervention
and about double that number should be
handled without paging on Colors even if
our observability setup starts emailing
us and saying something is ugly we'd
rather be proactive than reactive
especially when it comes to making our
customers happy so what can we do to
meet those needs and certify a power
track for release so a book that's been
sitting on my shelf for a while and kind
of getting dusty but I picked up again
recently it's Puli as hell to solve it
it was written for kids in math classes
I'm gonna be honest but taking the time
to think critically and analytically
about your problems is pretty generally
applicable bully aJE's a lot of good
analytical tools for problem solving but
his main idea is is just easier to not
try to tackle the whole problem head-on
there are four steps to this approach
and I find sometimes the best thing to
do is just to go back to first
principles I find it helpful the
structure my thinking about how to go
about finding solutions in this way the
first step is understanding if an
engineering manager comes to us and says
hey I'd love to have a test for this and
for this and for this we need s some
clarifying questions we need to look at
our existing metrics about the state of
the system we need to go talk to people
who are working on the product and
figure out what they think if we can't
restate the problem in our own words or
we don't know what we're being
to find her show it's somewhat difficult
or really difficult actually to write
tests so we've gathered some information
but regardless of how well we understand
the problem we should still check back
with whoever gave it to us to make sure
that what our expectations are aligned
with theirs the second step once we've
got once we've got a problem is to
figure out an approach once we know what
we're looking for we could start with a
list of potential approaches and try the
best one we could reduce our problem to
just the essentials or maybe there's
some small chunk that needs to be solved
first before the rest can be solved like
you know if we're testing something
maybe we need to get access to a
database maybe we you know whatever just
something small that you can pull off
first thirdly we'll try that out and
then finally we'll figure out if it
worked to our satisfaction for the new
version of power track making the new
rules API which allows folks to add
custom rules to their power track
streams behave more consistently and be
more reliable was some problem of making
all a power track reliable enough to
meet our essays making sure our new
operators work correctly was a sub
problem of making the new rules API
behave more consistently so the rules
API is pretty simple it allows you to
upload rules in JSON format up to a size
limit when you've successfully added a
rule it should give you back a 201 and
tell you which role was created if a
rule already is actively filtering the
stream you should get back at 200 a
message telling you so if the rule set
you were trying to upload couldn't be
added maybe it was too long we reject it
and tell you why the fourth step in
pellÃ©as approach is to reflect on what
worked and what didn't and try and fill
in any gaps or holes in our solution if
we didn't get the end result we wanted
maybe another one of our potential
approaches could work maybe we need to
take a step back and find another small
piece of the problem to solve before we
can get to what we originally thought
would be the first thing and this is
when we decide that so one of the things
we were working on adding with this
power track update was the ability to
filter on more of unicode instead of
just words and phrases in the various
languages that Twitter supports plus
hashtags we wanted customers to also be
able to filter with emoji code points
and
ticular one of these new operators the
soccer ball emoji was not behaving as
expected which is I guess just what you
get with a distributed system sometimes
they do things and you can't explain it
we had system tests that added a couple
simple rules and check to see that what
we got out of the system made sense but
we wanted everything to at least work
for the day of release comparing data
from powertrack streams with the same
simple rules served from different
Twitter data centers was a limited
version of the problem of making any
power track stream consistent with any
other power track stream with the same
filtering rules we helped the teams
working on power track write tests that
compared soccer ball moji tweets served
by power track from our various data
centers in our staging environment to
make sure that we were seeing the right
tweets when we expected them coming out
of this deep dive into the power track
products and the systems that make it up
I started thinking about the way I was
approaching testing data products as a
system and how it had been really
helpful to just be able to concentrate
on aspects of power check for a while as
at this point I was fairly new to
Twitter and to the idea of system
testing in general I thought that the
best thing I could try to do on my own
aside from whatever the day-to-day was
was to learn as much of our entire
architecture in detail which right now I
think is maybe 60 65 microservices as
possible so key we started experimenting
with embedding with feature teams more
often we wanted to be visible every day
and on people's minds so that we could
get a better sense for what issues
people had that we could take off their
plates another benefit that we hoped for
was the ability to pinpoint problems in
that team systems in areas that made
more sense so we weren't just beating on
services that were already sturdy that
we're kind of the obvious places to look
and ignoring things that might have been
more weak so these are all good things
but we didn't need to know absolutely
everything about the system in order to
just support it exploring and
experimenting and gathering information
are all important for building intuition
about the system both in terms of
traditional QA and in terms of red
teaming but it was impossible for a
handful of people and
not just one person to have deep
knowledge of our whole architecture at
the same time you lose context if you're
in an immediate contact with a service
or a subsystem for a bit the tests we
were able to write at this point without
enough context were often duplicating
more localized tests the future team
engineers had already written
we were also leaving a piece of the
system out of our model of data products
and that was the other engineers so we
had a pretty good set of experiments
going but trying to be and and testing
as a service was bottlenecking the other
engineers at getting features into
production
qe rating and maintaining all the system
tests and system testing infrastructure
controlled well for feature teams
possible blind spots with regard to
their own code and things they might
think were obvious but needing to
understand enough of our system in order
to file tickets for our bug reports that
would be accepted and acted upon was a
drain on the amount of time we had
available to write the right tests
sometimes with our limited system
knowledge we would think we found a bug
but it was really expected behavior the
system that just wasn't documented or
that kind of customer input would never
be presented to the API because there
were five other systems between it in
the edge sometimes we didn't have enough
facts in our quiver about the expected
behavior of the system and all its
individual components to talk to the
right person or team first a
relationship with feature teams needed
to be more collaborative the way we're
approaching system testing simply wasn't
optimal for the organization itself
now Conway's law is that the systems
that are created by any organization or
reflective of how the people in that
organization communicate with each other
system has to be considered in the
context of the people who work on it
every day and that's what this guy is
saying - it's really if you're gonna
think about testing something you have
to think about the people who wrote it
as well and if you haven't looked up
this paper totally do it's all about
like process and formal verification and
it's some of his examples are like
refrigerators but it's really cool so if
you'd like to talk before mine and you
like this you'll probably like that so
when another team made time for us like
with our work with power track before
our latest big update to it was to be
released we had success in drilling down
into that teams portion of the system
and its dependencies and helping them
with their interaction testing but we
were still having difficulty keeping up
across the org so it was time to
reconsider again so Sarah my is kind of
the goddess of feature testing her blog
post five factor testing is also
something that I like to revisit from
time to time when I am not sure what I
start with tests for a new feature
product so I'm gonna paraphrase a bit
good tests should tell other people how
to use the system and what they can
expect from it even in faulty states if
we just reinvent tests that already
exist
we aren't providing any new
documentation on the system and it's
difficult to prevent future editions
from moving away from the functionality
that our customers are paying for
without tests proving the entire system
works and tests proving the boundaries
of the specification good tests do not
confirm information that we already know
falling into the confirmation trap again
doesn't help us challenge the beliefs of
our organization if something's wrong
with the system to motivate change a
good end to end test should do two
things fail with the existing state of
the system if it's behaving badly and
then once the system is fixed continue
to act as a should always pass guardrail
until we reevaluate that test and decide
if it's needed or not otherwise we don't
really need to add that test to our
suite so once a quarter Twitter
engineers get a week to rate code to
just scratch any itches they might have
well we were kind of in a holding
pattern trying to keep up with
everything everyone else was doing
rating all the system tests and doing
manual testing a hack we sort of snuck
up on us part of our problem was our
tools for the tests we were trying to
write weren't the greatest they weren't
really written for anyone to use so here
was a nice sub problem really taking
some of the load of end-to-end testing
off of everybody by iterating more
complex tools into something simpler and
easier to apply that other people might
be able to use a couple of my teammates
worked on writing a new cleaner version
of some of our
testing tools that hack week so the idea
was with easier to use tools we could
get feedback on whether we were actually
finding bugs sooner and with enough
reasonable tests as examples we can even
take the time to teach teams that
already had two min expertise what to
look for in test results when their
systems weren't interacting correctly
with one another if we could teach
feature teams what to look for we could
write some simple client code for your
tools then teach them to write system
tests themselves and then fit those
tests together into an end-to-end suite
then we really only need to step in
ourselves to provide advice on how to
use our client code or to write and
interpret tests in more complicated
crossing cases when we had time to dig
in and explore but we were running into
another problem now our own expectations
about the quality engineering team and
where we fit as people in New York we're
different from those everyone else we
eventually went over our managers but
now we had to harden and scale our new
tools gradually phase out the old tools
and get the rest of data products on
board we did a lot of pair programming
writing tests using our library and
explained why we thought it would be
easier for everyone involved if we
didn't try to duplicate the expertise
and understanding I am about thirty-five
people with four so part of our initial
problem was the overhead of our tools
with no product manager it's easy to
fall into a habit of building things
just for our own team and automating
things whenever we feel like it but as a
platform and tools team that's not our
job and we're not our own customers we
needed to catch up with the amount of
testing we should have been doing in
order to best handle testing requests
and I am particularly wanted to see if
engineers following our process and
waiting for us to catch up with our
testing were happy and they weren't
there were some grumbles people were
fairly patient with us because they knew
we had a big job but we needed to get
smarter about our approach when Gannett
was much smaller and QE was just one
person writing tests for a few other
engineers at a start-up doing everything
ourselves and building things with just
our team in mind worked pretty well but
with you know
40 odd engineers adding new things every
day there is a lot more overhead so when
we deploy what eventually goes to prod
is what's running in our staging
environment which is Mira prod with fake
clients controlled by QE instead of our
real streaming customers one direct
benefit of feature teams writing their
own system tests is that they're at
least somewhat more likely to pay
attention when those tests fail so
issues have started to get fixed faster
a team whose service causes a test
failure was generally not able to get
their code in production until they fix
that code or fix the test that failed
since we treat system tests like a
blocking deploy step for each see each
service that data products owns if folks
want their new features in prod fixing
interaction and fidelity bugs has to
happen first so at least from the
perspective of QE and me this is a
better method however this is still not
a perfect system so it's time to take a
step back again and look for gaps in our
coverage did we have the right tests
where are we to focus on a couple of
products to the exclusion of others what
were we missing taking it back to the
early days again when we were gonna plea
had full control over our systems it was
easy to inject enough fake tweets at the
top of our staging pipeline to hit our
high-water mark and measure the results
both on the intermediate system and on
our fake clients as part of Twitter
though our apps are connected with our
in-house pub/sub system event bus and we
monitor our systems with tools
maintained by the observability team we
don't control our entire environment
anymore and had been missing a way to
load up the whole system at the same
time for a while but we'd been hesitant
to develop in this area since we'd also
be stressing out the team that builds
and maintains our pub/sub infrastructure
in particular so up there on the left
I've got Ellen's tweet from the Oscars
in 2014 and it was retweeted a bajillion
times and favorited and whatever but
that event isn't really like the other
two at all so we had two things on our
calendar New Year's Eve and the
migration of data products from the
general
I'll Twitter messaging cluster to her
own dedicated cluster data products
engineers especially the streaming team
wanted to know if we could help them
plan capacity for high load events like
New Year's and it was hacking again so I
paired up with somebody else to explore
what tools we might already have for
stress testing capacity planning so
sorter does have a back-end tool called
Iago for a form of stress testing but it
runs in pride which would mean serving
our people that are connected 24/7 as
customers fake tweets so that wouldn't
really work for us fake tweets served by
that tool are of course stopped before
they get near the UI but as data
products we have different priorities
than some other teams on Twitter and you
know people that don't do streaming so
data customers have somewhat different
expectations of our API than people
tweeting have from the Twitter you eyes
for example one pretty significant
drawback of both our stress testing and
the old GIMP environment and this other
tool Iago is that they just reply or
accorded data when testers use fake data
for stress testing they're
unintentionally imposing further
constraints on the system and are on
their model of the actual system and can
then based on the test results
reinforced part of the system that might
not need to bear the same kind of load
under either ordinary or above-average
traffic in production so in the middle
is a hurricane and on the right is a
football game recruiting a known
stressful event such as the Oscars where
Ellen took this picture might be a good
start for generating a fake data set but
the patterns that we're gonna see in
such a data set are not really
applicable to people doing disaster
recovery during Hurricane Marya or to
football games or whatever else goes on
that causes load spikes for us requests
per second load testing can definitely
inform a picture of the client
experience but it's less of a primary
concern since we're streaming well we
were interested in was more along the
lines of loading ourselves up with input
in terms of tweets per second as we're a
leaf node of Twitter and our customers
just hang out and consume data so
instead of taking the fake data approach
we decided to leverage what we already
had and in progress migration to our
very own
sub cluster and the abilities subscribe
to the stream of all tweets and sweet
related events as they're created in a
real-time just as if we were the initial
in gist app at the top of data product
streaming architecture so we worked with
the streaming team as well as the pub
sub team to make sure that we could
inject fluid into the topic the app at
the top of our architecture reads from
safely we weren't using fake data but we
had direct access to all events
occurring in real time so we built a
lightweight tool that allows anybody who
wants to pick a topic to subscribe to a
topic to inject to and a desired number
of events per second to inject we made
two design decisions with our load tool
like our original replayed load
generation tool the new tool has a
number of workers that you can use based
on how much you want your load to be
multiplied it but - unlike our old tool
each worker can be set to duplicate each
event that's ingested from our pub/sub
architecture a set a random number of
times so in this way we can bring
production load to our staging
environment without getting trapped by
patterns in a fake data set so since we
can randomize and pick the number of
events that we want to see every single
time the size and the type of the events
produced are different with each test so
engineers build out a simple load
profile like I described and you know
with with just three things really
you've got the topic that you're reading
from the topic that you're injecting to
and then what you want to see and then
they can run it in our staging
environment as long as they've notified
anybody who might be affected and enough
worker nodes are free so this also ties
back into our testing building blocks
idea we run one big load test on the
entire system after all the system tests
pass during our pre deploy run but
engineers can also compose their own
individual load tests for their own
subsystems with any other kind of system
testing that they would like so now we
had something approaching a fairly
decent tool chest for our architecture
what other kinds of
things could we take a different
approach with and improve so system
measurements and requests from
leadership and whatnot are great tools
for informing what we do next as an ops
team but without soliciting feedback
from people using our tools and fixing
bugs based on the information we provide
them there's no way to know if our
method it's working for anyone else so
we started on this track of reinventing
our tools and just trying to leverage
all possible resources we had because it
was taking us too long to test in
particular the new pirate rack version
and we were so focused on keeping up
that we weren't collaborating well with
the rest of everybody and we were you
know providing as good of tests or
helpful results as we possibly could
have if nobody is getting good
information from the tester rating or
from what you're doing if you're testing
even if that's your job description you
shouldn't be writing those tests we were
catching some bugs but we weren't
working at pace with the rest of the or
Gurley and we weren't preventing all the
faults that we could have so it's
important to take time to consider your
solution every so often change up your
approach if you need to and iterate to
stamp out any edge cases you might not
have considered it's also important to
consider what kind of organizational
culture that we want to work in do we
want to move as fast as possible or do
we want to provide stability for our
customers or do we want to balance these
needs we've shifted to a less high-touch
model of testing an deploy 4qe is we've
evolved as a team and the apps we test
have grown and changed but we still have
to balance the needs and desires of
engineers with the contractual
obligations we have to people who pay
for us to build things giving engineers
the ability to rate their own system
tests and run load tests when they want
to give an advance notice has started to
allow us to put less effort into every
system test written but the agreement we
have contractually with some of our
customers you know you will get this
many tweets at this time if they exist
should still be not ignored like we
still have to deliver a certain level of
quality and pay attention to detail but
maintaining a quality products should
be an excuse for not improving human
processes the system and the
organization grow and change we do want
to make sure that any new approach we
try will actually meet the needs of our
customers
but we also want to make sure we're
creating precedent for a place where
people get to work on new things and see
them running faster in production and
where people can have a non-call
experience where they're not getting
paged repeatedly every night at 3m
because no one likes that if you do talk
to me please
especially with testing it's important
to experiment and to consider what could
work best for your own org your team of
engineers and how they think about
testing and interact with each other as
well as your orgs current stage of
growth not every type or style of
testing or of deploy works for every
situation we've had a quality
engineering team since we were gonna pee
because it's continue to work for who we
are and the way our systems are but this
is just one way of doing stuff there are
many possible ways that you couldn't
catch distributed interaction problems
and I haven't thought of them all but
they're out there I'm sure we just want
to make sure that we're doing our best
to give people a reason to continue to
work with us both as engineers and its
customers so for now the quality
engineering team is providing and
maintaining system tests and deployed
tools and process but that may not
always be the case if the org changes
again system testing and data products
that Twitter is evolving process and I
can't wait to see where it goes next</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>