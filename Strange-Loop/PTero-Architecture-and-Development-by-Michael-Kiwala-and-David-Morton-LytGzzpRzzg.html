<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;PTero: Architecture and Development&quot; by Michael Kiwala and David Morton | Coder Coacher - Coaching Coders</title><meta content="&quot;PTero: Architecture and Development&quot; by Michael Kiwala and David Morton - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;PTero: Architecture and Development&quot; by Michael Kiwala and David Morton</b></h2><h5 class="post__date">2015-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LytGzzpRzzg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is David Morton I'm here
representing the mcdonald genome
Institute we're here in town we do
genomic research we're part of the
Washington University conglomeration
Michael koala is also going to help me
present we do a lot of research into the
genomics of cancer and Alzheimer's and
and non human genomes as well and so we
we obviously face a lot of computational
challenges both in like how do you do
that but also how to orchestrate the
processes that do that and so there's a
couple of levels of difficulty that we
have one of the systems that we're
building to solve the orchestration
process is this program the system
called Tarot and so today we wanted to
give you a sort of an overview of what
Tarot is and I wanted to kind of start
with the history of Tarot so what you're
in for what we think that we can do in
the time that we have is talk about the
history and background for about 15
minutes start you off talking about the
modeling system that we use to analyze
genomes and then how we've sort of
pulled from that the workflow engine and
then we're progressively making that
more and more service-oriented michael
is going to talk to you about the
architecture of Tarot and he'll talk
about the services what they do how they
interact with one another he'll give you
a realistic example to to sort of show
some of the nice features that we have
lastly we're going to talk about the
development and testing as well as our
deployment really quickly if we have
time i'll show you a really fast little
live demo the whole system runs in a
small vm the ulterior motives for giving
this talk are that maybe somebody in the
audience will find something also useful
about taro and want to contribute or
maybe point out something that we missed
and so
without further ado let's do some
history so a little bit of background
about what we do at MGI we do a lot of
human I'm going to move the cursor we do
a lot of human genetics and so that
typically starts with some people in a
lab sample preparing a sample for
sequencing and sequencing is now a
multi-billion dollar industry and it's
all sort of geared towards these giant
sort of computer boxes that have all the
wet lab in them as well and they spit
out essentially short reads of DNA
usually about a hundred base pairs long
and those then need to be aligned back
to the human reference so you might
remember back in two thousand three or
so we did this thing called the human
genome project and that was to just like
figure out what is the letters of a's
and CS and T's and G's that make up the
entire human genome and so that isn't
everybody's genome that's like four
people's genome roughly but it gives you
a scaffold to put all of these short
reads onto and so there's a handful of
tools that can do this for you it's
active area of research to do that kind
of thing we don't do that a lot adamjee
I but we orchestrate a lot of that work
and so we do or sort of we do production
scale genomic sequencing so we do all
the preparation all the way down to
annotation but we don't make all the
tools the next step in the sort of the
pipeline would be variant detection so
once you've aligned all of those short
reads back to the human reference you'll
find that the person that you just did
the sequencing on is different from the
human reference and so we catalog all
those and we try to detect them the
difficulty in those sort of operations
is that every step along this way is
error prone and so if alignment were
perfect variant detection would be
simple but alignments not even close to
perfect and so every step is really
complicated and there's competing tools
at every step and so one of the
challenges we face is how
we build pipelines to do this at scale
and allow the researchers also to swap
out tools and evaluate new tools easily
and so the final process that we tend to
do is call the annotation where there
are people that I have databases of sort
of how does the DNA relate back to
proteins and drugs and different things
and how how do we give that back to
researchers so that they can make
breakthroughs in curing Alzheimer's for
instance so another backdrop to this
whole sort of conversation is just how
quickly the cost to sequence a genome
has dropped so back when they did the
Human Genome Project it was roughly a
hundred million dollars to sequences
first for genomes after that it just got
cheaper and cheaper and cheaper and you
can see in this slide Moore's Law is
just a straight line because this is a
semi-log plot so each time that you drop
down on this axis is a factor of 10
another factor of 10 and so on and so as
genetic sequencing got cheaper and
cheaper we just kept doing more and more
of it and what you run into is this
orchestration problem that things that
worked well for orchestrating the
analysis of one genome now you've got a
thousand genomes and now you wanted to
cross comparisons and things like that
so back in about two thousand six we
started building this genome modeling
system and roughly what the landscape
looked like then was a lot of the file
based programs so the community had
built up programs to do an either
alignment or variant detection and
different annotations but you know their
long-running non-interactive programs
mostly third-party to us MGI and they
don't really stack together well like
you can write bash scripts for them or
you can you know sort of weave them
together but because they all have
different interfaces they don't really
work well and so you end up with this
sort of building block tower that's
really fragile you just touch it and the
whole thing falls apart and so we
started to sort of want to build
something more like Legos so we wrapped
all of these third-party processes and
pearl not our favorite choice but
is sort of the community standard in
bioinformatics we exposed all of those
command line program are all those
programs back to the command line with a
uniform interface and started to make
automated pipelines so that we could
scale up as genetic sequencing costs
came down and so as sequencing costs
came down we started doing more of it I
wanted to kind of show this this is data
from our sequencing center in the inset
that shows from 2007 it might be hard to
see up there from 2007 to 2009 that it
just looks like a hockey stick it's just
we're doing more of it it was not
uncommon for us to have every quarter we
were doing more sequencing that quarter
than had ever been done in the history
of humankind like just at that Center
and so it's an amazing sort of
atmosphere to be in when things are
growing so fast and so the genome
modeling system worked great for
building the first handful of pipelines
but we wanted to make it easier to build
pipelines and to allow researchers to
sort of more quickly change the
processes that they want to be in those
pipelines and so we pulled from the
genome modeling system monolithic
project out what I call legacy workflow
now it didn't used to be called legacy
workflow it was just called workflow but
now its legacy workflow and it was nice
because it had a declarative way to
describe your workflow instead of it all
being in code and lost in all those
interactions you had one document that
said this is the processes that will be
executed and they could have
dependencies on one another it
integrated with our data center
management platform so we have our own
data center which might make some of you
guys jealous and we use IBM's lsf to
manage that data center so what we can
say is you know I have a job and it's
going to take two weeks and it's going
to use 32 gigs of ram go schedule it
somewhere in our data center and lsf
manages that for us and so legacy
workflow interacted with that for us it
introduced this concept of parallel by
so a lot of what we were doing was
scaling up an orchestration rather than
like the genomes weren't themselves
getting any bigger and so what we had as
a concept is sort of like mapreduce but
without the produce so we would say run
this operation but do do it for every
chromosome or do it for every variant
for instance and I have centralized
tracking of all the workflows that were
being run at the center and so that was
really nice to give managers and people
who are you know in charge of projects
and whatnot so fast forward another
couple of years and we're doing even
more sequencing than ever before for
reference I've put a green box over the
previous inset that I had and so you can
see that like we just keep dwarfing
ourselves every couple of years and so
legacy workflow was working for those
years it helped us to move forward but
you know over those years the features
sorta started to fade into the
background and we started to hit some
limitations the legacy workflow couldn't
scale up so we started to have cohorts
of tens of thousands of people and we
wanted to do cross comparisons it just
couldn't scale up it would just choke
did we started to notice that it was
unfortunate that it was really highly
coupled to the perl programming language
and it had a bunch of hooks back into
the genome modeling system itself and so
other people couldn't easily adopt it
and finally it represented all the
workflows just as dags which is totally
common with in workflow engines if
you've looked into them but often what
we want to do is not just a directed
acyclic graph and so i'll give you a
sort of a preview of what we're what
we're moving towards and so directed
acyclic graphs they're great because
they can encode the dependencies between
your various processes they're great
because they can split out and do
concurrency really well but they can't
do choice not at runtime anyway and so
you have to choose everything you want
to do before you encode it into the
tag and we didn't want to do that we
wanted to make choices at runtime most
common way to do that is to use a finite
state machine they're really good at
doing choices at runtime but they can't
do concurrency and so that's not a great
choice and often what you end up with if
you adopt somebody else's workflow
engine that only does dags is that you
have a dag of fsms and so then you've
sort of mixed your abstractions and so
what we decided to do is in code at the
very lowest level all of our workflows
and in a petri net and I assume that
most people haven't heard a petri nets
we had when we sort of dug into this but
they've been really helpful for us
they're called place transition that's
but they're named after the guy who
invented them I can't remember his first
name but his last name's Petrie and they
can do both concern concurrency and
choice they can actually do more than
tags and FS m's but they they cover
those use cases really well i'll give
you really the basics of those just as a
teaser or whatever and then we'll get
back to the what we built the taro
workflow service so in Petri Nets you
basically have places and transitions
those are the only two concepts other
than directed arcs and you can only
connect a place to a transition and a
transition to a place so it's a
bipartite graph representation it's they
evolved very simply all the only rule is
that a transition will fire if and only
if all of its input places have a token
and after they fire they place a token
in all of their outputs and for that
reason there's actually a pretty large
mathematical background or like super
foundation to these people have analyzed
their behaviors and they're well
understood we we use them to do both
choice and concurrency this is sort of
an example of how you can do that so on
the left you can see this is an example
of currency so you make a transition
so the when you start with just a token
at the top now you've got two tokens or
in tokens this example is just too and
then you use those transitions that are
called launch process here to actually
have a side effect in our case it's
sending a web hook somewhere else to
launch a process and then so then
there'll be two tokens down here in the
launch state but there won't be anything
in the finished state and so that we
have another way to in our service to
say for those uh for those launch
process it processes to say that they're
done and then when they do say they're
done when they all say they're done then
there's this join transition that allows
you to continue on and you're in your
dag or in your and your workflow you can
also do choice choice sort of follows a
very similar power pattern where you
have a transition that just starts you
have a transition this is just launch
process it has a side effect and
launching a process in some other
service and then there's just two places
and so there's that choice that that
service gets to make did your process
succeed or did it fail and then what
happens is once you place a token in one
of those two places it consumes all the
tokens from its input place and it puts
them in its output place and so you
don't ever get the the behavior of
cleanup and continued both happening for
instance so what we did with Petry is we
made a highly optimized implementation
of ptree and we sort of think of it as
assembly code like nobody really wants
to work in Petri because it's a pretty
far from what you want to do there's a
bunch of boilerplate essentially and so
we made a flow on top of ptree it uses
Petri as the back end but it's still
encoded your workflows as Daggs because
that's how people think of workflows but
it allowed your DAGs gags of fsms to be
underneath sort of I'm stuttering now I
apologize
to be driven by a more mathematically
robust back in okay so flow alright so
flow matter needs of scaling up really
nicely because we made it service
oriented we had we had solved our issues
of being able to scale up in both the
number of workflows we could run at once
which we usually run thousands at a time
and we were able to run workflows that
were tens of thousands of nodes large it
was really reliable partly because of
its service orientation if any of the
services went down there were redundant
services that can pick it up it had some
limitations though because it tied back
in with our legacy system we made some
decisions in order to do that in a
timely manner that basically made it not
usable by the by you guys and so it
stores all of its status and our legacy
database it represents everything in the
old XML format which people find
cumbersome and it has a lot of really
complicated and heterogeneous api's so
this sort of shows you where flow came
in around 2012 we were developing it
it's been in used for the last few years
it's really scaled up nicely for us you
can see that we've been doing even
Moore's sequencing it's it looks like it
kind of leveled off but then this last
year new processes have been invented
and we're back you know vastly outpacing
Moore's law and so flow has been really
good for us and it's sort of I think of
it as taro 0.1 or something taro is
built on top of the same technologies
that we use in flow so what do we want
to do with taro we basically want the
same ability to scale up the flow has
but we wanted to be service-oriented
with restful api so that anybody can use
it and it's not tied to pearl and it's
not tied to any programming languages
we're going to use json for all of our
workflow descriptions as well as all of
our response bodies and
we went to have a pluggable job
execution system so that we're not tied
to just running things on a command line
or just running things in our lsf
cluster we could use Sun grid engine or
you could use AWS and we want outside
adoption so that you know the
maintenance burden moving forward isn't
tied to us and we can we can squash bugs
and everybody can use it and so Michaels
going to tell you basically what our
architecture looks like and then at the
end if there's time I'll try and run a
live demo so Michael okay thanks okay uh
so Dave just got done telling you about
the history and how we got here so I'm
going to pick it up and tell you a
little bit about the architecture of
taro taro is comprised of three types of
services we have the Petrie service
which is responsible just for executing
the petri nets we have the workflow
service which executes workflows it's
what users of taro primarily interact
with is the workflow service they submit
workflows there the workflow service
then instigates interactions with the
other services in the taro system and
the workflow service also manages the
inputs and outputs of the tasks in the
workflow then the third kind of service
is the job service right now we have two
varieties of the job service the first
kind is the shell command job service
and that kind of job service allows you
to execute a command on some execution
host the lsf job service dispatches
commands to our data center using ibm's
platform lsf the services also have a
few things in common they are all
written in Python exposed rest ap is the
requests accept and respond with JSON
documents if you post a JSON document to
one of the terrorists services then it
validates that JSON using json schema
and all of the services have web hooks
that users can subscribe to us so that
users of the services can listen for
events
okay before we move on I'm going to
cover some terminology that we use when
talking about tarot task is something
that you'd like to complete in your
workflow think of a task as a goal not
how you would achieve that goal but just
as a goal tasks may have inputs and
tasks may have outputs the inputs and
outputs for a tarot task are small JSON
documents usually they'd have something
in there like a path to a file or some
sort of web resource that needs to be
used or created during the execution of
a task a workflow is thought of as a dag
of tasks and the inputs required to
execute that dag of tasks now methods if
a if a task is a goal but how to not how
to complete the goal a method is how you
complete the the goal of a task it's a
way to finish a task we have two
different kinds of methods in Tarot we
have the job method and the dag method
when you execute a job method that
involves one of the jobs services so
that might be running a shell command or
dispatching work to the LSF cluster then
any time a task or method execute intero
we record that as an execution in our in
our database okay and this here is the
graphical notation that we use when
talking about tarot this is a task the
gray box represents the task itself the
inputs to the task are at the top of the
outputs are at the bottom and the middle
are the methods that the task has every
task has at least one method but a task
can have more than one method if a task
has more than one method each method is
executed in order the first method to
succeed will stop the execution of no
methods will execute after the first
method has it has succeeded so
if if a task has two methods and the
first one fails and the second one
succeeds then then the task is
considered successful both fail then the
task is considered a failure as a whole
here is an example of a task with two
methods and its corresponding petri net
representation the part of the pitcher
net in the red box corresponds to method
1 and the part of the petri net in the
blue box corresponds to method to you
see we're using our choice pattern here
and at the beginning of executing this
this task with two methods a token is
placed here in the begin place which
enables this first transition launching
a process and putting a token here then
once the task has succeeded or failed
let's say it succeeded in this case a
token is placed here in this success
place for the method and then that that
enables this transition here and the
entire task succeeds because this first
method had succeeded in the other case
if this first method had failed them
then this transition is enabled and
token is put in the method failure place
which enables the method to to fire and
method 2 may either succeed or fail
causing the entire test to succeed or
the entire task to fail ok so now you
know a little bit about how we translate
tasks into Petri Nets so let's talk
about submitting a whole workflow to
taro the sequence diagram shows the
different taro services Petrie workflow
and a shell command service they're all
running together there's also an
execution house represented here that's
where the shell command service would
would run a job that's launched the
arrows represent web requests between
the different arrows services the
arrows in red are happening those are
web requests that happen because of a
web book that has been subscribed to by
the workflow service and the green
arrows are optional web requests that
don't always have to happen when a
method execute but if you if you use
these optional web requests you can get
and set inputs so let's walk through
this the sequence diagram flows from top
to bottom so we start by submitting a
workflow a user of the system so it's a
work work flow to the workflow service
the workflow service translates the
workflow into a petri net representation
and submits it to the Petri service
which then begins executing the Petri
net once the Petri net is gotten to a
place where the first method should fire
then that causes a web hook to
communicate back to the workflow service
workflow then interprets that as a
command to execute a shell command on
the shell command service which once the
once the shell come in that may not
happen immediately but once the CL
command service is ready to run that
that job it'll communicate back to the
workflow service saying that the status
of the job is now running it actually
launches the job on the execution host
then that running job can communicate
back to the workflow service to get the
inputs for the method or task and then
do its work on those inputs and then
optionally set output spec to workflow
so that other jobs in the future can use
those outputs once the once the job is
finished the shell command service
notices communicates the success back to
work flow and workflow communicates to
Petri placing a token in the proper
place of the Petri net so that the Petri
net can continue executing until the
Petri net has completed successfully
which time the Petri service tells
workflow that all the work is done and
that's when the workflow is complete so
you see here that each service in the
Tara architecture does exactly one thing
the shell command service is only
responsible for executing shell commands
it doesn't do anything else it doesn't
know anything about Petri Nets the Petri
service only execute some petri nets and
the workflow service you know manages
the interactions and handles inputs and
outputs okay so here's a more
interesting practical example this
actually happened at work a couple
months ago my boss came to me and said
Michael we have a bunch of VCF files
these VCF files contain genetic variants
each file it represents all the variants
for one sample but in that sample the
variance cover many chromosomes and we
have a lot of these files my task was to
to make it so that instead we had files
that only covered one chromosome each
but contained variants about many
samples and so how would you do this the
way I approach the problem is I thought
well if I can extract from each of the
each of these VCF files information
about a particular chromosome then I
could have one file per input file /
chromosome so so each one of these here
so this just covers chromosome 14 one of
the input VCF files and and so on and
then I would merge everything for
chromosome 1 and create the the one
resulting file that my boss wanted and
so how would I construct a workflow and
taro that does such a thing so we start
with tasks over here we have a task that
runs tab x index tab x index is a
program that we have that takes a VCF
file and indexes it so it's easy to get
the variants for a particular chromosome
out of the file and that's something
useful and so we want to index these VCF
files before we do anything else
so this indexing test was the first
thing I thought of but what if the index
already existed for a particular VCF
file i wouldn't want to redo work by
creating indexes that i already have so
instead i made the slightly more
complicated task that has shortcut and
execute the job of shortcut is just to
check quickly to see if the VCF index
already exists if no VCF index can be
found to this first method here fails
along the second method to execute and
actually create the VCF index and once
that's done at the end we have an index
as our output of the task had the had
the short cut method found in index for
the VCF file the shortcut would succeed
and the execute would never run and so
we would save the work of creating
indexes that we already have okay so now
we build on top of that we're able
intero too because we can embed dags as
methods in other tasks we can build up a
workflow intero here I've wrapped two
tasks so we have the task that creates
the index for VCF file and then we have
a second task whose job is to pull the
variance for one chromosome out of the
VCF file using the index that was
created by the first step and the VCF
and the chromosome the VCF file path and
the chromosome that were interested in
and this whole task accepts as an input
one chromosome name in one VCF file
paths and reports the output VCF file
that only has information for one
chromosome it abstract sawey the idea of
having to create an index on that file
and the and so you see here this is kind
of a conceptual idea of what it's doing
it's taking a file that has information
about many chromosomes and just giving
you information about one chromosome and
now I had many VCF files to deal with
and and so
we continue then I modified the task so
this is the pretty much the same larger
tasks on the previous slide but we
changed it instead of accepting one VCF
file we accept a list of VCF files as
one of the inputs still just one
chromosome we're still dealing with one
chromosome name here and also we made
this box blue because we're using the
parallel by feature here and that means
that this this task will not just
execute once on the entire list it'll
execute n times one for each of the VCF
files that have been input into this
endless task and the result is that we
get many output VCF files one for as
many input VCF files that we have and so
what you have at the end is instead of a
few files that contain information on
all of the chromosomes we just have a
few files that contain information about
one chromosome but butter job's not done
yet so this slide kind of jumps ahead a
little bit this is the this is the task
you were just looking at with parallel
by vfc immediately after that it takes
the all the output VC ups from say
chromosome 1 and uses this task to to
join them together and create one output
VCF we wrapped that whole that whole
task our dag of tasks with the larger
task that then does accepts a list of
chromosomes and is parallel x
chromosomes and that pretty much
completes the whole job this is what my
boss wanted we now have a workflow
intero that accepts a list of VCF files
and a list of chromosomes that you'd
like to query and produces a set of VCF
files with information for one
chromosome each ok when we submit that
workflow to the taro system taro
responds with a
can't read it all with the URL and that
URL response represents the workflow and
you can use a command that we supply in
our SDK to get this kind of command line
view of the processing of the workflow
you might see right at the at the top we
have the status of the entire workflow
is succeeded then all the other lines
below represent one execution and said
the Nets all the the tasks and methods
as they executed and they all have
statuses have succeeded except for a few
that have failed which will notice is
that the failures are some of the
shortcuts failed like the first time
that we tried to shortcut on indexing a
VCF file the shortcut method didn't find
an index and so the shortcut method
failed which allowed the the next method
in that task which actually index the
VCF to run and that succeeded and then
later on in this later on in this
workflow execution we see a shortcut on
indexing a VCF that succeeds because the
earlier and executing the workflow we
had already done the work and we don't
repeat that work we create this view by
doing a topological sort on the workflow
tag and then show the the executions
related to to those tasks in order here
with the tabbing to represent how deep
inside the the workflow the execution is
okay so now we're going to talk a little
bit about how we developed and tested
workflow for development methodology we
used cmat which is the software
engineering method and theory it splits
a software project into several elements
you can determine the state of your
project by assessing the maturity of
each of the elements and then choose how
you'll proceed in your software project
by looking at the most immature element
and deciding how to
increase the maturity of that element
like if you don't have very clear ideas
of who your stakeholders are then you're
in your project is immature in that
element and you might choose to perform
activities to identify stakeholders then
for designing the software system itself
we followed 12 factor most importantly
from 12 factor we have made our
processes stateless and Soto runs as a
set of stateless processes that connect
to backing services to get their job
done the backing services that are
outside of taro hold all of the state
and the the stateless taro processes can
be scaled up or down or destroyed and
come back and the system is still pretty
stable for issue tracking we use github
issues taro is divided into many sub
modules and so we keep our issues on the
on the github repo that is relevant for
for that issue and so our issues are
spread across many sub modules so we use
waffle I 02 then aggregate all those
issues together and we have this one can
band style view of all of our issues
related to taro across all of the taro
sub modules okay now here's some metrics
about our the taro code base so you can
see our language choice is mostly Python
we have a little bit of lua in the petri
service to support interactions with
rett asst we also have the pearl SDK to
support our legacy infrastructure at TGI
MGI and the notice also that the slot
count for shell command on lsf the job
services is pretty low we are able to
satisfy the job service API and under
three hundred lines of Python so it's
pretty easy to make a new job service to
extend taro to whatever kind of job
service you need if the shell command or
Ella self-service doesn't meet your
needs
okay and this is how we do our
continuous integration for taro like I
said we have many sub modules for taro
and we have one main repo any time a PR
is made on a sub-module then that that
PR being open causes a sub module tests
to run usually on Travis CI once a once
its branches merged for any of the sub
modules that triggers the system test
and the system test succeeds then we
update the successful configuration of
commits in our main tera repo on github
you'll notice a couple places we use
Jenkins internally to do some testing
for lsf that's because lsf has a
proprietary license so we don't run that
on Travis CI we were in that those tests
internally and platform Ellis F has a
proprietary license not the not the lsf
service for taro and then also our
system tests we found this was the
easiest way to update our sub modules on
github was to just run this test
internally and so that's why our system
tests run on Jenkins in-house okay and
then in production we take our tarot
system test and we reuse it as a
production monitor it runs a small
workflow on the production taro and just
make sure that the production taro is up
and running Jenkins schedules these to
happen every 15 minutes or so normally
this day's blue meaning that
everything's okay but when this turns
red we can look at our cabana dashboard
to inspect the logs and try and get some
insight on what has gone wrong all the
taro logs in our production deployment
are sent to log stash and index with
elastic search okay and this is how we
have chosen to deploy our processes for
taro in our production setting the taro
processes themselves they all run on day
are they all Cameron on dais we actually
these ones with the stars next to them
these are workers for the shell come in
and lsf service we actually have those
run on specialized execution host so we
do not deploy them internally to the
today us but they run on separate
execution hosts then all of our backing
services run on not on deus on separate
VMs we use celery for messaging between
different components of each service so
all of our services use celery and we
use RabbitMQ for the messaging broker
and Redis for the results back end but
you might make other choices in your
deployment of taro but we do need Redis
for the Petri service for the storage of
petri nets so you see that right here
also the the workflow and lsf services
require relational database we are using
postgresql soon the shell command
service will also need a backing
relational database but it doesn't right
now so it's an it's not highlighted okay
and that's how we do our production
deployment but for development and
testing we actually run taro on our
laptops you can get started with taro by
doing a git clone of our repository CD
into that clone Terra directory do it
get sub-module update an it to update
all the sub modules and then do a vague
or not once you do a vagrant up all of
the backing services are running on your
bigger vm also the taro services are
running there as well if you set this
optional run tests equals one then a set
of tests will run against those tarot
services and populate have the side
effect of populating your database with
some test data ok I think now I'm not
going to be overall up all right well
I'm going to move on to the conclusion
so yeah taro is designed to carry out a
large complex workflows its service
oriented with the simple restful api s
its focus and scope a free and open
source it's under active development at
github here at the genome organization /
taro these are the people involved in
taro development Travis mark and
Charlotte are not actively developing
taro anymore they've moved on to other
projects but me and Dave and Josh
McMichael is not here today are actively
developing antero we also were part of a
larger Institute this is all possible
because of the the work that everyone
does at the mcdonald genome institute
and through our funding from the NIH and
the National Human Genome Research
Institute thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>