<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Riot Messaging Service - Message Players Like a Pro&quot; by Michal Ptaszek | Coder Coacher - Coaching Coders</title><meta content="&quot;Riot Messaging Service - Message Players Like a Pro&quot; by Michal Ptaszek - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Riot Messaging Service - Message Players Like a Pro&quot; by Michal Ptaszek</b></h2><h5 class="post__date">2017-10-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DnjXM-Z9uho" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you all for coming today it's
Saturday it's before noon so really
appreciate the audience
my name is maja shock and I'm a software
architect at Riot Games and I'm today to
talk about dry out messaging service or
RMS in short so before we kick off just
for my observation how many of you have
ever heard or played League of Legends
holy crap okay that's amazing
cool and so Riot Games is a developer
and publisher of League of Legends more
about the game for those who never heard
of it in a second but as a company our
main focus is really on the player
experience we use it as a razor for
making any decisions regardless of they
are product or technology oriented it
helps us tremendously that we are all
gamers here like we really play games
and join em and we can empathize with
with other players our mission is that
we aspire to be the most player focused
game company in the world so what is
League of Legends League of Legends we
we launched the game in 2009 and since
then we grown the player base to some
pretty big numbers and more about the
numbers in a second League of Legends is
a team based game called in a genre
called MOBA which is a multiplayer
online battle arena where two teams
compete to control and map and achieve
objectives
each player of these teams can control
Swan champion and typically teams
consists of five players the entire game
is is placed in the modern fantasy world
and to give you some more context around
the scale and types of problems we were
trying to solve when architecting write
messaging service here are some numbers
we really
in September 2016 regarding popularity
of league of legends so we are super
lucky to have over hundred million
players enjoying the game monthly and
out of which 27 over 27 million players
play league daily if you check our
systems at any given time of a day we
can see it at least seven and a half
million players connected to our servers
so all of these numbers heavily
influenced decisions we've made when
building RMS so not now let's jump to a
quick overview of what we're going to
cover today I would like to start with a
very high level design of riot messaging
service then I would like to follow up
with the implementation how we got there
and what word architectural decisions
behind RMS as well as what were the key
lessons learned for us when we created
it and also what are the plans for the
future for the service ok let's start
so let's before we before we jump
directly into design of RMS let's let's
kick off with describing why we even
wanted to build RMS in in the first
place in the old way when we started
building League the old way of handling
player to server communication for the
out of game events used persistent TCP
connections using LCDs RTMP protocol
that's a proprietary binary protocol
introduced by Adobe overall it's it was
designed to handle media but we decided
that is a great fit for game traffic or
not viewing game but out of the game
traffic as well and it turned out that
the protocol really was having really
big issues with security the way we
built the platform as a monolith
prevented us from growing it fast
deploying and like transitioning into
the micro service world with very fast
iteration loops and we also suffered
from pretty significant scalability
issues a single instance a single server
holding a player connections could only
handle around 30,000 players given the
scale we were operating it building out
the platform for you know hundreds of
millions of players was a huge challenge
and as we were building more and more
features for League and also other
products in R&amp;amp;D we realized we can't
really replicate the same pattern and we
needed something better and that's how
we created RMS RMS or riot messaging
service is is a back-end service
something we deploy on the server side
it provides asynchronous messages
capabilities notifications that are sent
using the fire-and-forget semantics to
from services from back-end services to
connected players so that's the way for
the backend micro services to somehow
deliver either notifications or payloads
to clients for example leaf client and
today RMS has been adopted by over 20
different products and features riot and
is used every day by all big players
around the world to give you an example
of what features use RMS it's clubs
which are player organized and
controlled social groups its missions
which are challenges for the players
with with rewards and honor which is a
system that encourages positive behavior
among the community to better illustrate
how our mess works let's let's use push
Mobile push notification analogy in the
beginning let's imagine that we have
someone sending us an email the email
hits our email server and email server
stores it in some sort of the database
the interesting thing is that the email
server because everyone now uses smart
phones figures out that ok that
recipient probably has a phone and would
like to be notified that he has an
incoming email
in that case that email server sends a
request to the push notification service
informing it that hey you've got an
email but because your incoming email
might be pretty big
it might also have attachments you don't
necessarily want to receive them as push
notifications as a matter of fact the
push notification servers have very
strict limits when it comes to size so
what happens is that push notification
service actually receives only a very
high level digest of of what the email
is which is the subject of the email
sender and maybe few first paragraphs of
text it sends a push notification to
your phone your phone buzzes and
displays a toast on your home screen
and then some time later you're gonna go
into your email app open it up and
download the original email including
attachments all the text and call the
context around the email but instead of
going to the push notification service
you're going to go to your original
email server and RMS works in a very
similar way but instead of the mobile
phones we have desktop clients or League
of Legends clients and instead of the
push notification service we have RMS
let's let's see how it works with League
of Legends example as clubs as mentioned
earlier clubs are player created
organized controlled social groups each
Club can be described using several
properties for example the club name
club roster which is a list of members
with a ranks message of the day and so
on first of all whenever you start
League of Legends you launch and
authenticate with with our servers as a
result you're gonna get a signed GWT
token claiming who you are so that you
can prove with other servers that you
are in fact you as soon as
authentication is done
RMS requires clients to establish a
persistent TCP connection using
WebSocket protocol as a transport
in order to receive any incoming
notifications this is an example of
WebSocket URI the aforementioned token
we got from the authentication service
is highlighted in in red here so we pass
that to RMS and RMS is essentially
authenticating validating the token and
letting you N and as one of the claims
encoded inside of the token there is a
field called subject which is which we
interpret as player ID player ID is a
standard way to have to identify players
in our ecosystem and is used by all
internal services at riot it's also
globally unique and immutable internally
we called this player ID poet as player
universal unique ID as soon as the
connection is established League client
will go to the club service to fetch the
latest state and by state
I mean list of clubs that this player is
a member of and for each of those clubs
all the details about the clubs and then
clubs are ready to go you have Chad you
have a list of members and you can start
using them what happens when there is an
invite incoming to the club in this case
clubs service will update their internal
state and then issue a rest request a
post request to RMS the with the
notification the notification itself is
a simple JSON blob that informs RMS what
to do with the message there are few
required fields in that blob one of them
is called resource which is a rest like
string denoting which component of the
client should handle this internal
notification you can think of the
resource as a routing key that is used
by the client to dispatch the
notification to the corresponding
or subsystem the next one is variant a
which is a string that allows us to
guard against duplicate so if the the
sender service fires few notifications
which carried the same version it is
safe to assume for the client that he
can ignore it it also helps with
notification reordering because we use
HTTP requests can actually get reordered
on the way to RMS and the next one is a
list of player IDs a list of poets and
that's the list of all players that
should receive this notification in case
of batch notifications for example
whenever the club changes the message of
the day instead of notifying 50 players
one by one club service can actually
issue a one notification with 50 player
IDs on this recipients list and lastly
we have a short optional payload to
deliver and more about it in a second so
as soon as RMS receives this
notification it will use apply internal
routing rules and forwarded notification
to connected client as a result client
will then go to the club service to
fetch the entire state to figure out
what actually happened why did they
receive that notification and then
display a proper toast to the player
I've mentioned earlier RMS can also
carry short payloads in the
notifications and they are completely
oblivious to RMS it's just a binary data
that is transported from point A to
point B and it can be anything from JSON
basic c4 blob XML or even plain string
okay let's let's dive deep into aromatic
fermentation as mentioned earlier rms is
a server-side application under the hood
it consists is built by two different
tiers routing an edge functionally
routing is responsible for accepting and
handling published notifications from
external services like clubs as well as
maintaining a list of all connected
players to the platform on the other
hand edge is responsible for handling
player connections those WebSocket
connections I was talking about and
forwarding incoming messages directly to
the clients both ears are completely
decoupled we can scale and deploy them
independently for example in one of our
RMS clusters we have four servers that
run in routing tier and around ten
servers depending on the scale and time
of the day
in the edge here their code has their
own repository and we release in version
them as completely two separate products
each tier is a collection of independent
servers that do not share anything with
each other each of the servers is
responsible for accepting incoming
player connections authenticating the
players by validating the sign 3wt
tokens holding the sessions so holding a
persistent TCP connection between player
and edge and also registering them with
the routing tier by registering I mean
going to issuing a rest request to their
routing tier telling routing tier hey
player with player ID equals X is
connected to me forward me any incoming
messages and incoming notifications that
are addressed to that player this tier
is typically memory bound because it
holds all the player connections in
memory and even though we load tested it
pretty extensively we were able to
achieve around 10 million connections on
a single server we decided that we
didn't want to put too many eggs in one
basket because losing that one load with
ten million players will result in a
disaster
so we decided to cap the number of
connections to around 350,000 per box
this year is also really linearly
scalable and we can increase this this
year capacity by simply throwing more
servers at it on the other hand we have
routing here this year is is completely
different from edge is it's a cluster of
very tightly coupled servers sharing a
global routing table mapping player IDs
to the edge nodes that hold that session
it's also responsible for monitoring
health of each of the hopper of the edge
servers and also accepting
authenticating publishers accepting
internal notifications and forwarding
them to the edge tier this year is CPU
bound and it's typically much much
smaller than the edge tier the routing
tier I mentioned a little bit earlier is
a simple translation table between
player IDs that's the column on the left
and the edge nodes that's the column on
the right this table is globally
replicated within and the routing
cluster and is used whenever there is
any incoming notification addressed to
the player we simply perform a look up
in the table and figure out what the
edge node is and forwarded forward that
message to the edge node let's follow
the flow of the published message in the
system first of all we have a back-end
service like clubs trying to communicate
with the players it sends a rest request
publish request to RMS we have a load
balancer sitting in the front of the
routing servers this load balancer
forwards of notification to any of the
existing routing nodes it can select any
because we have the global routing table
that is shared across all the servers
one of the routing routing servers will
look up into the routing table select
the edge node that halts player
collection forwarded to that edge server
which then delivers that you to the
player using players WebSocket
connection
the entire RMS stack is built using
Arirang Arirang is a functional
programming language which comes with a
great support for building highly
concurrent and distributed systems
certain language constructs or
philosophies like the one around actor
model or the concept of lightweight
processes or even like an excellent
standard OTP framework that comes with
the language they allowed us to focus
primarily on the core logic of RMS
instead of trying to solve scalability
issues the team behind RMS has already
built chat and few other services
internally at riot using this technology
so building RMS was really really easy
considering we had a lot of other
libraries that we used in other projects
if you would like to know more about our
rank I would highly recommend you
reading learners from around book by
Fred Hobart as mentioned earlier each of
the tiers routing and edge are
completely separate and we release them
independently even though they are two
different airline releases we we package
them in docker containers which gives us
great service isolation we can relocate
the servers without with ease and also
it gives us independence from the
underlying OS we don't have to make any
assumptions about availability of the
local libraries or installed packages we
have the golden image we can drop on any
server any cloud provider and we will be
sure it's gonna work and whenever we
build a new docker image we upload them
to AWS container registry so as you can
guess RMS runs on AWS however the code
itself does not have any dependencies on
the underlying infrastructure it is
completely possible to run RMS on better
metal your local Mac or any other cloud
provider for infrastructure automation
we
hashey corpse terraform which is a tool
to treat the infrastructure as a code we
store our infrastructure descriptions in
a git repository we version it we have
history of our infrastructure with all
the mutations and also we can express it
in a very declarative state the way this
approach allows us to reduce the risk of
running into very costly human mistakes
whenever someone clicks on the wrong
button in the UI but also gives us as a
bonus gives us an ability to create
ephemeral environments for load testing
or for QA within a matter of minutes we
can stand up the entire functional or MS
environment with one click of a button
and because our ms is is a core piece of
royal infrastructure as mentioned
earlier used by over 20 different teams
one of our girls when building our ms
was that we would like to be the first
to know
whenever there is any disruption of the
service to do that we build a battery of
automated health checks for simulating
publishers and clients and they run
every 30 seconds against our
infrastructure and the only measure
correctness of the service but also
verify that the latency is not affected
whenever we detect any anomalies in the
behavior we have an SNS queue that
notifies pager duty or Gmail okay let me
now try to distill some points that you
can use on on your own when building
similar services
and when building and testing
distributed systems locally on on my Mac
at least I'm I'm really I tend to ignore
and forget about the fallacies of
distributed computing those are the set
of false assumptions we typically make
about our applications as an example
that the network is reliable we assume
that with the net the split brain
syndromes will never happen to us or
that the latency is zero because I
tested it locally using my loopback
interface or that the network is
homogeneous and we will not see request
reordering to combat that to fight this
behavior in very early in the
development process we we started using
a tool called Jepsen Jepsen is a tool
for verification of distributed systems
which allowed us to inject failures and
faults into our system we were able to
simulate different types of problems for
example splitting our routing cluster
into two different house and then after
running some additional tests and
merging them back we were able to verify
that everything went back to normal and
we have consistency in our system one of
the problems that hit us very hard early
when we forgot about their butt Network
and the infrastructure does necessarily
have to be super reliable was the
example of having too many connections
on a single edge server so whenever one
of the instances in iws failed which
happens from time to time we lost around
500 thousand connections in a single
second all of those players because of
the logic in the client attempted to
reconnect to our MS immediately so we
essentially did ask ourselves which
which is fine I mean that's we would
like players to be able to to connect to
our mess at any given time but the
problem was that the load balancer that
was sitting between edge and routing
tier that was used for registering new
sessions was called
so what happened is 500,000 players were
trying to reconnect in the same second
we're hammering and that ELB that lb was
trying very hard to warm up I was
failing and for five to ten minutes we
had half a million of players were
completely dark they were not able to
receive any notifications and could not
understand what's going on like they
they were not able to see in game
notifications or other things we
essentially added at the end of the day
we removed the Glo balancer entirely
from from our infrastructure replaced it
with service discovery and also reduce
the number of connections we can have on
a single edge server but that's one of
the examples that actually demonstrate
how important it is to think about
failures when building your systems
another thing that is really important
and dear to my heart is visibility into
our systems we probably collect more
statistics that we realistically should
and we try to measure pretty much
everything things like not only messages
published by back-end service but also
distribution of different publish sizes
the latency within the system the number
of concurrently connected users
publish errors and so on it is even more
important that with ephemeral containers
with auto-scaling groups that we used
centralized logging and centralized
monitoring because nodes go up and down
all the time
and if we only keep the logs and metrics
locally in every box it is more than
guaranteed that are gonna be done within
the matter of hours and we will have no
history and no way to really triage
understand what really happened after
the fact we also learned that even
though detailed metrics and and
monitoring is super cool for developers
and maintainer of the system customers
don't really give a about it
right what they care about is is our
mess up or down do I like do my players
actually can receive notifications and
to do that we decided we need to like
take a step back
abstract few things and really present
them of a dashboard that only has two
states is it green or red and this
dashboard is actually built using those
health checks we we talked about few
minutes earlier but run against a live
system every 30 seconds another
important thing is a support for dynamic
configuration with a such an large
number of customers coming in we had to
find a way to fine-tune the system
effectively without having to restart
the service or go to every single
instance around the globe changing the
configs by hand
our configurations live in a git
repository and we have a Jenkins job
transformer that moves them from that
grid repo and publishes them to the
centralized configuration server we call
it configure Asst RMS poles that server
for changes every 30 seconds as well and
converges dynamically without any need
of restart on the new stage that out
it's a perfect thing for us for things
like feature toggles pretty much every
major change we introduced to the system
sits behind the feature toggle which we
can turn on and off with a single git
commit globally and see the change to a
plot being applied on all the servers
around the world in 30 seconds also with
a very small team behind RMS we only
have four engineers and many internal
customers we had to find a way to avoid
becoming a blocker for four teams to
really tree our day problems and adopt
our service we essentially had to build
Cyril
self-service capabilities
we invested a lot in client and
publisher SDKs in documentation examples
and easy to use triage tools as an
example here we have a real time
dashboard that allows publishers to take
a sneak peek into what's actually being
routed right now
in the system the dashboard allows us to
sample the traffic that's enroute and
display it to the publishers so ever
they see a problem they can immediately
go and see oh is there anything that's
actually being processed is there
anything that is maybe parsed
incorrectly and that way they can at
least help themselves first before they
come to us for a physical help we
intentionally block teams only on
creating new accounts and that's
primarily to understand our publisher
use cases needs and also to establish
the relationship between the RMS owners
and the customers another thing I'm
really passionate about is capacity
planning we load test the entire system
to understand its limits to establish
the baseline and also identify all the
bottlenecks for first global centralized
service like RMS we also had to make
sure that we have no downtime of great
capabilities and we can do it even
during the peak hours during the load
test it was really important for us that
we also were able to shut the entire
system down and bring it up even though
we had over 10 million players trying to
hammer and get back in that allowed us
to simulate and be prepared for any sort
of massive failures of our
infrastructure and giving us that
assurance that we can bring the back the
system online even if it's it's peak
hours and we have so many players trying
to reconnect
a question to you so this is a graph of
our concurrent connections to the
service and around the third of May we
experienced some warrior behavior as
anyone of you have any idea what it
might be so what happened is all of a
sudden within few hours we essentially
doubled our connection counts the team
was like super excited that was like
 yeah we just doubled the CCU we we
established new records chillin
champagne no need to party super cool
ride big numbers it turned out that
actually that was a patch day and the
client got patched and we ended up with
actually instead of one concurrent
connection we had two concurrent
connections going to RMS yeah pretty bad
so we had two options here you could
either release a hotfix to the client
which would trigger the full rebuild
process
retros and everything else player pain
because players will have to patch the
client log out and log back in do
everything like that or we just use the
capabilities of our Altos gaming groups
and resize the system to handle the
double of the load it turned out that
not a single page was issued within
those two weeks and because we load
tested the system to probably 10x of its
current capacity we didn't have to act
at all we just waited for two more weeks
that's the date of May 15 when the new
client was released after two two weeks
and the connection count dropped back to
the original number
okay cool um so what's next just to wrap
up I want to share few road map plans
for RMS first thing is obviously
widening the adoption of the service we
would like to have more teams internally
to to use our ms pushing even more data
for the service next thing we are
considering about is support for topic
based routing so instead of addressing
individual players we would be able to
say okay deliver this message to all
players Germany or deliver this message
to all players using Mac or Windows and
finally we would like to introduce
persistence into the system which would
allow publishers to to tell us that this
message should be delivered even though
player might be offline right now so we
could persist it somewhere and then
deliver it next time the player shows up
if you are interested about more details
on RMS we have a tech blog Engineering
we have games come there is a blog post
on it and now I can probably take few
questions but we can probably do it off
stage as well if is that fine
the question was which button do we use
for the global routing table we use mini
Jie it's a built-in Erlang distributed
key-value store we patched it heavily to
support the concurrency and dynamic
cluster topology but essentially the 95%
of the code is the standard error on
library code
how do you maintain WebSocket
collections sorry what was the second
part of the question Oh
how do we rotate edge servers with
persistent TCP connections so to do that
we we have an internal protocol that
whenever the server the edge server is
about to shut down we receive an SMS
notification from auto scaling group and
we drain the connections that are
established to that specific edge server
by before shutting the damn the
connection we send a message using
internal RMS protocol to the connected
client telling the client to hey I'm
about to shut down please open either a
connection to to the service that allows
us to drain I think 300 thousand
connections in like two minutes or so so
it's pretty fast process
and what we use for our load balancer
it's a standard ELB from AWS because
it's WebSockets and we use the classic
load balancers it's actually tcp-based
so we cannot use all the goodness coming
with a lbs or like HTTP our load
balancers but it's still pretty good
okay how do we how do we keep the
routing table up-to-date with the
connections coming in and going out so
essentially each each edge server is
aware of all the connections established
to that particular node and whenever
there is a new connection which is
authenticated and established to the
server we issue a rest call to the
routing tier to up to notify about that
change the same happens whenever the
connection is terminated so edge server
will inform the routing tier that the
connection is down and it should remove
that player ID from the table itself
yeah so when the edge node crashes I was
so the routing tier also maintains
health check logic for every edge server
so it has some sort of garbage
collection mechanism that allows it to
eventually clean up the mess in the
routing table we also have if the
unregister collection called fails so we
like timeout or whatever
next time routing tries to deliver the
message to that ghost session to the
orphan session it will receive 404 error
from the edge server that is suppose
that was supposed to halt that session
and will clean itself up as well
so what the question is what's the
protocol used for communication between
edge and routing tears
that's pure rest over HTTP we use
distributed airline protocol for
internal cluster communication within
routing tier those are normal the test
normal airline cluster for communication
between peers is pure rest which also
gives us a possibility to potentially
replace technology is used on both sides
so we don't have to use airline
necessarily on both ends
sorry I couldn't hear a question
sure so the question is about out the
scaling of the edge tear how concerned
we are about the cost and how do we
manage our policies for going like
growing the cluster or shrinking it down
we use two metrics for that we use CPU
utilization and also we use the
connection count metric
I think CPU comes out of the box for AWS
for connection count we have to run a
daemon that essentially publishes that
metric to cloud watch every minute or so
telling auto scaling group what's the
current number of established sessions
to that server we are not super
concerned I mean we are concerned we are
a customer but we are not trying to
optimize it to its limits we we prefer
to play safe on the reliability side and
make sure that players have a good
experience instead of like trying to
save every dollar but the the scaling
actions happen probably to three times
within a day so it's not that infrequent
how did we get and long distribution
working in a doctor every single
container we have has its own IP address
we also limit the actual range of
distribution ports to only I think five
or six there are VM settings to specify
the upper and lower range and we also
run the containers in the host network
mode so that specifically for the Aged
here is important because with the proxy
default default proxy mode the proxies
will will simply get overloaded with
like 10 million connections going
through them ok and I think we are
running slowly other time but I would
love to take any questions offstage
thank you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>