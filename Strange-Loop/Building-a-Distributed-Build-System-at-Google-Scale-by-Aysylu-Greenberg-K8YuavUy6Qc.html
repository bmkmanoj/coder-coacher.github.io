<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Building a Distributed Build System at Google Scale&quot; by Aysylu Greenberg | Coder Coacher - Coaching Coders</title><meta content="&quot;Building a Distributed Build System at Google Scale&quot; by Aysylu Greenberg - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Building a Distributed Build System at Google Scale&quot; by Aysylu Greenberg</b></h2><h5 class="post__date">2016-09-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/K8YuavUy6Qc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so how many of you use a build system
every day raise of hands okay let me see
if I can get more of you to raise your
hands how many of you use make me even
Gradle pip cabal rig okay so these are
all examples of build systems and a
build system is a software that
automates compilation of your projects
and produces binaries so today we'll
talk about building a distributed build
system at Google scale mi solo greenberg
i work a go go on Google's
infrastructure today I will talk about
the project that I worked on as part of
the developer infrastructure team which
i think is really really cool and the
past I've worked on search
infrastructure and currently I work on
Google drive's infrastructure so what
does it mean to build it should be the
build system at Google scale let's
unpack this title a little by little so
as we discuss build system is a software
that automates compilation of software
and so in order to answer the question
of what does build system actually do
let's look at some of the the two
different cemeteries so I built system
will do a build or can do a test so what
does that mean so we'll have a project
with some dependencies on the slide you
see you know just a template makefile
we're on the left right before the
columns is the target and then we see
the dependencies on the right side and
then the next line is the command you
would execute to get the target and so
now the build system will take in this
project with dependencies and it would
find it depends is for your so for
instance that means that she needs to be
actually fetch from a remote server and
then once it found the dependencies and
downloaded them it will build this
project with dependencies and then it
will emit binaries and so now the build
system allow each other download the
build artifacts or if you're interested
in packaging them and deploying them on
from a machine and putting them in
production the built is to Malawi to do
that as well and in the test scenario
it's very similar except for now instead
of
loading the build artifacts you are more
interested in the result of the test so
you would run the tests and then you'll
get the results of the test whether they
passed or failed which ones field so we
discuss about what build system does and
what exactly it means to build and test
so let's talk about what it means to do
this at Google scale and what kind of
challenges opposed on the build system
that is built at Google so we have
30,000 engineers and other 40 offices
and so that means is every single
engineer needs to use a build system
every day many many times during the day
and there is roughly 15,000 commits made
by engineers every day and about 30,000
commits per day made by robots what I
mean by that is that will have an
automated script there will be
committing code to the repository and
the reason we want to do this is because
you know even with 30,000 engineers
there's still so much more work to be
done so if we could just automate that
away so that the human doesn't have to
do the boring repetitive tasks and they
could just be delegated to a script you
would like to do that as much as
possible and so now all these commits
will have to go through the build system
and the source code is 2 billion lines
of code and half of the code base
changes daily so what that means is
every time I change the code base goes
we need to build it run the tests and
then ensure that all the failures of
figs before we commit the code and
there's roughly five million builds and
tests that go specifically to the bill
rapid system bill rabbit is the
distributive build system that is part
of the developer infrastructure and
we'll look at it in more detail
throughout this talk and we don't have a
generous on the order of petabytes of
output artifacts and some of these
artifacts are accessed in order to
deploy them to production some of the
artifacts well built system produces the
artifacts but the user may may not care
about retrieving them and all of this is
done in one repository so let's talk a
little bit more about what that means
and what kind of challenges it poses on
a build system
so when Google open sourced a belt
system its internal build system and
open source that is basil there were a
lot of comments and confusions about why
would Google at such you know such a
large company with so many engineers
will use monolithic repository and then
we already moved out a way didn't we
already decide there was so much you
know contention trying to push to Daddy
dinnick didn't we already agree that it
causes so many problems so let's look at
that a little bit more detail I'm not
going to go into too many details and if
you're very interested about how we just
do our source code in one repository
there is a great talk by Rachel potvin
that she gave at the conference code at
scale that a highly recommend you check
it out so to me working in one
repository is basically a trade-off
between having a repository of artifacts
were now from all the different
repositories that you have you just put
them in a is something like maven
central or it's building from source
because now you have just won a coat one
source code and so you can just build
from it anytime you need at a particular
revision that you're interested in and I
assume that a lot of you a lot more
familiar with what it means to work in
distributed code bases raise like many
different code repositories so today
everything is a trade-off but today
specifically I would like to talk about
the advantages of working in one
repository because I assume that you're
a lot more familiar with all the
advantages disadvantages of working and
many different repositories so first and
foremost when you have a monolithic
repository that allows you to have a
linear revision history every single
commit is an atomic change to the code
base and so there's not really the
history is linear so there's not really
confusion about what happens after work
what and having to worry about
interleaving a causality because all
that has been sterilized and this also
allows us to have extensive code sharing
and reuse because everything can be
cross-referenced if I'm interested in
deprecating my API and I want to figure
out all the youth sides of the function
in the eighth
yeah I can just look throughout the
whole code base and figure out if the
user are using it correctly and whether
anyone is using it at all it also makes
it possible to do large-scale
refactoring to modernize the code base
so if we decide that some base library
that will depend on for instance on
manipulating files needs to be
deprecated because we know a better
approach to doing things we can actually
do that across all the code base and we
can do this in a lot more informed
fashion and we can find all the
different instances and just replace
them it's a lot easier to collaborate
across teams now so if they if I'm
working on some new piece of code and
wondering what's the correct or better
use of the API what the best practices I
can look at other instances of uses of
it and I can figure out and even look at
the history of how the team progress
towards a better implementation or
better use of the API and I can learn
from their mistakes and from their
successes and also if I'm interested in
changing something in a library I can
just send a commit to them to the team
it send it for review and see what they
think if that aligns what their goals
and their in the sending of what the
library should be doing and then have
that submitted as soon as it's possible
it also simplifies dependency management
so as we talked about there's only one
single source of truth and so there's no
confusion about the authoritative
version of the file when you have it in
a central repository if data gets
corrupted or if there are many multiple
versions of the same artifact it could
be confusing like is this the one that
the team actually wants me to use or
should I use the older one what happens
if i use and not the latest version of
it would that cause some issues that i
should be aware of where's here it's a
lot more clear we can just specify that
we depend on setting and loved a team
decide what is the acceptable version
that i should use and there's no need to
fork shared libraries now if i need some
future the chances are some other team
and some other engineer might need the
same future from the library and so we
can work with the library team a lot
more closely to get that ad and make
sure that everyone gets the benefit from
this and finally we still have a way to
distinguish between work-in-progress
version that they say the library team
is working on and the released version
because we use ascending code components
for library releases which is similar to
get sub tree or a good sub components
we're basically the team can market ok
this is the release version everyone
should use this one and this is a work
in progress we're still trying to figure
out if it's something that we can
release later from the perspective of
the build system this also allows for
predictable repeatable build from source
so I know what my inputs are I know what
the source code looks like at a
particular revision and I can compare I
can come build the same target multiple
times and compare the outputs the
artifact if it happens that a different
code path was taken through my build
system I should be able to decide not
see in your results the user shouldn't
actually get two different binaries
because you know just because they took
a different cope out because machine my
machine failed and i had to route them
elsewhere shouldn't actually cause any
difference in an output and so it's a
lot easier also for me as a built in
junior to detect that and find that
improve that there are some differences
and we still have a lot of
optimizations to avoid compiling same
artifacts the biggest worry is that if
we don't have a central repository of
artifacts then you will have to
recompile this all the dependencies
again and again well no we don't because
we can just detect that something will
has already been compiled and so we
still can have very fast build and this
also allows to decouple each team's
processes as much as possible for
example if I'm depending on some library
on API and there is a bug in it and but
I need to release my code because my
users are relying on me I could just ask
the team to tell me which commits to
cherry-pick into my release and
proceed with my release well that seemed
verifies that everything on there and is
compatible and works as intended and
then release it at their own on their
own schedule and make sure that it goes
through all the processes that they have
in place so we discussed what the build
system does we discussed some of the
challenges the scale and some of the
things that the build system should
support when it works at Google scale so
let's talk about the distributed build
system how many people in the audience
work with a distributed system I see a
couple hands and you might actually be
using Travis CI maybe for your
continuous integration testing okay so
um distributable system is does very
similar things to that is basically but
the build system does is instead of now
being on your desktop and you use like
command line interface now you send the
request to a remote server and many of
you many of the companies don't really
need to have a distributed build system
built in-house and here's why so let's
look at how vote system evolves and when
the need arises to have a distributed
build system so have a person working on
a project that they are experimenting
with and hoping something good will come
out of it and they will be building on
their desktop using whatever a build
system that they using at the time and
suppose that their machine learning
experts and so they will have a
dependency on tensorflow and so the
they're built system will have a way to
download that dependency on your desktop
on their machine and build that and
compile that and so on so now as the
project becomes more successful a whole
team would be assigned to this project
and so now this team will set aside a
box on the side and it will do its build
and testing and all the continuous
integration testing on this box now a
different team might have the same
dependence on Tessa flow if they work
all in a machine learning company and so
now both teams have to have a boxer side
that they still have to worry about
updating maintaining running may incur
sure that all the bug fixes a pic
in the build system and if you know have
a bigger company with more teams every
single team now has to do this so it
poses an overhead for machine learning
it specialized it's you know maybe not
something not their area of expertise
and they don't really need to worry
about it but they have to because
there's not an infrastructure for them
well they can just delegate they're
built and so that's where my team like
mine builder but will step in or
basically instead of having different
teams having to maintain their boxes
they'll just be all in one shared
infrastructure and so a bill rabbit team
will provide the shirt infrastructure
where now teams can send build and test
requests and they can focus on what they
do best and what they love which is
their machine learning projects so let's
talk about Bill rabbit now which is the
distributed build system at Google which
is part of the developer infrastructure
team um go rabid the way I like to think
about it is basically basil in the cloud
so base always there build system that
google open-source i believe last year
and we'll see why i called on the cloud
so in order to answer the question of
what does Bill dabba doo let's see where
it fits in the you know the build and
test stack so build a big is it's very
nicely in the center of the building
task tag and it would be used by
something like continuous integration
system in order to be able to run the
test and the release infrastructure will
use it to be able to build their
artifacts and then use those artifacts
to be deployed to production and the
oldest orange boxes basically say that
this is part of the developer
infrastructure tooling develop
infrastructure a bigger team now other
engineer Google engineers and teams
might want to do this and so they
Martina in a different color and not
necessarily part of the developer
infrastructure team would want to do
some building and testing on a remote
server and then I integration testing
from work might want to do this so
basically building the services and
bring up those instances and then
running integration tests against them
and Bill rabbit in turn will have
dependencies
on a source system so we need to talk to
the source system to figure out what is
the state of the source term system at
the particular revision if it was
committed then it would just pull
directly and if it hasn't been committed
yeah then you will use the overlay with
the users proposed changes and now now
that it has the local environment setup
it will go into place which is Google's
internal build system where now police
would execute the build or the test
command and now it would put the
artifacts into the build artifacts
storage and / or ever will know how to
retrieve those artifacts and give them
back to the user and so now blazes I
mentioned has been released as basil and
so as you see this build rabbit it
doesn't do the builds itself doesn't run
the test itself it delegates it to base
I will blaze and all it does is provides
all this they distributed layer on top
of that it worries about what happens if
machines fail what happens if the user
connection breaks from the machine that
it allocated for the work and so on okay
so now we have not we're on the same
page about what distributed system does
let's talk about what it means to be
building it at Google scale so the build
system went through a transition where
it started out with one architecture and
then we started hitting the limits and
we had to redesign your architected and
so it started out with a very simple
client-server architecture which I like
to think of as a push architecture and
we'll see why in a second so in a
typical scenario it would have a client
library for the user that we gave to our
users and this client library will send
requests to the build a bit scheduler
which basically acts as a load balancer
and then it would get the response back
for which workers should talk to and now
the worker the client will request the
worker to execute its workload it's
build a test request and then it would
establish the connection were over this
connection will get all the build
artifacts and they build progress
information so this works really well
for many year
earth while we were figuring out where
this what this distributed system need
to do distribute build system need to do
for the company and for the engineers
worried what is it it's use case so the
problem is that we started running into
is that so the client library was very
thick and he need it had a lot of logic
related to routing and utilization of
resources the scheduler had some
knowledge of that too but also the
client library needed to understand a
lot of the transients and permanent
issues inside the build system and also
now the client library if the connection
broke between the user and the worker we
lost all the progress and so the client
library had to do the retries based on
that but what that means is for some of
the big projects and big big binaries
with lots of dependencies that meant
that we had to try over again and we
lost all the progress that we could have
made and so this was suboptimal and so
we made to we move to something called
that we call broad service were
basically we moved to the pool model
were before the user connected directly
to the work and push to work on to
attain like handle disabilities request
now instead the worker pulled the work
off of the persistent cure so the user
will put a it's Barack test test
requests on to the persistent queue and
it would be you know just an event type
of connection even base connection and
so now when build rapid worker has
resources available is up and healthy it
will pick up this work and so it would
get a drone RPC from a persistent queue
and now there were two different types
of outputs that we were producing those
the build artifacts OD the bynars that
were built and the build progress
information and so once the build a bear
worker produced those it would produce
the build artifacts and put them one by
one as it was compelling and linking on
the dependencies and build progress and
finish was like you know if we're
running tests which were which of the
test failed and which of them succeeded
on how far are we into our completion of
requests and so at this point if the
user was interested in getting the build
progress information than they could
connect to it and if either the service
that provides the progress information
or the user go away that's okay because
the data doesn't go anywhere as build
our worker produces it it just gets
persisted and so the user can come back
and figure out what happened in between
it's disconnecting and similarly for
builder artifacts if the user cares
about getting the build artifacts then
it would try to delay would connect to
the build artifacts service and get
those and I soak all the the four
components in green basically what
constitutes the build service at this
point with the old facade the API still
the same the types of things that a user
wants to accomplish what the build
servers are the same but then we changed
all the backends we change how we do
this for better robustness of the system
and so comparing them side by side this
is what we had in the old architecture
the user would push the work to the
build a bit of work directly and in the
new yorka tech chure the build a bit of
work would pull the work off of the
persistent queue and in the meantime
user could query whether the build is
executing or what's going on without
losing that I progress if it lost a
direct connection and so another
interesting thing in this point is how
they owe the control logic happens right
so in the previous architecture we had
resilience logic in the thick client
library where in order to understand a
lot of the details of what it means to
have a transient or permanent failure
did the user send a bad request and for
a father would never compile or did the
worker go away and then it stops
executing the work and that's what
happened in the user should just retry
or is in the new architecture now we
have resilience logic which is
centralized and so I know it doesn't
look like it's centralized because now
it spread over even more boxes but
conceptually now all the builds her
i owns all the resilience logic and the
user just it all sounds very simple
requests for putting the work on and
requesting some of the outputs and so we
needed to move from one architecture to
the new architecture from the old to new
and there were many challenges
associated with this so as I mentioned
previously bill rabbit sits in the
middle of the build and test tag we can
just ask users to stop building and
testing that's just not going to happen
and so what we needed to do is we needed
to migrate from the old architecture to
the new architecture and we need to do
this with no downtime downtime was just
a luxury that we couldn't have for it
and so in the next couple of minutes I'd
like to talk to you about some of the
key principles key considerations that
allowed us to do this successfully I'm
not going to go into too many details
but if you're interested I've given talk
about this at mid-ohio specifically
about how we pulled off migration with
zero downtime but let's look at some of
the key considerations in case you're
doing migration anytime soon and you
would like to know a little bit more
about it so first and foremost we needed
to ensure that we need to migrate back
ends first and so the reason for this is
you know we started hitting the
limitations of our architecture we
couldn't be supported quite as many
users as it would like and we couldn't
support the load that they're providing
on us and so we needed to ensure that
all of these new back ends in the new
architecture can withstand the load and
can also do well what they expect a girl
for the system as the system becomes
more robust more filtering and also what
that meant is that we needed to leave
the old facade up because we already
gave out our client libraries to our
users and so do you rely on particular
functionality to a promise the build
system does and so we needed to do this
with no user visible changes whatsoever
because they still expect the build
command would work is x and y used to
work and the test command use works
exact same way it used to work
they don't need to care whether now we
have multiple different services doing
the work that are used to oh it needs to
be is invisible or even better
experience and so we needed to also
focus on the mixed mode so previously
because we gave out our client libraries
to a lot of users we promised the world
supported for some amount of time
because it's unreasonable for users to
expect to keep up with our release cycle
we would like to release as often as
possible but we can't demand users to
rebuild their binary every time they
want to do this because that would
interfere with their release cycles and
they have their own system concerns to
worry about and so what that means is
that we need to bridge between the old
architecture and the new architecture
and everything in between as we were
transitioning from one to the other and
so we understood very well with the old
architecture was like we've you know
operated for some time we had much
better understanding of what the system
does i am under which conditions it
might behave wonky and in the new yorka
tech chure we thought a lot about it we
design it but and we understood where it
should go now we needed to ensure that
we can support all the transition
periods in between and so that's where
most of the energy and focus and
engineering time went in to make sure
that in between the transition would
still support all the old client
libraries and they still provide the
same functionality but now the work
seamlessly with the new backends as a
start coming up we also needed to target
launch friendly clients we couldn't just
say okay now everyone you're on your
system have fun with that and so what we
needed to do is we needed to find
specific users that would benefit most
from using our a new system and so we
could work with them directly and have
their help to tell us if something looks
bad on their end and might not look bad
on our answer when we didn't know that
there is a problem and so what that
means is that foot the three different
jobs it was actually three different
launch friendly clients there were some
clients that only cares about getting
the build artifacts there are some
clients that didn't really care about
any of that and they just wanted the
progress in
of the belt there were some clients that
really needed upgraded execution engine
to be up to the up to their needs and so
this ended up being three different sets
of lunch friendly clients and so that
meant is that we needed to decouple
launch of services we couldn't rely on
any one of the backends the new backends
to be up and so because we also had
three different a set of users that are
launched friendly users we needed to
ensure that for any of the backends that
will bring it up they relied on none of
the backends being up and running and
also all of them being up and running
and all the in between stages and it was
essential for us to get maximum
visibility into our system state it's an
expensive engineering effort to get you
know the perfect a laser view into what
your system does at all times but we
needed to have sunday we couldn't just
launched something and just not have
anything and so we need to have our
metrics up of course and we needed to be
able to query at any point of time and
answer the question is a system doing
well if it's not what is going wrong and
so there was a lot of the trade-off and
a lot of the balancing of you know we
could dump everything until info logs
right and then just worry them whenever
we suspect something bad but then you
know something is going back good luck
trying to pour all their info logs and
then you know finding what's actually
going wrong and it was just a lot of
balancing between you know what goes
into info logs making sure that only
essential information goes there but
also making sure that not just arrows
show up in the logs that if we're
successfully proceeding from stage to
stage the day I also shows up in log so
we can know which milestone where I add
and where exactly our system is stuck
and we had to practice the launches a
lot the launch that I lied which was for
the execution engine for the build a bit
worker into keying off of the persistent
cue that was that involved five teams
across three different geographic zones
and two different time zones and so what
that meant is that every single person
needed to be in the loop all the
stakeholders needed to be in
loop about what's going to be the next
step everybody's expectations needed to
be set correctly so that they knew what
to expect and that meant lots of lots of
practice launches that meant making sure
that everyone is on the same page about
what's happening next and which team
should start paying more attention to
ensure that their part is covered
correctly and so on and okay so we
create a lot of different checklists
where well we created one long checklist
that basically outlines exactly what
steps we need to take and also for
engineering for you know resilience for
people and being unable to get online at
the time when they need them or people
going and leave with we want to make
sure that everyone knows what's going to
happen in order to make the launch
successful and there are also meant that
we needed a solid rollback plan if we
were only three out of say hundred steps
of the launch in what does it mean if
things go bad how do we roll back and
sometimes it was as easy as saying you
know take the reverse of the operation
that you did and sometimes that meant
that the rollback was not trivial and we
needed to ensure that the right systems
go back up at the correct time and so on
otherwise the system will get confused
and it would be in the mixed mode that
we didn't even for see you it could be
an so a lot of you probably most likely
might not work on distributed build
system but I wanted to share a couple
parting thoughts that hopefully you'll
be able to use when you work on system
and as you're building large-scale
systems not necessarily for blonde
distributed systems but any kind of a
system and as you're planning the
migrations and as you applying
evolutions of architectures and
specifically I would like to focus on
distributed versus centralized control
plane so control plane is just a fancy
way of saying all the control logic
related to you know if this happens what
what's next and so specifically let's
talk about the distributive versus
decentralized control playing I touched
on it very briefly earlier so let's look
at a little let's discuss it a little
bit more so with a distributed control
that's what happens when
we had an old architecture where our
design was worked well for one machine
so for the CLI two and one desktop and
then we scaled it to work on many
different machines and to be able to
send the bill to remote server many
different remote servers and so a lot of
the logic ended up living in the thick
client library that we gave out to our
users and so we needed to support all
the different versions of the client
libraries that we gave out and what that
means is that it was very easy well
comparatively easy to scale that up too
many machines but then the bigger
overhead was during the evolution of the
system because now we had all this
overhead we needed to keep in mind all
the different previous implementations
that we supported and make sure that
previous client libraries that relies on
them still work as expected the opposite
of that is having centralized control
which is what happened when i had the
build system created and so it was a
distributed design from the very start
who were anticipated exactly which
component will be responsible for what
and what happens when one goes down and
how do we compensate for that and what
does fault tolerance mean that context
and the bigger overhead was during the
design phase we learned a lot from our
previous lessons but it is still we
needed to make sure that we design it
correctly because once the design starts
getting implemented and we realized that
we haven't considered something major
that would be a big problem that's
essentially just throwing away all the
work um on the opposite the upside of
that is that now we're able to evolve
our architecture evolve our application
a lot more quickly because we own a lot
of the control logic in our system and
the user has very simple way of
interacting with our system and so
things that we do can only improve it
but they don't have to worry about the
details of implementation and I don't
really have a thick client library in a
morgue they'll have a very much thinned
out version of that library so this is
all I have for today
about how we built a distributed build
system at Google scale I believe we'll
have a few minutes so I'd love to take
your questions thank
yes go ahead so the question is as I
mentioned we have a monolithic
repository does that mean how a search
and all of our different products are in
the same code repository yes that is
that is what I meant yes it is correct
which is actually pretty fun sometimes
to browse around and see how you know
people use and implement certain things
is like oh I didn't know I cirsium did
that yes right so the question is given
that we support it's a polyglot code
base I do the client I just put the
request so the request is abstract away
right so it's not language-specific we
use one build system which was open
source as basil so everybody describes
their dependencies in that way but the
build system itself supports all the
different languages all the major
languages that are Google supported like
Java C++ Python go and then I think
there are a couple others that have lost
presence but are still supported by the
build system yes and so all of them now
irrespective so like we don't have you
know different like maven and people
something talking together now all the
clients use one language one protocol
with which they express it but then
their code base their source code will
be in different languages and we can do
that and we can compile that and think
it does it okay go ahead
you're breeding at least some elements
of a tool chain which you're
distributing at scale presumably could
you talk a little bit about how you do
that with 30,000 developers when you're
trying to push out the tools that you're
creating okay so the question is could I
talk a little bit more about how I you
know since we're distributing the client
library to all the different engineers
how do we do that so once we compile the
client library we put it somewhere we
have a way for clients to pick up the
latest client library if they're
interested and it's basically it's a
repository of packages where they can
pick it up and use it like by just like
rebuilding their binary importing to the
latest so now the so that was in the old
architecture with a new architecture
there is actually to simplify that a lot
more or it's just the commands that can
send and now so we would can basically
extend this experience of you know
submitting in the command and then that
could go into the build system but the
basic idea is you know once we have the
client library that we would like our
users to use we put it somewhere and we
tell them anytime maybe if you want to
update your library you specify the
version number and then it would get
picked up so we have like a whole
different part of the building for
structure develop infrastructure that
deal was for that
the question is what's their operating
system unveiled machines i think i
believe it was talked publicly it's on
linux based the question is are they are
identical I mean it's a tough question
like are they you know on the surface
the year old linux-based operating
system but the Devils in the details and
how we do all the upgrades and how we
you know make patches to all the
underlying things that go into the
operating system yes go ahead okay so
the question is for the source code
repository is that gate or something
else so Rachel potvin goes into lot more
detail in her talk at the conference
code at scale it's actually it was based
on perforce but now have our own which
is called piper so you can learn a lot
more about that in her top from her talk
listen s is your hands raised so the
question is it is the build system
stateless and so if you send it to
different requires that would preserve
some state in between those two requests
right so to clarify if the two different
requests are given I would produce the
same result so we strive for repeatable
builds as I mentioned earlier and so
what that means is if the request is to
build at a given revision in the source
code and all the different variables are
the same it will produce the same result
so the build output outputs will be the
same the artifacts must be the same
otherwise it's hard to tell like which
of the versions of the build have I
produced that is there is correct which
one should I rely on so we do provide
repeatable build and then you know
obviously if it's building a different
versions then it's going to be different
if the changes in the commits affected
yes go ahead
so the question is this google keep
internal dependencies internally the
dependencies that it might have an open
source software I can't talk about that
today but there's a there's a lot of
different security concerns related to
that and because our motto is just not
to have you know a central repository of
artifacts but to build from source then
that influences a lot of our decisions
you know instead of just saying like Oh
from lining and just download editor
given thing at the given version any
other questions yes go ahead so the
question is does the build system build
a OS and Android apps so build system
built everything for google so every
single product that you know about that
Google has built built this build system
builds it and and so they build system
that builds as it plays and then build
rabbit just provides the layer on top of
that and so anything the blaze supports
build a bit support it see any other
questions I think I might be able to
take just one
mm-hmm so the question is when defining
a project or do we define the artifact
that it depends on do we find it covered
so you will find like the actual source
code so do I showed earlier and make
fire basically it's like the target
defines and then how to produce the
target so the build language you can
look on a beso de yo it actually shows
the example of a build file it would
basically specify very similarly a
target and then the list of dependencies
and those lists of dependencies are also
targets themselves and so the build file
would specify how to generate those and
so on and so then by specifying that
then the build system will be able to
say have I built this target before and
if I have then they will be able to just
link that directly without having to
compile that so it's all based on the
target we don't store separate
repository artifacts but it is in a
performance optimization that a build
system is able to do by recognizing
water has built and no one has yet to
compile
mm-hmm right so then so that part is
about the components that I talked about
so the question was about you know if
the library gets updated some dependency
gets updated how do I make sure that I
have the latest one and that's something
that we give the power to the library
teams where they can now specify and
then the source system will be able to
handle that correctly and then we'll
build a bit will be able to create the
local view of what the sort of source
code looks like at that provision and so
if the library team released something
and in the morning I come in it'll get
picked up if I'm building at the version
that is past the time when they built it
great so that's all the the total time I
have today but i'll be around here if
you all want to get doughnuts later and
ask me questions as we go get Downers
thanks for coming thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>