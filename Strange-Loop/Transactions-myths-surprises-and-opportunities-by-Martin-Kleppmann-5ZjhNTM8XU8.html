<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>&quot;Transactions: myths, surprises and opportunities&quot; by Martin Kleppmann | Coder Coacher - Coaching Coders</title><meta content="&quot;Transactions: myths, surprises and opportunities&quot; by Martin Kleppmann - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Strange-Loop/">Strange Loop</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>&quot;Transactions: myths, surprises and opportunities&quot; by Martin Kleppmann</b></h2><h5 class="post__date">2015-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5ZjhNTM8XU8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody thank you very much for
coming my name is Martin Clapton I would
like to talk about database transactions
and when I proposed this talk I went
through a few potential titles that I
could use and eventually settled on
transactions myths surprises and
opportunities because you know it at
least has a bit of a feeling of optimism
and they're my actual working title for
this was more something like
transactions Joy's challenges and misery
with maybe a subtitle of consistency
guarantees you want consistency
guarantees so you see looking at this
topic has driven me a little bit insane
and it's possible that the forbidden
knowledge in this talk will drive you
insane - so this is a little experiment
to see if I can drive a few hundred
people insane within the space of about
40 minutes so this what came out of
certain research I did for this book
that I've been writing for ages and
which still isn't finished but this is
the talk is kind of an extract of
Chapter seven from this book called
designing data intensive applications
you can find chapters 1 to 8 of this in
early release FS data intensive dotnet
if if you are so interested anyway when
we're talking about transactions
I should really briefly go back into the
history of where they actually came from
and the origin is kind of back in the
mid seventies when at IBM they were
building what is now regarded as the
first sequel database called system R
and this really laid the ground for a
lot of subsequent relational database
systems and if you look at the style of
what all the common relational database
is now do a lot of the ideas there can
be traced fairly directly back to system
are actually and the idea of
transactions that that you got in system
R was then kind of kept for the next I
don't know 30 years or so until around
the late 2000s or so dis no
cool things started becoming a a big
movement and then of course the
inevitable counter movement came back
again where people called it no sequel
but a new sequel whatever that means but
really these these things are less about
sequel and more about transactions I
have the feeling so exactly which query
language you using in which data model
is kind of relevant but really the
transactional guarantees of the
databases are what most differentiates
these different technologies and so of
course if we're talking about
transactions we're talking about acid
which has been mentioned many times but
I do actually want to unpack what this
means in a little bit more detail
because we kind of generally think we
know what it means we think we know it
means atomicity consistency isolation
and durability but what do those words
actually mean and if you look at in a
bit more detail you kind of realized
that a bit vague Eric Brewer put this
very politely when he said acid as a
term is more mnemonic than precise which
is a polite way of saying it doesn't
really mean anything precise it's just
kind of this marketing term that we use
but nevertheless there is a certain
amount of like definitions associated
with the things that start at the back
with durability that's kind of the
easiest back in the day durability meant
you wrote your transaction we do log to
an archive tape and because you know
discs were not that reliable and so you
could be sure that whatever you wrote to
the tape you would simply append all the
transaction logs sequentially do that
tape and if your discs died you could
replay the tape and thus restore the
state of the database but then of course
tapes fell out of fashion we got discs
instead and then people started defining
durability as meaning of the database F
sinks to disks so that you don't lose
data if something crashes and then more
lately that's kind of been replaced with
replication as a new way of thinking
about durability but that's really all
I'll say about your ability otherwise
this there's nothing too interesting
there how about the seed the consistency
now it should hopefully be clear that
the C and acid is absolutely not the
same as the C in the cap theorem if
somebody comes up to you and says we
can't have acid because cap theorem says
so then I suggest you very quickly run
in the opposite direction because you're
probably not going to have a very
productive conversation so I wrote this
paper about cap in analyzing it in a bit
more detail if you're interested in it
we discussed it last night I'm not going
to go into detail today so this C in in
in acid is kind of an odd one in that it
doesn't really have the same kind of
structure of property as the a the I and
the D do Joe Holliston suggested that
was kind of basically tossed in by the
authors to make the acronym work but
they didn't really as so they didn't
really attach much importance to it so C
is now generally associated as the
database the transactions the
application executes on the database
move the database from one consistent
state to another where consistent state
is defined through it meets certain
invariant and those invariants could be
expressed as integrity constraints that
are checked by the database but they
might be implicit as well so like in a
accounting system you might have the
invariant that all of the credits have
to sum up to all of the debits or
something like that and in some systems
you can express that kind of invariant
declaratively and others not but but
generally the odd thing about C is that
it's really a property of the how the
application uses the database it's not a
property of the database itself so
that's why we won't really go into
consistency into much more detail either
what about the a a about atomicity this
is a kind of confusing one because in
the context of multi-threaded
programming if you're talking about
concurrency there you have things like
an atomic increment and that means that
you know you can have multi multiple
threads incrementing some value and
you're not going to have lost updates
due to reading delay writing
but in the context of acids a the
atomicity is not about concurrency
confusingly the area concurrency is
dealt with under i and the isolation
we'll come to that in a minute
so here in the context of acid atomicity
is about how faults are handled if
things crash or things go wrong what
atomicity provides is that you're going
to get either all or none of the rights
that happen during a transaction so you
can think about this for example like
this say some timeline here time lose
moves from left to right say you've got
an auction website and somebody has just
won an auction and so you want to do
several things now with the winner of
that auction you want to update the
listing to say oh the buyer is the user
who bought it and you probably want to
send the buyer an invoice and maybe some
other things as well and now those are
several rights that have to go to
different places and what happens if
something crashes halfway through making
those various changes you could now end
up in some inconsistent state where some
changes have been made and others not
and the whole point of atomicity is that
if you wrap these things in a
transaction you can say okay that the
incomplete changes due to the aborted
transaction will be wrong will be rolled
back and won't be seen so fundamentally
here this means transactions are about
manipulating multiple objects as one but
doing that as one unit so if somebody
has transactions things they have
transactions but actually it's just
operations on a single object like a
single document in a document database
for example that's not really a
transaction because they're the whole
point is that you can wrap several
different mutations in in one logical
unit and if something fails rollback any
partial modifications that you've made
so if I was reinterpreting the
terminology rather than atomicity I
would actually call this a portability
because that's really the key feature of
a transaction is that there's always the
possibility that have can abort
and there's well-defined semantics of
what happens when it aborts namely all
the rights that it made all the side
effects are null and void and rolled
back so the nice thing about abort is
that they allow you to collapse a whole
range of different failure classes down
into just one thing that you have to
handle so it doesn't matter with whether
the problem is a deadlock during do two
concurrent transactions locks or a
network problem or maybe some integrity
constraint is violated or maybe some an
application server crash or maybe a
database server crashed or somebody
yanked out a power cable doesn't matter
all of these things all just collapse
down to a single thing that you have to
handle which is an abort and you could
for example then retry the transaction
so that's all about atomicity so let's
move on to the isolation which is kind
of the most tricky of all of these which
is why I've left it to last if you beat
the database textbook it'll say that
when you mean acid
isolation you mean serializable
isolation which is a rather strong
definition of isolation it means that
the effect of all of their transactions
is as though they had executed serially
one after another on the database that
is each transaction feels as though it
has the entire database to itself even
though actually what other things might
be executing concurrently but those
concurrent things are never visible to
the to the transaction but in practice
there is serializable isolation but
there's also this whole raft of
different isolation levels and the
history of DS can actually be traced
directly back to your system are where
you know they've they first implemented
serializable isolation and I realized
it's kind of slow so what do we do well
it's fiddled with our lock
implementation a bit and let's just
change some of the locks so that they're
not held for so long and then then let's
give those those levels with different
lock configurations names and that's
where these names come from but really
the definition of these is in terms of a
1970s they
two bases implementation details that's
why these things are so hard to
understand because they're they're not
defined as something that makes should
make intuitive sense
they're just defined as implementation
details of a particular database so is
there anyone here who thinks they can
explain off the top of their head
precisely what is the difference between
read committed and repeatable read
anyone want to volunteer so like if the
assembled bright people of strange loop
cannot tell the difference between these
different things even though I assume
all of you will have used a relational
database at some point so in theory you
should make an educated choice about
what isolation level to use but in
practice you don't understand what the
difference is between the isolation
levels so how are you supposed to make a
choice so let me look at this a bit more
detail here's a wonderful list that
Peter Bayliss and his folks Berkeley
made looking at what is the maximum
isolation level but various databases
support and what is the default if you
don't change the default and if you look
at it the maximum level is serializable
on only about half of them so half of
the database don't even support
serializable isolation and serialize
abilities which is the default in only
like three out of twenty or so so this
this means you have a lot of this
complexity do you actually think about
if you want to write an application
using one of these databases that
doesn't have race conditions let me go
into the differences between these in a
bit more detail because I think it's
actually quite useful to look at some
examples of race conditions that can
occur at these different isolation
levels and thus we start then unpacking
a bit more what we actually mean with
isolation so I'll start with read
committed which is actually the fairly
easy one to understand and it kind of
means mostly what what you think it
should mean sorry I skipped over slide
there it's
recommit it means that two phenomena two
anomalies are not allowed to ever occur
so these two things are prevented by the
database
those two things are dirty reads and
dirty writes dirty weeds is fairly
simple it simply means that one
transaction reads data that another
transaction has written but not yet
committed so that's what we'd committed
means yeah you only read the data once
it's been committed that's fairly clear
dirty rights is maybe not quite as
obvious and imagine you have this could
be again the auction website example for
example so you've got two transactions a
and B and they are writing to two
different keys and key value store two
different rows in a relation database
database whatever and they're called x
and y and they concurrently write to
both of these and the idea is that you
end up with either X being a and Y being
a or you end up with X being B or Y
being B those are the two things that
could occur in a serial execution but in
this execution you actually end up with
X is first set to a then set to B and
the final value is B whereas Y is first
set to B then step to a and the final
values a so these two are now
inconsistent but we'd committed prevents
this so this is already one type of race
condition that you don't have to worry
about but we have all of these other
things so what what do they actually
mean there there must be some other
kinds of race condition that can occur
at we'd committed which these higher
isolation levels prevent let's have a
look at this one it's called read skewed
and the example here is say this could
be an accounting system or a bank or so
and you've got two accounts called x and
y and they have a starting balance of
$500 and now you've got a transaction
that wants to transfer $100 from one
account to another and so the way you do
that is well you decrement the account X
by 100 and you increment the account Y
by 100 and now this is fine and you end
up with 406
now consider you have concurrently
running a read-only transaction it's
only reading the data but it's not
changing anything and you know maybe
it's a backup process or maybe an
analytics query or something like this
which needs to look at lots of different
keys in the database and at some point
in time it reads why and it gets 500
back and at some later point
it reads X and gets 400 back and it's
valid for it to get 400 back because at
the time when it's reading X the
transaction that did the transfer has
already committed so it's reading
committed data this is fine but we've
now got this this weird inconsistency
where originally there was a thousand
dollars in total in the system but to
this orange process here it looks as
though there are only 900 dollars in the
system so it looks as though 100 dollars
have somehow vanished into into midair
and so if this is indeed a backup well
you've got now an inconsistent backup if
you restore from this backup your
database is going to be meaningless
because you've seen different parts of
the database at different points in time
and this can happen on the read
committed and note like we'd committed
is the default isolation level for quite
a lot of those databases here and in
order to prevent this read skew anomaly
here repeatable read and snapshot
isolation were brought in there are two
fairly similar looking isolation levels
on from a user's point of view
implementation wise they're very
different in many cases when databases
now say repeatable read they actually
mean it's natural isolation just to be
confusing and they're basically two ways
you can prevent read skew one is by
taking lots of locks and this is what
sequel server does for example if you
use repeatable read and this is what
system our data back in the day but more
common nowadays is the snapshot
isolation which is implemented as
multi-version concurrency control
MVCC for short so this is what Postgres
does and my sequel does and sequel
server if you use the snapshot mode and
Oracle if you choose the serializable
mode note this is not so
but but they call it that anyway so the
purpose of a snapshot isolation is if
you're a transaction that is reading the
database you see the entire database as
it was as of one point in time so even
if other thing other rights are
subsequently committed that read
transaction even if it runs for a while
is not going to see those and that's
where the multi version comes in here
that means the database internally keep
several different versions for a data
item and it'll present each transaction
with the one that's appropriate to like
it's time cut here okay but we're still
not at the top there's still a
difference here between snapshot
isolation and serializable so can we
find another example that lets us
distinguish these what do we need to do
now imagine you have a say an emergency
dispatch service so like an ambulance
service for example and the service
needs to ensure the invariant that
always at least one doctor is on call
because if no doctors on call you know
no ambulances can go so there can be
multiple doctors on call that's okay and
doctors can't rate their shifts so if a
doctor is not feeling well they can ask
a colleague to take over for them that's
no problem and now this is the
transaction that will get executed if a
doctor wants to go off call so they say
I'm not feeling well I click a button on
the web interface saying can someone
else take my shift please and this
transaction will first check how many
doctors are currently on call and if
this number is greater than greater than
or equal to two then it's safe for that
doctor to go off call so then the
database can be updated and say the
doctor is no longer on call it's a very
simplified example but it's enough to
show a particular anomaly occurring so
in this imagine you have this database
and there's three doctors Alice Bob and
Carol and Alice and Bob are currently on
call and Alice says oh I'm not feeling
well
can I go off call please and the
transaction says yeah okay we have two
doctors currently on call it's safe for
you to go off call
so she gets updated to on-call false and
then concurrently Bob does the same
thing he's also not feeling well and so
but his transactions
also sees two doctors on call and now
both go through both meet their their
condition and now you've got a database
in which no doctors are on call and are
are invariant has been violated but this
problem is called right skew and it
really does happen I made a little test
case that shows in which databases under
which isolation levels it happens and
for example in in Oracle you simply
cannot prevent this unless you build in
explicit like locks or materializing the
conflict in some way where you really
have to think about what you're doing
there so just hope that your ambulance
service in your town is not built on
Oracle I guess but the the general
principle of this right-skewed can
happen in many different situations the
general principle is that you have a
transaction which reads something and
then make some decision based on what it
has just read so in this case the
decision is are there enough doctors on
call and then it writes its decision to
the database but different transactions
may write their decision to different
places in the database and so they're
not going to conflict so just a simple
lock is not going to pick up the facts
that those transactions conflict and the
problem here is that with concurrent
execution by the time a transaction
commits the premise of the decision that
this transaction made a premise may no
longer be true and this would then be a
serialize ability violation okay so so
we've worked our way up to serializable
and we've realized that there's some
fairly subtle but important race
conditions that can happen at these
various different isolation levels so
how can we actually implement
serializability well for 30 years or so
the only real answer was use lots and
lots and lots of locking and this
technique is called two-phase locking
I'll abbreviate it here with two PL and
the way that generally works here is
this so
whenever you read something you then
need to take a shared lock on all of the
stuff that you've read and the shared
lock doesn't stop other transactions
from reading it but it does stop other
transactions from writing it so you can
then be sure that once you've got that
lock and you hold that lock until the
end of the transaction then nobody is
going to come and modify those things
that you've read and the important thing
here is really that you do have to hold
the lock until the transit end of the
transaction which leads to some serious
performance problems say you've got a
transaction that needs to read all of
the rows in the database like a big
analytics query for example would scan
everything well that needs to now take a
shared lock on the entire database which
means that nobody can write to the
database until that transaction has
completed you're basically locking the
entire transaction the entire database
for right and this is why all of these
weak isolation levels came into
existence because people said like
there's no way I'm going to lock my
entire database for writes it's just not
going to happen
fortunately better techniques were
developed so in the late 2000s kind of
two competing approaches came up as to
how we could implement serializability
without two-phase locking and I'll run
through each of them briefly so the
first example is from H store which was
an academic prototype but then became
vault DB the second example is from
Postgres so the first example is
literally you can simply take all of the
transactions and execute them in serial
order one after another or it sounds
stupidly simple and obvious but actually
- it's a really good way of making sure
that your transactions are serializable
because by construction they are there's
no concurrency going on so you simply
take what would normally be these these
various interleaved threads of execution
and turn them into a timeline of course
this is only going to work if you make
each transaction really really fast to
execute and this is why it took 30 years
to get to the point of you know of
course theoretically people would have
been able to do this for a
but if you want to actually execute them
in in serial order each transaction has
to wait for all of the preceding ones so
a few things you need to do there is you
probably need to have all of the data in
memory because waiting for disk to come
back is just going to be too slow if you
want to execute serially and you
probably also need to take the entire
transaction and ship it to the database
others a stored procedure where the
database can then just quickly execute
it as soon as it's its turn you're not
going to have an interactive client
server slowly back and forth over the
network conversation because that would
just take far too long but like this if
you can get your transaction executed
within I don't know 100 microseconds
then you can actually get some
reasonable throughput and so this is the
principle that's used in multi B John
hug gave a good talk about this
yesterday so if you didn't see that you
can check out the video they told Mike
uses this is a similar transaction
commit principle as well I think ok so
that's one way of doing it
the third way which I find quite
interesting is you can detect any
serialization conflicts and abort
transactions this is what this technique
is called serializable snapshot
isolation so it's based on snapshot
isolation which we saw earlier but adds
additional checks to it in order to make
it serializable and this is what is used
in Postgres
since version 9.1 and in the first
instance it actually looks a bit similar
to two-phase locking that is you have
locks that you take whenever you read
something and whenever you write
something but the difference is that
those locks don't block so if somebody
wants to write to something that has
already been read and there's a read
lock there that write is not blocked it
can still go through these locks just
record everything take a little ledger
and brick write down oh okay so this
right from this transaction appears to
conflict with this read from this other
transaction and just records all of
those fat
and then at the end of a transaction
when it's time to commit an analysis
process happens and it looks at the
various conflicts that occurred and
decides is it safe for this transaction
to commit and if there's the possibility
that this transaction will introduce a
serialize ability violation that
transaction simply aborted and it has to
retry so this is an optimistic way of
ensuring serializability you're assuming
that the conflict rate is not going to
be too high because you've got a lot of
contention then you can't have a lot of
transaction on boards and restarts but
if contention is not too high then this
this can work very well in contrast
two-phase locking is the pessimistic
approach so it makes sure that you can
never ever get into a situation where
where transactions observe non
serializable outcomes but at the risk
upper the price of preventing a lot of
concurrency okay so that's our overview
over these different serialization
levels there's still more to say I think
so a kind of a big elephant in the room
I feel with all of these transaction
models and that is all of them work just
within a single database and generally
that database is assumed to be in one
geographic location but we're building
bigger systems than this now what about
if you're doing micro services and
you've got several different services
each with their own databases what if
you're doing stream processing and so
you've got these these flows which take
in input streams produce output streams
what do transactions mean in this kind
of world so we've got here these various
kind of boxes and there's a cylinder
inside each box and the idea generally
what I've heard recommended with micro
services is that you don't try to share
a database between several different
services because that would break a lot
of the goals that of introducing micro
services in the first place so you've
got these these records of application
code around around each database
and if you're doing stream processing
well the picture actually looks very
similar the difference is only with
microservices these arrows here are
request response like RPC or rest type
data flows in stream processing there
are queues and message brokers but
otherwise the structure is very similar
here and generally what people do what
seems to be the recommended thing is to
draw a box around each of these services
and say this is a transaction boundary
transactions only occur within one of
these and if it's using a transactional
data store sure it can use whatever
facilities that datastore provides but
there's not going to be any transactions
across different services and that's a
very good reason for that recommendation
and that is that if you want if you
wanted serializable transactions across
multiple services you would have to run
some kind of atomic commit protocol so
these are distributed systems protocols
which make sure that if you've got
various different parties who are all
participating in a transaction various
different services either all of them
commit or none of them commit and
protocols such as two-phase commit three
paid commits transaction managers etc
they implement this kind of thing
however as we saw earlier the whole idea
of serializability is that transactions
appear as if they were in a total serial
serial order now you can you can execute
literally in serial order or not but
that order is definitely defined so
whenever two transactions conflict
somebody has to decide which of the two
came first and this from a hey stop stop
stop
from a distributed systems point of view
this makes it effectively an atomic
broadcast problem that is the problem of
getting a bunch of messages to different
nodes in exactly the same order but the
issue with atomic broadcast is that from
a theoretical and also practical
perspective it's actually equivalent to
consensus and there's their deep connect
between all of these three between
atomic commits between total ordering
and between consensus and consensus we
we can definitely do but it's pretty
expensive to do it requires a lot of
coordination between different services
because we've got to get them to agree
on something and people tend to not like
doing things like two-phase commit in
services because it makes them very
brittle very sensitive to failures
because it only takes one service which
is running slow or which is temporarily
not responding and then suddenly
everyone else can't get any work done
either so it tends to amplify failures
this coordination mix system is very
brittle whereas the whole point of
wanting to introduce several services
was to make them more isolate more
decoupled from each other so you could
have independent failure domains don't
think it's even more fun if they're
actually distributed geographically
because if you're trying to do consensus
over a wide area network with very high
latencies with very variable Layton sees
a lot of the current consensus protocols
really don't do a good job at this
that's why people recommend run
zookeeper only in one data center for
example don't try to run it across
different data centers so we seem to be
stuck in this place where there's no
transactions across different services
but what do people do instead so what
I've seen recommended in the context of
micro services is well you might have
compensating transactions for example
that is you try to do something and and
the service does it and then you do
something somewhere else and that fails
and now you have to say back to the
first service Oh actually could you undo
that again please because this other
thing that was also supposed to happen
didn't happen and so now in order to get
back into a consistent state you kind of
have to do some cleanup afterwards which
looks somewhat like transaction abort
and roll back except that here
implementing it at the application level
rather than it being implemented by the
database in the infrastructure look at
sagas for example there's there's a
paper describing
this kind of stuff what do people do
about integrity constraints in this
world so say you have I don't constrain
like you can only ship as many items as
you have in the warehouse what happens
if you accidentally accept more
purchases than you have items in the
warehouse well you can't have to
apologize to some of the customers and
say oh sorry actually we ran out
he's a discount code for your next
purchase and we've refunded to you or
something like that and this is
reasonable like businesses do this all
the time but you can think of this these
kind of apologies really as you have
some integrity constraint but you don't
enforce it upfront instead you check
after the fact whether the integrity
constraint was violated
and if it was violated then you fix it
up again so bringing this back to the
asset terminology it seems to me like
these compensating transactions are a
bit like the atomicity aspect
we're kind of implementing aborts at the
application level and doing apologies is
kind of like the sea
of acid in that we are implementing
integrity constraints and validating
invariants again at the application
level so this kind of leads me to
thinking you're going to hate me for
this this leads me to thinking that
every sufficiently complex and large
deployment of micro services contains an
ad hoc informally specified bug-ridden
slow implementation of half of
transactions
I have no evidence whatsoever to back
the self I'm just like asserting it
boldly so what about the ie in in acid
so we I said like compensating
transactions are kind of like at emitted
implementing atomicity and apologies are
kind of like implementing consistency
what about isolation so take another
example say you've implemented a social
network and I don't know some one of
your users has just broken up with their
significant other and they're really
unhappy and so they go to your website
and they unfriend their there are
previous their ex girlfriend or
boyfriend and now after that they send a
message to all of their friends saying
how horrible this person is and the
intention of the user here is clearly
that their ex-boyfriend or girlfriend
does not receive that message because
they first did the unfriending and then
they posted the message so like if I was
a user this would make sense to me in my
mental model of the world except now if
you've got the friends database handled
by one micro-services service and the
posting database handled by another
micro service this ordering information
between the two info this two events is
lost so you could have for examples a
notification service which listens to
these and due to some transient glitch
in a network or so the unfriend event
arrives later at that notification
service than the post event so by the
time the unfriend event arrives there
it's already sent notifications to all
of the people including the ex-boyfriend
or girlfriend who was not supposed to
get the notification so the problem that
we have here is that there's a causal
relationship between these two events
and that causal relationship is it's
implicit the user hasn't explicitly told
us that there's posting a message to
friend depends on the unfriending event
it's just implicit through their
timeline but in this microservices world
we somehow lost that information now
this leads us to some interest
area of speculation and all research
which is a similar thing where at the
moment the way we're building micro
service based systems is just pure
eventual consistency
hopefully it's at least eventually
consistent maybe it might be perpetually
inconsistent who knows but we can we can
achieve eventual consistency
hopefully we probably can't do
serializability because as I said that
requires you to do two-phase commit a
kind of variant of consensus and so it
just becomes too brittle too fragile so
we want something in between which will
still capture causality which could
still capture this kind of ordering of
events and allow people to reason more
in a logical way about this and
causality is quite a nice thing actually
because if you think about what's going
on in snapshot isolation you know where
I said you get a consistent snapshot or
at one point in time that the data that
a transaction can read from well what
does a consistent snapshot actually mean
it means that it obeys causality that is
that if you see one item in the database
and that was written after some other
item then that other item must also be
in the database that's really what a
consistent snapshot here means and the
interesting thing is this can all be
implemented without requiring global
coordination without doing consensus
there are nice proofs which showed that
this is actually actually the upper
bound of consistency that can be
achieved without coordination whether we
can do it efficiently is another
question there are other interesting
proofs which show that you get quite a
lot of metadata overhead if you want to
track causality but I think that's going
to be a very interesting fruitful area
of research can we make causality
efficient so to summarize we've got this
this hierarchy of different isolation
levels which we inherited from system R
in the mid-1970s and nobody really
understands what they mean but most
people are using something somewhere
along the Huayra
a lot of people are using we'd committed
in fact because that's the default on
about half of the databases we saw on
that list but the interesting thing
about read committed is it can actually
be implemented without coordination and
even something slightly stronger
introducing causality and that can still
be implemented without global
coordination so maybe there's some kind
of way we can still do transactions
across some kind of micro services or
stream processing architecture which
will obey causality which will obey Reed
committed but which doesn't introduce
the whole overhead and problems of
coordination there's a lot of research
going on in this area I've got a totally
oversized reference list for anyone who
wants to read papers this is page one of
the references list this is page two of
the references list page three and page
four and so you can find them all of the
slides online I've tweeted the link to
the slides already it's a big topic and
I've mostly described problems and not
described many solutions because many of
the solutions are still unknown but
people are working on them if you fancy
working on them as well please do
contact me as a final thing I wanted to
say I'm actually going to be giving away
free copies of this book I Riley have
kindly printed off 25 copies even though
it's not finished yet but of the
chapters that are there so far and I'll
be giving those away in the lunch break
at 12:30 down by the entrance otherwise
there's also a discount code if you want
the the e-book and I think I'm out of
time I'm afraid so if you want to ask me
questions I'll be around here and thank
you very much for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>