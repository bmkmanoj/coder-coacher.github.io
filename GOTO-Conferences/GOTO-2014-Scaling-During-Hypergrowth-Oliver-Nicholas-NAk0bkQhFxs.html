<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • Scaling During Hypergrowth • Oliver Nicholas | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • Scaling During Hypergrowth • Oliver Nicholas - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • Scaling During Hypergrowth • Oliver Nicholas</b></h2><h5 class="post__date">2014-10-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NAk0bkQhFxs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to welcome you to the next
session here with Oliver Nicholas ah
he's from uber and it's I haven't heard
this to presentation i'ma say but he's
basically working in the same kind of
area as the law last talk we saw in here
I'm scaling pinterest they're just
scaling uba and of course we're really
happy to have uber in town not uh that
is the cars but uh yeah we're working as
they laugh we have a couple of really
cool tech laps here of course you know
we have a Google a period I did all the
v8 stuff for now we're doing dart we've
had some VMware cool stuff going on here
for a couple years and now we have uber
as another really internationally
interesting hot spot so that's it's just
nice it's not just try for doing cool
stuff so um you know without further ado
I'll just yeah let it over to you please
welcome thank you um yeah so I shout out
to the to the uber crew here we're we're
we're working on it I think hopefully
Copenhagen is next up uh than our hosts
soon thereafter um okay so my name is
Oliver Nicholas and the the title of my
talk here is scaling during hyper growth
you'll actually I think find that it
probably dovetails pretty pretty closely
with the talk before me not the least of
which reason is that i think pinterest
and uber both shares some early
employees who came from yelp where I
came from before uber and so we've
actually taken some of our some of our
methods to those to those other
companies so I think you know scaling
under the sort of hyper growth
conditions is is certainly an
interesting challenge
you know the hockey stick exponential
curves but I when you when you sort of
reason about it most of what applies
that you want to be doing under hyper
growth is like in this sort of
technology decision-making process it
really applies during regular growth as
well and it's there are some sort of
sound principles that you want to that
you want to apply that'll really help
you especially if you do get to that
that hyper growth stage so so this is
sort of that that conclusion really is
that it's it's much easier to grow if
you've made smart decisions early on
it's really difficult to change the
wheels on a moving bus once you get any
reasonable amount of traction momentum
and it doesn't really even matter how
fast it's moving if it's moving it's
really hard to do so we're going to look
at what's kind of universal from from an
early stage and some of this is not so
deeply technical it's it's more about
decision making so Who am I and why
should you listen to what I have to say
on the subject I'm software engineer and
engineering manager by trade you know I
studied computer science played with
computers as a kid the license plate on
my car says geek AFK these are my
credentials I also do a lot of things
last minute so two months before I
graduated from college at the University
of California and Santa Cruz I decided I
wanted to learn Ruby on Rails quickly so
i got my car and I drove six or seven
hundred miles from santa cruz california
to portland oregon and i took a
weekend-long class from dave thomas
who's actually a speaker here at go to
so with my my newfound railed rails
expertise under my belt I two weeks
before I graduated college I realized
like oh I should I should get a job and
so I did what people do or at least used
to do in the Bay Area and I went on
Craigslist to look for a job and I found
this little incubator called Google Labs
and which was formed by
this guy James courier and Stan
chudnovsky who had been sort of
successful in some previous ventures I
came down for this interview and I
discovered that google labs was four
guys in a like an office a room that was
big enough for about four guys and by
the time I left two years later or so we
had moved offices three times we had
grown to 16 or 20 people and I thought I
had a pretty good idea of what growth
looked like and so I went from go to
Yelp and it Yelp I was the ninth
engineer we're about a hundred employees
total Yelp is very sales heavy more than
half of that was sales and in the five
years that I was there we grew from two
floors 28 floors in this building we
doubled in size we did over 10 X on our
monthly uniques which was going from
maybe six seven million to about 80
million at one point like earlier on
when i was there i was the only clit
early the only engineer working on the
front end for like WWE yelp com which
was cool at the time and and then by the
time I left there were like two hundred
engineers in the company so as you can
imagine by that point I I like thought I
knew what growth looked like I thought I
had a good sense of this and then I came
to Ober and it ober I was sort of hired
on as like the second engineering
manager I interviewed in an August and
twenty twelve and there were 18
engineers and then when I started like
two months later I took a couple weeks
off when i started two months later
there were 30 engineers i was like what
happened like this wasn't the team i
thought i was joining so in the two
years i've been there we've gone from
100 employees to nearly 1,500 employees
we about 250 engineers globally I can't
say what our exact traffic growth looks
like but it's if you take the five years
of yelp and squish it down into one year
it like it starts to look like what
ebers growth looks like so now I really
think I know what growth looks like so
just for those of you who aren't
familiar since we're not anywhere in
Denmark yet
much to our collective disappointment
what is uber we are we're an app based
service that connects riders with
transportation options so you open the
app it shows you nearby cars you you hit
the button to request a ride you get
sort of a an automatic algorithmic
dispatch to a to a driver who can accept
your ride they'll come pick you up they
will take you where you want to go drop
you off you get out of the car and walk
away you have a credit card on file
there's no fussing with tips or you know
paper money or anything you just you
just walk away it's pretty awesome you
feel you feel like a boss and and it is
really convenient and on the back of
that we sort of grown to 200 plus cities
in the last three years so what my team
in San Francisco and the team here in
Aarhus build our the app is the is the
sort of back-end services that this
mobile app talks to so uber was a was
founded in 2009 in San Francisco we put
cars on the road and in 2010 to date
we've collected over probably close to
one half billion dollars in funding and
then most of our employees are not in
headquarters certainly most of them are
not here in Aarhus they're out in those
two hundred plus cities running
operations locally and we're hiring here
in our hosts if anybody's interested
okay so that's why you ought to listen
to me why should you maybe not listen to
me or take what I say with a grain of
salt I was like a poor student in high
school um you know I was I was too busy
like programming at home to like pay
attention in school and you know I
haven't been doing this for that long
and neither have I managed to read a
full book this year but I've spent a
fair amount of time with sort of
successful startups and and and the less
successful ones so I've seen a bit of
that life cycle and
I've made and witnessed a lot of the
mistakes that that can be avoided and I
don't know much about big multi-thousand
person companies every day I work at
uber it's the biggest company I've ever
worked at in fact when I went when I
went to interview an uber I was
discussing discussing it with the CEO
and the head of engineering and they
were asking me you know why you why you
leaving Yelp and I was like well you
know as sort of seemed like it was time
to go you know we got to a thousand
people it was it was big enough for me I
was like I'll help you guys get to a
thousand people but when we do that it
might be time for me to go and I've had
to eat my words I've had to hope that
they don't remember that conversation
cuz that happened way quicker than than
any of us were expecting I think okay so
let's dive in what is the most important
thing to keep in mind if you're if
you're scaling an engineering team is it
is it is it having enough servers isn't
having enough RAM in your servers is it
which language you choose it is none of
these things it is you need to have a
wiki home you need to have a place to
put your institutional knowledge you
will not survive without this the this
is like becomes this becomes where
everything that's important gets written
down you put run books here post mortems
contact numbers for your co-workers
emergency information your office public
IP address your how to's your how not
tues over time almost any question that
gets asked ends up being addressable by
the wiki the answer becomes check the
wiki doesn't matter where you host it
make sure you have backups of it and it
actually matters a little bit where you
host it matters that it's not on if it's
if this is what documents your failure
procedures it matters that it's not on
the same infrastructure as the thing
that has failed this is something that
we've done I've done wrong at two
companies now um and so it's an
important thing to keep in mind so this
this is actually this was interesting i
came to uber you know there were two
years into the things 30 engineers and
there was there was
wiki and so this is like this sign that
it really pays to start building early
on it'll help you on board new employees
quicker and it makes sure that
everybody's kind of on the same page so
to speak okay so what is the second most
important thing to scale an engineering
team also maybe not what you think it's
not the server's it's not the ramming
your servers it is the people that you
have at your company you effectively
this is your most important resource one
person can only one engineer can only
write one engineered engineer day worth
of code in a given day and so if you
want to squeeze that 22 you either have
to get two people or you have to feed
your one person like those drugs that
they feed fighter pilots to keep them up
for 36 hours but that's obviously can
only go so far with that so the thing is
is that people will actually become your
primary constraint pretty quickly it is
if you're doing the right thing if
you're if you're hiring if you're being
really rigorous with your hiring and
you're not making any compromises it
will be very difficult to to find really
good people and you should not make
compromises especially early on good
people be get good people they will hire
other good people they will help you
they'll help you improve always improve
the quality of the staff and this is
really important to do early on it's
like it's hard to over stress how how
how easy it is for this to go wrong so
so you want these you want these a
players you know people who who are or
were there to you know who won't mind
well you know who won't mind they're at
the bar and that's and it's the weekend
and something you know the site goes
down and they like without you asking
them they'll have their laptop and their
mifi with me and they can get online and
I'll start fixing stuff for though they
got the ssh client on their cell phone
and they'll they'll be on the dance
floor and they're like debugging stuff
this is like these are the people that
you need early on this is you can't get
around this if you have a site you want
it to stay up it's not going to promise
you and you'll
need you'll need people who who are cool
with that and we're cool with really
sacrificing some of their time to to to
help achieve that these are people who
who prefer getting their hands dirty you
want to hire generalists generalists um
will will help you keep your head count
low it'll help you like avoid needing to
hire more people they can dive in
anywhere and this is this is this is
pretty important you can't get to the
point of hyper growth if if you run out
of money before you get there because
you spend all your money on people who
are kind of doing like half a job and so
if instead you can hire people who do
who will do one and a half jobs that
you'll be integrate it'll be in a great
position later down the road so people
can fill multiple roles but obviously
that comes with a caveat which is you
wouldn't really want to hire a
generalist to do something really very
specialized so you have to have to
balance that there depends on your
problem space and then you really on the
subject of people you really a people
person if you are like a very early of
your founder super early employee and
you don't have like a people person and
you just you're just one of those people
you like to put on your headphones and
write code if everybody at the company
just likes to put their headphones and
write code you can very quickly run into
a situation where nobody's really
addressing the the kind of like softer
side the issues that arise so somebody
who like and sort of illicit emotional
responses can have those important
conversations with people it can be
really valuable it's entirely possible
that you know if you get a staff of
let's say 10 people you could easily end
up with somebody who's like maybe not
like sort of pulling their weight or
isn't really you know isn't really
engaging or doesn't way to understand
what's going on and without a pupil
person like there may not be anybody to
surface that information to you like
it's entirely entirely possible for that
to for that to kind of be difficult to
see so this is important to make sure
you have have one of these these people
persons okay so we have a successful
sort of
we have a wiki to put our stuff in and
we have people to to build our product
what else do we have to talk about the
next thing to talk about is is failure
when you are scaling fast or really even
when you're not things start breaking
this is a this is a fact of life the
sooner that you accept it the happier
you will be this will like this will
always come to bite you and you can work
around it you can you can you can be
prepared for failure so this this
picture here is a is a wine rack that I
I built it was sent to me in the mail
and I in pieces and I like put the
pieces together you you may notice
something kind of kind of weird about it
it doesn't quite doesn't quit something
doesn't quite seem right with it um I
did not build this thing correctly I
failed horribly at building this wine
rack was it it could I yes um yes I
guess it worked as well in space as if
it were built the correct way as a fair
point but you know I was I was prepared
for this I didn't I didn't panic I
noticed the problem before I committed
too deeply to it I I I rebuilt it I
don't have the corrected picture here
but I did fix it so failing once is okay
in fact and failing that the the same
sorry failing at different things once
is totally reasonable you should expect
this to happen it's it's even to be to
be expected or supported failing at the
the same thing repeatedly is is usually
a bad sign if you don't want to get
known for this you remember like the
Twitter fail well like like Twitter got
kind of got famous for for going down
all the time this is not like this is
not a great situation to be in and so if
you if you break your wine rack write a
post mortem document your learnings in
the wiki and then the next time we'll
will likely go much smoother
you can avoid repeating your mistakes so
we have a even if you have a team of
fantastic engineers and you yourself or
perfect engineer you still have to
design for failure even if you're like
the best they come like you still have
to design for this and the reason you
have to design for this is because even
failure proof things will fail at some
point your data center will absolutely
fail you like it's great to have you
know redundant power feeds to your
servers but if a plane crashes into the
power line right outside of the data
center you're it will won't do you any
good like you will lose power to your
whole data center the servers will fail
you you know redundant power supplies
we're done in network connections raid
disks and your servers are great but if
like a datacenter tech like pours coffee
on the server like it's like or like
walks by and unplugs it it's not going
to it's not going to help you your
database will probably fail you
databases are super complicated it's
like hard enough to explain how they
work it's like really hard to explain
how they fail and it's almost impossible
to fix them when they do and your code
your own code or other people's code
will almost certainly fail you there's
this brian kernighan quote debugging is
twice as hard as writing the code in the
first place so if you're right code as
cleverly as possible you are by
definition not smart enough to debug it
the this this is probably hit all of us
at some point there's there's a another
another one I kara member the line
exactly but it's like something about
the amount of time that passes between
it's between when you're unable to
distinguish between code you wrote and
code that somebody else wrote you know
you put down a piece of code for a
couple months you come back to and
you're like what was this idiot thinking
and it like turns out it's yours you
know your your people will fail you in
some ways in ways you maybe don't expect
but also you know people burn out
sometimes or people just sleep through
their alerts in the middle of the night
this happens all the time things will go
wrong
in some ways this all becomes true or
more true over time any of you like have
things hosted in AWS none of you well if
any of you do you would have just got an
email from from amazon like in the last
week telling them telling you that
they're about to reboot half of your
servers so even things you thought you
could you could rely on you can't always
so you should you should be designing
for failure because even if it doesn't
happen when things are calm it will
almost certainly happen when hyper
growth or any kind of growth kicks in if
there are edge cases if there are one in
a million chances those are going to get
found and and they will happen there's
this woody allen quote if you want to
make God laugh tell them about your
plants so it's not in just enough to
like design for failure if you want to
save enough stress to add a few years to
the end of your life you should just you
should just start expecting it just be
just be happy when it happens so plan
for it if you think that's too
pessimistic then plan for success and
plan for your plans to fail so you can
spend all sorts that you can spend
months designing like a system that you
think will perform well let's say under
load it probably won't so you don't just
flip switches with no way to go back if
you want to you want to hear about how
that can go wrong talk to talk to an
engineer from dig from the from the
website that when they did their V for
launch you uh if you if you burn your
bridges and have no way to go back it
could be a pretty scary a pretty scary
situation so in the old days we spent a
lot of money on like really fancy
hardware that was supposedly was like
really really stable but it would still
fail sometimes and you would just be
really upset when it did fail because
you weren't expecting it to because
you'd spend all this money on something
that wasn't supposed to fail in in some
ways we've been trying to internalize
this in the industry for a long time and
I mean it's been it's been much longer
than just this but you know raid
redundant array of inexpensive disks as
it was like originally known that that's
a concept or that's that was named in
1987 so we've been at this a long time
but it's actually in some ways taken
taking a long time for us
to to kind of accept that through the
stack and so sort of to really really
expect it in our application designs and
down from there so something to point
out tests won't save you here you can't
design around this good testing and
staging will reduce your development
time catches bugs but it won't won't
prevent failures there's just no way to
do it so expect it you'll be happier for
it so how do we build a failure
resilient architecture there's a lot of
things that we could talk about that
would be very specific to let's say what
uber is done or or what any other
organization is done those are very
specific to like particular problem
spaces I want to talk about some sort of
generically useful points that that are
kind of applicable everywhere what you
want to do is we want to we want to
minimize failures with redundancy and we
want to minimize the impact of failures
that happen anyway by isolating our
systems from each other and then we want
to spread out our operating surface so
that one failure doesn't take everything
down so even if you aren't building data
centers and very few people are anymore
it's it's really important that you have
disaster recovery plans you have to
geographically distribute your sort your
operating base redundant az's that are
far enough apart that you know
necessarily a giant you know a
life-ending meteor but if you know for
instance ubers based in California
California is really prone to
earthquakes it is it's not unlikely you
know that at some point in the next 100
200 years like a major earthquake will
come through and so you really need to
have geographically distributed centers
of operations so that if something
happens you are prepared I mean there's
there's other things that can happen
that are far more likely than a meteor
strike or a earthquake you know there's
their power outages there are
there are all sorts of accidents that
can happen and then there's stuff that
happens even outside of there right the
internet is a is a complicated web of a
place and you know internet routes can
kind of just stop working you want to
have a way to you want to have a way to
route around that so you know the sort
of the naive implementation here that's
totally reasonable is sort of just a
failover architecture right you have a
hot master data center then you keep
some some read slaves and your other
data center and then if the master goes
down you just flip over your DNS promote
your masters and and you're good to go
you're right another data center and
then and then from there you can sort of
get into some of the stuff that for
instance that uber is getting into with
trying to run you know active out of
multiple data centers at the same time
and this is something that's really
encouraged I think by by the design of
things like like Amazon's ec2 where you
sort of assume that maybe individual
components individual nodes or less
reliable network connections or less
reliable and it sort of encourages you I
think to build these these more
redundant systems so you also designed
for failure at your server level you
know just just by default you should
have rated disks so that losing one disc
doesn't like just doesn't ruin your day
you can isolate yourself from having a
ruined day really easily and you have to
you know you have to have backups hot
backups of your databases and you have
to have to be testing these this
promotion process of flipping over from
for masters slave it doesn't you know it
doesn't do you any good to have these if
you don't know what to do when when the
system goes down and it also doesn't do
any good to have it if or it doesn't
even do any good to have tested it if
the procedure is forward on your wiki
that we're on the architect
infrastructure that just went down so so
write a road and book have a run book
section of your wiki and put this stuff
there and then you really want to like
you really want to like distribute these
things like it's you have to realize
that sort of every unit of function
every every piece of hardware can fail
every kind of functional unit can fail
so you want to distribute these things
across racks so that rack failure is
don't affect
and then if you can you want you want to
put you know dual nics in your in your
servers so that even even that
top-of-rack switch failure doesn't
doesn't actually take anything down so
we're doing here is we're really like
where we're assuming that anything can
fail and we're adding like and one extra
layer of redundancy so that if it does
fail it doesn't actually take anything
down but we're going to design it we
want to design it all the way through so
that even if it does take out that one
server that one rack or that one data
center we can still we can still
continue operations so you also want
design for failure at the server service
level of the process level you want your
processes stateless wherever possible
you really want to maintain state and
sort of separate systems you want to be
able to you want to build a shift around
worker processes right if you have you
know one big database machine and ten
service machines you know app machines
the likelihood is that one of those app
machines is going to fail first and so
you want that to be as low impact as
possible and then you need to assume
that everything that isn't your in
process code is unreliable anything that
goes across a process boundary can fail
anything that's even in another thread
in your process can fail the disk on
your on the server is like it's pretty
reliable but like you know maybe not and
then anything that's across a network
boundary like you may as well be you
know like you may as well be sending
mail to Somalia like you have no idea
what's going to happen once you start
going out across the network to make
sure you got you got timeouts on these
things you make sure that you're
essentially prepared that kind of any
one of these systems can fail if you
have a job queue at a uber we run celery
which is a Python process or Python the
system there's also gear man as a few
others if you have jobs in this queue
that talked to like a third party and
that third party their service goes down
or randomly gets slow what's going to
happen is like each one of your workers
is going to you know it's going to chew
up normal processes and get to this this
process that talks to this dead service
and they're all just going to back up
and then suddenly your whole queue just
stops processing anything and then you
end up with this big backlog
and and it's just like it it's as
failure of your system and so you have
two segments you have to isolate in
segments your code that relies on
external systems and this is going to
keep in mind if you're doing some like
implementing facebook login or something
right I think it's considered really
reliable now but certainly in the early
days it actually wasn't and so suddenly
of the system where nobody can log into
your website just because this external
like third party system is down and so
along those lines you really want your
functionality to degrade gracefully if
something isn't accessible it's I think
they've talked about a bunch now on that
the Amazon Web homepage which is very
they have a very sort of
service-oriented architecture if each
each box on that page is a call to a
different service and if one of those
sub services and in each of those sub
services has a defined SLA and if one of
them doesn't defy doesn't return within
that defined SLA you know couple hundred
milliseconds you just don't see that box
on your home page for that load and it's
like whatever it's you know it's a
slightly degraded experience but the
home page still loads and some set
amount of time so along those lines you
really want this service oriented
architecture this is um you know I think
this is a concept that's that's a has
been gaining steam for a lot of years
now and it's it's it's it's sort of time
it's like these old monolithic systems
are our and ought to be sort of falling
out of favor having one big code base is
awesome for a bunch of reasons reef
actors really easy everything's in one
place and you have to deal with get sub
modules or whatever but having 100
engineers work on a single code base is
a is a total nightmare after a certain
point you can forget about daily deploys
and if you want to like really frustrate
your early engineers and convince them
to leave you tell them that they can
only deploy once a week this is like if
you go when you go from being able to
just like deploy whenever you want to
you know having the schedule and all
that stuff that gets really frustrating
for people I mean it in a truly large
scale if you get to like Facebook or
Google size or something good luck but
even even your version control system
probably starts failing on a one giant
codebase you know both Google and
Facebook have had to essentially rewrite
you know get mercurial whatever their
perforce whatever they've been working
with to to work efficiently with that
many people on the same on the same code
base so the great thing about a
service-oriented architecture is that it
helps you stay agile so there's less
concurrent development on any one code
base there's quicker reef actors there
if you have smaller deployable unit
smaller services that you're pushing out
there's fewer change sets going out at
any one time so fewer things are
shifting around and that means they can
affect fewer features so your outages
are more contained you get better to
find areas responsibilities so that you
know you don't instead of you know
there's five people on your team and
they're all responsible for everything
there's like one or two people on each
on each section and then you should
definitely better get a good code tool
searching good code search tool in place
super important once you if you've got
50 repositories like how are you going
to find anything so a subject of service
isolation you need to assume that any
one of your services might fail yes
they're all on your own infrastructure
but you know those those process
boundaries are no good those network
boundaries are no good and you're
deploying these things all the time so
at any given time one of them might go
down you want to you also want to
separate critical functions into their
own into their own services this
actually lets you kind of treat your
different services with different levels
of different levels of importance so at
Ebor we've actually divided our kind of
our entire architecture into like into
two different two broad areas there's
the real-time system and there is our
back-end system so when you are so when
drivers are driving around and they're
paying back their location to our
servers that's all going into this
real-time system when I open the app and
I see the nearby cars that's all from
the real-time system when I request a
ride real-time system when I as
we're on that ride and would be in you
know the system sort of keeping track of
where we're going that's in the
real-time system and it's not until we
get to the destination hit that end trip
buttons until the driver ends the trip
at the real time system which by the way
hasn't stored anything in like a
relational database hasn't touched a
relational database this whole time
there's a bunch of stuff in memory
there's stuff replicated between
different in memory processes but
nothing has really been written down
there's some logging but nothing's
really been written down so until all
that's done that that that that package
of information gets put together and
sent into the back end and on the back
end and it's actually you know sent
through a couple of queuing systems so
that doesn't get lost on the way and
then and then it gets processed written
down into a database we do our billing
we send you a receipt what that means is
that our entire back-end are our
database can go down and the core
functionality of uber still works and
with the end so that lets us be much
more kind of agile with that back-end
system we can deploy whenever we want
now there's still there still may be a
degradation of some pieces of
functionality if the back end or some
part of the back end goes down but it
lets us be very agile with that code and
we only have to be kind of this this
next level of protective with with that
real-time code and even then we segment
that by city and so we can we can be
really safe with that to it we can
deployed a really small city and then go
to a bigger one so it's all really
important to do your designing for
failure you're expecting for failure
then you're just like well whatever and
you go to sleep well the problem is that
you have to know if if a if a failure
happens and by definition at this point
the failures that are going to happen
are going to be the like unknown
unknowns you've prepared for all the
things that you think might fail but
there still are going to be things that
are going to fail outside of that so you
can't be blind when that happens okay so
even with good design decisions you're
going to experience failures and you
want to know when that's happened and so
the only way to really do that is on
automated monitoring and alerting and
then the problem is that not all not all
fail
are obvious so something can go from a
hundred responding in 100 milliseconds
to responding in 10 seconds it's still
returning 200 status codes every so
often but but it's actually gotten you
know a lot slower and that's that's like
as good as a failure your system may as
well be down it's really not working but
you might not know about it because
you're still seeing good status codes
come out of it so my philosophy here is
that you have to track everything even
stuff you don't care about because you
never know when you're going to start
carrying about it and that it's really
nice to be able to go back and say like
oh yeah we have this data you know it's
buried in our log somewhere but we have
it the only problem here is that this
requires like a an investment because by
definition this is going to be higher
volume than whatever is coming or at
least equal volume to what's like the
number of requests coming into your
system I mean at minimum you're talking
you know 4x of what comes in you got an
access log entry from your web server
you have an access log entry from your
application itself and then you're
sending out a you know a time series
stat for each one of those things as
well because you really want to measure
things at at each each section in the
pipeline because of if something if
something's failed and you have four
processes you know there's a there's a
web server and an H a proxy and an
application server like how are you
going to know which place it failed at
you have to be measuring each one of
these you can see where the drop off is
and and if you I think if you do this
right you might be you might be it you
know 10x you know for every one event
that comes in there's 10x that go into
your logs and event systems if not more
so the first first aspect of tracking
everything is logging everything you log
everything but like really just almost
everything don't ever log response
bodies or post bodies from HTTP requests
your your engineers will ask you to do
it they'll say hey can we just we just
get that data we'd really love to see
what's coming out of the server would be
really helpful don't do it don't do it
it's a trap I guarantee you if you do
this someday somebody is going to log
credit card numbers
our passwords and deer logs and then you
will if you're the responsible person in
the in the org will spend your weekend
writing code to like filter credit card
numbers out of logs and then you'll
spend spend your Saturday doing that
then you spend your Sunday running that
code on on all of your logs so you can
scrub out all the sensitive data because
it's stored in s3 for all time and you
can't leave credit card numbers there
this is not a fulfilling way to spend a
weekend and so I advise you to just to
just not log this stuff at all it can be
very helpful it could be really helpful
to have but to just do it in a blanket
manner is really unwise you know you can
add spot logging where you need to but
but this is just this one area where you
just you just can't just as a rule you
shouldn't write everything down even if
you don't log these up these post bodies
or response bodies somebody will still
find a way to put credit card numbers
into your logs I promise you I've seen
it a million times I've seen it like
five companies like people will do this
so you know something we've done is you
know you write a little all your logging
goes in through one method right so you
write a little filter in that logging
method that looks does this look like a
JSON object it doesn't look like it as a
field called credit card number or does
it look like it as a field called
password you can write an engine X
module that actually looks at your
response bodies and looks for 16 digit
strings of numbers and does the little
the lunch Exim check and says is this a
credit card number that we're sending
out and then you can you know throw and
alert or block the response or something
what you want to do is is make it
impossible for you to screw some of
these things up this is something that
you're probably unlikely to take advice
on like up front if you you know if
you're building a HelloWorld rails app
you're not really likely to this is this
all right necessarily a big concern but
remember this like the first time it
happens and then you can you can prevent
it from happening again all right you
write it down in the wiki you know like
oh that's right we have to we have to
get to be proactive about not you know
not accidentally storing sensitive
information and and and then you do
something about it aggregate your logs
don't don't make people go on to your
application servers to get the logs off
you really
want people as a as a as a rule you
don't want them going on to your under
your production infrastructure just to
do debugging order to just to get pieces
of information you kind of want that all
segments it you want to keep your
production infrastructure secure and you
want to keep it intact really there's
lots of good stuff for doing this at
uber as at Pinterest we're big into
Kafka but you know their scribe flume
logstash all sorts of stuff you want to
make your log searchable easily and I
suggest putting them all on s 3 s 3 is
cheap easy place to put them you can run
MapReduce jobs on them this is this is a
good way to do things okay you also want
to also want to graph everything Amit
stats into your real-time graphing
system the funny thing about this is you
can run into problems with this to all
you're doing is emitting stats like what
can go wrong turns out that if you're
using TCP to send these stats into your
stat aggregation system and your static
a shin system gets overloaded it'll
actually start then you're in you're
sending these stats like in process
it'll start slowing down your
application your application will get
slow it will it will it will serve slow
responses because it's trying to it's
trying to inform you about how many
responses survey this is like a really
unfortunate situation to get into so do
these things out of process I mean same
thing with logging you want to try and
remove anything that can get that can
get slow that's high volume you want to
make sure that it's isolated somehow so
that if it backs up it doesn't actually
degrade performance in your main
application and then you know put your
useful dashboards that you have put
those in your wiki and then this it
becomes your your primary system really
for triaging kind of degradation of
service what's going wrong well I've got
the suite dashboard that shows me
response times at every every part of
the system and to my external calls that
way you can really pinpoint where things
are going wrong at over we use graphite
for this at Yelp we used a front end I
call I wrote called firefly they they
both kind of suck in their own way so if
you can if you can come up with the the
killer app for for the system for
operations teams
that's a million-dollar idea right there
okay you also you want to monitor
everything so you've got your stats
coming out you want to make sure you're
keeping track of what those stats say
you have to know if something has gone
wrong so relentlessly at monitoring at
uber we have we have tens of thousands
of monitor things in nagios we have tens
of thousands of alerts that can go off
we distribute them among teams where we
can but but we never shy away from
adding a new one and this has been the
primary thing I think that it's improved
our uptime and reliability in the last
in the last couple years use pagerduty
pagerduty is an awesome way to like
share the responsibility and do alerts
along these systems but but it's funny
because Pedro duty can go down also and
if your system goes down while Pedro
duty is down you won't get an alert
about it or one of your engineers like
the new guy who joins the team can
forget to set up a notification rule in
Pedro duty so they don't have their
phone number there so it sets off an
alert but doesn't actually send them a
push notification or text message or
anything so you have to like officier
write code to monitor that to make sure
that's taken care of it's totally a name
but you have to do these sorts of things
to make sure that nothing falls through
the through the cracks and then when
things do go wrong write a post mortem
put it on your wiki add more monitoring
so the great thing about tracking
everything is that it lets you move
faster okay running a little low but on
time here but um we're sort of prepared
for failure let's talk about just some
choices to make use boring things this
is my is my best suggestion don't get
creative where you don't need to I think
engine X might be the the newest most
like radical piece of software we use
uber not entirely true but in in the
back end section things remember like
hiring people is going to be your
primary constraint so if you're using
new stuff that nobody not many people
use you're going to have a harder time
finding people who are experienced you
can find people who want to learn it but
you're you're taking a risk there with
newer platforms so that the rule of
thumb here is don't take those risks
where you don't intend
do anything interesting if you're just
surveying you know web pages don't pick
an interesting web server pick Apache
pick index and so at uber you know a
couple years ago we chose nodejs is this
real time platform component the back
end is in Python but we chose nodejs for
this real-time system and node was
actually a decent choice there it's the
system is lots of concurrent connections
lots of time just been waiting for for
Network I oh but but it was like alpha
software at the time this is a totally
crazy choice it's turned out fine but
you have to be careful of that where
ober did not get creative was its
database we like we chose my sequel for
the database like there's no reason to
innovate there we just want to store
data in it let's not get let's not get
too excited about it we did once do
something crazy we migrated from my
sequel to postgres literally like wheels
moving bus the whole thing like it was
amazing it was pretty awesome to watch
total disaster or we're still in the
process of undoing some of the mistakes
and some of the some of the problems
that arose because of that then you want
to choose the right tool for the job you
know for instance node was the right
tool for the job in that case you also
want to make sure that like when you are
storing things your database you
separate out your online and offline
workloads it is a total it's a total
mess to try to make one system service
like online transactional requests like
a new transaction we made some money and
then to have that same system tell you
how much money you've made every month
for the last ten years like those are a
totally unfair thing to ask of one
system so you want to separate those
workloads as early as possible before
the data gets truly large okay one other
really important thing is you want to
you want to be secure I'm going to skip
over most of this but you want to this
is a funny one this encrypted obfuscated
primary Keys show to the front end you
if you meet with any amount of success
you do not want your competitor is
figuring out how many new users are
getting just by creating a new user
account once a week or once a day and
see
how the primary keys increment you know
/ users / 5 / user / 10 this has come up
everywhere I've worked it's really funny
you don't think about it in the
beginning and then suddenly you're like
oh we're doing okay and then and then
the boss comes by and he's like why are
we telling people how fast we're growing
so it used to be that we'd like encrypt
the IDs because you were space conscious
now just use you IDs for your primary
keys there's all sorts of other stuff
you can do here but I want to get to the
next point though because my last slide
and it's pretty important create
frameworks you want to you want to make
doing things the right way really easy
for your engineers so at uber we have
clay and we have played oh and those are
the Python and nodejs frameworks
respectively that we've built in-house
and what those are those are actually
clay is built on top of flasks so it's a
you know flask is a Python HTTP sort of
micro framework and clay is some extra
uber code on top of that and what that
does is gives you easy hooks into the
database easy hooks into the logging
system into caching into monitoring and
alerting analytics what you do is you
make it so that the the path of least
resistance the easiest thing to do is is
do things the right way do things the
way that the people who have set up
those systems have prescribed otherwise
if you start growing really fast and you
don't have things to ficient lee
document on your wiki everybody's going
to invent their own way to do it and
you'll end up with this crazy
infrastructure that nobody can reason
about because there is no there's no
homogeneity this is how you sort of this
is how you propagate your cultural norms
as you start to grow you just make it so
that's an easy way to do things okay so
what we need to do you need a wiki you
got to hire pretty geniuses sorry but
you just have to forget what do it um
pic boring technology where you have no
intention to innovate and resisted the
urge to try everything new or refactor
everything every month expect that
everything will fail or in it and that
it may not fail in obvious ways you want
to reduce your state fulness so that
when there are failures the likelihood
is that it will not affect a system
where it matters that much track
everything keep your system secure
and encode all of these best practices
into into some some technological system
that is really straightforward for
engineers to use and gives them these
sort of easiest path to success and
that's all I got thank you any questions
yeah look like anyone came up as if I
was afraid somebody was going to ask
that yes we well actually the original
choice was based really on two big
features in Postgres one was online
schema changes like first class online
schema changes so the ability to add
columns or add indexes online without
taking any downtime as it turns out
that's like it's true mostly for indexes
but it's it's not always true for adding
in dec or sorry adding columns to tables
you know postgres has a fixed size
memory space that it uses for to
describe a table and if you go if you're
adding a column and it requires going
past that boundary you actually then
have to rebuild the whole table anyway
so it doesn't even work all the time but
that was one of the big ones right
having to take down time or lock tables
in order to do schema changes is really
frustrating and if you're growing fast
and you're adding new features and
functionality you're going to be doing
schema changes all the time so obviously
for my sequel there are there are tool
there's PT online schema change and
there's there there there are now pretty
well developed tools but this it's like
first-class support in Postgres that was
one of them the other one was that every
almost everything that Hoover does
almost every piece of data that comes in
has a latitude and longitude associated
with it and my sequel has really poor
tools around geospatial data and
postgres has kind of nearly first-class
support for geospatial indexes and we
were going to be we wanted to be able to
do a lot of querying around that but
that's actually that's that's
really my main point there about the
separating your online from analytic
workloads in two separate systems what
we sort of we kind of failed to really
to encode there was that those
geospatial queries we were doing were
really like analytics things those were
things we did after the fact to gain
insight into the end of the system and
the better pattern there would be two to
e TL those to copy that data into a
system like a postgres database that did
have those geospatial indexes but we
didn't necessarily need that
functionality in the main database so
our problems with postgres are around
scaling concurrent connections the the
the the authors of postgres tell you you
really only want like one or two x the
number of cores on your box that that
many concurrent connections which is
nowhere near enough for like a sort of
single master set up and we have managed
to tune a postgres instance with 64
cores to like 400 concurrent connections
which is like a sort of unheard of and
we're always a little it's always a
little terrified but but it works well
as we're kind of starting out the
systems and and and coming up with more
creative ways that are ultimately more
scalable anyway but we've we've been
forced into that sooner than we would
have been otherwise by choosing a
database that in the end doesn't
actually support like sort of the kind
of growth that we needed that well
obviously there's the lots of opinions
about postgres but yeah the veils cool
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>