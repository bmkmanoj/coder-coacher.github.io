<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Building Low Latency Microservices &amp; Monoliths in Java • Peter Lawrey | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Building Low Latency Microservices &amp; Monoliths in Java • Peter Lawrey - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Building Low Latency Microservices &amp; Monoliths in Java • Peter Lawrey</b></h2><h5 class="post__date">2016-10-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4FJufP8zwgg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon my name is Peter Lorre
and I'll be talking about micro-services
but within a context of applications
that are designed for high performance I
am a Java developer myself and I do
consulting and I've worked with hedge
funds and investment banks for about
eight years in the low latency space and
about twenty three years working in IT I
am quite active on Stack Overflow and in
fact someone last night commented that I
seem to be willing to answer just about
any question although there was a
question this morning which I didn't
answer because it said how do I multiply
strings in Java and even I was stumped I
have a small software company we've got
three full-time developers apart from
myself and we usually engage a new
client by starting with a one-week
workshop where we develop a new system
trying to incorporate best practices -
within the constraints of the sort of
technological decisions your company has
already made as well moving on my first
computer was a thing called an NEC a PC
one and in fact it went at a true five
megahertz as opposed to the IBM PC which
only ran at four point seven seven
megahertz and he had 128 kilobytes of
RAM and I used it for many years as a
teenager first using machine code for
two years so that was my first language
which if you've read any of my java code
that probably makes sense to you and
later use turbo pascal and in all that
time I never found a good use for all
128 kilobytes of RAM
so where do micro surfaces come from
there a lot of the principles of micro
services have been around for some time
some of them are more new but largely
it's a collective term for some best
practices now someone mentioned to me
yesterday that they that they'd heard
that micro services come from actually
in the finance area but from the
research I did it was actually short for
micro Web Services originally was the
original term so I think in general most
people when they talk about micro
service they think of web services and
therefore they tend to think of the sort
of constraints and technologies that web
services typically use however a lot of
these principles are not specific to any
given technology and you're not
necessarily limited to those
technologies and it can still be useful
now the reason why Web Services the fact
that it comes from web service or a lot
of people think of Web Services is
important is that when it comes to a GUI
application there's an inherent
threshold of a performance requirement
which is how fast can a human see an
update because any update is which is
too fast for human to see it doesn't
really matter how fast it is and in fact
most movies in fact one of the first big
films to switch from 24 frames a second
to 48 frames a second was actually Lord
of the Rings so it's relatively recent
that they even switched major films from
24 to 48 but you'll agree that most
films which have got a 24 frame rate
that you've seen you don't see the
flicker factors actually mentioned in
the Guinness Book of Records that there
are people can see the flicker because
they can see changes that quick and they
find watching movies annoying but of 24
frames a second is actually 40
milliseconds so anything that's under 40
milliseconds
probably doesn't matter because a human
can't tell the difference anyway
this is like a threshold and since
basically we don't care about delays
that accumulate to less than 40
milliseconds another thing about web
applications is that as computers get
more and more powerful there's a
tendency to make them the webpages
richer and richer and put more detail
more data more information so they tend
to just grow to fit the additional
processing power that your machines have
and in fact at least according to one
website the average size of a webpage is
now the size of doom the entire gang
doom originally shipped as 2.3 megabyte
again that was the whole thing and in
fact the average web page is around that
size as well so every time you're
watching a completely new webpage you're
actually downloading the entire of doom
or something of that size now I've
talked about micro-services in a number
of banks and we get the similar sort of
reaction basically the first reaction is
well microservices is a buzz word and at
this Bank we don't do buzzwords which is
fair enough there's good good reasons
not to the reason that I find that a bit
odd is because what happens is that as
they go into it a little bit more detail
they actually start to realize wait a
minute this all sounds very familiar we
do a lot of these things already so
they've gone from we will never do this
to actually we do a lot of this already
and then in fact they find that they are
actually doing most of these techniques
already and a lot of what what I
actually do it with clients is actually
rebranding what they do already so
taking some of the techniques that they
have and just relabeling them so it is
to some degree a marketing term and one
thing about marketing is that it's very
flexible you can actually rearrange what
you have and give it a new name but what
the benefit of doing this is not
completely empty changes is that this
can help bring into contra
what it is that you can easily change to
improve right so where are the quick
wins and finally it can help give you a
direction as to where you would like to
be in the medium term with minimum of
risk that's something that's actually
achievable so this is a chart that I did
recently with a client they initially
were thinking well micro service is not
really for us but there's a lot of Drive
to push us in that direction or we need
to come up with an answer for why we're
not doing micro services and then I
started talking to them or what are you
doing and in fact in reality a lot in a
lot of areas they were already doing
these things right so so to be able to
real able themselves as micro services
didn't mean they had to change
everything they were doing and in fact
that that's not realistic anyway you
shouldn't be attempting to change
everything or doing and so we just came
up with a simple star rating and we also
then identified where we thought the
quick wins were so this was the initial
draft in reality the last three were the
ones that they decided they would
emphasize on in the short term which is
transparent messaging getting making
changing the structure so that teams can
work more independently and lamda
architecture so what are the benefits of
introducing techniques from micro
services into trading systems well one
other thing one of the key drivers for
training systems is that time to market
is very important the cost of lost
opportunity is actually can be very high
so any techniques that allow you to get
your solutions out faster can not only
save money but actually help the company
make more money it can make them easier
to maintain as long as you're making
some sensible choices and it can also
simplify your programming models you
don't it can give you some structure and
a common language to the technique
that you're using we're microservices
tends to struggle and in fact this is
something that trading systems have been
doing for a long time is how do we deal
with asynchronous messaging how do we
test them how do we profile and debug
them and microservices this tend to be a
lot of well let's base it on rest which
is synchronous typically but you can
also use WebSockets it's sort of like oh
and you can also do this but really
there's not a lot of good practices
around asynchronous messaging whereas
trading systems have been based on
asynchronous messaging for a long time
this is generally how they actually
operate and affect synchronous messaging
tends to be more the exception that's
only really used for gooeys also trading
systems have a lot of techniques around
how do you build deterministic and
highly reproducible systems that you can
test again and again and get the same
results every time now low latency is a
term that comes up quite often but I
think it's worth trying to put some
definition around it because it actually
provides a bit of contrast with what
most people think of when they think of
web services and that is that with low
latency that the first definition should
really be the most broad definition is
that you actually have a view on how
much your response time costs your
business now that what that could be
could be different but I've heard people
talking about all we need low latency
and they say well how much would it cost
you how much difference would you make
they don't actually have a clear idea
really they're just sort of like will
users complain about Layton sees we
would like to users to complain less
that's really the extent of the thinking
whereas if you can have some clearer
view you get you've actually qualified
because it really is important to you to
have certain thresholds under certain
you you have an SLA behind it now from
for my purposes low latency actually
means you care about latencies you can't
see
and that makes a difference because you
need to have a toolset that will help
you record those latencies because you
cannot visually see that oh well this is
now going faster because in fact your
your your unacceptable agencies might be
still too fast for you to see so let me
give you an example we had recently had
an investment bank who actually they
actually refused to accept our product
at one point because our five-nines
worst one in hundred thousand latency
was four hundred and fifty microseconds
that was too high for them now that is a
latency you still cannot say so it's the
worst one a hundred thousand but you
can't see it now it took us a few days
of investigation to figure out what this
was turned out it was related to parsing
date times using local date time and
after we changed the code we managed to
fix it and we were we were under their
requirement which is 35 microseconds so
these are the sorts of this is a fairly
extreme example most of our clients
aren't pushing for these kind of numbers
but it is possible to really care about
Layton sees which even in extreme cases
are still not appreciable to a human so
you need to have tools and in fact we
ended up having to develop a tool just
to solve this problem which is why it
took us so long and that might be a
subject for another tool it's open
source by the way so ask me if you're
interested okay so one of the simplest
ways to speed up a system is to do less
work if you take work out of your system
then it can run faster and in particular
going back to the private data model of
a micro service you will get much better
performance if your process will spend
most of its time in its own local cache
by staying on its own local cache
there's no less contention between
different threads and you will get
better scalability because each thread
now runs completely independently
now these numbers the caches are
actually quite small there are a 32
kilobytes for data in your l1 cache 256
kilobytes in your l2 cache that this
makes a real big big difference to the
performance of application because if
you access your l3 cache then you
actually can take as 10 times longer and
that that factor of 10 is often more of
a performance improvement then you can
achieve by just putting out lots and
lots of machines so writing small
services components single threaded
units of work that will sit on one core
and not access outside the core you can
get bigger improvements in performance
than just throwing lots of machines at
the problem and this tends to suit
trading systems much more than it does
save for web applications with their web
applications the main problem is
throughput not latency so much as long
as the latencies are too small for a
human to see the next biggest problem is
throughput but for trading systems you
need to minimize latency and throwing
lots of boxes at the problem actually
just increases the latency it may
increase the throughput but it also
increases the latency so in reality you
want to have a minimum of boxes you want
to get the most out of every one of
those machines and a simple way of doing
that is to have services that or
components that use a small amount of
private data private to them so areas
where micro services and training
systems overlap that you want to have
simple component based design as I said
you can still have component based
design in a model if there's nothing
stopping you
having that but essentially micro
services will not work unless you've got
clear separations of responsibility
between your components clear less
levels of abstraction in a monolith
you're not really required to do that
and it's very often easy for people to
put in small simple solutions which
actually end up causing a big web of
dependencies between your components
whereas Microsoft's you just it just
don't work
work if you do that in trading systems
you do use asynchronous messaging a lot
more I believe that it's considered best
practice in terms of performance with
micro services although not everyone
does it you want to have automatic and
autom dynamic deployment of services you
for performance reasons not just for
keeping responsibility simple you want
to have private datasets as much as
possible now an important area is what I
believe is transparent messaging now
this is where debugging a distributed
system is made so much easier if you can
actually see the messages you can
actually check them yourself because
that immediately makes it easy to cut
your problem in half is my system
producing the message that's correct
is it producing it correctly or is it
just that the consumer is consuming it
incorrectly and at that point you can
immediately divide your problem in half
if you can read the messages also from a
performance point of view your system
may be behaving working correctly but it
may be generating far more messages than
it actually needs so for example it's
quite common for consumers to ignore
duplicate but say your producer is
sending everything three times right you
run your unit test you run all your
testing everything still works because
they're ignoring all the duplicates but
as soon as you look at the message you
go oh everything's coming out three
times so you can see immediately
problems which may not show up in unit
tests is it sending far more data that
it needs to is it sending fields that
never actually change but it's sending
them again and again and again with
every record you start to see
optimization opportunities if you can
see your messages okay it's moving on to
lambda architecture so one of the models
of lambda architecture is that you have
an ever-growing list of events as input
to a function that is stateless and
then that has a set of outcomes for
every input event now this is a very
simple model in fact it doesn't actually
work in all cases but where it does work
you want to use this as much as possible
because these are very simple units of
work very easy to understand and I
highly encourage people to take identify
where in their system they can use
lambda functions and extract that work
out so then that reveals the components
which have more complex life cycles now
as a sort of revision of this in this
model each output is completely
dependent on one message from its input
now there's a slight revision then I
tend to take which is performance
reasons is that in this case the
function is now a result of all the
inputs it's ever received right so in
theory you could just replay every
message to work out what it should do
with one message or you can maintain a
state and that state needs to be able to
be completely rebuilt from all the input
then that just becomes an optimization
so you can't have some state in a lambda
function
I believe provided it's completely
reproducible from all of its inputs now
how do we start putting these together
well you might have say a fixed engine
with data coming in market data coming
in and then you need to normalize this
fixed data into an internal format that
some fields you're not interested in
they're being sent to you but you're not
interested in you drop them some fields
need to be translated maybe some data
needs to be enriched and you produce a
normalized format so that might be
output ever-growing messages then you'll
have some sort of stateful component
that now needs to make some decisions
based on those inputs and it may produce
orders for example and those orders then
go out to an order manager that takes
your normalized order message and turns
it into figs so then the the external
system the output at the other end is
just fixed now the problem with this
is this is your critical path and from a
performance point of view you want your
critical path to be as short as possible
in terms of latency you want it to be as
quick as possible and that means
offloading work to other threads or
other services you want to be able to
take work out of this critical path
anything it doesn't need to be doing you
get something else to do it so that your
end-to-end latency is as short as
possible and one way of doing that is to
pass work off to an asynchronous task or
another way of putting it is to pass it
off to another land to function that
does that work in another thread takes
the results of say your control system
pass it off to a strategy or an external
system or we've got one that one of the
one of these components that actually
accesses a JDBC database JDBC can take a
long time so what you would do is you
send a message out of your control
system saying I need this data it goes
past off to an asynchronous handler that
handler then takes the results and it
becomes the input of your control system
now one of the benefits of using this
model is that you can replay because
everything that a system does is
dependent on its inputs you can replay
all of those inputs and recreate the
state of any of these components at any
point during the day and in particular
even if you're dependent on external
systems because you're capturing the
results and passing through them as
input you don't actually need those
external systems to be in that state or
even available anymore
so say for example you did a JDBC query
it's passed off to a strategy and that
should be called gateway or something
and it asks the database for some
results that get passed as an input
you've now captured what result what you
knew when you knew it as an input so
then just replaying the inbound messages
is all you need to do to recreate the
state of that system at that point and
you can measure that you can debug and
test your control system completely
standalone
using real inputs even with real timings
if you wish so one of the things that I
do suggest people do is they work out
what timing is important to them say
maybe it's a hundred microseconds so if
there's two events that are less than
100 microseconds apart you make it a
realistic gap if it's more than a
hundred microseconds then you only
actually wait a hundred microseconds so
that way you can replay a day's worth of
data with realistic timings but do it
within a less than an hour depending on
your spacing because you don't want to
actually wait a whole day to run our
whole day's worth of data you want you
wanted to be able to get your feedback
quicker than that so what do we mean by
a distributed system generally when
people talk about distributed systems
they're thinking about multiple machines
but in reality at the low-level timings
you actually have to think about each
core as an independent processor because
you want them to run as independently as
possible you want the tight the delay
between data going from one per core to
another to be as efficient as possible
and and in fact within a system you can
also think of a distributed system in
particular if you've got numerators this
becomes very important even if you're
not talking about the low latency space
if you've got a JVM that crosses Numa
regions you find your GC times just blow
out fantastically because it's in
reality when you've got multiple
numerators you've got actually multiple
machines connected by a high-speed bus
so even though it is a high-speed bus
it's not as fast as direct memory and in
reality the OS will actually do quite a
lot of work for you to make sure a
process stays within one Numa region but
it certainly if you try and create a JVM
that goes across multiple neumann
regions you will really see the
difference so as I mentioned you want to
actually model your application as a
series of single core machine processes
which have their own data and can run as
independently as
of all and then they communicate via the
l2 coherence bus let's take an example
we've got in this case it's similar to
the previous example I've arranged i've
contrived this arrangement you'll see
why but we've still got this call
routing system which is in green we've
got date market data which is feeding
data into your system
the only outputs really are data that's
come from the TCP you've got an order
management system going out and you've
got a number of asynchronous processes
that take a bit more time a bit more CPU
they're not as latency sensitive and
each one of these threads or core
processes to each core have their own
local data so how does that compare to
how a CPU is actually laid out now
across the top here we've got four cores
and across the bottom we've got another
four cores in between we've got the l3
cache which is quite a lot of silicon
but just immediately above and
immediately below the l3 cache we've got
our l2 caches I'm not sure exactly where
the l1 cache is but it could be to the
left so how does that map to our process
well we actually can lay out how our
program will run on silicon so now we've
got we know exactly which cause will be
used for each of these individual
purposes you can use thread affinity to
bind a process or a thread to an
individual core and now they just need
to be able to communicate via shared
memory or within the same process and
the the delay between one hop and the
next is actually just the speed it
locked the delay it takes to pass data
from one l3 to cache to another l2 cache
which is around 20 clock cycles no 20
nanoseconds so you can get a very high
speed connection
now I haven't actually said all there
are these different threads or these
different processes because in reality
it doesn't matter it should be entirely
down to you to decide do I want to run
these in different processes or not and
in reality what tends to happen is that
the more stable components it's actually
easier to release them as a monolith
once they become stable they're not
changing very often I don't want to be
able to release them all independently
it's just not it's just too much work
but what will tend to happen is like you
might bring a new provider online a new
market gateway that one is not so stable
I want to be able to restart that at any
point if it dies I don't want it to
bring down the whole system right I
don't trust it yet so I make it that
maybe just the second market gateway is
restartable or I make it that the
strategies m2s are restartable because
he keeps that trader keeps changing his
mind as to what he wants he's not too
sure he wants to be able to restart that
one so we make it restartable but
everything else can be deployed at once
as a single JVM or at once as multiple
jaebeum's it really shouldn't matter you
should be able to design a system where
your choice of how many processes use is
what makes sense for you it's not
you shouldn't have to rewrite your code
to make that happen
it's largely repeating what I just said
okay so one of the challenges
micro-services how do you test and debug
them because testing and debugging
distributed systems is very hard and
there isn't really very good tools in
doing this so the approach that I take
is to see a microservice is actually a
component a business component should
really say business component with a
transport and that's what makes it a
service but that transport should be
optional the transport shouldn't change
the behavior of your component and so
you should be able to test your
component standalone and in fact you
should be able to test two components
talking to each other without actually
any transport at all and if by doing so
you now have what is essentially for
testing purposes a monolith or a unit
testing purpose a monomer or for
debugging purposes but then once you all
your components work together and they
behave correctly you can say right now I
want to deploy this to multiple threads
multiple machines the components still
work the same way and so if there's any
issues it becomes because there's some
sort of strange interaction between the
fact that it's now asynchronous but the
actual functionality shouldn't have
changed so let's try a simple example
say we've got a lambda component which
at the moment only has one event coming
in which is a one-sided price so some
providers for market data only give you
the beds all the offers but not both at
the same time we want to write a
component that normalizes this because
internally most of our providers give as
top a book we're not interested in
getting individual events that's what we
want so we have a component that takes
as an input sided prices and it produces
an output top of book prices which have
both so our data model might look like
this we have a class called abstract
multiple which actually implements
things like two string equals hash code
all the serialize a
and deserialization but this model works
regardless of whether you have that or
not
so as an example for the serialize a the
two string we actually serialize as
llamo by default the benefit of Yama
over Jason is that it is designed to be
human readable and it supports types and
another feature we use a lot is comments
we use we put a lot of comments to add
aid tracing and one of the other
benefits is we can take something from a
log or an output and actually D marshal
it and recreate that exact objects so
recreating objects that come out of
production are very easy even if they're
really complex so we we know that no
information has been lost if it can be d
marshaled so let's have a look at this
example we have a sided price and it
goes and tries to find the relevant top
of book price that would need to be
updated and then we say if something has
changed then we trigger an event so we
have a very simple component now that
can take Sider prices and turn them into
events which have top of book how do we
mock our component well actually that's
really trivial because in fact our
component implements an interface we can
just mock the interface ok so how can we
test our component well again we our
outputs are just an interface so we can
mock the outputs we say these are the
outputs we expect to get then we create
our component we feed in the inputs and
then we verify we get all the results we
expected so writing a simple unit test
for this is not very complicated where
it gets even more interesting is where
you have a series of components so the
output of one because we've used a
lambda architecture the output of one is
the input of another so in this case
we're not only calling the combiner
directly but we can also call the order
manager so we don't have to feed
everything as input right from the start
we can take inputs to other components
now the important thing is
you can see that here that not only we
can write simple unit tests starts up
and shuts down very quickly we can run
lots of these tests we can do a lot of
data-driven testing but this is very
easy to debug right so when I wrote this
test there were bugs and I used this to
debug and all I had to do was break
point this line I step into here and it
steps into this code it does something
and then steps into that code I just go
up the stack and then I can go down the
stack and I can see all my variables and
I'm just debugging it like it's a
monolith but in reality these are
standalone components I can put a
transport between them and now I have a
micro service so adding transports now
though I had intended to add MPI to this
list but there's a lot of different
transports that you could use and in
fact I'm keen advocate that you should
be able to have no transport at all so
in the previous example there's no
transport the behavior of the business
logic should be the same regardless of
whether it's a transport or not now the
one I know the most about is chronicle
queue mostly because I wrote it so I
will spend a little bit of time talking
about it it is open source so you can
read the code and it's apache2
so there's no license cost for using it
what apache what a chronicle queue is is
that it's a bound unbounded queue it's
an unbounded persisted queue which means
that it's broke a list it has very low
latency you can achieve less than 10
microseconds 99% of the time it's
persisted so you can easily just copy
the files from production to test and
replay a whole day's worth of data for
any one of those services you can just
take the inputs for that service and
replay them by recording absolutely
every input that's completely recreate
abbu you don't need as much logging you
only nearly need logging for really
exceptional things that you just don't
expect to happen
we had one user try this
initially they diplomat
rating system they were getting 35
microseconds end to end typically and
after putting in chronicle queue they
actually realized that they were doing
some redundant work so after putting in
chronicle queue and removing the
redundant work the latencies went
dropped to 23 microseconds typically so
one of the things that people ask is
well if you're persisting everything
won't that slow you down
but in fact our experience is by having
greater transparency and better testing
you actually speed up your application
so what does it look like now
this is something we've worked on quite
a lot is to make it as transparent as
possible and in fact you can dump this
out without any knowledge of the
application because the messages can be
decoded into llaman so they're in a
binary form for performance but that
binary can be converted to Yammer for
readability I've taken out a lot of
formatting to fit it onto this slide but
that's this is all the messages that are
produced by that previous example and
you can see the header which is sort of
it's all metadata you don't really need
to know but you can see that the on
order idea which was the message that
was called earlier which is in the
middle here is called with these values
and in fact this is what you get because
it's a binary format the position of the
message this is a byte offset of that
message actually matters and this is
this is where this is a comment that
comes in so how does it form this is
done with a Microsoft jmh which is a
very good tool for doing micro
benchmarks one of the things it doesn't
do so well is the asynchronous messaging
which is why we ended up writing our own
called Java latency harness benchmark
but for doing simple tests it's very
useful and as you can see four nines is
under 20 microseconds the five nines is
much higher but that was actually due to
the OS I really need to upgrade my
kernel
it doesn't do that on a much cleaner
machine so one of the contentious things
that I think is actually quite important
is that most messaging systems again
coming from the web space are all
assuming you want flow control and flow
control is very powerful and very useful
flow control stops your consumer by
being from being overloaded by your
producer and as a GUI user that's the
last thing you want because you can't be
sending too much data to a GUI user
because they can't see it anyway and
it's just going to slow down their
machine so it's completely pointless
however there's lots of situations
particularly in trading systems and
compliance systems where flow control is
the last thing you want so for market
data you cannot tell the exchange hey
I'm having trouble keeping out can you
slow down for me isn't just not going to
happen or in compliance systems it's
usually something that other departments
are told they have to have but they
don't want to be slowed down by the
compliance system itself
so what chronicle queue does is is
implemented as like a massive buffer and
then you don't need to have flow control
so it never pushes back on the producer
to say I can't keep up it the consumer
may get further and further behind but
the consumer doesn't even have to be
running because the queue just just
writes more and more to a file that
grows and then the consumer could be
restarted or continue from where it's up
to and because it's all using off heat
memory there's no problem with it
running out of heap space or even main
memory space one of the things we test
is what happens when the consumer is
further than main memory behind the
producer we find that it actually does
hurt performance but it does continue to
run so where can I try this we have some
examples much fuller examples of the
code I just showed you on my blog repo
the actual source code to Chronicle Q
and some of the other things I mentioned
are all open hft and to give you a
little idea as to some of the products
we have everything in green
opensource the one that's most relevant
to this talk is Q which is persist
everything model
I'm banner Q wire is one that's turning
out to be more and more useful it allows
it's a standard API that allows you to
write to lots of different formats and
in particular it allows you to optimize
the format for a particular use case
without having to change your code so in
summary microservice doesn't mean that
you have to do everything differently so
I'm much more in favor of evolution
rather than revolution it's a you can
see it as a goal you can you might
decide that some of these best practices
are for you
some of them are not most likely the
ones that are most relevant to you are
the things you're already doing but
perhaps not as well as you could do so
in fact if you're not already a
distributed system in some sense of the
word then turning your application into
a distributed system may not be the
right decision but if you are already a
distributed system such as a lot of
trading systems are they have to pass
work between multiple threads maybe
there's some things in micro services
that can help you improve your practices
already and yes I'm a big fan of lambda
architecture although I recognize that a
lot of real world applications do not
fit well into it completely but you can
take work out and turn them into a
lambda architecture</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>