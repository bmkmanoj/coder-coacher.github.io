<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • It's all a Numbers Game - the Dirty Little Secret of Scalable Systems • Martin Thompson | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • It's all a Numbers Game - the Dirty Little Secret of Scalable Systems • Martin Thompson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • It's all a Numbers Game - the Dirty Little Secret of Scalable Systems • Martin Thompson</b></h2><h5 class="post__date">2013-03-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1KRYH75wgy4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">but what next Systems skill and I don't
peddle any products so I won't be doing
any of that but what it will be sharing
with you is experience I've had over the
last 20 years trying to build systems
that scale effectively I'm gonna go into
it and the perspective alkane have
learned because I've actually run my own
company I put my own money into it and
lost it and it gives you a very
different perspective on how you just
play with technology for what you want
to do so we're not just gonna go and
build rows and rows of boxes with things
in them what's more interesting to me is
how you do something like this how do
you start with like a small corner shop
and turn it into the largest shopping
mall in Europe I used to work next to
this shopping mall in London and what's
really behind it is it's all about cost
it's not actually a technology problem
it's much more cost problem and when I
talk a lot about how the cost filter
through the technology so what does it
mean to scale well if you look at the
axis on this we want to look at volume
my volume can be units of work
transactions amount to data process
whatever you want it to be but you want
to increase volume well as you increase
volume what you want to happen is not
have your costs go through the roof
because that is not scaling scaling is
where whenever you increase your volumes
you keep your cost reasonable so the red
line this is probably bad but it's
probably bad for certain volumes so if
you're just doing a small scale thing
like I ugh using another analogy is like
forget the shopping for a second imagine
building a bridge if I want to build a
bridge that goes across a little stream
at the bottom of my garden a little bit
of wood is probably fine and the wood is
not going to scale to go across a great
expanse between two islands and I'll use
different things so that might be fine
but it won't scale up but when we start
looking at proper scale true scalability
people will talk about in the linear
sense is like the blue line but I'd
actually argue from a commercial point
of view that's not very good if your
volumes are going up and your cost for
chemical and up almost equal in stab
yeah it's okay but you should be getting
economies of Steel so you want to be
looking more
the green line as you're going up and so
when you start thinking about stuff and
I'm talking about skill cost is so
important and got a look at what are the
costs that make up what you do so what
makes up costs well it comes down to
typically two things there's fixed costs
and as variable costs and fixed costs
and things do you must sync it could be
like development effort upfront it could
be buying kit and putting it in a data
center it might be building that data
center itself but before you take those
jumps you need to work with the business
to understand what is the volume you're
gonna be dealing with and it's the
collaborative effort when we're dealing
with this because then you can work out
do I go for a variable cost model like
do I put this on Amazon ec2 or do I
actually build my own datacenter from
scratch if I've got no idea how many
transactions I'm processing I might as
well just guess so
work with the business to work out what
that's going to be and be sort of a word
that there are things like both best
kinds you can do and you start
purchasing and any sort of skill so
prefer for that and think about it in
the future
so can a typical question I would ask of
anyone is if you add another node to
your system how many more units of work
or transactions or whatever do you get
out of that unfortunately when I go in
to work with many people this is almost
the response I get no one has a clue if
you don't actually know how many units
of work can be done by a given node or
part of your system how do you even know
how to scale well you can just throw
hardware at the problem which is the
thing that's thrown around our industry
a lot and I think it's a little bit of a
shame because it is just effectively
like burning money if you it's okay that
it does cost a lot to develop stuff but
do you know if the cost of where that is
to develop is actually later a greater
or less than the cost of actually buying
the hardware if you know that it's a
very easy decision if you don't know
it's not so easy and well how do we get
to doing some of this so I'm going to
just jump into the meat of this which
I'm going to go through it if it
guidelines that I kind of know often
apply quite often when I'm working on
projects thicker than the scale they're
not set in stone they're not rules I
don't believe there's rules in anything
but let's go into the guidelines and
what's important so first up I'm a great
believer in that the model for the
business would be at the core of what
you do and this may seem strange in a
scalability talk but I think this is
absolutely critical if you're gonna
build anything for a business start off
with the core model great work for the
likes of Eric Evans to me and driven
design with alistair cockburn has been
doing on hexagonal architectures it's
all but get the pure application at the
center keep it free from all sorts of
infrastructure build that and measure
that on its own can that skill and that
work independent of all the other things
we wrap around it and you probably find
that it can and you'll probably find
that you can get really good clean code
but take an assertive approach well in
the hexagonal architecture approach well
how do they then persist that well you
provide an adopter that persists it
right that's one of the design
approaches and not I'm not gonna be in a
detail about how to do that but also our
agaev ins talks about using repository
designs for how you persist stuff and
not where you can get around a
persistence side well how do I get data
coming in you're gonna layer things and
you're gonna have adopters and channels
they'll bring the data into you so you
think let's get the core model together
let's test it let's make sure it works
correctly it is what the business
require I can then feed it with data and
then I can see that see if Li I can then
work out what all of the characteristics
to make that skill but it's important to
get that core model correct and don't in
fact it with infrastructure for how do I
see if something how did I do something
so I'm probably gonna be there for
heretic if somebody starts saying use a
know around I just run away screaming
because you're just mixing concerns at
that point and you're gonna really
struggle with scaling because it's just
limited all of your options if you have
a pure clean model and then you have I
said a repository pattern that you can
choose to store it in somewhere you can
then choose to swap to storage however
you want to do that and you've got those
options and that way you can work and
scale your system so I mentioned we need
a performance test and profile I think
this is absolutely critical how many
people here go on a profiler at least
even once
a week very very few how do you know
what your code even does how do you get
a feel for what it's gonna perform like
the understanding it there's a good
quote back from one of the Microsoft
press I while ago saying that if you're
gonna develop systems you should all
walk through it in a debugger and
there's something I took on I find it
was very very valuable write your code
right do you so write your test write
your code run your tests step through it
in a debugger and so amazing the things
you see and gives you a whole reinforced
picture of the model and how it works
same thing happens with the profiler you
start to understand the characteristics
your system you develop a much better
relationship with your system and just
understand and get a feel for it but
writing good performance test is hard I
find that the common problem that most
people do with their performance tests
is they'll write them write their system
run a performance test and said my
systems not fast enough more often than
not I find the performance issues
actually in the tests rather than in the
target code so how do you address that
well first thing do test-first
development and this really modders is
even more for performance tests than it
does for a normal unit test is you write
it against the blank something that just
returns straight away with the false
answer or even just a random answer and
then see if your performance test can
run fast enough for what you want to do
then plug in the real code because you
know at this stage with your performance
test is fast enough it's not you know to
go back and work on that there's lots of
common mistakes people make they'll also
do things like only testing the same
customer the same product or the same
entry of whatever type over and over
again what's going to happen it's gonna
be hard in cash it's not going to give
you a realistic view of the real world
well how do you find that realistic view
ideally you take data from production
you should know your production
characteristics and you have your tasks
influenced by that if you're starting
out and you're new well at least talk to
your business people and find out what
they are expecting the characteristics
you can guess you can be wrong at that
stage but at least you're working off
some sort of model that you can involve
and understand over time what's really
important
find especially when you start digging
into something you got a performance
problem it's theater constraints quite
often people will look out of profilers
I put and they'll see the thing the
second or third gone I know how to deal
with that easier than the number one
problem that's a very common thing it's
Oh easy just be tempted into the thing
you know how to deal with the code
you're comfortable with the part of the
system you're comfortable with but
actually the number one thing if you fix
it you may find the two and three and
four all just disappear because they
were just a an artifact of the first
problem so always fixed the first and
the largest problem that comes up and
you just find that the characteristics
completely change after that so it's a
simple rule to apply but that's what you
should do and human behavior for
whatever reason people start picking the
easier ones that they think are further
dime and in fact that if we have
performance tests and we have all of
this data and we have realistic models
we can start making sensible economic
decisions forward optimize so like
optimization being the root of all evil
there is a lot of truth in that
but it's a big difference between that
knowing the capabilities of your systems
and knowing what you want to achieve so
if you find your twenty thirty percent
off your target and you're gonna be
going live you need to fix that at that
point well how do I fix that do I spend
a bit more in hardware or do I spend a
bit of time actually optimizing the
system if I know the costs of adding
another node to the system and I can
roughly estimate the time I'm gonna put
into it I can make that call you may
also factor into that risk if it looks
like it's going to be very complicated
and risky to make those changes to the
system and not be much better to just
add the node but make sensible decisions
based upon the economics and the risks
that you're handling I thought given
stage algorithm behavior this is a
really interesting one I see lots of
people produce systems and their test
case size going through a system is one
and you think really incredibly common
one of my first experience is going out
and consulting
was back around 95 96 one of the first
internet banks I've been doing a lot of
work internally on products I started
going outside to work and consultants
him one of my first gigs because I've
been working on the performance stuff
internally was to go into this large
Bank who was having problems that
whenever the switch doll and I just kept
crashing and I went through he reviewed
the code reviewed the tests and being
quite young and naive at the time I said
to their board did you test this with
any more than one person and all the
heads went died and that's pretty much
what it happened and so this is the sort
of dawn of the internet and people
realize in the scale that can happen
that has spent a lot of money on TV
advertising campaigns that huge loads
come hit the system and it just fell
over over and over again and a lot of
the flaws that there had made was they
didn't know the algorithm behavior of
their system and they had some N squared
and n cubed problems as load hit their
system with response time just went up a
hockey stick so the end up was a she's
over here rather than nice order one
things down here so when you start
looking at the behavior of your system
what is the behavior under certain loads
and understanding those characteristics
now we can all go wide if we can get so
much transaction per second out of a
node and if we've got no contention
which we'll talk about that lets you go
wide but is that the answer is in itself
especially when considering cost how
much transactions can we get through a
node so how do we improve that and make
it better and a lot of that quite often
is dealing with the algorithms we're
using on the orders of complexity that's
in there we need to understand the
hardware that's under that I'll talk
about that a little bit more but one
point out sort of may ignite it's not
very well known but I think a lot of the
future as we go multi-core and we're
dealing with large memory space it's a
thing called cache oblivious algorithms
it's a whole talk in its own right I'd
recommend to anyone go on google for
that and look into it it's really really
interesting this but hi we have memory
hierarchies and we have multi-core how
do we decompose those problems and chunk
our way through them in a memory
friendly way to get at them and it
actually can work very well for many
different problems
it's on large data and ice but the one
killer that happens so often I'll bring
it out this is a special point is
unbounded queries I how often do you see
on sites where people I'd to give me all
of the transactions I've done on my
account give me every unit of something
that has been sold somewhere in the
world those queries can just go on and
on and on for a while where the start
getting spectacularly bad is if there's
a nested query in that so you query a
lot of things and then based on results
of that your query and again you can end
up with huge problems so how do you
solve that
well that's bound or queries so that any
unit of work has a maximum upper bind
and so I only want to bring back a
hundred rows if I'm gonna go to a
database for example or just even query
and anything in the system bound their
mind so you deal with it manageable
chunks that can be all motorized and
cost it up and then that way we can
start working I'd hire algorithms skill
and what are the chunks of work that are
actually going through those I mentioned
contention this is the biggest killer
when it comes to scalability and it
happens in the large and the small
what's really interesting is the
techniques used to manage scalability
are off our contention are often the
biggest cause of contention themselves
within the system locks being an example
quite often you'll see small chunks of
work that really only are a few hundred
or a few thousand processor cycles to do
but they're surrounded with a lock and
it's contended and that lock is probably
aardeen many millions of processor
cycles as it does context which is the
kernel the deal with it so whenever
you're dealing stuff you've got to look
at what does the management overhead
cost in what you're doing
and once you've got any contention
points you get huge queues behind them
and that's what's going to cause the
scalability so the key to a shared
nothing architecture is have no
contention points so you don't have the
single database that everything goes to
don't have the single load balancer that
everything goes through cuz they're
always going to be a contention point
and it's assumed with getting to you
itself not Kanna for anyone who's
actually looked inside the code of the
disruptor this was an interesting lesson
for me is that the disruptor was sort of
given birth out of the need to develop
something that fought contention or
unlocks and queues and given the problem
said that we had a tell max I find it
worked really really well but now it's
kind of exploded and loads of people are
using it in other wares and often
without as much control over the
hardware as the head so in this small to
how you use the disruptor you had to
claim a slot you put your data into the
slot and then you publish the fact that
that data is available then the
consumers can take it now how you did
that publishing process it's got an
internal thing called the cursor the
problem that was happening when you've
got multiple threads putting in at the
same time is the cursor becomes a point
of contention and it's the one
weaknesses of anybody's been using the
can disruptor with lots and lots of
concurrent threads heading it because as
you claimed you the end of waiting for
other people to move the cursor forward
and whereas it's a lock free algorithm
it's not a weird free algorithm and
again it doesn't have a fixed cost for
the number of steps to do with that so
there's been quite a lot of work since
that to move that forward and I'm gonna
be releasing some new cues later in the
year through a different open source
initiative that completely removes that
issue because what I've worked out how
to do is how to take that contention
point out and I've got completely linear
scalability now with doing that the
ideas that is also feeding back into the
disruptor and there's going to be a
version 3 it's going to start dealing
with those sort of issues as well the
kind of interesting things about picking
up on some of that is we also improve
lots of other things along the way but
fighting the contention so that's an
example in the micro we've had loads of
other examples in the past with things
like false sharing so you don't even
realize you're having contention memory
is moved around and what's known as
cache lines and there's 64 bytes at a
time and you can find you get two
variables sitting in the same cache line
or separate threads updating them and
your point of contention then is the
memory subsystem moving those cache
lines between different cores so it can
be hidden
that's the micro well what's in the
macro
quite often you see is this your
database this is a classic problem and
even more shared tables within those
databases how do you get rid of that
problem well if you put a service in
front of the database and you only talk
to it via that service you've got lots
of ways of know removing a lot of like
contention for example you don't need to
read into the database the service can
actually cache the data out of that the
service communicant what I do I shard
that out and eliminate the contention
being across different products users
entities whatever it happens to be so
think of it what are the contention
points in the system and eliminate them
many people will suggest use a load
balancer as a way of skill in your
system it does give you certain
characteristics and they also did
certain things at a point in time and
what often things are moving forward so
a load balancer sits out in front of all
your web servers and challenger traffic
3 typically does two useful things for
you it doesn't any others but two major
ones quite often is doing SSL offloads
so it's dealing with the encryption and
having specialized hardware to do that
ever since then the healin processor
which is nearly 4 years ago now I we've
had support for cryptographic
instructions inside modern processors so
we can be doing that local if you're
running open SSL on the list versus the
Linux they're just automatically using
that and it's available to you other
things is doing is connection
multiplexing PC feast actually getting
better and better all of the time so
it's dealing with those problems but
it's not a point of contention and it's
also a point that has very different
skills and so you're teams aren't
scaling the same way as well because one
of the ways to make it your team skill
skill the best is have very similar
skills and vice you'll know technologies
that everybody can use you've got a
weird technology sitting over on the
side you've no I got a contention point
in how your team works as well as a
contention point in your architecture
and so avoiding specialized hardware
unless it's really really given you a
big advantage will also help scalability
in your development efforts itself well
then how do I deal with a whole farm the
server's you may ask well as part of the
boot start processes you can act you can
get a range of servers
can connect and your clients can then
just choose and all of a sudden you've
now got a problem that Federer itself in
skills of much much better and so I
think it's really valid exercise to just
walk your architecture kind of
end-to-end and see do I have single
points of contention and then as you fly
load with your load test and you start
profiling those hotspots will soon start
to light up and you'll start seeing if
they're in this year they're not so
getting them right well how do we often
get hot spots like that either for
system well one of the ways we usually
do is put a queue in front of it rather
than have things bang on it and littles
law is a beautiful little piece of
mathematics incredibly simple and well
worth sort of exploring understanding
but a simple way to think about it is if
any execution you'd be a threat of
process a node of some description takes
a period of time say that period of time
is axe to do a piece of work on average
and you have a queue in front of it and
if your queue is got 10 items in it the
last item is going to have to wear
panics before it actually gets his work
complete and so you can start predicting
the response times of your systems and
having the queues orion's kind of
interesting but most people quite often
don't think about queues in their system
the accuser everywhere whether you
realize it or not queues are there I
would say embrace them and just deal
with the issues that come with them and
also understand them and get the
benefits so he said well I say I'm
queues or everywhere
well let's say I'm not gonna put a queue
in front or something but I'm gonna put
a lock around it what do you think
happens when multiple threads could
access the same thing that's protected
by a lock one gets it the others all
stop they're put onto a queue by the
operating system to be scheduled to run
at a later stage and woken up at a huge
cost leaving your process address space
having your caches polluted
haven't we woke up in rishi angel may be
scheduled again on a different core at a
later stage this is all very expensive
and in the meantime making no progress
take the other approaches let's say your
systems are synchronous and you've got a
queue is you hand the piece of work over
to the queue to get done you
often do something else you're busy
you're useful you can be scheduled in
the same place and have a thread the
other side doing the piece of work so
here's a typical system that you'll see
you typically have d it where's dealing
with the outside world and your
customers and you typically have some
sort of transaction service grossly
simplified there could be many more
layers many more ways of splitting that
ID but we usually have a network stack
with approval threads taking things in
those pieces of work typically at the
gateways you're applying border security
protocol translation turn it into
something useful you're going to do
internally so it goes onto network stack
again to arrive over to your transaction
service when you do your piece of work
whatever that happens to be which may
end up updating storage but whether you
model these queues or not they're gonna
exist as you could write to your network
you're gonna be queueing are you're
going to read from the network you're
gonna be queueing if you've got a thread
pool here as you go to submit jobs to it
you're gonna queue I'm the great
believer in bring those keys out so you
can see them once you can see them and
you're managing them you can start doing
interesting things first really
important thing is bind them if you put
a lock in front to something and it's
got no bind what can happen is the queue
can grow and just keep growing and
growing and growing indefinitely
now the problem most people will
probably see is their system blows up
with out of memory or that the problem
happens way before that that's just the
final cause it's like having a nasty
disease in and dying of pneumonia it's
actually the nasty disease that got you
the first thing you want to watch out
about and the nasty disease here is your
cash overruns so as you're adding things
to the queue so I've got a cure and it's
building up and so it's chugging along
is getting bigger and bigger sooner or
later that queue doesn't fit in your
cash then what happens is as you're
removing in a last in first out manner
nice pristine for start miner the thing
that's going to be picked up by the
consumer has been sitting so long it's
actually been evicted out of your cash
and so your consumer that isn't catching
up
is getting cache misses and the world
has got even worse very quickly then it
slows down even more and in the Kudus
grows on a rapid red filling memory and
the whole world is not so good anymore
better thing to do is keep the queue
binded because on then also you can
honor a response time characteristic cuz
you also don't get a horrible response
time experience to end users so I'm
gonna say I want to keep my response
time within a given bind I can work that
I given littles law given the queue size
what it should be and what happens when
my cue felt what you should do then is
apply back pressure so you don't accept
any more units of work and you apply
back pressure back so in this case let's
say the storage system isn't keeping up
and this threads trying to write well if
it's blocking in this stage that's fine
because this queue then is gonna back up
now if this queue backs up it's gonna
fill and then this thread is not gonna
be able to DQ anymore network packets so
it's gonna put back pressure to here
which means that this thread can no
longer in queue and in more network
packets which then fills that key which
stalls these threads which eventually
stops consuming from the network which
eventually stops no new work coming into
the system that's the better experience
for end users to keep processing and
deny new input rather than just have
your system crash because at least
you're still offering service you're
offering good service up to your maximum
capacity I like the analogy of I'm
running a coffee shop or something
I'll only allow in so many people and
then at that point of say come back
later please now let's come back later
please
and this will it could be a message to
the user like a 503 and a heaps of pay
response sent servers busy come back
again later but at least everybody's in
there is getting a good experience and
that's all working now in fact I've got
all these queues and we know the average
time it takes to process something we
can actually work out really interesting
things like what is the average response
time a user is actually getting in the
system how many transactions per sack
are we dealing with is the rear of which
things are going through queue so making
the first class concepts gives you a lot
of insight into what your systems doing
and bringing them right also makes
things nicely decoupled and easy to
model and test and so if I want to just
low test this lot as an individual
component well I can just throw in and
out of these two case works really
nicely and that way I can get faster
feedback cycles as we all know that one
of the most important things with agile
or lean methodologies is fast feedback
cycles apply it and all levels of what
you work at so I can get much faster
feedback cycles if I just have to run
pass over and over on this rather an end
to end once I've isolated the problem
but I can also test end to end as well
but I've got the choices I've got the
ability to do this now in a night I can
also sound full things going through I
cannot tracers and different things to
know what's going on it's so important
to know what's going on in your
production systems when you want to work
out if you need to scale and then you
can start taking this data for things
like capacity planning because capacity
planning quite often people will look at
well what's the memory utilization
what's the CPU utilization what's our IO
utilizes these are useful things but
they're not enough it's actually even
more powerful the further you come up
the stack because that's what's really
telling about everything else that's on
me if you're finding your response times
and your queues are filling up you know
you've got an underlying problem much
much quicker and you can deal with it so
kind of interesting one on this is a
problem I'm seeing over and over again
and I kind of call it the curse of
logging libraries littles law great
example of high queueing theory works
armed Al's law another great description
of high parallel computing can work and
what is the parallel limiting component
I spend a lot of my time helping people
scale systems and one of the biggest
problems is usually logging happens over
and over again it's just me one of the
biggest limiting factors and so I spent
a lot of time looking inside most of the
common login frameworks sort of log4j
log to forge a log bag does Marty put
slf in front of it the built-in JDK
logger all the diverse ones
they all pretty much did the same thing
internally and they are a perfect
exercise in how to screw up systems
design and disregard littles law Nam
doll sloth by changing how we do logging
massive changes can happen in our
systems and this is one of the open
source projects I'm currently tackling
the case logging uncontained 'add single
thread with nothing else going on in the
system through the normal JDK logger
takes 16 microseconds on a very fast
processor with an SSD underneath it on
average that is horrendous 16
microseconds you add two threads to it
you have to 35 micro seconds on average
it's not even getting caught air cost of
skill it's getting worse you start
adding ear threads and you're wearing
the hundreds of microseconds and they're
all queueing up to basically log and
then you get all sorts of really nasty
characteristics so if you're the unlucky
thread that fills the buffer then has to
be flushed down to disk
everything else is behind it because the
whole thing's rock with a lock you get
this sawtooth of latency and everybody
behind it suffers in the tail you reach
the end of the log and you need to roll
it guess what happens everybody in the
Hat in the queue suffers again and how
it happens really simple design
approaches can fix this and get much
better but it is the perfect anti
pattern to how to build a scalable
system just look inside with loggers do
and do the opposite and you'll probably
do better the thing is we need to log
it's important it should become a
first-class concept and we should bring
it up to the top like I've even seen
systems whereby people have protected
the code with his debug enabled around
them I run a profiler and find out in
many cases 18% of total CPU time was
spent calling his debug and evil it's
crazy
let's get better so what's one great way
of eliminating contention in our systems
well separating reading and writing is a
really really useful thing
because it's so often we'll put a great
big lock or a management system to avoid
contention around our our core data our
core model that we want to read or write
to those reads should not be impacting
the rights that are going on get so
often they are so if we can separate
them we can have both sets of traffic
flowing completely freely well how can
we do that a really good technique I'll
find is using event sourcing and CQRS
come on c'mere a command query
responsibility segregation and what we
do with that is the changes that are
happening to one model we have a stream
of events that go elsewhere and we can
rebuild that model many many times over
and over again and I think about back
when I said let's build a model
completely independent of all the
infrastructure and let's not tangle them
to it
no I we have a model we can run anywhere
if we've put the same stream of events
into that model we can rebuild it all
those models can answer loads and loads
of different queries as many as we want
in a very very scalable fashion and even
if we get down to the stage well well
how do I get the data to all of those if
you don't take that step there's
multicast and all sorts of other means
of getting data out very fast too many
different ways so when I separated reads
and writes writes are still an
interesting problem well one of the
biggest issues with writes is if you're
actually doing updates or replacements
and I think the world are starting to
realize now that doing updates in place
is a very difficult thing to get right
and get the concurrency right on it's
much easier to have worm style storage
where data is immutable and you create a
new version like de Tomac is a good
example of this is d this effectively
facts like where I live know is the
property of my address and the time now
if I move house my old address is still
completely valid why do I want to update
my address field I should be writing a
new address with a new data from which I
started not so when I want to query
where do I currently live
I want to look at what is the current
address that hasn't been superseded by
another address now by having those
append-only two style structures it's so
much easier to start dealing with the
concurrency issues like a good Dante
like said database I know my sequel and
Oracle very well and I know many of the
database have similar characteristics
and a lot of the new column stores and
no sequel databases are doing this as
well
if you write append only you can get
much much better throughput so in Oracle
you can hint a query to be append only
and so when you're doing an insert and
what it doesn't do is it doesn't try to
find space within the table it goes
right to the end to the high-water mark
and writes after that and if it's
writing after the high-water mark on a
table any queries that are currently
running will always get a read
consistent view so it doesn't interfere
with running queries my sequel can do
exactly the same thing so thinking in
those sort of app and only immutable
structures works incredibly well it also
works incredibly well when you look at
the underlying hardware that is running
on disks do very well sequentially
spinning disk can read and write for
normal amount of data sequentially and
so that's kind of great and you're out
in them SSDs have a lot more performance
but also they have issues with updates
SSDs give you completely random read
access when it comes to write if you
override part of a file that's currently
there it has actually mark that data as
to be garbage collected and then other
kids space elsewhere and writes that
data in and because the way the work is
you can read and write a fork a page at
a time but if you want to read something
you've got a as a whole block because
they're organized into an array and the
the blocks are two megabytes in size so
if you're gonna run modifying all of the
time you very quickly were write an SSD
whereas if you just write append only to
go through a really fast really friendly
and you don't even have all that GC
cycles going on hurting you in the
background so it's append-only also
important to cache cache seen is the way
to get into anything quite quickly and
so stuff that's really really important
to casual said tell a story of the
client I had earlier this year who had
an incredibly peaky problem they were
selling things online and most of the
traffic happened on a Sunday night or
around holidays the rest of time it was
quiet and to deal with this problem they
started throwing technology on it I
think this is the disease of many
developers that I call CV driven
development so they started off with
they had Python it kind of worked needed
to scale
they had postgrads and then to end the
back end to store their data and they
then moved from that well let's put
memcache in front but nothing's a little
bit of a problem well the work started
getting really really complicated so
let's work let's put this work out to a
form and install G unicorn well G
unicorn couldn't work so well with
memcache the installed Redis and so all
the data starts getting copied around
all of this different things and this
was getting bigger and bigger and bigger
and more complex and I just sat down on
a quick calculation and realized all of
the reference data for their catalog
fitted into a few hundred megabytes no
one had sat down and done that
calculation as they get rid of all of
this technology read it at a Postgres
whenever you start and just cash it in
all of the nodes it hardly ever changed
and what they did do is you use memcache
then it's just a way of coordinating
between the nodes with the nodes with
pole occasionally and it's just a flag
set in memcache to say whether you
needed to read pull your cache guide
because it happens so infrequently and
they're able to run off that system went
a whole lot faster and also scale
perfectly because they just all red eye
to this the adding a new node on peak so
we end up running an elastic cluster
whereby whenever the pigs have when you
spun up new nodes they just read out the
cache data I'm Nikki just all handled
requests
absolutely no problem so thinking about
that don't get hung up in technology
just go back and be simple and the
simple stuff works really well as I
mentioned fact based theater works
really well as well because we don't
have to invalidate that you don't have
to invalidate a cache when you're
dealing with fact-based theater at any
given point in time that fact is valid
on a later point in time another fact is
valla but you don't need to delete that
old data we have so much memory so much
storage now I take those sorts of
approaches you can eventually garbage
collect and purge if that's an issue
it's so easy to drop a table trunk
editable whatever updating individual
rows is incredibly expensive and also
for many business problems quite often
perfect is not
right answer you go through a booking
process like you can read availability
on hotel rooms you can read availability
on a flight you can read availability
and most things for logistic systems and
if the 20 you read it there's like a
hundred or so things left and even later
you don't need to reread again just
attempt the transaction and at that
point that can feel the cost
availability is gone in which case you
can choose to teach your cash don't try
to keep perfect and up-to-date all the
time there's a lot of things we can play
with like mechanical sympathy is kind of
one of my big passions and it's kind of
driven by just knowing the
infrastructure and what you can get out
of it whenever I started Lmax I got in
an old team that I had worked with and
they built part of our front end that
was customer facing as web traffic and
we run on some profile and we run the
tests on it and this is I'm really pride
we've got 800 requests a second for each
note that's just not good enough and
they're like what do you mean that's
pretty good that's as good as we see
else were and they didn't see where I
was coming from as as well if I see look
at what's going on the networks back the
whole infrastructure coming right up
through the app server that we're
helping to use and I know from previous
measurements could handle on that hard
work 37,000 requests a second so yet
hundred not actually that good really
what he's doing with the rest of it and
they just run the profiler we found a
couple of hotspots and we will fix them
and put them right but being a were of
the capabilities of the platform that's
underneath makes it really powerful for
knowing what you're gonna get out of it
I like to think of this just the scene
was like if I was a structural engineer
I would be fired if I dunno the tensile
strength of steel the compression
strength of concrete if I'm going to
start building stuff you don't need to
know how to go on that concrete you
don't need to know how to go and make a
net work hard but you should know what
it gives you should know the
characteristics of what it has and so
when it comes to things like writing
those low tests and performance tests I
write them for the infrastructure as
well so I know what it is then I know if
I update a driver if I update a third
party component on all of a sudden thing
have changed I can find it fast feedback
cycles and get and I know what is going
on and you can also then sensibly
capacity plan for yourself or on top of
the 52 is going to do I think it's also
really important to know what happens
when it breaks so as you load test
things to the point of breaking does it
fall apart or does it sort of follow
curve nigga I just can't take any more
and continues nice and flat that's
actually quite a nice user experience
some things are designed very well over
so as I talked earlier but queues and
back pressure if you designed like that
you get that nice characteristic you
come up to maximum capacity and you
continue a maximum capacity and you deny
new requests but everything else gets
processed at the same rate much better
than going from bang and falling off
after a dare curve you find that out but
testing things to breaking point so do
it with your own code do it with
third-party components is there it's
actually quite cues fun it's a really
good way of firing up people in the team
and giving them some funds and passion
and what they're doing and they learn
awful lot and your code and the system's
all gets so solid as a result of doing
these type of things I'm really
interesting what I find kind of here at
the bottom is when you get failures and
your guard hot-swap components what's
the behavior when the replica takes over
because quite often people task feel
over in different scenarios usually when
there's no load in their system it's
really important to test those at the
point of stress because quite often find
you get cascading failures and systems
and it's usually under extreme stress so
if you find something that overheats and
the thermals cause a problem it's
usually when it's under extreme load
it's not going to happen whenever it's
just sitting there idle not doing very
much so then it kind of finished with a
very different sort of point and that is
avoid being commercial a group believing
you should understand the business
you're working on because if you
understand the business you understand
the demands of that business what are
the characteristics to which your
systems going to be used if you get
passion about that and knowing about it
it just informed so many of your
decisions so much better about how the
system should work how your low tests
should work by different things work and
quite often to find people working with
the business and often not working very
very well or as well as there could be
and I've got a kind of golden rule that
some of my teams is and any interaction
with the business no matter what they
ask for
no is not a valid answer because it's
not it's it's really unfair for just
saying no you can say yes we can do it
and here's the consequences of doing
that those consequences might be very
difficult to deal with but that's an
informed respectful conversation just
saying no and pushing back get you into
a way of working with the business that
you don't find out all the things that
you need to know like what other demands
the system's gonna put on you
what are is gonna happen next month the
month after so you're kind of ready for
dealing with that so breaking down those
barriers there's some really simple
things you can do to how building those
relationships and getting them working
right like a classic one is get your
team leads to just make sure they're
going have a regular coffee with their
counterparts on the business side
understanding their problems what
they're facing and making sure you're
building great software that actually
suits with their need eat together it's
a really primal thing seriously
it sounds so dumb if you work and you go
and get a sandwich you come back to your
desk and you don't even eat with your
own team never mind your extended team
in the business I find you've probably
got quite a dysfunctional company that
doesn't work anywhere near as well as a
team that spend time together care about
each other and just do some of the
primal things like just having meals
when you do that you quite often find
informal structures grow that work
really really well for building that
team's sort of philosophy and the best
way to get something done is by
influence rather than Authority and
working through all of that and
especially in the non-functional side of
things you really need to do that in a
kind of subtle way because it's it's a
give-and-take and no one really knows
the right answer so you've got to work
together it's like what is the löwe
gonna be at the end of this year
you need to work with the business you
know what they've got plans coming up on
product promotion on marketing just what
ideas that got through the sales
pipeline and then you can start working
or not together with them I think in a
final point there's key to getting this
have fun actually doing a lot of us
doing the things like breaking the
system right in the task knowing how
it's doing getting the numbers up and
graphing and working with the business
and having fun really helps improve it
and have people talk about them because
performance is a great thing that you
can go and get public talks and stuff
from now on so getting people so I've
worked out how to drive the Linux TCP
stack to handle and million concurrent
connections makes a great talk
but you learn so much doing that and
you'll write much better systems as a
result so in that point I'll finish and
take questions
yes it if pledged to the first point
that I made that we want to work at what
is the economics per node when if you're
not having lots of management overhead
like lock contention among its in that
you can work out quite well what is the
total number of units of work you can do
per second or per cycle on your machine
so that gives you a really good metric
the thing you've got to mix in with that
is the ability to shard so if it's just
the read style of kiss will you just
send the stream of events that elsewhere
and you CQRS the right side you have to
shard so you're gonna look at well how
do I show my dated I see that showed up
a customer by product by Dale type
whatever there's usually one of those
three dimensions I had a friend of mine
a few years ago coined the phrase of
saying everything in the business world
breaks down to people's stuff and bails
and you can usually shard by one of
those dimensions and scales up nicely
I do do login I write my own logging and
I find this to be a common thing so I
quite often on fixing performance
problems logging is usually one of the
first things I end up fixing for people
so what's this space and probably before
the end of this year there will be a new
high performance logging framework
available my goal is to have it
completely low touch sort of sub hundred
nanoseconds cost for a log message with
no contention issues but also people to
do nice things like when we start
running in clustered environments
especially virtualized you typically
don't have local storage so you don't
want to be trying to use traditional
logging mechanisms that that's these so
having the ability to send logs back to
a central aggregation point and deal
with them efficiently and deal with them
in a very friendly way because a lot of
people who write loggers tend to have
very little machine affinity or sympathy
going on so whenever I'm logging through
large batches of data I want to apply
adaptive bachchan and smart batching to
what's going on
so if I get a burst of traffic going
through will underline in things like
say the Linux file system I can use
things like send file so I can map so it
said memory map data that I've written a
lot of data into across to a file for
archiving I don't - a socket thus and
elsewhere and those sort of things are
going to start becoming available
because everybody's just doing these
sorts of things over and over again and
Finance for - everybody I know ends up
building a framework that does something
like this but it's happening elsewhere -
anyone who's got significant volumes
that can't use the traditional login
methods thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>