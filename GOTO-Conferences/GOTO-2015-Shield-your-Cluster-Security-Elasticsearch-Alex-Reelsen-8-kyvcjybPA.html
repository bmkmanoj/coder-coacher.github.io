<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • Shield your Cluster: Security &amp; Elasticsearch • Alex Reelsen | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • Shield your Cluster: Security &amp; Elasticsearch • Alex Reelsen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • Shield your Cluster: Security &amp; Elasticsearch • Alex Reelsen</b></h2><h5 class="post__date">2015-06-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8-kyvcjybPA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Thanks okay
so before we get started the mining
exercise of raising your hand who of you
is using the Eric stack okay I take that
as a majority is anyone using shield
already for security it's like - did
anyone do his own homegrown solution to
secure elasticsearch and is happy with
it
that went down fast okay
so this talk is about shield which is
the security extension to elasticsearch
but I just don't want to talk about its
features I also want to talk about like
why did we do it and how did we do it
like what was our thought process to
like write shield to get up and running
with shield to make it secure what's the
design behind and stuff like that I will
cover a couple of the features how easy
it is to get up and running because this
is one of the important parts of
elasticsearch it should be easy to get
up and running and it should also be
easy to get up and running with shield
because security itself is hard enough
we try to not stand in the way in terms
of the implementation of security I will
cover like a couple of use cases what
can it be used for I'm going to cover
like what will be next
because she'll basically just started
this year with the first official
release and yeah we obviously have time
for Q&amp;amp;A and if you have questions just
raise your hand and I will hopefully see
it before we get going just a quick
history of elastic because I think it's
important to know that elastic search or
that the company is not only covering
elastic search anymore but a couple of
products so the company was founded 2012
and it was basically to provide an
ecosystem an economic system around
elastic search the year after we had the
lack of Rashid Khan and Jordan sisal
joining so we created the egg stack with
elastic search and oxidation key banner
and Hadoop integration we also created
the elastic search clients which is a
small layer between HTTP and any
programming language like Python Ruby
PHP Perl because it's that important
today and at that point in time in 2014
we launched the first commercial product
being Marvel 2004
we also find out that take some time to
actually build the team around the one
man product like Akebono rocks - that
it's a fair share of time and this year
shield wengie a packet beads and found
joint elasticsearch and watcher our next
commercial plugin is about to be
released one of the interesting parts
about the elastic itself is we a
completely distributed company we have a
couple of offices like here in Amsterdam
Mountain View London Phoenix Bolin I
myself live in Munich being the only one
basically I have my small little room
doing home office I joined in March 2013
and I work on the elastic search and
shield so if you have questions on both
of them I'm happy to answer them if you
have questions after the talk I'll be
downstairs at the booth so just come by
ask questions and yeah let's let's get
up and running so the first question is
why did we do shield and back in the
days when you installed elastic search
there was no security out of the box so
you all know this wonderful feature
Peter already talked about it this
morning if you execute the delete
statement at the /url all your data is
gone we consider the future once you
executed it you did not consider it a
feature obviously the question is should
everybody be able to delete data not and
our stance was with security it's a
tricky topic either you do it right or
you don't do it at all if you do
anything in between it's a bad idea
usually because you introduce
inconsistencies so elastic search didn't
have any encrypted communication elastic
so didn't have any authorization or
authentication no audit logging and like
as a company grew the customers grew as
well in size and for many companies and
users it's like either there it's not if
there is no security we cannot use this
product so we thought okay we need to do
something about it
luckily the community thought about the
same and there were many people
implementing their own solutions and
like one of the well-known ones was I
just put an engine X in front of
elasticsearch and then everything is
kind of secured and you could configure
nginx or any other web server like
Apache or lighty you name it - block by
URL - block by HTTP method to block
P so you had some control of basically
rejecting requests and allowing rights
only by a certain IP or whatever um you
could implement basic user management
using beta cores and like using HT
possibility based files in Apache or any
other web server you could alias aliases
and filters to like reduce the number of
documents a single user is able to see
and it worked it had some basic
functionality for basic security but how
to solve multi index operations like
imagine you have two indices and you
filter by regular expression and you
want one user only to access your lock
data in this example there's the evil
index which users shouldn't have access
to it's just somewhere in between so you
need some smart handling of the URL in
nginx to make sure that this index
doesn't get filtered while you use
aliases and you always make sure there's
only one index being queried but it's
something you need to take into account
and you need to keep in mind in order to
secure system public and mighty
operations are another problem nginx is
really nice to configure your L
endpoints and to only allow certain
certain HTTP methods or something like
that but if you want to filter back
requests you actually have to go inside
of the HTTP body and find out is this an
index the user is supposed to write or
it is not an index and this is really
tedious and this also implies that you
have to pause the body twice once with
nginx and elasticsearch is doing the
same obviously to find out we have to
indict the data so you have twice the
work the next thing is elastic search is
not only HTTP we have our own protocol
called the transport protocol which is
used for no to note communication so you
need to make sure that no one is able to
bypass your tiny little nginx
configuration which is supposed to be
secure it's easy you can put a firewall
in between and off you go everything is
blocked everything is fine if you take a
look at this you're going to find out
that you have some sort of operational
overhead at this stage you have elastic
search which stores the data you have
nginx which is used for AC else you have
potentially nginx which is used for you
user man
but maybe you have somewhere algebra
which is used from user management so
you basically start to scatter all of
your configuration across you use your
firewall for IP filtering and the
operational overhead of running this
especially if you want to scale and
you're a big company and you need to ask
one department for changing a firewall
configuration you need to ask the other
department was changing your nginx
configuration
it just gets huge because you don't have
a central place to configure this and
when the operational overhead gets too
huge people don't use systems and this
was one of the thoughts behind shield we
want to keep things easy you can solve
security like this but it's going to be
a lot more work ok so how did we do it
one of the things most people know is
that elasticsearch is highly pluggable
and one of our first considerations when
writing around something like security
is that you don't want to have
elasticsearch the open source version
and then you have elasticsearch the
secure version and you have to maintain
two different elasticsearch versions
this will just kill your productivity as
part of the internal team because
elasticsearch is module on pluggable we
didn't have to do this we just had to
create the right hooks inside of
elasticsearch so we could plug in a
security plugin and that's what we
basically did it's it's much more easy
it's much easier so a lot of work when
we build shield was actually to change
the core to support all of those
features because the core was not built
with security in mind at the beginning
this means we had to change like 50% of
the work for shield was in the core to
actually get the core where we wanted it
to have and then after we did this we
were finally able to build security as a
plugin and another thing is that we
don't have some sort of first or second
class protocols so HTTP is as important
to be secured as a transport protocol
when they talk into the node to node
communication and the last part
obviously became more important as the
number of products inside of our company
grew is the integration part it's not
sufficient to just secure elasticsearch
we also need to make sure that Cabana
keeps working with it we need to make
sure that lock stash is able to index
into elastic search even if it's secured
with shield stuff like that so
duration is really important as well as
testing in this case one of the nice
parts about this is if you secure
elasticsearch and you do this on on an
elasticsearch level you basically get
security for Cubana as well because
Cabana just Clarisse elasticsearch and
we just need to pass through all of the
authentication information and we don't
need to add security inside of Cabana
everything is just there and ready so
how does it work or how does it look
like if you take a stock elasticsearch
which is the green line the basic idea
behind shield is that the hooks are
certain positions we just put an
additional layer around elasticsearch
and this layer is doing all of the
authentication authorization checks so a
request comes in it contains some
authentication information username
password session you name it we don't
care
and basically the layer around first
checks does this user exist if it exists
is the user allowed to execute this
action and only if this both is granted
and given we just basically pass it on
to the elasticsearch core which is doing
the work as usual and yeah you either
return and 200 ok or you just return
that the user is not allowed to to
execute this action also the layer can
just do IP filtering like we can check
at this layer already is the IP where
this request is coming from
authenticated or do we allow it or do we
reject it again one of the yeah basic
ideas behind elastic search and security
was that it should be really really easy
to get up and running and all you have
to do is basically you have to install
elastic search stock elastic search 1.6
there's a licensing plugin for shield or
for commercial products and you need to
install shield so it's basically just to
plugin installations as well and once
you start elastic search after you did
this you can immediately see that it's
working because by default it just
rejects every request if you haven't
configured anything you need to add a
user to actually get back even the the
basic request at the /url by default you
get an authorization error
okay so before we start with what any
questions about this great so what kind
of features do we have the first one is
IP filtering as I said you can't
completely implement this in your
firewall configuration if you want but
it might actually be a lot of work if
you have like adding more if you keep
adding or removing nodes continuously
and you don't want to change the
configuration somewhere else all the
time the IP filtering is basically just
a simple parameter in the elasticsearch
llaman file which is shield or transport
filter and you can add IP addresses
which should be allowed or rejected so
in this example I'm basically rejecting
the whole class CNET using the Siddha
notation here and I'm allowing a single
IP I could also just write underscore
all in the deny parameter and it would
reject any incoming connection one
important part is that the setting can
be updated dynamically because if you
put it in the configuration file and you
add a new node you would have to restart
the note in order to take the
configuration into effect but using the
cluster update settings API you can
change it to the runtime so you can add
nodes remove nodes and change the
setting if you want yeah this this
filter is basically internally reused
nettie for network communication and
basically this IP folder just kicks in
first checks is the incoming IP of a TCP
connection is supposed to be allowed and
closes the connection if it doesn't so
it's a simple check we even do this
before we do as a negotiation for
example because it does make a lot of
sense to spend CPU cycles and necessary
negotiation if you then reject the
request afterwards okay that's the quick
introduction into into IP filtering
encrypted communication when do you need
it why do you need it one reason might
be that you want to use it because you
want to use HTTPS for external
communication so if you have an HTTP
client accessing your data another
reason might be that you don't know
enough about your infrastructure if it's
secure like take AWS for example you
don't know who's in
between you don't know how big the
distance between two services maybe
they're in the same zone but maybe they
are still potential hackers in between
which are able to read you clear text
data so if you don't know about the
infrastructure encryption even from node
to node makes a lot of sense if you
absolutely su sure that you have a
secure network
maybe the node to node communication
doesn't need to be encrypted at all it's
it's a matter of taste and maybe a
matter of requirement depending on your
setup so how does the configuration book
and I wrote that there's a key story
quiet key store are are basically it's a
java invention and the key store is
basically just the container to like
handle certificates in Java if you use
Apache or nginx or something like that
you don't use a specific file but you
just refer to the certificates as clear
text files and in Java you need to put
them into a specific format so the java
application can read it and this is
basically the the key store
configuration which is mentioned here so
you need a path which is a file to this
key store and you need to password the
password is basically just to unlock
this key store so it can be read since
elasticsearch 1.6 you can also read the
password on startup so you don't have to
store it in the configuration file if
you think it's it's a very important
secure password so you're going to get
prompted when you start elasticsearch
you can configure SSL or TLS based on
the transport so if you just want to
secure HTTP you can just configure this
setting if you want to secure HTTP and
the transport protocol so the note to
note communication you can do this as
well who few works with SSL and has a
fun of dealing with certificates and CAS
and stuff like that awesome when I said
fun everyone raised his hands that's
scary
SSL is really it's complex and hard to
get right and one of the things that you
shouldn't do which you can do is that
you use self-signed certificates which
basically means you don't have a CA you
have a single certificate and when you
have two nodes they have to sign each
other's certificate when you're at a
third node each node again has to sign
those certificates so when you have 10
notes every certificate needs to be
with a certificate of the other note and
this doesn't scale you can't do this but
only because you can you should not the
correct solution is to use actually a c8
which signs each certificate once and
because it's signed by the CA it's done
then this certificate is considered
trusted and this is how you should
usually do this everything else is just
a maintenance overhead you should
refrain from that yeah you can use a
different config for a cheapy and
transport we also have the notion of
profiles in elastic search terms 1.4
which basically allows you to listen on
a different IP address in a different
port so elastic search can listen on
several IP port combinations and you can
even configure a certain certain of
those combinations that they are only
considered to have incoming client
traffic so if you use the Java API and
you configure those explicitly you can
configure shield to automatically reject
requests which will never come from the
outside but are supposed to be sent
inside of a cluster like shard level
requests and you could also configure
another setting you can do if you use
the Java client is that the external
communication from the Java client is
encrypted but the node to node
communication internally is not
encrypted if you want it if you use SSL
I would highly recommend you to use Java
8 because SSL encryption performance is
much more faster because it uses the CPU
extensions which exists for AES
encryption for example Java 7 doesn't
support this yet okay let's talk about
education who are you the users thing of
authentication is that you supply a
username and a password and the service
receiving the username in the past will
go somewhere and validate that you
actually a user that you actually have
this password the Oz mechanisms in
shield are called reams and reams can
have like different implementations
right now there are three and the next
release of shield will have fourth one
to take piqué piqué i1 the simplest one
is the years users real it's basically
just a file based real man we'll see a
couple of examples for this in a second
the second one's the Alda program so if
you don't want to manage users yourself
but you already have most of your users
in some LDAP directory on some Active
Directory that's the ad reham you can
just use this for authentication one of
the nice parts here is that dreams can
be chained so you can have an es users
freedoms and then an add up one the main
reason for this is that you may want to
be able to still access you note even if
the add a prelim is actually down if
someone is doing adds up maintenance and
you can't get any monitoring statistics
out of your cluster because your user
isn't this L up directory as well it's
bad and that's a moment when you should
have certain users in the es user
sweden's and other in the LDAP directory
and there's support for caching so
imagine your index with 100,000 requests
a second what you don't want to do is
hammer the LDAP server behind that also
with 100,000 authentication requests a
second because that's that's not really
sustainable so there's a caching
mechanism in between which can be used
for wheels and you can clear that one as
well the region of the piqué piqué i-i'm
basically allows you to use client-side
certificates so what this means is that
you give every client who wants to
access elasticsearch its own certificate
and the certificate contains some more
information which uses this is and
instead of supplying a username and a
password with each request the identity
of the users automatically defined when
the connection is initiated with SSL so
this means you don't have to care for
for any username password in your
applications and you can potentially
just reject this certificate if you
don't want to use it anymore as part of
the trust store which is in use okay
how does it look like so this is a basic
configuration for the rims again this is
the elasticsearch camel file you can see
here the the name of the rims you can
freely choose those the type is
important and the order and what you
should usually always do is that the es
users rooms has the order 0 which is the
rim which is going to be tried first and
that's the local one this is always
going to be the faster and doesn't need
any any network
trip and yeah the other ones can be
stacked arbitrarily but I highly suspect
you won't have more than three rooms in
your regular configuration yeah the
first one is usually used for stuff like
monitoring users if you have a neg user
which queries every minute for some
nature's check or whatever you would not
want to put this into a lab and it's
it's likely like if you're in a big
corporation you don't have add up right
access anyway to add new users so how
does the users the es users real
actually work at the end it's just some
configuration files which are local and
there's a seal I on top of that so
instead of you using an editor to to
handle adding or removing users there's
a couple of command line tools which
ship is chilled internally we basically
create two files and those two files are
continuously watched by elasticsearch so
as soon as you change them and they're
being written
we basically reload the whole
configuration and there's one users file
which basically looks like the files
from an HT possibility they contain a
username and a password and there's a
user roles file which is like etc' group
at the end of the day which contains a
username and the group's the user
belongs to this is a kind of command
line options you can execute you can add
a new user you can change the roles of a
user this one adds the admin role and
remove the user role you can list all
users and their specific roles
there's also checked that if a user has
certain roles that don't exist you will
get a warning because you most likely
have a configuration issue there and you
can't delete users but again this is
just a tiny little layer on top of the
files one of the features you can also
have with the authentication is
anonymous access by default we just
reject any requests and shield that is
not associated with the user because we
think this is an error either you
provide a user you don't in a secure
cluster but if you want to create
something like a read-only system so
that you only need authentication and
authorization if you write data
this actually might make sense so you
can always read data you can always
search for data then you can enable the
anonymous user specify some user name
specify some rules which request without
any authentication in fluid half and
from then on this word this has been
editing shield 1.1 so whenever you
install an up-to-date shield this is up
and running ok the second big thing is
the authorization so are you allowed to
do that and this is the this is a
configuration file
it's a yum and fire so again we use the
same kind of syntax you know from the
elasticsearch configuration and you
basically define a role the name of role
admin and you define what kind of
privileges this role is supposed to be
allowed to execute so the admin user is
allowed to do anything in this example
and we basically decide between three
different namings here so we have a role
which is the name set of permissions we
have a permission for example all is a
permission which is very close and there
are privileges and privileges are
basically the single operations so a
permission like all is just a group of
privileges so a couple of more examples
here you have an example that you have a
user role which is allowed to read all
indices and read again is just grouping
for stuff like executing searches
executing suggestions executing gets for
single documents so everything that
allows you to extract data out of an
index the events user is a bit more
limited because you can only read data
with an index that starts with events
underscore but you can still read data
and you can go even further you can
define a log file use a read-only role
that's only allowed to read data from
this decade something this 201 in here
and this is basically the the previous
example this user is only allowed to get
data so it's not even allowed to search
from the events index and this is how
you build your configuration and key
roles inside
okay the last common feature here is the
audit trail that's going to be released
in the next sheered version which is
hopefully coming very soon and the audit
trail know this feature is already there
an extension through this is missing so
the audit trail basically allows you to
lock every action which is being
executed and you can decide about the
granularity you may want only you may
only want to know about access
violations so someone with an IP address
that had been rejected can be locked
someone this broken authentication wrong
password someone trying to execute an
operation which is not supposed to
execute so you can kind of configure the
granularity and the lock level here but
you can also just lock everything but
this means every search requests on
every single shot will produce a lock
line so this is pretty verbose and its
disabled by default if you want audit
enabled there's the configuration
setting and the granularity is just in
its own login configuration file which
you can set ok let's talk about
integration here as I said at the
beginning we have not only elastic
search anymore so we need to take a bit
more care of the different features and
the different products and the first
integration is pretty easy the transport
client because it's a Java API we can
just apply the same configuration we did
for the last four last exert itself the
locks - with its elasticsearch outputs
and its elasticsearch inputs basically
needs a configuration for a username and
password which it needs to send with
every request whenever it's indexing
data cabana 3 is pretty simple because
it directly accesses elasticsearch and
just uses the HTTP response codes for
authentication and stuff like that so
it's just a popping up basic auth window
qivana for right now also just forwards
requests back and forth between
elasticsearch in the browser so it also
just forwards the HTTP basic also
information watcher needs to work with
shield obviously the next commercial
plugin Marvell needs to work with shield
Marvel has basic
some built-in reporter that continuously
writes statistics into elasticsearch and
this one also needs to supply
authentication information when writing
data and that's the the kind of
integration we have and the interesting
part is the transport lined this is a
regular configuration foreign transport
lines you configure cluster name and the
connection we have to connect to but the
interesting part is the shield user
configuration because whatever you put
in here is used with every single
request being sent so this is a log
stash use case with every request that
is being sent for indexing more log data
or whatever data you index with locks -
those credentials are added this works
for locks - but imagine you have a web
application which uses tenant based
approach so that the user depending on
the user that is locked and in your
session you want to write with different
permissions this doesn't work with this
approach but what you can do is you can
configure a header in every single
request
so you extract the username out of the
session of the user configure this for
every index operation and you can be
sure that the user is only allowed to
write in the indices you granted him to
write into okay a couple of more use
cases here and we will see a couple of
configuration files we will see a couple
of em more common use cases and the
first one is the the monitoring
application so what's special about that
usually monitoring doesn't need any
write access it just reads data the
simple approach would be to grant
anomalous access to this however I'm not
so sure if all of the node stats and
enforce and indices statistics which may
reveal a lot of information should be
accessed in a read-only phase you may
want to protect those because they
contain potentially sensitive data so
what you can do is you just add a
specific role which has the monitor
roles attached for indices and cluster
privileges and this means you can read
all of the statistics which are used to
create the Marvel stats for example
second use case is locks - locks -
usually doesn't need any read access
unless you use the elasticsearch input
but I think 95% of the people are just
the elasticsearch output to write log
files or whatever data they have unlocks
- and obviously you need index
permissions to you in DC's if you use
the default configuration - all the
locks - - indices and you need to the
allow writing for index templates
because we lock that starts up it tries
to store an index template you can
disable this and locks - or all you need
to grant the permission then there's
Marvel Marvel consists of two components
one component is used to display all of
the metrics which are being stored the
the monitoring part and the other one is
the component which reports and stores
into elasticsearch and this one is
called an agent and the agent also needs
to write index templates and get them
and it needs to write into the Marvel
indices and create them because it also
uses rolling indices per day on the
other hand if your Marvel user using the
Marvel component we are web the web
front-end thing you need to get
information about the license you
internet information about the notes and
you need to have access to all of the
Marvel indices this is this is just
another yeah use case and we should talk
about the kind of setup I think it's
pretty common in e-commerce this is how
I started with elasticsearch I use the
last exert as a product search engine
and usually you have or you potentially
have one component which is doing some
sort of bulk index in the night like
reenacting all of your data and this one
needs to write into an index called
products underscore anything the idea
behind this is that you that you have
one index products one and another index
products - and you have an alias called
products which always points to one of
those indices and in the night you
execute a bulk indexing into the one
index that is not used for life searches
then you wait until this is done and
then you flip the switch and point the
alias to the other index so completely
indexing does not affect your search you
search that's currently going on and if
you have a bulk role which is basically
just executing this nightly cron job you
need to allow to write it to both of the
indices you need to add this manage
capabilities because you need
flip the switch for the alias and you
may want to add read capabilities as
well - maybe ensure that all of the data
you've written is complete like counting
it for example then you may have another
process which continuously updates
things over the day so for example if
the stock changes of a certain product
you may want to update this on a per
document base the moment it comes in
instead of having a nightly import job
and this one interesting enough points
to products so it doesn't point to real
index it's always only allowed to update
the index which the alias is actually
pointing to so this one always updates
the life index and it doesn't care if
it's the products one or products two
index and it can potentially also delete
documents the next role would be the
application executing the searches
itself so this one just needs the search
operation the get operation that's
potentially it everything around search
no writing nothing like that
you may potentially have a monitoring
role which needs to extract all the
information you want to graph somewhere
or this is Marvel or you have your own
homegrown graphing solution and in
addition you may have a sales
representative because you're also
storing your your sales and
elasticsearch or some social events you
want to read whatever so you have
additional tenant specific functionality
okay so this was the kind of kind of use
cases if you read the shield
documentation we have kind of
extensively covered this and explained
it when how you need this next chapter
is what's next so she was still pretty
young the first release although the
public release was in January of this
year 1.0 now it's it's one point one
point two one point three is coming up
and we still have tons of ideas like how
to extend it we may potentially want to
simplify the SSL configuration because
this whole key store handling Java
people are used to this but mere mortals
are not we may want to simplify that a
bit by just specifying certificates
instead of taking certificates putting
it into a Java specific file and then
telling someone to use this it's just a
bit more tedious
one of the weaknesses of the es users
room is that you need to copy the file
that are generated to all of the notes
because all of the notes need to have
the information of usernames that
passwords and roles you can automate
this obviously with puppet whatever you
name it but it would be much more easier
if the cluster will do it we already
have a component which talks over the
network so might actually make sense to
to use this and so the plan is to have
an API driven user and role management
so you don't have to care for files
stored on disk you just send a command
to some API and we distribute this over
over the classroom opening up the real
API is a big topic for us so as soon as
you start having a sort of modular
mechanism for authentication people are
like yeah it would be great if you
support a JSON web token we really want
oo auth what about cameras and all the
things and we really want a session
based authentication mechanism because
right now you always have to supply user
name and password with every request but
it would be nice if you had something
that times out after some point in time
we could write all of those ourselves
but this would be really really tedious
and people would always have want to
have another real around that so our
idea is that we basically open up the
whole API around that that we have tell
people this is how you implement your
own room we already have caching in
place and stuff like that so you can
just reuse a lot of components think of
elastic search plugins
but just a plugin for shields on top of
that another thing is field level
security everything I was talking about
was basically known index level so you
can protect an index that it's not being
able to read you can use aliases to make
sure that the user can only read certain
parts of an index but the user can
always read the full document or it can
there's nothing in between but if you've
documented that contain really sensitive
data you may want to make sure that this
documents can be searched for but you
don't want to return this data and you
want to make sure that some users cannot
even search for certain fields and this
is field level security we have some
ideas about this we have some prototypes
going to that direction this
implementation is tricky content
potentially has a performance impact as
well so
we'll take some time until we get out
with the solution but this is something
which is up enrollment because it's a
well-known requirement and the last part
which is being worked on right now is
the audit trail I just talked about
which is currently written into a log
file might actually just makes sense to
write it into elasticsearch because then
you can search the audit trail and
another lines feature that you could use
watcher and get automatically notified
in case you have a certain amount of
login failures or security violations or
stuff like that so reusing stuff makes a
lot of sense and so the next release is
able to actually enact the auditor into
elasticsearch instead of writing it in
the file yeah Kerberos is another thing
actually Java has pretty pretty good
support for cameras so we can come up
with another room there as well
kiba integration is another topic we
want to share so that users can only see
their specific dashboards instead of all
of them but this are all things which
are on the kind of long term roadmap so
we don't have definitive dates for that
okay I think I'm almost done a couple of
resources before we're done there's a
sheave documentation available online
there's a talk by Ibanez also the kind
of team lead behind beyond shield
there's another webinar if you want to
know more
there's discuss forums so we recently
switched our forums away from Google
Groups to discuss which is on the
discussed of the elastic CEO there's a
specific shield forum so if you try it
out and you run against any wall please
please please please write we give our
best to answer so but just make sure you
tell us when something is problem yeah
if you want to read stuff for the first
book I heard you just have to ask a
single question it must be good so this
is a great book about search in general
if you work a lot with SSL MT is a TLS
read this book it explains a lot about
the vulnerabilities of the last years
and how to treat as a cell in the right
way and the guy I think his name is even
rich he also has an awesome block so if
you don't want to be the 500 pages take
your time to read
block it's really important to kind of
keep up-to-date what happens with
ssl/tls with all the vulnerabilities
which happened the last one or half
years and the last one it's called
security engineering that's a free ebook
as well it just covers a lot of aspects
when you need to implement security in
distributed systems so this is this is
important we have tons of meetups come
by join one if you're missing a city in
the 81 cities where you live talk to
Leslie she will totally make a meet-up
happen and we can cover everything about
less as such okay that's it
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>