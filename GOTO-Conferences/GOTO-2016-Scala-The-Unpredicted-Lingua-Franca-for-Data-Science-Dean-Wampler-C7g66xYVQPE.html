<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Scala: The Unpredicted Lingua Franca for Data Science • Dean Wampler | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Scala: The Unpredicted Lingua Franca for Data Science • Dean Wampler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Scala: The Unpredicted Lingua Franca for Data Science • Dean Wampler</b></h2><h5 class="post__date">2016-09-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/C7g66xYVQPE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">then this is unbe Walker a work for like
been with the company on my spell up and
actually big data for Scala but like
that because it turns out that scholar-
suddenly found it may be best asian big
data world also by accident there's a
lot of the big open source projects like
spark and conquer rip and Scala and then
it turns out you know let's say subset
certainly not majority but there's a
certain core people who actually really
like spelled up for data science so
typically big data environments break up
and let's call them three groups there's
always the operations people who can
keep things running and forget about
those guys gals then there's like the
engineers who design the plumbing we
know how to make coffee at work you know
how to debug stack traces and spark
which is actually for company things
like that under the data scientists once
if you like machine learning models or
statistics and all that as more common
even this part to work in Python and our
languages like that for days science
that's what they're used to but you know
developers are going to stick with
Tiffany jbm languages like work JPM
centric like Java and Scala and we've
seen a bit like that that the spark in
particular because it's so hot right now
has been driving a dodge scholar but
this is really a talk about why that is
kind of a mix talk in a way it's why you
skull really nice for big data but also
in general
the advantages it has overrun nitrogen
so it's just a little bit of something
for everyone this was only three slides
and I'm going to just demo us the whole
time the first going to see usual
reminder to rate the session of post
questions to the speaker thing quickly
grab it when we're done or we can just
do it the old-fashioned way I am a
feedback y'all put it in there as well
let's see it and when you get the slides
the thing I'm going to demonstrate will
all be at this git repo escala for data
science you might get and that's it for
the slides maybe I'll put that back up
at the end but what we're going to do
actually is use a notebook metaphor for
the demonstration so this is something
that's not too familiar to most
developers but data scientists love
these things it's really an electronic
version and what scientists have been
doing forever which is keeping lab notes
and books you guys probably did this
like chemistry and physics classes the
nice thing about them is you can do
stuff like you know interleague
commentary like over the top of this
this is all marked down you can end of
it you mark down way that I can embed
code run the app intermix that and even
do graphs plotted a that's so forth so
as you can imagine if your data
scientists especially like the plotting
part would be really great but any
language that supports our rebel an
interpreter it's really nice tonight
things work in this way if you've ever
spent a lot of time working in a new the
interpreter for any of the languages
except jossa like Python and Scala
closure Ruby after a while to get really
tedious it's great for messing around a
little bit but and what if I want to add
something
it's to be difficult notebooks actually
take you a lot farther I can easily go
back and edit a log and rerun that self
is there gold in this particular
notebook reevaluate everything I want to
and so forth so that's why I decided to
do this particular demo because it's
partly aimed that's I mean people that
aren't used to using these to sort of
evangelize the idea but also to show how
useful they are so I've actually got a
couple of you that this particular
notebook happens to be for a a spark an
exercise that I wrote for some training
like like they did you know shameless
plug you want to buy training from us we
can sell you that too um but actually
the reason I brought this up is really
to show you sort of what a real notebook
actually looks like i'm using a
particular notebook environment called
spark notebook that sort of skeleton
trick you may have heard that one which
is much more popular actually big data
and never even talked to a fight left
person you've probably heard Jupiter or
what do they call it now it was I hi I'm
Abby call tuber so but it's the same
idea edit cells evaluate them mixed
documentation sequel queries anything
like that together to get this
particular notebook basically riff some
sort of the hello world of big data
which is work I've got a whole bunch of
documents I'm going to suck them in
tokenize into words and then count all
the currencies of the words into the
thing so it would be like the very first
thing you would write if you're building
a Google killer some sort of search
index like that actually though that you
would use work and you use the next
thing which is called a burden index
because the problem work out as you
forget where you found stuff what you
care about is council reason I brought
this up though is that I had people in
this
your size go through and a tweet this
algorithm a little bit to do additional
things in this particular case they
would count words get the cows and then
we would group over the counts what that
means is how many words appear twice how
do you hear of three times how many peer
10,000 times that names though and
what's interesting about this exercise
for data scientists is that it kind of
gets to the natural behavior of language
like some kinds of words appear all the
time up here a lot other words much more
rare of any more interesting because
their words and so here's just what a
typical scholar program of the Allspark
looks like this instead of I get up
front here but I'm basically loading
some data that I had with this exercise
tokenizing in my words is what this is
doing creating temples were you know in
this case have to give each order making
a couple out of it were the words the
key and there's a scene camps at one
reduced mikey is an optimized group body
that just fits all the words together
because they're in a key position and
sums up all those counts so now i have
my words in my house and then the rest
of the logic for the next few lies it is
all right no i don't actually want to
sort my words anymore and i want to sort
by counts or in this case fruit my house
so this is this group by statement
things like make this a little bigger
I'd be a little hard to see in the back
that's a little better roubaix key in
this case because I've already said used
to count as two key
in a previous line now I'm going to get
all the groups of words together that
your place or whatever sort them you
know by by this count in this case I'm
sorry to sending this of the false means
and there's some final output that I do
and caching is an optimization what's
interesting though about this data is
that then you get down I write it out to
files which is you know that interesting
if you let me scroll a little better and
if you actually plot this what you find
is it way down here it looks something
like this where you have some words that
just appear an enormous number of times
and then it quickly falls off so you get
something that looks useless at like the
interpreters but notice I'm graphing
this data is I look at it I'm thinking
how can I get something more meaningful
out of this well the first thing that a
typical day sciency person scientists to
would do is say you know maybe think
Claude logs then this sort of extremity
would disappear and this first one we're
plotting I think it's just log than one
axis but the interesting one notice i
can even embed la tech this is the usual
la tech stuff for those who like me
wrote a dissertation in latte anyway so
i can event formulas let me evaluate
that cell against you soon for many just
disappear there so it looks like you
know regular math but if i actually
brought blog blog it gets it gives me a
straight line more or less except for a
lot of noise that actually turns out to
be a power law distribution which is the
idea that some things really occur a lot
and then other things Taylor off really
fast and then to tend to form a line and
you know if you remember here it was
high school math you can calculate the
slope of the line because you take the
log you can get the exponential
because it really like going up as a
factor and square or whatever and then
if you actually looked at this day at
the detail you've noticed these things
that appear lata roars like the key and
all the so called stop words that nobody
cares about because they don't tell you
anything this is this is actually the
place of Shakespeare and if you go down
here and look at these words which the
day the doors are actually lost at this
point you'd see that it's you know like
Hamlet and Ophelia and you know Henry
and there's kind of more specific words
anyway I just wanted to show you this to
point out why this these notebooks are
attractive is a tool of but then it
raises the question well what's the best
language here on you Scala but most
people would use Python or maybe the our
language so instead I wrote another
notebook that actually walks through
some of these advantages of Scala over
other languages before the spark
supports our skull at which it's written
in a job Python are and Java is not so
bad because it has lambdas now you
didn't have lambdas that block of code
rows like grouping things reducing that
would have been like several hundred
lines and then free java 8 in Java 8
would be maybe three times as much but
there's a bunch of things that just make
it so fast to write these kind of data
applications with relatively little
amounts of code and Scala has a unique
set of capabilities have been enabled up
but then so the other languages do to to
a degree so this is real
sort of an essay if you will in the
strengths and weaknesses of skull albury
specifically finish with weaknesses too
because it's not perfect well the first
point is that it's Scala is kind of a
nice compromise or less public dumb full
screen you get just a little more real
estate here screen there we go we see a
little better now people debate this in
the functional programming role but it
is kind of a nice pragmatic balancing of
object-oriented functional programming
it turns out that in my opinion big data
is really kind of the killer a
functional programming if you know that
ten years ago when everyone was talking
about what they were obsessing over it
was the end of Moore's Law and suddenly
going to get better concurrency and your
people who were thinking about this
proper eyes you know actually sort of
the functional style is better freakin
currency because it emphasizes things
like functions who don't have side
effects that only modify when you pass
in or about men even modify the orbital
you pass it and then return something
new but they'll change state immutable
values where if I create an object in a
state I'm never going to mutate it and
risk having somebody who dated
simultaneously leading inconsistencies
referential transparency is the idea
that you know if I call a function with
a value it always gives me the same
answer for that input that I can
actually substitute a cash answer in
subsequent implications it also has
implications for London I do caching
very nicely but I can even move that
function around because it doesn't have
much connection to the rest of the world
I can call it anywhere once i know
it'swork higher-order functions of the
idea
like if I'm doing a math over a list
I've passed a function to map the
function is a value it's called higher
order because it's a function rather
than a regular value and of course map
applies that function to each value and
let this will see all the examples of
all of these if any of them aren't clear
and then the really interesting idea
that's right I did I had to do this all
the time I double-click things to
highlight them and it goes an editor
hood so I'll keep doing this a good so
here's here's more markdown each of them
work but the different idea from object
re-programming that it's great to
actually separate occupations from
structure so I can have like lists or
trees or maths and I can have iterators
and battery teens and flat mappers and
reducers and stuff and just uniformly
applied which turns out to be fantastic
for closing the kind of stuff that we
build with data but it turns out that
pure functional programs tend to be weak
on facilities for modular ization in a
way I should build up big applications
and have nice boundaries so that it's
not just everything in global and in the
decision skull and some other languages
have made is to use continued use
objects as a modularity mechanism in the
in the case of Java versus say F sharp
or o camel markandkirsty you created
Scala said all right we'll just
basically let you do everything you can
do in Java we won't try to say only use
objects in a certain way we'll give you
full facility with objects but we really
like to encourage you to use functional
programming there you can use objects as
your modularity tool and it generally
works pretty quality and follow the good
practices that way a few of these cells
here's another cell actually you know
compares this to some of the other
languages and most of what I just said
is more or less true Python you can do
you know a nice balance of functional
and the doctor Jordan program in there
if you want to Java is starting to catch
up with out of Atlanta and now that they
have streams it may be more of a
functional style collection interface if
you will the our language written by
statisticians so it doesn't really fit
the usual paradigms of object-oriented
 per se is more of a statistics
language have a specialized language in
a way that matlab's have specialized for
matrices and stuff like that but there
are some folks that were you can do sort
of a version of object-oriented and
standard functional programming if you
want now a sub topic of this functional
versus object or a trade-off is an
interesting idea called traits that are
sort of the replacement for interfaces
in skul now before job a so nobody Java
8 added is you can now define default
methods for the interfaces that you
define in your interfaces are we declare
so we said more precisely you see is
this terminology a lot of C and C++
where you have a declaration and a
header file in a definition you know C
code same is true here in job I used to
be that interfaces could not actually
define the function they can only
declare it and that became a problem
because if I wanted to have a common
abstraction for logging let's say a lot
of times it's just obvious what I want
to make sense as the implementation but
there was no mechanism in the language
to do this I always have to do some sort
of had to implement the logging
interface but then delegate to some
helper method or upper class or
something but Scala and now job unless
you actually attach definitions to these
deck declared functions and that it
enables the mix in architecture record
mix in behaviors like blogging or
persistence or whatever more more
seamlessly implemented separately and do
composition rather than inheritance
there is actually some other things that
traits give you beyond interfaces like
you can have fields in trace so I can
carry along stay like yeah Chris the
longer I'm going to use or is in Java 8
you still can't have feel these
interfaces just messes but there are
ways to get rid of them this what this
point 1 B is something that a friend of
mine mentioned who's been doing a lot of
job is stream programming so this is
like a new thing they added to the
collections API or you can take a
regular job and collection and turn it
into essentially a lazy stream where
you'll just pull off values as if just a
supremely dated coming in and then you
can do this sort of functional kind of
processing life map over the stream flat
map and filter so forth and he found it
to be somewhat awkward to go back and
forth between these metaphors whereas
languages that were designed more with
functional programming in mind like
Scala tend to make that more of a first
class and this is the only way you do it
is you operate with these operators and
here's analyst list actually look at
some example so let's talk about a bunch
of functional style collections so I've
got a little method here a skull from
Wikipedia to determine managers prime
it's actually not a very good method
it's kind of pretty it does the job if I
want to make sure there's compiles in
this case let me just do shift return or
they can click this arrow button here
and just to verify that this work so it
compile it successfully tells me how
long it took so forth but it's just ask
Allah method it does some of the things
that we said are good for functional
program and there's no side effects I
using capsulation of a second method
inside here so I don't have that
function winking through the abstraction
there's a bunch of things that are nice
about and functional way but for the
most part it's just sort of the standard
method that you might write in any of
these languages so remember what he said
about you having higher order functions
here i'm passing a function to map it's
going to take each of these integers so
then generating from window under
conclusive i'm going to map over those
and for each of those integers i'm going
to get a function that returns a double
and this is one of the things I'll get
to later but this is really nice and
Scala that you have this literal syntax
and I put stuff in parenthesis and
automatically creates an instance of a
double for me in this case the integer
that I and whether or not it's cool it's
over prime so it will be no manager
bubbling it will be the type there's the
periods at the end this is really one
big expression and then I'm going to
group over whether or not it's prime so
I'll get to big records you know
wonderful the non progress along with
the prize and then the last thing I'll
do is map over that double and I can get
no next I saw that here here's how you
get the second element of the tunnel
underscore too here's the first and so
forth I account from wando's to sort of
historical thing as opposed to zero and
the last thing though I'm doing with
this map statement is I just want to see
true false and then the first thing is
going to be the true false because that
is what I use
key in the earth by and then how many of
them do I have and ironically enough and
look at the number it comes out that 75
are not prime in 25 price it's exactly 3
2 1 105 of interesting results bailout
us what this notebook and some of the
others will try to do is when you return
in this case a map it will try to show
it at a table for you automatically and
that's what it's done here not terribly
exciting in this case but it's pretty
useful if I just want to see a
representation of the data okay let's
look at the same thing in spark so here
so subscribe is basically inspired by
the scholar collections notice these
three lines map group I in the map again
with some setup actually create my
collection and it's park is basically
the same code there's some set up on the
first line and then the exact same three
methods and then finally I'll explain in
a second I could in a spark context is
the entry point into my cluster or in
this case my local machine parallelized
is sort of the analog of just computing
a range and turning it into sparks own
internal data structure called a it's
called an RDD you can actually see the
type here at that stands for Brazilian
distributed data set Brazilian assassin
there if you lose a node you can recover
reconstruct what was lost and distribute
it because I have the illusion of one
big collection of memory but it's
actually partitioned over cluster but
once I set that up and I have
exactly the same methods at my disposal
so I could have even put this code in a
library well the typing is actually a
little different it wouldn't quite work
to go from scholar collections to spark
but because of type in front I don't
care I just copy and paste it actually
works the reason this is here after I've
constructivist RDV what collect does is
it turns my big rdd into a local scholar
collection one major difference between
these are DB operations and the scala
collections is the rtd operations are
lazy they don't do anything until I
state get you so and that's what collect
does wear a scholar collections are
heaters they would ran right away so I
did have to do a little cleanup at the
end if you will but otherwise you know
once i have these for that affords of
you know mapping and flatten my
happiness all that you know just
transfers beautifully over to like big
data environment and we get the same
results and go it's not a probabilistic
risk it okay so that's kind of cool that
inspiration you know looking at the
scholar collections the guys who created
sparks said you know this work the kind
of problems are trying to solve
manipulating data it's sort of a that's
not really a streamlined yet but it is
the idea of i'm going to decompose this
I need to get from here to here I'm
going to decompose it into steps I can
have all these operations of my disposal
to do that deep so they they basically
copied the scalding API and your Python
and our code would like kind of very
similar that November's there with Java
until they added lambdas and the string
API it was none at all elegance to do
this
in Java the spark EV I did give you
these kind of methods in their job Apiru
it was just a lot more noise trying to
create anonymous inner classes for all
of these little simple methods but
elet's at least got wet one of the
powerful things though that Java still
doesn't have although there's rumors I
guess they may have this in Joe the nine
or Java 10 is an actual interpreter so
that instead of just writing unit tests
and writing and eclipse or whatever to
see what you've got you can actually
just fire up something like this
environment which is actually running
the interpreter or use the Scala command
and then interactively work they
experiment with your NPI's and so forth
so I didn't even show an example because
all the things you seem so far I have
actually been using the Scala
interpreter to do it behind the scenes
and everything that jolly has this
capability so that's not really too new
I mentioned this about ready to the nice
convenient Tuffle syntax because it's
really nice to think of Records as being
like temples of data and even some you
know sort of database tier you'll see
that that kind of description and Scala
makes it really easy to just put stuff
and rent the season bag and gotten
tougher and sort of love ceremony so
that's what I'm useful up here and it
turns out python is all mrs. is nice not
quite as flexible escala are his some
couple types as well with this this
syntax a little less convenient and java
doesn't have anything at all like job it
really could easily add double types to
the library about how is your wedding
heaven so spark added one for the most
common double which is a two element
Tuffle representing key value pairs and
i wrote this in scala but basically this
is what you would do it in java and
spark to span she a couple but yeah this
is really noisy right i don't want to
have to do
do you don't care get the types right
peasant arguments I just want to write
stuff like this idea what a most
powerful idea is the functional
programming and you saw that if you're
at Stephen oskis talk about airliner
minute ago is pattern matching which
lessens nothing figure out what we've
got I passed you something but we can
tear it apart into its constituent parts
really easily rather than using like
getters and so forth like that so here
I've rewritten the last spark example to
use the pattern management facility for
tuffets so instead of saying group I and
then here's your tub we'll grab a sec
melvin the temple I can use this case
keyword and also i have used curly
braces in terms out i could say i want
to expect in this case i know i'm going
to always get a two element Tuffle i'm
going to ignore the first field so i use
underscore to say don't care about it
and but notice i can apply it a nice
name which in this case is it prime or
not and then that the value that was
pulled out of the BB assignment reality
and in this case I'm just going to
return that and that's how I know
whether we going to group by cruise
pulses and do it again here matched by
criminality and values the again these
are variable names arbitrary choices but
I can read this code and no of these
fields me which is pretty nice and then
I just returned when I watch this online
yourself so it's a it's an elegant way
of tearing things apart but it also lets
you do things like use useful names for
things that are automatically signed to
the pieces of ID yeah the most a prime
number that if you go about you and
about 70 full oh you know why I fix this
is because i went up to but not
including right now right yeah good cash
thank you for that it's prettier awake
so yeah you always have just somebody
the audience that's a week at six
o'clock so there we go your are you
happy i'm very happy thank you all right
and just to show that this is pretty
flexible actually in airline every the
same it's actually a pattern match
expression you do that muscala to hear i
can drive to deeply nested Tuffle and
then just did an assignment on the left
hand side where I just you know named
variables that happen to match the
structure and it just automatically run
that again automatically extracted all
the pieces in a nice way pretty cool i
use this a lot the only catch is i have
to know exactly that i have the right
number of elements and right investing
otherwise they'll get an exception but
if I know exactly what if god this is
really handy for like having multi
return values from a function and just
pulling it apart as it comes back then
this is something I don't think you can
do it any of the other languages I don't
think Python do nested tough estate by
the way home requests are welcome for
this as a member lee expert life on an
are so by making the space welcome it
correctly Oh case classes are a really
useful feature in scholar that also
proved to be very useful they're often
used to represent records in spark the
idea is this case keyword says not only
am I just defining the regular class but
I want you to automatically take all
these arguments and make them fields I
want you to
having to string equals hashcode and do
it the right way not the a QA the most
of us end up doing and I also want you
to build of the something called a
companion object basically some static
methods that are used to do this pattern
matching so I'll be able to beam pattern
matching on this person type in just a
minute just because like what the case
he were there and also there's some
other things they throw in that are very
nice it's a really useful way of just
building these little structural classes
that keep it old state and then readable
by default and I would have to put in a
fuckup vows are a bar keyword i want to
say first name to be beautiful but i
could do that i want it's another
example of scholarly do you do stuff
that you might want to do but
discouraging you from doing it by
default so here I list a few of the
things you get but here let's see those
is some pattern matching with this and
I'll I'll actually do it in spark all I
could have done this and just regular
Scala so I'll take my spark context I'll
turn this make up to people hear me a
completely bogus age independent I am
who actually co-wrote the stock and in
control he's actually got it created
smart notebook so we've got two people
we're going to turn them into a rde like
parallelizing this collection over a big
cluster and I'm going to tell you here
is map over this collection and pattern
match on person instances and then
extract first last age in this case I
just do something stupid return double
but when i do that you know I get what
you would expect the two colors instead
of two persons so that's really that's
really why the case keywords used both
pattern matching
the ears been really because it gives
you this duality of construction and
deconstruction which is just fantastic
for getting work done quickly and like I
say case glasses are often used with
spark to represent the schema of your
records time all right yeah and some of
the other like really not the other
three languages actually support
anything like this at all they support
things like regular operation parsing of
strings which is our old standby but not
this sort of structural decomposition
you have to go to something I passed
over Emma or you're lying to do it or
close your nap this is facility oh so
one of the big arguments of course in
our industry is whether you should be
statically typed or not oh I want to get
in that argument here but what if what I
will say is for it does have some
convenient properties in this case first
of all scholar will interpret in furtive
in a lot of situation it's not all
Pascal can actually do in almost all
tough situations but here it I didn't
say anything about the types but it just
inferred that I was working with the
injuries and the trivial thing and if we
look at you know one of our examples
from before there's nothing in here at
all about types in this grouping by
finality example and yet it in first
correctly and conveniently that I have
an RDD when it's finished and this
notice that you can actually use the
tumble siggno young quote things or
rather breathless II study
and in the type of signature as well so
this is actually at uppal to is the
record type in our TV is like a
collection on top so like this is so
actually did some stuff with the Python
API not long ago and it actually was
really kind of frustrating that when I
would do something like this which is
via the syntax is very similar I
wouldn't quite nobody had when I was
done I always ended up like printing the
first five or something just to see what
I ended up with whereas at least here I
know of the structure yeah so guy
described it a little bit more to tell
you could also write that signature like
this but like this so you can see a
little better still they're bears yeah
so I could have written a type signature
like this but it also lets me write the
same sort of special syntax like I'm
used for value suppose two types so for
me this is a nice balance of inferring
types to keep that stuff out of my way
most of the time but also catching a lot
of errors and tell me what I need to
know what I need to know yeah like I
mentioned as he was kind of a hassle not
having this in Python and Java course
instead be tight but you have to won't
be more explicit about typing those
things except for like lambdas and stuff
and r is also a lot like pythons
dynamically type what are they
interesting things you run into when you
do java this is kind of a technical
point for java versus skull it is scala
actually gives you the illusion that all
the primitives are regular object types
reference types you write like ants duh
you know to burp are sir that's a good
example you do it like into to float or
something if you want to
flow whereas in Java it's very clear
that you're working something that's
entirely different from you via type
hierarchy but there are cases where you
have to do these boxing conversions to
go from like a primitive into a integer
when you put it in a collection and that
kind of duality doesn't exist in skul
you can just have your list of strings
or list events and it just works now
what is actually doing though it's not
completely perfect it is actually
wrapping these integers which is really
bad in big data you've got billions and
billions manager so it's not a perfect
solution it's a convenience for us there
is a annotation called specialized your
I'm clicking things again oh we can just
point at it right there that will say
all right actually I want you to
generate customized implementations for
a B and C like editor float injured
double and long or typical that won't do
the boxing but it also is not a perfect
solution but nevertheless it's nice
light than just uniformly tree
primitives and reference types
identically in every method actually
always returns something even when you
would normally return void in Java you
can return the unit and scholar which is
a type that basically functions the same
way but it gives you a uniform syntax
which makes your thinking more you and
we also get really elegant surprisingly
delicate tools for creating the main
specific languages this is one of the
things that Ruby's famous for that you
can kind of make lame little languages
that are
Ruby code it looked like something that
domain expert would write because if
test tools are a great example of this
if you've never looked at our spec Ruby
or though I forget what some of the
other tools are cold better than Python
even a but for some reason skull happens
to have a number of facilities that even
though it's statically typed it's
actually really nice for writing these
little domain-specific languages an
example i have here is actually one it
was it's basically implementation and
scala of a python api it's was so
basically copied from the our language
what's called a dataframe api it's
basically the way to do sequel queries
and spark so you start by doing some
imports that aren't really that
important for us to go into but the one
thing that is important to seek the was
called the andre to this api is
something called a sequel content once
you have that creative I'm go ahead and
do that again actually I think I won't
do it probably haven't evaluated all the
cells Ron you can see this output that
your sins because I just ran the whole
thing a little bit little while here I'm
going to use the sequel context to
recent Jason data so i need to figure
out what it is and i set up environment
variable in advance tell me where it's
located so I'm just this is like the
typical Java system environment called
to get an environment variable you can
see it's going into this directory here
nothing too fancy but the sequel context
is in the ability to get a reader and
then I can say all right I want to reach
jason and here is a JSON file that
represents an airport data for
commercial airports in the US nice thing
about this is it actually vs. schema
from the JSON files so here's the schema
came up with based on that data so the
airport is like you know that the main
used to like o'hare city country
latitude longitude I atas the codes you
see on your tickets like Ord prepare atl
atlanta things like that so we can
actually ask him for some of the records
so if you do airport show it starts
printing out things that you're used to
seeing you've ever worked with like you
know one of these database interactive
tools you just press out the first 20
records and caching the summons this is
really abstraction latex you have to
tell us apart well if I'm going to work
with a state over and over again don't
keep rereading and every time I do a
query just cash in memory so here's our
you know the first 20 records to this
Airport data and we can actually write
typical sequel queries you can just do
sequel and then a string is to rate your
sequel query that's a messing method
available on this Airport data frames or
we can use this idiomatic API and this
is really kind of the point I was
getting up to this API is very similar
the way to brighten in Python which is
an entirely different language where
they have a sort of dollar idiom for
saying I want a group by the state and
cap on a country and then I want to
count they capsized those groups and
then I want to order by the countess
ending so it's this sort of an idiomatic
way of writing sequel a little bit more
typesafe than just embedding a strength
that you won't know it's a runtime well
you got it right and when you do this if
you run that query print the schema you
have nice little tools for seeing what
you ended up with so state and country
where my keys for grouping in an account
and then here's the descending counts of
the results where it turns out Alaska
it's the most airports they're mostly
like these little bush airports out in
the middle look like you know kill them
Denali National Park that's it
are over there and texas is up there
california and center court here said
here's a little bit okay but the thing
is i could have also written this as a
sequel query and it would have been
parsed a sequel and we're gone into the
same run time but they use this that
they'd be I guess technically this is is
more affluent dsl or fluent api the
natural dsl the other some debate about
those terms but the point is that when
you need to do something that hopefully
will be familiar to people coming from
another language community and you want
that or even a domain where they think
in terms of some sort of like rules or
tax law or something the language
supports being able to build that kind
of stuff early nicely but to be fair you
could have done this in java too so it's
got a lame argument it's the best I can
come up with anyway so it was actually
borrow this API was that inspired by a
python api called data frames which is
actually inspired by our put the same
thing okay Oh see how we know for time I
wanna make sure we all right direction
you're done so I broke this into two
halves and I'm not going to go through
the second half because we're almost at
a time the reason I call this a few
other things is the stuff that's after
this is really valuable for regular like
Java versus scala developers but
actually less useful for big data people
or Dave signers playa for example
where's it go here well sometimes a
service girl not sure I can find it
quickly but well the things I was going
to mention like recursion so Scala does
blue patrolling Hale calls automatically
for you that's not really something you
using data because usually you're your
system is doing any sort of iteration
not you yourself things like that are
there is a tail recursion optimizations
actually very useful if you're a regular
developer not so important for days
scientists and so what this an actual
section turned into is just a laundry
list of all the things where I think
skull is better than job so just
personal preference i guess but i think
there's some interesting things to
consider here for design greater like
steel class hierarchies or throw the
idea what if i declare an abstraction
and i only want to be able to declare
sub classes and never let anybody else
declare something else as a sub class
because there can only be the ones that
I'm declaring a good example is
something called the option type which
is an alternative to know where I either
have something and that's going to be in
sight of some or som e or I have nothing
and that will be inside and done and
there's no pub there's no middle ground
we're not worried about the law of
excluded middle if you know this we're
going to assume it can only be in those
two states so I'd like to be able to
find the distraction it captures that
but never let anybody else possibly add
a third subtype and as scholars amazing
story they're called some types to be
technical anyway let me go to the bottom
which is a few things worse college
isn't so great and just to prove that a
lot of complete school a drinker in some
of these actually if you're really
interested in them especially if you're
doing Spartan I think did a talk at
straw to San Jose and here's the legs
for those slides that you want
see those who I go into a lot more
detail I can talk about now well one
thing very practically is that r and
python much richer data libraries and
scala job do so there's a lot of stuff
you just don't have with with the
scholar job like really good journey
libraries it's hard it's hard heart you
can set up in the JDM itself has some
issues it turns out that the fact that
arrays are indexed by the integers is a
huge pain in the ass because if you
think about it that means you can never
have more than two billion of anything
and already because we also going on
site types and if you have like in our
mouth by your way like your serializing
something object into a bite around
you're stuck with two gigabytes is how
big big and yes amazon just rolled out
two terabyte you know instances in AWS
right so we've been able back to kind of
the problem we had with das wonderfully
dating my stalker the young used to be
with Josh you why would anyone need more
than 640 kilobytes of memory well we've
pretty much figured that out quickly
there's no we actually a lot more so
they came up with all these crazy
schemes to layer indirect addressing of
memory segments and that's kind of what
we stuck with you can't just allocate
like big you know 40 giga byte array
anymore you have to break it up so this
is actually a real problem we don't
belong indexing it turns out in the JVM
memory model it's garbage collection
away plays out objects is really
flexible really powerful at service
really well but it completely sucks for
big data and the problem is when you
have a billions of the exact same thing
you want to use the the regular model so
just as an example here's a diagram of
you know what a person type might look
like it memory or I some reference to
him personally
Spence maybe the person s names it's the
string that's another reference offic
memory somewhere else if their age is
integer I can inline that a bit more the
compilot ago but you know if I have a
pointer to their address that's a
reference to yet another object has its
own stuff like other strings it turns
out every single arrow here is a cache
miss and I'm doing this billions of
times to process these peoples and all
this stuff is loop it's a garbage that
just torturing the garbage collector so
what we really want is something like
this where we encourage objects in like
a single array of bytes that hopefully
fit in the cache align the CPU and this
is actually what spark is done in the
dataframe API where instead of
allocating stuff it looks like this they
allocate a lot of fewer little bits and
stuff and it all fits in a cache line
like so and it just quickly with this
palace is structure some fields could be
no and so they have a big field for that
so if a bit if the field happens to be
Mel there won't be anything a
representative field the rest of the
record and then they have eight bytes
for field what's left if it's a
primitive like a long that fits in a
bicyclist in line and otherwise that the
value and then a bite field will be an
offset into this variable length section
for things up strings and stuff so this
is all knowing far better for cache
performance it reduces the amount of
garbage and just makes the thing they're
getting like factors 100 improvement in
performance just by doing these kind of
optimizations and the diversion of
scorpions coming out the summer spark to
is going to add another 10 x or more
performance by doing things
there's little queries that I showed you
they're just going to generate bytecode
for those they're not even going to
compile them to like you know method
calls and stuff they're just going to
generate quite good for those kind of
queries to get it down to its absolute
barest so lots of cool stuff you can do
when you get good abstractions and you
layer underneath it lots of room from TV
now there's also some weird problems
with the way the repple works the
interpreter of its really causing a lot
of pain from one of my big banks in New
York we may have to actually reengineer
the repple to work better for really big
data sets anyway probably had a time
here but though so this is basically my
argument for why Scala is actually a
really good language not just for
engineers but for your data scientists
but like you say it's not</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>