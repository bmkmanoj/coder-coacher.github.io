<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • Beyond JVM: How the Platform is Evolving for new Languages &amp; Features • Charles Nutter | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • Beyond JVM: How the Platform is Evolving for new Languages &amp; Features • Charles Nutter - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • Beyond JVM: How the Platform is Evolving for new Languages &amp; Features • Charles Nutter</b></h2><h5 class="post__date">2015-07-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0cpGFO6-giU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay we'll get right into this so the
preliminaries I Charles nutter I work
currently at Red Hat various open-source
things but mostly on JRuby JDM language
stuff making it easier for new languages
to approach the platform expanding the
platform to put a native language and so
on so the structure of this talk is kind
of going to be a story a play in three
acts talking about work we've been doing
on junior beautiful pastels decade now
the challenges that we've had to meet
dealing with the JVM and some of its
limitations and how the JVM and jari
they're evolving together to make this
easier in the future so we'll talk a
little bit first jruby horse is
basically just ruby on the JVM products
been around for quite a while and 2001
was the initial commit to JRuby was
basically just started implementing the
parser I think mostly they were looking
at doing it for tooling and then decided
hey we've got a parser we can interfere
we can start building an actual could be
run time on top of the jury yeah but for
about five years or so it didn't really
move very quickly it couldn't run most
Ruby code it was based on an older
version of Ruby the rails singularity
that happened within the Ruby community
was during that period and so around
2005-2006 I got involved in the project
and we thought well why can't be is
there any reason we couldn't actually
have Jay Ruby on Rails Ranji rails
running on top of the jamie i'm taking
advantage of JVM libraries with JDM so
and in 2006 we actually were able to do
so we presented at javaone show running
on top of Serie B on top of the GBM
there very early primitive pack together
form but it was soon as a beginning
that's what we're do this
and we're working on JRuby 9000 now this
is actually the ninth major release of
JRuby that we've had since 10 so when I
was the first one and we're now it's a
jruby that is orders of magnitude faster
than the one from 2006 we're looking at
running much more radhika running rails
applications in production for real
customers like square it's like
SoundCloud on JRuby as key parts of
their members right it is a lot of work
to get to this point this is what I ran
last night just to see where our master
branch is that this is 29,000 commits
that are getting to cut into JRuby over
the years and this doesn't count all the
work we've done on maintenance branches
for each of these releases too so
there's obviously a lot of work but why
would we suffer through with the
challenges of bringing a language like
Ruby to the JVM for 9 years 10 years now
but the bottom line is that we were all
Java guys you know the JVM well we know
its skin tones at ins and outs or at
least we thought we do all of the things
and outs and it it just fits the way we
wanted to build Ruby want to be able to
focus on implementing the functionality
of Ruby the language Ruby's core classes
and not have to build our own in
our own GC all those other pieces that
the JVM gives us for free so the JVM is
kind of this magical black box that we
can just throw our stuff at throw our
code at throw a ruby interpreter do
little byte code generation whatever and
it will run our code great and
everything runs fast except my brother
and that's what the challenge of the
interesting part working on traitor
nine years is actually in when the JDM
can't do what we needed to do that next
step how do we cross that line of
impossibility the jvm can't do it most
people just give up we don't have an
option so we find new ways to do it so
we're left with this challenge we want
to make Java and Ruby work really well
together and not only work well together
as a JDM language but work well as a
ruby implementation so Ruby users get
the experience they expect out of
running JRuby running Ruby on top of the
JVM so we're going to talk about three
specific areas here that have been
challenges for us and how the JVM is
evolving to make it a little easier in
the future for folks like you and for us
to continue working on here so first off
we're going to talk about just the most
basic primitive concept in Ruby where
all the performance is generally lost
all those method calls that we make in a
typical Ruby application here's a metric
Iran real quick just this morning just
to see how many simple calls and this is
like just calling the Foo method or
calling a dot foo on some object so
there's other types of calls doing super
in vacations and whatnot that aren't
listed here but you know for doing
things like gem installing rails and all
of the dependencies that it needs
315,000 calls that are made Ruby calls
making generating a new rails
application 606 thousand and this is
generating the app plus installing
additional dependencies needed for a
running application I did some quick
crud application and just ran through
all four operations about 16,000 to do
all four crud operations so that's you
know roughly going to be like a 4,000
per request just to do a really basic
display of some data so we need to make
these method calls fast and the problem
we always had in JRuby we've got a
simple example here foo calling bar
calling Baz and on Java five and six
what we were building jaber beyond most
of the most of its lifetime we've got
all this extra call logic in between
these we need to go and look up the
method type we need to make sure we've
got the same class in hand that we
expected last time we've got to do that
and unfortunately the JVM is very bad at
optimizing through a generic piece of
code and buy generic I mean you've got
lots of different call paths that are
all going through this same piece of
code that becomes a optimization black
hole nothing can get through that that
set of calls nothing can get through
that intermediate call logic that we
have and so most of the great JVM
optimizations we hope to get out of
building Ruby on the JVM don't actually
materialize and that's where invoke
dynamic comes in how many folks are
familiar with invoke dynamic at least on
the surface okay so most of you ever
heard at least a little bit about it
we'll go through a little history here
the JVM authors actually did talk about
having other non Java languages on the
JVM and didn't at least it didn't
exclude that possibility in the future
and because of that lots and lots of
folks of targeted languages on the JVM
some of the earliest ones were in you
know 96 97 shortly after Java actually
was first released some of the more
obvious more well-known ones like Jai
thon were actually before 2000 dry now I
think was before 2000 or early 2000s so
very early before the JVM was even
considered to be a multi-language vm
people were building languages on top of
it the problem is that although the JVM
was always expected that it could run
other languages it was never designed
for a lot of these other languages there
are tricks like dynamic dispatch that
didn't work very well with a statically
typed vm there are optimizations that
dynamic languages or other new types of
languages would need different sorts of
call patterns so we're stuck in this
kind of this bubble this world of JVM
opcodes which is pretty much all we can
work with and and there's three primary
areas of operations that you can perform
at the JVM level you can do calls
invocation you can access fields on
objects set and get so putting data out
there for other threads to see an array
access again similar sort of thing
putting data into some data structure
that's going to be passed around and
used by the world these are the pretty
much the only operations that will
potentially have some side effects some
visible side effect outside of that
operation and then there's all these
other flow control and stack
manipulation new
Eric operations that don't really have
any side effect their kind of functional
operations at the jvm level but they
serve to move data into the right places
for these key out operations like
invocation and and data access and if
you're going to build a language on top
of the JVM you pretty much have to stay
within here you have to color within
these lines and that's where invoke
dynamic really came into really really
enables the platform to do a lot more
than it did before this is from the JVM
this is the quote that I mentioned in
the future we will consider bounded
extensions to the Java Virtual Machine
to provide better support for other
languages this is back in 1997 talking
about the possibility of adding
additional features just to support
other non JVM non Java languages and
that leads us the jsr 292 which was the
java 7 jsr for invoke dynamic invoke
dynamic is basically a user definable
bytecode so we've got our invokes and
we've got our field accesses this is a
special byte code that we program
internally the JVM asks us what to do
when it runs into this bytecode rather
than having a predetermined set of
operations that it knows already it also
comes with a set of fast method pointers
called method handles you've seen the
java.lang invoke package those handles
are now getting the point where they're
much faster than calling through
reflection they can optimize much better
the JVM can actually optimize through
those handles through those pointers as
if it was a direct call that's a very
very empower phul way to start wiring
things together just with simple
reflective handles that optimize nicely
and then as stuff for caching and
validation the general idea was that we
wanted to build a bytecode that to end
all byte codes we could we could have
this one opportunity to make a change
the JVM spec which is really really hard
to do we need to do it right and so it
was made very customizable very
configurable and it's it's actually
starting to play out really well for
different languages talk about a few
users obviously JRuby and groovy existed
before invoke dynamic and we have grown
into it jruby was the first adopter of
invoke dynamic we actually started using
it before it was any inner release
before it was even finalized as an API
just to try it out
and help test it kick the tires I
believe groovy has a mode you can turn
on now that uses invoke dynamic to great
effect new implementations as well Diane
Diane Jas and nas horn built on top of
invoke dynamic from the very beginning
and so they're they're getting the
advantage of having that from the day
one and designing the whole system
around it this is a little known fact
that Java 8 lambdas and the lot of the
Java 9 features are also building on top
of invoke dynamic all of these
roadblocks that kept the JVM guys the
jdk guys from adding features to java
now start to get swept away as well we
can add java features based on invoke
dynamic that were impossible to do
before or at least hard to do
efficiently very exciting stuff and I'll
talk a little bit more about all those
work later so now what exactly is
invoked enemies any actually know how
invoke dynamic works not one awesome
that means everybody was going to get
something new out of this okay so invoke
we've got our four basic invocation
types static for you know static methods
simple enough virtual when at the
compile time you know the concrete class
that you're calling against interface
when you're calling against an interface
you don't know what the concrete class
is JVM has to figure that out later and
then constructor and super calls which
are called special invitations and these
boil down into four different byte codes
here with some of the the garbled
internal mess that represents the method
they're actually going to call their and
if we take these four operations and
break them down into what actually
happens at the JVM level you'll start to
see that this is largely the same thing
for every one of them there's minor
changes how it looks up methods how it
caches optimization guards that'll be in
place in some cases but not in others
it'd be nice if we could take those
pieces those individual pieces out of
each of these invokes and stitch them
together however we want do caching our
own way do method look up our own way
and that's where invoke dynamic comes in
all invoke dynamic does is call back to
your code you set up call site caching
you go look up methods you go stand up
values in memory and then you give that
back to the JVM and say here's how you
do this operation here's how this invoke
dynamic
supposed to work it does that from then
on with these optimized method handles
with all the caching logic that you've
got and ideally optimizes it as if it
was are straight thru static call that's
what we're looking for a bit more
graphically we got the this kind of a
switchboard metaphor here so up at the
top we make our call and that's an
invoke dynamic that we we need something
out there some piece of data we need to
get some method we need to call or
invoke dynamic starts right there that
goes to the JVM the JVM then we'll look
for our bootstrap method it's going to
call back into our code into user code
from the JVM level bootstrap method
finds the right target wires everything
up and uses method handles to do it
that's kind of the wires here connecting
one call to another those method handles
then will eventually terminate on that
target method or Target Field or some
piece of data and before Java 7 you know
similar sort of process was done for
every every call in Ruby every jruby
call but we had to do it every time we'd
go back through this process look it up
maybe check a cache do the call look it
up check the cash do call its and so on
over and over again and lost a lot of
time and then again the JVM can optimize
through that the magic with invoke
dynamic is that after this first
bootstrapping process all of that stuff
disappears and you get direct calls from
that invoke dynamic to the target method
from that on and that is where the real
magic happens so you look at JRuby on
Java 7 that intermediate logic
disappears or at least as far as the JVM
is concerned it disappears we wire it up
in a way the JVM understands a little
better we get straight through call pads
as far as the JVM nose goes directly
from food a bar to baz and all those
great optimizations that we get out of
the JVM like inlining actually do happen
and we see this happen Ruby in line to
in line into Ruby Java in line into Ruby
vice versa all of this stuff actually
does work and works really well on
current JVMs so invoke dynamic in JRuby
obviously we're using it for method
dispatch it's pretty simple our logic
look up the method we have a cash that
says what the last type was and whether
it may
matches invalidated if the class changes
or if some new type comes in and we
might chain those so we have two or
three types that are available three or
two or three types of a two or three
methods cash we also use it for places
like constant look up in Ruby constants
are defined lazily they can also be
forcibly modified it's generally not
done but it can be done so we have
similar caching logic we go out and look
up the constant we have a cache that
rarely is broken rarely has to be
invalidated the JVM then can see that as
a true constant even an even though in
Ruby it can change at any time and it
can optimize it as if it were a constant
written in the code directly all right
now how well does it work for us this is
a chart basically showing how much
faster we are than ruby 1.9 the
performance numbers for 2 point 0 2 2.2
aren't significantly different here
these are this is j Ruby baseline this
is I think jruby 1.7 or so on a Java
five or six where we've got that logic
preventing the optimizations from
happening and this is what it looks like
with invoke dynamic just by switching
these calls to use invoke dynamic and
changing nothing else in the runtime we
get this much improvement a significant
improvement on the red black there so
much though that we actually wanted to
explore this a little further so this is
a benchmark of building up a big red
black tree traversing it a few different
ways deleting elements and then you know
dumping it and trying again on this
particular benchmark Ruby 20 and a pure
ruby implementation of red black took
about 2 2.5 seconds or so and this is
this is reusable reason why people in
the Ruby world will turn to see
extensions they've got an algorithmic
thing like this a data structure a heavy
heavily hit data structure that they
need to make fast and so they write it
and see instead there's only so much you
can do to get the standard
implementation to Ruby to run faster see
is the usual go-to and that does help
that brings it down to about half a
second for every or every iteration of
this particular benchmark and now this
is the cool part JRuby with the same
pure ruby implementation running the
same benchmark we're actually faster
than Ruby with a see extension and we're
seeing this more in
more that rather than having to go to
sea to get performance out of Ruby code
people can just move it to JRuby and get
the same sort of boost alright so back
to the users talking a little bit about
how they are using it neighs horn I
mentioned earlier built on invoke
dynamic from first from the get-go
they're doing a lot more advanced
compiler work advanced optimization they
speculatively say okay this is probably
going to be a method that uses intz or
lungs and then they use invoke dynamic
as a wit as an escape valve when it
turns out you're not doing this
algorithm against along it bails out via
invoke dynamic generates some new code
and then goes into that modified version
very cool stuff looking at performance
starting to be comparable to the v8 on a
lot of benchmarks just by a little bit
of extra magic on top of them both
dynamic and there they're very
interested in turning the work they've
done on nasrin into a general language
framework so everybody that wants to use
invoke dynamic a dynamic language type
specialization optimizing numerics and
whatnot we'll be able to plug into this
and have the same thing so that's really
cool i mentioned that lambdas in java 8
use invoke dynamic it's if it works very
similar to inner classes but it lazily
generates that class it doesn't need to
have that on the on the command on the
file system it doesn't need to be in the
jar you don't need these dollar zero
dollar one and all that invoke dynamic
is used to basically generate it on the
fly if you actually get to that point
and this is an example of a bytecode so
we've got a sort that we're doing with a
simple integer compare that's our that's
our lambda there the invoke dynamic goes
out and generates that class wrapping
all of this code that is used for the
the actual string length calls and the
compare right here and then that class
is returned and plugged right into the
invoke dynamic so it's never never has
to be generated again it's as if that
virtual that anonymous inner class
always existed but you don't have to
have it on the command line you don't
have to have that extra that extra
hassle of dealing with the files this is
the uses of invoke dynamic get cooler
every year there'll be no java nine is
going to be doing generic specialization
reification for collections you will
have an ArrayList of int that operates
like you expect it's one of the key
features that's coming along and it's
built very heavily on top of invoke
dynamic again at the point where you
need an ArrayList of int you need a
primitive collection which we never were
able to have before without specialized
libraries it will go off ask the in
invoke dynamic call into some JDK level
code generate what an ArrayList of int
would look like and then plug it right
back into the invoke dynamic so we can
actually have these reified collections
without generating them all ahead of
time without having an ArrayList event
and an ArrayList of char it'll all just
work and it'll all plug in using invoke
dynamic under the covers very cool and
they've got working prototypes of this
already if you want to try out the the
project valhalla branch of OpenJDK okay
so i think that the method call a
performance getting Ruby method calls to
optimize the way we want is is kind of a
solved problem at this point there's
obviously the type specialization stuff
we still need to work on the next area
that we really ran into and we still run
into to this day is that Ruby is largely
a wrapper around UNIX or a wrapper
around POSIX see looking at a few
examples of this this is the air no
class and all of the classes defined
under it for for generating errors and
these are basically just all the air
knows that you'd see at sea level stuff
that you would recognize if you're
building any application see another
example the etsy library in ruby has a
number of methods for accessing the
password file on a unix system group
information user information and so on
and they didn't even bother to make
these names nice these are just the sea
functions pulled up into ruby directly
so if we want to implement it and have
all the ruby code out there work we need
to be able to do these things we need to
have these operations available to us at
the JVM level how do we do that and
that's where having better native
interop at the JVM comes in so we look
at the JVM world here we got our nice
quaternized happy area where there's no
native code its write once run anywhere
right and then we've got this native
world where we got all these cool
library
is that we need to call we've got
operating system features that we need
to access new libraries new graphics
subsystems and there's really no good
way to do this right now as part of the
standard JVM is a great quote from John
Rose if non Java programmers find some
library useful and easy to access it
should be similarly accessible to Java
programmers and this has not been the
philosophy of the Java platform for most
of its lifetime the the philosophy of
the Java platform is generally been if
John on Java programmers find some
library use easy useful and easy to
access we should port that to Java so
that we can have it on Java and then
what everybody who's done a port or a
copy of a project knows that you're
going to get out of sync you're going to
have to maintain two different versions
it's kind of stupid that we can't just
call down into some of these libraries
and use them directly right I think it
seems stupid so the example I want to go
through here is what if we just we just
want to get the pit for the current
process we've got some monitoring system
or some administrative DevOps sort of
thing that needs the pit so it can shut
down send signals whatever this is not
possible to do on the JVM right now you
can't even get the current process ID
the only way that you can get added is
if you're going to write some C code
yourself and basically write an
extension for the JVM so for jni is
anybody done Jay and I okay there are a
few folks I feel for you in Jay and I
we've got our user code at the top that
needs to make some call there will be a
jni native endpoint on the Java side
some C code that represents what that
could that that native logic is supposed
to do and then eventually we get to the
target library and now if you start
looking at what Jay and I does its kind
of deceptively simple here oh we just
stick native on there and we've got a
native call it's like magic suddenly
down calls into native code and we're
good to go but then there's all this
stuff that has to be done behind the
scenes that has nothing to do with the
Java code you just wrote you need to
generate the jni headers for all of your
native endpoints with some nicely
mangled function name so the JVM can
find it
you need to implement that for this case
it's obviously fairly simple get paid
returns in an int that fits right fits
nicely into the j long we could do some
casting logic to make sure it fits right
or signed right for a particular call
but but this is not too terribly bad so
we got our header we've got our C file
now we have to build it and this is
where I fall down because I don't know
how to do any of this crap I don't know
which libraries I'm supposed to connect
up and on a per platform basis how I'm
supposed to do this different compilers
do things different ways and then you
got to think about users either you're
going to have to build this ahead of
time for all platforms or your users are
going to have to build it whenever they
need to use it and that sucks for them
too so you're going to have to have make
files you're gonna have to have other
stuff that's not part of the Java world
there got it there's got to be a better
way to do this all I want is a damn kid
right so that were that's where Java
native runtime comes in so the Java need
of runtime is a java api for calling
native code we have a sort of a layered
runtime a bunch of different utility
libraries that you can use if some of
you may have used j or seen java native
access around very similar we've taken
some we takin jnr a little bit further
it's essentially a foreign function
interface you've ever dealt with
anything in the sea world that uses ffi
it's a foreign function interface for
the JVM I mean there's the organization
on github where we keep all the projects
jnr so what we can actually do with jnr
is go from this situation where we have
to implement everything in purple here
including stuff that's not in Java and
has nothing to do with what we actually
want to get done to this we've got some
user code that defines an endpoint in
some C library says here's the library
here's the function here's the
parameters and return value just let me
call the blasted thing the jnr stub
underneath that figures out how to wire
that into Java types the J and I call
actually makes the invocation and then
all that logic is pre built for every
platform so we can just ship a simple
library plus our two or three lines of
Java code so this is what get pit
actually looks like in jnr we define an
interface that has roughly the function
prototype of the c function we want to
call
we tell the library loader to create a
version an implementation of this
interface based on the function coming
out of some C library and then we have
in hand an object that we can call those
see functions directly on much better
than the J and I version i think i
mentioned the layered run time that
we've got we're going to go through a
few of these JN a kind of stops at the
point I just showed you can call into
functions you can define function
endpoints but there's not a whole lot of
support code available for you first of
all I want to mention the platform's we
don't want to have to rebuild this every
time we go to a new system so we ship
with support for all of these platforms
tiny little native stub that's in the
jar unpacks itself as needed and then
dynamically loads if anybody has a
platform that's not on there I'd love to
hear it we do have yes openvms is is I
think working now for all you openvms
users out there that really want to jump
on that as400 z Linux mainframe Linux
we've got all the support for the stuff
there are certainly other platforms that
are come along but it's very easy for us
to build a small stub on any platform
stuff it into our build and it's good to
go so Jay NRF if I is basically the code
that I showed you that's the the jna a
equivalent that's the basic library for
for wiring up some C library and again
there's that example pretty easy to do
and of course once you've got the basic
plumbing wired up it's easy to add new
functions to this as they come along but
of course a lot of these things are
standard operations that everyone's
going to need like get pit we don't need
to have everybody in the world write
their own get pit interface and their
own load logic to get the get pit
operation available so we built our
first layer on top of j NR j jnr posix
which basically pre wires a whole bunch
of common POSIX functions that most
users are probably going to need this is
a small snippet of the ones that are
defined things like chmod chn that you
still can't do as well through JVM api's
fork is on there and does not work very
well you can imagine what sort of
horrible things forking a JVM would do
you know things like geez
GC threads that don't come along for the
ride kind of a problem what else is good
our get paid call is in there kill doing
actual signal operations getting raw
environment variables and setting by our
variables into the current environment
down at the bottom hard to see here
we've got POSIX spawn which is like the
ultimate tool kit for spawning
subprocesses and it incidentally there's
a Jan our process package that I'm
working on it that skips all of the
process builder nonsense and goes
straight to a raw spawn call so you get
real real channels out of that process
you can do select you can actually
control interactively a subprocess
pretty much impossible to do through the
JV jdk process api's so here's what our
get pit looks like now we go to posix
factory we create a new instance you can
create your own handler you can make air
nose raise an exception rather than do a
return value things along those lines
whether you want the native features to
be enabled or use only the pure java
emulation of POSIX api's and then at the
bottom once you've got it in hand you
just do the call so we've got POSIX we
got a POSIX instance for every JRuby
instance and that's where all of our
logic goes to call through to the native
functions so now we need to build a
little bit more on top of this so what
if we do a c-level open call to open a
file or a socket call to create a socket
at the native level and we've just got a
file descriptor in hand well as not much
we can do we can go back and do a read
call at the sea level and keep going out
to the native side every time I want to
do a read but then you get things like
select which are more complicated
they're different across platforms the
api's don't quite match there's low
level control over these file
descriptors that you really can't do at
the JVM level so that that's where we
get e NX ioj in our enx i/o extended
native cross-platform I oh it's
basically niño compatible so it fits
into all everywhere else we use channels
and streams in the JVM but it does
native calls for every operation through
jnr it does a native read it does in
native k q or e poll for the select
logic it does the right things on all
different platforms but you can just
pass in a native file descriptor
one thing that's really fun to do with
this you can take the you can take this
and pass in any of the standard IO
descriptors and have fully interactive
selectable standard i/o which you don't
actually have any way to do on the JVM
today you can't select against standard
in for example when you're waiting for
input trivial to do with this built on
top of this again trying to add more of
these features we expect lots of people
are going to need jnr unix socket uses
jnr en xio provides a eunuch socket
implementation in an ni o channel just
looks like any other niño channel in
the system selectable works like a
socket it's perfect so what else can we
do with this well honestly if we could
have this at the JVM level in the JDK
itself we could do we could have done
niño niño to ourselves in Java
basically we don't need to have the JVM
include a bunch of native code to do
these things anymore that's really
exciting especially for the JDK
engineers that want to add some of these
new features especially like desktop
integration have to wait for the next
JVM version to include C code for it
that sucks we don't want to do that
accessing unmanaged memory a lot of
people go through unsafe to allocate a
chunk of memory and work with it
directly we can do that tribulus call
malloc that's all there is to it call
malloc and you get a pointer object back
that lets you interact with that chunk
of data out in the native world very
easily I mentioned selectable standard
i/o and process io give much more
interactivity to those streams and then
you know other socket types new API is
new crypto libraries all the stuff we
could do without ever writing a line of
c code so i think this needs to be in
the JVM and we'll talk about that a
little bit later performance-wise
getting back to why we don't use je na
sadly we do a lot of work to make this
run as fast as possible the code leading
up to the jni call is generated so
ideally it's just one hop from your call
to the jni endpoint on the other side
the C code we actually generate assembly
on the fly link it into the JVM so that
it's hopefully one call from your native
invocation in the Java side to the
actual library so maybe two or three
hops total this is comparing je na and
jnr
je na is the pluses Jr's the circles
here and this is a logarithmic scale jnr
is roughly an order of magnitude faster
than J and so you can imagine when we're
using je na to do all this stuff before
how much of a performance impact that
actually was you can improve it further
there's ways that you can customize
these calls if for example you're not at
all interested in the air no value and
you don't need it to be saved off you
can throw ignore error annotation on
that and get another twenty five percent
or so out of it if it's a raw call that
you know is unlikely to fail like a get
pinned call there is certainly more to
do here though this is jnr with with and
without the ignore error jni in the
circles unusually there that jnr with
ignore errors faster than the jni
version I'm not sure why that is and
then the GCC 03 potentially optimizing
some of this get paid call away but
there is still non-trivial work being
done here for the benchmark and it's
much faster than what we have at the JVM
level just going through jnr so that's
where project Panama comes in this is
JEP 191 fi for the JVM what we're
working on right now or what the JVM
folks are working on right now is the
JVM side of this how to actually make
these native calls optimize fast so that
when they generate optimized assembly
for the Java code it just goes straight
through to a C call to get pit rather
than going through all of our extra
plumbing that's possible to do when
they're getting close to that now
there's also possibilities of getting
better security for this having fine
grains to control over I want to load
the Lib C but I only want these
functions to be mappable nothing like
that exists in the Java security model
at the moment and then having method
handles for this is a native function
make it optimized like a method handle
and invoke dynamic so I can actually
have my Ruby code do a native call and
it does the native call in the generated
assembly cool stuff and of course native
memory layout manipulation there's lots
of work on different structure shapes
and and value types and other things
that are being done at the JVM level
having more knowledge of what native
code wants to see will make the JVM
build better structures for those the
final area I want to talk about Ruby
equals objects and I mean lots and
lots and lots of objects somebody did a
measure of how many how many strings a
single rails request creates during the
process of handling it and it was
thousands and thousands of strings I
mean the amount of objects created for
just basic operations in Ruby where
everything is an object it can be
overwhelming and it really can swap a
system I I contend that allocation is
really the root of all evil ninety-nine
percent of the performance problems that
I diagnose in Ruby and Jay Ruby or other
projects almost always boil down to
allocation you really need to keep an
eye on that and so you need to monitor
GC logs and whatnot it's more specific
case here jruby 1 dot 0 included a
feature by default called object space
Ruby object space allows you to walk
every object in the heap and you know do
whatever calculation you want on them
you can go look for all I o objects or
look for all strings or whatever in
order for us to implement this on the
JVM we could use the debugging api's but
then we'd have to have debugging api's
turned on all the time so initially we
implemented it by having a separate week
graph a week tree of all of these
different objects with a little wrapper
so that we could walk our structure in
memory and that would see all the
objects that have been created and
ideally if their weekly referenced they
should go away once it's been garbage
collected massive performance hit from
that every object that we create had to
have a second double so that we'd be
able to object spacewalk it and it's for
a feature that's only used largely for
debugging or exploration not used for
typical runtime execution of the system
did kill any application if you use it
at runtime even on the standard
implementation so that was our first big
boost we were like we just can't do this
sorry Ruby guys this is going to be
disabled by default even at the JVM
level this isn't going to be efficient
for us to do this here's our actually
he's the G some of the GC output for
when we had object space enabled and
note that each of these there was 20 28
more of these lines we were pushing
about 1.8 giga second through the memory
pipeline and on my system here the max
bandwidth i'm getting is about 10 gig
and so this is
taking almost twenty percent of the
memory bandwidth of the system for a
simple benchmark in one application
obviously not going to work this is
after we decided to turn off object
space by default very little evacuation
happening here we're not burning through
objects nearly as fast so now there are
things that are hard to fix
unfortunately closures and blocks they
need to hold on to and be able to modify
surrounding methods surrounding
variables and so on so those need to be
in a heap structure somewhere there's no
way for us to read across calls and see
local variables in a method from a
closure numerics in JRuby are still all
objects so every time you do one plus
one will create a to object as a result
we are going to be working on some
specialization above the JVM level but
ideally we'd like the JVM to be able to
see this and do it for us hopefully that
coming that's coming down the line and
then there's a lot of Ruby code that
looks like this we've got these
transient data structures that are used
to do some simple operation very
convenient api's here we're going to
take each of these objects we're going
to call 2's on them to turn them into
strings sort them and then get the first
one out of that now unfortunately this
is going to create a bunch of separate
data structures internally we've got the
map result that's the 2's call we've got
the the new collection that's a that's a
sorted result and then finally we pull
one value out I ideally we'd be able to
optimize through all that and just do
the logic that does this sort the call
and the sorting without the intermediate
objects and this is also very difficult
to do on the JVM right now we can make
it lazy but that still doesn't help a
whole lot because we're creating a lazy
enumerator in the middle ideally there
should be no objects created here other
than what the 2's call does itself can't
do that on the JVM right now so help us
jbm well it does include a feature
called escape analysis which i believe
is enabled by default in the Java 8
builds and the general idea here is that
if it can see an object never leaves a
piece of code and is never visible
across threads it will eliminate the
actual allocation of that object and
just use the data that's inside it and
then recursively if those are objects
and they're never they never leave this
scope eventually you get down to just
primitive register values that optimize
the way you
want them to unfortunately this fails to
work for us in pretty much every case
primarily because the escape analysis in
hot spot right now can't work across any
branch which I mean every one of our
calls has a branch to at least see that
we've still got the right method cached
so every method breaks escape analysis
in JRuby in the very simple form that's
on the JVM hopefully there's
improvements on the way I don't know
what the status of that is so this
brings us to another framework that's
been helping us bridge deal with some of
these allocation issues truffle so we
start out we look at what it takes to
actually build a language on the JVM and
this could be a dynamic language of
static language whatever you want the
same basic steps apply we have our JVM
language that we've parsed and possibly
turned into some bytecode form maybe we
have just have an interpreter to that's
fine but we're going to take the case
where we take a JVM language we compile
it into byte code and then toss it to
the JVM to run the JVM is going to throw
that into a bytecode interpreter for a
while it's going to profile it optimize
it throw it into the JIT and then
eventually we get native code that comes
out the back now the problem is that all
that magic in the middle the cool stuff
we'd have no control over there's no way
for you to give compile hints to the JVM
there's no way for you to tweak it at
runtime you can tweak some of the
settings some of the the metrics it uses
at the command line but no way to change
it at runtime so we have no control out
of this on this section and this is
where it gets very frustrating for us we
try to find a way to form fit Ruby code
for what we know the JVM can do that's
not what that's not what we want so what
if the JVM jit optimizer we're actually
written in Java and we could customize
it and call in two different pieces and
and change the way it optimizes our
language and that's where growl comes in
growl as a project out of Oracle labs a
hundred percent Java gyp essentially you
can think of it as hotspot ported to
Java and then running on top of the JVM
it can either admit assembly or it can
actually output hotspot ir which is the
intermediate form hotspot uses to
generate its code so you'd have Grall do
its work hotspot do its work and then
finally you get native code at the
bottom but you can directly control all
this code generation and you don't have
to even you don't have to use JVM
bytecode which can be sometimes a blunt
tool you can say much more directly I
want to access this memory I want to
call this function so we look at this
with the grawl setup we've got our JVM
language instead we generate a growl
intermediate representation we use
growls bytecode essentially that's a
goes into growls optimizer and again
this is all Java code so we can plug
into this process and add our own
optimizations that our language specific
make things optimized more like Ruby
should optimize and then eventually we
do get our native code out the back but
we get a lot more control over the
process now in force the problem here is
that not everybody is a compiler writer
and this is a very low level very rich a
very complicated I are within brawl as
you can imagine this is bridging from
the Java world to actual native
operations CPU level operations so
there's a lot of understanding required
to actually write crawl I are we need
something a little bit better than this
too so the dream is that we design the
language we've got some amount of steps
in the middle and at some point in the
future everybody loves our language and
we travel around the world to talk about
it what we really want this to be is the
absolute minimum amount of work right
the interpreter and we're done everybody
can write an interpreter probably
everybody in here has written some sort
of interpreter for a general-purpose
language or a specific language at some
point in their life if we could just do
that and then have their language be as
fast as everything that'd be great and
that's where truffle comes in truffle is
a language framework built on top of
Grall it all you have to do is create
your parser so you know what your
language looks like produce an AST and
the AST gets marked up with some
annotations in truffle to say here's an
integer path here's a long path here's
how you here's a data structure that's
only going to be used temporarily and
based on the interpreter and the way
that it runs truffle can actually do all
that work in grow for you and optimize
it as if it were written specifically is
that the vm were written specifically
for your language it's really amazing a
very similar approach
to what they do in pie pie if anyone's
familiar with that you write interpreter
and based on how the interpreter runs it
can generate a jit for you really cool
so all we really need to do here is
write our jvm language and create a
truffle AST that goes into Grall
optimizes down and we've got fast native
code that's based on our languages
actual semantics Chris Seaton is working
with Oracle labs right now on ruby
implementation as part of JRuby using
truffle it's called Ruby truffle before
we kind of call it jruby plus truffle
now because we I I guess I talked to the
right people we managed to convince
Oracle to take this closed source ruby
implementation that was amazingly fast
and give it to us under our licenses I'm
shocked that actually happened but it is
part of JRuby now and it will be
released with JRuby 9000 and how much
better is it this is a graph on the far
left hard to read their JRuby 17 with
invoke dynamic which as I showed is four
or five times as fast as the sea
implementation of Ruby and on the right
the far right that's truffle Ruby
truffle in some cases as much as 20
times faster than JRuby plus invoke
dynamic and this is ninety percent of
this improvement is because they can get
rid of all those extra objects because
it sees through allocations much better
than the JVM does and we can give it the
hints that it needs to wipe those
objects away so I mentioned Ruby truffle
is part of JRuby now and will be
released in JRuby 9000 there are many
other languages in the works and it's
actually really easy to build languages
on top of truffle just the trivial case
an AST an interpreter that just uses
object it'll still actually optimize
pretty well then you can throw in your
specializations as you need them it
might be become part of OpenJDK it's
possibility in the future it's not
likely to happen in java 9 given the
time frame this is the other happened
this is that it's also forcing the
hotspot folks to think about priorities
hopefully the hotspot guys will realize
that you know Escape analysis with in
with the with unclear future for those
objects is something we actually need we
need to be able to do escape analysis
even if we can't see through all the
code alright so wrapping up here so what
we learned I mean
I can't really say what you've learned
hopefully you've gotten something out of
this what I've learned over a decade of
working on JRuby is that there's really
nothing that's impossible on the JVM if
we're willing to route around it a
little bit and I've also learned that
the folks that work on the JVM and work
on the JDK are extremely interested in
avoiding those workarounds and making it
possible for us to just use the JVM as
is and have all this all these features
come along without extra libraries an
extra work so hopefully that's what the
future is going to look like the JVM
does have a problem but we can fix them
and in the interim folks like us so help
you route around them now one thing I
want to the last thing I'll say here is
this is all open source you really can
help with this gras truffle all the jnr
stuff obviously JRuby JRuby plus truffle
the JVM itself with that with an open
source with a little bit of knowledge
nothing is impossible on the platform
and we can make all this stuff reality
for the future thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>