<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Humio: A Fast and Efficient Way to Understand Log Data • Kresten Krab Thorup | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Humio: A Fast and Efficient Way to Understand Log Data • Kresten Krab Thorup - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Humio: A Fast and Efficient Way to Understand Log Data • Kresten Krab Thorup</b></h2><h5 class="post__date">2017-02-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kAhQUAUw-hk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">next up is our last speaker of the
evening Kristin cup from who me oh and
he's going to tell us about actually
what what Paul just talked about as well
hewo which is a new tool for ingesting a
lot of data and doing ad hoc queries
what the blue box okay um well thank you
very much so yes my name is Kristin ah
I'm CCO in inhumi Oh which is a Danish
mostly Danish based startup and I'd like
to talk about not so much at least in
the beginning the product itself but why
we're building it and I'm hoping to
bring this maybe this whole discussion a
little more down to earth to some real
data so hopefully you will hang around
for that do you think we can get this
head this hands-free mic working because
I might be needing both hands at some
point well where is it even
you can work or not okay can you hear me
yes is this fine okay so just a little
bit about me i'm dr. crap Creston I
spent some years that next computer in
Silicon Valley i'm founder of dry fork
which is this solutions consulting
company we about 450 people now and i've
been CTO for that for about 20 years
doing odds and ends and all kinds of
firefighting and consulting internally
and externally I've had quite a lot of
activity with Erlang in recent years I
know here at Clara at least there's
quite a lot of Erlang going on I've been
hacking or Lang quite extensively the
last few years i built an erlang BM and
java among other things so the last year
I've kind of taken a leave of absence
from my fork to do a start-up this human
thing and I'm quite excited about that
so so why are we doing all this really
um the thing is I'd like to I'd like to
bring back this notion that you can feel
the hum of your computer used to be you
know there's an old gear computer in my
computer science department in Aarhus
you could actually walk into this you
would walk into this to sit in front of
the computer to work with it was right
there in front of you and when I studied
25 years ago you know I would have my
next work station right in front of me I
could feel it I could hear it I could
interact with it you could hear the
spinning disks you hear the interaction
between the network card in the sound
card it would go in strange ways and
this this hum this feel
I'm really missing that today I don't it
have you ever been on a big big fareal
ocean boat and all of a sudden the
engine stops right the hum disappears
it's really bad something that's really
uncomfortable you didn't notice that
sound that going on all the time until
all of a sudden oops it's very very very
strange and uncomfortable that the motor
isn't going and you know today you know
we might be sitting in our tranquil
development environment here lerner
somewhere well the computers are
somewhere else and we have no idea it is
a system on fire is it does it operate
as normal is this you're completely
completely disconnected from this way in
so many ways we only realize when the
system you know everything goes down and
you know often that's what happens so
what do other kind of kinds of
engineering do when you have to have an
idea of what's going on when you have
something that's far away you know with
microservices and cloud and everything
today we're moving the computers and the
systems away and we're losing kind of
touch that's kind of the premise wait in
other areas what you do is you know you
built an OP Center like this I've you
say you're sending Apollo mission out to
the moon you have an OP Center and you
have this various displays that give
these people a sense of normal you know
it might not make sense to you and me
and initially it might not even make
that much sense to the people sitting
there but you know if they sit there for
a long time and look at this they get
this sense of normal so if all of a
sudden something is different they'll
notice and that's kind of the same thing
that we do with software right we've
built ops centers put that sports on on
the wall and this is super super
functional to have some kind of des
portes sitting on the wall there's a lot
of lots of software for this but if you
use it I urge you to actually buy a TV
and put it up on the wall because this
simple fact of having
sitting right there means you can just
glance up at it we could look at it any
time and it might not make that much
sense what's on these screens but what
happens is you know we'll take the
machine-learning up here in your brain
and apply it to 22 what do you see and
if something changes if the you know if
there's an odd pattern that the motor
stops or maybe something more subtle
than that you'll recognize that so it's
really inexpensive way to apply a I or
actually it's just I to to this kind of
problem right so 10 years ago so the
team behind Hugh mule actually we all
worked on this same project a try for 10
years ago where we built this healthcare
infrastructure for the Danish drug
administration and so this is basically
all prescriptions and drugs all the
legal drugs in Denmark at least go
through this centralized service and
it's super important to have high
availability there's high security
requirements there's lots of servers and
lots of connected systems where things
can go wrong and in doing this we were
introduced you know to being careful
about our locks and actually monitoring
and and working with this and it's super
valuable it's super valuable to have a
sense of what's normal because assume
you have maybe this today the graph
looks like this you can see
prescriptions being issued there's lunch
break for the doctors now they're not
prescribing so much drugs in the lunch
break you know the next day looks like
this is everything is going normal the
ship's engines humming away and then all
of a second you see a different pattern
um and this kind of was the start of a
much more pervasive change in our
projects to bring on locks because this
was the foundation the foundation of all
this was just locks lock data this is
the you
see that that's the noise that's being
emitted from our IT systems that kind of
it's a signal that we can make some kind
of sense out of it's a log files you
know they're just lying there and lots
of projects are not really using them
much but once you start working with
locks and say what happens if log is a
first class citizen it has a profound
change in your project and I think this
is a kind of thing that you will see
over time this is kind of like when we
were first introduced to test first
programming to continuous integration
yeah you couldn't really see ya you
could really see the value immediately
and it's kind of cumbersome to always
you know I have to set up a CI server
before you start a project ah but now
when we look at a new project we we care
about the locks and and there's lots of
interesting interactions happening
around this because once you but you
care about your locks there will also be
kind of gardening of the locks you'll
start to look at the locks and say maybe
we can make them better how can they
best provide feedback to how our system
works so this goes this changes a lot of
things and having of course the locks in
themselves if they're just sittin down
there in some files and some all these
servers you saw before in the data
center they're not providing much value
so we want to bring them to life and be
able to interact with them and extract
information out of them so there's a lot
of things you can do with locks you can
observe and monitor your system right
just like the dashboard you saw before
you can you could take the value out of
the logs bring it up to monitor see what
goes on and actually the most simple
thing of just put up a dashboard with a
graph that shows how many log entries
are even happening it's super super
basic don't even try to filter anything
to show how many log entries are
aggregate in the system
you know if if that graph suddenly falls
there's something wrong or if it
suddenly rice is there something it
doesn't have to be complicated it
doesn't have to be you know that you
have to extract and describe lots of
patterns or scheme as for what's really
in these locks something as simple as
discounting log lines as an aggregate
number enjoying a graph like that is you
know that's a start it's a starting
point and and it's important to stress
the starting point because what's
happening happening as soon as you start
caring about your locks you'll want to
iterate on it you want to improve how
you extract information out of them and
you want to and this can happen in baby
steps it doesn't have have to happen
overnight so I have a long list of
things we do women use it to understand
and debug your system you know as the
systems become more and more distributed
the locks become the reference of what
happened in the system so to understand
interactions when something goes wrong
it's super valuable to be able to grep
in them or search the search the locks
so of course we used to do this uh when
I was sitting on my way a single next
station I would just tell F a log and
see what happened when when I changed
something I would grab it to find
something but we kind of lost that
ability and his distributed
microservices cloudy world because it's
just you just throw the systems out
there and then you kind of hope they're
running and maybe you have some various
monitoring systems that look at bits and
pieces of it but if you can pull it
together you can actually use tools to
provide value across that you could also
use it to feel comfortable with your
system say in the health care project
you know you'd know that say at some
point we had 10 integrations and we had
another batch another region of Denmark
that we were bringing on their hospital
systems do we have capacity to that to
do that and and how do you know because
it's impossible to know by you know
computing some numbers up front
architecture and scaling of architect
as always you know you have to measure
and try to run it and we run real
systems you want to know you have
capacity say to bring on another couple
of clients to run the system so you can
use the locks to do that you don't have
to build specific infrastructure to do
projections and compute trench you just
look at your log are you a activity
levels so that makes sense of course if
you also put in your locks how much free
disk space do I have how much capacity
do I have how does that compare to other
trends and number of transactions of
what not and it's super valuable when
something goes really really wrong to
keep blocks for a long time so you can
go back and see actually often logs
because they're text right you can go
back and look at something you didn't
know was interesting no we tried other
try some of these logging mechanisms
where you basically extract the data and
put it into a database but then often
what happens is at some point you
realize I wish I index that I wish I
extracted that information out and saved
it and text locks are they're super easy
to compress they don't they don't take
much space space on disk so keep them
around because it makes it easy to try
to run post-processing after the fact
and compared to say what happened last
month or last year you can also easily
say I see the events that happen in the
lock can be used to run tests so if you
have a at a logging system where you can
easily separate separate streams of lock
say this is my test system you could
have system integration tester sighs
when I'm finished running the set of
things I expect that these things are in
the lock and that's an easy way to do
integration across a range of different
server
so it could be different products where
you don't control the log so it could be
SQL servers or cloud services or whatnot
as long as you can extract these texts
locks it's a it's an it's an easy
platform to build these kinds of
validations and built that in to the
test Suites also then there's DevOps and
here I put a bar between Devon ups in
our healthcare project we we actually
had you know we're trying to run DevOps
but at the same time we had this high
security issues because of course we
were dealing with health care data and
people's drugs and and all this you can
you can imagine there's lot there's
strict security requirements but
actually providing providing a lock is
it's a read-only view into the system
actually so what ended up we ended up
having getting permission to actually
read the locks of the system but reading
the logs with an audit with an audit
lock so every time we actually go and
query the locks of this health care
system we have to describe why we do it
and there's an audit lock I describing
who did what at one point so the logging
system ended up being you know a view
into this completely isolated system
orbit we couldn't affect it everything
every deployment every change that's go
through change control and change
management and ticketing systems etc etc
but at least we could get this view into
the into the live system when need be so
we also often ended up using the locks
the prototype new features our new
reports that could be included in the
product because once you have the logs
all this information there if you have a
flexible accessible way to extract
information out of it you don't need to
build a program a complex system to
extract this or put that in production
just do a prototype or ad hoc kinds of
technical intelligence
often there would be some business user
calling and saying how is house are
there any cases where doctors do so and
so in relation to a patient within a
given period could run these simple
transactional loads on the logs to see
how often these things happened so all
these information is right at our other
foot it's sitting right there and you
don't really have to do you don't have
to lock a lot you don't have to
necessarily change your locks a lot but
basically in most cases these locks are
just they're just there they're sitting
on these servers and there are difficult
to access and they're difficult to
iterate on so that's the whole idea I'm
jumu'ah is to bring that up make that
accessible and if we look at the bill
little bit on the design space of this
one important thing when you work with
this is you want you want the experience
to work with these logs with this data
so you say this is data-driven software
development right just like we had we
had all these talks very talked about
data driven or AI driven businesses
right this is applying the same thing at
her you know in our own in her own turf
you want to be able to be interactive
you want to be able to iterate and
explore this data it's kind of I like to
think of it like what Excel is two
numbers you know but the Excel
spreadsheet you can easily sit and play
around and you can sort different
columns and you can can send it to each
other and talk about it and it is right
there flexible kind of permissive you
want that for your data you want it to
be readily available and interactive so
we always hear you know data sciences
bats and scale and large Hadoop clusters
and there's so much data you can't
really do what you have to program it
and then run a batch job and maybe you
get a get the answer the next day or if
you're running locks you know
you want it you want it to be available
so you can you can use it another thing
is so that's this is one thing we want
right the interactive pneus you also
want there's also this kind of tension
between historic or live data you want
something where you can access the data
as it happens I know several of these
logging systems use so much CPU power up
front to process and save the data that
you know you can inquire e the logs of
what happened yesterday or this morning
or something like that so we're actually
trying to bring stream processing and
in-memory processing kind of like what
you do in high frequency trading 22 log
analysis to make this readily available
of course if you have larger historic
data you also need to be able to process
that and then finally I think it's an
important learning that it's super
valuable to have the flexibility of of
defining your schema at the point when
you when you read of course as soon as
you try to do anything structured with
your logs you need to you need to pull
out some information out of it then some
numbers or some some names of things or
services or names of errors etc and epic
at that point it becomes data we extract
the data but it's super super valuable
to actually keep the text around so you
can extract new value out of it as you
go along so of course if you do schema
and write if you do something where you
discard the text and you know you take
the important pieces out that could
potentially be much more efficient
because we discard information you don't
want to you don't want to save but in my
experience it often happens that in this
situation as I said before you say I
wish I index that I wish I saved this
piece of information because kind of in
retrospect you're in a live system you
can't necessarily afford to rerun
some experiment it already happened I
wish I had that information and and
often this can be the presence of
particular strains errors that you
didn't think important a front so
actually having the text the richness of
being able to apply the human
intelligence to interact with this data
it's quite important so that's that's
what we're trying to do and I'd like two
shoes that's what we're trying to do
allow people to iterate explore show
data this kind of liveness and feel of
excel just like what Excel is for
numbers you want to be that for log
files and for for data for text based
data so it's kind of a pity not
everybody is doing this No huh not
everybody is doing this and it's about
time somebody democratizes and make this
available and easily accessible so so
it's so kind of in a nutshell human
tries to be fast flexible affordable
simple to use to install kind of thing
that you could just throw at your lock
files and get started so I'd like to
show I know it's kind of late but um
like to show just a bit what it looks
like so here's a screen that shows me
just doing a free text search for klarna
so the data set that I have behind here
is we have i've set up this demo system
you can go check it out yourself at demo
humor calm is the the live locks from
github all the public repos on github so
we're pulling in that live and so if
it's just free text search for kleiner
over the last week here I could try to
rerun the query
it takes a bit of time you can see it
kind of runs backwards through the good
huh blocks things that happened at
github over this runs through free text
the entire thing that happened to get up
to last week so it's still pretty fast
and what comes out here these Jason
structures that are kind of long long
and and in this case it's something that
describes a push event so that means
somebody you pushed some data to a
github repo so just avoid that the
typing here a next step could be I take
all these clarinet locks and then I pipe
it into an expression it says group by
the type this is the type of lock and a
repository name and sort and we get
something like this the most interesting
thing are the most active thing was push
events happening on clarinet / you I
repository again that's something you
could do in a live system you can see
them the meaning of explore and iterate
is that actually get two responses
relatively quickly and there's a it's as
simple you just thinking of this as this
is like ah like the unix command line
where you say grab into sort into a WK
some something kind of modern version of
that that works across a data set so so
we have some visualizations so you can
instead of just showing a table you
could do a time chart so these are the
clarinet repositories that where there
were commits being pushed to see again
the most active one here being klarna
you I sorry I don't know what that is
but it doesn't matter somebody who looks
at the whose data this is their own data
it makes a lot of things so it's kind of
I also experiences when I show this to
people you know if I show somebody
else's data and something they don't
understand it doesn't really make sense
it's kind of like imagine somebody
demoing especially to
with numbers or accounts or something
that doesn't make sense to you then
maybe maybe Express sees in Excel isn't
that appealing but if if it's your own
numbers that you can sort then all of a
sudden it becomes more interesting so so
there is recently this this huge
discussion on on hacker news right which
is better is it rust or go all right so
I thought I before tonight i'd make a
little dashboard it shows at least the
activity level on github so you see here
this is the activity the last hour of
just kind of all kinds of events this
could be comments or pushes or or
whatever rust was one this this last
hour i can tell you earlier today there
was more activity on golang the overall
activities the last week it was most on
rust the likes so that's that and on
github you can do these just like kind
of facebook likes they're called watches
watch the events people watching it
actually go was getting more and here's
some examples showing up the top
committers for go vs rust and actually
rust has a lot more commits so this
Boris guy I don't know is at 532 commits
this week so we could try to dive into a
few of these the most simple one is this
one so if we look at the query so here
is simply said I want to look for the
repository name is goal and go or the
repository name rustling rust and then I
want a time chart where the serious is
the repository name and span of detail
that's the width of these bars is one
one day so that was that we could make
it one hour you can see it our about our
competition
between these two repositories right so
again this is actually running over the
entire github data set for a week so it
might look like it's a bit slow but
there's actually quite a lot of data
behind it so let's go back and see the
this activity the overall activity the
last hour this is seen total we had
total of 61 events happening the last
hour here so again I say repo name is
golang star or rustling star it's just a
broaden its all the repositories in
those spaces and then I run a reg ex I
extract this what this does it extracts
the first part of the repository name
the owner of repository my and then I
group by that and you can because the
way the data data stories is built and
that's the secret sauce in all this of
course is a it's a super-nice datastore
for doing this both stream processing as
we see here and MapReduce processing
over larger data sets where you can
actually run so we could try to run this
over so basically if you say when a
fixed date range here then I runs a
MapReduce job over that or a time window
then it runs the MapReduce job over the
historic data and it starts an event
processing stream for new incoming
events so we could run this over the
last month Paul github data you can see
the competition coming along as we go
backwards their account so here it's
actually running at this is just a
running on a single on a single machine
you can see under order of 5 gigabytes
per second processing through the log
data which is like 5x the disk speed
so I guess that was it now you have
little idea of what humans there is of
course a lot more to it that maybe there
are some questions we can take care
thank you yeah okay do we have any
questions up here Thanks how question
what's the advantage of whom you
compared to if somebody uses loco stash
and Cabana you know to put like the data
elasticsearch and use Cabana well it we
try to make it really simple to operate
right so it's um one thing is it's just
a single thing that you can you can much
more easily get it up and running so
operationally it's it's a lot simpler
but I think the big thing is that we
move the cost of organizing the data
from from ingest time to when we query
the data so that means you don't need as
much disk space so when you load data
into into elastic generally it bloats a
bit it gets bigger depending on how much
you index if you just throw json at it
and say index everything it could be up
to 10x maybe it also uses a lot of
processing power so it might be that
you're you can easily get in a situation
where it's behind it takes time before
you can actually before the indexes are
up to date so with whom you we we really
squeeze the day they ingest time to a
minimum and these real-time queries we
call them which are really kind of
stream processing queries they actually
attached to the ingest pipeline so
they're just those results are readily
available at any time so we actually
applied back pressure through the ingest
pod line to be able to compute computers
so again storage so so it so does the
cpu load
and the storage load on injustice is a
lot smaller that also means what's also
humne oh we actually compress the data
so we often end up with data that say
five to 10 X smaller and then the
ingested data set and and then again so
so if you compare 5 to 10 x bigger to 5
to 10 x smaller then we're you know
often in a situation where there's you
know 25 to 50 x disk space and then in
an elk setup so if if that's something
that matters it's in our system that
means we can have this multiplier more
of data essentially sitting in the
operating systems that disk cache so all
the men all done if the working set of
water you're querying say a day or week
of data if you compress that 10x and can
fit it in working memory far of the
system then you actually get these
really nice performances of querying it
so yeah I have one more photo so I mean
given that somebody's willing to do dope
from upfront investment for in terms of
the disk space and if you forget about
the performance in terms of clearing the
data like if you can mention the
advantages of homeo compared to see some
turkey bone i would appreciate that as
well in terms of querying the data one
of the goals is to make them make just
make it really easy to do I mean we have
this query language if you try to query
in elastic you have to write these
arcane relatively arcane Jason
expression it gets kind of backwards so
we had focus on the usability of
actually make it easy to do when play
with so that's part of the experience
right and I'm sure you could imagine
writing a front-end to elastic that
would allow you to do some of these
things saying
so in terms of so if we're not talking
performance so them so access to it
actually we have so to view mural
implements the same interest AP is
elastic so if you have an elastic set up
in actually just pointed to humor and
your injectors will think that it's an
elastic cluster it's very easy to try
out or try out in parallel what a I do
you put on top of whom you whom you own
human itself we don't have AI but you
don't have any interface to we basically
know we basically we have some people
doing it but they select and extract
transform data so there's a JSON API to
extract data and we'll pass it to python
systems that do ray aight calm yes i
want to bounce a bit on your question so
so what you have describes that it's a
system that is more efficient that let's
say elasticsearch husqvarna but does it
feel that does it fulfill the same knees
or well elastic is a great general
purpose database that can do all kinds
of things that you do we text search its
scale and you know it's much more of a
database system in it has all kinds of
other requirements and properties of a
large database system this is a specific
niche tool to process text locks right
said it is it is a different I mean we
have an advantage then we have a much
narrower use case well I'm is that from
an outsider perspective is not very
clear if it's if you use cabana plus
elasticsearch if it's fulfilling the
same needs or the different needs I
don't see the difference to be honest as
an outside it might be much more
efficient for large volumes but that's
how you solve the problem my question is
do you solve the same problem well I
think we provide so what we what we see
is that people want to play with this
right with cabana I've so I'm did
several of our customers we come into
them they've set up cabana and uh and
they don't they don't go and change it
they don't go and play with it and
understand it but when it's when it's so
accessible with this simple pipeline
language there everybody can go and play
with it and they can you can easily go
and change these dashboards yourself so
so it's much more of usability probably
experience there now it's the
flexibility of being able to go back and
extract values apply a red x over old
text blocks is this an issue or is
something that that you know makes it
the flexibility of that but just trying
to be really friendly to the user I can
say yes
I'm sorry I interrupted you maybe I just
wanted to comment on your question
because I have some experience in
working with come on an elastic search
and in the mean in the meaning that i
really didn't understand how i should do
it because in these systems that we have
set up they are we have all these you
know the open-source stack of data
analytics right but they were they are
basically too complicated so we tend not
to use them and we tend to analyze the
logs manually instead because you know
how do i set up a date a surgeon in in
kabbalah I just don't get it and there's
no documentation for it and yeah unless
you're maybe okay any more questions as
you also will over here's one more this
is our last question yep so given that
you postpone a lot of the complexity to
the query time how do you make sure that
this actually stays performant if you
have like growing data set of historic
data so so there is if you have to run
over a large data set then there then
there is the does take time so as you
saw on a typical single machine setup
when maybe we can run right now five
gigabytes per second all right so if you
have a terabyte of data it takes a while
right to run a processing job over that
and it's just that that's just how it is
right then if you if you if then set it
up as a stream processing inquiry then
it becomes attached it essentially to
the ingest pipeline and you select and
compute those numbers as part of
computation so typically you would
explore and figure out I'd also like to
compute this number and maybe it takes a
little more time in the beginning maybe
you run it over a shorter time interval
or try it over longer and then set it up
as a live query and then you basically
is just a small incremental over
head on in just cost because it's just
sitting there and it's shared across
multiple users that say put it on the
same dashboard so yes there's a
different there is a different trade-off
definitely and they're definitely
workloads that are too big for this to
be meaningful like if you have a
terabyte data per hour then you can't
really do any any meaningful computation
back in time right then okay thank you
you're welcome to come up here and ask
me questions also yes so I feel kind of
stupid talking into the blue box so I'm
going to switch over here thank you all
very very much for coming I hope that
you enjoyed this evening and that you're
going to join us again in April remember
to answer our survey two for the chance
to win a ticket to go to Copenhagen
thank you I saw you sitting there
smiling</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>