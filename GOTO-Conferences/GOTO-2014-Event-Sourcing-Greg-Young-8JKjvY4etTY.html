<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • Event Sourcing • Greg Young | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • Event Sourcing • Greg Young - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • Event Sourcing • Greg Young</b></h2><h5 class="post__date">2014-11-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8JKjvY4etTY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it really was weird for me when I went
to Copenhagen and they had it in the
bleeding edge track so the first time I
actually talked about event sourcing was
at Q con 2007 Martin was actually at
this talk and I was scared senseless so
I'm coming and talking about new ideas
around domain driven design messaging
and these kinds of ideas and my front
row was Martin Eric Evans and Gregor
hoping I think I probably had about 30
glasses of coffee before I started and I
went through my entire talk in like 17
minutes it was a really awful talk to be
fair I think Martin gave me a red card
Eric even told me is a bad talk which if
you've never met Eric he doesn't really
say bad things but it's it's not new
ideas and even going back as far as like
2006-2007 these ideas go way back way
past that what really brought us towards
looking at event sourcing was that we
needed to have a log and we need to have
a proper audit now for some systems is
very very important for other systems
it's not so important that's what led us
towards the idea of event sourcing and
I've actually used a slide now since
2007 because the slide is so perfect
what's interesting for me when we talk
about event sourcing is that if you look
at mature businesses and when I say
mature I do not mean Groupon every
single one of them event sources
conceptually finance banking gambling
insurance all of them are naturally
event sourced how many of you have a
bank account do you think that your
balance is a column in a table or is
your balance an equation your balance is
a summation of all the previous
transactions that have happened on your
account correct what if it were just a
column so you think that you should have
10000 crown the bank says you have 1200
crown and you call them up in the phone
and say I think my balance is wrong
and they say all hail the column it's
not really useful but if I say that your
balance is a summation of all the
previous transactions have happened on
your account before can we take out a
pen and paper and figure out what your
balance should be you can argue about
whether or not a fact exists but you
cannot argue about the overall result
that's derived from them this is one of
the main reasons businesses are actually
going towards event sourcing having this
first level derivative that is provable
or disprovable is actually a very very
valuable thing we're going to talk about
it more there's a lot of other reasons
why all of these industries have gone
and they're naturally event sourced
before we get into those oh let's go up
with a definition of event sourcing when
you talk about an event store system you
only store facts all state in your
system is a first level derivative off
your facts just like we said bank
balance is an equation that runs off of
your transactions all state can be
worked this way we can say that we will
only ever store events inside of a
system and any state that you want to
have I don't care whether it's a reed
model whether it's a domain object
anything that you want to have is a
first level derivative off of that and
we can do this to any problem I've been
using this example for a long time and
how many of you already know what event
sourcing is okay good number I'll try to
go through the definitions and and
showing people it fairly quickly so we
can get to the more interesting things
now this is my canonical piece of state
so we have a purchase order with n line
items and some shipping information
associated with it this is not the only
way of storing information although this
is what we've been taught to care about
how many of you spend large portions of
your day talking about the shape of data
and if you start storing this you run
into all sorts of interesting problems
how many have written a sequel migration
script before fun right um to be fair I
will never ever do a Big Bang release
ever again where I take down a piece of
software and bring it
the reason why is not because I'm
worried that I will take it down and
when I bring it back up it won't work
that's really easy just do a rollback
right what happens when you take it down
you bring it back up and then it works
fine for a week and then it breaks how
many of you write migration scripts to
take the new schema with all the new
data and bring it back to the old schema
so we've had a joke that when you have
this problem where it runs for a week
and then it dies you have your choice
you can either wear the fireman hat or
the cowboy hat
to be fair I actually recommend a lot of
teams to do this the hats are basically
a token and if someone walks into your
office and you're wearing the fireman
hat they just walk out I'll come on how
many you guys have worked in production
before and then someone came in to ask
you about the Christmas party and here
like get out of my office but I can't
say this now this is our canonical piece
of state but this is not the only way of
dealing with a set of information I can
also deal with it like this where we
have cart created three items added and
then shipping information at it now that
three items added is actually three
distinct events if I put three distinct
events there that boxes just got really
little and you couldn't read them any
more but any given point in time I could
take these five events and I could
replay them and I could give you back
this in other words I can give you back
a piece of state and what we're going to
say in an event source system is that
all state is transient don't get me
wrong it may be persistent but it's
transient
what matters is my events because any
given point in time I can always replay
my events to give you back any piece of
state in the entire system my book of
record is my events if people here in a
functional programming another way we
could say this is that current state is
a left fold of previous behaviors I can
fold over my previous behaviors and give
you back some interpretation of it and
that may be your current interpretation
tomorrow it may be a completely
different interpretation and this is one
of the main values of Venn sourcing is
that this concept of state becomes
transient which changes more in your
system how you view your use cases or
your actual use cases
your events align directly back to use
cases now it took me forever to find
that original slide because there's an
accountant erasing in the middle of
their journal accountants don't do this
unless they worked for Enron in which
case they probably do in an event source
system we have no concept of an update
or a delete you can only ever append
things and this works the same way the
Countians work
how many have taken an accounting class
before maybe in university they probably
told you that accountants don't work in
pencil they work in pen once you've put
something into your ledger you never
ever take it out and we do the same
thing in the vent sourcing now I'm just
curious how many of you have an update
or a delete statement in your system
today okay now keep your hands up
how many of you at a c-level board
meeting to decide that that information
was meaningless and it was perfectly
fine to get rid of how did you determine
that that information had no value how
do you even do a cost-benefit analysis
over information so I've got some data
today that's happening inside of my
system in order for me to a cost-benefit
analysis I need to know how my business
wants to look at this information in
three years how many you can predict
where your business is going to be in
three years and what questions will ask
you so how do you make this decision to
update or delete data do you take out
your magic 8-ball shake it up and try to
figure out what the business is going to
ask you how much does it cost to store
one gigabyte of data for a year today
you have to be fair it's actually zero
you just open a gmail account and store
in your gmail account but it costs
almost nothing how many facts can you
fit inside of a gigabyte worth of data
let's imagine they're all 1k each that's
a lot of facts you can store you don't
need to get a lot of benefit out of your
data down the road and I'm gonna talk
about this a bit more later but I get
this question a lot of if I only ever
just stored
everything then my date is going to get
really really big to be fair most
systems I can fit your data on a micro
SD you're not big data even if you're
appending you're talking about oh so
over the next five years we're going to
end up with a hundred gigabytes worth of
data okay how much does it cost the
store hundred gigabytes we don't have an
update or a delete operation when we
talk about events or systems there's
only append and just like what
accountants do we always append a new
event so if I were to go back and I were
to look at accounting let's say that I
made a mistake and I transferred you ten
thousand crown as opposed to one
thousand crown because I fat-fingered it
Y is going rates to zero in the ledger
right no so I'm gonna do one of two
things I'm either going to do what
accounts call a partial reversal in
which case I'm gonna put a journal entry
that says I'm I take nine thousand back
from you and put it into the other
account accountants don't like doing
this okay if it's nice round numbers ten
thousand and nine thousand you can
figure out what I originally intended to
do is one thousand what if there's six
accounts involved and they're not
perfectly round numbers well then you
have to take out a pen and paper and try
to figure out what my original intention
was so instead what accountants will do
is they'll do what's known as a full
reversal I fat-finger I give you ten
thousand I take ten thousand back from
you I mark that the first thing was in
mistake and then I do what I actually
intended why because an auditor can come
back through here and he can figure out
exactly what I intended to do it's the
same thing we talked about events or
systems we tend to prefer full reversals
as opposed to partials but let's say I
want to remove an item I could say that
we now have cart created three items
added one item removed and shipping
information at it is this the same as
cart created two items added shipping
information at it I always love this
question because about half the room is
going yes and half turns going no and
you haven't become a good consultant if
you haven't learned the words it depends
there's actually a great page up on the
c-4 wiki about it depends now if I were
to look at it from this perspective yes
those two are the exact same correct I'm
going to get out an order with two line
items and shipping information
associated to it from both of those
streams what if I had a different
perspective though what if I were
counting how many times items were
removed would they be the same no and
it's actually a fun trick if you can go
into your system today and you can come
up with any set of use cases that leaves
you at the same ending state as another
set of use cases guess what you're
losing information you just show that
you do not have a perfect hash how did
you value that information who did you
talk to about it this is one of the main
benefits of event sourcing and why
businesses actually go towards the
concept of event sourcing regardless of
whether we talk about it in code or just
conceptually event sourcing does not
lose information it is the only model
that you can possibly use that does not
lose information and you can have lots
of different implementations of it a lot
of accounting systems for instance they
are a table with a descriptor column
that says type of row and then they join
off to another table to get the rest of
the information for that row that is at
its heart event source they are storing
facts and deriving state off of their
facts an event source system is the only
possible way you cannot lose information
when you talk about storing data any
other form loses something now to see
the real value of this what we have to
do is we have to go through and try an
example so let's imagine that we're
using this model for I don't know
something like Amazon and our business
user comes to us and he says you know I
think people that remove their items
from their carts within five minutes
before they check out are more likely to
buy that item in the future than the
other things we randomly show them well
why do you remove an item from your cart
five minutes before you check out so you
go and look at the cart it's going to be
like 800 crown you're like well I could
buy all that stuff but my wife will kill
me or I can remove
three items from the card I can get some
of the items from the cart and my wife
will not kill me so I will take some of
those it's not that I don't want the
other things it's that I'm D
prioritizing them compared to the rest
so with this model we're now going to
take this card off the wall we're going
to try to implement it so I guess our
first thing that we'll do is we'll add a
new thing off the top called removed
line items and then we're going to is
we're in write a report that looks for
removed line items and does a sub-query
back to see if the person has bought
this in the future so we put it out to
production our business user says I want
to run this report and they run it what
do they see that report starts from here
and moves forward
let's try in this model now when we talk
about deriving state off of an event
stream it's known as a projection what
we do is we project a set of state off
of an event stream if you're in the
functional world again this is a left
fold it's a whole lot easier in the in
the functional world they don't have all
these fancy pattern names so what I'm
going to do is as I fold over the event
stream I'm going to look for an item
removed and when I find an item removed
I'm going to put it into my state as
unfolding over and I'm going to say I
found this item removed and now I am
looking for the checkout when I get the
checkout I compare the time that the
item was removed to the time of the
checkout if it was within five minutes I
mark that I am now looking for this item
in the future found equals false and
then as I go through all the rest of the
events over time if I find the person
actually bought this item I mark found
equals true easy enough now what I
haven't told you about projections and
this is a key idea on events or systems
is that they must start at event number
zero the very first event that your
system put out is where a projection
starts and you run it all the way
forward until it comes to right now and
then it continues into the future and
this may take a while it may take a
weekend you can imagine you've got 500
million events they don't go that fast
really we can currently get
up to about 70 hundred thousand per
second coming off the persistence but
even at that time it's going to take you
a while to replay so we can imagine we
start it off because it's going to go
into sequel server start off on Friday
come in on Monday we now have our read
bottle there our projection is completed
so now we write a report against our
read model or as martin call in eager
read derivation beautiful term by the
way that does my users see when he runs
his report well he sees all the data we
never knew that he would have this
feature but we didn't lose anything so
we're able to actually give it to him
not only that i can also go to my user
and i can say you know this new report
that you came up with this is what that
report would have told you on august
14th 2011 at 1:30 for in the afternoon
if you have this report at that point in
time you can take any report and you can
show it at any point in the history of
the system and you can do this you can
do it deterministically if anyone
happens to work with machine learning
there's actually some very well known
algorithms for dealing with machine
learning over event streams in
particular it's called alpha beta
training so what you're going to have is
you can imagine alpha is at the very
beginning of the list beta is ten events
ahead and alphas job is to predict where
beta is but this is one of the major
benefits of event sourcing you can go
back and you can take new ideas and
apply them to any point in the history
of the system you cannot change what
happened in the past but you can have a
new perception of what happened in the
past let's just bring it back to a
real-world example I don't recommend
anyone does this but imagine if you
could take your current mind and bring
it back to when you're 13 years old at
your first dance can't change anything
but you can perceive it as you currently
are don't recommend it in general most
people are looking at event sourcing due
to the these kinds of reasons these
business read
however there's a whole lot of program
or pornography associated with this how
many you bought a hard drive before so
there's two speeds on a hard drive right
there's one for writing sequentially
there's another one for writing random
okay to be fair on SSDs it doesn't
really matter that much but if I have an
event log if I'm actually storing this
do I write random or sequentially
everything's sequential and you can
become very very very fast with a fairly
naive set up on a single node we can
linearize you to probably fifty to a
hundred thousand transactions per second
how many of you do more than ten so
we're a couple orders of magnitude above
where you really need to be and you'd be
amazed how simple things get when you
can actually linearize everything
there's some other cool things you can
do for me the biggest one is this
ability to go back in the past and does
anyone know what this is
this remote they used to be really
popular in America they allowed you to
go back and watch television
TiVo exactly actually be fair the
biggest thing they did was they let you
skip commercials and then they took that
functionality away they had the one
button they would actually find the end
of the commercial and you could just hit
one button it would bring it right back
to the show
but you can basically go forwards and
backwards through time inside of an
event source system and time becomes
explicit which is another big thing but
are some other really cool stuff that
you guys can do how many right smoke
tests so what I like to do is I store
every command that my system has ever
processed you told me to do something I
wrote an event saying that I did
something and I replay them every week
and I compare what I did this week when
I reran every command my system has ever
done to what it produced last week are
these things that I expected to have
change or are they things I didn't
expect to have change as I change
behavior over time this is a really good
smoke test now this will not save you
from all production bugs there are still
black swan' sorry but you should be
reasonably comfortable pushing something
to production
if you've already taken that software
and reran every single transaction your
system has ever done before through the
new software and not have errors or
maybe you're bad coders I don't know you
hide it your bugs very well but there's
some other cool things we can do how
many have heard of a superuser tech so
the definition of a superuser attack is
a rogue system administrator or
developer attacking your system they
have root access and they want to hack
you they probably have access to source
they can do anything they want right
years ago actually had a deal with one
of these and we're building gambling
systems and we were actually event
sourced and we had to go one step
further in our security so that we could
prevent it an event sewer system can
prevent a super user attack if you want
to go look this up the guys name is
Chris Horne and there's a Wikipedia page
and it's actually a one-hour HBO special
about this guy it's criminal masterminds
which I never quite understood because
all their criminal masterminds got
caught so we had a pool it was called
pick six and basically a pick six is you
have to pick the winners of six races in
a row
obviously that's hard and you get paid a
lot of money for doing it this was way
back in the csu/dsu days and what we
were doing instead of moving all the
bets over the track because when you bet
this kind of pool you normally bet like
100 bets at a time what we were doing
was we would actually store them at your
local track until the end of the fourth
race and then we would only ship over
the ones that could still win back to
the hosts track well that gets rid of
99% to your network traffic which if
you're going over serial ports is fairly
reasonable so what he was doing he would
go and put a bet one two three four all
all at a remote track he would watch the
first four races and then he would go
edit his bet on disk to change one two
three four with the actual winners of
the first four races good scam right and
he got caught not because he was stupid
but because he's unlucky
how many have heard of Breeders Cup it's
the second or third largest race of
America and he was doing this scam
during Breeders Cup and he went through
into everything what normal put a bet
over the IVR got on the machine changed
the bet one two three four to the first
four winners and then 299 to one horses
came in the fifth and six legs he was
the only winning ticket in the world it
was like a three million dollar ticket
now normally there's going to be let's
say 30 40 winners to this kind of pool
and you know you're just going to get
paid here's your hundred thousand dollar
payout no big deal what do you think
happens when there's one winning ticket
in the world and the mutual manager goes
and looks at it and it's a bet like five
three seven six all all no punter would
ever bet this because if you if any of
the first four lost your entire all your
tickets loose so he sees this couple
million dollar ticket he says let's get
on the phone Catskills figure out what's
going on over there
anything interesting oh there's a
developer on the maintenance line that's
interesting oh the audit tape is ejected
yeah that's interesting but this is a
fairly typical superuser attack and and
to be fair most superuser attacks they
will only ever get caught if they're
unlucky but we can prevent this with an
event store system and we did how many
of heard think of a worm drive right
once read many once you've written to
the disk think about an old cd-rom you
can only physically write to the thing
once you can't write do it multiple
times now if my current state is a first
level derivative off of my audit log you
need to put something in my audit log to
change my state now if my audit log is
on a worm drive how do you attack me now
to be fair there is a way if I'm writing
it very slowly you could make an entire
copy of the worm drive and you can
switch them we can reasonably control
that I could write a message to the worm
drive let's say every hundred
milliseconds how many you can change a
worm drive for another worm drive within
100 milliseconds
always detect that somebody's probably
changed a drive at this point it's
useful for this kind of situation now
I'm not going to say that you should
only ever use events forcing if you
happen to deal with super user attacks
um it's just one of those things if keep
it in your toolbox if it happens to
become useful will be very useful
now conceptually we always start at
event 1 and we go to the end of the
stream and this works really well for
most things when we talk about events or
systems most events or systems are not
having one giant log they have millions
of tiny logs think about a mortgage
application in a bank so it comes in
there's some approval process that it
goes through and basically you get
events that are written how many events
happen to a mortgage application in a
bank for single mortgage application for
a single one that'd be weird
they're very busy normally we might be
saying 50/50 things will happen to this
throughout its lifecycle um we can think
about it as being a document in a
document database so we have all the
events for a given document now normally
this is not going to be a problem to go
from one to the end of the stream
replaying it'll be very very fast um how
much more expensive is it to read 20
rows from a database as opposed to
reading one in terms of time to actually
load it however there's some things this
doesn't work so well with what if I were
to take the order book of Google in the
stock market at about two o'clock in the
afternoon how many things have happened
to Google throughout the day
okay now you're millions and if I want
to do that playing from one all the way
to the end of the stream even though I
do that conceptually would be very very
slow so there's a related pattern where
we can start dropping snapshots at given
points in time inside the stream so here
for instance as opposed to reading from
one to the end I'm actually read
backwards and we start with six and then
Emissary you snapshot no five are you a
snapshot no snapshot are you snapshot
yes so now I take my state at that point
in time and then I apply five and six
afterwards in general never ever store
your snapshots like this um what happens
when I store them like this and we're
currently at version four and I say okay
I'm gonna write a new snapshot down then
he writes version five concurrency
problem is my snapshot any less valid at
version four because he put version five
in the event stream so you take your
snapshots add a version and point back
to the event stream you basically store
them off the side along with this you
can have many many different types of
snapshots pointing back to the same
event stream I can have 25 different
folds that all go over the same event
stream with their own perceptions of it
and I can snapshot all of them
independently in general avoid snapshots
if you can and you can't always avoid
snapshots if you have something like 5
million events for Google sort of book
you need to have a snapshot at some
point but when you introduce a snapshot
you're going all the way back and you're
introducing this again and you will have
all the same versioning problems that
you end up with a sequel database or any
structure that you actually store when I
store vents I have a lot less versioning
problems I can bring it to servers side
by side that each have their own
perception of the data and I don't need
to talk about database refactoring
patterns although I really love that
book in the series when we talk about
state state is very hard to version now
what's really interesting for me about
event sourcing is there are certain
businesses that are out there they're
already naturally event sourced and if
you introduce event sourcing to these
people it will make total sense to them
how many have talked to a lawyer before
so when I have a contract that's been
written and then you want to make a
change to it we just go edit the
contract right or do we put it Bendel's
to the contract and over let's say five
years we have 37 addendums to our
contract and the only way to figure out
what our contract is is we take the
original contract we apply all the
addendums to them one after another in
order that they happened sound familiar
how many been to a doctor before
hopefully at least once now when you go
to the doctor does he take a picture of
you and put it in your file and throw
away the other picture of you or does he
constantly append papers into your file
and we wonder why they have a hard time
understanding crud systems when their
conceptual model is I have a file and I
append things into the file all the time
we run into this a lot there are a lot
of domains that are naturally
eventsource and events forcing is not a
new thing where I talk about this more
but I've managed to track it all the way
back to Mesopotamia and Sumeria so when
people ask like well how do i how do I
deal with this new thing it's not new
how many use a sequel database today
guess what you're already using event
sourcing there's this thing called
transaction log in your sequel database
but your run into a problem if I only
store my events how do I write a query
I'm looking for all customers with the
first name Greg so I know we'll replay
every event my system has ever done
before we'll fold over every single
stream and we'll put a filter at the end
of the fold to say whether or not it's
currently Greg this will be fast right
Big O of n is awesome when we have 500
million events and most times when
you're talking about events or systems
you you don't query your events you end
up with a concept known as read models
and this leads you back to CQRS we're
gonna talk more about secrecy caresse is
mainly teaching pattern but I write
events and then I have n read models
that I can actually read from and this
will actually save you quite a bit how
many have heard of these before you know
you're not cool if you're not using
these except for one of them one of them
is really not that cool I'll leave it up
to you guys to figure out which one but
when I look at this I am reminded of
about a decade ago especially in
Scandinavia how many remember these
object databases grant take over the
world they were ten times faster on OLTP
operations than sequel databases they
had no impedance mismatch between your
domain model and your data storage they
were awesome or as my wife sometimes
likes to say a vase um how many of you
use an object database today one I
thought they're going to take over the
world does that remind you of these at
all currently we're saying these things
are going to take over the world because
they're they're the new old thing no one
had ever come up with a key value store
before document database no one ever
heard of oh wait Lotus Notes but why
aren't people using these today
how many have used one of these so I've
worked on quite a few projects that used
one of these it always ended up at the
exact same failure so we would build out
our object database if everything was
beautiful with our domain model and we
could go back and forth no impedance
mismatch we are really really happy and
then one of these incompetent business
people would come over and they would
ask us for a report and they would want
us to do something like roll up sales
based bond customer and town and we go
well but that means I need to load up
all those objects in memory I can't do
that
and then we remembered we had these
things called sequel databases that
we're actually really good at this and
we decide the object database is totally
suck because they don't do that very
well and therefore we should always do
sequel and we run into this a lot where
we come through and we want one tool
that fits all of our problems to be fair
you will never have one tool that fits
all of your problems no matter how hard
you try and every single database on the
planet sucks when we talk about
databases there their writing isn't very
interesting most of them they work the
same way as what we were talking about
with the Venn sourcing
append to a log years ago I was giving a
talk actually that event store which is
Davis I work on and in my talk someone
actually said you know you guys work the
same way internally as Cassandra you're
the same as Cassandra it's like yes we
use a log structured merge so to see
qualité so obviously sequel light also
as Cassandra when you talk about a log
structure merge what I do is I append to
a log and then I've got chasers on the
log that are updating in-memory
structures and then I batch update them
down to disk this is how many many
databases work think about sequel server
your transaction file same general idea
when we talk about databases what's
interesting with them is not about their
writing it's about their reading and we
run into all sorts of accidental
complexity if we have the wrong models
so you guys might have heard this small
startup before what Twitter is in its
core is a topic based pub/sub correct we
have topics names hashtags and people
subscribe to topics so if you ask me to
build Twitter the first thing I'm going
to do is I'm going to ignore everything
that's been done in the financial
industry for the last three decades and
I'm going to build it with Ruby on Rails
and my sequel because that's obviously
how you should build a topic based
pub/sub correct how many remember the
fail whale luckily they had enough
capital that they were able to get out
of this problem but they introduced
massive massive XML complexity the point
they were managing hundreds and hundreds
of my sequel instances to build a topic
based pub/sub really bad idea I would
not recommend going that way however if
you do happen to make the next Twitter
let me know I'll invest early there's
other examples this how many of you have
created something like this before so we
have id parent ID and some data and what
we're building here is essentially a
tree and you go and you release it to
production and they say you know it's
taking nine minutes to bring
up that report and you go but it works
on my machine so what's the problem well
on your machine you've got about a
hundred rows in that table in production
they've got eight hundred thousand and
you have built a recursive query so you
were smart and I really want to know how
many of you are willing to admit this so
you come up and say I'm going to do this
so now we have parent ID zero parent ID
one parent ID two parent ID three and
now you can do an or query and get back
all the rows that are underneath a given
thing just by using your ORS on the
parent IDs come on be fair how many of
you actually done this I'll admit it
I've done it and this is a piece of
massive accidental complexity because
we're trying to deal with this
particular kind of read model we're
forcing ourselves into it an alternative
to this might be this thing called a
graph database graph databases are
really really good at dealing with this
and I want to be clear I'm not saying
that you guys should go have a database
per problem keep that as an opportunity
and then you can talk about the
operational complexity that you
introduce by doing it at the end of the
day if I go put a graph database in
production somebody needs to maintain it
operations needs to know how to back it
up they need to know how to deal with it
but these are all examples of accidental
complexity I actually watch this exact
problem at a company in America and what
they had was they had people and then
they had a table called people
relationships that linked to different
people together based on certain
relationship type and what they want to
be able to query was so from this person
to this person using only these
relationship types can I find a path
they had a half-million-dollar sequel
server doing this we replaced it over
the course of two days with neo4j and
the laptop was faster than their
production server this is normal when
you choose the right model keep in mind
the wrong models will introduce massive
accidental complexity to your system
when we talk about events or systems we
have as many read models as we want
inside of the system all they have to do
is subscribe to the events that they're
just a projection
what will normally end up with is we'll
store all of our events and then we'll
end up with n read models that are off
on the side now I know we're in
Scandinavia so I have actually done you
guys the favor of giving you your
questions and these are things I've
heard over time
like for instance events or systems need
a service bus know almost nothing needs
a service bus if you bring a service bus
into the problem you're probably making
a bigger problem in most circumstances
and to be fair and in most cases like
especially if you bring in something
like this talk you not have two problems
you have the problem you have BizTalk
and you have the best talk consultants
and they're gonna take forever to get
rid of well I see people try to apply
eventsource
systems with service buses all the time
and they almost always fail and the
reason they fail is because there's two
different kinds of subscription models
there's what's known as a consumer
driven subscription think a blog and
there's what's known as a server driven
subscription which think AMQP and
normally when we talk about service bus
they're going to be using something like
AMQP rabbit and queue underneath or ms
MQ or some websphere queuing system now
the problem that we run into is now
we're publishing our messages off over
the bus but what happens when we have a
new model and our new model now wants to
get our historical events through the
service bus so now we need a control
channel it needs to be able to talk to
the thing on the other side to say hey
by the way I want on my history and we
got another problem what do you do with
the events that are currently happening
while you're replaying that history
through the service bus the next thing
you know you're in this massive pile of
accidental complexity there's another
way of doing this and it's a
consumer-driven subscription which is
what most people are using when they
talk about event sourcing how many have
heard of something called Kafka
Kafka is a consumer-driven subscription
system and works just like my blog how
many you have a blog today how many read
blogs okay so let M queue now to
distribute my blog so what's going to
happen because we're going to work just
like this when you want to read my blog
you send me an email saying hi I'd like
to read your blog can you create me a
queue so I go through and create your
queue for you and now your email client
for the blog will actually get the posts
from your your rabbitmq queue cool but
then you decide you actually want to go
read my older posts so you send me
another email telling me to put all of
the older posts into your queue for you
this is what I mean when we say control
channel you need to have this other
mechanism of talking back and forth of
me in order to make this work a
consumer-driven subscription works
exactly like blogs when you come to my
blog I give you an atom feed the atom
feed says hey this is where you
currently are do you want to go to the
oldest event this is where the oldest
event is and then you can come forward
in the stream all the way up so you get
to current maybe you want to work like
Twitter so when you first come you only
want to grab the last 50 that's
perfectly fine as well the consumer
remembers where they are in the stream
not the producer now what happens when
I've got 200 things subscribing over a
consumer-based subscription well it's no
problem what happens if you have a
consumer-based subscription you want to
replay you want to pull down all of my
old blog posts do i as a server care do
I have to do anything this is the
difference if we look at most people are
using service buses they end up in this
trap where they end up building control
channels around everything and then
sourcing is more complex more complex
than what
is it more complex to build an event
source system than to build a typical
crud app yes but not because of event
sourcing it's because you have to go
through and you have to figure out these
things they're called use cases and you
have to talk to people about how they
want to use the software not just what
data they want to store and manage and
it's a different way of doing analysis
if you talk about systems we are
actually going with a use case driven
approach and comparing them event source
versus non event sourced they're roughly
equivalent in terms of effort to be fair
if you come into an event source system
for the first time you will have a
learning curve you guys have been
working with crud for how long you
already know a lot of will edge cases
that you run into with crud you know how
to handle them you know what the risks
are how to deal with issues in
production and there's a learning curve
associated to get the same information
out this is one of my favorite questions
what big companies use event sourcing
well basically all of your database
companies do you're probably already
using it you just don't know it and this
is a really really bad way of making
decisions I actually died a little bit
inside every time I read this on an
email list because obviously if this big
company is doing this then that makes
total sense for us to do it because
we're a big company in they're a big
company people talk about events or
systems being slow and you can imagine
if I've got five million events and I
have to replay five million events
that's gonna be really really slow but
what if I were to store them in an actor
and I would distribute my actors across
a cluster and I would keep them all in
memory why don't I have to replay at
this point when I lose one what if I
were to drop a snapshot every one
million events so now we're capped the
most live referee play is 1 million and
that's only in a failure circumstance
otherwise all my data is currently in
memory does this sound like it'll be
slow you can make these kinds of systems
very very fast in fact if you start
going you start looking at systems and
need to be fast and
really care about things like latency
oddly they tend to use these ideas
things like trading one of the biggest
ones that kills me is that people have
now associated event sourcing with
object orientation and in particular
domain driven design to be fairer and
events first model is not object
oriented it is functional um when does
an event change so we have entire and
mutable data and or to get current state
we left fold over whatever things that
we have in the past this is obviously
object oriented yes but people with
associated events or systems back to
object oriented and domain sure and
design based systems and it's totally
not it is it is inherently functional
it's a functional system of dealing with
data storage and I've been asked this
question a lot and the best framework
that you're going to have is probably
none when you start going back and you
start saying I want to be able to replay
these events to come forward okay it's a
left fold do you really need a framework
to implement the left fold for you
although to be fair there are some guys
here in Aarhus that are building a CQRS
framework so when there's breaks they
live locally and you can annoy them so
that may actually be a good one to look
at and over time I've had a lot of
people talking about this data
requirement and we're going to store
everything we're going to pend
everything in our system how many have
ever looked at what your current state
is in your sequel database versus the
size of your transaction look
transaction logs a lot bigger normally
but what you really need to ask yourself
is not about how much data we're going
to have what you need to ask yourself is
at what current rate and what future
rate will be we be retaining data and
you need to take this you need to bring
it back and compare it to Moore's law if
I want to go five years ago and buy a
one terabyte SSD how
much would that have cost today it will
cost you about 500 US dollars I'm
willing to bet you within three years
we'll have two terabyte SSDs so you need
to figure out not only what is my
retention but how does that curve fit
when I plot it next to Moore's Law and
if you are slower than Moore's law then
guess what your data is going to get
cheaper and cheaper and cheaper to store
over time and there are some systems I
can tell you today you need to be
sitting at about three to five thousand
messages per second before you actually
have to start worrying about things once
you hit that level you do need to worry
about things way into the future if
you're underneath that cheaper and
cheaper and cheaper and there are some
systems but literally how many of you
guys have more than ten transactions per
second again we're like two or three
hands most systems are doing very small
amounts of information and we are
getting into premature optimization when
we start talking about all these kinds
of issues and to be fair if I can put
all of your data on a micro SD it's not
big data now I've been talking with
people about CQRS for a long time
and CQRS was never really intended to be
much of a pattern I actually agree with
the functional people when they say that
we don't have these things called
patterns we just write code and I don't
need a pattern for every single thing
that we're going to do CQRS the whole
thing that was behind CQRS was really to
make people start looking at their
domains in a different way to lead them
to everything else that we're talking
about it also leads them to things like
building out actors for their domain
model which doesn't work very well if
you're supporting all of your reads off
your domain model with a or m behind it
once you start saying that we're going
to have our read models over here and we
query them separately than what we
actually write - everything turns and it
works out really well but the main thing
I had to do in bringing people towards
this these ideas was to give them
something to separate the reading in the
writing part of the process
without that they would never make the
jump from object-oriented I use air
quotes because they're not really
object-oriented they're like C++ object
orientation domain model to going all
the way to something which is
eventsource with read models on the side
and secure RS has been a big part of
that but it's not something that you
really need to go off and get really
deep into if you fares PI the dumbest
pattern ever written if you have a void
return type you go here if you have a
non void return type you go there it's a
whole pattern which also makes for a lot
of fun when you hear people talking
about how CQRS is crack for architects
and you're like ah by a dumbest pattern
ever written and that was already there
and I've gotten this one a lot from to
be fair the only thing I can really say
is thanks it's not Enterprise e at the
same time it will not cost you you know
a half million dollars just to get in
something really simple um you don't
need a lot of the big frameworks people
are using or and if you really want
Enterprise you stuff well iBM has a lot
of stuff to sell you and with that I'd
like to thank all of you guys for
actually coming out I do appreciate that
everyone actually came to the talk and
with that if anyone has any questions
hey don't you have a problem when you
need to decide which details to star in
the event because you don't know which
details you're going to need a five
years from now when you're going to
build a new model okay that's fair
so when we talk about the fact oh sorry
um it was how do we determine which
details to actually put inside of an
event and it is the exact same analysis
that you do for use case analysis when
we're talking about what is the fact
that actually occurred inside of our
system and what do we care about and
this is absolutely an analysis problem
and it takes some time of working with
it to get it right a normal follow-up
question to this is what happens
what's in the the fact changes over time
and how do I version it over time for
I'm sure someone will actually ask that
question I always get it there's a
45-minute answer to that question um
sorry it's on DDD CQRS calm and it's in
the middle of a six and a half hour
video it's about two hours in uh-huh I
would highly recommend watching the
entire video but that's unbiased when we
talk about facts over time and how they
change generally what we start talking
about is using a hybrid or weak
serialization in order to get that
working as well but you can do it with
strong serialization then I literally
it's a 45-minute answer when you
introduce event sourcing for already
existing applications with an already
established changing domain model what
would be the best way to kind of
avoiding the penguinese that's actually
a great question and when we talk about
migrating existing data over to an event
source system there are two ways of
actually dealing with it the first way
that we can deal with it and that's
actually the same way I don't know if
anyone hears our worked as an accounting
system so it's the exact same things
that you would do with an accounting
system so if at the end of the year I
want to switch from Great Planes to set
for my accounting I've got two options
I can either migrate all of my data out
of Great Plains and bring over all the
transactions on all the accounts into
the new system or I can bring over just
the initial balance of all of my
original data and and there's times you
will use both and for any problem you're
going to use both of these solutions on
it I will go and let's just use DDD
terminology quickly aggregate by
aggregate in some aggregates I may I may
bring over as a snapshot a customer
initialized of it our customer
initialized from old system other ones I
may be able to reverse-engineer the
history and in those cases I may
actually bring over the history a
the events as well and what this can
allow me to do if I if I do it
reasonably well I can actually start
running side by side so I make the old
system at least raising the events along
the way and it may still be stirring off
to a third normal form as well but now
the new system is also raising events
and what we can do is we can project
those events back into the let's say
third normal form model so you can
basically run side by side with these
kinds of systems as you're moving in
terms of migration there are the two
strategies and you can and you will use
both of them concurrently um it's really
a thing that you come into and it's
aggregate by aggregate or document by
document depending how you want to look
at it and you you will choose both
strategies oh sorry
okay well then I will thank you guys for
having come out and if you have
questions I know particularly in
Scandinavia people like to ask questions
one on one um so I'll be around probably
most of the rest of day today and at
least in the morning tomorrow Thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>