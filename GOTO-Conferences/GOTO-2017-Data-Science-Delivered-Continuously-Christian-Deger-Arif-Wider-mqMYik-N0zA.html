<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Data Science, Delivered Continuously • Christian Deger &amp; Arif Wider | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Data Science, Delivered Continuously • Christian Deger &amp; Arif Wider - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Data Science, Delivered Continuously • Christian Deger &amp; Arif Wider</b></h2><h5 class="post__date">2018-03-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mqMYik-N0zA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right welcome everybody thanks for
joining this session so we're talking
about data science being delivered
continuously today and this Christian
Christian is chief architect at
autoscout24 but he actually joined
autoscout as a mere developer and then
made his way to his current role and we
as taught works
we've worked quite a lot with Christian
over the time at Scout and I think I can
say that we've enjoyed each other's
company quite a bit so and I have with
me Arif and I will agree on the
complement that was a pleasure working
with sort workcenters Arif on this
project we both were there from the
beginning assumed so
Arif is senior consultant at sort works
Scala is his favorite language of choice
especially in the context of big data
applications and before joining sort
works he has been in academia doing
research on applying functional
programming techniques to data
synchronization and I assume you know
autoscout how's the code 24 is a
used-car listing platform it's the
biggest to use car marketplace
europe-wide we are present in in 18
countries and everything has to do with
use listings that are offered by dealers
and private sellers and for sale and you
can contact those who are trying to sell
a car we have about 2.4 million listings
on our platform at every given time and
this is the data we will be talking
about what we can do with this data and
we have a lot of users on the platform
so typically all our engineering
problems involve some form of
scalability issues movie or which we
will also see later on yes so christian
outlined autoscout has a lot of data and
specifically our scott has a lot of data
about how cars
and at what prices and now our task when
we started this particular project was
to turn all this data into something
actually useful for the end-user so our
task was to create a consumer facing
data product where users can quickly
estimate the current value of their used
car and its product that we built works
as follows so here you see the German
input page of that product and you see
you put in some basic information about
your car like make the year of which the
first registration the model fuel type
few other things and then importantly
the mileage because it's about used cars
right and then you go to the next page
and you can optionally indicate extra
equipment or the condition of the car
and then you get a price range so that
is what we believe the car is still
worse so here we say that this
particular Forks were angles you can
still get something between 11,000 and
12,000 700 euro for this of course this
is just an estimate so therefore we show
a range here because we only have so
much information about the car
especially the condition yeah can vary
quite a lot so this is what we built now
what we actually had when we started
this project was a prediction model
already or a prototype of a prediction
model because that was what the data
scientists at Otto Scott had already
built a prototype and the language that
they used to build this was our and the
approach the data science approach that
they selected for this problem is called
random forest who of you has heard of
random forest before okay quite a few
nice they actually changed over the
years
so so let's have a look at how this
roughly works so in this case we take
the car listings of the last two years
for instance from the platform all the
car listings that have been on the
platform and then we use this to train a
prediction model and what you get out of
this are many of such decision trees and
you can see decision tree works like
this so you have a decision here at the
very top of the tree for instance
whether the mileage is more than fifty
thousand kilometers and then let's say
this is the case then the next decision
might be whether the fuel type is diesel
and then if this is the case you already
get a price and you can already see that
there's a hierarchy in those decisions
so it actually matters which of those
decisions come first because the one
most on the top here is the most
important one because it's decided
before the other ones now random forest
is the algorithm that basically analyzes
the data or strength with the data and
thereby decides how to arrange those
decisions in this tree and beyond that
random forest is a approach or a
technique that works against overfitting
so what is overfitting overfitting means
that a prediction model a trained model
all the works really well on pretty much
exactly the data it has been trained on
but produces rather bad results on data
or input that is different from what it
was trained with and obviously as the
main use case here is to predict prices
of cars that we have not seen exactly
like this on the platform and this
wouldn't be very good
so therefore what random forests does
and that's why it's called forest and
Autry is it generates many of those this
trees and it generates them kind of
randomly by randomly selecting subsets
of the training data so that all those
trees are slightly different and then it
basically asks all those decision trees
to make a prediction and then more or
less averages of all those results and
that's a pretty effective technique
against overfitting and this is what we
wanted to have here so up until here the
data scientists were working on their
own and coming up with this prediction
model prototype of the prediction model
and now our task was to actually bring
this into our high performance web
application so now we have a lot of
non-functional requirements for this
page it needs to be available it needs
to be answering in this fast response
times and it needs to be need to be able
to operate this without too much
maintenance so at that point in time we
looked into whether it's possible to put
our directly into production and this is
some years ago and we didn't found a
good solution the open source version of
a far it's not multi-threading so it
wouldn't handle the concurrent requests
and it was also from memory consumption
and other concerns it was not a valid
choice for us to actually put on into
production this could have peaked loads
when we when we start a marketing
campaign or something like this and we
don't want to see our cluster going down
so what is the potential solution a
potential solution would be and we have
seen companies doing this take the our
model transfer it into a programming
lead language where engineers are
capable of putting this into production
and that can handle the load in our
example it would also be chava so an
engineer would transfer the model from
our into java code and we could then put
this in production we have the
performance characteristic and all the
engineering practices around a well
understood run
unlike Java the downside with this is
it's a manual task transforming the
thing so that there can be errors in
there which you need to find out and
also you might have the problem and we
actually also had the situation that the
data scientist might change their model
so in our inception for this project
they already mentioned we started out
this linear regression then we tried
this model currently we are favoring
random forests but we might change this
model again if you learn that we can
improve on the model so having a manual
transformation of the programming model
into a different language would be not a
good idea if the model changes yes so
therefore obviously we wanted to
automate this right especially in cases
that Christian just described if the
model drastically changes and but also
generally when the model is retrained
obviously so um therefore we looked how
can we automate this and a technology
that helped us quite a bit with this was
h2o
one of you has heard of h2o before okay
a few
so h2o is a Java based analytics engine
that can be programmed using our and
there was something in that case that
our data scientist liked and and that
was really the important piece here for
us
h2o provides the possibility to export a
fully trained model to Java source code
interestingly it's not compiled Java
code but Java source code and actually
quite a lot of it but we'll talk about
that later of all over this then really
allowed us to integrate this entire
model generation into a continuous
delivery pipeline so on now let's look
at a very simplistic version of a
continuous delivery pipeline to get
through the basics we are talking about
here a typical continuous delivery
pipeline starts with
repository in our case typically a
github repository version control system
we have the code for your application
for your service then at the commit
stage you take all the source code
compile it if you have a compiled
language run your unit test do all the
verifications you need to make sure that
you have a proper version of your code
and generate an artifact out of it which
you then store in an artifact repository
in our case this is as simple as an s3
repository and then comes the second
part of the continuous delivery pipeline
and you actually take this just compiled
artifact and deploy it on to your
servers hopefully you do this without
interruption there are many techniques
out there like Bluegreen delivery and
all of those things to actually make
sure that your roller the new version
without interrupting what what the user
is doing and bring the new version in
front of the user why are we doing
continuous delivery like this we are
doing this because we want to bring
changes to our application to a code and
in this case also to a model in front of
our users to get feedback and to us as
often as possible and with less friction
as possible and such a continuous
delivery pipeline needs to be repeatable
every time I run through this pipeline
the same result should be delivered
meaning this need to be fully automated
manual processes will not work there
it's also traceable in our case this is
not so important but in general all
changes you can trace back to a commit
in your version control system and it
should be reliable so the changes you
bring through the pipeline you should
have confidence in it that you actually
are not breaking have broken services in
production so you need to verify and
make sure that this is working properly
from this most idealistic version of our
delivery pipeline now to how our
continuous delivery
I'm firm model or setup so we have to
pay status this is the historical data
of our listings 2 years of listings are
stored in there and the our scripts from
the data science make the cleaning of
the data train the model using h2o and
then have a proper model as an as a
result into the verification on top of
that then we generate the Java code as
Arif already mentioned this is exported
out of h2o a big Java classes and many
of them they are compiled into a char
and stored on a stream
so whenever the data scientists now
change something in the model
implementation or we want to just to
update the model and train it with newer
base data the whole delivery pipeline is
run through and the model is created and
stored on s3 this is what we call our
prediction model pipeline in addition we
have the more traditional web server
pipeline where we have in our case a
scholar play application which also is
in the commit stage built into a web app
char which is then deployed as seen in
the continuous delivery pipeline model
on to ec2 instances and the application
has started so now and when the
application is started it's also in
pulling down the the char from from the
h2o model from s3 and this also do
loading this into the classpath so with
that then have a web application that
has the model in process and can
directly access the model there and the
idea is that whenever the web
application changes the delivery
pipeline is run and a new version is
deployed or when something in the
prediction model pipeline changes this
is also delivered to the instances
yes oh yeah so Cristiano said basically
we want to fully automatically deploy a
new prediction model true production
when something changes there when the
data sign does improve something or it's
trained when you data so if you really
want to do this without any manual
intervention you have to have enough
confidence to actually do so so to
automatically push it through to
production and have users see it
basically and therefore we built an
extensive model validation workflow so
yeah let's see and let's start with how
a model is usually trained and how the
success of a training is evaluated so
let's say you are a data scientist then
what you usually do in the beginning is
that you divide your historical data
that you have into training data and
test data and importantly those two
datasets need to be disjunct and I'll
explain why in a minute
and then yeah typical relation is maybe
that this is 80% of the data and this is
20%
something that yet so then you take the
training data use it to train your
prediction model and then what you get
out in our case here is an a trained h2o
prediction model and then we take the
test data or specifically we take the
input data from the test data for
instance the age of the car the mileage
model and so on
and then ask the fully trained model
what price do you predict for that car
and then we get those tests estimation
results here and now we can compare
those tests estimation results or the
prices here with the actual prices here
in this test data because this is
supervised learning so we actually know
what the perfect answer is right
so these the results here will never be
exactly the same that's the perfect
results in here but this way we can
compare them with each other and thereby
get a score of how well the model is
trained and now you may see why it's
important that those two data sets here
are distract because if you would if you
trained the or if you tested the model
with data you actually used to train it
you would get perfect results in here
they would be identical to the ones in
here but unfortunately this would not
tell you anything about the actual
behavior of the prediction model to new
data so you would have a great score but
it would actually not give you any real
information about how this thing would
act in the world so um yeah therefore
those need to be separated and this
score in this case that we get here is
basically the first quality gate it
shows us whether the prediction quality
is still good enough so only if this is
beyond a certain threshold then we go
further and now we also want to check
that this train prediction model that we
then export first to Java source code
then compile and then actually integrate
into our web application so loaded into
the class loader of the web application
and so on we want to make sure that this
compiled trained prediction model
actually behaves exactly the same as the
original h2 all trained model because it
doesn't help us much if here we got a
good score and see okay that's great and
the data scientists think okay that's
really good what the model is doing but
then things go down go back down the
line here and the web application is
actually doing something entirely
different so that's why we have to check
that again
and here we use those tests estimation
results again and now simply check
whether this compiled prediction model
gives us exactly the same results as the
test estimation results so we know that
really nothing has gone wrong down the
line and then finally we also make sure
that the prediction model fulfills all
the requirements that the web
application poses at it and this is
called consumer driven contract testing
so in this case the web application is
the consumer and it consumes the
prediction model and therefore because
it's consumer driven the web application
states in a contract what it expects
from the prediction model and in this
case this is a lot about the interface
so it's what are the names of the
parameters yeah what data types etc and
that was really useful in the beginning
because yeah they simply really happened
quite often that the interface changed
all those things and then things would
break and production so this really
makes sure that this doesn't happen so
only if all those three things are good
the general quality of the prediction
it's made sure that it's doing the same
thing as a compiled prediction model and
it works well together with the web
application only in this case if all
those gates are green then we actually
release automatically to production so
you have seen now what I would like to
call data def ops you've seen the data
science part where the model was
predicted and we had some engineering to
actually take this model into a web
application in India and we also take
care how to put the things into
production so what we really learned
from this project that in the beginning
we actually worked all together this was
cross-functional teams data scientists
were in the same stage
up next to the engineers discussing
pipelines discussing model changes and
because we also are really touching all
of those points and this is what all in
the end the idea of a consumer facing
product and the feedback cycles around
improving this product and the model and
the software and the deliverer is around
because then those people actually owned
a whole pipeline they understand it and
therefore can bring changes fast into
production and reason about all the
parts there are no handovers in between
there's no data scientists producing a
model an engineer writing an application
for it and then an ops guy bring it in
production and the obstacle would not
know if the predictions are wrong but
somebody would call him in the night
there is something wrong with the
prediction so this is very valuable for
us especially then working together with
data scientists we are software
engineers I have a software engineer
background I learned a lot from data
scientist so it's fascinating to learn
how random forests work and what they
are optimizing for and also the quirks
of the model how does then actually
impact the product that is coming out
there and what can be improved from for
example what data are we collecting from
the user to actually make this
prediction and how do we transform it
that it's meaningful for a model and for
the data scientists they learned a lot
about our engineering practices they
learned that you actually can do unit
testing in RN justice actually makes
sense and actually also found errors in
our clean up script where we were losing
data and some points
also as I Reeve already mentioned the
notion of stable interfaces so up until
that project the data scientists were in
exploratory mode they could change their
model that parameters everything to
their liking
but now for our prediction model in
interacting with the application we
needed to have no stable contracts they
couldn't just go ahead and rename their
dimensions and features because then
the application will break lucky enough
we had those tests in place we found out
those things but yes this is things data
scientists learned also the usage of
version control system for four there
are scripts and all of those things and
what also was very valuable to them is
to actually get feedback of the impact
of their work so not building up a model
handing it over and doing them the next
project but actually staying with a
project learning and improving on the
model and actually seeing how real users
use it and how the rest of the of the
application is working with the model so
in essence working closely together make
those fast iterations possible
especially at the beginning of the
project to be fair in later times when
the engineering part was more dominant
engineers were not so deeply involved
but you need to take care and actually
be we need to recognize when Windows
close collaboration is actually required
all right so um now before going a bit
into some really technical lessons
learned during that project I just
simply want to emphasis what I'm sorry
what what Christian just said really
this if you take one thing home from
this talk for me it would really be make
the data scientists and engineers
software engineers work together so all
the technical stuff is minor this is
really D take away message so looking a
bit into the internal technical
challenges that we had many of them had
to do with actually generating gigabytes
of Java code and that is quite a
challenge if not to say an abuse to the
JVM I mean you have to imagine those
this this generated code from random
forests it's basically huge if-else
trees and that's it
so it's absolutely huge amounts of
stupid
source code also doesn't look very nice
and anyway even compiled it's still
really big so one lesson that we learned
at that time was that we had to use the
back then not default g1 garbage
collector by now it's default and this
was not a case because the the older
garbage collector it would literally
take seconds to sweep through all this
all this data and obviously if you have
a web application running in production
yeah you don't want the application to
freeze for one or two seconds so this
the g1 garbage collector is actually not
that much quicker to go through this but
you can set a maximum amount of how long
a sweep can take and that really helped
us a lot because then it may occur more
often but we can control how much it
effects the performance and
responsiveness of the application so
that was rather straightforward this one
we really learned this the hard way
cleared compilation is a pretty smart
feature of the JVM and it continuously
optimizes the generated machine code
basically on usage patterns and so on
the problem is if it tries to optimize
gigabytes of Java code continuously it
literally did not come back out of
optimization any more and and we
actually only found out about this when
we switched from Java 6 to Java 7
because in Java 6 this feature was
already there but it was turned off by
default in a Java 7 it was turned on by
default and we basically thought a while
what can go wrong switching from 6 to 7
and nothing worked anymore so I name was
really quite tricky to actually find out
what a lot of lack it was that ya made
everything being on hold there and then
the final lesson here was to do
extensive warm-ups with the prediction
models
and it's also because the JVM is rather
smart as it only tries to load into
memory what is actually being used and
it generally makes sense but again if it
only loads a huge amount of code into
memory at the time first users hitting
that particular part of the model then
it simply takes too long and the
application again is not responsive so
yeah we're really kind of engineering
quite effective warm-up procedures here
in fact we even used the test data that
I showed before to really make sure we
have a good distribution for those
warm-ups in order to really make sure
that we get all the all the data into
memory because yeah basically those
those prediction models they are
programmed code and data in one write so
this is an example of for warmup time
this is the CPU utilization of an
instant that's just started and as a
reef mentioned the warm-up phase kicks
in and this is quite sometimes it takes
so this is over an hour takes to warm up
just easy to instance on AWS and it's
not a small instance we are using there
but you also see that after the warm up
the CPU utilization and the normal load
is then quite good so this is then not a
heavily used model we have additional
use cases that I mentioned soon we're
actually distanced and feels like being
warm up again the warmup time in itself
is something we could live this but it's
still one of the most problematic parts
now in this project as we were talking
about fast feedback and continuous
delivery and now everything is fast and
at the end of the journey you have to
wait for an hour for instance to come up
and additionally as this should be a
highly available application in case of
a disaster if
of those instances goes down or god
forbid two of those instances go down to
do some configuration error the time to
recovery is quite high so we've changed
the project a little bit the first time
we had an incident we had the instance
up and running again hooray
but then we had to wait for an hour
before the instance was wound up so
there are some lessons to learn and of
course of all the advantages we already
have talked about this is the big
disadvantage we have in there
in addition still having the chavvy mm
production is a very good thing if the
cherry M is warm
she's very very fast and if you would
have are in production one of the later
use cases would not would not have
worked we are now one of all I will talk
about this later
but currently we are also not using this
app for the standalone web application
but we are also estimating the prices
for all the listings we have in stock
and we also have this needs to be
repossessed for xena quite aggressive
SLA I believe half an hour so putting
2.4 million listings against those
models in in half an hour and something
that JVM can handle and we have not
tried this Bazaar but I believe this
would not be such an easy thing to do
yeah so basically when the model is was
warmed up we were really seeing response
times for a prediction of low
single-digit milliseconds so really like
two three milliseconds to get a
prediction and yeah I mean the warm-up
definitely has its drawbacks but this is
really an incredible performance then I
mean it's kind of makes sense because
the yeah JVM only needs to go through
those if-else statements in bytecode
more or less so it's really quick so
then beyond those technical lessons I
also generally learned then after this
project when I switched to the
to emo billion Scouty and berlin other
Scott's sister company and was doing
other kind of data science in production
projects that this approach of applying
continuous delivery principles to data
science projects is really really useful
independently of the specific technology
that you use so in this case we were
using an entirely different text deck it
was done using peyten and spark and we
saw similar advantages of creating such
delivery pipeline with quality gates and
so on and was even more useful in this
case it was also entirely different data
science approach it was about
recommendations so in this case model
evolution was much quicker if we look at
this price prediction use case we were
looking at basically a sliding window of
24 months so yeah if you update this
sliding window of 24 months with a
couple of days the prediction will not
change dramatically right but in this
case that we were doing at the mobian
scout we were actually using user
interaction data as input for the
prediction model and this changes much
more quickly and here yeah it simply
doesn't work without having everything
autumn eyes of retraining the model and
adapting it so this was what's really
useful here as well interestingly not
only the advanced which is very similar
but also the challenges were also
similar so I'm quite often after his
talk I get asked like why didn't you
simply do this in Titan and and I mean
it also worked quite well this way but
it was very similar we had similar
challenges there also when we got for
advanced modeling techniques and we're
loading a lot of data into the
prediction models the models got big
then we need to make sure those models
are loaded
into memory to get acceptable
performance and indeed we did not have
those really JVM specific issues where
we have basically to fight against
optimization features of the JVM but
then on the other hand we also did not
get or never came to the point that we
had debt superb performance of the JVM
so it was actually be a trade-off which
you can obviously consider what makes
sense in which situation and yeah it's
why we come here again to audit to the
general conclusions continuous delivery
really allows us to bring prediction
model changes life very quickly and this
is only possible with extensive and
automated end-to-end tests that really
provide you with confidence to deploy
for to production automatically yeah
especially what I show you before that
you also then again have to test whether
the compiled model behaves the same way
as to one before those things and then
this specific technical approach of Java
code generation has its drawbacks but it
also allowed us for very low response
times and in general excellent
scalability then because of those
response times and actually by preparing
this talk I looked up at the h2 old page
and then now have more or less
paraphrasing what we are told telling
you here that there is a problem with
the big models and they have an
additional approach now where they're
not in addition to exporting at this
Java code they're also now allowing it
to export a model and a Java code so
that they're actually using the model
data in the java code so it's you you
still have the same general approach but
with less of the drawbacks we haven't
not tried this in production but this
could be something we try out so we are
not the only one who are suffering from
big Java like sizes but now let's look
at
what afterwards came so we talked about
the standalone application people
entering their data now we are also
using the this model to predict the
price when users are creating your
listing on our site to actually give him
an indication with this data but our
model says is a good price for the for
the car to sell and we also and this I
already mentioned that we are now
estimating the prices for all the
listings we have on stock and put them
into categories so is this as arive
already mentioned there is a price range
coming out of there that the model
prediction and then can say is the price
of the current car in comparison to the
other comparable listings is just a top
price is a good price is a fair price is
it a bad price so we can put categories
on top of it so now I believe 90% of the
listings on our site have just price
labels and then the price estimation
attached to them when you search for
them these are the categories where the
listing is then entered into and this is
now also usable as a criteria for for
searching so that you can only search
for top prices or a good prices on on
your listings which extends on the usage
of of this model and as already
mentioned this now is when re indexing
or reevaluating all the all the listings
puts an additional burden on the model
services to actually answer in a
meaningful time so this is the end of
our presentations thank you very much
please remember to rate the sessions and
now we are here for questions I assume I
guess yes first of all there's a comment
from the audience here one saying this
is finally a real case finally it's
interesting to see the theory meeting
the limitations of the real world here
quite a lot of the questions here attack
the size of the generated code asking
questions like that you try to optimize
it did you look for duplicated code so
in this case this as a Reeve already
mentioned the initial version using your
random forest directly this is a lot of
if-else statements with if mileage is
like this if this so of course you can
reduce the code size by changing the
model parameters how deep is the tree
how many of those trees are in the in
the in the random forest the biggest
improvement which we did not mentioned
in the targets that we actually switch
the model once again and we are now
using radium boosted machines which are
solving some of the problems so the code
size gets smaller nevertheless this was
necessary because the model size grew to
a point where we actually for certain
make model combinations were not longer
able to actually export it out of h2o
because they hit the 1 gigabyte limit so
we need to switch the model still by
increasing the number of features in
there that they're still quite big but
there of course that there are
trade-offs you you can do there if if if
this would go to two or three hours
warm-up time I believe we would just
adjust the model accordingly
but typically refactorings or something
like that is not doable because it's
just the model as code and the sets as
big as the trees and of and in the tree
sizes there I think with those
optimizations was possible to basically
bring down the most
to about the tenth of the original size
but it's more or less in the same
ballpark right it's not a thousandth of
the of the original model because as
Christiane said it contains the data
that's the thing you're yeah kind of
putting data into compiled code so it's
pretty much like drowning at 100 meters
or centimeters yeah you explicitly check
the compiled model a to comply compiled
model for correctness do you have a
reason to believe that this is necessary
why wouldn't it be correct I mean so if
we changed everything breaks at some
point in time and this is one of those
things you're transferring from
technology a to technology B and that at
some point in time things go wrong and
it's not only about that the model
itself might break but it's also how we
interface with the model so that coming
out of there is a predict function in
Java with a lot of parameters and if we
just mix up the parameters we pass in
their due term some refactoring or
something else then this would also
break so it's not only about is h2o bug
free which I believe the question is
hinting at so I'll be verifying that h2o
is a good job but also the compiled
model and how we interface with this
model needs to be verified so we have
not found a back in h2o to be honest
and obviously this actually happened
that we mixed up the numb the position
of the parameters and then things were
wrong and yeah so we actually also
needed this this kind of validation at
the integration into our application is
correct at the end of the day this is
about selling cars and giving the right
price so do you have any feedback from
the receivers from these prices did they
increase the sales or did they did the
customers accept the prices or there is
a lot of learning involved so I will not
go into many of those details because
this is not something
on camera I will talk about what all
please turn off the camera what we
learned No okay up there but it's very
very interesting if you put something
like this in front of a user who also
believes he knows the so that the users
who believe they know something about
cars and the particle prices and users
that don't know about car prices and how
do they react and so first of all the
initial version they believe this isn't
bought advertising so because of just a
top price there they don't believe that
this is actually data science trying to
predict the price but they just believe
this is like Google Ads or something
like this this is why we'd in later also
introduced that they're not so good
categories which increase the trust
level because then users actually see a
there are also labeling bad prices
therefore this seems to be more
trustworthy and over time the trust has
increased from from from the user their
desire using this it must be a very
dynamic world I mean it's not just only
Volkswagen it's every model every every
type of car there must be new new models
hitting the market all sorts of things
affecting the prices so it must be a
very dynamic type yes for example now we
have the diesel gate and thus the
digital gate actually influenced the
model and so yes and no the thing where
you can react and make the model more
dynamic or less dynamic is how much of
historical data you are using so if you
using a lot of historic later the model
does not change as fast because it's
also taking into account previous data
and if use not you're not going some way
back in history then the model becomes
more dynamic so you have actually have
an influence in how the model reacts to
print price changes that are happening
in reality but of course we are lagging
so because we are not using the current
stock but we are also using historical
data we are lagging behind if something
changes for
sample like in dieselgate exactly and we
just learned that the results do not get
better than the assumptions yes dictate
so of course and did you ever consider
to use some feedback mechanisms or to
apply deep learning did you try that
before you chose your strategy here so
from from my data's I just the last week
to to the data scientist who's currently
involved with the project and in the
next iteration of course we will try out
new models so they are looking into all
kinds of models to actually change the
crucian and the behavior of the model
but to be very honest the the total last
of accuracy is not so much of a problem
of the model problem but also of a data
quality problem so if you have all the
parameters for a car and you don't know
exactly the condition the car is in
because the one who's putting listing
online did not fill out the form
properly or even we didn't get gave him
the opportunity to properly formulate
the condition of the car or is not
giving you all the parameters you can't
make good prediction so it's it's a
trade of you it's a good indication for
users how how does behaves but it will
not be exact truth and and you can't go
the last mile that to actually say
justice this is a an authoritative price
you're giving there this out seeing the
car which is also some some of the
critiques we are getting there how can
you predict the price of a car without
actually seeing the current seeing its
condition but yes it's artificial
machine learning and not real-life
inspection thank you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>