<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • Globally Distributed Cloud Application at Netflix • Adrian Cockcroft | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • Globally Distributed Cloud Application at Netflix • Adrian Cockcroft - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • Globally Distributed Cloud Application at Netflix • Adrian Cockcroft</b></h2><h5 class="post__date">2013-04-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Mn0_Xmw4rQs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm going to talk about what Netflix did
and basically yet another slice of cloud
architectures what it looks like when
you basically build on top of amazon's
public cloud and build out a large-scale
global service and then i'll end up
looking at all of the open source pieces
that we've been putting out and some of
the ones that we haven't actually put
out yet but some of the plans we have
for things we're going to be releasing
in the next few months so that you can
then go and build your own public based
cloud application or use these pieces
for other subjects so i'm going to talk
about what did Netflix do why it did it
and when it did it then look at the
globally distributed cloud availability
model look at the open components and
then talk a bit about how you assemble
these things to build your own platform
so what Netflix did we moved a whole lot
of things to Assasin's our corporate IT
basically we moved some of our internal
tools that we use for managing our
systems like pager duty which is the
core rotor and the the escalation trees
for who you call when something's broken
our developers are all on call for if
you push code if you if you have curve
running production then it's somebody in
your group or yourself will take turns
being in charge of if it breaks at 3:00
a.m. you get the call and then
AppDynamics uses an application
performance monitoring and we use things
like the we as large high-level tools
like elastic MapReduce which is Amazon's
Hadoop
we then built our own platform level
customized deliberately to get make our
developers as productive as possible
that's the number one thing we're aiming
for here and we started about three
years ago there wasn't much choice out
there and there wasn't anything that
would extend to the scale with it we
were trying to build out and we knew we
were going to build out and then we
moved all our incremental capacities of
infrastructure as a service we have the
same small data center that we had in
2008 we have approximately 10 times as
much capacity in the cloud as we have in
that little data center right so we're
something like at 90% of our capacity is
cloud based and the data center has less
and less in it basically runs our
us-based
business and some of our corporate IT
all the streaming application now runs
in the cloud
to the point that if you break our data
center or turn off the connection to our
data center streaming still works and
you know a lot of our outages over the
last year have been the data center
broke and took down some piece of cloud
functionality so we finally cut those
links so now we're just completely
dependent on the cloud which we've
actually found to be more of highly
available than our data center and I'll
talk a bit about how we arrange to do
that so why do we use cloud there's all
these things that we didn't want to do
because we we don't have time we get
bored too easily right so you never get
stuck with the wrong configuration you
say you know I need 500 machines with a
16 gig of ram oh it doesn't really fit
Dan well you know I'll take you know
this twice the number of CPUs let's take
250 machines with a 32 gig of ram are no
problem there we go just redeploy we
don't have to ask permission if you're
pushing something into test if you're
allocating systems in tests every
developer can just create as many
machines as they want in test there's no
permissions there's no approval process
there's nothing if you're going to
production those one approval from a
manager to say yes I know that this
thing is going into production right
that's the only check we have and really
all you have to do is enter it's a
change ticket number is really all it
asks you for and you could type anything
in that so if you do that too often and
break things we'll have to take a fairly
dim view of you but so we treat everyone
like an adult and we we assume that the
developers know what they're doing and
that turns out to give good
responsibility people think about it and
that giving people responsibility helps
everything we never run out of space and
power we were able to launch in Europe
by just deploying a thousand machines in
Ireland and Amazon with a few weeks
notice what really didn't give anyone
notice it just took us a few weeks to
get all the code lined up and get
everyone to push code we didn't have to
go hire anybody in Ireland and planned
stuff and all those kinds of things and
and we don't really have an IT
department anymore we I don't have to
have meetings with them to say please
can I have some machines and they say
it'll take
it's like it used to so there's a whole
bunch of sort of like what I call
developer trends or management buzzwords
if you like there's there's you know how
many people are doing big big big data
hadoop in production right now few of
you okay how about Amazon Cloud in
production a few of you APM application
performance management anyone doing that
it's big new sort of you know things
like out dynamic steinege dynaTrace
stuff like that
integrated devops people doing all that
you know getting the dev and their ops
together okay continuous integration
delivery okay that was most of the
tracks people learning how to do it no
sequel a few people doing no sequel so
give me a mere platform as-a-service
fine-grained service-oriented
architecture like nobody oh I got one
person doing fine grayness away the rest
of you going I have a big application
it's just one big thing social coding
open development on github do you pull
code from github your your application
code that you developed and put on
github is that the source of your bill
that you put in production a few of you
right so we we've been doing all this
stuff the first time we did this this is
the first year we did this in production
so we've done all of these things and
that's the issue I think you know why
I'm up here talking to you rather than
the other way around because we figured
out how to do all this stuff over the
last few years and there's a whole
separate presentation about why Netflix
gets so far ahead of most of what's
going on in the industry which is to do
with sort of corporate culture and brave
management and scared engineers being
dragged forwards by people with crazy
ideas and management anyway so I'm gonna
talk a little bit about as I explained
this we built everything on top of
Amazon
so we're with we're doing Amazon
specific things that depend upon the
features of Amazon and so some people go
well and you're locked into Amazon and
there's really two views of this there's
the operations focus and how many people
here it's consider themselves are
operations people so we've got a few
okay and then most of you probably
developer people right
operations are always worrying about
vendor lock-in data center use cases and
cost savings because it's simple to
measure cost and you know you tend to be
very cost focused developers are more
focused on getting stuff done quickly
and if you have one mature supplier
there's less complex test and debug you
have to test everything twice if you've
got two suppliers because you know even
if the code and the interfaces are the
same you know it's not going to work
quite right and on the other supplier it
means you get a faster time to market
for your products and your yourse cost
savings come in developer time and I
don't need as many developers to get
something done right now Netflix sees
developer time as the biggest constraint
on our business not operations cost now
it may not be true if you're building
widgets and you're trying to churn stuff
out cheaper widgets than somebody else
but if you're in a rapidly growing sort
of technology innovation business it's
how fast can you how many good
developers can you find and how much can
they produce becomes how you get ahead
in the market so we're focusing on
making those developers as productive as
they possibly can and giving them the
best possible platform of the most
functionality which is why we built on
top of on top of Amazon so what we have
is an infrastructure base is every
feature of AWS including all the ones
that we've beaten them up to develop so
they you know if they don't they have
missing features we keep pointing out
the missing features and Amazon listens
and maybe six months or a year later
this feature pops out and we meanwhile
we've built something that kind of does
that so we retire the thing that we
built and use the Amazon one in a few
cases of that and they're just things
like you know there are Amazon clones
but they don't have all of the features
like nears s3 but it only has five
gigabyte files but we want to put
terabyte files in there
and and when you write a terabyte file
you can't just do a put because it craps
out after you know ten gigabytes and
then you have to start again from the
beginning
so multi-part it writes basically means
you paralyze you multi thread in chunks
and you write all your chunks and if a
chunk fails you only repeat that chunk
so if you're doing you know that then
that's a lot more complicated than the
simple s3 stuff so you get all these
deep features and then on top of
we added our own features large-scale
it's again it's mature flexible
customizable we have a console called
Asgard which replaces the Amazon console
I'll show you a picture of it later but
that's up on github
we have these somebody else earlier you
know what's a what's a what's the chaos
monkey I'll explain that a bit later but
think of like you have demons and a
eunuch in honor Linux that run stuff for
the operating system the monkeys are the
autonomous services that we run in our
cloud and we have chaos monkeys and
janitor monkeys and things like that the
tidy stuff up and kill things and then
we spent a lot of time working on
Cassandra and zookeeper automation to
make it trivial for anybody to deploy
Cassandra cluster so at the end of that
if you're a developer I got an analogy
here if you're a rock band and you're
about to go on tour and you're trying to
decide who writes the setlist
is it the roadie or the musician alright
let's say you're Eddie Van Halen and you
want to take you want you do the setlist
but this setlist requires an awful lot
of different guitars to play it whereas
the roadie would like you just take one
guitar right all right so that's the ops
guy I want to keep it simple one guitar
it's easy I can get like three copies of
them and they'd be identical and
everything will just work and it'd make
my life easier but van hey Eddie wants
to play all of the songs over his whole
history and he needs all these
cool-looking guitars including the
weirdly shaped one over there so really
you do need all those guitars on tour
and if you're a developer you want to
choose the highest functionality
platform that will let you build the
richest product in the fastest time now
the problem with that we call freedom
and responsibility at Netflix developers
are leveraging the cloud to get their
freedom we have no silos it's a single
organization there's no separate IT ops
organization that constrains what the
developers can do there's no separate
product organization where one
integrated product unit that includes
all development and operations needed
and at the bottom it just calls Amazon
api's directly but thou developers are
responsible for those things that the
Ops guys used to look after for you so
you've got compliance performance and
availability and all these things you
just have to deal with it
and we turns out that you can train
developers to be responsible for
compliance performance and availability
and there's this nice eddie van halen
quote that he went to rehab and he was
able to change and get better at doing
these things there's a version of this
slide deck that has van halen references
on every single slide because i did a
conference where the the guy who ran the
conference was a heavy-metal freak so
i'm gonna talk briefly about what is
this service because most of you
probably haven't seen it but it is
coming soon we announced a few months
ago that we're launching in the nordic
countries so we're now into october so
you know maybe later this month may be
coming soon in a few more weeks you'll
see the netflix streaming service now if
you go to netflix calm right now in
denmark you get this nice screen here
saying please give us your email address
and we'll let you know when it's coming
but one of the cool little things and
you can't really see this down the
bottom at the bottom of the page there's
something that says EU west one and then
there's a funny hexadecimal looking
thing and this is D K so that funny
hexadecimal thing if you have used
Amazon is an instance ID so that so that
basically we render in the bottom of the
page which which machines had sent it to
you so that is a a Amazon Ireland USA EU
West one machine it's kind of little
thing I'm sometimes these things maybe
you don't come up I have sort of various
debug stuff turned on but kind of fun so
that's that's what it looks like if
you're not a member when you I remember
you get something that looks more like
this it gives you a whole load of movies
we have Facebook integration so I can
see stuff that my friends watch and you
know all the things I've been watching
recently and there's a whole lot of bits
and pieces there so basically that's the
member website it's basically giving you
hundreds of movies to choose from
and trying to pick the ones that will be
best for you rather than just giving you
this sort of alphabetical list of every
movie we have now that's delivered by
our another service we have which is a
CDN now we have our own built in Netflix
CDM we previously we used Akamai
limelight at level three but we've
recently augmented that with our own CDN
and that's what we're primarily using as
we're rolling out new services
so Netflix is a hardware company this is
the box that we have it's a hundred
terabytes of movies in a box we give it
to you for free if you are happen to be
an ISP with several gigabits of Netflix
traffic coming through and that saves
you money and saves us money
it runs BSD and we've put we've did all
of the ipv6 work on this box and we put
the fixes back into BSD we found some
bugs in the ipv6 support about half the
world's ipv6 traffic runs from these
boxes the other half is YouTube and then
there's noise on the axis which is Yahoo
and Facebook and everybody else that
does ipv6 but you know about ninety you
know forty nine percent of the world's
for the ipv6 is Netflix the other forty
another forty nine percent is is YouTube
that was sort of world ipv6 day loss
last summer and we've open-sourced not
only the software for this box but the
hardware design of the box too so this
kind of kind of open hardware and sort
of open source here's an old picture
from a few years ago we now have eight
hundred different streaming devices that
we can operate two digital TVs and
things we've been shipping these digital
TVs into Europe for a while and what
happens is when you turn on the service
is that their software internally says
oh look I do Netflix now Netflix icon
will appear on so we have sleeper code
in lots of a lots of a consumer
electronics devices going global so the
major services we have we have a
non-member site a member site CDNs all
the backend for controlling and managing
and routing traffic to that CDN or
running on the cloud we have API store
internal external devices an API for
controlling video playback DRM QoS and
all this so all these services run as
fine-grained SOA on Amazon so this is
you know one view of ourselves each
little box here is a different tiers
it's not a machine it's a group of
machines so this source of the traffic
between the different services this is
actually just running in our test
account at one point because the
production one is even harder to draw
and it's got too much stuff on it so
some of those boxes might have 500
servers in some might be just five or
six something like that if you actually
pick
servus let's say our web frontend it
looks something like this again this is
a out of our test account so you see it
says start that this is a view from app
dynamics which is basically doing Java
bytecode instrumentation and measuring
the flows through all of our code CC and
end-user calls the the web service and
it's talking to a whole bunch of other
bits and pieces there's a sim service
which finds movies that are similar to
other movies and a bunch of other things
like that
yeah now that's the the overall flow but
what you really care about is a single
request
so another thing AppDynamics does is it
captures single request and it sees
something slow it'll just grab it so
what I can do here is I can click in on
any of these little boxes those little
black boxes the drill in and it gives me
the stack trace of the code and where
and which what exactly what HTTP or
sequel request or whatever it made or
memcache D or whatever so you basically
you get all the code and you can see
what's going on so this is how we drill
in and figure out what what's going on
to get some idea it's a very fine-grain
architecture each of these services does
one thing so the similar service you
give it a user ID and a movie ID and it
returns all the movies which are similar
to that movie but filtered for that
customer right so if it here you might
find a movie like scooby Doo right which
adults and kids both like if I do a
scooby-doo thing it would get like adult
stuff that's like that but if it was a
kid that did it it would get the kid
stuff right so different things like
that so we have a bunch of architectural
patterns I'll talk a bit about the
availability model here so we have lots
of little isolated services that do one
thing
they're stateless and they're replicated
horizontally that but to deal with that
the developer model is resilient
business logic so let's say I'm that I'm
the web service and I'm going to call
this similar service and say hey give me
the movies that are similar to this what
happens if I don't get an answer back
well I have to write code that assumes
that I may never get an answer back from
any other service that I call and I have
to do something sensible and in this
case you just you know the movie the
home page is lots of different rows one
of the roses are similar 0 if the
similar service is
now you don't get those rows you get
like normal John Roos or you get some
other kind of roads we just that we just
drop out things so that it gradually
falls back to a less and less
personalized experience but it's a
gradual degradation you can't really see
it happening unless you know exactly
what it's supposed to look like we also
put everything in three balanced
availability zones and I'll draw some
pictures of that a bit later and then we
triple replicate all of our persistence
and I'll show explain more about that so
I'm giving a talk later tomorrow
afternoon in the no sequel tract about
highly available no sequel with
Cassandra and I've got a whole load of
slides a lot more slides there about
Cassandra and how we use that and then
we also build our isolated region so US
and Europe are separate and don't take
each other down so if we go back to that
diagram that we had before we test this
with the chaos monkey which kills
individual services so I say I killed
that service everything else just keeps
working we have a latency monkey which
adds latency or makes that service
return errors at some rate so I can make
it return 500 series error codes or make
up an error code and see what happens
and the services that call that just
have to deal with that or you can insert
three-second delays or or make it never
respond occasionally and what happens
then is you get the errors and the
latency ripple out from that point and
there are other services have to absorb
it we test that in production we test
the chaos monkey killing individual
instances in production so like I said
we have three balanced availability
zones so there's a load balancer at the
top it's feeding all the traffic into
Zone A B and C which are separate
buildings which are around a millisecond
apart right so they're separate data
centers within Amazon's environment
there are Us East has five zones Europe
has three zones so you pick three of
those and where the all the
communication between zones if this is
concern up completely correctly
according to the architecture which of
course it isn't quite but it's near
enough for most cases you do all the
replication side to side at the
cassandra and caching layers so the
persistence layers that the stateful
services talk sideways to make sure that
that state exists in all three zones all
the stateless services just talk
amongst themselves so that if we lose an
entire zone when there's a power outage
which has happened occasionally the the
load balancers have to complete two
thirds of the capacity still there and
everything should still work I've got
two copies of the data when the data
comes back I can you know copy the data
back get everything back in sync and
keep running again but or I can
basically just turn off the traffic so
I'm no longer sending traffic to that
zone and then you know it'll be clean I
should Bing have no errors now if this
works right we get a slightly higher
error rate for about two or three
minutes after losing like a power outage
on a third of our capacity right when it
goes wrong it takes out the whole thing
but that's bugs the way it's supposed to
work is that it's supposed to keep
working and it has done that a bunch of
times now if we look at the persistence
layer when we're doing maintenance on
Cassandra like updating a Cassandra
cluster we have a continuous operation
which is updating nodes with the newer
version of Cassandra or doing
maintenance on them and so it takes a
Cassandra node out of service
now there's three copies of the data so
what that means is that I'm effectively
losing one copy of the data that is in
that local zone I still have two other
copies so what happens then is while
it's under maintenance we stop
replicating traffic to it and any
machine that's in that's tries to talk
to that particular persistent store goes
and gets the data from the other zones
okay so it's where this is happening all
the time we don't need to test this
because we're walking around darkest we
get a new version of Cassandra every two
months so we got very good upgrading it
automatically have Jenkins jobs that do
nothing but upgrade Cassandra so we have
about six hundred Cassandra instances
and about sixty distinct clusters and I
don't know why we have 60 clusters
except that we made it trivially easy
for developers to deploy clusters and if
you make something trivially easily you
get a loss of it right so like there
with a few clicks on a website I you
know not really that much of a developer
nowadays I can deploy an arbitrary large
Cassandra cluster just go please make
one and it comes up and it appears about
five minutes later so then we have US
and Europe and they it's a clone
right we have three things so you see my
little diagram I have now have six
complete copies of all my data I have
six copies of six batches of every
service but I have traffic from Europe
going into the right traffic from the US
going to the left something like a ten
to one ratio right now although the
European site is growing you know
proportionately faster than the u.s.
site so there that's gradually that
ratio is decreasing and then we have
global replication between the two and
this is done across the public internet
with you know SSL protected TLS
basically between them so it's encrypted
and the back ends but the Cassandra has
a way of finding the nodes and the other
and the other cloud and we have security
groups set up and stuff like that so
basically what that means is that
anything I write into Europe within a
second or so will actually be in the
u.s. it's sort of you know 100
milliseconds hundred 50 milliseconds
latency so it's just whatever however
many round trips it took to send the
data what that means is that if you sign
up as a member of Netflix in Europe and
you went when that happens we write the
data in Europe but it spreads to
everywhere in the world so if you then
visit America on vacation or for a
business trip you should just use
Netflix you were already a member of
Netflix you're a global member so it's a
global membership model it's like so
even something I mean there are members
everywhere in the world and you just
sign up once and you're done
ok so if the network between US and
Europe goes down temporarily or gets low
or whatever everything just keeps
working both sides work independently
this is if you think cap theorem this is
AP right partition and I'm available
both sides are available I can write in
to both sides I can continue to sign up
members and later on it goes away and
Cassandra's anti entropy mechanisms kick
in and everything gets back in sync
again and when we lost the entire zone
we didn't have to Europe didn't go down
we lost the zone in the US and and we
didn't have to deal with it when the
u.s. came back it just cleaned itself
back up we have some manually sequence
jobs but it's mostly Jenkins jobs that
this Jenkins processes that run around
sequencing when we do repairs if we lose
an entire
you know region the other region still
works so that's the way we currently
work so let's look at that in terms of
failure modes and effects got the
failure mode probability of it which I
rate as high if it happens every week
because we deploy new code very
aggressively and we just deal with it
it's low if we you know basically hardly
ever see it and or have never seen it in
its medium if it happens on often enough
that we want to be able to have a
mitigation strategy for it so if the
application fails that's the circuit
breaker pattern or dependency commanders
we call it basically your software just
has to be able to deal with the fact
that everyone you're calling might might
be dead and it may be because they just
pushed some code and that codes broken
and they didn't realize and they are
they'll be broken for a couple of
minutes before they realize and then
they'll switch it back so that's kind of
we we have very rapid rollback set up
and it's as soon as you tell so we the
way we push code is we we let say we
have 500 API servers and we have a new
build will create 500 new API servers
with the new code well actually the
first thing we'll do is recreate a a
sample of a canary server we call it
over the new code that and then if the
canary survives we'll make more of them
and then we'll point all the traffic at
the new ones but we leave the old ones
there at least through that night's peak
traffic so about 8 hours 12 hours they
automatically disappear 12 hours later
that's one of the janitor monkey cleans
them up or something like that various
mechanisms for doing that but the idea
is that the old code is still sitting
there and the machines are warmed up and
ready they're just not taking any
traffic and if anything goes wrong with
the new ones you can flip the load
balancers back to it immediately
let me see region failure it's it's
happened occasionally and it's typically
there was a routing error which took out
the entire region because it black hole
routed all of us east for about half an
hour every outbound packet could not
leave you could send traffic to it but
you never got anything back so that is a
total region outage but as soon as they
fix the routing everything came back and
no machines were harmed in this it was
just no traffic
that's really the only region level
outage that we've seen most of the other
outages that people have called like
hourly entire Amazon region went down
was actually a zone going down that
caused a bunch of websites that were
based in that zone to go down and the
last outage we had was because we had a
bug and we didn't correctly handle the
case of losing at one zone so zone
failure we say medium at which we intend
to be able to run out of two or three
zones we have a chaos gorilla which is
the bigger version of the chaos monkey
which is designed to kill an entire zone
and see if everything runs and we last
time there was a major net Amazon outage
was just before we were about to test
that case and we didn't find the bug in
time so it's kind of annoying but yeah
the cows girl is an interesting
interesting thing to test it's also
capable of moving aren't all of our
infrastructure to a different zone so
Amazon has five zones so we're running
in three of them if one of the ones
we're using breaks the chaos gorilla has
a mode where it moves if all of our
infrastructure from that zone into a
different zone and you have to reprogram
all of the load balancers and everything
let me see datacenter failure happens
reasonably often we've basically been
moving everything to the cloud because
our datacenter turns out is less
reliable and the machines we have in
there things like Oracle goes down or
corrupts data and you have to wait for
it to come back and that's a kind of
annoying datastore failure we haven't
really lost anything out of Cassandra
but we do continuous backups I'll talk
more about that tomorrow we can restore
from s3 it's a single rest call will
cause a Cassandra cluster to restore
itself to a point in time very very tidy
and then if s3 fails which again is
we're never seen we have a remote copy
of all that data which and we have
another copy it isn't even on Amazon at
all so we just have our backup files and
we keep them on the other side of the
country and everything so the actual
deployment and at the the rollout we
took the first thing we started
processing on Amazon was the the videos
themselves so we're getting all these
videos in and as we got more and more
we have to reinstall the format and the
consumer TV formats and then we they
will want it slightly different formats
so we're to reprocess all of our videos
and we make we don't do any dynamic
we're not reading them on the fly we're
doing very high quality encode so we're
putting a lot of effort a lot of CPU
time into getting the best possible
picture at the lowest possible bandwidth
right that it's worth spending longer
because we have enough customers that we
know we're going to be showing that
video stream lots of times if you do
on-the-fly encoding which is sometimes
used particularly for mobile because
it's relatively low bandwidth and there
are too many different types of mobile
devices you tend to get much lower
quality in codes that way that feed CD
ends goes out through the ISPs and the
the data rate we are feeding to our
customers is measured in terabytes it's
still measured in terabytes at 4 a.m.
our low point on our graph is still
terabits right where the high point is
lots of terabytes and the last number we
saw was 32% of the total US I am
bandwidth to customers like 32% of the
Internet's capacity at peak evening
Netflix watching time is just Netflix
streaming movies to people in the US I
believe in three months in the UK we
were two percent of the total the UK
traffic that was after three months of
launch and after about about nine months
in the UK we have a million customers so
I'm not sure how many so it went
probably several percent now YouTube's
about ten percent of u.s. bandwidth who
lose about 2 percent of 2 or 3 percent
and bit torrents about 10 percent so
we're like several times bigger than
BitTorrent ok so we did that once we had
everything running at volume we had too
many logs so that didn't fit in the data
center we move log processing to the
cloud that's when we started using
Hadoop and that was in 2009 then we put
the api's that handle the playback and
DRM in the cloud because we were getting
too much of that traffic and then in
2010
we had this big race and we had these
slide decks with pictures of aircraft
trying to take off and big explosions at
the end of the runway
and says okay you have to but we we
didn't have a back-up plan it's like no
we are not going to build another data
center we know we don't have enough
capacity to run the website by the end
of this year right and we are not going
to build that capacity and you have to
make it work in the cloud before the end
of the year right so this was the the
oncoming thing and we just got up so
like just before Christmas we got the
last service out of the out of the data
center that we had - and that really got
everybody's attention and tonight
focused on making it happen so that that
worked pretty well in the end it's
almost all Cassandra at this point the
rating system still runs on my sequel
that's actually a very old application
and they're in the middle of rebuilding
it to run on Cassandra that's the only
customer facing app we have running on
my sequel and if this if it breaks we
actually the site works without it it's
the thing that tells you how many stars
each movie is should be for you it's a
personalized star rating and we just the
stars just we do without them we just
they disappear from the UI then we got
the API out and then finally in 2011
with that things like customer service
started can transferring to the cloud
and in 2012 we've been moving billing
we're relatively get getting somewhat
close to doing a PC idea PCI DSS in the
cloud that's probably going to happen
start happening in the next six months
to a year and we're already doing Sox
compliance which is those of you in the
US know about Sox compliance so that's
already been happening in the cloud
alright so we've talked a bit about some
of the patterns that we've been using
here and to start with we're going to
get look at the goals so we started off
with this code in the data center this
was about three years ago and we had
team of about 12 people that we took off
of all the other projects and and I was
managing most of them and I managed this
this process and that's how I ended up
as this cloud architect kind of thing
but I was a development manager then
and this was what I wrote down three
years ago says we want to be faster
there was a different VP asking for each
of these I won VP wanted it to be faster
aren't lower latency the guy cares about
product then the guy who was kept
worrying about getting us into the cloud
we can't use any more data center
capacity we don't want to buy anymore we
don't do any build-out we have to do
this in the cloud and we have to have no
central vertically scaled databases we
needed better availability and my
manager in particular was saying we need
to optimize for agility of a large
developer team and leave behind this
eight-year-old tangled codebase where it
really was one big blob of code where
everything was all integrated every two
weeks and we'd sort of try and debug it
for another week and then chuck it over
the wall hope it worked so we went to
this clean layered reusable component
model and I came up with this list of
anti architecture things because one of
the things I found with working with
developers as if you say please build
this they were built that or something
that looks like that and they'll tack on
all these interesting things around the
edge that they thought of quite often
things that you didn't want that's why
you didn't ask for them so what you have
to do and this works quite well as I say
I don't care what you build but I wanted
to occupy this space and I'm going to
surround you by anti architecture right
I have a slide deck on this I did once
so these are the things I want you to
not do and if you can remember those
then and and even if you can't remember
why you can can say or Adrian said don't
do something don't make my instances
stateful I wonder why he said that
because I want to do a stateful one
because I used to do that and then maybe
they'll come and talk to you about why
and after a while this gets you know
everyone gets it in their heads and it
becomes sort of culturally just part of
the pattern but when we're trying to
establish this pattern that was quite
controversial that we were going to do
stateless instances and we won't go to a
cookie base routing it was sort of
controversial that we weren't good that
we were going to do no sequel and
putting session data in memcache and
things like that so it was this was
something that took a while and that
works out to be
quite a useful approach I want to pick a
instrumented code I mean most people
write things in their code if it was
just printf everywhere or you know
whatever the equipment and whatever
language you're doing you're printing
out stuff but we had all this code that
lever arm is instrumenting but they were
instrumenting it their own code as they
wrote it what we did for the cloud was
we moved from having the unit of
integration being a jar file where
everyone built their own jar file and
then the QA people tried to integrate
them all - everyone builds a service a
rest service so you had to design your
own rest interface but everyone was
using the same base servlet and we
instrumented that base servlet with a
annotate able object and you just
annotated your thing and it was the same
object spitting out of this thing
through like log4j streams off into the
into our backend it all turns up in hive
or wherever where if you want to analyze
it with the same names on the same
columns and you know you end up with a
very stable predefined way of producing
things and that means that you even if
you write add no instrumentation at all
to your system it's already instrumented
because all of the the libraries and the
things you're build up all of the
building blocks you're using are all pre
instrumented and that that makes it the
instrumentation patterns make it much
easier to build tooling that can then
use those patterns to give you nice
visualizations of what's going on and if
somebody ignores the standard patterns
then they get annoyed that they don't
have tooling then eventually that kind
of it sticks and carrots right this is
the carrot approach to getting people to
follow patterns you build tooling for
the pattern and then people use the
pattern because they want to use the
tooling okay so let's talk about this
chaos monkey we did a tech blog post
because we've open sourced this recently
computers died data center computers
died here if you're running low-cost
commodity stuff it dies more often but
even the expensive stuff dies so the
problem is that most developers like to
think of the Machine they're coding to
as being a perfect machine that works
because that's your normal experience
and it's annoying when they die so you
don't want such as keep unplugging your
laptop or pulling the battery out in the
middle of coding just to see what
happens right
but in order to test the error handling
code that you've built you have to
inject failure and what we're doing here
with the cows monkey is in a distributed
fine-grain SOA killing individual
instances is a way of injecting that
kind of failure mode so between 9 a.m.
and 3 p.m. every weekday we randomly
pick a machine in production and we kill
it of it just randomly picks stuff and
then in test it does the same thing but
at a higher rate because there are fewer
machines and now you can opt out if
you're in the middle of debugging
something and something's in a certain
state sometimes you forget which is
annoyed bit but you opt that one out so
it doesn't get killed underneath you and
we made this a default on and it was one
of the first services we set up so as
people started to develop in the cloud
there was always a chaos monkey there
right you can't put it in as the last
thing you do before you go live because
it will discover all of the bad things
that you've been sneaking in there and
there's a nice little chaos monkey logo
that somebody Amazon made some nice
t-shirts with cows monkeys on to maybe
may wear that tomorrow alright so this
sort of chaos monkey idea spawned a
whole lot of what we call the simian
army of monkeys so we have many of them
now and we're open sourcing more of them
soon ok so if we make developers
responsible for failures then and wake
them up at 3 a.m. when they write broken
code that breaks at 3 a.m. then they
learn to write code that doesn't break
and you know it's possible I mean if you
don't learn to then you probably just
leave or get pushed to leave but
generally speaking we you end up with a
team of developers who can go really
fast because they're not having to deal
with other people getting in their way
but they're responsible for what they're
doing
who's incident reduce to find gaps to
fix make sure there's no blame in the
room nobody gets upset in an incident
review it's all about ok what do we how
can we fix what continue it's either you
give somebody the wrong context or the
information wasn't there there's always
a good reason if somebody's incompetent
two or three times in a row it's better
than you know we'll find a way to get
rid of them right but that doesn't
happen that off
so a couple of things that you figure
out one is that there's a natural
tendency when when you notice things
timing out is to extend your time out
it's actually better to shorten your
time out because it's much better to
turn a time out into an error
the sooner you turn that something
that's slow into an exception the system
just behaves better with propagating
errors through it than propagating
latency because what happens when
something gets too much latency in it is
you start tying up all your threads and
everything gums up and you start
affecting the ability for other services
that would otherwise have succeeded to
get into your system so be very careful
about that
and the other thing is to make all your
configuration options dynamic you don't
want to have to push code to tweak the
timeout on a service or turn off a
feature or something like that and we
have we built and we've open sourced a
hierarchical dynamic configuration
system called Arceus and what this gives
us is the ability to apply override
configuration at any level of
granularity I could do a global override
everywhere that we have this service
deployed I can do it to one instance in
one zone or I can do it all the
instances in Zone A or I can do it to
all the instances in Europe so it's got
a hierarchy and a series of overrides
and defaults and we do a it's actually
the backend we have four that is
actually a Cassandra cluster which runs
in all of these different zones so you
change this property and it propagates
through Cassandra and then every every
region uses it if you were if you've
been following Michael my guards release
release it book that's a really popular
book in this audience there's a circuit
breaker pattern in there again we did a
we have a guy I mentioned this book to
some of our developers and they read it
and went often implemented circuit
breakers and we're where the guy was
supposed to open source this but he
keeps getting busy so we're going to be
open sourcing our implementation
listening a java circuit breaker thing
which basically kicks off a thread runs
a future calls the it wraps up the
dependency library in this in this
future thread and if it if it fails
there's a fallback methods and it's got
all of the other stuff and then it's got
a monitoring thing that gives you one
second granularity
dates of the state of each circuit
breaker so you can see in real time
which circuit breakers are failing
this was the API front-ends team teams
approach to not getting woken up at 3:00
a.m. because one of their dependencies
are broken and of course the arrows
always turn off at the front so they
built this as a defense mechanism to
point the finger at now it's these guys
that are broken you can leave them
sleeping nicely whereas these guys that
built a bad library or a bad service get
the call because the guys who figure out
who to phone have this tool that points
the finger very accurately who actually
is causing the problem right now and I
won't walk through this whole thing
that's it's a very the guy that did this
Ben Christensen is a really cool
developer and he was there's a video
think of him he was at Q con Brazil
koukin Sao Paulo so there's probably a
video of him giving this talk up online
right now so we're going to build our
own platform out of all these components
and you know I got a nice van Halen
picture he's got a guitar collection
there all looks slightly similar but
they will get the same picture so it's a
pattern right pattern based approach to
building your own guitars the components
you need so we've got a build code we
have a continuous build system Jenkins
based so that turns code into AM is our
unit of provisioning as an ami we're not
using puppet or chef we use yum once and
that makes our ami and then we tell
Amazon please make 500 of them and
everything's pre-installed there's no
yum yum can't fail because I've already
done it right you need Amazon accounts
we split our accounts by test production
across those things so we have the same
account running in Europe Europe in the
US and wherever right we have a
production account that's global but
then we have a test account that's
global right so that's how it works you
could slice the other way but that seems
to work best for us we have a gateway
I'll show you what that means for
getting into the cloud if you want so
login that gives us some auditing and
controls there we have a service
registry configuration properties
service bunch of persistent services
could have monitoring alert forwarding
backups and archives and that's the
basic platform pieces with all those
pieces then you can just start deploying
code and building your own service on
top
so there's a talk by Brian Moyles and
Gareth Bale's on SlideShare which is it
from a Jenkins conference or something
this is what our bill pipeline looks
like we've got we build from perforce
we've been using perforce for many years
now we've started using github because
it's out there and we have an internal
git repository as well so the system
basically goes and pulls the data out of
github
puts it in the front into the build
pipeline chunks it through so if you
want to deploy new code to production
for one of our open source projects in
most cases it's not always crude but
most cases you put the code into you
test it locally but the production
version you actually put it into the
open source repository and then it flows
back down through and been reusing okay
so the other standard steps within
Jenkins sink resolve check compile bill
and then we publish the components which
are jar files or whatever in artifactory
and then yum pulls those out and
basically has builds rpms and you know
we stick it into our are currently
CentOS but we're also moving to using
Ubuntu soon so the architecture that
this runs on is that there's a master in
our data center because we had to start
somewhere and we didn't have cloud you
get a little you go to bootstrap
yourself and that runs a bunch of slaves
that are also in our data center but we
kept running out of capacity and this
guy said we're cloud people we can
figure this out so these Amazon VPC to
create dynamic built slaves in the cloud
that are an extension of our data center
based built system right so whenever we
do a complete rebuild and we need you
know 1500 jobs appear it'll just spin up
an arbitrary number of cloud based
machines and if one gets idle for half
an hour it just quits and goes away that
kills itself right so that's basically
the model the software for doing that we
call it Dyna slave and that's on our
list of things we're going to open
source fairly soon later this year
Nader's they do this quarter now so
there's a bunch of things we're doing
we've been using Ivy for a dependency
mapping which sort of poke at maven a
little bit and then we've ended up sort
of migrating everything to Gradle which
is sort of the best of both
and that that's our current approach and
we use a lot of groovy Grails kind of
stuff in house anyway so we've just
ended up using groovy as oh and you know
Java shop you end up using groovy is
your kind of fallback language for a
bunch of things so this again will be
part of the open sourcing I think so the
bakery basically creates these base ami
so well the first step in the bakery is
that about once a month we create a new
base ami that contains all the latest
patches and stuff that we care about and
built-in monitoring tools and stuff and
then everything that you then build is
installed on top of that base ami so
like it's got Tomcat and Apache and all
that stuff so on top of that the build
process takes the base ami and just
drops in your drops in your war file or
whatever else you wanted mounts it
mounts an EBS snapshot installs and
bakes it and it bakes into tests so all
of our build system produces code that
goes am is that go to test once you've
figured out that that's working code and
you like it there is a rebake operation
that what that applies script level that
modifications and changes some
environment variables and takes that ami
and makes a production version of that
ami and then you will to scale that into
production right so the build system
never generally never goes directly to
pod it or product always goes through
tests and there's a bakery in each
region because ami is a the s3 stuff is
regional okay so the accounts we set up
we're isolating concerns with each
account so we've got a test account for
development and tests we have a tagging
operate mechanism so that an individual
developer can say well I want this these
two or three services to be my special
version that I'm working on but the rest
of these 300 different services I'll
just use the generic ones right so that
that gives us a way to flow your coat
your traffic through a a purpose-built
stack because we've got a few hundred
developers sharing this environment and
then in production it's Auto scale
groups only there are no isolated
instances in fact the monkeys in prod if
they see an isolated instance that is
not part of an auto scale group they
will terminate it
that teaches you not to do that fairly
quickly they won't they don't wait or
argue they just say no they're the
architectural e-everything must be in an
order scale group and if you're not you
shouldn't be here
so that's the implementing policy in
code you don't to get it out you can
shout at the monkey but he doesn't care
let me see what else have we got we have
an audit a separate account for audit so
you want to if you're going to be
audited for Sox or PCI you want to keep
that that concern as small as possible
so we've got back 10,000 machines in our
regular account this one's got maybe a
hundred less than a hundred machines and
there's a few developers that have the
special LDAP Group and the special
privileges and they've and they can get
into those machines and then we've had
the people have been had like a positive
background check that can actually get
into the vault where they keep the
credit cards and all that kind of stuff
so you know you've got to have more and
more access controls and more auditing
to get access into these systems but you
know 99% of our code and developers
don't need to touch that and they don't
deploy to that system so we can leave it
very open and free so for them to work
what do whatever they want and then we
have a separate account for disaster
recovery and archives that is a read
write but no delete and it uses s3 in
the overwrite mode so you can't delete
something by overwriting it it just
stacks the versions right and then it
times everything out after a few months
and that's where we keep our archives on
the other you know we put them in a
different archive into a different
region and it's a different account with
different credentials so you can't sort
of crack the account and then get into
this one and we encrypt the data we put
there too just because we're paranoid I
guess reservations there's some
interesting thing so people are still
trying to figure out with Amazon but you
can combine all your accounts into one
bill and you can pull capacity to get
bigger volume discounts and reservations
there's a couple of cool things you can
do but just by using reservations that
you get a 71 percent discount once you
get a few million dollars worth of
volume there's additional discounts on
top of that so if you're putting a
substantial amount of traffic into
Amazon it's like any Enterprise vendor
at this point you're into increasingly
large discounts so don't look at the
list published list price and assume
that that's the the the price up volume
you
yet you can get it down you also get
priority with reservations so if I've
got an unused reservation I say please
give me that machine you're guaranteed
to get that machine precision to you if
you're doing the on-demand thing and
they happen to be out of stock then you
can say you can get it back a little
error sorry we don't have any of them
left kind of thing but if you reserve
that you get it and then this very cool
thing which may if you don't get this
and you and you care about it talk to me
later unused reservations are shared
among accounts so we have unused
reservations in production because we're
that's where we want our extra capacity
to be and we want to always guarantee
them but we have all these machines in
test though it's just fluctuating
capacity and when at the end of the
month the unused reservations in
production are applied to the machines
we have in tests so we actually end up
using you know getting a big discount on
our test machines right but I'm getting
the benefit of the reservations in
production
it's a neat trick and anyway it's don't
worry about it if you didn't get what I
was talking about so we have a bunch of
developers and they want to get in
everyone's been to Kings Cross and
London you and the anyone's into all the
Harry Harry Potter stuff that's actually
they have this sign up on the thickest
station between state platform 9 and
platform 10 there is actually a a wall
with platform nine and a half or
three-quarters or whatever it's written
on it so it seemed like a good gateway
thing so we have all these different
product my web server service and I have
data access layer service and I never
Cassandra service and all of the
security groups are set up for them to
be able to talk to each other and I want
to SSH in and mess with one of these or
what jail run vmstat or something like
that if I log into the front-end I can
then the SSH I can't get to a different
service I can only be in one instance I
can only get to my instances through the
gateway which means I can audit log
every party that ever logs in everyone
that ever copies data back and forth so
we have a history and we use this in
production but for the auditable ordered
account this is quite important we can
track because we can control
and we can see what's going on we don't
stop you doing it but we we log
everything so we know who logged into
which service and what they did and
whether they copied anything back and
forth right and that's very useful if
something weird happens and you find the
systems in a weird state you can find
who did it and ask them why they did it
and you know it's not usually for of
like malicious reasons this usually
somebody didn't realize that they were
doing something that would break it but
the key thing here is that the security
groups on the individual machines only
trust the gateway they don't trust each
other and that's we always set it up
that way everyone's used to it and it's
a neat way of configuring everything so
let's add some code oh my god am I out
of time yeah okay let's see I go a bit
quicker this is our github account which
we made using some Netflix things but
these are the different apps we have
let's go through we have well we have
all the different phases so image baking
aSG's launching register and
configuration all the runtime stuff just
as god
I got a summary thing somewhere there we
go so this is the open source projects
we have I can't even read it on my
screen here make this bigger all right
so we have priam is Cassandra as a
service I can deploy a Cassandra cluster
trivially it automatically configures
everything Aslan axe is a client library
all the ones that are in red are
currently on github it's an improvement
if anyone's been doing Cassandra they
may probably using Hector which is the
standard Java client we'd there were a
bunch of things that weren't working
well for us that we wrote our own cache
jmeter is a test suite that modified
jmeter for for putting stress testing
and then we have zookeeper and zookeeper
patterns a discovery service like a
service registry a dynamic property
service that I mentioned we have auto
scaling scripts which basically
published business metric
to Amazon so you can use as an auto
scaling metric log4j streaming Asgard is
this massive console big groovy Grails
app then we have the cows monkey so
those are the ones that we've got out
already we've had blog posts about Evie
cache and circuit breaker but those are
going to come out as code as well pretty
soon we have an explorer's framework
which is all of the consoles and
dashboards you need to view everything
there's a Cassandra Explorer that lets
you see all the clusters and see the
schemers and rummage around and see
what's going on
so it's anyone here used juice Google
juice for dependency injection
so that's Governator coming out soon
that's a we've got there's a big mess of
trying to figure out how you initialize
everything in the right order and we're
just reengineering everything right now
to be based on Governator which is a
juiced version of juice sort of Odin
with Asgard an Odin we ran out of greek
god so we started doing Norse gods or
one of the groups decided to there's a
workflow orchestration piece entry
points is a interesting service with
it's a log of the state of the system of
everything in the cloud with a full
history so I can say exactly what state
was the site was everything in at this
point in time and there's a bunch of the
monkeys tend to rummage around in that
it's all database service latency air
injection a more sophisticated middle
tier load balancer janitor monkey goes
and cleans up Amazon resources that you
haven't used for a while the whole
bakery invite dinah slaves I've
mentioned so that's basically we've got
what we're working on next I mean we've
been working through this year on
resiliency more automation hardening
lower latency open sourcing everything
for everybody getting more ipv6 work
through everything and we're going to be
our goal is to get as much of this out
as possible by the end of November which
is where amazon has a huge conference in
las vegas called AWS reinvent and we've
got I think more than 10 Netflix people
presenting including our CEO
doing a talk on the future of cloud and
we saw an internal version of that and
half the in you more about cloud than
half the engineers in the audience it
was kind of interesting so that's that's
it basically
so then</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>