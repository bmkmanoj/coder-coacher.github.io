<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Knee Deep in Microservices • Ádám Sándor | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Knee Deep in Microservices • Ádám Sándor - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Knee Deep in Microservices • Ádám Sándor</b></h2><h5 class="post__date">2017-11-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2Rn2VxG2tBU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you so welcome to my talk
knee-deep in microservices this talk is
a spinosad going to be about micro
services cloud native and hell demons
from doom
I'm a senior engineer and consultant at
Amsterdam based consultancy container
solutions and I'm a big fan of the
classic FPS video game Doom and while
playing Doom I made some realizations
how similar it is to running micro
services in production or building them
so doom is a game about scientists on
Mars opening a portal to Hell to channel
clean energy from health to earth
well as you can expect that did not work
out too well and demons over ran the
facility with only the green Armored do
marine standing between them and
humanity micro services are a very
popular architectural style today
because big companies like Netflix and
Google basically had to architect their
applications this way because of their
scale and people in smaller companies
are looking at at this this
architectural style and wondering
whether we should also use it or going
heads in and not really caring about the
risks or realizing them so not everybody
is deploying a large number of
components to buy a large number of
teams and having millions and millions
or billions of requests per second on
their servers but is it really is it too
big a risk or do the benefits outweigh
the risks today what are the benefits of
micro services and what are the demons
you might unleash upon yourself if you
go
that way first of all micro-services
will be we are a great architecture to
utilize elastic cloud resources so you
will be able to scale your application
more efficiently if you have several
teams delivering your software or even
sometimes it can work with one team your
development velocity can increase
because you are delivering components
independently thus after an initial
increase or decrease in velocity you can
you can later when your applications
complexity increases you can you can
deploy faster resilience of the
application to failures increases
because it's you can run it over several
geographical locations several data
centers and today's public cloud
providers make that easy for you but you
do need an architecture that will
support that finally cost optimization
if you have components that have very
different performance and resource usage
characteristics with a micro service
architecture you will be able to scale
them independently and thus utilize
cloud resources better so when I'm
talking I want to clarify a bit what I
mean by micro services it's a very vague
term used in lots of different ways over
the years how what I mean by them and
during this talk is separate code
repositories for separate modules or
separate components of your application
that are packaged as independent
artifacts that then are deployed in a
way that those artifacts are not for
example lumped together into a single
virtual machine but I under but are
managed by the production platform as
independent components all of this this
diagram could be modified in lots of
different ways for practical reasons so
you might want to have a mono repo for
your code but somehow build different
artifacts or what
of the time happens with or many times
happens with monolithic applications the
code might even be separate but it's
lumped together into one big artifact or
everything is separate but then it's
installed on a VM and run everything is
run together so these are all valid
options it's there is not a right way to
do micro services I'm not trying to
imply that especially with legacy
applications evolutionary reasons how
things evolve with an application those
are all valid approaches but now I'm I
want to show the benefits of a pure
micro service architecture especially in
relation with cloud native technologies
so what are the demons that you will be
unleashing when going micro services oh
yeah as a slide about being prepared so
even though these scientists failed
horribly they did actually have a status
message saying demonic invasion in
progress
that's very cool so your first step to
being prepared is sitting on this talk
so first of all you will need to need to
package your application you need a
technology that actually allows you to
package applications in a uniform way in
a way that can be delivered to
production unchanged so you don't
produce something that then is packaged
by the ops team into something else and
then is deployed in a different way onto
a production system you also need a
technology that allows you to deploy
every artifact you have in a uniform way
because otherwise the complexity will
make your production environment into a
very heterogeneous environment where all
kinds of different artifacts are
deployed differently so in the end there
will be a pushback from the operations
team to just keep it simple and let's
just make everything Java for example
local development will be a problem with
micro services you when you run one your
ID and everything is
made to run an application maybe - you
make your configurations and everything
if you have 10 15 20 of them then then
it becomes much much harder to run your
application on your local development
environment and that people sometimes
don't even realize but slows down the
whole team velocity by a lot because you
have a hard time testing features or
your changes in isolation observability
also just as local development was
always something to solve but with micro
services it becomes harder monitoring
also becomes much more crucial and
different and more important than than
with the monolithic architecture
self-healing is something you will need
up to at least some degree because if
you're running hundreds of small
services in production you cannot have
the sysadmin guy go and restart
something when and it fails and finally
your biggest problem will probably be
organizational silos at least for lots
of companies actually funnily for some
companies it's not a problem because
they just don't have an ops team for
example and well they have other
problems then but at least the the
infighting between the ops and
development teams does not happen
so basically operations want stability
ones doesn't want things to change
because that's how they manage to keep
things running developers want to push
new features suddenly developers decide
that instead of three artifacts they are
going to break the application down into
20 or 50 and all that stuff that has
previously been contained inside the
application suddenly becomes visible for
operations and starts communicating over
the network so it becomes becomes way
more complex for them for this you need
good cooperations between the operations
and the development teams to handle that
newborn complexity but well for all
these scary demons you have cloud native
weapons to battle them
cloud native technologies are have
become quite popular in the last two
years they bring completely new ways to
handle these challenges of micro
services I'll show how they don't solve
all the problems so I don't not trying
to imply here that you just use
cognitive technologies and micro
services just become super easy and
everybody should do it they don't help
with your overhead of developing
independent components so then the code
side there is not that much that has
happened in the in the last few years at
least I didn't see it but on the right
side and the middle the artifacts we
have now a good packaging format and we
have a new way of deploying stuff docker
is the technology that lets you package
your applications slice up your virtual
machines and deliver to production a
range of different technologies in a
uniform way that the operations team can
understand can handle can run and the
development team can actually also
utilize in their development process
docker compose can help with local
development because docker containers
the whole technology around docker is a
new kind of better way of virtualizing
or at least a lighter way of
virtualizing things you can actually
have that virtualization on your on your
laptop using either docker compose or
other tools like mini cube or
minimizer's which actually even run a
more realistic cluster on your machine
and thus you have a nice and easy way to
actually run several components together
on your laptop until you were the
resources on your laptop allow it so
that's also great and then the big guns
this is the what you see up there is the
BFG the big gun from doom it's
the most powerful one as the
orchestrators are the most powerful
tools we have to manage your production
environment
container orchestrators take a set of
virtual machines and utilize them to run
your containers over them this doesn't
sound all that revolutionary it's a it's
a nice thing because once you package
containers you will want to run them
somewhere and this Orchestrator takes
care of that
but what why this has enabled a very
different way of managing your
applications is because the orchestrator
is looking at your application at a much
higher level of abstraction than
previous tools so when you were
installing VMs using puppet your your
level of abstraction is pretty much
files in the operating system and
services and stuff and and not
applications with containers this level
of abstraction moves higher and I
container Orchestrator actually
understands that your three front end
containers that you see they're
distributed over two different nodes
belong to the same application component
and thus pro can provide load balancing
over them or scaling them up and down
our service discovery with even dns
server that you can just address front
end and your traffic gets routed to the
to the right container so with these
things become quite different which can
be seen how monitoring solutions so
monitoring solutions are a good example
of how how things evolved these days
there is a short list of some of the
cloud native monitoring solutions that
have that can now monitor containers in
production that are running on a
container orchestration system it's very
interesting to see that zero to a go
monitoring solutions were really much
trying to catch up to monitoring
containers because containers have a
much shorter life span than than VMS and
also you cannot just install agents and
stuff into the container because
container is designed to
just run one single application process
so the way these monitoring solutions
were working before was not tenable for
the container world but what was
fortunate for these monitoring solutions
and why they actually managed to even
improve compared to the previous state
was that the container Orchestrator the
master actually has a global view of
everything that's happening in the
cluster it has actually all the
container orchestrators have an API that
you can query and get this global State
and as I said it's a closed to
application level abstraction so that
means within my example here I have the
cost care monitoring tool showing me CPU
time of my services and those services
up there I know this is a laser okay so
these services up here in the olden days
I would have had to configure what a
service means I would have to tell some
how the monitoring solution now I no
longer have to do that the monitoring
solution could just ask the kubernetes
api grab all the services ask what
containers are behind those services
monitor those containers and aggregate
the results into into this graph here or
it can monitor how many containers
actually make up the service so you can
see here is a spike in the CPU usage and
you can see a spike in the number of
containers so actually the service is
scaling up unfortunately here the labels
are a bit messed up so this is this
shows very nicely how in the in the
cloud native world the concept of micro
services is becoming easier to manage in
production next is continuous
integration it's something the continues
all these these tools have caught up
with containers in a way that they can
they can build containers they can talk
to orchestrators usually not in a very
nice way there is still work to do in
this
but you can use your tools that you know
and like to build and deploy containers
so what's gonna be the pass your company
will have to take when you decide for a
micro services cloud native adoption you
will need to implement independent
delivery of components to leverage your
micro service architecture you will need
to be able to manage these cloud native
environments you will have to have a bit
updated CI CD chain in case you already
had a relatively good one you will have
to be able to manage production
workloads efficiently do training of
your people in these technologies and
you have to do good devops monitoring
yes you have to do good DevOps and do
the cloud native monitoring so I will
use this example from one of our
customers that we work together with
them to put their application in
production on a kubernetes cluster in
the Google container engine it's a
relatively simple application it's not a
example of a thousand micro services but
it illustrates my point on why it's not
just cloud native technology is helping
Micro sir helping you to run micro
services in production but also the
micro services architecture is very not
sure is it absolutely necessary but at
least it's very well fitting with what
you can do in the cloud using container
orchestrators so what happened here was
his company decided to migrate away from
their legacy application which was a
classic dotnet web application with the
database behind it
to a whole new stack actually in source
it from from a outsourcing company and
build it build everything on top of
nodejs and elastic search and
what they did was build an exporter from
the old database to push the data that
was still updated to RabbitMQ and import
it through the new application into the
new database this was necessary because
the migration happened step by step so
the new application was slowly catching
up with features to the old application
and until that actually was finished
they had to build a proxy that actually
proxies request for features that are
not yet finished to the to the new
application and they build this by
creating an admin component a front-end
component and lumping three back-end
components together into a kind of a
monolith so the problem we encountered
when we were working together with them
and putting this application into
production was for example the importer
component needed to be scaled together
with the back-end so what happened when
load hit did the application that the
importer scaled up and started importing
messages from the old database or the
data from the old database actually
faster while the application was getting
extra load from the front-end that was a
very bad situation there the proxy
component while actually that one could
be scaled with the backend and even it
kind of makes sense because they are
under load at the same time couldn't be
monitored separately from the back-end
so when we had a high CPU usage or
overflowing memory there was no way to
tell whether there is the importer the
proxy or the back-end that is at fault
and with this cloud native as I saw the
monitoring solutions and with the
orchestrators actually it's very you get
a very deep view into your system if you
actually split it up into separate
containers running separate processes so
your next step will be independent
delivery of components if you want to
really leverage that you have a micro
service architecture don't deploy them
all of them together it might be
good intermediate step but try to move
towards separate source called rapport
separate artifacts and don't share code
between it between them the common
library that contains 80% of the code is
a very bad pattern because then you just
cannot deploy things independently even
though it might look like that until you
actually change something in the common
library that will break something in a
completely different service and own
application components should own their
own database again to be able to be
deployed independently how we did this
in in the with this application we
packaged in kubernetes every service as
a kubernetes deployment that can be
scaled independently they are grouped
together into a service and auto scale
using a horizontal pod autoscaler so
this is a super simple diagram
everything is very uniform everything is
a container everything is a deployment
everything is a service that load
balances between those containers and
everything has an HPA and horizontal pod
autoscaler is called in kubernetes it's
a component or a piece of configuration
that actually scales up the application
up and down and their database is a
managed database at elastic so that's it
also very important tool you can use in
a cloud architecture just let somebody
else handle a database the queue or your
kubernetes cluster especially for
smaller companies that's a very good
idea to do managing environments you
have to have environments that are cheap
disposable and versioned to have a very
stable predictable delivery in
kubernetes I'm keep using that as an
example you have nice spaces for this in
namespaces you can isolate your
containers deploy the same application
several times while it's in every
namespace it's it feels like it's the
only thing on the cluster and you can
create these namespaces our practice
they practically don't cost anything so
the creation of a new environment took
us as long as the slowest container to
start up which means pretty much seconds
production workloads you will need to do
auto scaling I mentioned the horizontal
potahto scalars before other container
orchestration systems have different
ways to do this
you will have to be fine-tuning resource
limits and reservations on your cluster
I think this is the most at least for me
when working with with kubernetes
cluster this was the most surprising
thing that needed to be that was hard to
do was actually deciding how much CPU
should a service have reserved and how
much it can use at maximum with lots of
different environments running on the
same cluster and sharing resources you
have to be very careful about this if
you give too much to a container to high
reservation and you will be running lots
of virtual machines to accommodate all
those containers with all those that are
hungry for CPU and maybe not doing
anything because when you reserve a CPU
to a container its reserved and the
container might not be doing anything
and it will still be occupying the CPU
of course auto scaling mitigates that
but it's a it's an effort to to get this
right and if you give it to a few too
little reservation than other containers
can can store it continuous delivery
it's this part doesn't differ that much
except from the from how you were you
might have been doing it before it just
becomes even more important you need to
make sure that whatever bill tool you're
using supports docker most of the good
ones support it now how to push
artifacts to environments that's
something you'll need to figure out
people have some new container
orchestrate or you're a new way of
deployment is talking to the API of the
container Orchestrator so you will have
to
have a way to do that actually these
days the CICE tools mostly lets you just
script it pretty much which is not very
hard to do it would be nicer if there
would be deeper integrations with the
video orchestrators and an important
point about DevOps the ops team creates
the tooling for the continuous delivery
they don't take the artifacts from the
development team and put them into
production because if that's happening
then there is no scalability in the
number of services because there is just
no way the ops team will understand
every new service that needs to be
deployed your operations team will have
to understand container orchestrators
debug them tie them together with
monitoring and log regression solutions
while your development team needs to
also be confident users of the platform
it's very important we found that when
developers just have a don't have a
sufficient deep knowledge of how things
work and they don't need to understand
how kubernetes internals work but they
need to understand the concepts and how
to be a proficient user of it then they
if that doesn't happen they don't like
to touch it and the whole DevOps
experience suffers because they will be
pushing more and more onto the ops team
and the monitoring again I already
showed it but now from a DevOps
perspective again the ops team delivers
the tools you will might need
inter-service debugging so you or you
will need inter-service debugging so a
log aggregation solution will absolutely
be necessary put in some correlation IDs
so you can actually find - what happened
to a request over different services
that we're all involved in actually in
servicing it that will be probably work
we see usually that companies don't have
that good monitoring and survive with it
pretty well with the monolithic
architecture my guess is is because of
stack traces mostly because they carry a
lot of information to find out where how
a request went inside the application
but you lose that when you
have services that communicate over a
network so you'll have to actually
aggregate them logging from several
services and then make sure they don't
look too much because then it's again
hard to find out what's the important
stuff
and finally DevOps DevOps will be an
absolute necessity and you will you
might need an ops team to develop the
tooling or you go to some completely
managed solutions which are out there
like we did with this customer of ours -
they went to Google container engine
which is a managed kubernetes use the
managed monitoring solutions in Google
call stack driver use the manage
database and still they ended up with
one ops guy having to service things so
make sure and if you already have an ops
team in place make sure they are not
deploying applications but they are
developing the tooling that the
developers then use to deploy the
applications and the environments should
also be the developer's responsibility
the operations team shouldn't be like
keeping up the test environment up and
running somehow yeah so the ideal view
of of this system would be this all
components separated and and running in
the orchestrator so they do marine
travelled to hell to defeat the demons
there and succeeded only to see the
demons still getting to earth where he
had to go and defeat them again to
finally save humanity it's gonna be a
rough ride but don't forget your cloud
native weapons and don't be afraid of
the demons thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>