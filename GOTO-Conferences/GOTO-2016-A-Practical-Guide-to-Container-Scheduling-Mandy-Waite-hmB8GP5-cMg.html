<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • A Practical Guide to Container Scheduling • Mandy Waite | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • A Practical Guide to Container Scheduling • Mandy Waite - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • A Practical Guide to Container Scheduling • Mandy Waite</b></h2><h5 class="post__date">2016-09-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hmB8GP5-cMg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you the schedule 820 past never
mind let me not get more time or less
time right splits away okay plug-in make
sure on the right slide are you doing
today
you could I bought the Weber with me I
nobody bring marine everywhere I go it
rains but today last two days it's been
amazing for those these times I see I'm
a little bit worried at spring we kind
of miss spring we've gone straight to
summer and I like spring I like spring a
lot oh there we go alright make sure my
clicker works dump my phone make sure my
phone doesn't go off nobody finds me
anymore
I don't do phone calls anymore I keep
telling people I don't do phone calls
and they keep finding movement right
okay
hello everyone hey doing contain the
schedule in a practical guide so I'm
gonna be talking mostly about
communities I'm not gonna be talking
about docker swarm and things like that
I think I should do actually
you
I'm kind of worried about what the
timing is now because I have no idea
finish so I'm each run freezes very
quickly this is me you can get me on at
Tech girl on Twitter and how many of you
using Google+ tell everyone last night
no I so easy it's still really good it's
got more than hundred forty characters
which is pretty cool so you can also
find my Twitter handle and every single
size right at the bottom there and along
with the go to Stockholm and support the
event the event is fantastic it's really
amazing they're doing these events
everywhere in Europe and let's talk
about Cuba it is hit straight in there I
used to talk about Borg our cluster
scheduler tour thing that we have in
Google and compare Balkin kubernetes
together I'm not going to do that today
there are talks have been doing that
online and there's also a guy called
John Wilkes he talks about Borg he's one
of the guys who builds our
infrastructure and Borg is Athena
ultimately takes all of the work that
our software engineers do and runs it on
our clusters our large data centers and
cool places is kind of an attempt to
make that kind of power available to
everybody not just a Google scale but a
very small scale ten machines five
machines you don't need to have Google
Scout to do this coop nation ultimately
is a strange word how many of you can
say kubernetes if you need lessons I'll
be outside I will be a teacher how to
pronounce kubernetes coburn eight is
it's a French Japanese word I'm using
Japanese as well but it's really Greek
you means helmsman not going to get into
that but ultimately it's going to run
and manage containers and that's good
because we ultimately probably have a
lot of containers right now because
you've probably all discovered
containers how many of you haven't
discovered containers you know your hand
up that's good see you end up with lots
of containers and you need something
ultimately to manage them for you and
that's what kubernetes does and I
mentioned Bolg earlier the thing that we
have internally and kubernetes is
ultimately
informed and inspired by a Borg that
thing that we have internally and one of
the experience that we have building
that thing the last twelve years or so
it does support multiple clouds
it runs on AWS just as well
it's on Google cloud platform so there's
no lock-in here there's nothing to do
with Google this is a community project
which is why I don't have the Google
logo anywhere maple on the first side
I'm not sure
it supports bare metal environments as
well also you can run this on Prem you
can run your native yes you can my own
Google Cloud you can run it on digital
ocean wherever you want to
it also supports multiple container
runtimes for some value of multiple we
have a cup order moment we have rocket
and docker and now we have an open
container format effectively we probably
have more container formats that we can
run ultimately on accumulators under
cent open source and is written in go
doe is extremely popular for this type
of application and ultimately to go
always for managed applications and not
have to worry about machines even if
your operations you shouldn't have to
worry about machines too much most
people nowadays will ask a question
about whether you're developers or
operations people say yeah a little bit
about so everything at Google runs in
containers all the things that you use
all the services you interact with on
Google running containers effectively
the containers was a byproduct of what
we needed to do and ultimately we work
with Linux to create C groups and
namespaces which is the thing powers
ultimately docker and other container
formats we run our mapreduces and our
batch jobs in containers and even our
Google cloud platform virtual machines
effectively run in containers so you're
on a container on Google cloud platform
virtual machine you're on the container
and a VM on a container we launch over 2
billion containers per week but let's
not think too much about what a
container is let's not worry about it so
much it's a process it's a process that
carriage all of these dependencies
around it and has a tall chain and has
an ecosystem but ultimately when you
spin up a container you spinning up a
process and most we can do more than one
process but ultimately you care about
one thing in a container so if you think
about 2 billion processes have been two
billion containers between two billion
processes and it's not such a big
stretch to get to do with it so yet so
you discover this ultimately we've done
all this work this is replaceable my
containers slides I use up an
explanation of what containers were 10
12 slides and I'll replace it
this one slide which the containers are
awesome so let's run lots of them and
that's a natural conclusion for most
people to reach when it comes to
developing applications today containers
are good we want to run them where do we
run them how do we run them how do we
manage them how do we do their updates
how would do we wire them together
and that's the kind of thing that
kubernetes ultimately gills with with
Google makes is you start with a cluster
the cluster could be a laptop so you
could run using the vagrant provider you
can spin kubernetes up on your laptop
using ultimately VirtualBox as your
virtual machine provider now I think
you're supposed to be my as well
probably I've never tried that you can
also use multi node high availability
clusters on Chrome or in the cloud
they could be hosted or self-managed we
have a managed version of kubernetes but
you could be managing yourself it could
be on-premise it could be on on the
cloud you'd be running on bare metal or
virtual machines but it's pretty much
every operating system even so it's
exactly always code you lose your word
right so we actually implemented it when
somebody's Raspberry Pi Kuster and so
fired no trust I'm running kubernetes
you probably don't to run that in
production now and ultimately if you
want to find out more about how to fit
it to your needs as a cluster there's a
matrix a cluster matrix there at the
bottom a link and you'll have the slides
at the end setting up a cluster is
fairly straightforward
I made some update this we don't use
this kind of bash script pull down a
bash scripts and money immediately after
pulling it down anymore nobody does that
anymore so that's a bit of a legacy so
you'll pull down the tar ball and tar it
and then ultimately run the installer
second your kubernetes provider to
something like GCE or AWS or vagrant or
core OS depending on what your run now
if we don't have an out-of-the-box way
of installing it on your platform
there's plenty of recipes and that goes
back to the matrix on the other page we
have or there are various distros red
hot red hat atomic core OS tectonic
moranto Maria Morales Morano and various
others and yes so this recipe is and you
can also use a hosted version such as
google container engine and this is what
kubernetes looks like ultimately we have
a bunch of machines nodes we used to
call them minions again point things
slaves and minions is a little bit not
so nice so now they're called nodes and
each node in the cluster runs this thing
called a couplet and the Kubler has the
credentials it needs to be able to
connect to the cluster and become a
member of the cluster and so when these
machines come up they find the master we
have a single master it they connect to
the master by the API and they become
part of the cluster and the scheduler
which is also part of the master the
slide probably is a little bit of an
update and that's wrong I'm not gonna
change that we use xcd for configuration
management but it's ralphed based not
packs or space so that's a mistake on
the slides that's kind of what Borg does
then we also have dashboards and we have
controllers we also have a container
registry as well so because we care
about running containers things like
docker or a rocket then we care about
where those images are located and this
container registry could be hub accom it
could be some registry you created for
yourself which is fairly easy to set
them up or it could be also a Google
cloud platform if you're using Google
container engine and ultimately what
happens is we push an image out to the
container registry we provide a
configuration push that out to the KH
master the scheduler comes along looks
at the state of the system what it is
currently looks at the state of what it
should be as according to our persistent
storage and it says okay there's
different let me fix that and so it will
make the actual state the same as the
desired state so we have this notion of
desired state and then the scheduler
whoops the lines go back the scheduler
will decide where that work will run
those resources will run and it will
talk to one of the machines
and the machine will pour over the
binary the image and then run that
container on that machine and the
schedulers work is what we're going to
be talking about today and somehow that
lines got stuck at the back there so
that's basically how kubernetes works
but let's go into somebody resources
that communities provides so the first
is the the Aksum of scheduling we don't
use containers directly to schedule work
on kubernetes we use a thing called a
pod and a pod can ultimately run
multiple containers it's a bit of a
convenience but if you think of it like
a logical host if you create a machine
and you run multiple process of
processes on that machine because they
need to work together they have some
kind of synergy and it makes sense to
have them on in a pod so effectively
what you're going to do is take that
machine which could be virtual or
logical and you're going to take it into
a format and convert it into a pod to
come run on the cluster and just like a
machine it also has its own IP address
so we can be running multiple these pods
on a node but each of them will have its
own IP address which is not NAT 'add so
we can talk to it directly where it's IP
and all of the containers running inside
the pod have a shared namespace so the
idea of the port namespace is the same
for multiple containers running inside
the pod and they can talk to talk to
each other by a ports or they can talk
to each other by our IP sees this they
have a shared eyepiece namespace IPC
namespace as well so containers running
in a pod look like and think that they
are running on a single machine so
effectively we're creating these
machines and we're scheduling them on
our cluster that's really calm really
powerful and we also have volumes the
notion of volumes this example shows two
containers one of them is very simple
nodejs application serving traffic
externally there's other mechanisms we
can use to expose that externally which
we'll see shortly and we also have a git
synchronizer and this job in the world
is to synchronize with the git repo so
whenever you push changes your git repo
to get synchronizer well see the changes
pull them down and store them on the
volume and we can now serve that
those updates to users I said kind of
push to deploy mechanism but again
they're just two examples of containers
this is what we call a sidecar pattern
where one where there's a synergy
between the containers there are also
other patterns as well often you may
only run one container in a pod which is
perfectly fine there's no overhead and
we said that pods can talk to each other
by our IP addresses directly and that's
the way it works we have an underlying
overlay network for Google cloud
platform that's our advanced routing for
AWS is route tables we can use calico or
flannel or weave or open V switch or
cloud provider or open contrail and
others there's also one from Cisco Cisco
contrive there's probably some of
missing there's anybody's representing
those companies I do apologize but yeah
so it's fairly easy to set up but again
if we have multiple pods on a node and
they're all exposing the same pole that
is fine they had their own port
namespace effectively so you if you have
multiple pods exposing port 8080 on the
same node that's fine
that will work these are fundamental
requirements we had to have this for
kubernetes to work we also had the
notion of labels labels is the single
grouping mechanism of kubernetes we can
label pretty much everything include
natives and then we can build tools to
read those labels a label is a key value
pair it doesn't have any semantic
meaning tsukuba natives but it probably
has semantic meaning to you like the raw
equals something the type equals
something the color equals something
something that has semantic meaning to
you that will allow you to group these
artifacts these resources within
kubernetes here we see them apply to
pods we have pods with multiple labels
resources can have multiple labels and
we're building dashboards and tools that
will use those labels and the way we use
labels is wiring called a selector and
so basically we just say to the API give
me everything or all of the pods with
this label and we provide what we just
label based on the selector and we have
services with include natives that also
use that mechanism to group pods and to
grow
other things as well the service is the
way we access to these pods that are run
in containers they expose a pall
ultimately there's no natin as we
mentioned but ultimately we have free
identical pods or running the same
nodejs application ng engine X
application but all identical then we
need to somehow route traffic across
them we need to communicate with it
anyway even if there's only one pod and
to do that we put a service in front of
it
so the service constructs effectively
groups a number of pods from one some
many and the service provides East a
stable virtual IP address and also a DNS
entry so clients can access that those
running pods by that virtual endpoint
that we provide in there and traffic
coming in from the clients will be low
balanced across multiple pods and it's
done randomly currently used to be
round-robin so that's the service this
can be used to discover services
internally so we could be wiring things
together internally or it also could
ultimately expose an endpoint externally
as well so we have the notion of a load
balanced endpoint and we have a cloud
provider such as Google cloud platform
where AWS we can actually expose the
service on an external endpoint to the
rest of the world so you have multiple
different types of services we also have
the notion of replication controllers
but they're now being superseded pretty
much by replica sets they are the same
we're just trying to get away from what
they're mostly the same we're trying to
get away from using controllers in the
names of these things there's everything
started to be a controller and it kind
of got confusing so now we've decided to
keep views everybody by changing their
name so now they're called replica sets
but they are fundamentally the same as
replicas replication controllers so if
you're familiar with them then you won't
see much change they do have a new
generalized label semantics so rather
than just having equality for labels you
can have inequality and you can also do
sets so like a value in some set of
values so we can do labels a little bit
differently with replica sets
but fundamentally they're the same and a
replica set ultimately is responsible
for making sure we have our desired
States for a given pod deployment we
provide a pod template which contains
the containers ports labels and suchlike
for creating pods we say how many we
want in this case we want to and well
one in two in the left hand one on one
and right hand one and we pass that in
as a configuration tsukuba natives and
say make it so effectively so we say we
want an X number and we want this
template the replication controller will
wake up and look to see if that there
are any pods that match those labels
City it has in this case it cares about
labeled version equals v1 and so it will
look to see these exist if they don't it
will make an API call and ask the API to
create them for it and it will
continuously monitor to make sure
there's always the right number running
okay so if you've asked the two they'll
always be two running and it's fairly
straightforward we can have multiple
replicas sets many many different
replica sets managing different pods
they may even be just different versions
of your pod and we up some scenarios
coming up we will show you deployments
are very new we have you ability to a
role in update to those pods so once we
change what dated our image container
image we may want to roll that out as an
update a rolling updates for our service
our production service and the way we
used to do that was a thing called a
rolling update and it would basically
create a new replication controller in
those days which would create one new
pod take an old pot away create another
new pods like an old pod away and
eventually it gets to a state where all
of the pods are in replace and it can
remove the old replication controller
but that was all done on the client side
so you could do control C at any point
and you would be left in an unknown
state you could finish it off at some
point later but it was fairly
unpredictable and liable to problems so
the deployment object is something new
introducing 1.1 which is now in beta in
1.2 but it's also the default as we
trust it so much and this effectively
does our role in updates for us
on the server side so you basically
create the deployment of demo in shortly
we create the deployment we push it out
it will create the replica sets for us
and if we want to update we can just
edit that configuration using the
command line
very very simple it's very powerful and
then it will roll out the updates to the
pods and you don't have to worry about
it anymore it's not happening on the
client side
so that's deployments scaling is fairly
straightforward we can just change the
number we can instruct a replication
controller or a replication replica set
to have more pots in this case we say
one you tattoo it will create another
one or it will ask for another to be
created if we say we want for it we'll
go ahead and instruct the API to create
another two and your code thing
continuously check to make sure it is
always four so scaling is very very
straightforward and at a bottom here we
have a service which is cares about
labels with type equals Fe and all of
the traffic coming in to this service
will be routed across low blanks across
all of those pods okay rolling updates
are done like this now so we have this
deployment object we have a replication
controller again it's managing two pods
we have a service down here which I'm
naming so these two pods here being
managed by this controller and this
deployment is managing this whole thing
and when we update the deployment maybe
by changing the image what will happen
is that we will create a new replica set
and we would then create a more tell it
to scale to one scale the old one down
and then scaled a new one up to the new
number of pods it's ultimately we've
done a rolling update now so we've
updated all of our pods to a new image
and we're left with this thing despair
replica set here just doesn't go away
because basically we create these
replica sets based on a hash of the
template that we use for creating pods
so we hashed that template gives us a
number and we add that to the name with
a replica set in this case all we've
done is scale it down to zero and if we
ever reuse that template the same
template we can reuse that replica set
but at the moment it just sits there
with zero pods
canary is very simple so sometimes you
want to roll out an update to a partial
update just to test it to do a/b tests
in effectively in this case we have two
replication controllers we have one
reversion equals v1 pods and one with
one version equals v2 pod and the
service is aggregating all three of
those and but no balancing traffic
across all three so 33% of our traffic
will go to this pod and 66% will go to
these two pods and so we're effectively
Canarian we may have one point out of 10
to do an even smaller percentage but
ultimately we can see the results come
back for that new version if we like it
we can then roll out a deployment update
to update it completely or we can roll
it back so that's how we do Canaries and
we all said to alter scaling which I'm
not going to really cover alter scaling
is based on performance metrics
utilization and we can scale clusters
based on that so those various scale
pods based on that that's very easy jobs
we also have the ability to run short
running jobs batch jobs effectively so
these will run to completion and is
various rules you can set for them so
pods are no longer had to be
long-running there were ways to make
them not long-running before but now we
have an official job so you can
eventually say I want to run this to
completion you push it out to kubernetes
it will run and finish it and we also
have line this sum readiness checks
people ask about how we know whether the
pods they're running pods alive where we
have the ability to set up live with
some readiness checks both the process
level and also at the app level but I'm
not going to go into details of them but
you'll see the slides later so you can
get access to this and also the ability
to gracefully terminated pods if we're
doing downscale events with scaling
downwards we need to be able to shut
down the pod gracefully so we give it a
30-second window of opportunity by
sending in a signal and so then the pod
can do its own cleanup in the future
we'll hopefully be able to do some kind
of draining we'll be able to make lame
ducks that can be taken out of the
actual pod management system from the
replication controllers point of view
and we can take traffic stop traffic
from going through it so I can finish
processing its requests
and once it's done we can remove it we
don't had that currently but we probably
need it in the future and also we
support multi zone clusters now so if
you have Google cloud platform and on
AWS you have noticed that the notion of
zones availability zones in AWS now you
can have parts of your cluster across
multiple zones so if for example San
Francisco or the Bay Area was pulled
into the Pacific Ocean for some reason
and you lose a zone you'll be still be
good because Europe would still be up
and running we don't want that to happen
right away so I'm going to next week or
this week even say it's not had to
happen so yeah so it's really good for
availability and that's available in GA
now incriminated 1.2 and just cool one
from scheduling very quickly I wanted to
do my demo anybody know how long I've
got ten minutes okay yeah all right I'm
just going to go through the scheduler
okay so scheduling basically when we
schedule things we send some object to
the API server and this scheduler will
look for new objects have been stored in
the configuration and it will schedule
them it will make a decision ultimately
it will decide where it's going to go
it's going to decide which node the
arrow should have appeared afterwards
but it would end scheduling on a node
that matches its scheduling policies the
scheduling policies can be controlled
and we're going to them shortly but it
has to make that decision on which node
without a scheduler nothing can happen
really so we can send st. accumulators
and you won't know what to do with it
but it has no scheduler but you could do
it manually you could say I want to run
it on this specific node and you provide
the node name when you send it they
basically run this pod run this service
run this pod effectively on this node
that's the old way of doing it and
that's the mechanical way of doing it
where you have to identify a machine if
you have a large data sense it may be a
machine in Iraq a machine a single
machine in Iraq in a cluster and decide
which one you want to run it on and
that's not very good so that's why we
have a scheduler we have the scheduler
and make all of those decisions for us
kubernetes understands resources in
terms of CPU and in terms of memory
and we can request given level 2 CPU and
memory for our pods so we can ask for
two CPU cores or two-and-a-half CPU
cores and 10 gigabytes of RAM for a pod
which we know that's what it's going to
use and Kuby X's can then basically
schedule based on the resource
availability on those nodes because you
have a certain amount of cores available
in your cluster you have a certain
amount of memory available in your
cluster and cooping excess can therefore
make a decision as to where this can go
because it will fit on there I've seen
these two cores where does it go where
where do I have two cores available and
also if you will can also set limit so
if you go above your limit we can then
say I'm sorry you've run out you're
using too much memory we can kick you
off that's how we handle memory because
memory can't be compressed
we cannot photo memory allocation we can
photo cpu allocation we can use CPU
quotas in the linux kernel to say we're
going to control how much CPU you have
access using quotas but the memory we
can't do that
so CPU is compressible it can be fought
on the memory can't be so if you use too
much CPU you'll just be fraught 'old you
won't be able to use too much CPU you've
asked so so much you're only going to
get so much if you ask for too much
memory you'll be kicked off because
you're you're trying to do something we
can't allow you may compromise other
running processes yep and ultimately a
request and a limit are two things we
have we can specify when we're asking
for memory and CPU for a running pod a
request is basically how much you think
you're going to need for this and it may
be based on empirical evidence probably
will be based on empirical evidence this
is what you think you're going to need
to be able to run this and so
effectively the minimum you need and if
you say you want this much resource we
will only run it on a place where
there's that much resource available
okay so if we need to cause we're
running on a machine that has su cores
available and we won't over commit that
so if you had lots of pods requesting to
CPU cores and you have eight CPU cores
you can run forward a money
you can't mine anymore we will never
over commit a request so we effectively
guaranteeing that you had that mount
available here but you also had the
notion of a limit and a limit is the
maximum amount of a resource you can
request and the reports to be scheduled
ultimately all we care about is that the
amount of request is available on a
single node but if you ever hit the
limit they're not ultimately we're going
to kick you off or we're going to follow
you if you request zero then we can
always schedule your pod but you may not
get anything you may not actually be
able to do anything but you'll have no
CPU shares but if you request zero and
you go into a node that's empty it'll
run you know do whatever it wants to
have as much as it needs but it will be
compressed down to the point where it
may have nothing ultimately and you
won't have any resources available to it
at all and again we can't do that for
memory but we can do it for CPU
and ultimately we can use different
combinations of requests and limits who
set resource guarantees or almost like
protection how much protection does our
unemployed have so this is for super
your memory is effectively gives us
quality of service if we specified a
request equals zero then we're just
saying one B whenever you can run this
pod whenever you can if everything is
maxed out the whole class's max down
then you won't be able to run yet we're
scheduled but it won't be able to run
but it hasn't as CPU shares but if
there's spare capacity then it will be
able to run and use whatever it needs if
you say two requests is less than the
limit and you're saying sometimes I know
we need this much but sometimes we need
to burst and have more and this is the
limit we're going to go to and that's
basically burst of all we can run that
we can't always guarantee it's going to
have that available to it I said we try
to you more views more it may be fraught
order may even be kicked off but
guaranteed we specified a request and a
limit to be exactly the same so this is
really the highest priority it's not
really priority currently but it's
basically a protection guarantee so
effectively you're saying this pod will
always be able to run because it's never
going to want more than its limit and
when you specify your configuration it
looks something like this use especially
find a memory and the CPU for a request
and memory and CPU for a limit
pod which actually I should mention this
is applied on a container level not in a
pod level so if you have multiple
containers you can set different quality
of service parameters for each container
and also spod scheduling we may care
about other things as well we may care
about a particular machine has a
particular thing that we need it may
have a graphics card a GPU that we can
use it may have fast SSD disk as opposed
to other spindle based discs so there
may be something about a machine that
means we want to run it there so this is
kind of like affinity and we can do that
with labels and we talked about that in
a second but we may want a certain
amount a resource and we may also need
certain discs we may have created a disc
that we need specifically or we may just
need a certain amount of disc that can
be provided to us so ultimately we're
super your memory when we trying to
schedule when the scheduler is looking
at how to schedule this pod it's in a
look at the memory and CPU requirements
and it's going to find a machine or
multiple machines that can actually have
that resource available to it or if it's
using that if it's using a specific disc
an example we've always able to do the
demo which I'm going to get chance to we
have a MySQL pod and the MySQL pod
mounts a disc and the disc has all of
its data on it it's effectively online I
know I know DB file system and if that
pod needs that disk then we have to
schedule it on a place where that disk
is available now that this may not be
mounted at all in which case we can
schedule on any node and we could then
mountain that disk on that node that's
what Kubb mesas will do automatically
for us it was scheduled it on that node
and you will find a disk amount mountain
a disk for us if we've already mounted
that disk then you were running on that
node because it leaves that disk and the
same move we have a thing called
persistent volumes so we need a certain
route of ram somewhere around a certain
amount of disk we would have created
these things called persistent volumes
which you can mount in on demand and
also you may as I mentioned you may want
to run it on a machine with fast SSD
disk or the GPU or we bother some other
configuration you need and it may also
be that you have some kind of affinity
need maybe you have all of your machines
in the cloud and some on premise which
should be possible soon with a thing
called new Benitez and that way you can
actually say I want to run it on these
Buster's some on-prem something cloud
you'll be able to use scheduling
mechanisms for node affinity to say only
run on machines on premise never run
this on in the cloud we'll also have
this thing called anti affinity coming
up as well so if you want to make sure
it never runs on this or never runs on a
machine with another pod that might come
into conflict with or very several
reasons why you might not want to run
two things on the same node and you can
use anti affinity policies so make sure
that never happens okay and that's
coming up probably in one point free but
don't be GA one point free but it will
be available and ultimately what we have
to do is rent called the potential nodes
and decide which one to use and then we
have different policies we can decide on
whether to balance the CPU a memory I'll
make sure we have the machine we leave
the machine with the most free resources
or even make sure that the pods that we
have are scheduled on different machines
as much as possible so if we had 10
points and 10 machines we should have
one on each node we have a bunch of
things coming soon you'll see this in
the slide when you get them velocity
we're moving very rapidly at some 1.2
we're at 12,000 stars now or something
there and 5,000 commits we're now at
plus 50% unique contributors from work
we were with 1.1 so kubernetes is
dramatically rapidly growing in
popularity and also it's in the top
naught point naught 1% of all projects
on the github which is extremely
significant right so two orders of
magnitude less than 1% or more than 1%
and so is extremely popular 800 plus
unique contributors we have many
projects that are based on kubernetes as
well
many companies contributing it's not
just Google Red Hat a serious
contributors to this as are many of
these others and also many companies are
already using companies in production
and in terms of plans we're at 1.2
currently at one point freezing progress
should be in June we're trying to do a
free month release cadence and
we also have container engine which is
Google's hosted platform there's only
time I'm gonna mention Google stuff
those platforms as a service read hi
OpenShift Deus Stratos there's also
destroys such as Cora isotonic Moranis
Murano Red Hat atomic Moses and finally
it's open source completely open source
we are not governing this project the
communities governing this project I
used to work at Sun and we did this
thing called opensolaris and it wasn't
really very open source at all Kublai
seeds is fully open source and is easy
for you to contribute you can do pull
requests you can get involved in issues
and there's many really good debates on
there so if you're interested in helping
kubernetes develop or you have a feature
you need you should go there and raise
that question and talk to the engineers
I did I build Annette Cooper makes is
that IO is for the documentation on
github it's kubernetes a scuba natives
on slack slack k eights do on Twitter
it's accumulated IO the slides for this
talk
well my canal my canonical deck is
available there so it has more than
these slides so if you look there you'll
see all of the slides for this talk and
more and that's it basically
I've got time the questions or
so we have a bunch of questions actually
that we we have a couple relating to
relating to region awareness so is
scheduling for region awareness
available how do you do that do you use
labels like you would for graphic arts
or is that a separate feature I think in
a moment you'll have to use labels there
are annotations so we can make sure that
persistent volumes and nodes run in the
same place so we can actually identify
where they're running through these
annotations we didn't talk about
annotations but they're kind of like
metadata they can be attached to running
objects just like notes basically so we
can use annotations effectively but I
think at a moment you probably after you
labels okay
what about so another question here
about anti affinity you mentioned that
it's an upcoming feature but is there
any way to deal with this today if
you're clever
is there any way yeah so you probably do
it with a you could probably do it with
lay boys I mean providing if you're if
you labels are not the best way but
you're really kind of limiting the
scheduler to what your options are when
it comes to wear it can schedule its
pods and that's a bit of a problem just
taking some of the power of it away from
it
so you could say only schedule these
pods on these nodes and you schedule
these pods on these nodes and so there's
a problem of having maybe multi-tenancy
problem that could be a problem
we also have namespace in but we can't
schedule based on namespace in currently
so probably a best place again use
labels so group things by labels and
that will probably work for you okay so
do you see any benefit of running
kubernetes on top of missiles
yeah I think it's a good standards-based
API so I think ultimately we're not
really in the best business of building
clusters so if there are other cluster
mechanisms and that's great I mean so
you can build a cluster somewhere and
then stick to braces on top of it like
you can do with a DCOs from mesosphere
then that's fine so any kind of cluster
mechanism anything that can actually run
that stuff providing we can stick a
kubernetes layer on top of it and that's
fine
no more Wes all we have to do is rely on
the cluster to do the scheduling or to
do carry out the instructions of the
scheduler for us that's what we need to
do so yes I think it's very possible
I thought you did you say there are so
many benefits yeah benefits yeah double
use any benefits that's specifically
well you could if you have familiarity
with medals from one embezzles clusters
and that's great or you if you have
muscles in your organization exactly
already and USANA use carbonates yeah
exactly we already using measure so you
can stick combinations ashumen on top of
it effectively and then you can just
move around as you need to portability
is a great one of the goals of computers
so having that's having that API
available but not having to worry too
much about the infrastructure underneath
is really important in fact I mention
Bolg earlier they may I'd love to see a
day when we have at the moment we
schedule stuff on virtual machines we
could you think we could effectively get
rid of the virtual machines and just
schedule them on board directly in the
future at some point so that's exactly
the same thing we have a cluster we have
a kubernetes interface a facade almost
and we send instructions accumulate is
like we always would do but we don't we
don't care about the mechanics of what
happens underneath the hood okay so
another question here will console be
supported at some point I don't know
will console be supported I think you'd
have to ask them you'd have to go them
to get up and find out I'm not sure
there's any plans but I think pretty
much everything that can be done with
kubernetes has been discussed on to
github project because everybody's
interested nobody's excited and
everybody has their own problems they
want to solve it so I think if somebody
wants to run Kon Suay probably would
have raised it it maybe it went into a
dead end maybe you could revive it and
again I think that's the great thing
about it is that you can just this is
what I want how can we build this and
people will be interested and they'll
tell you why you can't or why you can or
how you can so I think getting involved
in those discussions is really important
what about something like auto scaling
you know we saw that you could scale up
by manually going in as I want to notes
but could you write is there any sort of
support for that in kubernetes already
yeah or would you have to write a
external program that would scale it up
and down for you alright so I would have
talked about portal to scaling there's
quite a thirty-minute is quite short for
it also showing all these things in
there demo is hard and auto scaling
demos are really really terrible because
also scaling the same happens over a
long period of time not ever not in a
two minute demo so it never really works
properly
but the fact is is if you are if you
have at one pot servicing traffic and
you're monitoring the resource
utilization of that pod you can scale
when it reaches a certain level and add
more pods and those pods will be
distributed across the nodes and the
traffic will be low balance pilot
service to each of those pods at some
point you may run out nodes you may not
be able to schedule anymore pots that's
the whole point of kubernetes you could
potentially run out of nodes and at that
point if you're in the cloud
we could also scale your nodes or some
providers so for Google container engine
and Google compute engine if you're on
nano's we can do it today and we're
looking for other people to do other
mechanisms that allow them to the scale
dynamically but we can then scale more
nodes and add new no new pods onto those
new nodes the one thing we can't do
currently is rebalance
so once we've added nodes we can't move
things so we can flatten out the cluster
again that would probably come in the
future where we can reshape the cluster
based on adding new nodes will you be
able to sketch or scale based on other
metrics rather than just load like the
face of the moon or I think so anything
that could be measured and if it can be
measured on a scale and be scale
proportionally then I think that will be
possible
I don't what we can do that with nodes
already so we can do it on lots of
different metrics I said we can be
monitoring the nodes and scale based on
metrics that are happening on the nodes
so it could be disk utilization or
something like that but we really want
kubernetes to be the one that's
responsible for ask in the underlying
runtime it's a scale not having to run
some scale water Matic you on its own
because that kind of squeeze our
scheduler up a little bit so basically
we should have an interface from
kubernetes on to the underlying runtime
so that we can say hey we need more pods
can you give me more pods and you may
create a purchase order and send out to
Dell so you get another computer
delivered one for clear for the next day
and you stick it in and you add it to
your cluster and that's also scaling a
very manual process of auto scaling but
yeah I think it's it's very possible
today with container engine but again
that's not I'm not recommending you
choose container engine but it is one
option but
today for AWS and others a lot of people
run Knutson AWS it's probably possible
check it up to their autoscaler as well
but again you may be relying on the
operator the cloud provider to do the
scaling for you as opposed to having
accumulators request you'll - scaling
which is a bit different cool
Thank You Mandy thank you very much so</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>