<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • Explorations in Interactive Visual Analytics • Doug Talbott | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • Explorations in Interactive Visual Analytics • Doug Talbott - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • Explorations in Interactive Visual Analytics • Doug Talbott</b></h2><h5 class="post__date">2014-12-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lBZmuo6OvvY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we've been working for the past few
years on on an interactive analysis and
visualization environment for analysts
and for actually for developers so it's
a bit of an ID so what I'm going to talk
about today is I'm going to give you a
one slide on big data I'm not going to
bury in that you probably hear it all
over the conference and I am going to
spend another slide on who the user is
because a lot of this talk is really
about the design considerations that you
have when you're building a user
interface where you're trying to support
large data sets and so I'll talk about
what we are trying to accomplish in the
space i'll talk about some of the
challenges and then i'm going to spend
my ninety percent of the talk on some of
the things we discovered and how you can
do it if you want to actually build
products that deliver big day to some of
the considerations okay so let's just
get started so this is a real in a
nutshell big data data sets are
basically too large to be handled any
other way and so they require new tools
and new technologies to actually be able
to manage them there's a lot of talk
about the 3 v's so where they are volume
velocity and variety and to give you an
example here thirty-three percent of
companies today like these are based on
Gartner statistics and forests or an IDC
and those sorts of things so
thirty-three percent of companies are
now working with data sets of 500
terabytes are better we have velocities
by next year they're sort of predicting
that sixty-three percent of analytics
will be done in real time which is
pretty amazing and the variety is what's
really significant we're moving away
from structured data sets to
unstructured data sets eighty percent of
companies have unstructured data and of
course we want to be able to get at that
and actually take it and actually
structure it so that we can work with it
and analyze it that sort of thing in
terms of who's doing it right now what
we have our data scientist these are in
any kind of discipline that you really
want to think about but really what they
have our critical thinking skills their
model based programming skills and stats
guys and there's going to be a huge
shortage of them and they're predicting
a hundred and fifty to 190,000 vacancies
in this job market in the united states
alone so if you're looking for a new
career path looks like it's good the
language is right now are kind of
surprising though most of the languages
are not ninety percent this is based on
a Katy nuggets poll so it's a pretty
small poll but ninety percent of the
languages that people are using this
space or things like are and there's a
new SAS language and SQL and that sort
of thing and the language is that you
would normally expect to be seeing in
that space like Java and things like
that very small percentage right the
adoption everybody's sort of the you
know the Hadoop craze is going on but
it's really only representing about nine
percent of the market right now in terms
of people who are in the space and those
are just a brief sort of summary of the
applications because they really it's
soup to nuts across the board sentiment
analysis fraud detection and those sorts
of things and you know everybody's seen
the Amazon shopping carts and their
suggestions to you as to what you might
want to buy and that's all got to do
with processing large data sets okay so
what are we trying to do in this space
okay so first of all the big question is
do you actually believe that you can
actually look at and understand and
interpret big data in other words most
people what they want to do is reduce
this down so that it's small data and
then they can work on the small data
okay and we do that too but point is
that you have to believe that there is
actually value in looking at big data
and hopefully by the end of this talk
you'll sort of see it and then the other
thing we're trying to do
with what we're working on in terms of
our project is we're trying to allow
people to work with big data as if it
were small data in terms of things like
the interactivity the response time the
ease of use we're trying to make it as
easy to use big data as it is to use
small data and so what we've done is
we've built this thing called the IV
workbench which is really this analysis
and visualization environment and just
to give you a little bit of sort of an
understanding of what it is it's a it's
a vector functional columnstore it has
an sql-like language but the actual
underpinnings under the language is
actually functional it just looks like
SQL but it's actually doing column
column searches and this sort of thing
under the hood okay it's distributed
it's grow a bull to zillions of cores
and you know all over the place so I
what I'm going to do is sort of talk
about the things that are in orange here
you know we have ETL wizards and those
sorts of things that we've been playing
around with but I'm really going to
focus on visual inspection just line by
line editing and spreadsheet editing
just to keep it kind of focused in an
area excuse me so there's a bunch of
challenges look just want to make sure
sorry so there's a bunch of challenges
that you really have to consider when
you're working with big data that's
different than small data and that's
you've got really a bunch of potential
user interface problems and those are
over on the far side of the screen there
things like short attention span so if
you're waiting 25 seconds every time
you're trying to visualize something
you're very quickly going to become very
annoyed and so we want to have you know
good processing speeds so that you can
get the visualizations up so that you
can look at your data things like loss
of context if you're waiting too long
you actually
forget what you are actually waiting for
and actually what you were looking for
other things like limited short-term
memory we don't want to bury people with
you want to be able to keep the
interface in in your face so you can
actually still remember what you're
trying to do there's increased cognitive
load because you're now dealing with
more complex data so you have to you you
know make your control smarter and I'll
show you some examples of of this and
you've got increased perceptual load
because you're trying to look at lots of
data right so you've gotta use different
techniques and pattern matching
techniques to make sense of it okay so
that's all nice and abstract what I'm
going to do is go through a case study
and this is a case study from the vast
challenges and you can look them up
online they have lots of these different
challenges this one is analyzing cell
phone records and we're basically
looking for a social network of
criminals and so what we're going to do
in this case study is we're going to
look at a table of records which has a
source column a desk column adoration a
timestamp and a tower in it okay so this
is just sort of giving you the lay of
the land of what we're going to look at
the phones in this or numbered zero to
four hundred so they're contrived so the
sources and destinations are all these
matches of 0 and four hundred and the
record set for the vast study is only
nine thousand records so I made it 35
million to make it kind of interesting
and so what we're really looking for is
suspicious behaviors and I just want to
let you know what that behavior is
because you're now going to be analysts
here so typically you're looking for
destinations that receive a lot of
incoming calls that's when drug dealers
do they received lots and lots of calls
from lots and lots of different people
so you're going to look for that kind of
pattern you're going to look for
destinations that suddenly disappear and
then suddenly reappear somewhere else
but they're all talking to the same
people okay and the third thing you're
going to be looking for is I in this
fake study it
cell phone 200 is a known suspect ok so
I started giving you the lay of the land
here we're going to use our tool but you
could probably try to do the same kinds
of thing in our studi studio or pentaho
or probably palantir ok and we're just
using an internal server it's nothing
special and it has four cores ok so I'm
going to start off just by explain
because things are going to go by
quickly at the start here I'm going to
give you a little sort of understanding
of what's going on here so we have a
little bit of code here we're going to
execute it line by line it's an
interpretive environment so each line
execute sand you see the results and
you'll see the results down here so
we're going to load the data set and
then we're going to do a summary
analysis of it that's that little second
line of code there and then you're going
to see we'll get into the visualization
aspect so it's a a load of a little
summarization and then a quick query and
then we'll get into visualizing so here
we go we're loading up ok so it's loaded
now we're going to summarize that and
that's really just doing a reduction on
it so the reduction is done and that
tells us things like how many records
there are and all that sort of thing and
now we've done the quick query so we're
going through all of this stuff in real
time and now I'm opening up our
visualizer and what I'm going to do
first is I'm going to look at the shape
of the data so the first thing I'm going
to do is look at source records and I'm
just going to stop here and sort of
explain what's going on what we've done
is we've just visualized 35 million
records and we bend it into 2 20 bins so
what we're going to do now is we're
going to move forward and we're going to
do fine grain binning and then I'll just
sort of narrate what's going on as we go
along because I know you're not analyst
but i do want to give you a flavor for
the speed
and the ease of use so here we are we're
increasing the granularity and what I'm
really looking for is is there a pattern
in this data that I can recognize and
right now no I can't so I'm going to
look at another column I'm going to at
the destinations now so that's
interesting right like we see down in
the zero range there's something funny
going on and up around 300 so I'd write
those numbers down for later reference
and here we are looking at here we are
looking at days like dates and so what
we can see here is there's about 10 days
there and then we can see some
interesting little things going on in
here whether sudden spikes in the middle
of the night so again I would make a
note of that and I'd be doing some more
analysis specifically around that the
same thing I'd be looking for different
durations and what we're going to see
here is that it's a standard
distribution so we don't care so now I'm
going to move over and I'm going to get
to the chase of it which is I'm going to
look at date and destination and again
I'm going to stop at once it's finished
it's 35 million search okay so the
interesting thing here is that now we
just bend these records by date across
the bottom and destination up the side
and what I can see here is that there's
a lot of activity up until this point
and then there's some funny little
activity up there right all of a sudden
it picks up at the top so basically by
like I should explain that I actually
looked at every dimension of this data
I'm only showing like the good bit okay
so I was spinning this looking at all
the different combinations of all these
columns and adjusting different things
you'll sort of see this as we go along
so what I'm going to do is I found this
interesting so now I'm going to increase
the resolution again so now we're going
to go down a little deeper so here's our
35 million and we can definitely see a
pattern now in this data that's kind of
interesting we see it suddenly drop off
we see a pickup so now I'm going to go
down a little further and I'm going to
take a look at the individual items and
here I notice a few items and so again
I'm sort of cheating what I've done here
is I you know just took out a bunch of
time and I found a few guys I was
interested in looking at up there so
what I did and what I'm doing now is a
real-time query looking at a few
interesting destinations and what I what
I do is as I'm looking at this I'm
noticing i'm getting some cruft in the
data so you're going to see that I'm up
in the top line there i'm going to start
removing values until i get down to the
interesting values like in real time
this took me probably about 20 minutes
to figure out as the terms of the puzzle
so now what I'm going to do is I'm going
to actually visualize this as a social
network so I flip over I look at it as a
directed graph my destination is the
parent and my child or all the sources
and I see the directed graph and
interestingly enough what we really
notice here is the 30 6 &amp;amp; 5 &amp;amp; 1 &amp;amp; 3 09
are talking to the same identical people
and so what that tells me is probably
these are the same guys and they've
dropped your cell phone and picked up
another one okay which is a classic drug
dealer behavior all right so so oops I
want to slow that down there just a sec
so I'm just going to back that up a bit
so that was one way of looking at the
data what I now want to do is I want to
do a pagerank analysis so these lines of
code here we'll do a pagerank analysis
and tell me who the most important guys
are in this data set so what we're going
to see I'll just back it up a little
further
here we go there we go I want you to
sort of get a sense of the timing on
this so I'm going to highlight that and
I'm going to execute this in real time
and it's found my interesting example
and you'll notice 200 appeared there
which is very interesting he suddenly
appeared so I'm going to look at that
social network again in the context of
these interesting people who are
considered the most important cell phone
users in this set and then what I'm
going to see is that we basically have
two hundred at the center of these
people who are talking and they're all
kind of interrelated so in the space of
about 15 or 20 minutes I've kind of got
an answer to this vast puzzle so what do
we do here okay well really what we're
doing is we're real-time tumbling of
data and what we're trying to do is
explore relationships so what that means
is that we were able to you know very
quickly load a bunch of Records
visualize it analyzed it flip and filter
it look at it from a number of different
ways Bennett and the interesting thing
is that I think you can see that we were
doing that where there was really no
human factors impediments in other words
we word our attention span really wasn't
lost the process seems fast enough that
I can manipulate and manage this data
and all of these things are sort of
working together so what that means is
our like what we learned out of this can
be binned into three categories there's
a couple of analysis things that I
noticed while i was doing this while
we've been working on this project we
have a bunch of visualization things and
a bunch of interaction things and i'm
going to go through each one of these
things in a bit more detail and sort of
you know sort of rewind what we were
doing in that demonstration and take a
look at it sort of in a different way so
the first thing you want to do and if
you're going to be building out these
big day
applications is my belief is that you're
going to want to have an integrated
analysis and visualization environment a
lot of the products today that are on
the market do one or the other so they
either visualize or they analyze and
where the real power is is in being able
to take those two things and put them
together so ben shneiderman has this
thing about data visualization he's got
this mantra of overview first and you
know Zoom Zoom next and then
details-on-demand I think that's great
but I think you also have to add to it
all of the other things you do in
analysis like extracting and loading you
know transforming scripting and for me
scripting is actually quite key and
sorting and categorization these kinds
of things all have to be put together
and then of course is annotation and
collaboration at the end of this process
the really interesting thing is that all
of these things even though I've got
them in a list really need to be able to
be done in any order at any time so you
need to be able to go back and forth and
what I mean by that is that you have to
support an iterative nonlinear task flow
so if you think about what I was doing
in that demonstration if we take it out
and we pull it apart there's some things
that are done on a visualization axis
and it's really all about how we
organize data and a guy named Richard
Saul Wurman came up with something
called the latch model look and it
stands for location alpha time category
and hierarchy and it turns out that he
came up with this theory they said
people organize information in these
ways this is this is this set so if you
look at what visualizations do is they
really map to that set and then on the
other axis we have all these analysis
techniques that we can apply
summarization clustering etc and if you
think about it you'll want to have this
matrix where you can do any one of the
things on the top with any one of the
things on the left you want to be able
to do them in any order at all because
you're really trying to discover
answer to a problem okay so that's all
wonderful because you're looking for
those relationships but the real issue
is if you can't process quickly you're
toast okay so that really comes down to
this next point which is you've got a
leveraged server-side processing for
your queries but you also have to
leverage it for your visualization and
I'm going to sort of walk through a
couple of things so here's a typical
case study where well actually this is a
benchmark or as Dave likes to call them
bench lies what we've got is 100
terabytes of data from Nasdaq I'm sorry
from New York Stock Exchange so this is
the training quote system that comes in
off the you know out of the trades that
are done on the new york stock exchange
there's 5,000 days of data it's a
trillion quote you can see the
specifications here the 16 cores 256 gig
that sort of thing so these are sample
queries that we've done against that
data set and what happens is we're
taking this 100 terabytes and on a small
day we do a query takes about 18
milliseconds and we get a working set of
around 35 million records so if you
think about it that's why I showed you
35 million records in that other example
because what we do is we take a 100
terabytes pull it down to 35 million
records and now you can work with it
quite cleanly and it's the same thing
with the large day right we can do a
large a larger query 290 milliseconds
and now you're working with the data set
of about 600 million records I'll show
you the numbers on that a minute okay so
now you've got your data and you want to
render it we tried a whole bunch of
things where we were rendering on
clients using javascript and d3 and all
these other languages and it just didn't
work so what we ended up doing was
doing all our rendering on the server
and shipping bitmaps to the client and
then what we do on the client is when
you touch something we send the points
back to the server and do another look
up and pull that one point out and then
tell you what the point is and what's in
there and that sort of thing and so this
works out a lot better because we don't
have to we don't care about anybody's
client right we don't have to worry
about whether it's a you know a windows
machine and what the processing power is
there we know this we know what this is
going to be it's well known for us even
if it's in the cloud like even if it's
an Amazon instance we can know what that
is so we don't have to worry about the
client at all I can give you more specs
on that later so here are a few numbers
okay rendering a million points on the
client using jsu d3 or one of these
things takes about 22 seconds if you
render that on a server and do a PNG
it's about four seconds that's on a
vanilla cloud 10 million points you
can't do it on the client I have yet to
figure out a way of getting canvas to
render that many points it's about 28
seconds 28 seconds to render on the
server so it's very fast for that I'm
number of points and here are some
points if you do binning so that benning
technique I was showing you million
points a second 10 million points is
about four seconds it actually starts to
get better than linear at some point
right hundred 100 million 22 seconds 500
million 116 seconds so that's getting
pretty slow and I'll show you later how
we brought that number down as well okay
so now we're now we've got this thing we
can tumble we can do it quickly but what
we want to do is we want to leverage
pattern matching so all of those
techniques at the top already exist
today right in other words all those
analysis technique
like aggregation summarization these are
well known techniques you can look them
up in papers the really interesting
thing is a lot of them can come right
across so in other words you can
actually do them at scale and and use
the technique so in other words our
ability to Priya tentatively process
that information is all we really need
to worry about and we can do these
things you know on the fly at scale so
what I'm going to do is I'm going to
sort of give you a crash course in watch
I'm going to ask how many people know
about pre-attentive processing okay I
included this because I figured no one
knew right ok so what pre-attentive
processing is is basically without
paying attention you recognize patterns
okay and so these guys McKinley and
Burton did a lot of studying and they
figured out that when you're working
with quantitative data you notice the
position of the data you notice the
length of the data you notice all of
those things in that order okay and so
to give you an example something like
length means that when I look at this
picture here the bar on the end is twice
as big as the other bars and you can
tell that and you don't even actually
have to think about it this is like
lower brain stem activity and you just
know it it's interesting that things
like shape you can know that shape is
different but you can't actually map it
and say that circles twice as big as
that square right you don't really know
anything about the data sorry about the
you know it's different but you don't
know how different it is ok so in
certain cases you can apply these things
to quantitative data and you can apply
to other properties of data so what we
did in that example was we used
aggregation to use binning to show the
pre-attentive use our pre-attentive
processing capability so that you can
suddenly recognize these patterns so
what this means is I can change the bin
granularity I can change the aggregation
and I can apply it to different kinds of
visual attributes such as size shape
color opacity and I can suddenly pick
out these patterns and the nice thing is
I can apply this to like opacity and so
I can show you where you don't want to
live in Sacramento so here we are
looking at ten million crime records in
Sacramento and we can easily see where
we don't want to live okay and that's
just applying bin counts to opacity and
here we are doing a scatter and we're
now applying the same logic to size and
here we are doing it again doing a heat
map in opacity so really the same
technique can be applied against a
variety of different graph types get
implied to bars histogram so here again
it's the same binning technology being
applied across the board so this means
that you can tumble your data in a
variety of different ways and experiment
and explore and that's really what it's
all about so here's the thing though is
that plotting all the data can actually
hide the data so here is a picture of a
million points plotted from 0 to 1 using
opacity took about five seconds and
here's the same picture okay so this is
the same data set plotted and you can
sort of vaguely see this little oval in
there right and it's just by applying
that opacity and really what we're doing
is we're removing data points when we do
that so it turns out and I hope this
isn't i hope this you can see this let
me see back here yeah you can see that
circle up there all right okay so that
circle is about 2,000 points in a
million so the turns out that this
pattern matching to
nique can work on you know a reasonably
small data set against like a million
points in that kind of ratio if you have
concentrated patterns you can see that
you can pick up 200 points out of a
million which i think is sort of the
equivalent of a needle in a haystack I
it's pretty interesting that your eye is
tuned well enough that you can pull that
out of the out of the map okay so it
turns out now that we can do these
patterns now we have to start to cheat a
little with visual design because all of
the rules for visual design have been
written for viewing grass not
interacting with graphs and I think
there's a big difference so the tough
Tok Edward Tufte came up with this set
of fantastic rules for visual design and
you can look them up and they're well
known and this is a terrible graph and
it violates quite a few of his rules one
of his rules is that you should erase
ink that has no relevance to your graph
so the thing like the the 3d box and the
white bar are rounded in that little
tinted background that should all be
gone that's just junk and the other
thing is he's cheating right he's
showing thirty-five percent in
thirty-nine percent but he's cut off
most of the bar so it really looks much
more dramatic than it really is so Fox
tends to like to do this with their
graphics so the thing though is that
with big data sometimes lying and not
showing all the data is a good thing and
I showed you that example and now i'm
showing you this example where this is
kind of cheating right we're not really
showing all the data there it gets even
better if I process it if I image
process data using RGB technology I can
actually drop even more points out and
the pattern gets even stronger
so with small data and with viewing
graphs we follow these design rules but
with big data we kind of got to forget
those rules and got to throw them away
so the other thing is that there's that
whole telling the truth with your data
and showing the X the Y axes and I will
show you that example where we we clip
the talks well here's an example points
from zero to fifty thousand and we're
showing all the data right we're showing
the whole bar and we're showing it from
zero to fifty thousand the problem with
that is that here I'm showing the same
identical data set but I'm showing it
from the min and max range right which
is a no-no but it's interesting because
I suddenly see a little gap in the data
so the point is that that rule of always
start from zero on a y-axis for this
kind of graph it doesn't really make so
much sense anymore when you're dealing
with big data same thing here there's
another rule it has to do with the plus
or seven sorry seven plus or minus two
and are cognitively in other words we
can only remember seven things plus or
minus two and so therefore if we have a
legend we shouldn't put more than seven
things plus or minus 2 on the legend
because we can't maintain contacts
between that legend and the graph this
is sort of well-known theory right
that's the rule don't use more than
seven things well here's a graph that's
the top 80 keywords out of moby-dick
real and a natural language processor /
Moby Dick found the talk keywords
plotted those top keywords so you now
have a tiki keywords here in a legend in
series data an interesting thing is that
one thing we can tell about this is that
certain things pop and the other thing
is that we can find an area where
there's an actual gap in the document in
other words it was a point where there
was really nothing
mentioned in Moby Dick so it's kind of
interesting that a rule that would be
normally applied in small data can be
removed in big data and suddenly have
some value okay so and then of course
you can always filter out the noise
right there if because its interactive
we can always knock down what's
backgrounds and that sort of thing so
that we can enhance so here we're
looking at Jonah and I can move that
around and I can quickly because its
interactive I can do other things with
it okay another thing is maximizing the
plot ratio the UI to plot ratio and what
I mean by that is a lot of interfaces so
these two pictures are pictures of
interfaces and the black area is the UI
control area that's where all the
buttons are in that sort of thing and
the one thing you kind of notice is that
most of the the interface on the far
side most of the control area the most
of the interface is allocated to
controls where is over on the right hand
side here most of the interface has been
allocated to the graphic and why that's
important is when you're looking for
these patterns you want to be able to
actually get as much real estate
allocated to the to the actual plot so
you can actually look at the details in
there okay so we need to take how strike
a better balance between UI widgets and
controls and actual plotting and so
here's another example that our first
interfaces hid the controls we had them
all hiding so that we could maximize our
real estate so we had everything in
little little pop-ups and the problem
with that is that you end up using these
controls so extensively that you want
them on the screen so we ended up making
this trade-off where we have a scrolling
list of controls on the right-hand side
because we want to maximize that area
for the viewing of the ground
and so I think that means that these
small data interfaces might have to
change to handle more of the big data
problem okay so the the other thing hand
in hand with that is when we talk about
those controls they have to manage data
complexity and what I mean by that is
you'll see in a lot of the small data
interfaces they make heavy use of check
boxes and things like this on the far
side of the screen well when you start
dealing with large data sets where
there's large numbers of controls
imagine having four hundred cell phone
numbers that I have to check each time
what I want to do a query so over here
I've got this query it's harder to learn
but it gives me a lot more flexibility
and it's a lot more compact so we really
now have this range of choices between
easy to use right it's easy to learn and
it's easy to use but it has much more
complexity when you start looking at
numbers of columns and and ranges and
that sort of thing imagine if you had a
range of like say 7,000 you were trying
to use a slider on it be very difficult
so we have to start looking at ways of
making that easier to use but at the
same time making them also flexible to
use and that's we're right in the
process of working on that sort of stuff
and it's the same thing with user
interface as a whole so we've built a
visual query tool and it's modeled after
Ms query but there's a range of these
kinds of interfaces and so it's arguable
that the node and link style interfaces
that you see in a lot of products are
great for you know learning but when you
start having to do filtering upon
filtering upon filtering they suddenly
get very complex by the same token
interfaces like texts are
hard to learn because you have to
remember everything and then on top of
it you end up in syntax Hell where
you've got all these brackets and
everything else get lost so what we're
now looking at is some kind of
compromise in the middle where we can
try to eliminate the syntax of text and
also eliminate the that the hell of real
estate over here on this side so we're
still experimenting with that the other
thing is small data interfaces tend to
update automatically every time you do
anything they update that turns out to
be a really bad idea in big data
interfaces so imagine you want to change
the color the size of the granularity of
your bins and say remove the background
if each one of those operations is auto
updated and it takes you seven seconds
to rerender the image that's 21 seconds
rather than seven seconds and so it
changes the way you actually think about
all the auto magic that people
automatically do in a small data
interface and it's the same thing with
executions a lot of small data
interfaces will immediately execute the
query as soon as it changes so it turns
out in big data interfaces what you want
to do is you want to provide examples of
the output and you want to provide
estimates of how many records you're
going to get back once you hit that
database so in other words in this
example I've got approximately 192,000
matches out a 10 million but I really
don't want to do that query I just want
to sort of play around with it until I
get it right and then I want to execute
the query and it turns out that not a
lot of interfaces are doing that sort of
thing okay so my last point is
making scripting a primary task okay and
why I say that is that using our
interface we can render if you know 500
million points in about 116 seconds but
if i get really smart about my query
with a very little amount of what i'll
call sql-like scripting i can drive that
time down to about 27 seconds and if you
look at that line it's really not that
complicated to kind of figure out now
it's doing as I say it's doing a vector
functional search underneath it's
actually doing a functional query under
the hood it just looks like SQL and the
other thing is that if you use scripting
and this is really about going back and
forth between visualization and analysis
is you could actually end up with a
right answer or a wrong answer and if
you if you decouple these things where
you're only doing visualization and
you've got it divorced from analysis you
can actually end up with the wrong
answer and what I mean by that is if you
look at the example I gave you earlier i
did a visual analysis and i discovered a
set of nodes i also did and i didn't
show this to you i also did a markov
analysis in about the same amount of
time as it took to do the PageRank
analysis and i concluded there was
nothing interesting in the markov
analysis and then I did the PageRank
analysis I went oh that's interesting so
then I was able to take that page rank
analysis and I was able to couple it
with another visualization and actually
come up with the right answer versus the
wrong answer if I just stopped with the
visualization first it would have been
wrong so it's the ability to be able to
do these things in any order that's
really actually quite important so I
thought I'd just you know really sort of
clothes off here with this little
demonstration of another part of the
interface
and this is really more of an analyst
interface that would allow them to use
like a spreadsheet type interface okay
so it's really it's a you know this is
all working in a browser i'm looking at
a different you know different cells and
now out what i can do is i can actually
interpret them without actually having
to inspect them immediately so you know
really in this example we've loaded up
you know three million records or so and
we're just sort of playing around with
it here's the kind of scripting that's
involved here so you can actually do
things like on the fly declare keys in
in tables and do left joins and things
we call as of joins or window joins so
you can actually look at streaming data
the same way and you can just sort of
again work with it fairly interpretive
Lee and then get your answer back the
other thing that you can do okay so I'm
just going to show you inspection here
right so again you can actually inspect
out of this same environment so really
this is just working under the kind of
the Excel model another thing you can do
is you can actually create function
calls so here is in a simple summary of
you know using this kind of function
inside of a spreadsheet so you can
actually start to now build up little
languages that you can use over and over
again and then this is really putting in
a higher-order interface so that if
someone wasn't really savvy technically
you could actually build a front-end
that would allow them to do this using
regular standard controls like
checkboxes on that sort of thing and
really it just leverages JavaScript to
allow you to do this so you can all sort
of put it in the in the front end the
other thing that you can do and all this
will just take a set of a second to get
to it is then you can actually replace
the engine so if you don't like the fact
that it's
red sheet and it behaves like a
spreadsheet you can actually write your
own spreadsheet and in this case here's
my little sample app and what it does is
it actually allows you to like in this
case it's just multiplying the number by
two so any number you put in there gets
doubled I mean it's not really that big
a deal but the point is that it then
gives you power to actually deliver that
kind of thing it really opens up the
interface so that you can now use dsl's
and that sort of thing to allow your
users to have a higher level of order
and so what I really mean by this is
that if you're going to put scripting in
your product you want to support certain
kinds of behaviors and those behaviors
are allowing people to script on the fly
allowing them to go back and forth
between visualization and analysis and
allowing always showing them some output
so rather than in a compiled a scenario
where you're constantly compiling and
then waiting for an answer you really
want this stuff on the fly and then
finally you want to be able to take this
stuff and encapsulate it and turn it
into domain-specific languages so that
you can use it over and over again and
build up your own private sort of
library if you will so really this is
sort of a summary it's these main points
basically my view is that if you have to
do at least these core things that I've
talked about if you want to really
deliver yourself a small data Internet
experience at scale and still provide
those things like interactive you know
interactivity response time ease of use
and that sort of thing</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>