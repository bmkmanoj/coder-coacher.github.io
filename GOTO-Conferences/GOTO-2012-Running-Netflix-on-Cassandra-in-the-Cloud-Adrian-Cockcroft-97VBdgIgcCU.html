<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • Running Netflix on Cassandra in the Cloud • Adrian Cockcroft | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • Running Netflix on Cassandra in the Cloud • Adrian Cockcroft - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • Running Netflix on Cassandra in the Cloud • Adrian Cockcroft</b></h2><h5 class="post__date">2013-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/97VBdgIgcCU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm going to talk about running
Netflix on Cassandra in the cloud but
I'm going to start off by just
mentioning something for the locals
which is that Netflix announced a few
months ago that we're coming to the
Nordic countries in q4 and it's October
now so it's q4 so at some point in the
relatively short future you're being
able to get Netflix and right now if you
go there they just want to collect your
email address but I don't want to
highlight this quite nice little point
down here so if you look down here it
says EU West 1 DF 6 9 b e0 9c DK thats
that's an instance Amazon instance
running in Ireland and it's no it's
figured out we're in Denmark so any
that's just thought I'd highlight that
and hopefully there are local people who
will sign up for this thing so yesterday
I gave a talk and I went through a bunch
of the stuff about Netflix and cloud and
whatever and I assumed that some of you
would have been in that talk as well so
I'm not gonna go over that much in great
detail talk was video do you be able to
find the slides anyway I actually have a
t-shirt that says blahblah cloud blur on
it but today I decided to be chaos
monkey lots and lots of slides of
slideshare.net slash Netflix this is one
of the slides from yesterday just to
give a bit of context for people that
weren't here yesterday we have over the
last three years basically moved all the
different pieces of Netflix to the cloud
and all of the blue boxes on here are
basically backed by Cassandra you know
all of those there's like a purple one
that's my sequel that one is got in the
process of being moved to Cassandra and
that's actually there's a bit of content
management and some of the stuff on that
first column should actually be
Cassandra as well nowadays so I'm going
to talk about Cassandra running on AWS
which for us is a highly available and
durable deployment pattern which looks
something like this
what we have here is all of the client
services that want to look up this
particular piece of information they all
make rest calls to a service tier with a
you know standard rest interface that
then makes is the only thing that talks
to our Cassandra cluster I think the one
that I grabbed here is I think there's
24 notes here and probably about 50 here
and then some of these they're like
fight some of these blood boxes have 500
machines so these are distinct services
not distinct machines right so we have
300 different services and some of them
are backed this middle tier rest service
also synchronizes with the data center
to post things back there for various
reasons when you're doing transitioning
out of the data center the data flow
like if the data center is your master
you can basically serve cache misses or
lookup stuff in the data center and pull
it back in for the cloud when we now
have the master copy of all our data
running in the cloud running in this
Cassandra cluster but we still post
stuff back to the data center for some
systems that aren't critical to the
streaming service but which we still
need and this diagram was done using app
dynamics which are you see pop up
occasionally so that's one Cassandra
service think of it as like a
materialized view would be for Oracle or
my sequel so this is for example it's
the subscriber service the only thing I
can do to the service is ask questions
about the subscriber I can give you a
cookie or something and it'll tell me
which subscriber it is that kind of
thing it doesn't do anything else you
can't join it there's there's just a
single purpose service and a team owns
that service and an engineer built that
service and if it ever goes red and has
bad response times we know exactly who
to call and this Cassandra cluster
serves only that one purpose right so
we've partitioned everything out so what
that means is that in production we
actually have you know lots of these
things there's more than 50 distinct
Cassandra clusters they add up to more
than 500 Cassandra nodes varying
various sizes the smallest cluster was
we do a six nodes the biggest we have is
72 our daily backup volume and that's
compressed backups is about 30 terabytes
so that's what we are backing up on a
daily basis and one of these clusters is
doing over 250,000 writes per second and
it's actually mostly logging data we
have a quite a few high traffic clusters
and we have another cluster that has
about 10 terabytes of data in it so we
have one very big cluster one very high
throughput cluster and lots of stuff
somewhere in between so that's not kind
of where we've ended up so the took a
bit about the rest of the architecture
here we get high availability by storing
three copies of the data on the local
instances on Cassandra we use the local
internal disks that are ephemeral that
go away if the machine dies okay so we
have three copies of that and those
three copies are in different data
centers in different buildings in fact
the writes go to ram and then it acts
back and says that's fine you're done
you can get a fast write by saying let
me know when one of the clients when one
of the Cassandra nodes is completed you
can get a durable write by doing a two
out of three quorum right and a two out
of three quorum read after if you want
to get read after write consistency
these availability zones a separate
building separate power there I'm not
sure how far apart but there are a few
miles apart and they are about a
millisecond apart which means that if
you want to get a fast request you call
you talk to the Cassandra node that's in
the same building as you because the
clients are also spread across these
three zones right so think of three
different buildings if you're in one
building there is a Cassandra cluster
node in your building that has a copy of
your data if you want consistency you
have to talk to one of the other
buildings as well to make sure everyone
has the right data so we triple
replicate we take our little map of all
the services talking to each other and
we have three copies of it in three
different building
and then we have Cassandra back-end
replicas and Cassandra's doing the side
to side replication that basically keeps
everything in sync so if you write some
data it gets copied sideways and then
the load balancer sent the traffic into
the top if we're doing maintenance on
Cassandra which we do for upgrading
their version of Cassandra or doing
repairs or or compaction sometimes
depending on on what we'll talk later
about why we do that we take a
particular Cassandra instance out of
service now that means that I only have
two copies of that particular data I
mean there may be this is a part of the
data for this tier one Cassandra
instance comes out of service what I do
is stop replicating to it and then all
the machines that are in this plus it in
this availability zone talk to Cassandra
in the other zones so that that's normal
operation we're doing that continuously
we have Jenkins jobs that continuously
walk around our Cassandra clusters
running repairs and compactions and
doing software updates for the new
versions of Cassandra
so that's rolling continuously so the
individual nodes I'm going to look at
how they're Opera House set them up so
we have a base ami it's currently CentOS
we're gradually moving towards Ubuntu
but we've been spending most of it this
year doing that we haven't got there yet
it's fairly low priority job because
CentOS basically works
we run mixture of Java 6 and Java 7 the
Cassandra cluster is mostly run 7 now
within the main JVM we have Cassandra
1.09 currently we're just moving to 1.1
there's an app agent monitoring it which
does instruments the java bytecodes here
and we have some garbage and thread dump
logging on the side here outside the
main java virtual machine we have
monitoring log rotation a machine agent
that's telling us how busy linux is
things like that and we have this
service here called Prien which is our
Cassandra management process
that's running in a tomcat server which
is our standard for our platform every
instance we have basically almost every
instance runs a tomcat server normally
we deploy code into Tomcat and that
would be the main JVM on the machine
consuming most of the RAM for our
Cassandra nodes it's a little different
because we run Cassandra in a separate
JVM that consumes most of the RAM water
well actually we let's say this is a 64
68 gigabyte machine we run Cassandra in
about six or seven gigabyte heap priam
and these other things are probably
using a gigabyte and the rest of memory
is file system cache so that's that's
roughly how we configure it so Priam is
a available on github this is the the
secret sauce or that was not secret I
guess public sauce which we now have for
doing zero touch order configuration of
our Cassandra cluster so for me to
provision a Cassandra cluster I just
have to say please make 24 nodes those
24 nodes will self organize into a
twenty four node Cassandra cluster and
from me deciding to do it to have a
running cluster that I can write to
takes about five minutes five minutes
okay and three of those minutes is
Amazon creating the instances and
booting them up and then there's like a
minute or two of priam starting your
Tomcat and then it's going to decide
what to do configure things and start
the Cassandra nodes themselves so what
that's doing is figuring out what the
Cassandra configuration file should have
in them figuring out is this mute I'm a
new Cassandra node I just came into
existence am i joining an existing
cluster should I be repairing that
cluster you know because I repeat
replacing a broken node that's one case
if I'm a brand new cluster I set up as
an empty system or I can actually also
do a restore from backup and you just
tell it what time you want to restore
from and it pulls all data up to that
point in time so it's doing token
allocation assignment Okin and
I've talked about this so the other
thing that Priam does is continuously
archive into s3 all the data that's
written into Cassandra is log to disk as
single as large immutable files whenever
one of those files is written we copy
that data into s3 and we compress it as
it goes and then we can restore that
into from s3 the other finally you can
also grow and shrink the ring you can
tell it to double or half currently and
I'll talk more about that later and
those are all functions which are
implemented with pre and print also has
some monitoring interfaces think of this
as a rest-based user interface to
Cassandra on a per node basis we have a
client library that we've also written
is also open source called Aston acts
all this stuff's on github this is an
improvement of the the previous Java
client library which is called Hector
obviously we're all very familiar with
Greek mythology it's part now of the
interview questions at Netflix so
Cassandra was a Greek woman in Greek
mythology Hector was her boyfriend and a
stone axe was the son of Hector but not
actually Cassandra's son I think I'm not
sure and later on we'll run into some
more Greek without Greek characters so a
stone axe gives has a much better
abstraction and is for the RPC protocol
handling and the thread handling has a
nice fluent style API I'll show you an
example on the next slide it has a much
better retry and back off semantics and
it's also token aware and explain what
that means with arson ax comes a bunch
of useful recipes because our developers
keep doing stuff and keep asking the
same questions so we've built standard
recipes for things like distributed
rowlock which you can do with zookeeper
but we figured out how to do it without
multi data center rowlock uniqueness
multi o constraints and then this large
file storage if you want to store like
10 gigabyte chunks into Cassandra which
yeah seems like a reasonable thing to do
I guess then the problem is
that if it fails near the end of writing
a 10 gigabyte chunk you have to write
the whole chunk again so what we have is
a chunked and threaded thing which
breaks your large write into lots of
little writes and they can all be
individually retried and that's a
standard feature of the Aston acts
client this is what a query example
looks like we're not using any sequel
like language where you're sitting there
cutting and pasting strings together to
create a query we just create it
directly you can see it's this fluent
style thing ding-ding-ding
so this basically says give mice you
know here's my query is preparing the
query get the key I want to paginate it
you set it all up and then you just sear
it get the result zip through you're
done right it's a nice easy job up level
API so let's look at what Trista
Cassandra write flows look like we've
got the clients in the middle they pick
a croissant this is a six node
Cassandra cluster with triple
replication so I've got to read item to
read nodes in Zone B - yellow nodes in
zone a into blue nodes in zone C okay
and the clients from whichever zone
they're in pick a node at random send it
a request there's a fairly small chance
that the machine were there fact in this
case there's a 50% chance that that node
is actually contains the data you asked
for but the client has no idea where as
the cluster gets bigger the probability
you hit the right node reduces that node
acts as a coordinator figures out
whether data lives talks to the other
three nodes writes the data to all of
them and you can see the sequence I've
sort of walked through here the key
thing is that you get an echo soon as
you're in memory on the other nodes and
about ten seconds later it gets flushed
to disk and you could say well that's
not fully durable but I have three
different buildings that I have data in
memory and if I lose a building I still
have two more copies in memory it's
quite hard to take out three buildings
within a ten second period without
having a problem that's big enough that
you no longer care about where the
cassandra is running or not you know if
you drop an atom bomb on the side of the
US
last thing you care about is whether
Netflix is still running right so what a
stern axe does is that the client goes
in pulls a describe ring every now and
again and knows what the token range is
for each client R and what it can do is
say well this data I know is in the red
node here and it's not in this red node
here so I'm going to go directly to that
one and then that will also coordinate
the other two copies but what it means
is that if you're trying to do a read
one or a write one which is the fastest
request type in this case I actually
have two hops to get to my data there's
two Network hops which you know every
time you go across the network there's
delay so it means that there's less
latency and there's less that can go
wrong every time you go across the
network it could also fail so by having
a single network operation we reduce our
latency so all of this gives us an
improvement over their previous Hector
library that gives us more resilience
and lower latency now we run Cassandra
in multi region mode and in that case we
have a cluster in Europe and a cluster
in the US so this is running in Amazon
Ireland this is running in Amazon east
coast so here's a US client writing some
data this is the previous diagram but
the coordinator this time knows it's got
a bunch of buddies in Europe so it sends
one copy of the data to Europe that
coordinator in Europe makes the other
two copies and then those two nodes act
back to this coordinator and you finally
get an ACK the patchy the fight the ACK
happens early for these clients it
happens as soon as it's consistent
locally the remote coordinator holds a
copy of the data until it's heard from
the remote cluster so this this copy is
not delaying the write but it's making
it resilient to downtime so it'll it
says called hinted handoff and it
basically sits there and says until I've
heard that you've got the data I'm going
to keep a copy of it and once I hear
from everyone they've got it I can drop
that so what that means is the overhead
of having another remote cluster is a
little bit of extra memory thread
handling and traffic here but there's no
latency overhead to the local cluster
and that applies symmetric
so if you sign up in Europe in a few
weeks time whenever it is that we have
Denmark support you will enter your data
we will log you as a local customer here
your data will then move back to the US
and then if you happen to visit the u.s.
one second later for example because you
have a very fast aircraft or something
you'll find your customer in the US we
recognize you your foot every everyone
every member of Netflix is a global
member of Netflix it doesn't matter
where you sign up you get local content
for whatever content we've had we've
licensed in the US or the UK or or
whatever depends what country you're in
but it is a global membership model when
we were booting this up we had the US
region running with Cassandra and with
no downtime at all we set up the
European cluster and linked everything
together so to get there we actually
took backups and copied the backups to
Europe and did a restore here so this
was mostly up-to-date but a little bit
running behind then we did a global
repair to get everything back in sync
what that meant was we were it was all
faster and it put a little less traffic
on the US to go from a backup but we
could if it's a small cluster with a
small amount of data in it we just do a
sync this particular thing was for
clusters that had a lot of data we
actually use the backup to try and just
accelerate the thing so let's talk a bit
more about the backups oh I like my
analogy here by the way
it's just like taking a 737 on a
domestic flight you know and while it's
flying strapping on some more engines a
bigger fuel tanks and flying it to
Europe okay without landing it and that
worked out pretty well we've done this
for we have approximately ten or fifteen
clusters that are globally create run
run in a global mode okay for backups we
have let's see these are all the nodes
what they do is they have a they're
configured with an Amazon s3 bucket and
whenever they write to disk they write
an immutable SS table file was just
described in the previous tour and that
SS table is compressed using snappy
compression which is nice low overhead
method gives you reason about of
compression and is copied to s3
that happens incrementally as everything
writes and we do a daily full dump where
we clear out the incremental x'
afterwards but we've got then a full
snapshot so we have a daily fall and an
incremental every time you write an SS
table once a day we also take a copy of
everything that's there and copy it to
the other side of the country in a
different region with a different Amazon
account and we encrypt it and all that
kind of stuff because you get paranoid
and this is you know business protection
basically this is the master copy of
Netflix this customer database I don't
want to lose it
okay and I probably don't want you to
steal it
so we have a lot of these clusters we're
going to be open sourcing this soon but
we built a Cassandra Explorer and you
can see this is an old list of some of
the clusters that we have we just have
an endless number of them so why do we
have so many clusters because we made it
really easy for developers in a few
minutes to create their own cluster and
we encourage developers to have their
own cluster that they access that
doesn't get in anyone else's way no one
else hitting there's no one else sending
traffic to it so it's separation of
concerns makes everything very
identifiable so what this well the
problem with that is you end up with too
many clusters to monitor and what we
built was our own tooling that gives you
a list of clusters you can go in and you
can look at the schemas and key spaces
this is a sandbox with a bunch of test
stuff in it running in our test account
so our the explorers is based on a
framework it's a I think the client-side
it's mostly based on groovy Grails
back-end and I think it's d3 is the
front-end gooey stuff so there's a bunch
of basic Explorer technologies we've
used in many cases to build these kinds
of dashboards for managing stuff and
that framework should be out in the next
few months so another problem we have is
it's because we've got so many Cassandra
clusters is ETL normally you go to your
big Oracle one my sequel back-end that's
got all your tables in and you suck
everything out and you do your
integration queries and then you spit
everything into business
intelligence but the trouble is we've
deemed normalized our dataset across a
huge number of clusters and there is no
place where you can do a join and in
some cases were logging data directly
for business intelligence but in some
cases they just want everything like
they want the entire membership database
to crosslink with everything else the
more event stream stuff they'll take as
a log but sometimes some of the things
they need is the complete set so we
built a Hadoop input formatter that
reads Cassandra SS tables there is no
actual Cassandra running but it consumes
the backups and this is called a justice
justice in Greek mythology is the person
that killed Cassandra because there is
no actual Cassandra here a dead cluster
cam is imported by adjust this and we'd
basically in bulk we have a continuously
running single cursor single Hadoop job
Hadoop cluster which sucks in large
amounts of Cassandra does the joins and
the ETL and all that stuff and then
sticks it into our back-end
Tara data we have a little Tara data
it's not getting any bigger
we're surrounding it by more and more
Hadoop and the master copy of our
business intelligence software is now
moving from that being the Tara data
version which no longer has room to hold
it to being in Hadoop and that's all
running and the Hadoop is all running as
Amazon EMR in the cloud so I've been
talking about the user interface for
running this and I'll show you that what
the screenshots look like later but our
web-based cloud management tool is
called Asgard
we have we got fed up with well the team
that was doing Greek god names hasn't
there's a different team they decided to
do Norse gods for the hell of it so we
have Asgard which is where the gods that
manage the clouds live I think something
like that somebody probably there's
probably much better experts on Norse
mythology here than me
groovy Grails JVM based supports Amazon
regions it's very specific to the AWS
feature set it goes very deep but the
key thing we built was to hide the AWS
credentials to have one copy of asgard
per account and so we have a ten
at portal basically for managing test
and production and things like that so
the projects that we have the red ones
here are things that are currently on
github the blue one this is Cassandra
which is already out there but we've put
a lot of we've checked in a lot of code
into Cassandra specifically for managing
we're a major contributor to Cassandra
basically particularly for ec2 support
it just this is currently a blog post
that we you know we haven't had anyone
actually ask us to open source it yet
and then we have these two which have
been described in blog posts but we're
now planning to release them as open
source things on github circuit-breaker
is the you know release it pattern if
you've if you've been listening to
Michael no guards talks and a lot of
other talks been talking about circuit
breaker pattern so this is a Java
implementation of that that we'll be
releasing soon so let me see frame I
talked the these two I talked about
cache jmeter is a tenth a load test
suite we use for checking out
benchmarking and testing new versions of
Cassandra I'll show you that again a
little bit later we use ooh keeper for
distributed coordination so we have
zookeeper patterns that's called curator
that's probably the most popular project
we have it's got nothing to do with
cloud if you're using zookeeper you
probably want to be using curator
exhibiter is like priam but for keeping
zookeepers running in the cloud and
doing backups and things like that let's
see we have a service directory called
Eureka which is we also internally call
the discovery service we have a dynamic
property service we call our caius which
is actually a it's a kind of a gecko
it's some lizardy thing it's got nothing
to do with Greek mythology although it
sounds like it should all the scaling
scripts and things as God and the chaos
monkey I'm wearing the shirt obviously
but this thing is a robustness
verification if you look at the chaos
monkey code it looks overly complicated
for what it does the reason is it's a
framework so for supporting about ten
different varieties of monkey that are
all specialized for different tasks and
the two that we
working on a latency monkey and janitor
monkey latency monkey doesn't go out
killing things it goes around doing
latency and error injection so instead
of killing a service it causes the
service to return an arbitrary error
code or to delay the response at some
random frequency that has been extremely
useful for finding all kinds of
interesting bugs
janitor monkey cleans off unused
resources it notices that you have an
order scale group that's not been
touched for a week or so and just rude
it's basically it sort of harvesting
things they both use a database which
we've internally called entry points for
obscure reasons think of collecting
everything about your your
infrastructure you you walk through the
Amazon instances that you have new to
describe instance on everything that's
there you can enumerate everything in
your infrastructure this means you can
make strong assertions about your
infrastructure which is a very
interesting concept for security but
that's not really a subject to this talk
entry points is where we log that data
so entry points is walking the describe
calls against all the AWS entities it's
also talking to Eureka and pulling our
service directory and pulling all of our
describe data and a few other things and
it builds a lot a complete view of the
world but the interesting thing is it
doesn't throw it away it remembers the
differences so now I can get the view of
what my Amazon environment looked like a
week ago
and I can difference that to now and see
what changed which is how the janitor
monkey works it's how it knows that you
didn't touch this thing for a week
because it can see the state of it
hasn't changed and we can do interesting
things now because we have a view into
history so again that service is coming
out it's basically a big pile of JSON
objects that are being indexed in
interesting ways
and you know the fairly obvious
implementation of that is to use MongoDB
because that's kind of what MongoDB is
so that's you know it's a big JSON
integration slice-and-dice thing
Explorer as I mentioned I showed you
we're doing we're rebuilding all of our
internal
code using juice as dependency injection
framework that's going to come out soon
it's called Governator as the name of
the library the it's it's a bit sucker
extended version of juice
Odin is coming from the same teens that
did Asgard
as you may be able to guess which is
doing workflow orchestration there's
logging latency injection of bits and
pieces here oh the diner sleeves this is
part of the build system so we have a
bakery which is how we build our a.m.
eyes
dinah slave is a way to cloudburst
you're jenkins system that's sitting in
the in your build system your continuous
integration build system sitting in the
data center but every now and again you
have to do a big bill then it takes too
long
so what dinah slave does is create build
slaves in the cloud as part of an amazon
VPC extension of your data center to
handle the burst and then they disappear
again and fall back to your your data
center based sort of baseline so that's
basically it
okay so that's most of the architectural
stuff not going to talk about some
benchmarks and scalability some point
about a year ago almost a year ago we
ran some scalability benchmarks so we
took the version of Prius that we had at
the time we took the version of
Cassandra that we had at the time we're
the biggest cluster we were running were
sort of 24 to 48 nodes we were just
firing up 48 nodes and I said well you
know what happens when you scale and get
a bigger and bigger Cassandra cluster
and how far can you take it and I said
well let's keep going until we get a big
number and I decided that a million
writes a second was a big number so we
did that and data stack started
advertising that you could do a million
writes per second in the clout I mean I
was getting pop-up ads for my benchmark
it's just if you click through data
stack to site you ended up on the
Netflix blog site so that was cool it
turns out that the universal unit of
marketing is a million whatever you're
doing scale it so it was a million of
write you know units of per second per
hour per day where if you say it's a
billion it sounds like
trying trying too hard and if it's only
a thousand it's not impressive right I
mean even if it's a thousand per
microsecond and you know a billion per
month and a million right so you want to
pick a million and this is a you know I
did forty eight ninety six hundred forty
four and then we just went for it did
two hundred eighty eight nodes these
were running fairly small nodes because
I was trying scalability I didn't want
to have I wasn't going for vertical I
was going for horizontal so yeah you
could do a million rights per second
with a lot fewer nodes but I wanted to
see was it linear and this is a pretty
much as a straight line as you'll see on
a scalability graph and we got bored at
this point and we don't know it properly
goes up here somewhere right thousands
of nodes it sounds plausible the
individual nodes were about as busy at
this point as they were at this point so
the overhead of adding more capacity
goes up this is for right a hundred
percent writes now reads is a different
story and but particularly cassandra has
always been good at writes it's getting
better at reads that's just that's the
best way of thinking about it
this benchmark was run in about an hour
only needed 288 machines for an hour
cost a few hundred dollars gave it all
back we're done right this is the beauty
of benchmarking in the cloud so the next
benchmark we did was more recent and
we've been looking at this for a while
and you know where is the future where
are we going in terms of cloud and
storage and we'd been twisting Amazon's
arms for a while over solid state disks
so what we've been running on mostly to
now is these machines there m2 Forex
large they have 68 gig of ram one gig
network you can do maybe 500 i ops
there's two internal disks you can you
can get about a hundred megabytes per
second on the disks and you know they're
not they're kind of relatively expensive
but you get a decent amount of memory
and a reasonable amount of this is the
cpu so what they came out with was these
SSD based instances so you get two
terabytes of built in SSD they're
stripes of smaller instance smaller SSDs
it's got a bit more CPU power about the
same Ram 10 gig network a hundred
thousand diets and a gigabyte a second
to disk
a gigabyte per second on the network so
it's of a balanced throughput gigabyte
per second server that you can do a
hundred thousand I offset and we're okay
give us hundreds of those and we'll move
all our Cassandra cluster to X so that
was the general idea so we posted a
benchmark we did a load test driver we
had the same rest application service
and this is what we're currently running
we're running 36 m2 extra-large memcache
DS because Cassandra's not always that
good at read performance and then we
have 48 M for extra-large Cassandra's to
have enough IO bandwidth to handle the
reads and writes this is handling the
bookmarks while everybody that ever
watches a movie it remembers where you
are in that movie and that's updated
once a minute so there's a large amount
of traffic on this read and write what
we benchmarked it against was a much
smaller cluster of Cassandra nodes with
no memcache in between and the same
service hitting it so this is the
comparison that was approximately
equivalent and the since there's so many
fewer of these and we don't need any
memcache is to get good read performance
it worked out about half the cost so we
currently have about a hundred SSD
instances that Amazon has finally given
us that we can put into production
they are pretty rare right now there's a
it's a bit easier to get them in the
European clusters but that's a this was
this is kind of the future this is a
system that has an excess of i/o
capacity and it always used to be that
in the cloud you didn't have enough i/o
capacity so there's a bunch of things
that we don't do this is a slide I used
esterday but it's one of my favorite
slides right particularly when we're
benchmarking but some of that one of the
things we do do and this is going back
to the Cassandra summit a few months ago
I ran a benchmark live on stage just as
like show what it's really like to run
Cassandra I'm just gonna rerun the
results of that I'm not actually gonna
run it live and this is what I actually
put up missus I came about to start this
benchmark and you know Yolo means you
only live once this happened a week
after another
and this is an event that redefined the
meaning of the word hard for an
engineering project remember what that
project was what happened this summer
that sounded like you know that's hard
you know it was it's kind of hard you
know let's let's let's just land a truck
on Mars from a flat from a sky crane you
know that's hard what I'm doing isn't
hard that's hard right
so that was my okay you know if we could
if people if they can land a sky crane
on Mars I can do a live demo that was my
attitude there was a guy from JPL in the
audience who liked that slide so that
was cool too so we have a Jenkins
automation we have the J meter code that
we put up we have a Jenkins wrapper
around it
the traffic was reading am writing
hundred column rows we have 25 million
row keys which is sort of rough our sort
of customer base size benchmark we rent
it for 10 minutes and then doubled the
size of the cluster so 10 I mean I take
takes me 5 minutes to create this
cluster attempt intent by run traffic
into it for 10 minutes and then I tell
it to double the size and I run it for
another 10 minutes
wrap up the whole thing in about 30 40
minutes and I'm done
so let's see if I can get through this
first thing this is Asgard before I
start running you can all the one in the
middle I have an m2 test on a SSD test
the m2 is the non SSD one and I have no
instances and no order scale groups and
Dennis set this up for me here's my
Jenkins I have to build scripts and I
checked it this was I tested a few times
to see if it worked and Dennis built
this stuff for me and debugged it so
this is the bill parameters when you hit
run you get to look at all this stuff I
won't go through it in detail this is
basically controlling how many Jake how
many Jamie two drivers we need so it's a
build job that creates a benchmark and
it creates the the jmeter drivers and
then it the output of the build job is
the graphs that show what happened
during the benchmark right so the output
of the build is a bunch of plots and the
machines no longer exist they gave them
back again so it just runs to completion
so after running it a bit here we have
three aSG's one per zone we have 12
instances I've got two 12 node clusters
this is another one of our cassandra
monitoring explorer things it's called
kick loss which is Greek for circles and
these are this is I had 12 nodes running
and then as we were scaling it up you
could see the nodes appearing this thing
was updating every few seconds and it
was showing the new nodes coming in so
the the red ones are ones where
Cassandra is not yet running but it's
declared that it's Priam has told has
basically grabbed a token there's one
going to be here
it's grabbed a slot this is
bootstrapping pulling in the data the
green ones are in service and there's a
when they garbage collects or it
basically goes yellow so you can see if
you've got too much garbage collecting
going on a little bit later this one for
some reason it can finished a little
earlier this one was coming up and if
you mouse over something you can see how
much data it has on it and what else is
going on there's a bunch of other things
so out of this whole talk when we gave
it last time everyone just wanted this
tool it turns out ok so that's an it's
coming soon so this is what happened you
know it was a one-hour talk this was
where I kicked off the machines and this
is how busy the machines were and you
know this is the CPU this is the network
and this is the eye ops so you can see
here we couldn't really drive the m22 4x
larges above 500 I ops but the SSDs were
up at 3000 I ups at peak so this is the
initial running of the benchmark this is
the auto scaling or scaling it up and
then this was it I guess Drive doing
this resyncs and things ok and this is
the output of the build which is a whole
lotta graphs this is one of the graphs
which shows how many instances there
were availability grows own so I have
four and it ended up at eight which is
good and here's the read latency and
right yeah the read latency between the
two
so this is in microseconds so this is
half a millisecond so you can see that
the read latency for the SSD instance
stayed under a millisecond in the whole
time it sort of ramped up as week sent
more traffic to it but it stayed pretty
flat the read latency for the one using
regular disks was about 50 milliseconds
at peak and basically you know it was
happy until we tried order scaling it's
a pretty nasty thing to do to order
scalar Cassandra cluster has to shuffle
all the data around so the next steps
when migrating are production
Cassandra to solid state disks several
clusters has done about a hundred
running the next step from a code point
of view is to take priam and start auto
scaling Cassandra in 1.2 cassandra has V
nodes basically right now the only
sensible way of scaling Cassandra's to
double or half the size of the ring so
that each node splits to a buddy or
coalesces to two to one v nodes is an
extra level of indirection in the token
mapping that lets you add one node and
move just some data to it without
affecting too much of the rest of the
system as part of the original Dynamo
paper it's something that react already
has so it's it's not new technology it's
something was just missing from the
original implementation of Cassandra so
with V nodes we can start adding and
shrinking our cluster by one node at a
time and what will end up where there's
a cluster where you just launch it when
you start writing data to it will say
let's have no more than 100 gigabytes or
500 gigabytes of data per node and once
it happens it fills to that point it
will just start growing because capacity
growth so you never run out of disk
space your cluster just gets bigger and
then as you send it traffic it will grow
to the level of traffic
additionally right so if you need send
it enough traffic it will end up having
half the data so if you said it twice
the traffic now every night at 4 a.m. we
need much more we'd probably need much
smaller clusters than we would at 7 p.m.
so what we can do is have our Cassandra
clusters order scaling and growing and
shrinking every night and one way of
thinking about this is historically we
a lot a lot of the advances in
technology come by learning how to waste
something some some resource that used
to be really really difficult to find
and was constrained suddenly becomes
abundant and here I ops were a
constraint now they're abundant so what
I'm doing is figuring out how can I
waste my ops I'm wasting I ops here in
order to save something else and what I
can do here is waste I ops by
auto-scaling a back-end stateful server
and save on money by having fewer
instances and also make the whole
automation better so this can be sameen
SSDs are game changers for it and are
going there's going to be new
applications new things that you
couldn't do before
ok so I've got a few minutes left and
I'm going to talk about a Netflix
internal hack day project which luckily
was not actually implemented it's Skynet
okay so Skye Skynet is the computer from
the Terminator movies and luckily it's
only been implemented in PowerPoint but
I figured out how to implement Skynet in
Cassandra so those of you that don't
know what I'm talking about the plot the
bits of the plot that matter is Skynet
is a sentient computer that defends
itself if you try and turn it off ok
that was the sort of the initial part of
the plot was they tried to shut it down
and it decided it didn't want to be shut
down and since it was in charge of the
weapons systems of America it fought
back right there's a guy called Connor
who kills that eventually and then
there's terminator who sent back in time
to kill Connor and a bunch of stuff like
that right so those are the basic that's
the plot and it was a hack so these are
the actors so we have a Cassandra
cluster that detects it's going being
attacked and responds we have a Connor
monkey that kills Skynet nodes and we
have a terminator monkey that kills the
economy key notes right and there's
going it
there's the counter guy and there's the
Terminator guy so how are we going to
implement this well we're gonna have
Cass kinetic store a history of its
world everything it ever sees we'll just
log into a memory
and then it will have like a brain that
has actions and the action if it loses a
node it will obviously replace that node
but it said you know if I'm losing nodes
I should get bigger so I have more nodes
to start with so the reaction to start
to killing off a Skynet cluster is it
gets bigger and bigger and bigger the
more you try and kill it right also if
it's being targeted in one zone it
should be able to replicate itself to
another zone than another region and so
this cluster and then split so that
there's now replicas of this Skynet
cluster sort of floating around in
Amazon sort of looking in different
places and trying to hide and also if it
sees a Conan monkey it starts up a
terminator monkey to go shoot that kind
of monkey right so the implementation
plan here was in Priam we already have
Otto replacing missing nodes we know how
to grow the cluster we actually need to
figure out how to replicate to a new
zone but that's actually pretty simple
code and and move to a new region that
we've got some automation there but it's
not quite done yet so there's a little
bit of new code there we have a couple
of key spaces actions and memory and
then maybe a cron job once a minute that
just reads actions and runs whatever's
there right another interesting thing is
if if you split this off and you
actually successfully kill some of the
nodes it would actually forget things
right if the only parts of this key
space were in a certain node it would
forget but if it ran into another copy
of itself they could merge and they
could repair each other's memory so you
could have a self repairing self
learning so that means I can inject a
new action into one Cassandra Skynet
node and as it ran around it would
infect all of the other nodes with that
new action and then the chaos monkey is
pretty obvious so here's my simulation
this is us east with four Skynet's
running in it there's Connor monkey
who's popped up in Amazon Brazil because
it was far enough away to draw the
picture he zaps I Annette and takes one
out here's the Terminator monkey who
obviously is in California because
that's where the Governator lives and
Skynet repairs it
off and then he goes and he takes out
the yeah so that it's a wonderful
simulation isn't it anyway so I thought
I'd be amusing for everybody but that's
basically I'm done with slides I'll take
some questions now and as you can see
it's a good idea that we have good job
we haven't actually implemented this yet
so the question is do we anticipate
problems with SSD we're out since we're
right heavy there's two things one is
the wreck our workload isn't right heavy
we have some some we have 50 different
clusters we have everything from almost
100% right to almost 100% read and
everything in between
on the other side what does what does
SSD wear out mean when I'm renting them
by the hour I don't care that's Amazon's
problem it's baked into the cost right
if I if an SSD wears out kill the node
and get another one right they you know
if I get if I find that one of them I
might need to put some measurement in
there once they get a bit older if that
I could detect a bad one and just kill
that node and keep replacing until I get
good ones but it's not the same model
this data center where you buy stuff and
keep it for three years all right I
don't care about SSD where are basically
the other thing actually with Cassandra
there's no write amplification in
Cassandra's writes the disk it's
basically the perfect SSD workload it
does a single right it's a big right
it's sequential and it never writes it
again it never updates so and it's a big
right so and then a bit later it doesn't
merge and deletes that file so it's a
very clean interface where if you put a
b-tree on top of an SSD you're working
it really hard and it will wear out much
quicker the standard Amazon instances
have two to four disks spinning rust and
you can do you know they spin it like a
hundred Hertz right so you get you get a
few hundred I ops that's all you get two
disks two spindles worth of capacity on
the other side it's a stripe of a large
number of SSDs and SSDs there's nothing
spinning I mean even one SSD will do
cheap SSDs do ten to twenty thousand I
ups
easily and this is a this is an array of
small SSDs that have been abstracted
upto as a volume four from Amazon's
point of view so SSDs it's just rich
random Reeves flash right so that's that
you know we measured it we got a hundred
thousand reads nine hundred thousand you
know you can do a hundred thousand i up
to reads or writes depends you know the
right the right patterns matter but the
the the cassandra right pattern works
really well so yes it is a rags to
riches thing i now instead of being
constrained at a few hundred i ops i now
have just a ridiculous number and i my i
drove them to three thousand right i
have a hundred thousand i got as far as
three thousand some of the the worst we
did so far i think we got up to about
forty thousand i ops after doing some
particularly pathological stuff to one
of our clusters which was a bug but the
system was just going yeah i can do that
right so the question is if I'm sending
a remote copy of the data do i do anti
entropy on it well cassandra has several
anti entropy systems in it so the first
thing is hinted hand off so as I
mentioned when I sent data remotely I
hold the copy of that data locally until
I've heard a knowledge meant that it
arrived and if I don't hear the
acknowledgement you know after a long
period of time maybe this node crashes
or something and you've lost the
doorstep but that covers short-term loss
right for you know days even we also the
the the to maintenance operations are
focus and row and these maintenance
operations are relatively IO intensive
or one is compaction which compresses
the SS tables into fewer bigger ones
the other one is repair so repair takes
all the nodes and it builds Merkle trees
of all the data that's in each node and
then it shifts those around and then
compares them and on the regular disk
based nodes it's painful because it
overloads the nodes and you get io
shortages and you get you know it
affects your read performance when you
when you're doing this with the SSD ones
we have enough i/o bandwidth that we can
run repairs and compact
without taking the nodes offline we
typically take a node offline to do
compaction and repair on it
the trouble with repair is you're
comparing all three copies of the data
so if you're trying to get good read
bandwidth you have three copies that are
all busy so there's a thing called
staggered repair that tries to stagger
the hard work and it sort of helps a bit
it doesn't help completely the real
answer is to run on SSDs okay right
thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>