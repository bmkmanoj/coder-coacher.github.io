<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • What I Learned About Going Fast at eBay and Google • Randy Shoup | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • What I Learned About Going Fast at eBay and Google • Randy Shoup - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • What I Learned About Going Fast at eBay and Google • Randy Shoup</b></h2><h5 class="post__date">2014-11-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JP3tWboFozY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">cool effects jazz its Wow it's wonderful
acoustics in here just testing them out
before the before the talk great so as
jazz said very briefly until about a
month ago I was the CTO of a gaming
company in San Francisco called Kixeye
it's about 550 people in San Francisco
and it builds real time strategy games
for web and the browser and some of the
things that I'll talk here are going to
be about that experience before that I
was director of engineering at Google
for Google App Engine which is the
world's largest platform as a service
and many of the lessons that we'll talk
about here will be from Google and then
finally earlier in my career for about
six and a half years I was chief
engineer and distinguished architect at
eBay and I did a bunch of different
things but I mainly spent time working
on multiple generations of eBay search
infrastructure and again we'll have some
stories from there so if the question is
how do I go fast I often like to reverse
things and ask well why do organizations
go slow why aren't we always fast and
just sort of organizationally I like to
break it down into talking first about
organizational culture then about
process and finally about people so
first let's talk a little bit about
organizational culture so the first
thing that I think is important to
instill into your culture is this idea
of quality over quantity and whether
you're making a mobile app or a web
experience or an internal application
for people in your enterprise or a
real-time strategy game you want to
think about the whole user or whole
player experience and we have this
discipline in our industry and it's a
wonderful one called user experience and
I assert that the experience of the user
is more than the user experience
discipline if you see what I mean right
it's an extremely important one right
but it's beyond just the user experience
it's also the functionality it's the
performance
it's the bugs or the quality of the
software all of that stuff comes
together in the experience of the user
and frankly I mean I'll say as a user
myself I don't care whether the
unpleasantness that I'm having
an application is the cause of by
quality or operations or development or
specification or whatever I want the
experience to be good and so what does
that mean for us as developers or as
people that are producing software well
I assert that actually less is more and
what I mean here is that I'd much rather
have my team spend their effort solving
100% of one problem rather than 50% of
two problems and I don't not only prefer
that sort of as a leader but I'd also
prefer that as a user so again imagine
you know the last time that you updated
the OS on your phone or got a new app
you know you are on iOS 8 or whatever
and I doubt you said to yourself wow
it's wonderful there are 57 new features
here the thing that's what you say to
yourself is wow there is this one or two
or three new features that are relevant
to me and wow they really work
wonderfully so it's again from the users
perspective it's not about the quantity
of the output it's about the quality of
the output for the things that I care
about so how do we make that happen
again cult culture comes into play
because it talks about a good culture
has a good discipline around building
quality so I like to assert that quality
and reliability are priority zero
features you know we often think about
as we correctly should the
prioritization of features say in our
backlog and we might say priority 1
priority 2 priority 3 something like
that I assert that having the app be up
or the system be up is a priority zero
feature and again back to the user it's
equally important to the user as the
feature set so if I want to play you
know one of Kick size games and the end
the system is down it doesn't matter to
me how many wonderful features it has
right it's unavailable so how do we make
this happen without talking about
particular details
I personally assert and instill into my
teams the discipline that developers are
responsible for this full end-to-end
experience right so it's obvious that
developers ought to be responsible for
features
also claim and there's good evidence to
support it in the hedgehog community
that developers should be responsible
for the quality of the software they
should be responsible for the
performance of the software the
reliability of the software and the
manageability of the software right
as a developer all of this stuff is my
responsibility and I might have other
people in the team that will help me
with all these things and we should all
work together as a team but ultimately
when I build a feature or I add
something to the product all of these
things are my responsibility so how do
we do this this won't be a new idea for
people that come to a track like this
but again I believe that developers
should write tests and code together now
it's not particularly important to me
whether the tests come first or last or
in the middle or on the side but
ultimately before I'm done with the code
that I write for a feature or a system
I'd like yeah it's important that there
be tests so that means in practice you
know continuous testing of obviously the
quality of the features also continuous
testing of performance of the load
characteristics and why do we do this we
we know but it's worth saying explicitly
that this gives us the confidence to
make risky changes to the software right
if we know that the tests have our back
we can be extremely confident extremely
courageous in in making substantial
changes knowing that we're gonna be able
to catch any of the bugs or the issues
or performance regressions that we might
introduce and of course we catch bugs
earlier and fail faster and again we all
know it but it's worth reminding
ourselves that a bug that's caught by
the developer you know five minutes
after he or she wrote it is a lot easier
and cheaper to fix in fact orders of
magnitude easier and cheaper to fix then
something that's caught by somebody
later on several hours later in a
regression test or on another say
separate quality team you know a day or
two later it's certainly a lot less
expensive than being caught by the
actual customer right each one of those
iterations is at least an order of
magnitude maybe two in terms of cost and
latency between introduction and fix and
so it behooves us all just for an
efficient from efficiency if nothing
else to try to catch all those issues as
early as possible in the development
cycle so we often hear and you know
anybody who's been around for twenty
five years and has
either gray hair or no hair like I do
has heard something like this right we
don't have time to do it right and I
actually say wrong I say we don't have
time to do it twice right if we do
something partially if we do something
knowing that it's actually not fully
solving fully addressing the problem we
know that it has issues we're going to
have to come back around into it again
so it's it behooves us to think very
carefully upfront about trying to do the
best job we can from the beginning and
I'm not asserting we should try to build
perfect software a there's no such thing
but we should do an 80/20 type of rule
you know we should but we should spend
that 20% of our effort building the 80%
of the feature and and do the best job
that we can the first time around rather
than building something in where we know
it's it's not gonna not gonna last
and I also assert that exactly in the
case where you were constrained in other
words exactly when you are constrained
in time and resources that's exactly the
point where you don't have time to do it
twice where you should really spend the
time to do it properly the first time so
the specific example I'm going to use
here is the engineering discipline of
the group's at Google and this is a
wonderful cultural aspect of working at
Google very solid development practices
so there's no code that makes it into
the main line of source control that
doesn't have at least one review by
another Googler it's just it's it's part
of the system there's sort of no way you
can you can get around it and which is
good and again this is not something
this is something that's part of the
culture that everything else automated
tests right so you know some tests are
better than others but the discipline is
that I'm not done until it's not only
complete feature-wise but that it's also
well tested and Google despite the
extreme 'lord extremely large amount of
code it has has essentially a single
logical code repository with a few
exceptions every piece of code that's
written at Google can be looked at by
any engineer within Google and what that
leads to both of these together is
something that I'm making up this term
but it's something like an intern
open-source model so a bunch of this
code isn't available to people that
don't work at Google but the effect that
it has is that all the code in all the
repositories sort of act as an internal
open source repository and what does
that mean in practice so again as I
mentioned I was director of engineering
for Google App Engine so App Engine is a
platform as-a-service there are about 3
million different applications worldwide
that run on its infrastructure including
about 15,000 internal Google
applications so a bunch of Googlers are
directly our customers and they get to
see our code this is great so there's a
bug reporting system of course as with
as at any at any company that does
serious engineering and so if when not
if when they when our customers find
issues with App Engine they could file a
bug and that would be totally fine but
because they can see our code because we
have a code review process in place and
because they're engineers and all
motivated they oftentimes more often
than you'd expect instead of a bug
report what we'll get is here's the bug
that I found here's the place in the
code that was broken here's the fix here
is the test that demonstrates the bug
and verifies my fix awesome right
that was so wonderful and that happen
you know multiple times a week we get
submissions from people that I'd never
personally met I mean Google is a 50,000
person company and frankly often from
places that I didn't even know Google
had an office right you know a guy from
really far away you know completely
opposite time zone from from San
Francisco would submit something and
make a wonderful typically a wonderful
change to App Engine and again there's
no there's no danger there because it's
like a pull request this person this
this this engineer would submit
potential change and then we would code
review it and decide whether if we would
accept it into into the mainline and
oftentimes we would because it was again
we could look at the code we could look
at the tests it was great excellent so
the next cultural thing that I want to
talk about here is something that I call
service teams
and Amazon and Google and Netflix all
have this sort of have all evolved
independently around this similar model
where teams that build a particular
service or a particular you know product
are tend to be small and focused and the
discipline here is to make sure that a
team is responsible for you know a
single service or a set of related
services that those that that service or
services has a minimum sort of
well-defined interface to the rest of
the company or to the customers that
they have and even more importantly a
discipline around being able to rely in
a formal way on other people's work
within the company and so the discipline
that I use there is is talk about vendor
customer relationships so when and I'll
give specific examples in a moment but
when I choose to build my service or
product on top of another service
whether that service is provided
internally by somebody else at my
company or externally we should have a
kind of vendor customer relationship
there right in the same way as I would
choose a third-party vendor and sort of
evaluate them and be careful and be
explicit about whose responsibility is
what we should do we should have that
same discipline when we operate even
within our own companies so clearly that
contract if you like between the teams
you know no matter how formal you want
to make this have some idea of
functionality right I mean that's the
easy part
well we can see the specification in the
rest interface or whatever so that's
that sort of denominates or it denotes
the agreed upon sort of scope of
responsibilities you pass me this I'll
give you that but more more importantly
particularly at a Google scale it's
important to be explicit about service
levels and about performance right so if
you send me you know tenza if you call
my service ten thousand times a second
and it's ten thousand Killip there are
ten kilobytes for per requests you can
expect this particular latency
distribution right you can expect to get
an answer back in you know 20
milliseconds at the median and whatever
forty milliseconds at the naive 90th
percentile etc I mean we have this we
can have this conversation and you say
what workload you'd like to send me and
I say how much
what sort of latency and performance and
throughput characteristics you can
expect so again I'll illustrate this
with some examples that from Google so
all engineering groups at Google call
themselves services and this is not just
people who build API only service this
is consumer consumer facing things as
well so people talk people that are
around the Gmail team say oh I work on
the Gmail service people who work on App
Engine talk about I work on the App
Engine service those are consumer or you
know non-google are facing services but
there are tons and tons of internal
services that support all the all the
different products and services that
Google offers externally so one of them
is BigTable there's a famous paper about
that from 2006 well worth reading it
inspired a lot of the no sequel movement
and that's an internal service again
built by a relatively small team that
has end-to-end responsibility and full
autonomy for what they do the again
these services are sort of
self-sufficient and autonomous
right so each of these individuals
individual groups sort of act as their
own little bubble and again with this
nice well-defined interface to the rest
of the company and then they they are
layered on top of one another so in in
the example of just to take one
particular example the hosted no sequel
service that App Engine offers to its
customers it's called cloud datastore
not trying to sell you but just as an
example is something that's built by a
team of about six or eight people and
that is layered on another service
called Megastore there's a paper about
that that team itself end-to-end is six
people which is in itself layered on a
service called big table which is
layered on a service called Colossus
which is layered on a service that does
cluster management underneath and the
details aren't super important here but
what you can see are several things you
can see that there's this very explicit
layering of services and each each of
those services has its own sort of set
of responsibilities and adds a new piece
of functionality that's critical you
know layering up that stack but also
that each of that all those services
because they can rely so fully on the
underlying service to do its job
those those particularly at the top
layers
the stack those teams can be extremely
small right so the largest no see posted
no sequel service in the world can be
run by six or eight people amazing this
works because of autonomy and
accountability right so the autonomy
angle is giving those teams within their
little bubbles the freedom to choose the
appropriate technologies the appropriate
working methodologies and sort of the
appropriate working environment for
themselves and you can do that because
you've you've defined in a well-defined
way the interface to the rest of the
company so again like encapsulation of
something in a programming language you
can't see inside right it kind of
doesn't matter how it works inside as
long as it works correctly from the
perspective of the outside so autonomy
is one half of it but the other half is
accountability or responsibility right
giving teams the ability to make those
choices also means they should have the
corresponding responsibility
the Acorah sponding accountability for
the results of those choices right if
the choices go well great if the choices
go poorly
that's the team's responsibility that's
not something we can shift to someone
else so I like to hold my teams
accountable for results rather than how
they get there so again the little
mantra that I say to myself is I like to
give a team a goal rather than a
solution and let the team only a bit on
the way that they get to that goal the
best way to achieve that goal so the
example I'm going to give here is from
the gaming company Kixeye and we wanted
to move toward a micro we did and have
moved toward a micro services
architecture from something that was a
bit more monolithic quite a bit more
monolithic actually and after much
thinking and working we realized that
you know we want to build micro service
but services but it's actually hard and
we tried to and it took us longer than
we expected and we tried to figure out
why and one of the answers to the why
was that what we really needed we
thought was something that we ended up
calling the chassis and I hope people
are familiar with this term this is a
picture of a chassis of a car so a
chassis is the underbody the foundation
of an automobile and people may know
that automobile companies tend to have a
relatively small number of Cassy's right
so in
erricka there might be a chassis for
trucks there might be a chassis for
mid-sized cars or sedans there might be
another another chassis for compact cars
or small cars so a relatively small
number of chassis but each of those
chassis has a number of a large number a
relatively larger number of models of
different kinds of cars built on it
right many different kinds of trucks
built on the same chassis so that's the
analogy we were going for here there was
a wonderful team of three engineers that
volunteered for this job and I was so
happy they volunteered because I was
going to ask them to do it anyway
because they were the perfect team to do
it but they volunteered which is even
better so three excellent engineers and
they really didn't you know minimum
resources minimum time right so it was
basically three engineers and they had
about a month and I suggested not
directed but I suggested hey Netflix has
dumped some done some really excellent
open source work in this area for their
microservices architecture you really
should check it out and they said oh
great we didn't know about it we'll let
you know how it goes these guys
completely exceeded my expectations in
every way so they not only co-developed
the chassis that I mentioned you know
the underbody of a car you know
monitoring cooks and clients LED
load-balancing to downstream services
and failure tolerance and all sorts of
things that really every well-behaved
service should have they also develop a
transport layer which supported sort of
http restful style you know synchronous
request response but also a WebSockets
bi-directional style which is extremely
important in a real-time game they
developed a service template to make it
easy to build new services from within
an IDE they did a whole build pipeline
in this case it was using maven that
from that template would stamp out a
service and then again leveraging a
bunch of the wonderful Netflix tools can
deploy code in this red-black way up to
Amazon Web service doesn't have a
running service so and again we made
ended up making very heavy use of the
Netflix open source projects which were
which made it possible for this small
team to do great work and what we ended
up with was we were able to we were now
able to go in about 15 minutes from no
code at all to a running service in AWS
amazing this was possible not because my
guys did all the work it's because they
assembled together and built on top of
the work of other excellent engineers
mostly at Netflix and we thought that
this was so this was so helpful to us
and completely transformed our ability
to build new micro services so we
actually have ended up open sourcing
this ourselves so you can see the link
there to kick sighs github page where we
have open sourced this in some sense
conglomeration or aggregation of these
different Netflix open source projects
with a little bit of layering of our own
great great work so the final set of
sort of organizational culture things
I'd like to talk about is around team to
team interactions and in collaboration
so it's obvious and I hope we believe
this by being in this track but it's
again worth making explicit that the
best organizations act as one team
across all the disciplines right so
across the disciplines of engineering
product operations etc and that gives us
the ability to focus on solving problems
rather than pointing fingers right and
again I'll give an example of Google so
Google has to my mind a sort of
old-style
organizational structure but it actually
works in a wonderful way so engineer at
least when I when I was there
engineering was sort of one organization
product management and tended to be
another organization operations was yet
another organization support was yet
another organization all call rolling up
to sort of parallel fancy VPS that none
of us had ever met but different
reporting structures for the different
disciplines so you'd think that that
would be a recipe for complete disaster
but in practice what happened is
everybody who is responsible for a
particular product or a particular
service despite the organizational
structure all sat together and work
together as a virtual team with a single
goal so everybody for example that
worked on Google App Engine all sat
together in the same floor in San
Francisco well the same floor in San
Francisco and then correspondingly
another floor in Sydney Australia and
and all the disciplines work together
even though we had entirely different
organizational structures
and those co-workers were not them they
were us and the story that I like to
tell that illustrates that is this was
in the Google San Francisco office and
the facilities people the people that
actually ran the building came to us and
asked asked the leadership about how we
thought the App Engine organization was
going to grow over time and if we were
gonna need to move floors and they
wanted to do some space planning and
after at the end of this conversation
where they asked us you know do you need
to sit with this team do you need to sit
with that team etc we'd figured out you
know we don't need to sit to lift
support they're loud we don't need to
sit with developer relations they're
always speaking at conferences you know
but we do need to sit with this team and
this team and this team and she asked oh
well I see these other people these
product folks that are in your cube area
now do you need to sit with them I said
well they're us yeah and then she said
well I see these operations people and
they're you know they're in your cube
area now as well do you need to sit with
them and I said yeah they're us it never
even occurred to me that they were not
part of our organization never even
occurred to me so I like that great so
that's sort of organizational culture
now I'd like to move on to talking a
little bit about process so in a very
important process and really any
organization is instilling the process
instilling the discipline of constant
learning so we all know this but again
it's worth being explicit that any
process any organization any product can
always be improved in some way and of
course we're humans so we're always
going to make mistakes and rather than
beating ourselves up and blaming one
another let's use mistakes as a learning
opportunity so I liked it so when
something goes wrong something always
goes wrong when we have an outage or
something like that some customer
impacting failure I like to ask instead
of asking what did you do I like to ask
what did we learn and if we can take the
emotion and personalization out of it
you know if we can take the blame out of
it and replace it with a spirit of
trying to improve the system we rent
we're gonna end up being encouraging the
teams to be much more iterative and to
be able to move a lot faster
so the quotation that I like here is
from a US president around the turn of
the 20th century Theodore Roosevelt and
he says said that failure is not falling
down it's refusing to get back up right
we know we're gonna trip we know we're
gonna fall we know we're gonna make
mistakes and the key thing is not to try
to prevent ourselves from making
mistakes it's what we do after that so
again I'll use the example from Google
and this is a discipline that isn't
unique to Google it's well sort of
understood and practiced in the DevOps
community something called blame free
post-mortems so after every major
incident we have what's called a post
mortem where we document exactly what
happened we say to ourselves because
it's worth reminding ourselves what went
right because often lots even if there
was a disaster lots of things actually
did go right and did go as planned then
we document what went wrong and we have
an open and honest discussion of all the
people that were involved and affected
about what contributed to the incident
what could we have done better and what
you find is if you can again take this
emotional ization emotionalism and blame
out of it that I found over and over
again to my wonderful pleasant surprise
that engineers actually end up competing
to take responsibility so you'll have
two teams two different services that
were sort of tangentially involved in
the in the failure and you know one
service will say okay you know we did
these things and we should have had a
better run book and so on and the other
team says are you kidding you know we
weren't even taking backups and our
replication wasn't working and we'd had
data corruption I mean it's this
wonderful if you make it if you make it
about the improvement the engineer and
you make it a safe environment you know
just our engineers nobody's gonna get
fired or reprimanded everybody is very
open about their own their own
weaknesses it's wonderful and the other
reaction that is seems a little bit
counterintuitive but I probably think
that it was familiar to all of you as it
was to me it's a lot of times people
react people's reaction will be this you
know I'm really unhappy that we had that
outage but finally we can fix that
broken system that I've been complaining
about for months right people have that
experience finally we can put we can
correctly prioritize this failure that I
say
was gonna happen I just didn't know when
right anyway that's a wonderful example
of this so of course we come out of this
meeting with a set of you know action
items a set of things we're gonna do how
are we gonna change our process our
technology our documentation what could
we have done to have automated some of
these problems away how could we have
diagnosed it more quickly how could we
have returns restored service more
quickly even if we didn't know what was
wrong how could we have you know
leverage replication or redundancy in
some way to make it so the customer
experienced less downtime and then of
course the key thing once you say all
these wonderful things and we add them
to our backlog as we actually should
follow up right so weeks months whatever
your whatever is appropriate for your
iteration cycle follow up and say hey
how are we doing against these against
these issues that we found so a related
idea here in terms of process is about
iteration and experimentation and I'll
this is true really of any product but
it's particularly true of games that
it's important that we not assume that
it's gonna be perfect the first time we
launch right any online product whether
it's a game or a website or whatever we
have the opportunity to change it while
you know online and we should take
advantage of that the thing I like to
say is launch is only the first step
right particularly true for a game we
launch it and it's typically we
typically don't get everything right the
first time
we assume that we're not going to get it
perfect on the first time but what we do
is then iterate over and over and over
again making this improvement in that
improvement noticing customer feedback
making more improvements and so on and
ultimately instead of hoping for a
success planning for a success
engineering a success
and they the the what I've seen time and
time again is that many small
experiments many iterations that make
pretty small incremental improvements
end up summing over time two large wins
right if you know the analogy is kind of
like evolution right there are typically
not big changes but there are very small
changes that happen incrementally over
time which sum up to extreme extreme
changes in overall animals as we go
forward so the example I'm going to use
here is actually not from evolution but
it's from ebay so again I mentioned that
I worked on the search engine there and
people are probably familiar with the
general idea that what one of the things
that makes the search engine good
perhaps the thing that makes the search
engine good it's what's called the
ranking function it's the thing that
decides what response to your query goes
in the first position what goes in a
tenth position the hundredth the
thousandth the 10,000 etc so Google has
PageRank that was you know that
revelation in 1998 was what what allowed
Google to sort of win the search engine
wars at least in terms of at least in
the late 90s and eBay had a similar idea
so for many years
eBay 's ranking function was a
hand-tuned ranking function by it you
know very smart product manager sort of
hand-tuned these factors and it was a
very simple linear combination of
weighted combination of some factors and
to be fair to the product managers that
contributed to this they were pretty
good and it took us as I'll show you a
year for us to beat it but then we blew
past it and have kept going since our
goal we knew that we had thousands of
potential factors that we could use in
the ranking and our goal was to leverage
those thousands of factors really the
only way to do that is with machine
learning but we needed to figure out
which factors actually mattered so we
embarked on this long year-long process
of experimentation so we had a bunch of
extremely talented data scientists that
built a bunch of predictive models that
about how you know somebody who queried
for let's say an iPhone 6 what was the
likelihood that they would view a
particular item once somebody viewed a
particular item what was the probability
or the conditional probability of they
would
purchase that item etc and we were we
did for a year actually still ongoing
but particularly for that year hundreds
and hundreds of parallel Abie tests at
any given moment so we were testing
slight variations in the factors slight
variations in the weights of the factors
and how we combine them and so on
we made a full year of steady
incremental improvements and when I say
incremental I mean many improvements
that we were extremely happy about would
make a difference of you know 0.05
percent in the downstream metrics that
we were tracking so extremely small but
but statistically significant
improvements in the outcome after a year
of this after just the first year of
this the output was an increase in 2% of
the overall revenue of the EBA company
so at that time in 2010-2011 eBay's
global revenue was about 6 billion u.s.
dollars and so 2 percent of that was 120
million u.s. dollars straight to the
bottom line with a relatively small team
and a relatively focused set of effort
on just improving the ranking function
of search alone changing nothing else
about the site and as an aside at the
same time as we did this effort we also
were doing a completely separate
parallel effort on improving the speed
of the search part of the site so the
insight here was you know if people if
it's a little bit faster to find things
that people are going to buy more and
that ended up contributing a totally
independent and complementary additional
2% so in aggregate we got about 240 250
million dollars of revenue a year from
these two parallel sets of small
incremental experiments the other thing
I want to talk about process wise is
technical trade-offs and this is just
engineering that we all do we're always
whether we're aware of it or not we're
always making trade-offs and what I say
is the important discipline is not to
pretend that you're not making
trade-offs it's to be explicit that
you're making trade-offs so we're all
extremely familiar with that dreaded
triangle of you know date and features
and quality so choose to and we've
already chosen the third so for example
if you choose a particular date you put
a line in the sand or a stake in the
ground
you choose a particular feature set
given a fixed set of resources you're
going to you've already chosen a
particular quality and whether that's
the appropriate thing to do or not be
open and honest with yourself that
that's what you're doing right be open
and honest and not pretend that we can
get be perfect on all three of those
metrics and the other the other
discipline that's very related is when
not if when we make decisions that
accumulate technical debt that accrue
downstream thing is that we know we have
to come to come around and fix manage
that be aware of it be open and explicit
and honest with yourself that that's
what we're doing so plant so when we
accrue this technical debt plan for how
we're gonna pay it off right and this is
exactly analogous to how we all I hope
run our personal lives where if we take
out a loan to buy a house or to buy a
car we should have some expectation
about when and how we're gonna pay off
that loan and of course it's appropriate
just as it's appropriate for personal
financial situations it's equally
important in engineering team situation
to maintain a sustainable and well
understood level of that debt great so
the last section here I'd like to talk
about is about people and the first
thing I'd like to say is something
that's easy to say and a little bit
harder to do is hire and retain the very
best people and you know Google's not
the only place to do this Netflix feels
the same way and others is it's the
discipline is around trying to hire a
players trying to hire the best the best
people that we can and we probably we
know this a bit intuitively but there
have been a bunch of studies since the
1960s and 1970s at Yale about measuring
engineering measuring productivity
particularly in engineering disciplines
and what those studies have found
consistently is that the distinction the
difference between the best performers
no matter how we measure productivity
and the lowest performers is not a
little bit it's not like two times it's
ten times so the best performers but the
most productive people tend to be 10x
more productive
the least the least productive people so
it certainly believes you to try to hire
those 10x people and even if you crassly
only thought about money you know you
typically don't pay those 10x people 10
times the same amount so you're actually
winning on if all you think about is
money
you're winning on that score but you
also win on the coordination overhead so
people are probably familiar with with
Amdahl's law and laws about coordination
you know the more in our interactions
that you have the slower things will get
so you can imagine if we really truly
believe that you know if we accept that
some people are 10x more productive than
others then we can have a team of three
people with extremely low coordination
overhead extremely fast velocity that
could do the same work as 30 people
right which is gonna have which is going
to be individually much worse much more
expensive and also much more hard to
move forward simply because coordinating
30 people is a lot harder than
coordinating 3 the other angle here is a
future a future angle so if you if I
have a team or if you have a team of you
know eight players those three eight
players and you need to grow the team
those eight players are going to want to
hire other eight players right this is
like in professional sports right a top
people want to play with the people who
are at the best want to play with the
best and the sad truth unfortunately is
where a players want to hire other a
players that's actually not true for B
players typically the instance the the
experience is that B players often try
to hire C players and the intuition here
is a players want to play with the best
because they're confident in their own
abilities they're confident in their
ability to improve over time and they
want to test themselves B players are
often less confident in their own
ability is less confident in their
ability to improve and so even B players
would be a threat you know so the the
trajectory tends to be the B players
will higher or higher C players and
that's a sad that's a sad truth but it's
something that is something to be to be
aware of great so I'll give a particular
example of the Google hiring process so
go love the ghoul hiring process is only
to hire top talent and you know you
never make it perfectly but that's the
goal that's why the system was designed
the way it was false negatives are okay
so when I say a false negative
I mean reject so Randy applies they
reject Randy but ideally Randy would
actually be worked well at Google but we
mistakenly reject Randy this is okay
Google is very comfortable with
rejecting me find what they absolutely
do not want to do is a false positive
where they mistakenly hire me but I
really shouldn't be there does it make
sense so they're willing so as I
describe this process the goal here is
to apply an extremely harsh filter so
that only top people come out the other
end but it's okay if we lose some people
along the way now that's that's the
trade-off everything's a trade-off so
the process is a famously challenging
set of interviews particularly for
engineering disciplines very detailed
interviewer feedback right so for a
45-minute interview an interviewer is
probably going to prepare ahead of time
minimum half an hour and typically it
takes two to three hours to write up all
the feedback from the interactions that
that person had with in over those 45
minutes so it's a huge investment of
people and effort into this hiring into
the interview process unique in my
experience hiring managers actually do
not make decisions directly about
whether people are hired at Google often
they're actually not impartial about it
hey I've got this need I need to fill
and this guy's okay let's let's take
that person there's actually a separate
hiring committee that is more objective
and evaluates all the potential
candidates that come through you know in
a particular area and that person that
hiring committee makes the first order
choice should this person be hired at
Google or not and then there's a entire
second order set of choices about okay
we decided that we'd like to have this
person at Google now let's find a team
that's the right fit so there's
typically a subsequent set of interviews
which are more like having coffee and
informational style interviews that
about testing the candidate on their
skills but their about is it a right
team fit is there an interest interest
match between the team and the candidate
and the end result of this isn't I don't
try to say this is the perfect way of
doing it there are lots of different
ways of achieving the goal but what I'm
trying to illustrate here is how much
effort and thought has gone into this
particular process right whether you
agree with a detail or not a lot of
thought and a lot of effort and a lot of
metrics have gone and gone into this and
the the proofs in the pudding again it's
not the only way to do it but the end
result is that there is that on average
there is an extremely highly talented
extremely engaged engaged set of
employees at the company so once you've
hired them treat them well so in
particular this is a bit of a thing for
me is I strongly believe and it's
everybody's experience I hope that
people aren't interchangeable right you
know your life partner isn't
interchangeable with somebody else
certainly in engineering we all have
different skills and interests and
abilities even if we had exactly the
same training you know we might have
different interests in different areas
and the analogy that I like to use here
is that we shouldn't try to create a
factory where everybody is the same we
should try to create a symphony where
everybody complements one another right
so a symphony like actually might be
performed right here on this stage you
know this weekend for example is a
mixture of complementary skill sets
right the symphony doesn't have all
violins they have violins and cellos and
they have horns and woodwinds and
percussion you know all these different
skill sets each of which playing are
playing well and playing well together
all together making a wonderful
wonderful music so that's the analogy
that I like to use for teams not that I
want everybody to be the same but I want
everybody to be excellent at what they
do and complement one another in their
skills and interests people are your
most valuable and most irreplaceable
asset Moore's law does not apply to
people right the machines that we use
every day are getting cheaper and
cheaper and faster and fast
every year not true of the humans so
it's important that we treat those
valuable humans with care and respect
right and if the company values its
people and demonstrates that value
people are going to turn around and
provide good value for the company so
the counter example that I'm going to
use here is something that a development
process that was at eBay when I first
joined 10 years ago and I want to be
super explicit that eBay doesn't work
like this today it's actually a great
place to work now but it was not the
best engineering organization in 2004 so
circa you know 2004 2006 eBay's
development process went like this
somebody would propose a project so
typically it'd be a product manager we
had a great idea for a new feature or
whatever they'd have this idea great so
they'd specify a specify it was very
waterfall process which I'm not
endorsing but just to that's what it was
so here's the specification of what I
want you to build then an engineer would
be asked to estimate that to estimate
that that work so that ends the engineer
would say how many engineer weeks
how many train seats in ebay terminology
would this cost right so oh I think this
I think this feature is gonna be 10
weeks 10 engineer weeks worth of effort
great so so far so good
then then it starts to go off the rails
so that engineer that that estimated at
who implicitly had in his or her head a
design for what they were estimating was
not part of the implementation team and
correspondingly the people that
implemented that task weren't pop
weren't part of the estimation process
or the design process or that or the
sort of specification process so the
designer didn't implement the implement
didn't design yikes
extremely dysfunctional results in the
engineering culture and eBay survived
essentially despite this not because of
it
the engineers so first order problem is
engineers were treated as
interchangeable cogs because the other
thing that I didn't mention is that the
engineers that were assigned to those
implementation teams were pulled out of
a random pool like oh you know just
randomly okay I'm gonna take this
engineer and you're gonna work for four
weeks on this HTML work over here and
you know but I'm a server engineer yeah
so what it crazy you know know so very
little regard for skills for interests
for experience of the engineers
everybody was treated sort of
equivalently but the worst thing you
know the next worst thing is that for a
particular task even if everybody's
behaving perfectly well and well-meaning
nobody had any pride of ownership in
that thing right cuz again the people
that are implementing it didn't design
it didn't estimate it even worse think
about the think about the effects that
this has on the long-term codebase
because nobody has long term ownership
of the particular area of code that
they're working in right now right so
again even if everybody's behaving
rationally and well there's very little
incentive in fact no incentive to spend
extra time on making sure that this is a
maintainable thing because you know
we're now you're never gonna be back in
the same part of code ever again
so really bad really bad results overall
please don't do this
but eBay's learned from this as well and
has a much more agile process something
that would be much more familiar I hope
to people here great
so just to recap again I like you know
to answer the question how to go fast I
sort of like to think about how why
people might be slow we talked a little
bit about organizational culture we
talked a little bit about process and
then we talked a little bit about people
so now I'd like to give you an
opportunity to ask some questions in the
last few minutes thank you
okay so if you want to ask questions
please check them into the question
thingy the first question is how do you
coordinate priorities between customers
for a service team and avoid politics
excellent
so the question how do you coordinate
between customers for a service team
there's no easy answer but let's think
about it a different way let's pretend
that this service team doesn't work for
us but it's an external vendor same
problem right you know so now how does a
vendor prioritize it's the same exact
question how does a vendor prioritize
which customers are first well typically
a vendor is gonna do a cost-benefit
analysis for you know for which for the
prioritization so you know maybe the
biggest customer gets it first or if I
can make a simple change that can get us
a new customer maybe I'll do that so it
tends to be what additional incremental
benefit can I get for a particular you
know resource input in the team so so I
would I would say I would apply that
same discipline back inside so the
service team should it should talk with
all those customers and also communicate
back to them hey here's my current set
of prioritizations
and if any if there's an issue with them
like hey you know team a and Team B
disagree about the prioritization that's
something that's very appropriate to
send up the management chain right
whoever is the you know sort of decider
that come where the organization comes
together between the a and B ought to be
able to helpfully make those trade-offs
if there is no such thing or that
doesn't that takes too long then I would
make that decision based on shared
understanding of the goals of the
company right I mean I hope we can all
agree even in a large company where
nobody knows each other I hope we can
agree on you know the goals of the
company are we trying to hit revenue or
we're trying to increase market share
etc and typically when people can have a
conversation about those shared goals
typically people come up with pretty
similar prioritizations so I would I
would encourage
again that vendor customer discipline
and then I'd also encourage a sort of
collaborative discussion with with those
things because the benefit that you have
when you're all one company is that
there's no secrecy between the customers
right I mean that's typically not
something that you have as as an
external vendor you typically can't say
too much you know to customer a about
what customer B is asking for but that
is a great advantage that we have inside
a company okay one more question before
we go to it please remember to vote and
give render your feedback on the session
before you go the last question is how
do you make code reviews at Google and
what do you think is the most efficient
way to do code review great so how are
code reviews done at Google it so Google
has its own custom set of review tools
but it's very similar to several that
you can leverage outside so there are
plenty of plenty of tools that are out
in the open source and commercial worlds
that you can use internally that would
mirror Google's process so the typical
process is you know I develop some code
and before it is checked into the trunk
it lives off on a little you know branch
has a change list and when I check it in
before it goes into the trunk I say this
person should be a reviewer and that
person gets an email and then there's a
- there's a web-based tool which allows
them to very easily see the code changes
that I've made comment on them and we
can go back and forth on you know I
think you should put Const here or
you're missing the point you know that
sort of thing so we can go back and
forth and then we typically come to you
know an agreement on what should be done
and then ultimately it gets checked in I
hope that makes sense happy to answer
more detailed questions offline if that
wasn't sufficiently clear what do I
think the best way to do it is ice I
guess there are lots of different
choices here the thing I believe very
strongly in is the review happens before
it goes into you know the trunk or
production or before it has effect
I'd like to ideally the review happens
as part of the development process
whether that rather than as a developer
I think I'm done
and then I have to come back around and
do the review and I hope the intuition
makes sense there because I want the
developer as a developer myself I want
to feel like the review is not something
that's done after the fact when I'm done
it's something that's part of the
conversation part of the part of the
development process in the same way as
writing tests says and typically at
Google when I was first there and
developing building a service myself I
had lots of people that taught me a lot
about how to build things at scale and
how to build things particularly at
Google and we spent hours going back and
forth and this wasn't contentious in any
way I learned a lot the person who was
reviewing it actually learned a lot so
it actually was beneficial for both of
us
so yeah for sure before it goes into
trunk and then tool wise I like
something that allows us to annotate the
code and have a conversation back and
forth and the particular tools I'm very
agnostic about but those two things I
feel pretty strongly about and happy to
talk more about it in the break if you
like and sorry how long roughly does it
take to go from submitting the code
review request to getting the code
review in general so that's a great
question how long does it take it
depends as with any interesting question
it depends so one stupid mistake that I
did write here you know at the time 23
years of experience I submitted a
thousand line change list as my first
one because I wanted to get it just
right does that sound like a good idea
do you think the person who is my
selected code review was happy with that
he was very polite because he worked for
me but no he was pretty unhappy so
what's the best way to break up a
thousand line change list it's ten
hundred line change lists or 110 line
change lists and of course the insight
the intuition and insight there is
exactly project wise but RIT small its
incremental changes are very easy to
review for correctness and for you know
performance etc and they're much easier
on the reviewer and it leaves the
cognitive load is much smaller for both
the reviewer and the and the writer and
so the discipline that I had
subsequently to my massive thousand line
change list which actually grew after
the review to like eighteen hundred
lines you know too bad for the other guy
and I learned my lesson so that took you
know days back and forth actually but it
was improving every time and there was
other work to do so it wasn't like I was
totally blocked but what I learned from
that was what I would have preached you
know but didn't follow is do small
change lists you know small deltas and
review them as you go along and that's
much easier on everybody yeah okay thank
you very much Lundy thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>