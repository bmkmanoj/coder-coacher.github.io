<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • Fitting the Model • Sean Treadway | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • Fitting the Model • Sean Treadway - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • Fitting the Model • Sean Treadway</b></h2><h5 class="post__date">2013-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iEVhHUlPw1c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today I want to present to you a use
case how under why of why we chose a
database to fit our data model rather
than trying to adapt our data model to
our database so with relational stores
you've pretty much taught there's one
size fits all for your database so
figure out your data model and you'll be
fine and we did the opposite so first an
overview of what we do at soundcloud
sound clouds a social platform you can
think of it like flickr for sound or
youtube for sound and social sounds are
at our core so these are these are the
sounds the little wave forms and social
activity around them like likes and
comments and favoritings sharings the
this is how the sounds propagate and in
this network on our site on our devices
and also other social networks like
Facebook and Twitter so when logging
into soundcloud you get this page
overview of the sounds inside of the
network sounds that other creators have
uploaded and we're working hard on
fine-tuning that listening experience so
if you'd like to see our our next a next
website you can go to next stop sound
gotta calm and I get a preview we've
seen some pretty good adoption and some
success we serve a lot and we store a
lot and there's a 20 million registered
users that are also helping us unmute
the web but this is going to be a talk
about databases and so after introducing
soundcloud when I introduce this talk so
thinking about what to present I often
go back and I say what should i tell my
past self so he can be smarter and in
this talk I wanted to ask myself what
was the most important lesson that we
learned at soundcloud about our database
usage and save you the suspense it's
simply understand what your database is
for you so to put that in perspective if
this was a
architecture or if this conference was
for building architects that answer may
come in the form of understand what
you're building materials do for you and
it's not too far off in the world of
structural engineering it's still
putting boxes together maybe using
concrete instead of arrows and wanted to
shine a light on a dirty little secret
in our craft it not everything works as
expected so believe it or not much of
the books that we read are coming from
experience of things that don't work but
there's a definite lack of sharing these
stories in the conferences like this and
I want to break out of that trend and
and talk about stuff that hasn't worked
so well as much as stuff that has worked
so I'm yeah if we're if we're using the
metaphor of building materials we also
need a metaphor of abstractions we need
these abstractions in our jobs to do our
jobs effectively we can't be laying
bricks every day and but in our world of
engineering we live under the
constraints of physicality so we have
space and time and the laws of physics
as our core constraints as engineers so
those constraints from the base of our
materials and our abstractions form the
rest they form the art of aircraft so as
an industry we often put our our
blinders of abstraction on so this means
that you may be looking at your problem
or may be looking at your source code
through the facade of your IDE you may
be looking at your solutions at the
stock imagery of a vendor's brochure or
you may even be looking at your
architecture with boxes and arrows
they're all abstractions that help us do
our jobs effectively but it's too soon
that we forget the limitations of the
physical world and how that applies to
the abstractions that we use so seen
with these eyes I'm going to introduce
the case study
first I'm gonna weigh sorry this this
case studies going to go into details
somewhat in depth but I still consider
them high enough level where I feel that
they're important for all of us to know
is we're going to talk about MySQL this
was the data store that we used for most
of our online data and Cassandra and
this is the data store that we chose to
replace a mysql for specific use case
databases are different things to
different people and so i'm going to
limit the scope of this talk to just
persistence so i'm not going to talk
about acid atomicity consistency
durability or transaction ality or base
I can't even remember what basse dance
floor I'm just going to talk about you
write it you read it it doesn't go away
and there may be some some kind of
low-level details I have time after the
talk to go into more depth I have a
little bit of time before catching a
plane is so I'm going to try to leave
some time at the end of the talk for
questions in the audience and then a bit
at the podium and some of these details
may be old hat for you but whether or
not the message is still the same
understand your materials I'm going to
use some terminology called primary
storage and secondary storage just to
clear that up primary storage is the
storage attached to your CPU secondary
storage is the storage attached over
your i/o bus primary storage is your RAM
secondary storage are your disks and the
materials I'm going to be talking about
are going to be spinning disks so
sometime during the talk you may think
oh just why are you wasting your life
use SSDs and everything will be okay and
that's not the point the point is if you
do use SSDs understand them if you don't
need to go to secondary storage
understand your primary storage if your
solution fits in RAM understand how your
DRAM works understand
how your memory bus interacts with your
CPU and how your CPU interacts with its
caches and if you have massive storage
on the exabyte scale if you're from
google here your materials will be
magnetic tape so you may also think I'm
in the cloud I don't have to worry about
storage I'm in the cloud I don't have to
worry about Hardware the cloud does the
hardware for me but the clouds also
based on the same first principles that
we deal with his engineers based on the
same materials and you can't ignore you
can't ignore the the hardware that runs
the cloud so the lessons learned here
apply equally well in the cloud so the
data that we're storing and serving it
composes our activity feed it's the it's
the dashboard page after you land after
you log in and this display changes when
somebody in your social network either
contributes or creates a a social link
between them and a sound or whether they
upload a sound themselves so this is
going to be the scope of the data model
that we've taken out of our primary
storage sorry with primary database and
have built a specialized solution for
this feed for the sake of the product
needs to be consistent so if you log in
once and you see these these tracks and
then you log in again and the third
track has disappeared that's an
inconsistent experience and so for us as
in the product development it was very
important to have that consistent
experience and that meant saving off for
us the easiest thing that we could do is
save that off beforehand and display it
once you log back in as a graph of our
data model we have viewers and creators
these are both the users there's a
relation of following there's relations
to creations in the site and then
there's a transitive reduction from
viewers to sounds over creators and
their creations we chose to
who materialized the first degree of
this traversal and what we call events
there's other options when building an
activity stream you can also materialize
that page on read but we chose to
materialize it on right for the goal of
maintaining a consistent experience and
having it load quickly so we wanted to
pre-compute all of the events so they
would be ready when the next next time
that anybody logs in so we wanted our
goals where latency and and a consistent
experience as a entity relation diagram
kind of an incomplete diagram but this
is how you can model it with this is how
you typically visualize it in a
relational store and we just took this
mapping to the first degree of creations
and and save that off into its own table
yeah this worked out really well and
there's some benefits for for having
this because at any point in time we can
go back and reconstruct the dashboard
it's also robust against changes to our
algorithm so if we say that we're going
to introduce or remove creations the
prior creations don't change and so you
can maintain that consistent experience
and the event store in MySQL worked out
pretty well but we extrapolated some of
our growth in the relation of how much
we're restoring versus how many
registered accounts we were seeing and
we quickly realized that we could be in
trouble but in fact we were actually in
trouble and we were seeing up to two
seconds of load times for some of these
dashboards and something that we tried
to optimize for up up front by doing all
this work calculating everybody's
dashboard before they even login and so
we were we were awful like how can we do
this we're writing fine but we're
reading slow this is completely opposite
to our intuition of what relational
databases are good for
and and so we sat down we tried to
understand a little bit of why this was
going on and I'm going to jump around in
time in the past and actually describe
what was going on because it shouldn't
have been this way we were we were
writing just fine because of some
natural properties of our data set so to
illustrate that the the table that we
were writing at a surrogate key meaning
that this was a sequence number sequence
that was managed by the database this
sequence created an ordering that
closely matched the ordering of events
that were coming in and this acted like
a sequential clock and as a result the
database could use that to its advantage
now to understand what the database is
doing for you we need to understand how
the database is storage engine interacts
with the material and by using an ODB
storage engine the common storage engine
for MySQL actually an awesome storage
engine from MySQL it can throw it
whatever and it's not going to lose your
data and it's optimized for a broad
class of access patterns it's really
just the go-to general-purpose storage
engine for mutable mutable state and the
way it manages this is it carefully
balances a b-tree actually be plus tree
on disk and this data structure has some
branches and in its leaves it stores the
records so in an ODB everything's a
b-tree even the even the records of
themselves are stored as a b-tree
they're actually stored in an index form
and it not been in ODB this the primary
storage for nodb is as a clustered index
meaning that they're ordered the records
are ordered off of the primary key so
with that sequential primary key we get
our
leaves which contain our records grouped
into pages which contain the i/o unit
nicely laid out sequentially on disk so
the on disk format which we weren't
really aware of at the time it turned
out to be quite efficient this is why we
were able to accommodate so many
rewrites per second because we were just
keeping that order and writing right one
after the other adding one page after
the other but on Reed we couldn't read
in this order we needed to read off of
those viewer IDs that were coming in
each viewer gets a unique index a unique
view and as such we needed to index our
data set so that each viewer could pull
out those records and we could follow
the next degree of the traversals and so
nodb being a super bee tree stores the
index in index order in a bee tree as
well and on this this turns out to be
have the exact same format the records
traversable to the leaves into pages
with the values of the Leafs being the
primary key of the item that the
secondary index is indexing so what this
that what happens when when adding and
updating the indexes is that when as
you're as you're adding more and more
items to an individual user's activity
stream it may actually fill up one of
these pages so one of these I 0 units is
16 kilobytes and if you've storing more
than 16 kilobytes of primary keys in
this nodb performs a page split the
it'll bump up against the next page so
there's no more room next to each other
so I don't need to go off and create a
new page typically at the end of the
tree and fill that up with your index so
this is one characteristic of B trees
when you run out of space
you need to go somewhere else but once
you do get those primary keys and you go
into the clustered index primary index I
you may be loading up different pages
just to get a single record out of each
page and each of the pages that you load
could potentially cause a Sikh and this
may be a review for a lot of you but I
really want to drill it in that that
that why Sikhs are important when
understanding your database because this
is what this is what got us the database
is simply an API to your storage medium
that gives you some abstractions to
reason about your data but seeks are not
something that we can we can ignore
quickly overview the disks are spinning
you can pull data off of it or put data
on to it as quickly as they're spinning
but you need to locate to where that is
and yeah when the when you locate to
where to put the data on the physical
device you may need to move for where
you are and that causes a Sikh it's
difficult to conceptualize how bad these
actually are without understanding a
little bit or having a mental model of
time and I'm it I think it's important
for all of us to strengthen our mental
model so this is a nanosecond and it's
kind of hard to visualize because we
have no sense i mean this you make toast
and it pops out in a nanosecond or or
what there's nothing that we deal with
except our computers that work in this
time scale so if we translate this time
to distance we can get a little bit more
intuition about how long that actually
is so our everyday you can't go faster
than the speed of light unless you watch
Damian's talk and you're using Perl so
if we if we go the speed of light in one
nanosecond that equates to the distance
of about this much about the long side
of a piece of paper about 30 centimeters
so that's that's a nanosecond that's a
that's one nanosecond
this is the fundamental unit of our
computers this is what we're processing
our code at this is what the this is the
unit of measurement that we measure our
inner functions this is this these are
the fundamental units of our computing
universe but yet these are the units for
seek it could take up to 10 milliseconds
to seek and so how how long is a
secretive to a nanosecond how long is
going to disk relative to staying in
code and how far could an electron
travel at the speed of light in 10
milliseconds so to give you an idea 10
milliseconds for an electron is like
going to Milan from here and coming back
so it's like a round trip to Italy to
send something to your disk and this is
why we started to see those two second
response times so once you have a Sikh
you pretty much know where your
bottlenecks going to be next next thing
that we saw is that it wasn't just six
it was also storage so from our growth
we knew that we couldn't keep all of our
events on one host and we also knew that
we were creating more and more pages for
our index and and this was all for what
was ultimately mostly cold data mostly
data that would never get loaded in the
in one day so the more storage we had
the more space the more storage that we
required the more space between our
pages that we were seeing and the more
seeks that we were incurring and yeah
and we also knew from extrapolating our
growth that we wouldn't be able to fit
on one host for very much longer so we
took we took a 10,000 foot view and as a
reminder we didn't know this back then
so we didn't know it what I just
described and so we only had a tenth
doesn't
10,000 foot view back then we didn't
realize that what we were having was a
disc problem what we saw was we were
having a database problem the
documentation about all this is tucked
away on MySQL documentation site so we
eventually did learn it but at the time
all we saw was outgrowing our storage
capacity of getting response times that
were out of reason and yeah and not not
having a way of mitigating it with the
database that we were running on so we
have some tough questions to ask we
needed to ask ourselves do we want to
keep this materialization on right
strategy did we want to change
everything and go with the
materialization on Reed and then we
considered the product and we consider
the amount of change that it would take
to completely change the way that we are
displaying the activity feed and we said
yeah let's stick with the
materialization on right and let's look
at the databases but before looking at
the databases we we spent a little bit
of time understanding our data model and
yeah some of the some of the properties
of our data model work quite beneficial
it's eventually consistent and by this
meat I mean eventually consistent in
that the rights are observable in the
same order by any reader but at
different times so if you write if
there's rights of a B and C every reader
will see a B and C but they may not see
a at the same time and they may not see
be at the same time but this is fine
because we just have one reader so if
you only have one reader everything can
be eventually consistent as long as the
reader is not the writer and on top of
that because people don't log in not
everybody logs in at the same time I
could be multiple days between logins
and so a reed could have been actually
much much later than I right
we also knew we had hot rights all the
contents being created right now and so
if we're going to materialize that first
degree of our graph we are writing all
at the same place and no matter how fast
you can go if you have a hot spot you're
likely going to run into a bottleneck
eventually so we knew that we needed to
have at least some measure to be able to
distribute these rights over multiple
locations but identifying your hot spots
also means that you can use some
techniques you can stay in primary
storage for a little bit longer before
going to secondary storage so you can
basically buffer in RAM before writing
to disk and this will mean this means
that you can accept more at a higher
right rate and then flush out we also
had cold reads so most of the pages that
we were rendering we're going to be
rendering from storage there was no way
that we could keep everybody's dashboard
in RAM this would be totally wasteful a
use of of RAM and we could count on the
viewers once they load their dashboard
to only only load the head of the
dashboard sequentially ordered by time
once they once they find their index we
also because we didn't want to stay with
we wanted to avoid having hot spots on
rights and we wanted to distribute the
cold reads and we knew we weren't grown
able to fit on one host we knew our data
set needed to be partitioned and this
was a one of the more important things
that we identified is that we identified
our partition key the thing that we
could split on and the nature of the
data model is that is only the viewers
that need their data set so we could
partition on the viewer key or the
viewer identifier and by knowing this a
lot of options came up because we only
needed to be consistent on a single
partition for a single viewer ID
so given with that we yeah we stuck with
the storage heavy approach and we needed
to grow over multiple hosts so there's
really only one thing that we could do
let's throw hardware at the problem so
we knew we had to throw harder at the
problem and I thought we needed a new
database we thought because we thought
that okay we have multiple machines we
need a database that can live on
multiple machines relational databases
can live on multiple machines but we saw
that there was this was in 2009 we saw a
lot of new technology show up on the
market that ad promises of just making
partition making partitioning a
non-issue and so we evaluated a little
bit of the market but we also evaluated
our workflow and how our workflow would
integrate with the databases that we
were looking at ultimately when you have
data in one form and you need to get it
to another form you're going to be
applying this age-old technique of an
ETL this is fine this is eventually
consistent in its core because I extract
each of these steps happen in different
transactions and or could happen in
different transactions and for us the
extract was taking the changes as they
were happening the transform was finding
those edges to materialize first and it
was the load part that we were having
problems with because we needed to write
enough edges so this is a in the orders
of hundreds of thousands a second to
materialize our index and we needed to
load them in a way that was reasonable
across multiple hosts but what we really
wanted was to take advantage of our
right hotspots and write everything into
memory and then take advantage of our
secondary storage and read everything
from disk and somewhere in between we
didn't want to have to do any work and
this is where we had this dream of like
what if we could do this
so we started to evaluate our databases
based off of whether they can fill in
the magic happens part and it turns out
we found one so cassandra has this bake
10 but it also had a lot of other
properties that were quite beneficial so
being ignorant that we actually had a
disc problem we decided to throw
cassandra at it because it it aligned
with our data our data model and it also
offered some conveniences to make our
ETL simpler so cassandra is eventually
consistent in its core so you've
probably heard a lot about the cap
theorem and cassandra is in the AP
section of the cap theorem I and every
every observer every reader on any node
will see the rights that happen even on
other nodes eventually every note is a
master and the slave so you can connect
to any node in in your database cluster
and read and write to it you can think
of it as like a distributed hash table
where you whether the rights will get
routed to the to the right host and
there's well understood a multi data
center replication so I believe later in
the track Adrian will talk about how
Netflix achieves this with multi-zone
replication which is the same thing as
multi data center replication multi-zone
multi-region and yeah and this ten
rights are distributed so as the rights
come in we can split out and avoid those
hot spots by distributing our rights
across different nodes in the cluster
because you can write to any any single
one cassandra also offers this so the
the beautiful nature of our right path
with that sequential with the sequential
writes nice ordered data set
and then sequential reads as well means
that we can actually we could it gives
us the promise of no more trips to Milan
seeing this I just imagined okay yeah
seeks can get under control but how
Cassandra does this is an understanding
of its data model and it's also an its
storage model so first to start with
Cassandra's data model which is
important to understand that so to know
how Cassandra turns its data model into
on disk storage and being in a
distraction over the storage layer of we
first need to understand what what kind
of abstractions we have to work with so
I've never liked SQL when I started us
started writing SQL statements I I tried
to do this so I imagined okay I have a
little bit of data over here a little
bit of date over here make the same
schema and then I want to choose which
data I'm going to query from and this
has always bugged me because
conceptually I group my I group my
partitions in tables mentally and I want
to select which table i'm looking into
so i didn't have any theory about why
the this language doesn't allow that but
i've always found it annoying but if we
could do this and we could create 20
million tables then partitioning is a
no-brainer we just select the users out
of the user to each of the user IDs
tables and that would have been great oh
yeah and Cassandra yeah cassandra has
this model baked in but I've also wanted
to do this so I didn't know about the
entity attribute value pattern I guess
where you can create a table and then
you can with that table you can create
arbitrary attributes sets I always
wanted to know what attributes i have
available and so I make these super wide
tables I mean this was a long time ago
make super wide tables and then say i
want that column that call
in that column I want to know what
columns are there so I'm going to make a
very wide table but again you can't do
this with SQL but even if you could do
it in consent or you there's a there's a
trade-off in Cassandra you don't get a
flexible order by statement you can only
order by your columns so this is okay it
just means you have to pick your columns
carefully and yeah that's that's about
it for this kind of butchering of the
SQL metaphor so if we look at the actual
terminology you can think of the data
model and can Sandra like a giant matrix
you have row keys column names and then
the values and at the intersection of
the row key and the column name you can
find your value and the important
difference when considering this data
model versus a relational model is that
you can only scan over the columns you
you don't scan over the rows and in
concentra the order in which you scan
over the columns is defined in your
schema so it's like setting that order
by statement but up front it's possible
to tomorrow a data set like you would
with a relational model like if you were
to do a user's table you just pick some
column names which are stable so you can
have your your name your avatar URL your
email and this would be fine but which
you sacrifices that you can't scan give
me the all the roads that have a first
name of Bob Cassandra does have
secondary indexes not going to go into
those but this would be a typical entity
modeling in Cassandra Cassandra can do a
little bit more
so if you use the same key as your roki
is as you're in your column name you
could easily model and adjacency matrix
and with this you can model a directed
graph and so the columns just grow the
Rose grow and you can scan over give me
all the outbound edges that I have for
any given node it's also possible to
model our existing activity stream so if
we had a globally consistent sequence
generator which was generating those
sequins IDs we could use the column
comparator in and order our columns in
the same sequence order that that we had
in our relational store but if we didn't
have that globally consistent counter we
could end up creating duplicates and
it's not as important with our dataset
but we decided to model it slightly
differently to make the duplicates
impossible and we did this by using the
clock of time but even time can have
duplicates because not all clocks are
unique so to make them unique we encoded
them as you you IDs so it's a variant of
version 1 u.u IDs where the high bits
are where the timestamp lives low bits
or where the uniqueness lives so this
this is no SQL for me this is no
skewville for me is when you model your
data how you want to read it this is how
we're going to access it we're going to
access it in the order of time based off
of our column comparator and we're going
to create a very sparse matrix of this
and we're just going to find the row
find the columns and we'll be done with
it and I think most of the no SQL
databases provide means to have finer
control of your access patterns based
off of how you model it your data in the
database but for rights
we need to understand Cassandra's
storage model which is unique it's
different than then in ODB's but first
we get a little bit of the partitioning
out of the way I'm not going to go into
depth about partitioning it's one of its
strengths and it has a lot of coverage
so you can read up about it later but
the quick overview is that for any given
roki it only uses the RO key and then
finds a host to store and retrieve that
row from it can be more than one hosts
if you have redundancy and we use the
random host partitioning meaning that
any given roki could be on any host but
is predictable this is why we can't do
row scans because the row keys may not
be sequential on in each of the nodes so
the storage engine in mysql i'm sorry in
cassandra is based off of section in the
Google's BigTable paper which splits the
problem exactly where we would like to
have it split so for for rights and for
hot reads the primary storage is used in
a data structure called mem tables or
memory tables these are you can think of
them as just hash tables and for
secondary storage for our rights and
also our cold reads Cassandra our the
big table paper introduces the concept
of sorted string tables the sort of
string tables are where the magic
happens because if you have a sort of
data structure you can actually infer
quite a bit your searches into it become
a whole like a degree simpler in
complexity and and this sorted string
table allows you to maintain that those
sequential reads and the sequential
writes so in memory as the rights are
coming in they're coming in in any order
they could be any Roky any column name
it doesn't really matter because they're
just going to primary stores they're
just going to ramp
and so if you have a read immediately
after a right you don't even hit the
disc cassandra is durable so it'll it'll
commit to a commit log before actually
committing to the right or the mem table
so it's still fast to accept rights but
when that table fills up and it will
fill up as you're adding stuff work or
adding changes it periodically needs to
flush out of ram and and this is where
cassandra and also HBase will take take
the mem tables sort them off the Roku
then sort them off the column names to
create a stable sordid structure and
it'll take that and just sequentially
write it to disk and then start with a
new men table it's important to know
that that nothing is overwritten on disk
so in the b-tree nodb spends a lot of
work reusing portions of the page we're
using portions of the pages that the
leaves make sure that like if you run
out of space on the leave it'll leaf
it'll add a new one until it does a lot
of magic in place on this can that keeps
it keeps the data set pretty dense here
it's just flushed out but what that
means is that as your fleshing memory
tables out to SS tables on disk you're
going to be generating a lot of s's
tables because it's only going to be
what you were storing in RAM this could
be yeah 100 100 Meg's tops not even that
is so I think we're at net to Meg's so
but we actually flush even smaller or
smaller sizes so so what you end up with
is a directory full of SS tables and
when performing a read your columns
could be living in more than one table
and so Cassandra keeps some bookkeeping
around of what what columns are and what
rows are in which SS tables and then it
needs to consult all of them
to satisfy your read so you can see if
there's really not much difference if we
if we had all these SS tables on disk
this would be very similar to having all
of our page splits in our bee tree and
we would be we'd be seeking all over the
place to try to satisfy a read so
cassandra has a compaction phase which
will read these numbers which will read
a set of SS tables in since they're
already sorted the merge is incredibly
efficient it'll flush that out
sequentially and this is a constant
background process that happens that
converges the SS tables to reduce the
number of tables that are needed to be
consulted before a read can be satisfied
but even so you can't keep everything
compacted at the same time so you would
think that you would be just trading
Sikhs in the b-tree to Sikhs across the
SS tables for us we have a maximum of 20
SS tables that we consult for our
dashboards so this is this is from a
couple days ago a histogram of number of
SS tables consulted for all of our reads
bucketed by the number of s's tables an
important thing to note is that the
trend goes left so the trend in
cassandra is to reduce the number of
sikhs reduce the number of tables that
need to be consulted or leaves that need
to be consulted we're in the way that we
are using the b-tree in mysql that trend
was the other direction so it's too
seeks / SS table and at2020 SS tables as
a maximum for satisfying our reads we
now know our upper bound our / bounds 46
and with this we we have a great we have
a much more confidence but even at 46
one would think consulting all those
tables for cold reads would increase
latency
so even in out in milan land where we're
consulting many of their tables were
still much better off than we were with
mysql so we're under under 200
milliseconds in our upper bound for the
for satisfying a dashboard read and
these are our cold reads these are the
reads that for somebody that hasn't
logged in for a while that that needs to
pull that up into ram our average is 20
milliseconds and that's still a couple
trips to milan but it's much better than
we were doing before and writes our
whole different unit of measurement so
when you see a distribution like this
you you know you're doing it right
because it's nice and tight you don't
have any outliers and you can just
pretty much ignore the units because it
because you're doing your you have a
predictable distribution but for us this
is in microseconds and at this load our
cluster can easily take bursts of 75,000
writes per second so 75,000
materializations of edges per second so
we did it so we've addressed some of the
concerns about partitioning with
Cassandra we're accepting a lot of our a
lot of rights from all of these edges
that we're materializing and we've
stabilized our load times and with the
partitioning techniques that Cassandra
gave we were able to actually perform a
live datacenter migration so we set this
up before we had moved from England to
Amsterdam and this this database or this
database cluster was the only thing that
was on line the entire time during the
during the transition we also could
accept rights from multiple data centers
so we were writing an Amsterdam writing
in England and you may find this amazing
but those metrics on those Layton sees
were from a cluster that's running an
ec2 with a notoriously slow IO IO times
so
with Cassandra and in the SS tables and
nodb with its be trees they're using the
discs effectively but in different ways
so Cassandra accepts random writes very
well much better than nodb but as in a
DBZ creating random writes by
maintaining the index but once that
index is established in 0 DB does much
better for performing reads because it
can't reverse that that tree that with
fewer hops then the number of SS tables
that you need to consult but it hasn't
been such a great experience with
Cassandra we spend a lot of time on
operations of our clusters and we have
about like 5 to 10 hours a week of just
a be sitting it there's alerts that go
off that where nodes can't communicate
it's distributed but and we understand
why it's happening but we still have to
respond to it periodically we have to
intervene and say yeah you need to go
down for a while or you've been down for
too long and we're going to bring up
another and nom as a result it we
haven't quite worked out those those
kinks but if you do go with Cassandra
you can expect trading trading at a time
for for operation of anything larger
than a 5 node cluster and in in
hindsight we know now how exactly we
would have built it with MySQL so the
problem that we were having of our index
splitting could easily be mitigated by
optimizing our table periodically
rewriting the entire be tree and
splitting that up into smaller chunks so
it doesn't create outages or offering up
our workflow our ETL and RAM before
riding out to MySQL to avoid being down
during that time so to wrap this up of
how we fit our database to our data
model we did a lot of right things a lot
of smart things along the way we found
our natural partition key
we isolated a reed path from a right
path we identified the volume and the
access patterns both for our read and
write loads whether they were sequential
or random and we developed a sympathy
for the materials that we're using but
most importantly we never forgot our
goal of delivering a product that kicked
ass so we architected first without
fully understanding what our building
materials were because we assume that
our database is going to hide that from
us so whether you try to fit your data
model to your database or whether you
choose to fit your database to your data
model ultimately the success of your
design and the ability to hide your
ignorance to the public is really based
on how well you understand the materials
that you're working with so that's it
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>