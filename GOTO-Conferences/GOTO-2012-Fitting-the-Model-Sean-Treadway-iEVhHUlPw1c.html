<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • Fitting the Model • Sean Treadway | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • Fitting the Model • Sean Treadway - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • Fitting the Model • Sean Treadway</b></h2><h5 class="post__date">2013-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iEVhHUlPw1c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today I want to present to you a use
case how under why of why we chose a
database to fit our data model rather
than trying to adapt our data model to
our database so with relational stores
you've pretty much taught there's one
size fits all for your database so
figure out your data model and you'll be
fine and we did the opposite so first an
overview of what we do at SoundCloud
those sound clouds a social platform you
can think of it like Flickr for sound or
YouTube for sound and social sounds are
at our core so these are these are the
sounds the little wave forms and social
activity around them like likes and
comments and favoriting z' sharings the
this is how the sounds propagate and in
this network on our site on our devices
and also other social networks like
Facebook and Twitter so when logging
into SoundCloud you get this page
overview of the sounds inside of the
network sounds that other Creators about
bloated and we're working hard on
fine-tuning that listening experience so
if you'd like to see our our next next
website you can go to next stop
SoundCloud a calm and I get a preview
we've seen some pretty good adoption and
some success we serve a lot and we store
a lot and there's a 20 million
registered users that are also helping
us on you at the web but this is gonna
be a talk about databases and so after
introducing SoundCloud want to introduce
this talk so thinking about what to
present I often go back and I say say
what should I tell my past self so he
can be smarter and in this talk I wanted
to ask myself what was the most
important lesson that we learned at
SoundCloud about our database usage and
save you the suspense
it's simply understand what your
database is for you so to put that in
perspective if this was a
a architecture or if if this conference
was for building architects that answer
may come in the form of understand what
your building materials do for you and
it's not too far off in the world of
structural engineering it's still
putting boxes together maybe using
concrete instead of arrows and I wanted
to shine a light on a dirty little
secret in our craft it not everything
works as expected
so believe it or not much of the books
that we read are coming from experience
of things that don't work but there's a
definite lack of sharing these stories
in the conferences like this and I want
to break out of that trend and and talk
about stuff that hasn't worked so well
as much as stuff that has worked so I'm
yeah if we're if we're using the
metaphor of building materials we also
need a metaphor of abstractions we need
these abstractions in our jobs to do our
jobs effectively we can't be laying
bricks every day and but in our world of
engineering we live under the
constraints of physicality so we have
space and time and the laws of physics
as our core constraints as engineers so
those constraints from the base of our
materials and our abstractions form the
rest they form the art of our craft
so as an industry we often put our our
blinders of abstraction on so this means
that you may be looking at your problem
or may be looking at your source code
through the facade of your IDE you may
be looking at your solutions at the
stock imagery of a vendors brochure or
you may even be looking at your
architecture with boxes and arrows
they're all abstractions that help us do
our jobs effectively but it's too soon
that we forget the limitations of the
physical world and how that applies to
the abstractions that we use so seen
with these eyes I'm going to introduce
the case study
but first I'm gonna okay sorry have this
this case studies gonna go into details
somewhat in depth but I still consider
them high enough level where I feel that
they're important for all of us to know
is we're going to talk about MySQL this
was the data store that we used for most
of our online data and cassandra and
this is the data store that we chose to
replace mysql for specific use case
databases are different things to
different people and so i'm gonna limit
the scope of this talk to just
persistence so i'm not going to talk
about acid atomicity consistency
durability or transactionality or base i
can't even remember what base dancefloor
I'm just gonna talk about you write it
you read it it doesn't go away and there
may be some some kind of blow level
details I have time after the talk to go
into more depth I have a little bit of
time before catching a plane
so I'm gonna try to leave some time at
the end of the talk for questions in the
audience and then a bit at the podium
and some of these details may be old hat
for you but whether or not the message
is still the same understand your
materials I'm gonna use some terminology
called primary storage and secondary
storage just to clear that up primary
storage is the storage attached to your
CPU secondary storage is the storage
attached over your i/o bus primary
storage is your RAM secondary storage
are your disks and the materials I'm
gonna be talking about are gonna be
spinning disks so sometime during the
talk you may think oh just why are you
wasting your life use SSDs and
everything will be okay and that's not
the point the point is if you do use
SSDs understand them if you don't need
to go to secondary storage understand
your primary storage if your solution
fits in Ram understand how your DRAM
works understand how your memory bus
interacts with your CPU and how your CPU
interacts with
it's Cash's and if you have massive
storage and the exabyte scale if you're
from Google here your materials would be
magnetic tape so you may also think I'm
in the cloud I don't have to worry about
storage I'm in the cloud I don't have to
worry about hardware it's the cloud does
the hardware for me but the clouds also
based on the same first principles that
we deal with his engineers based on the
same materials and you can't ignore you
can't ignore the hardware that runs the
cloud so the lessons learned here apply
equally well in the cloud so the data
that we're storing and serving it
composes our Activity Feed it's the it's
the dashboard page after you land after
you log in and this display changes when
somebody in your social network either
contributes or creates a social link
between them and a sound or whether they
upload a sound themselves so this is
going to be the scope of the data model
that we've taken out of our primary
storage sorry with the primary database
and have built a specialized solution
for this feed for the sake of the
product needs to be consistent so if you
log in once and you see these these
tracks and then you log in again and the
third track has disappeared
that's an inconsistent experience and so
for us as in the product development it
was very important to have that
consistent experience and that meant
saving off for us the easiest thing that
we could do is save that off beforehand
and display it once you log back in as a
graph of our data model we have viewers
and creators these are both the users
there's a relation of following there's
relations two creations in the site and
then there's a transitive reduction from
viewers to sounds over creators and
their creations we chose to materialize
the first degree of this traversal and
what we call events
there's other options when building an
activity stream you can also materialize
that page on read but we chose to
materialize it on right for the goal of
maintaining a consistent experience and
having it load quickly so we wanted to
pre-compute all of the events so they
would be ready when the next next time
that anybody logs in so we wanted our
goals were latency and and a consistent
experience as a entity relation diagram
kind of an incomplete diagram but this
is how you can model it with this is how
you typically visualize it in a
relational store and we just took this
mapping to the first degree of creations
and and save that off into its own table
yeah this worked out really well and
there's some benefits for for having
this because at any point in time we can
go back and reconstruct the dashboard
it's also robust against changes to our
algorithm so if we say that we're going
to introduce or remove creations the
prior creations don't change and so you
can maintain that consistent experience
and the event store in MySQL worked out
pretty well but we've extrapolated some
of our growth in the relation of how
much we're restoring versus how many
registered accounts we were seeing and
we quickly realized that we could be in
trouble
but in fact we were actually in trouble
and we were seeing up to two seconds of
load times for some of these dashboards
and something that we tried to optimize
for up upfront by doing all this work
calculating everybody's dashboard before
they even log in and so we were we were
baffled like how can we do this we're
writing fine but we're reading slow this
is completely opposite to our intuition
of what relational databases are good
for
and and so we sat down and we tried to
understand a little bit of why this was
going on
and I'm gonna jump around in time in the
past and actually describe what was
going on because it shouldn't have been
this way we were we were writing just
fine because of some natural properties
of our dataset so to illustrate that the
the table that we were writing had a
surrogate key meaning that this was a
sequence and a number sequence that was
managed by the database this sequence
created an ordering that closely matched
the ordering of events that were coming
in and this acted like a sequential
clock and as a result the database could
use that to its advantage now to
understand what the database is doing
for you we need to understand how the
database is storage engine interacts
with the material and by using an ODB a
storage engine the common storage engine
for MySQL actually an awesome storage
engine from MySQL it just can throw it
whatever and it's not going to lose your
data and it's optimized for a broad
class of access patterns it's really
just the go-to general-purpose storage
engine for mutable mutable state and the
way it manages this is it carefully
balances a b-tree actually a B+ tree on
disk and this data structure has some
branches and in its leaves it stores the
records so in an ODB everything's a be
tree even the even the records
themselves are stored as a be tree
they're actually stored in an index form
and in in ODB this the primary storage
for in ODB is as a clustered index
meaning that they're ordered the records
are ordered off of the primary key so
with that sequential primary key we get
our leaves which contain our records
group
the pages which contain the i/o units
nicely laid out sequentially on disk so
the on disk format which we weren't
really aware of at the time it turned
out to be quite efficient this is why we
were able to accommodate so many
rewrites per second because we were just
keeping that order and writing right one
after the other adding one page after
the other but on Reed we couldn't read
in this order we needed to read off of
those viewer IDs that were coming in
each viewer gets a unique index a unique
view and as such we needed to index our
data set so that each viewer can pull
out those records and we could follow
the next degree of the traversals and so
nodb being a super b-tree stores the
index in index order in a b-tree as well
and on this this turns out to be have
the exact same format the records
traversable to the leaves in two pages
with the values of the leaves being the
primary key of the item that the
secondary index is indexing so what this
that what happens when when adding and
updating the indexes is that when as
you're as you're adding more and more
items to an individual users activities
stream it may actually fill up one of
these pages so one of these IR units is
16 kilobytes and if you've storing more
than 16 kilobytes of primary keys in
this nodb performs a page split the
it'll bump up against the next page so
there's no more room next to each other
so I don't need to go off and create a
new page typically at the end of the
tree and fill that up with your index so
this is one characteristic of b-trees
when you run out of space you need to
go somewhere else but once you do get
those primary keys and you go into the
clustered index primary index I you may
be loading up different pages just to
get a single record out of each page and
each of the pages that you load could
potentially cause a seek this may be a
review for a lot of you but I really
want to drill it in that that that why
Sikhs are important when understanding
your database because this is what this
is what got us the database is simply an
API to your storage medium that gives
you some abstractions to reason about
your data but Sikhs are not something
that we can we can ignore quickly
overview the disks are spinning you can
pull data off of it or put data on to it
as quickly as they're spinning but you
need to locate to where that is and yeah
when the when you locate to where to put
the data on the physical device you may
need to move for where you are and that
causes a seek it's difficult to
conceptualize how bad these actually are
without understanding a little bit or
having a mental model of of time and I I
think it's important for all of us to
strengthen our mental model so this is a
nanosecond and it's kind of hard to
visualize because we have no sense I
mean this did you make toast and it pops
out in a nanosecond or or what there's
nothing that we deal with except our
computers that work in this time scale
so if we translate this time to distance
we can get a little bit more intuition
about how long that actually is so every
day you can't go faster than the speed
of light unless you watch Damien's talk
and you're using Perl so if we if we go
the speed of light in one nanosecond
that equates to the distance of about
this much about the long side of a piece
of paper about 30 centimeters so that's
that's a nanosecond that's that's one
nanosecond this is the
fundamental unit of our computers this
is what we're processing our code at
this is what the this is the the unit of
measurement that we measure our inner
functions this is this these are the
fundamental units of our computing
universe but yet these are the units for
seek it could take up to 10 milliseconds
to seek and so how how long is a
secretive to a nanosecond how long is
going to disk relative to staying in
code and how far could an electron
travel at the speed of light in 10
milliseconds so to give you an idea 10
milliseconds for an electron is like
going to Milan from here and coming back
so it's like a round trip to Italy to
send something to your disk and this is
why we started to see those two second
response times so once you have a seek
you pretty much know where your
bottlenecks gonna be next next thing
that we saw is that it wasn't just six
it was also storage so from our growth
we knew that we couldn't keep all of our
events on one host and we also knew that
we were creating more and more pages for
our index and and this was all for what
was ultimately mostly cold data mostly
data that would never get loaded in the
in one day so the more storage we had
the more space the more storage that we
required the more space between our
pages that we were seeing and the more
sikhs that we were incurring and yeah
and we also knew from extrapolating our
growth that we wouldn't be able to fit
on one host for very much longer so we
took took a 10,000 foot view and as a
reminder we didn't know this back then
so we didn't know it what I just
described and so we only had a 10,000
10,000 foot view back then
we didn't realize that what we were
having was a disc problem what we saw
was we were having a database problem
the documentation about all this is
tucked away on my SQL documentation site
so we eventually did learn it but at the
time all we saw was outgrowing our
storage capacity getting response times
that were out of reason and yeah and not
having a way of mitigating it with the
database that we were running on so we
have some tough questions to ask we
needed to ask ourselves do we want to
keep this materialisation on right
strategy did we want to change
everything and go with the
materialization on read and then we
considered the product and we considered
the amount of change that it would take
to completely change the way that we
were displaying the Activity Feed and we
said yeah let's stick with the
materialization on right and let's look
at the databases but before looking at
the databases we we spent a little bit
of time understanding our data model and
yeah some of the yeah some of the
properties of our data model were quite
beneficial and it's eventually
consistent and by this meet I mean
eventually consistent in that the rights
are observable in the same order by any
reader but at different times so if you
write if there's rights of a B and C
every reader will see a B and C but they
may not see a at the same time and they
may not see B at the same time but this
is fine because we just have one reader
so if you only have one reader
everything can be eventually consistent
as long as the reader is not the writer
and on top of that because people don't
log in not everybody logs in at the same
time it could be multiple days between
logins and so a read could have been
actually much much later than our right
we also knew we had hot writes all the
contents being created right now and so
if we're going to materialize that first
degree of our graph we are writing all
at the same place and no matter how fast
you can go if you have a hot spot you're
likely going to run into a bottleneck
eventually so we knew that we needed to
have at least some measure to be able to
distribute these rights over multiple
locations but identifying your hot spots
also means that you can use some
techniques you can stay in primary
storage for a little bit longer before
going to secondary storage so you can
basically buffer in RAM before writing
to disk and this will mean this means
that you can accept more at a higher
write rate and then flush out we also
had cold reads so most of the pages that
we were rendering we're going to be
rendering from storage there was no way
that we could keep everybody's dashboard
in RAM this would be totally wasteful a
use of RAM and we could count on the
viewers once they load their dashboard
to only only load the head of the
dashboard sequentially ordered by time
once they once they find their index we
also because we didn't want to stay with
we wanted to avoid having hot spots on
writes and we wanted to distribute the
cold reads and we knew we weren't grown
able to fit on one host we knew our data
set needed to be partitioned and this
was one of the more important things
that we identified is that we identified
our partition key the thing that we
could split on and the nature of the
data model is that it's only the viewers
that need their data set so we could
partition on the viewer key or the
viewer identifier and by knowing this a
lot of options came up because we only
needed to be consistent on a single
partition for a single viewer ID
so given with that we yeah we stuck with
the storage heavy approach and we needed
to grow over multiple hosts so there's
really only one thing that we could do
let's throw hardware at the problem so
we knew we had to throw harder at the
problem and I thought we needed a new
database we thought because we thought
that ok with if we have multiple
machines we need a database that can
live on multiple machines relational
databases can live on multiple machines
but we saw that there was this was in
2009 we saw a lot of new technology show
up on the market that had promises of
just making partition making
partitioning a non-issue and so we
evaluated a little bit of the market but
we also evaluated our workflow and how
our workflow would integrate with the
databases that we were looking at
ultimately when you have data in one
form and you need to get it to another
form you're going to be applying this
age-old technique of an ETL this is fine
this is eventually consistent in its
core because I the extract each of these
steps happen in different transactions
and or could happen in different
transactions and for us the extract was
taking the changes as they were
happening the transform was finding
those edges to materialize first and it
was the load part that we were having
problems with because we needed to write
enough edges so this is in the orders of
hundreds of thousands a second to
materialize our index and we needed to
load them in a way that was reasonable
across multiple hosts but what we really
wanted was to take advantage of our
write hot spots and write everything
into memory and then take advantage of
our secondary storage and read
everything from disk and somewhere in
between we didn't want to have to do any
work and this is where we had this dream
of like what if we could do this
so we started to evaluate our databases
based off of whether they can fill in
the magic happens part and it turns out
we found one so Cassandra has this fake
ten but it also had a lot of other
properties that were quite beneficial so
being ignorant that we actually had a
disk problem we decided to throw
Cassandra at it because it it aligned
with our data our data model and it also
offered some conveniences to make our
ETL simpler so Cassandra is eventually
consistent in its core so you've
probably heard a lot about a cap theorem
and Cassandra is in the AP section of
the cap theorem and every every observer
every reader on any node will see the
writes that happened even on other nodes
eventually every node is a master and a
slave so you can connect to any node in
your database cluster and read and write
to it
you can think of it as like a
distributed hash table where you whether
the writes will get routed to the to the
right host and there's well understood a
multi data center replication so I
believe later in the track Adrian will
talk about how Netflix achieves this
with multi zone replication which is the
same thing as multi datacenter
replication multi-zone multi region and
yeah and this end rights are distributed
so as the rights come in we can split
out and avoid those hotspots by
distributing our rights across different
nodes in the cluster because you can
write to any any single one cassandra
also offers this so the the beautiful
nature of our right path with that
sequential with the sequential writes a
nice ordered data set
and then sequential reads as well means
that we can actually we could it gives
us the promise of no more trips to Milan
seeing this I just imagined okay yeah
sikhs can get under control but how
cassandra does this is an understanding
of its data model and it's also and its
storage model so first to start with
Cassandra's data model which is
important to understand so to know how
cassandra turns its data model into on
disk storage and being in a distraction
over the storage layer we first need to
understand what what kind of
abstractions we have to work with so
I've never liked SQL when I started us
started writing SQL statements I do this
so I imagined okay I have a little bit
of data over here a little bit of data
over here I make the same schema and
then I want to choose which data I'm
going to query from and this has always
bugged me because conceptually I group
my I group my partitions in tables
mentally and I want to select which
table I'm I'm looking into so I didn't
have any theory about why the this
language doesn't allow that but I've
always found it annoying but if we could
do this and we could create 20 million
tables then partitioning is a no-brainer
we just select the the users out of the
user to each of the user IDs tables now
would have been great
oh yeah and Cassandra yeah Cassandra has
this model baked in but I've also wanted
to do this so I didn't know about the
entity attribute value pattern I guess
where you can create a table and then
you can with that table you can create
arbitrary attributes sets I always
wanted to know what attributes I have
available and so I make these super wide
tables I mean this was a long time ago I
make super wide tables and then say I
want that call
I call him in that column I want to know
what columns are there so I'm gonna make
a very wide table but again you can't do
this with SQL but even if you could do
it in consent or you'd there's a there's
a trade-off in Cassandra you don't get a
flexible order by statement you can only
order by your columns so this is okay it
just means you have to pick your columns
carefully and yeah that's that's about
it for this kind of butchering of the
SQL metaphor so if we look at the actual
terminology you can think of the data
model and Concentra like a giant matrix
you have row keys column names and then
the values and at the intersection of
the row key and the column name you can
find your value and the important
difference when considering this data
model versus a relational model is that
you can only scan over the columns you
you don't scan over the rows and in
Cassandra the order in which you scan
over the columns is defined in your
schema so it's like setting that order
by statement but up front it's possible
to to model a data set like you would
with a relational model like if you were
to do a user's table you just pick some
column names which are stable so you can
have your your name your avatar URL your
email and this would be fine but what
you sacrifice is that you can't scan
give me the all the rows that have a
first name of Bob Cassandra does have
secondary indexes not going to go into
those but this would be a typical entity
modeling in Cassandra because Sandra can
do a little bit more
so if you use the same key as your Oki
is as your in your column name
you could easily model an adjacency
matrix and with this you can model a
directed graph and so the columns just
grow the rows grow and you can scan over
give me all the outbound edges that I
have for any given node it's also
possible to model our existing activity
stream so if we had a globally
consistent sequence generator which was
generating those sequence IDs we could
use the column comparator in and order
our columns in the same sequence order
that that we had in our relational store
but if we didn't have that globally
consistent counter we could end up
creating duplicates and it's not as
important with our dataset but we
decided to model it slightly differently
to make the duplicates impossible and we
did this by using the clock of time but
even time can have duplicates because
not all clocks are unique so to make
them unique
we encoded them as uu IDs so it's a
variant of version 1 uu IDs where the
high bits are where the timestamp lives
low bits or where the uniqueness lives
so this this is no SQL for me this is no
SQL for me is when you model your data
how you want to read it this is how
we're gonna access it we're gonna access
it in the order of time based off of our
column comparator and we're gonna create
a very sparse matrix of this and we're
just gonna find the row find the columns
and we'll be done with it and I think
most of the know SQL databases provide
means to have finer control of your
access patterns based off of how you
model it your data in the database but
for rights
we need to understand Cassandra storage
model which is unique it's different
than then in ODB's at first we get a
little bit of the partitioning out of
the way I'm not going to go into depth
about partitioning it's one of its
strengths and it has a lot of coverage
so you can read up about it later but
the quick overview is that for any given
row key it only uses the row key and
then finds a host to store and retrieve
that row from it can be more than one
hosts if you have redundancy and we use
the random host partitioning meaning
that any given row key could be on any
host but is predictable this is why we
can't do row scans because the row keys
may not be sequential on on each of the
nodes so the storage engine in MySQL I'm
sorry in Cassandra is based off of
section in the Google's BigTable paper
which splits the problem exactly where
we would like to have it split so for
for rights and for hot reads the primary
storage is used in a data structure
called mem tables or memory tables these
are you can think of them as just hash
tables and for secondary storage for our
rights and also our cold reads cassandra
our the BigTable paper introduces the
concept of sorted string tables the sort
of the string tables are where the magic
happens because if you have a sort of
data structure you can actually infer
quite a bit your searches into it become
a whole like a degree simpler in
complexity and and this sorted string
table allows you to maintain that those
sequential reads and the sequential
writes so in memory as the writes are
coming in they're coming in in any order
they could be any row key any column
name it doesn't really matter because
they're just going to primary stores
they're just going to RAM
and so if you have a read immediately
after a right
you don't even hit the disk Cassandra is
durable so it'll it'll commit to a
commit log before actually committing to
the right or the mem table so it's still
fast to accept rights but when that
table fills up and it will fill up as
you're adding stuff for adding changes
it periodically needs to flush out of
RAM and and this is where Cassandra and
also HBase will take take the mem tables
sort them off the Roky then sort them
off the column names and to create a
stable sort in structure and then it'll
take that and just sequentially write it
to disk and then start with a new mint
able it's important to know that that
nothing is overwritten on disk so in the
B tree you know DB spends a lot of work
reusing portions of the page we're using
portions of the pages that the leaves
make sure that like if you run out of
space on the leave it'll leave it'll add
a new one and it does a lot of magic in
place on this can that keeps it keeps
the data set pretty dense here it's just
flushed out but what that means is that
as you're fleshing memory tables out to
SS tables on disk you're going to be
generating a lot of SS tables because
it's only gonna be with what you were
storing in RAM and this could be yeah
100 100 Meg's tops no not even that is I
think we're at net 2 Meg's so but we
actually flush even smaller or smaller
sizes so so what you end up with is a
directory full of SS tables and when
performing a read your columns could be
living in more than one table and so
Cassandra keeps some bookkeeping around
of what what columns are and what rows
are in which SS tables and then it needs
to consult all of them
to satisfy your read so you can see if
there's really not much difference if we
if we had all these SS tables on disk
this would be very similar to having all
of our page splits in our be tree and we
would be we'd be seeking all over the
place to try to satisfy a read so
cassandra has a compaction phase which
will read these numbers which will read
a set of SS tables in since they're
already sorted the merge is incredibly
efficient and it'll flush that out
sequentially and this is a constant
background process that happens that
converges the SS tables to reduce the
number of tables that are needed to be
consulted before a read can be satisfied
but even so you can't keep everything
compacted at the same time so you would
think that you would be just trading
seeks in the B tree to seeks across the
SS tables for us we have a maximum of 20
SS tables that we consult for our
dashboards so this is this is from a
couple days ago a histogram of number of
SS tables consulted for all of our reads
bucketed by the number of SS tables an
important thing to note is that the
trend goes left
so the trend in cassandra is to reduce
the number of seeks reduce the number of
tables that needed to be consulted or
leaves that need to be consulted we're
in the way that we were using the beech
tree in MySQL that trend was the other
direction so it's - seeks per SS table
and at 2020 SS tables as a maximum for
satisfying our reads we now know our
upper bound our per bounce 46 and with
this we we have a great we have a much
more confidence
but even at 46 one would think
consulting all those tables for cold
reads would increase latency
so even in out in Milan land where we're
consulting many of their tables were
still much better off than we were with
MySQL so we're under under 200
milliseconds in our upper bound for the
for satisfying a dashboard read and
these are our cold reads these are the
reads that for somebody that hasn't
logged in for a while that that needs to
pull that up into RAM our average is 20
milliseconds and that's still a couple
trips to Milan but it's much better than
we were doing before and writes our
whole different unit of measurement so
when you see a distribution like this
you you know you're doing it right
because it's nice and tight you don't
have any outliers you can just pretty
much ignore the unit's because because
you're doing your you have a predictable
distribution but for us this isn't
microseconds and at this load our
cluster can easily take bursts of 75,000
writes per second so 75,000
materializations of edges per second so
we did it
so we've addressed some of the concerns
about partitioning with Cassandra we're
accepting a lot of our a lot of writes
from all of these edges that we're
materializing and we've stabilized our
load times and with the partitioning
techniques that Cassandra gave we were
able to actually perform a live
datacenter migration so we set this up
before we had moved from England to
Amsterdam and this this database or this
database cluster was the only thing that
was online the entire time during the
during the transition we also could
accept writes from multiple data centers
so we were writing an Amsterdam writing
and in England and you may find this
amazing but those metrics on those
Layton sees were from a cluster that's
running an ec2 with a notoriously slow
IO IO x so both with Cassandra and in
the SS tables and you know DB with its B
trees they're using the disks
effectively but in different ways so
cassandra accepts random writes very
well much better than in ODB but as in
ODB's creating random writes by
maintaining the index but once that
index is established in ODB does much
better for performing reads because it
can traverse that that that tree that
with fewer hops than the number of SS
tables that you need to consult but it
hasn't been such a great experience with
Cassandra we spend a lot of time on
operations of our clusters and we have
about like five to ten hours a week of
just babysitting it there's alerts that
go off that where nodes can't
communicate its distributed but and we
understand why it's happening but we
still have to respond to it and
periodically we have to intervene and
say yeah you need to go down for a while
or you've been down for too long and
we're gonna bring up another and nom as
a result it we haven't quite worked out
those those kinks but if you do go with
Cassandra you can expect trading trading
at a time for for operation of anything
larger than a five node cluster and in
in hindsight we know now how exactly we
would have built it with MySQL so the
problem that we were having of our index
splitting it could easily be mitigated
by optimizing our table periodically
rewriting the entire be tree and
splitting that up into smaller chunks so
it doesn't create outages or offering up
our workflow our ETL and RAM before
writing out to MySQL to avoid being down
during that time so to wrap this up of
how we fit our database to our data
model we did a lot of write things a lot
of smart things along the way
we found our natural partition key we
isolated a read path from a right path
we identified the volume and the access
patterns both for our read and write
loads whether they were sequential or
random and we developed a sympathy for
the materials that were using but most
importantly we never forgot our goal of
delivering a product that kicked ass
so we architected first without fully
understanding what our building
materials were because we assumed that
our database is going to hide that from
us so whether you try to fit your data
model to your database or whether you
choose to fit your database to your data
model ultimately the success of your
design and the ability to hide your
ignorance to the public is really based
on how well you understand the materials
that you're working with so that's it
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>