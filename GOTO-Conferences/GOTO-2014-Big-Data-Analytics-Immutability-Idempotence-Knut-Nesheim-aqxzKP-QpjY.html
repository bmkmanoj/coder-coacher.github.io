<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • Big Data Analytics: Immutability &amp; Idempotence • Knut Nesheim | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • Big Data Analytics: Immutability &amp; Idempotence • Knut Nesheim - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • Big Data Analytics: Immutability &amp; Idempotence • Knut Nesheim</b></h2><h5 class="post__date">2014-11-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aqxzKP-QpjY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this presentation is about to some
stuff we learnt when we built this kind
of data heavy application and especially
to lessons that I want to try to explain
to you that come from like fancy
computer science ii terms that if you
studied computer science you would know
them and instead of applying them to
like small algorithms and data
structures we apply them to like a whole
system at the time so that's why this is
kind of interesting so what we do is a
game analytics a success on the tin it's
a kind of business intelligence service
for games this sdk that you put inside
of your game so if you have a mobile
game on iOS or android let's say you can
whenever use it does something that you
instead interested in measuring you can
send us an event we collect and analyze
these events and then there's a nice
user interface where you can look at
nice reports and understand your game
and then you can use that information to
improve your game let's take the whole
idea so similar to Google Analytics for
e-commerce and so on for example so this
is what our system looks like data flows
you know from top to bottom so SDK goes
into a collector services which we have
a bunch of it all goes into a log and we
have done different few consumers of the
log and this the really streaming
real-time analytics their stuff like
funnels which we do like a special case
computation four funnels and there's
also more lots more and in these
different systems our query directly
body user so there's a HTTP API that
customers and browsers use directly so
so that this is kind of the context that
we work within so some bragging numbers
just to show that the system actually
works so we have around three billion
events a day now
up from around three thirty million when
we went live a year ago so i scroll 100
hundred fold and daily that's 600 750
gigabytes of JSON uncompressed data so
this is what our system turns through in
in one day so on to like what we learned
so there's also we made a lot of
mistakes unfortunately it's not enough
time to talk about all of them so i'm
going to share like what we did right
and one big concept that turns out to be
really valuable is to use a log analog
conceptually is the same as the if you
think of like a file system log dislike
application log on the file system let's
say syslog our sister greta to always a
pen stuff file for example that's
exactly the same concept but you want to
apply it on a bigger scale and so does
this thing have like the oh so a log is
like messages or lines or chunks or
whatever you want to call them and you
always write by appending to the end and
you read somewhere in the middle of the
log or you know if you have a real time
system you want to read at the tail of
the log if you want to go back in time
and recreate some state let's say you
would tell your consumer to not have the
latest offset by the earlier offset so
if you have used kafka kafka is exactly
this thing and the nice thing is or
there's a few nice things is few
nothing's so one big upside if you have
a data intensive application is that you
can then split up the writer and the
reader in the system so in our case we
have around 50 million devices every day
that produce data so they talk to our
data collector services and then they
that's these guys that actually write
and then we have a few different systems
that do the actual processing and what
is nice about this is that
the writing can be always available and
we can actually turn off the analytics
processing so whenever we have like a
bug or we need to make some changes to
our setup of anything or we have a new
release is something we can just disable
the system change the code and whatever
and then start up back up back up again
and nobody actually ever lost any data
or is upset in any way also you can make
this thing kind of smart a simple and
smart at the same time so what we do is
we write all of data directly into s3 so
we have big chunks of data industry and
and we kind of emulate a bit or Kafka
does with partitions and topics and so
on and so we have some ordering
guarantees and we have pretty good
parallelism on the consumer side you can
ask me more about that later for you if
you're dying to know how it works so
lesson number two is around i dint
potency so Adam policy is that you can
do the same operation twice and it's not
going to have any adverse side effects
and so we apply this to our whole
analytics pipeline there's some parts
where we have yet to make it work but on
the whole it works and that is when
we're running with our business logic
that computes all of the metrics and so
on if there's a bug in your business
logic not so it crashes but so it makes
the incorrect calculations and we want
to be able to go back in time I kind of
fix it and we do it so this is how we do
that so we have some this is a concept
that we call a checkpoint and I need to
explain a bit more like the background
and so we process everything in 24 hour
patches and so a metric like revenue per
day only make sense if you have 24 hours
worth of data so that's the window so
our streaming real-time analytics system
what it does is really
keeps 24 hours of data in memory which
keeps one day of data in memory at any
point and then once the day is over you
know the revenue for that day and you
can write it into a database so this is
the same as if you run a rep reduced job
like on yesterday's data and we do the
same except on today's data where we
don't have all of data yet so it's kind
of like a partial we can run it on
partial data and it's the state sticks
around instead of the reducers which we
didn't actually use the query and to get
the real-time data anyway so this check
pointing system what it means is that we
have like last successful day we
processed that's the last checkpoint and
so when our system starts up we go and
we go and read this checkpoint and then
we go into a log and we find all of the
data that occurred after this checkpoint
and then we process all of that until
the next checkpoint which would be the
next day so in this example from the
first of October midnight to the second
of October midnight and we can actually
know when all of the data has been
received and since we can go we know we
have this collective services and we can
know where there are n times we can
actually see that now all of the
collector services have sent all of the
data for the previous day and then it's
safe for us to say we have received
everything everything is kind of good to
go we can write everything to database
and then here comes like the big idea
that we also overwrite anything that is
already inside of the database and what
this means is that lets say you use my
sequel for example to store the kind of
computing metrics so use to a revenue
per day as just a number instead of my
sequel we also see me set up in such a
way as we don't get duplicate rows so
let's say we mess up some at some point
during the day or we mess up when you're
right into the database we partially
right
so we write eighty percent of the
metrics for example then we need to go
back and redo that and this is the safe
to do we can do it many times it's over
right anything that is already in there
and then if that is successful we set
the checkpoint to the next window that
we just successfully saved crystal clear
so what if you want to have a system
like this like where can you get one so
we built our own system to do this and
it's like Erlang in-memory distributed
system because there was no alternative
when we started today there are actually
some alternatives like this a purchase
some stuff for example which does
exactly what you want and a lot more and
a lot better so you should totally go
and have a look at that I think today if
it was started put it over today
definitely we would have a hard look at
Apache some stuff before we would do
anything different as a purchase am sir
does uses a kafka runs inside of Hadoop
yarn it does all of this checkpointing
but much nicer has much more fault
tolerance and so on then we do and so
check that out okay thank you so we use
DynamoDB for a lot of stuff yeah all of
you forever yeah it's so so we just pay
money to Amazon so we don't worry so
much about that are a table now with
metrics contains over a billion entries
but i still think is we've only played a
couple of hundred dollars a month for
that table so we don't we don't really
care thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>