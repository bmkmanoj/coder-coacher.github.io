<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Machine Learning in the Wild: Techniques for Understanding your Audience • Sarah Guido | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Machine Learning in the Wild: Techniques for Understanding your Audience • Sarah Guido - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Machine Learning in the Wild: Techniques for Understanding your Audience • Sarah Guido</b></h2><h5 class="post__date">2018-03-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Qa40wuIfwkM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">cool thanks for having me here today
I am going to talk to you about how I
use machine machine learning at Mashable
to understand the audience for the
content we create and so my title
machine learning in the wild techniques
for understanding your audience
so first Who am I
I'm a senior data scientist at Mashable
in New York City so I've come a very
long way to be here today it's also my
very first time in Berlin very lovely
city yeah I am a one of the
co-organizers for the NYC Python meetup
which I believe is the largest Python
meetup group in the world with over
10,000 members I'm a conference speaker
I speak at a number of conferences I'm
an O'Reilly Media author I co-authored a
book on machine learning and Python and
you can find me on twitter at sarah
underscore guide oh yes my last name
name is pronounced guide oh yeah so I do
all of my work in Python I used to do
spark and Scala listening to the
previous talk is kind of interesting and
I'm just glad I can do Python now so
this is what I'm going to talk to you
about today I'm going to give you an
overview of Mashable which is really
important for understanding all of the
different use cases for machine learning
that I'm going to talk about so I just
set it set up like wow what Mashable is
like what sort of audience we have in
all that and then I'm going to talk
about sort of three higher level
categories content engagement audience
segmentation and social media strategy
I've applied machine learning to each of
those segments and then some wrap up
with some takeaways so this is more of
an advanced that will talk I'm going to
assume you know what machine learning is
already the the difference between like
unsupervised and supervised methods I'm
going to assume you understand basic
algorithms like k-means clustering and
linear regression I'll explain a little
bit about some of the more complex
algorithms that I use the goal for me is
to show you how I use these methods in
the wild so to speak so in my everyday
so beyond just you know learning the
theory in the classroom like how are
these methods actually applied so and
how I apply these methods to do my work
in Mashable so how many people have
heard of Mashable okay decent amount of
you so for those of you that don't know
Mashable is a digital media site we
publish articles related to internet
culture related tuned like tech news
science entertainment you know we don't
really do hard-hitting journalism you
know we do things like talk about what's
happening on Twitter Twitter is
rethinking the blue checkmark we have a
lot of like I guess more lighthearted
content although I will say our science
coverage of like climate change I went
on is pretty good if you're interested
in that stuff so this is sort of our
like brand statement and this is
directly tied into who are audiences so
Mashable is a media entertainment
company for super fans we're not for the
casual curious we devour culture and
tech our ideas shape the future and we
speak to new influencers and the early
adopters who obsessed with us around the
globe so we've been around for about 10
years and so today we're finding
ourselves at this juncture where we
don't want to write about everything
that's out there that's there's a lot of
stuff out there and that's not the best
use of our resources it's a very
specific subset of content that we're
really good at writing about so what we
want to do is basically articulate who
we want our readers to be in order to
help shape our editorial coverage so
before we get into that stars the
publishing landscape goes Publishing you
know 10 15 years ago used to be you
would read the newspaper or you would
watch The Evening News or something and
maybe you still do that but now people
rely really heavily on social media
networks for their news distribution and
and we're constantly receiving the news
in a very asynchronous fashion you know
we're getting push notifications on our
phones all the time for a variety of
different content there's not just like
one centralized place anymore
there's a variety of distribution
networks so in order to like understand
who our audience is and where they are
what they're doing we have to understand
the distribution networks first so the
business problem that we're trying to
solve here is how can we understand our
audience in order to know what content
to write optimize this content in
real-time and deliver the right content
to the right people at the right time
which is the ultimate goal and very
difficult so because of the way the
publishing landscape has changed we want
to be able to understand the social
diffusion of our content so how our
content spread spreads through social
media networks as well as the social
diffusion of other publishers content
thoroughly enough to gain a competitive
advantage so this brings us to our core
problem here so the first question we
want to ask ourselves is how do people
interact with their content and not just
our content either the first step that
we took towards this is crafting this
very particular brand statement in order
to draw in a very specific a very
specific audience that's really
interested in the stuff that we're
really good at writing about so you know
the internet culture stuff the tech
stuff the entertainment stuff you know
we're not trying to go out and get every
facet of every audience ever just just
this particular subset of it so in light
of the decentralized distribution
networks that we have sourcing content
is is like the very first step in
actually like sort of crafting this
audience so we created something called
the velocity suite to help with the
sourcing problem so the velocity suite
is our main tool for helping the
editorial team source content and figure
out what to write about and this this
sort of predates my tenure at Mashable a
little bit but it's really important for
all of the audience work that we do
because it really helps us understand
the how the virality of content and how
content goes viral so the the platform
helps the editorial team source this
content by crawling the web for content
that is going viral and we determine by
reality based on the
the velocity if you will have shares so
how quickly shares are accumulating so
the origin was to make content sourcing
much easier for editorial team the
product team at Mashable looked at the
process that every writer was using to
source their content and decided to try
and automate this so they took a survey
of the landscape and what this product
does is ranked what's on display by
virality and use that to predict
virality based on share counts so it
does this in an iterative fashion so we
crawl something multiple times we update
share count we predict virality so what
does this actually look like so I don't
know if you can see this this is like
basically a time series of shares along
the x-axis you have times and then you
have accumulated share counts on the
y-axis so velocity curls content across
the web track shares on social media and
uses that to predict future performance
in an adaptive iterative way so what I
mean by this
again we don't just cross something once
we crawl to get a share count we make a
prediction based on that share count and
then we also make a determinate
determination about when to crawl again
for an updated share count so when
that's time span is passed we crawl
again getting an updated number for
accumulated shares use that to make an
updated prediction so this prediction
the lighter blue the the not filled in
blue there is basically the predict the
prediction of the accumulated shares
trajectory so this will adapt as we
crawl again and again which is kind of
nice so to generate these predicted
share counts we build a model on the
network of shares that occurs on a
particular article so we do this for
each article the network of shares is
called a cascade so the velocity
algorithm is based on diffusion on a
random graph and it models what the
probabilistic spread will look like and
how the shares for the article are
spreading through the graph so basically
like how are these shares going to
spread what's the end count going to
look like the algorithm is time
dependent and it parameter fits
probabilistically against the data that
we've already seen in the
ask ade so it makes determinations about
the the final prediction based on what
we've already seen as well as what we're
seeing now uses acceleration and the
depth and curve of the accumulated share
counts as well and we end up with a
prediction for the time scale so like at
what point in time we're going to reach
approximately like 95 percent of the
total shares that this article could
possibly achieve it uses an evolutionary
adaptive solver to do to do the
parameter fitting so this allows us to
build out this the social Cascades for
each article so what does that look like
here you have six different social
diffusion graphs if you look at the
little small rectangles under the tree
structures you can kind of see that the
patterns are similar you know a like an
initial like QuickLIFT and then sort of
a plateau growth there however if you
look at the actual tree structures
they're vastly different so figuring out
what the actual graph looks like can be
a little tricky so let's look at how
this works in practice so this is a
graph of so this is like best-case
scenario blue the blue line there is the
accumulated shares over time this was an
example very smooth growth and then the
yellow is the projected shares over time
so remember that the predictive aspect
is adaptive and adjusts in real time to
share counts that we're seeing basically
on the move so you can see like right at
the beginning the predictions are a
little bit more unstable but over time
you can see that the pattern of
predictions sort of levels off a little
bit so like this is the best-case
scenario like okay final predictions
sort of and kind of close to where the
final share counts actually are so this
is also kind of interesting for this
particular article it's sort of which
was shared a little frequently at first
and then accumulated a lot of share
growth later on and that's that blue
line there is the the actual shares over
time so this could be like an article
was published on the lesser known site
and then it was picked up somewhere else
and like a much larger larger media
outlet and shared and you can see that
the yellow
which is the projection shares over time
was actually able to adjust to what was
happening in the in the balloon in the
real segment which is pretty cool so
sometimes the update algorithm can be a
little finicky and we end up with
something like this
this is thankfully very rare but um it's
very very terrible you can see like
right at the beginning it's kind of okay
and then things just go like off the
wall we're constantly working on the
algorithm for our velocity suite because
obviously we don't want to like deliver
this sort of information to our
editorial team because that's really not
helpful at all so this is actually what
the editorial team will see in our in
our in-house product so here's an
example of an article
husky showers baby with kisses during
adorable play session so like I said we
don't really do hard-hitting journalism
so this this was like a really good
proof of concept for the velocity suite
we discovered this article right as it
was published we collected a bunch of
data points around it
there were several points where the
share trajectory changed over time and
the prediction was able to adapt to it
and then early projections very
accurately modeled what was going to
happen so a good starting place for us
in terms of understanding our audience
is taking a look at the polishing
landscape at large and identifying
content that's going viral that we've
determined would be very relevant to our
audience and this is how we start
crafting an audience by tailoring our
content to a specific brand image so
that might sound like very like
marketing speak but again consider that
we don't want to necessarily draw and
that's like extremely focused on only
hard-hitting news we want the audience
interested in like cute husky videos and
so we kind of get this interesting
feedback loop where we set out with the
intention to craft an audience and then
that audience in return kind of comes
back and crafts us and and what we cover
a little bit so now that we have an
audience great we have the audience well
what can we actually say about our
audience how do we describe what's going
and any sort of characteristics about
our audience and the way that we do that
is with audience segmentation so
audience segmentation at a very high
level is just a way of breaking down
your audience into meaningful groups of
people so like Joe is 56 and drives a
Mercedes and really likes reading about
Huskies that would be like a very
specific segmentation but basically
thinking about it in terms of like user
profiles like who-who are the real
people behind just the numbers we're
seeing when we look at our audience um
so you can do audience segmentation in a
variety of ways you could go the third
party route you can actually pay
third-party software to do it for you
there are pros and cons to that pros you
just use it on box it's really easy for
non-technical stakeholders to interpret
and they're the end users so that was
something that we considered you can
also automatically import data from a
variety of sources which make which
means you don't have to worry about the
ETL pipelines or anything like that the
cons is that they're not necessarily
customized to your own particular
audience or what they're interested in
so again our so our audiences is this
very specific subset of like a much a
lot larger all-encompassing audience so
when we think about audience
segmentation we wanted to think about it
in a way that is really really specific
to the kind of content we are producing
and then also it can be sometimes
unclear how third-party software
generates their audience segments so
because um you know that's their secret
sauce so we decided that we wanted to go
the in-house route because it would be a
little bit more flexible so the data
that I used for this was from Google
Analytics so Google Analytics will give
you back if you send it a list of URLs
to the API you can get back information
like the gender breakdown the age
bracket and then the interest group of
people browsing specific URLs so when I
say interest group I mean things like
movie lover or news junkie so kind of
well they may or may not be actually
useful to the work that we're doing but
they're interesting nonetheless although
it's kind of like who isn't a movie
lover it's like some of the categories
are so like all-encompassing that they
may not be actually that useful so
anyway the goal is with audience
segmentation were to give the editorial
team profiles of users identify groups
of similar users within our audience to
analyze different sections of the site
so we have the Mashable main page we
have the Mashable Culture page the
Mashable tech page you know we wanted to
see are these audiences different on
different sections of our site and then
use this to drive content creation so
this is a snapshot of the data that I
used so in a URL column the segment the
the demographics get returned on a URL
by URL basis and then in the our
features there are basically like the
combinations of all gender age and
interest groups in the set so we have
art and theater aficionados which is our
interest group age bracket is 18 to 24
genders female and then you know for all
of the different interest groups and
whatnot so as you might imagine the
sense of being a very very sparse matrix
and then the the actual values are the
percentage of that particular
demographic set that is present on that
URL so in order to actually build the
segmentation we had to transform the
data in this way there there ends up
being something like 383 features in
this set after we we explode out the
feature set which I know they tell you
not to you know create high
dimensionality but I assure you the
curse of dimensionality has been broken
or has it will find out so how should we
model audience segmentation tried a few
different methods that largely fell into
two categories clustering and also
decomposing the audience
you could go about this in a couple of
ways you could use k-means clustering
for example to identify like groups of
similar similar URLs in there their
demographics sets but when when we
thought about that there's more of a
hard separation between the features so
you would have like all of the the art
aficionados in one group and then like
all of the movie lovers in another group
and we kind of didn't want to do that we
kind of want to think about our audience
a little bit more holistically so we
decided to go the decomposition route
which would create segments for us that
our combinations of features rather than
just hard separations of features so try
it out a couple of methods for this
non-negative matrix factorization as
well as archetypal analysis so what are
these methods so archetypal analysis is
really interesting it finds rather than
finding cluster centers it finds
extremely points in the data to
designate as arc types so arc types
therefore are a convex combination of
identified features and archetypal
analysis favours features that represent
corners if you will in the data so so
not necessarily just like single feature
points but rather like corners and
intersections of these features that are
that are extreme extremely distance from
from each other and the non-negative
matrix factorization which I will
henceforth refer to as nmf because
that's a lot shorter to say in fact it
factors matrices in a way that allow for
easier inspection so nmf does a
factorization that usually leads to two
different matrices with a property that
all of the matrices will then no longer
have negative elements so this non
negativity makes the resulting matrices
a lot easier to inspect it has some
inherent clustering and decomposition
properties in there through minimization
of an error function so the
representation of the vectors that we
end up with are obtained in an additive
fashion by superimposing the components
without subtracting so then we
up with these clusters segments if you
will that are once again combinations of
features so both of these methods give
us a way to think about our audience in
in a very holistic way but which should
we actually use or even before that are
the segment's they create useful and
then which should we use so here we have
an example of a particular segment so as
it turns out both of these methods sort
of highlight some similar groups of
people so here is the most prominently
identified segmentation result from each
method we will call this the Millennial
segment there are a few differences so
if you can see nmf weights mail more
heavily than female while while
archetypal analysis primarily focuses on
the 25 to 34 age bracket but they're
also but they're very similar similar
age ranges so like this probably isn't
that interesting like we know that our
audience is going to be Millennials so
let's look at something that may be a
little bit more interesting so this is
more interesting segments this is our
our fourth segment for nmf and
archetypal analysis and you can see both
segments are weighted much more heavily
female and also are primarily in the 55
to 64 age bracket so it's especially
interesting that both nmf and archetypal
analysis agreed that this is a segment
worth surfacing so the other segments
kind of varied so it's kind of hard to
say like which method we should
particularly go with likely we would
want to look at both and sort of figure
out which tells sort of the best story
so a lot of it has to do with the story
that we can tell the editorial team
about their audience through the
editorial team
it's our end user for all of this work
so the question that we want to optimize
for is what's the most accurate
segmentation we can model that also is
the most interprete belen a meaningful
way to non-technical
holder's that that can be the hardest
part sometimes is um you know we have
this data we run some very complex
transformations on it and then we have
to deliver it in a meaningful way to two
people without the same training or or
background in these methods so one of
the most important things I've learned
so far mashable is that you might have a
really excellent sophisticated
state-of-the-art model but first of all
if it's hard to interpret and if it's
hard to present in an actionable way to
those to those stakeholders is not
necessarily useful so the goal for us is
to describe our audience as an answer to
a business problem so I have to be
mindful of that and the work that we do
so that we solve the problem for
everyone and not just the data science
team but this was a particularly nice
solution that we were able to actually
talk to the editorial team about and say
hey we found a couple really interesting
segments you know is this useful to you
and then the next part of this project
will be to actually sort of sit down and
take a deeper look at the actual content
that ends up in these different segments
and then we can try and piece together
like okay this demographic is really
interested in this very specific subset
of content like Huskie videos or
something so there were some caveats
with the Google Analytics data we were
using the free version because the next
level up I think was a little bit too
expensive for us so the data that I was
working with was not a super ideal we
had paid for it we probably would have
gotten around some of these issues but
alas so when I would hit the API I would
only really ever get back a third of
data for a third of the URLs that I sent
the API would just sometimes not work
and then there's this like semi useless
interest data like movie lovers which
okay whatever so like going forward we
we want to find a much better source of
demographic data or maybe just actually
given and paid for Google Analytics okay
so now we thought about a way to
describe our audience now we want to go
out and find them so again we're
thinking about a very decentralized
issue distribution across social media
channels so we want to make sure that we
giving the right content to the right
people the right time so the next
logical question we can ask is where is
our audience and how do we reach them
okay
so first a little bit of background
about the state of Facebook so pretty
much most of our traffic comes from
Facebook if you're a publisher of any
kind most of your traffic is going to
come Facebook that's just kind of how it
is and it's the game we have to play so
Mashable on Facebook we have quite a
number of different pages we sort of
have this like Mashable main page and
you can see at the very top one up there
where traditionally we have put most of
our content and then we have a number of
secondary pages we have like the
Mashable tech page we have you know the
news page the France page and so on I
just want to highlight on our main page
is a couple days ago we have 6.4 million
followers so this is like gigantic
compared to the smaller pages that we
have so we have like for Mashable tech
we have 798 thousand for
click-click-click we have 600 26,000 so
so when we think about how to distribute
content we have to do it in a way that
is sustainable so we can't just put all
the content all the time onto our main
page because if you post too frequently
to Facebook they the Facebook algorithm
will actually punish you a little bit so
it to be very strategic about where and
when we place content so in light of not
only having a multitude of content but
also having a multitude of pages you
know how do we think about how do we
think about developing an optimal
Facebook strategy so somewhere we can
start when we think about this is is
there a relationship between views and
shares on Facebook and if there is a
relationship is it predictive and what
can we learn about it so it is
predictive
spoiler alert um so for this project
which is actually like fairly simple but
kind of interesting the goal is to
predict views from shares so when we see
shares on Facebook on a weekly basis can
we predict what the view counts are
going to be so this is a snapshot of the
data that I use I went back all the way
to 2016 i grouped views by week I was
also using average engagement so average
number of shares oh and when I say
engagements I mean like shares likes
comments and reactions on Facebook's all
the different things you can do on
Facebook to engage with a particular
piece of content so total total views
average engagements the number of URLs
we produced and average engagements so
this is a very simple linear regression
proposition so we also decided that we
wanted to optimize for making the error
rate as low as possible so rather than
try and optimize for a low r-squared we
decided to optimize for root mean
squared error as well as mean absolute
error so our squared will tell me how
well the model fits the data but doesn't
necessarily tell me anything about
errors or the magnitude of the errors so
root mean squared error is a standard
deviation of residuals in a linear
regression model measured measures the
spread in the regression fit is
basically a measure of the noise in the
system and also mean absolute error is
the average magnitude of errors which is
a little bit less sensitive to large
errors so here we want to be very
thoughtful about errors because we want
to use this data to have an idea of this
so if we publish n number of articles in
a week and we publish them to these
particular Facebook pages it's likely
that we're going to get like n number of
views which can then help the social
teams meet their goals so we want to
really optimize for views here and we
want to make sure that that the
information we're delivering is as as as
accurate as possible in terms of error
rate so again very simple so here are
the predicted versus actual views over
time we got a pretty good model fit and
we were able to achieve decently low
error rates the blue is the weak to
predict and then the green is the error
I'm sorry the blue is the actual number
and then the green is the predicted
number so again this is like a very very
simple application but it's something
that we haven't discovered before and
that was actually like sort of hugely
revolutionary to the editorial team
who's like oh ok so now that we have an
understanding that views and shares are
have this relationship we we can be a
lot more thoughtful about where and when
we publish data while we're wearing when
we publish articles so another very
important facet of this when we think
about develop developing an optimal
Facebook strategy is are we making any
decisions that are actually harming us
so the thing about posting to Facebook
is that a lot of people have a lot of
preconceived notions of like what
happens when you post on Facebook or the
mysterious Facebook algorithm like it's
it's it's a common thought that when the
Facebook algorithm changes everyone's
content suffers we're actually working
on a way to prove that's not actually
the case
but another so so we do some with some
myth-busting if you will and if the goal
is to achieve as many views as possible
in facebook we got to know if we're
properly set up to achieve that goal so
the editorial team had a theory they had
a theory that once an article reaches a
thousand clicks on Twitter it should be
posted to our main Facebook page and our
main Facebook page is the one with like
6.4 million people so we want to know is
is this a good heuristic so we produce a
lot of content we have limited resources
for posting
we obviously can't put everything on
Facebook or on our Facebook main page
all the time because if you published
again too many times in an hour or
something the algorithm will punish you
so the one of the heuristics the social
team was using was if something had
around a thousand clicks on Twitter we
would then publish it to the Facebook
main page so you know is this a good
heuristic just intuitively it's probably
not because the audience is on Facebook
and Twitter are very different but you
know we want to go beyond intuition so
we wanted to prove it out scientifically
so the data that we have we have the
number of clicks we get on Twitter we
have the Facebook page a particular
articles posted to one of the views
performance and using this we can
actually sort of dig in and figure out
what's going on so what does it mean to
ask is this a good heuristic good is a
relative term so let's define that a
little bit more thoroughly so we want to
compare populations so breaking down the
problem when I compare populations we
want to compare articles that achieved
at least a thousand clicks on Twitter
and we're posted to Maine and those that
were not posted to Maine we want to know
if these populations are different we
want to know what the performance is of
articles on our Facebook main page in
each of these populations and then does
using this heuristic perform better
overall in terms of views than using
absolutely no heuristic so remember the
overall goal is to get as many of yous
as possible when posting to Facebook so
we're trying to figure out is does this
here a stick help us achieve this goal
so the first thing we want to figure out
before we even do anything else that's a
little bit more complicated is are these
populations statistically different so
we have population 1 which is articles
that have achieved thousand clicks on
Twitter and then we're posted to our
main page then we have population 2
articles that were not posted to Maine
so we have the main population in a not
Maine population so we can test the
differences in these populations with a
two sample come over up smirnoff test so
KS test returns a p-value and if it's
lower than some threshold like say 0.05
we can then say that the populations are
statistically different
it's a nonparametric test of the
Equality of distributions so Ranas tests
and we determine that these two
distributions are statistically
different or these two populations are
statistically different from each other
and do not come from the same
distribution so that's good we didn't
think that they would so the next thing
we want to figure out knowing that these
populations are different so we want to
know the probability that an article
with a thousand clicks on Twitter and
it's posted to Maine outperforms an
article that is not posted to me you
know if you think about it since our
main audience is so large
if you post an article to Maine it's
probably going to get a lot more views
and if you don't post it to me but we
want to understand what's going on here
in the context of this a thousand clicks
on Twitter heuristic so if in fact a
thousand click Twitter article is you
know maybe not likely to be jan article
with less than 1,000 click search or
something then the heuristic is kind of
rendered useless so to model this out I
use a technique Goff seein process
regression which is a very very elegant
and awesome technique um so Gaussian
process regression characterizes
nonlinear relationships between
variables this is a nonparametric
approach which doesn't mean it has no
parameters but there is an infinitely
large possibility of parameters you can
use Gaussian process regression to find
the distribution over functions that
best fit the data that we're modeling so
why why use this method so because we
get a full a full conditional
distribution we can calculate the
probability that an article will achieve
a certain number of page views both
using the hair stick and not so the way
that I did this was I built out two
Gaussian process regression models one
on the the main set and the one on the
main set and then I built a simulation
in order to see what would happen so
first this is the regression so this is
log scaled the dots are fused data in
the in the main and not main populations
if you can see the blue that's kind of
the blue dots that are kind of higher up
are the main population the green are
the not main population so and then the
red line is the regression for the main
population so you can sort of see that
posting to main does give a lift a lift
to views but there's but it's not like
super clear-cut it's not like every time
you're posting to main you know you're
you're beating out non main pages every
time so how do we actually figure out
what's going on here so the method I
used was to first build the models for
each population so you just saw that in
the previous slide so the next thing we
want to do something that Gaussian
process regression was really great at
is sample a large number of times at
Twitter clicks equals a thousand for
each of these regressions and make a
prediction for views based on that
sampling so then we want to determine
two things we want to determine how
frequently a main sample has higher
pageviews in a not main sample by
drawing from each sample a large number
of times this is using the Twitter
heuristic then we also want to determine
how frequently a randomly chosen main
sample beats a not main sample no
heuristic so all this to say when we use
the a thousand click heuristic on
Twitter for yeah a thousand clicks on
Twitter to post a main posting to the
main page achieves how your page use
then not posting to main 65% of the time
okay we kind of expected that because
posting to main is going to generate
more page views however by selecting
randomly and using more heuristic
posting two main achieves higher page
views than not posting to main
78% of the time so it's actually so
actually choosing articles supposed to
main randomly will likely outperform
using this a thousand clicks on Twitter
heuristic
kind of mind-blowing so using this
thousand clicks on Twitter here stick to
post is actually detrimental to the goal
of achieving as many pages as possible
when we presented this information to
your social team they they were like
whoa some of them had actually like
raised the question like should really
we be using this heuristic so it wasn't
totally shocking to anyone especially
because like it's generally known that
there are significant different
differences in in the audiences on
Twitter and Facebook so in addition to
this sort of myth-busting one of my
other co-workers he had worked on
figuring out like the app the optimal
number of Facebook posts a day for each
Facebook page so we sort of did this
like tag-team war where I was like hey
don't use this heuristic and he was like
but here's a different heuristic you can
use could you never just want to like
kill everyone's hopes and dreams all at
once without giving them any anything to
hold on to so that's also like a fine
balance of strike um but it's also it's
been very useful because after so after
we presented this information they
started they used the information that
my coworker provided to actually
determine the number of things to post
and then stopped using Twitter clicks
and our views have actually gone up a
little bit which is pretty cool okay so
I have some final thoughts to wrap this
up um here's some takeaways so audience
data can be messy and complex sometimes
you won't have all the data that you
need you can also do a ton of things
with it so I I presented like a bunch of
kind of like smaller things that I've
done in the past a year so I've been at
Mashable um but there's a number of
other things that I didn't present that
we've been working on like we've
actually done a whole like competitive
landscape analysis looking at a bunch of
other publishers like like BuzzFeed and
Gizmodo and Business Insider and because
we not only go out and crawl these pages
but we can get their Facebook posts from
the Facebook API and sort of try and
understand what are their social media
distribution strategies is this
working for them in terms of shares and
then try and like optimize our own so
another takeaway is to make data as
usable as possible for non-technical
stakeholders you never just want to like
drop a bunch of numbers on somebody and
walk away then they're like what do I do
with this um so also having an
understanding of both the audience
reading your content in the audience
you're developing for or or finding
insights for that's also good know what
metrics you want to optimize for maybe
not going to be the same metrics every
time know what your end goal is so like
if your end goal is something vague like
is this a good heuristic that sounds
like a nice easy question to answer but
it's not you actually have to think
about it a little bit more in-depth and
then optimize for interpretive
interpretability so the end of the day
again we don't just hand off the numbers
to the editorial team then disappear we
strive to give them the best information
possible to make their jobs easier and
their job is writing the right content
putting it in front of the right people
the right time so we're trying to help
them do this so our current and future
work Facebook index I just mentioned
that actually it's like the the
competitive landscape analysis velocity
2.0 we want to make the velocity
algorithm a lot better we would love to
do a behavioral analysis of session data
that's been something we've been
thinking about for a while
headline optimization we've got some
headline optimization stuff in the works
right now that another one of our data
scientist is actually working on then
all things video because video is the
future as they say so here's um some
resources papers and blog blog blog
posts that were useful a structural
vitality of an online diffusion is a
really awesome amazing paper on how
content spreads across the internet so I
highly recommend that archetypal
analysis also really interesting um I
actually had never heard of that kind of
analysis before I started working at
Mashable my manager was like hey this
might be a useful audience segmentation
technique um I have some Python packages
and whatnot I'll put my slides online
source then you can actually like look
at these
and that's really all that I have um
thank you for listening to me ramble on
about audience analysis you can find me
on twitter at sarah underscore guide oh
and yeah that's it thank you so much
thank you that was a lot of data
scientists theory and I think that kind
of states the takeaway from the previous
talk let's please do work together
engineers and data scientists yes yes
pain the girl yeah I have a couple of
questions here other than Google
Analytics what sources of audience data
do you use yeah so for demographics um
we're currently just using Google
Analytics unfortunately we're trying to
find a better solution but we also have
data like like session data like when
you go to the website you know how long
did you spend on each page where did you
go what's your journey we can get
facebook like aggregate data on Facebook
we can't we can't get individual data on
Facebook which which is fine we don't
necessarily ever want to say like this
is you that person um
we're also like constantly on the hunt
for like better and new data sources
yeah I have a number of questions
regarding the limits for posting to
Facebook to Facebook that might not
exactly be you're part of the company
determining what to do there do I know
what is the punishment for to active if
you're too active on Facebook and this
was not only like an anecdote that
people on the the social teams were
saying but um my coworker actually
studied this if you post to actively
then Facebook stops like promoting your
content in the newsfeed so people don't
really see it as frequently that's like
that's about the extent that I know
about
but we also do a lot of work to try and
figure out like because we're actually
at a point where we're starting to
rebrand our Facebook pages because as
you saw we have a bunch of followers
unlike our main page but we have smaller
numbers of followers on our secondary
pages we're actually trying to grow the
audience's on our secondary pages so
we're starting to develop out more of
strategy in terms of like when and where
to post that's not just like throw
everything on the main page but like how
do we drive people either from the main
page to Facebook or go out and get new
audiences for the secondary pages it's a
question for you as a Python geek is
there female hockey geekus what machine
learning library
do you prefer what did you use here
scikit-learn um I think I use
scikit-learn for everything except the
archetypal analysis because archetypal
analysis is not part of scikit-learn I
actually listed the the Python package
that I use for archetype it's for the
archetypal analysis it's a third bullet
point there
um but yeah oh yeah and my last point is
scikit-learn documentation um I love
scikit-learn it's really great it's
really comprehensive sometimes if I'm
doing more stats based stuff I use stats
models it's kind of nice it has like the
are syntax of like creating regression
models which can be nice but yeah I'm
also used secular how's the learning
curve that um yeah soso I can't learn
the documentation is really written for
people who already have a pretty good
understanding of machine learning can be
really hard for beginner just just sort
of dive in and and start using
scikit-learn however there are a lot of
really great tutorials online that have
been given at Python conferences like
PyCon often by people who actually
commit code to the site killer in
project so if you're a beginner I would
definitely recommend like finding one of
those tutorials and and starting there
and then diving into the documentation
once you like
decently good understanding of machine
learning the psyche learn documentation
is awesome but you have learned the
learning curve can be a little stupid
first one final question did you
consider using text analysis for doing
your selection grouping text analysis
for which Oh for audience segmentation
yeah okay um yeah that would have been
interesting so I'm assuming text
analysis means like a textual analysis
of the URLs or is that what that person
means maybe that that would be like that
I think is like the next thing that we
want to do is do like a much deeper
content analysis of the kinds of
articles that particular segments are
interested in yeah yeah we definitely
want to do that um we've we've done some
like topic modeling stuff at Mashable we
haven't done a ton of it I'm put I'm
really interested in that subject I
don't know too much about it but the
opportunity arises for me to do that I
would love to explore that a little bit
more cool thank you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>