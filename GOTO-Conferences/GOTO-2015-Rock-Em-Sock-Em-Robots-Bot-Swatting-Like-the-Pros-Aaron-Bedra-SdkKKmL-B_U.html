<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • Rock 'Em Sock 'Em Robots: Bot Swatting Like the Pros • Aaron Bedra | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • Rock 'Em Sock 'Em Robots: Bot Swatting Like the Pros • Aaron Bedra - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • Rock 'Em Sock 'Em Robots: Bot Swatting Like the Pros • Aaron Bedra</b></h2><h5 class="post__date">2015-07-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SdkKKmL-B_U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today I want to talk about asking
questions more important I want to talk
about asking questions about data that
we already have and making choices as
people interact with our sites human or
robot most of the time we will take care
to fill fulfill whatever request they
send to us but at the end of the day
they're good BOTS and bad BOTS and in
the case of bad BOTS we're really
talking about is a case of asymmetric
warfare how many of you have a business
where the sole business purpose is
keeping malicious traffic and machine
traffic out of people's networks okay
great so since that's the case butyl
addley here this is not your full-time
job you have a job to make money or
provide a service or whatever but the
people who are doing the bad stuff their
full-time job is doing the bad stuff and
so it's not fair and hence the title of
this talk it's not about how we scale as
humans it's how our robots can find
their robots and that's really the point
is you want to make a robot that's
capable of fighting the other massive
robots out there but a point about
robots first is that the internet is
effectively powered by robots um you
know these are the robots that are good
if these robots if you take these robots
away bad things happen to your search
engine rankings you don't want them to
go away you want them to be around and
actually this is so important that we
put together teams of people to manage
them right SEO and SEM and all the other
teams like these are very relevant teams
most companies have internal teams that
do this sometimes they're quite big it's
very important especially when you're
into the e-commerce space or other
things like that that you come up in the
right places so we employ teams of
people just to manage to use good robots
but on the darker side not all robots
are created equal for example is this
request in a log file a robot are human
is it is mechanically driven or as a
human clicking on a browser robot why
it's a robot I mean it's a curl request
right it's coming from a you know a
low-level API of some sort probably just
the curl command and but that one's
pretty obvious how about this one
machine the robot there's like I've
machine a robot right human or machine
machine why it says wow what does what
does wow mean yeah if you didn't know
that what anything else give it away it
looks like a fairly traditional browser
user agent maybe google it it turns out
to be a bot the point is they're both
robots and it gets harder as people fake
user agents people do different things
it gets harder and some robots are more
trouble than they're worth and this is
the most important thing is sometimes
it's not valuable to process this
traffic um but first how many of you
actually know how much of your traffic
is not related anybody ever take a look
and see how much stuff you got and is
actually BOTS yeah not many of us do
this for the couple of you that raise
your hand how much of it should be robot
related traffic versus how much you
allow I get to the next question so for
those of you who didn't raise your hand
about bot traffic this question may be a
little more relevant who here actually
does testing and tracking
experimentation a/b testing multivariate
testing that kind of stuff I should see
a lot more hands now probably will you
do that um or maybe you're just tired
anyone raise your hand and how did the
bad robot throw off your tests does that
data ever messed with your testing and
experimentation is I never affect your
results anybody see that happen before
yeah um
yeah they'll trigger the air they'll
they'll blindly follow experimentation
paths that will create noise that may
look like you know something is working
that you that really isn't if you have a
convergence model it may take longer to
converge because you're trying to filter
out that noise it eventually humans will
win but it might take a longer time to
get the answer and so even for testing
and tracking stuff it throws off
analytics throws off isn't intelligence
and then the last question is like what
are these bots actually doing in your
site throwing off analytics it's not
what they're doing they have a purpose
of an intent of some sort and Gavin that
intent is really important but before we
go too far I want to talk about some
common types of robots and kind of
classify them and talk about how we
identify them before we go into the
deeper details the first and the most
obvious stuff is spiders things that
will basically let's visit links and
crawl your site every link available try
to capture what's going on spiders at
the root of pretty much everything else
we'll talk about all the other types of
robots usually have spiders associated
with them to gather information and the
using set of scrapers and scanners to
look for content to find things
sometimes people just use them on their
own in particular things like Google bot
right it's going to do that and figure
out what the pages are indexed them and
general it's a trivial to build anybody
can build these you can download them
they're readily available right the how
to build scraper is basically you know a
couple of steps you go to a page gather
the links you put them in a queue and
you repeat until it's done or until
you've exceeded some kind of you know
threshold spiders are incredibly easy to
spot because they deviate from a typical
behavior they visit everything how many
of you have a few min visit every page
you have before they go away all right I
guess how many of us struggle keeping
the user on one page for more than 30
seconds exactly exactly right bounce
rates is more is more what we're looking
for here they don't bounce they just
keep coming back for more but they
deviate really quickly from behavior and
more importantly they one of the biggest
deviations that you'll see is the verb
associated with with the request
most of the time we'll have some break
down some distribution of HTTP verbs in
our log files this is a bunch of numbers
that I think equals a hundred percent
but the idea is that you'll have some
distribution you take a slice in time of
all your traffic and sampling and put
this out and then compare that against a
single actor if they deviate
significantly something doesn't add up
right if it's a hundred percent get
requests and that doesn't match the
traffic you know the spiders they stand
out really obviously really quickly so
simply sampling traffic and looking at
deviation catches a lot of activity
really quickly it's a super simple thing
to do it's cheap it's effective and more
importantly the idea of this talk is
about thinking about how to ask these
questions and then designing systems
around them and I'm going to start with
a bunch of simple stuff well everything
I want to talk about is pretty easy it's
just about combining it and making
choices based on the information
velocity right how often did they hit
the site it was it once every half of a
second and it's probably not a human
unless they're really good at
computering but even then its velocity
is a big indicator of this bad robots
bad scrapers sorry by the spiders
there's going to hit as much as they can
as fast as they can and then go away for
later they might come back with a
different agenda later but that's that's
the idea of auto scrapers subscribers
are kind of an evolution of this once
you see a spider sometimes you little
file with the scraper and the skaters
have intent scrapers want your data they
want to find out what's in the page they
want to save it they want to download it
they were do something with that data
it's important to them and there are
ways to deal with scrapers there are
easy ones right the first scenario is if
you have an API and you basically just
stop them outright or set under the API
right say hey no machines go here second
scenario is you don't have an API and
they shouldn't be doing this right the
answer is really easy you just stop them
say you're sorry the third scenario you
don't provide an API and you should
stop being lazy but that's it right like
when it comes to scrapers and they want
your data and somehow you haven't been
able to provide them with a way to get
your data so they're probably scraping
your page or they haven't paid attention
or too lazy to get your API Docs and do
it the right way in this case API server
machines and web interfaces every humans
if somebody wants your data and you want
to make your data available that's what
the API is for make sure you force
robots down that path because what
happens here is scrapers are dumb and
they're broken most of the time and they
might actually get the wrong data so if
you're trying to you get affiliate see
my partners that want to provide a
broadcast your information and they're
using a scraper they might get the wrong
data and produce the wrong content and
then mess with amplification of your of
your ideas if you provide them the data
from the raw source and they're from the
real source of truth they end up getting
it right and presented the right way
down the road so it's a win-win when it
would have done the right way right if
there's no reason for a machine in one
scenario or human in other scenarios you
don't want to allow it you just stop it
this is super important a lot of us
tolerate these robots robot activity bad
behavior because it's not really affect
anything they're not hacking you they're
not taking you out it's it's kind of
tolerable bad behavior but it get
compounds over time it does really bad
things but you can you can squash this
easily like I said most of the time
scrapers are pretty done this is enough
to full of scraper in most cases um
simply put in a comment in a page with a
link and then looking at whoever follow
that link because if they follow that
link they're not human because they
didn't see it they can't click on it is
a scraper follow this link so if you see
this this is actually good enough to
throw off a lot of a lot of those papers
to come around simple it's stupid but
it's effective and now some of the you
know the newer kits that have come out
that are escaping pages they'll bit of
context aware and they'll deal with it
but start with simple this that little
trick will reduce your about traffic if
you if you actually block that track the
actors quickly pretty significantly
right off the bat and but in this
scenario you have to accept that a small
portion of intelligent actors will make
it through at some point in this battle
like I said it's asymmetric warfare you
don't care about this full time one
hundred percent is not the goal here you
want to get effective enough that you
don't notice the Delta anymore the noise
is reduced and you can keep keep moving
on that's the whole point of this and
the detection of this is similar to
spiders right scrapers are going to
basically spider first and make note and
make the noise that way but they're
going to hit pages in a consistent order
and they're going to do a lot of get
requests that's usually kind of the
habit here right and in fact like I said
a spider might precede a scrape or just
to figure out which pages it wants to
look for but that behavior deviation
which I'm getting on a theme here is
acceptable right the mechanism for
picking this out still kind of applies
across spiders and scanner or spiders
and scrapers but then there's a more
malicious category of robots which is
scanners the idea of scanner is to be
purely malicious that's it they want to
try to actually break in they're looking
for vulnerabilities in your application
that's their purpose it might have
scrapers and scanners and scrapers and
spiders built in but they're looking for
vulnerabilities in particular they're
also incredibly easy to spot how many of
you have done any payload inspection or
logging on request to come in like post
requests that kind of stuff anybody look
at that's kind of stuff what kind of
tools are using for that for that yeah
are you looking at anything from a kind
of malicious traffic detection system or
anything like that at all look examine
into payloads for script tags or weird
stuff like that yeah I mean based those
these scammers going to deviate from
behavior but they're going to submit
obviously malicious data you're gonna
look at it and you're going to see you
know a string of 50 million A's to see
if they can you know overflow something
or a script tag or single quotes or
something something instead of the
signal to them that there's a problem
here and once they find that they'll
keep going
manually and try to get through but
something that's more important is these
scanners will look for URLs that they
know to be kind of standard vulnerable
default open URLs so they produce a ton
of 40 force so easy way to spot scanners
is look at the amount of 404s that a
single actor producers or single IP
address produces and you'll be off the
charts for these scanners because that's
they're just looking for things like PHP
myadmin and you know WP dash admin and
like just defy trying to find what
applications you have running behind
your server and to see if they can say
okay great I'm running plex and it's i
found this version so i have a set of
exploits and go ahead and launch them
right and that's how this works you want
to block these like you just want things
to go away the asterisk here is
important make sure that you talk with
folks in your company because sometimes
your company what we do this doing this
for real legitimately like trying to
figure out if you have vulnerabilities
so before you just block all the
scanners out right you want to talk to
your application security more info SEC
team and make sure they don't have
planned scans like you you want to allow
those through to make sure that that's
you know tested properly so it's you
want to block everything but your own
company doing this kind of stuff the
reason I asked about tools was web
application firewalls can help here now
they actually do this inspection on the
way at the web server level laughs are
nice I don't like them but they're
they're nice tools I prefer to running
in passive mode basically passive mode
says when I see something I'm going to
let you know about it but I'm not going
to actively stop it if you ever seen web
application firewalls pitched or sold
they are the silver bullets all web
application security and it's a complete
joke what they are is regular expression
matters and that means false positives
lots and lots and lots of false
positives you will get those you will
block with it i'm at traffic if you turn
them on and just leave them but we
require lots of tuning but more
importantly they tip your hand so if i
say and submit a you know a form with
script tags and I say 403 forbidden and
then i do the same form it doesn't have
it doesn't block me you just told me you
have a laugh and now i'm going to employ
my wife evasion
niques and just get around it because
the other thing is regular expressions
are done and you can accomplish the same
you know the same inputs but it is
bypassed that regular expression because
most of them are pretty weak and so you
don't want to tip your hand when you
have the laugh you want the laughs let's
say to us that something bad is
happening and behind the scenes be
working on kind of outright blocking
them instead of giving showing it in
their hand and having them experiment
until they get by your wife because they
had you given them at that edge and
there's kind of an other classification
and these are also malicious but it's
fraud denial of service espionage that
kind of thing fraud in particular
something that a lot of a lot of you for
running ecommerce sites are going to
deal with at some point in time the
denial of service is going to be a tough
one I'm not going to say anything about
the bike volumetric denial of service
attacks like where they just slam you a
bunch of trafficking to go away that's
at a lower level I'm in denial of
service attacks where somebody finds the
slow pages on your site and just starts
to load you up with that kind of with
the quest that way right and so they're
taking you down via the application not
via the network layer these still fall
in the malicious category though right
they're doing something abusive
something malicious on purpose but they
behave differently and what these tools
usually do is they have a focused target
right they're going after credit cards
they're trying to you know buy more
things that should be able to do based
on the rules they have some kind of
obvious agenda it's like if you look at
the traffic they're obviously going
after one particular thing in one only
so they're very they're very pointed at
what they're doing and detection here is
somewhat harder because you have to
figure out what it is they're after but
once you figured out that pattern super
easy because like I said they're always
super obvious about what they're doing
and I'll show you a couple examples in a
minute of what that looks like and that
comes down to kind of like what we look
for and in this case there's one common
pattern right the verb distribution was
kind of a simple way to get you know get
going here but there's an anomaly piece
of this we want to look for anomalies
and we want to we want to start with a
null hypothesis null hypothesis and find
everything lets us reject that that's
what finds our nominees but this is the
hard part and actually this is the most
important part
is we first have to define normal how
many of you do a lot of log aggregation
how many have a query analyzer some way
to look at your logs not a lot of hands
you do that first right you have to have
all your stuff in one place so you can
see what's going on visibility is super
important to all of this so if you don't
know what normal is you don't know what
not normal is you have to define normal
first it's super important and you have
to know what has to change in order to
be not normal right so you say if these
things hold true we're all good nothing
is wrong here but if if these couple of
things change it's very important to act
and and figure out what this actually
means it doesn't mean it's bad it means
an anonymous and you want to figure out
what it means more and so here's a
couple of requests and I want to look
through these and see if you guys spot
the intent we can talk about facts all
you want but we want to look at the
intent of the request so let's have four
requests and then we'll kind of talk
through it the first request is a post /
logon has a user agents we've already
identified some things about this being
possibly machine-generated but here's
the first request first request this
address ever made second request is a
post to users king Roland credit cards
the credit cards endpoint here's the
third or the third of us and here's the
fourth request so what did folks see
here in this than that saniflo requests
trying to validate the credit card mmm
validating credit card there's a certain
kind of attack associated with then
called a carding attack or people take a
list of stolen credit cards and they see
which one still work so we do a great
thing for usability where if somebody
has an account on our system we have
them enter the credit card details ahead
of time and we validate them to make
sure that when it comes time to check
out we can separate from them from their
money as fast as possible right impulse
buys are the way of the internet and so
we want to do is make sure that
that friction is gone and so this is a
second this is a separate step to make
sure that all future purchases are just
kind of you know one button one click
whatever we can do right this service
works against you and the form of these
guarding attacks how do you take credit
cards online e-commerce of some sort
another how many of you are aware that
this happens I have seen this attack
before right if you haven't seen it go
look go ask your logs you will find it
pretty much everybody that accepts
credit cards has this behavior they're
actually pretty good at flying under the
radar as far as like the amount of
requests they do based on your size but
most people will have this and just big
credit card fraud rings looking for
valid credit cards and selling them or
possibly turning around using them
against your site when they validated
them which is the other thing right so
we've identified this is the carding
attack which we explain what carding
attacks really and I kind of are on the
surface but let's look at the request
again there's a bunch of stuff that
makes us aware this an anomaly right the
first is the login request is should
have been preceded by what a get request
we didn't have the get request we have
it's 200 here that's that's all good but
the next request is I'm sorry actually I
was looking for ya next request we have
a one-second delay in our accounts or
request one second one second one second
one second every everyone I cannot cite
my credit card number and information in
one second it hit submit that's not
possible maybe you have a form pillar or
whatever but I'm going to say that's
fishy one second on the dot every time
and we see that it's a firefox eight on
windows seven is a strange combination
of things we've also identified that the
widow's on windows thing is another
thing to look for but the user agent is
it's fishy it's not quite doesn't quite
add up and the next thing is super
important too geoip location right where
is this person coming from if I told you
this is a US company doing good space
business it's a fictional company but if
i told you that information and you saw
this request you may think you make our
start asking more questions right we
don't normally see traffic from from
bulgaria what's going on here why
is this why is it all sudden popping up
and then finally there's a bunch of 30
tues here that never get followed right
and needs to enter compliant browsers
going to follow the 30 see you when it
gets it that's that's how it's going to
work so we've seen a bunch of stuff it
lets us know this is not human and
actually more important it's not a
standards-compliant browser and so
there's something fishy is something
different about this now we could go on
the other facts here this actually
continued 10,000 more times so then you
be pretty sure it's a carding attack the
idea though is to spot it identify the
intent and stop it before it happens
10,000 times right if you do it in the
first couple of requests life is better
right they forgot to maybe they got to
try one credit card before that you stop
them okay I mean they could scan it spin
that over 10,000 IPS in a day and that's
fine but they're going to exhaust their
pool of ips before they do any real
information they're looking for volume
they want me to have hundreds of
millions of reddit cards to test they're
not going to expend all of their ips on
your site if you can make it too hard so
it's about behavior deviation what how
do we identify that i reaiize like that
right and there are tips for this there
are simple things like velocity how fast
if you look at how fast a normal user
interacts and then you look at the delta
on a single actor you can find a pretty
quick every site has different patterns
some people who are clicking through
pretty quickly some people are maybe you
have a big list of things to show and
people want to read it and look at it
and see collections of things right a
groupon we have lists of deals you might
spend several minutes looking through a
single page just to see what deals are
there so velocity is important if we see
people click click click click click
click click there's too much information
on the page there weren't that clicking
that fast unless they know exactly what
they're going for it but that's not
normal the access patterns the time of
day when arse when is somebody doing the
thing they're doing if you have peaks
and valleys of traffic you know let's
say you're on commercial service and
doing lunch or something and all your
traffic comes at lunchtime and then you
see a big spiky like 2am that's not
normal right that's something different
so access pattern also involves the path
at which somebody takes to get to you so
this is where you X and security meet a
good user experience it's actually one
of the best things you can do for
security if you design a user to go down
a certain path it's really easy to
figure out want to deviate from it but
if you have a bad user experience and
lead people all over the place you
actually can't figure out when they're
deviating from the standard common path
so go back and redesign your user
experience to drive people down a common
path you'll see the deviations they'll
be way more obvious let user experience
is super important and how you drive
your users to act on your page helps you
figure out when they're doing when
people are just kind of moving off like
it's a time of day super important
geolocation this is huge it goes back to
access patterns but where are people
contacting you from what's going on you
want to know where they're coming from
and really kind of time of day and
geolocation is also super important
right so if your aside back to the lunch
example if your lunch than the in the
central time you'll be looking at noon
if your lunch in London it's going to be
a different time so you want to match it
up with lunch in whatever country you're
serving if it's if it's office it's 2
a.m. in london and it's a look and
you're looking for london stuff you know
that's that's going to be the anomalous
thing so geolocation also comes in that
time of day and access pattern stuff you
want to see what do you want to line
that up so it's multiple factors that
have come into kind of modeling this
again verb distribution was a example we
already showed user agent so user agents
fine but user agent spoof able anybody
can put whatever the hell they want in
the user agent header and off we go
right we don't really really check that
it's not something you can trust but you
can combine this with other information
like the header order how many of you
ever seen the the actual order in which
the headers come down from a browser
anybody ever looked at that does it
change no never changes every browser
has a fingerprint the order in which the
headers come down is static for every
browser so if somebody says I am Safari
and the headers don't come in the order
that Safari comes in it's not Safari
just straight up like that it is a
blatant lie now it doesn't mean it's
malicious but it means that they're
lying like you can identify that almost
uniformly in every single situation
and so this is super simple trick to do
an around a small little module for your
webserver that'll give you the raw order
the headers and then fingerprinting and
so you build up a little finger for
database and just check success rate how
often did the request succeed like I
said the scanners time to throw us off
pretty far but you know an example here
this IP address made 1878 requests and
had a zero percent success rate they
were all 40 force this is this is not
this is not do it don't you want to
block that right but you want to have
his kind of sistex available to you
right 403 is 40 fours five hundreds and
you know 40 ones they're all important
all relevant pieces all right 500s are
also a big giveaway if you have a couple
five hundreds it means you got an error
and we have a couple thousand five
hundred two means they're probably
trying to exploit something they found
to see if that's 500 could be you know
dealt with in kind of nefarious ways all
this stuff there was surface stuff we
can go a lot deeper here because what
we're looking for is really intent you
want to know what they're doing why
they're doing it so we can kind of guide
them off the right way or really want to
figure whether they're a robot or not
actually that's that's the whole point
of this talk right finding out whether a
robot is beans examining how a robot
things a robot thinks and it behaves
differently about a human and we can
play on that notion and actually make a
robot tell us the robot by just asking
it to do certain things that a human
would otherwise normally do or in this
case a standard compliant browser would
normally do and it ignores it's been
programmed to do a certain thing and it
will do that certain thing because it
was programmed to do that sort of thing
like the 302 like following 30 G's right
if it was never programmed to do that
it'll just keep going and any normal
compliant browser would just file the 3x
you so we can actually kind of if we're
wondering if or suspects have asked the
ball to do something that you know
healing would normally do the right way
and see if it doesn't it doesn't you're
identified a robot and so we can
basically push the bots to tell us their
BOTS by a by just asking to do certain
things and ultimately we have a goal
right we want to block robots quickly
possible that's it and the story it's
few requests possible the robots go away
we move on with our day it's not a court
business we don't really care the end of
it I wanted to be afterthought I want to
go look and see and make sure that
everything is going on and the bluffs of
bats are being blocked but I want to go
back to the business as usual there are
other things we can do if we go deeper
we can embed scripts and our pages that
basically ask the boss to tell us about
themselves actually everybody tell us
about themselves but we're really
looking for about information right a
small set of JavaScript the posts back
to you can be correlated back with other
traffic patterns to say hey what what
about this is significant what about
this browser is user significant now
that sounds fishy right like you're
doing tracking you're asking questions
what you're really trying to do the
whole intent though is find the bots
JavaScript does not bunch of stuff here
and there's one piece in particular it's
actually great the Navigator objects I'm
gonna sue manator object ever seen that
before pop open your browser or your dev
tools and go look at the Navigator
object or just look at the docs than was
own website they're really good for
example a couple of things we can ask we
can ask the user agent so we can
actually see what the browser says and
what the what the actual header that
comes down the stream says and if they
don't match that's you know usually an
indicator or something interesting the
screen resolution how many of you ever
run a browser one by one if you see one
by one to dead giveaway that it's not a
human because we can't quite see that
but those adventures are important the
platform that's it's running on it's
important right you might you might it
might say that it's linux in the browser
and then the user isn't says windows on
windows wait a minute what's going on
here or it comes back as Apple right you
can also use flash flash is kind of a
dying breed right you splashes to be way
more valuable here but less people most
people have flash blocker installed
flash is kind of dying away javis were
becoming more powerful so I bet on
JavaScript this point but you can still
use flash but basically the details make
it easy to spot a bot I mean there's the
one obvious thing if it doesn't execute
and come back to you it's it's probably
about right they might have JavaScript
disabled then again most of us don't
have sites that work when javascript
disabled so the user experience is
pretty crappy anyways but we can take a
look right I talked about user agent
before right see if it lines up with the
user agent that was actually sent to you
over the wire the screen resolution like
I said was for was interesting but
there's other stuff cursor movement the
pattern of movement the mouse makes a
human has normal patterns of moving the
mouse they're smooth even if they're
fast they're smooth a cursor will track
across the screen even if it's faster
slow there's always a pattern robots
kind of headless browsers or sorry web
drivers don't do that they just jump the
coordinates will just jump from here to
here to here to here they know how to do
the coordinate in the page and the
amount of it will be so erratic that a
human could never have possibly
generated it so you can look at the
movement patterns on the cursor and see
the deviation behavior see that kind of
fingerprinting what plugins are
installed you can ask for the plug-ins
that are installed for most browsers our
faults are the plugins so it says that
you know I'm Internet Explorer and
internet explorer has a default list of
plugins but it's like the plugin either
come back empty or say you know I don't
know some chrome addon right that
couldn't ever possibly were going to an
explorer you know there's a mismatch the
fingerprints the browser fingerprints
are interesting and they actually have
gotten a lot better fingerprinting used
to just be kind of like what's install
what fonts are installed hash but now
that stuff exists and even down to
details of load me the canvas object and
ask so much of questions about canvas
and WebGL and there's fingerprints that
happen even put inside of that and so
you also store the fingerprints of known
box so once you decided that there's a
certain browser fingerman that's bad and
you see that fingerprint again you can
go immediately to you know don't pass go
and block them right away because those
standard kind of like web drivers the
kind of things that are that really will
return data will always have the same
fingerprint you know to forever there's
an open source library from using called
Val this is not this is not that with
the company the user's name is just
valve its fingerprint jas and he also
has something called fingerprint ASG
which is a kind of reworking
but it's kind of a it's a great way to
kind of get started and see how you
interact with navigator how you and I
are with WebGL and canvas and the things
to give you that fingerprint
availability so I want to wrap up and
give some time for questions but a
couple other things to remember is that
we do employ teens and people to manage
the good robots this is a well-known
practice it's it's established what we
haven't done yet is building teams of
people that manage the bad ones it's
important to actually build a team who
can do this to write these bad robots
are becoming so much more prominent as
time goes on that their noise ratio is
disruptive like the best case is the
disruptive the worst case they actually
star breaking in and doing bad things
the carding attacks the fraud the abuse
it makes you look bad it slows you down
it messes with your metrics otherwise
just causes issues it's not healthy to
allow this stuff we need to make a
system that lets us do this right
detection is part of it you want to be
able to figure out who's doing what and
when to block them but you also want to
be able to block them when it's time if
you ever tried to do this in a normal
way it means putting an entry in a web
server or a load balancer and reload in
the configuration and so that happens a
couple hundred thousand times a day or
10,000 times so they are actually just
ten times a day and you have a couple
hundred web servers it kind of sucks to
reload that over and over again the best
ansible playbook in the world doesn't
really make this pain go away right we
have to have something of more dynamic
you have a place you can look it up and
so there's an open source tool out there
called rap sheet that kind of does that
it embeds itself on the web server and a
past year engine X and uses an ephemeral
cash for lookups and so anything that
you have behind the scenes can shut data
into it and it will quickly look up the
information and decide what about to
block somebody so you can have a dynamic
blocking rap sheet is just a couple
projects a because it's of a couple
things written in C it's an open source
project under my github account or
actually has this on goetaborg now and
i'll link to it later but it's a small
amount of C code that just takes a look
at what's in what's need cash and like
10 knots or block somebody and then does
some logging and that kind of stuff it's
available as
server module 4 engine X and for Apache
there's a core library that they share
and see there's also a Java version of
the lookup library that you can embed as
well for if you're doing straight
servlet stuff not behind a web server
and down to the application environment
it's it's kind of that that buffer
before you hit your app environment it
can also trigger stuff down but you have
that slot of band processing you have
excellent reputation feeds your other
things and there's a lot more I can get
into if you want to talk more about this
find me at the party after party I can
tell you a whole bunch about how this
stuff works because we are actually
using it now and we've actually built a
bunch of visualization around it so we
can look at any point in time and see
what's going on what are the bad robots
what are we blocking you know this is a
picture of a capture where you're seeing
several thousand or hundred thousand or
million blocks in some cases I mean it's
pretty easy to tell the ones in the US
on those corners or Amazon the AWS and
if you zoom in you'll see just a bunch
of AWS stuff and you know actually
that's important to remember right the
asn of the ugly request right who owns
the cider block but is a contacting you
is important because it's pretty common
that stuff from digital ocean and
Rackspace and AWS and any of the VPS
providers it's going to be robotic
traffic right people aren't renting AWS
instances to as to computer right that's
not that's not normal they have computer
so they don't need other computers to
just do normal computing there it's
driving it's executing programs and so
you want to think about that sometimes
and not any of its inherently militias
or bad it's rented space so anybody can
do it but whatever they want to it for a
little bit of a price and actually those
fraudulent credit cards that people
validate on your site they use those to
buy time on the hosting company so they
can ever be tracked back and never get
shut down but ultimately we want to
reduce the noise bought text is about
reducing noise and figuring out what
really matters for your business you
want to reduce the impact of attacks if
BOTS get driven away before they can
really figure out a problem on your site
you're less likely to have you know kind
of a drive-by vulnerability attack so
chances are you going to get have some
kind of security incident on the road
but you don't want to get hit by the
scanners right it's embarrassing to get
hit by the robots and get hacked and it
shows kind of lack of maturity so you
want to avoid that you want to get those
robots away and of course the end of the
day good defensive programming and a
good good software security practice
helps you do that but you also want to
make these scanners go away right the
more they spot the more vulnerable you
end up becoming and you want improve
competent confidence in your data this
business analytics intelligence testing
tracking it means money the faster we
figure out how to convert capture drive
interaction the faster we get to the
money and they of the day and I'm
speaking on behalf of you know an
e-commerce platform but at the end of
the day when you're in that that range
you want to drive the interaction and
you want to know what works and these
robots that offer you and so it takes
you longer to find the money and at the
entity that's that's really what matters
so I've got a couple references for you
get out calm / rap sheet has all of the
packages code available for the rap
sheet stuff that I talked about and like
I said if you want to know more about
that I'm happy to talk at length about
that later the Navigator object from the
Mozilla documentation like that's the
key to fighting these kind of simple
little scripts you can embed in your
page to do some checking analysis what's
going on there the fingerprint Jas and
js2 are the library's I was talking
about about the basic fingerprinting
there are better tools out there the
commercial services I know there's
nothing I don't to endorse anyone any
particularly but you can go out and see
fingerprinting services that are really
good they have big giant expansive
networks of data you pay per request
basically they can be pretty expensive
but there are some really good services
that do this and they're driven by a lot
of really interesting science and so I'd
encourage you if you really want to go
down the way if the fingerprint jf stuff
works go and find those commercial
entities I can help you out with that
too because you can get some really
great data from that and finally we have
some time for questions
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>