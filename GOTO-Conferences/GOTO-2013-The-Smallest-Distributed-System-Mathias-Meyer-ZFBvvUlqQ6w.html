<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2013 • The Smallest Distributed System • Mathias Meyer | Coder Coacher - Coaching Coders</title><meta content="GOTO 2013 • The Smallest Distributed System • Mathias Meyer - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2013 • The Smallest Distributed System • Mathias Meyer</b></h2><h5 class="post__date">2015-10-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZFBvvUlqQ6w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome I want to talk about the
smallest distributed system and probably
in the literal sense as much as in the
physical sense it is really really small
my name is with the admirer I go by a
roid rage on Twitter there is no
relation at all to steroids in his name
um I work for a tiny company in Berlin
called travesty I it is a kanay hosted
continuous integration system and it
started out as a very simple idea
originally that was by the end of 2010
there was the github was on the rise for
sharing and collaborating on open source
project but there was no platform around
that gave projects the ability to run
their tests to have a you know in what a
well-integrated continuous integration
platform that would whenever a new
commit came in or when someone
contributed a patch would take that
patch run it and say everything was
green with that with that thing we
started out as an open source project
basically a platform targeting open
source product as much as our own code
being open source but we since then
moved into a commercial space as well as
reporting private projects but that is
not really what this is about today I
want to go back to 2011 2011 was when we
saw very very full initial traction we
had a quite a few open source project
adopt a dog Travis CI and we ran about
seven hundred bills per day that
included project like rails um a couple
of smaller projects but it was not a lot
all that run of a single server we had
one box that basically ran all of the
tests it was virtualized using
VirtualBox was a pretty good head up
into it and all but it actually took us
quite far um yeah and the application
itself at a very simple architecture if
it's basically it took a few weeks to
build Travis CI the ideas originally was
to have you know to head on to the
status quo of continuous integration
systems one idea was that the user
interface should be live updating you
know that when when a build is running
you should see the lock stream it live
on the UI should update his
builds finish this built started and
stuff like that basically should I was
supposed to have a nicer user interface
than that other continuous integration
system that was very popular and still
is at that time but it was supposed to
make all that freely available for
anyone to use basically when you had an
open source project you could plug it
into Travis CI and it would run your
tests and it would run them fairly well
the original architecture was very
simple we had a web application that
talked to the database that web
application was responsible for serving
our API for accepting new commit
notifications from github whatever
newcomb it is pushed to get help we get
a wipe of notification from them saying
here's the new data that just came in do
something with it that's so that was
what the web frienda was also
responsible for it basically took that
that payload and turned it into
something that was runnable and the
component that was responsible for
running that stuff is called hub it in
the end turned out to do a lot more than
that but that's its basic function right
now in the in this scenario and there's
a fourth component that's called jobs
which is basically an agent or an agent
that was running on a one built server
that would take new jobs execute execute
a couple of scripts in the virtual
machine and basically send a
notification whenever it's finished that
component was also responsible for
streaming bilblox so when as a builder
and would just see it would just sift
off the late the output from the build
and send it send it in chunks and this
is where hub gets a little bit more
complicated because jobs basically owner
only interacts with hub and the only
thing it basically sends is that hub
sense jobs here's a noob using a job run
right what jumps does is it takes a job
runs it streams the bills are build
locks and the build result and that
makes help already a little bit more
complicated we have a little orange the
orange rabbit down here which is
RabbitMQ it's basically the single point
of interaction between the jobs
processing in the hub itself so it kind
of became much to my dismay are one of
the central components in our system we
have
q and how would also and that would that
is a part where the life updates come in
how would also notify push her when when
new jobs when new job results her locks
came in through epidemic you help would
take thee to take these things and push
them and push them forward to pusher
using pusher we could then do live
updates in the user interface simply
push like by now we push 40 mil 40s 50
million messages per day to push her but
back then was fairly minimal but it was
it was a great aha effect for people
looking at the user interface I just see
all of that update of life and it's
still a fairly simple architecture but
as you can see it's already somewhat
distributed but it was built with with
good intentions in mind it was basically
assuming everything would work all the
time and everything with work in ways
that we can foresee and that we can
expect um unfortunately that really
didn't turn out that well if we forward
to 2012 we already we added 1 magnitude
of bills we're now at 7,000 builds per
day we've been adopted by a couple of
open source projects by a lot more
popular one in the PHP world we had a
lot more languages we headed Erlang
rated Java we headed pearl even PHP
Python all the popular languages were
basically supported out of the box the
problem was that with the platform being
adopted more and more we had a lot more
responsibility to keep it running
travesti I was originally billed as
basically an idea as n is an ideal and
it was run by people as a side project
basically is something they you know
that people could use and something that
well it was just it was just run on its
own and we would just push a little bit
of code every now and then and
personally that really didn't that
really didn't work out that well because
we started to have problems we started
to see problems in our architecture and
we started to see everything break at
the seams basically in the very literal
sense and the biggest problem we had was
actually visibility the most
embarrassing the most embarrassing bits
were and that's when i joined travis
yeah i was in the beginning of two
12 was that people come from the
community came to us saying hey my bills
are not running or something something
isn't right my bills are crapping out my
dolls are ever hang out because we
integrate with a lot of external
resources we integrate with dependency
dependency mirrors like maven rubygems
PHP's dependency system pearl these
post-event of these pythons and so on so
we had people from the community
approaches and say this stuff isn't
working and that in the in the in the
beginning turned out to be our biggest
problem we have no idea what travesty I
was doing it was already a fairly big
distributed system we now had 10 build
servers running running tests so we had
a total of sixty VMs available and that
meant a lot of stuff could already start
breaking at any point in time and we had
no idea what would break and we had no
idea when it would break because we
didn't have any visibility by means of
metrics we didn't have any aggregated
locks and we didn't have any alerting
and that was kind of embarrassing it was
embarrassing that people actually had to
push us to look into things and it was
also embarrassing because we started
working on a commercial version you know
being notified by customers that my
product is down is still one of my
biggest nightmares so we had to work a
lot to actually to actually make this
happen that was my first project and it
was the most important one because in a
distributed system when you when you
have no visibility you cannot reason
about what it is doing in production
that's why my biggest takeaway from this
deleted for in this particular piece is
that if when you build the system you
need to think about monitoring upfront
you need to think about how how to
monitor it what to monitor for and what
thriller it on and that's not really
what Travis here was built built based
on it was just you know assumed that it
would just work and that things would be
running smoothly and that's right ah
that thanks for running smoothly and
that was my biggest takeaway that in a
distributed system even as small as it
is you need visibility you need to be
able to see what is happening in it at
any point
time you need to be able to reason about
what it was doing at a particular point
in time and thankfully for us this is a
lot easier in a smaller distributed
system than in a system that does the
thousands of requests per second that
handles thousands of things in the
background in our system which is still
fairly small it was easy to reason about
what is doing but we actually had to
work on making this happen we had to
work on adding logging we had to work on
adding the ability to trace requests as
they came into the system as they were
handled by the system to actually
realize what is happening because it's
it's it's become the central bit of our
system to be able to be able to monitor
it to be able to run it in production
but the biggest problem we have was once
we're at that monitoring we actually saw
things breaking that's why visibility
brings the brings a little bit of a
responsibility with it if you see if you
see feeler happen you need to do
something about it otherwise you don't
need to put in monitoring the first
place but it's what distributed system
become somewhere often of a notion that
you accept that things break and you
accept that you have to do something
about it that was my realization of the
last year when things are broken we have
to do something about it unfortunately
things started breaking a lot that was
the downside our original little
architecture in 2012 started falling
apart as we did seven thousand bills per
day we push a lot of lock messages every
day and it just wasn't reliable it keep
did kept falling over it just kept one
component in particular I kept falling
over and that's the hub the hub was
doing a lot of things it was handling
bilblox it was handling notifications
basically when when it built was
finished we notify users that by email
or by campfire via IRC that of the built
result it was handling build requests as
they came in from github it talked to
github it talked to github we interact
with the API a lot and it talked to
pusher and it's synchronized user data
11 bit of Travis is basically to go sync
all of the data for use that we have all
over the repositories that they have
access to and basically store that
information on our end so that we know
who has access to which repository
and how was a single process at the time
and a single process is not really a
distributed system and the problem with
that single process was that it was
impossible to scale out we could not
just fire up a new one and have all of
this run in parallel we had to have this
single process and it did a lot it did
actually fairly well it did up to the
lock processing inside a single
container did i'm up to a hundred
messages per second but that really
wasn't enough because it kept us from
drawing and one particular piece that is
has become very ingrained in how I think
about distributed systems I have a good
example for and that's the giddy there
talking to the github API we're one of
the github api's heaviest consumers we
fire hundreds of thousands of requests
through the API every day and we assumed
all that we when a built request comes
in we fetch the build configuration
which is stored in the repository from
github we synchronize user data with the
github API and we do we update commits
based on how I build wind get up you can
have when you when you have a pull
request in github you can have a nice
notification that the build has failed
or it succeeded and that's also what we
do we treated the github API as a good
friend and you know it's this is more
about the integration with the github
API then with me bitching about that it
is terrible it is more is a learning
point for us we basically treated their
API as I had a good friend we call it
and we expected it to pick up the phone
and answer us pretty quickly and always
enter and pretty much always answer us
and that turned out to be a wrong
assumption it did not always answer as
it did not always answer us with the
data that we expected it did not always
answer as with the reasonable result it
throws all kinds of crazy error codes
and you have interface and at some point
about a year ago it really if things
really started falling apart that was
one of her biggest outages here
and that was we that was because we use
the somewhat undocumented feature of
their avi I that's where it already
starts but it was a feature that was
added for us and that feature at that
time was simply disabled because it
caused problems on the other end on our
end it also cause problems because all
hell broke loose and we basically
stopped blowing anything we stop
bullying anything because when a new
build comes in we go to the API which we
check a few things that need to be ready
for us throughout the build and that
requests basically we gave that request
initially about I think 10 minutes and
on that particular day um the requests
actually started taking ten minutes all
of our API requests basically started
timing out but the time up was enormous
when we when we do seven thousand bills
per day we basically we get at peak
times we get about a thousand and build
notifications per hour back at that time
so one handling one built request in a
setup that is basically just a single
process and requiring ten minutes to
handle that single bit request is not a
great idea it basically made everything
fall apart and the worst thing is our
built processing did not handle fur hair
is very roll so after the timeout
actually hit after 10 minutes we would
just drop the belt request we would just
silently drop it on the floor and move
on to the next one and time out again it
was not a great it was not really great
it was a very long night for us to
figure out what actually happened we did
have logs that we did have metrics at
the time but it wasn't enough to
actually figure out this particular
problem what we learned from that was
that all the external resources that we
have that we use need to be treated as
something that could break at any time
and that doesn't really mean that we
need to protect ourselves from that we
need to put up a barrier and stop
talking to the github API but that we
need to have means in place to fail fast
and to respond to the response of these
failures usually when they get up api is
found or is unavailable it's a temporary
issue it will come back within an hour
and for us it started to be a business
decision do we want to drop build
requests when they get up api style or
do we want to continue to function and
we decided that for our users and for
our customers it is important that these
builds run so we had to do something
about that what we basically did is we
isolated all the API requests and we
added shorter time outs much shorter
time outs they're still way too long in
one way in their own way but they're in
an acceptable fashion basically that
particular time a request is now down to
I think 20 seconds which is still a lot
but if it if it times out we just we
just ignore whatever that whatever that
API call was supposed to do is supposed
to be returning and we continue doing
what we wanted to do but then a self was
not enough because it only allowed us to
fail fast but it did not allow us to I
don't know to retry basically what would
the other thing that we had it in
response to that is after isolating it
to win the request timed out with an
error we headed retries to it and to
make it a little bit more resilient to
two problems in the github API and you
know basically stopping reducing the
pressure that we put on their on their
api we headed exponential back-off
seeeeee so this is the meme that has
slowly emerged in our application that
not even talking to external external
components but even talking to our own
components we treat them as something
that which we started treating them as
something that that can break at any
time that could plunge into chaos at any
time and there is an inherent
uncertainty about using external API is
and even about using your own AP is in a
world where you move to basically
service-oriented architecture basically
every application every application that
use is a means of uncertainty they can
break it any time and because everything
breaks at any time it's the unfortunate
reality of a distributed system that
things just break and we had we started
having to deal with that uncertainty
because Travis was built with you know
very good intentions it just assumed
that networks would not partition that
api's would always be available no
matter how many requests for
at them and that our own components
would always be available and just we
have to learn from a lot of from a lot
of failures and outages that that's just
not the case the dealing with
uncertainty has become one of my memes
for distributed systems even when you
have full control over this assume
itself you cannot assume that everything
is always running it all the time and
that is always running well there's
always another problem and at some point
they will come together and break
everything spectacularly leg on that
this particular weekend it took isn't a
full weekend to figure out what happened
with a github API and in the end they
basically flipped the future back on on
their end and everything was working
again that was an unfortunate weekend
for us but I want to get back to the hub
and to our problem of just being able to
run one process hub did all these things
and all on their own they're pretty easy
to break out we could break out
notifications we could break out
processing the builds as a kid's commit
notifications given from get out we
could push out synchronizing data with
github because it's not really concerned
that needs to be you know in the in the
focus point of what is running of what
needs to run all the time so we
basically had all these blocks that we
could basically just pull out the pull
out of the hub and make up just a tiny
tiny tiny box that by now really is only
doing two things it's scheduling bills
basically when new bills are available
it pushes them on a cue for the job
process to run and it processes the
built results so when a job is ready the
hub will get a notification that will it
will do whatever is necessary to process
to post process it built so now we had
components that took care of
synchronizing and talking to github when
it built came in components that handled
the build blocks itself and components
that handle called the build
notifications it send out emails
temporary notifications IRC and all that
and that was pretty good and it was
actually fairly easy for us to do
because the responsibilities were
already very isolated so in the we ended
up with very simple processes we had
very simple apps that we could scale out
pendant Lee we could scale we could have
notifications process at any time when
the queues were backing up and the
queues are always backing up the other
mimin of the distributed system there's
always a cue that's way too full so that
was actually we solved that problem
fairly easily it only took us a few
weeks to break out these apps which was
very good it brought an unfortunate
problem with it though our code is
structured in a way that all of the
business business logic isn't is some in
its single library it's called friends
core and it's that giant box in the
middle and we have all these components
around we have treads tasks we have the
tasks which basically involves
notifications you know sending email all
that stuff we have the locks processing
we have the API which basically our WEP
user a web interface talks to we have
hub and we have gatekeeper which takes
care of synchronizing data with github
and handling when a build request comes
in and so we suddenly had this big ball
of mud in the middle that all drafts
were depending on so scaling out these
applications suddenly turned into a
dependency problem because this core
library contain all of our code all of
the apps depended on it and to make sure
that every app was still working after
we made a change to the code we had to
make sure that we deploy all of them so
they started to be a certain fear of
shipping a new feature because of this
giant big ball of mud in the middle we
paid dearly for a lot of architectural
decisions and what we what we wanted to
do what we want to do instead is focus
on very small dependencies and very
small responsibilities we have the
responsibilities in our applications
already they're just not in the code the
code is nicely separated and all but
it's still one giant library that all of
these applications use so it started to
get into the way of us shipping things
as deploying things it just became a
matter of confidence and that's why what
we're currently working on is getting
this big ball of mud tearing it apart
and pushing all the deep into that all
the responsibilities and the
dependencies for that into the small
apps because they're already fairly
isolated it's just the code
it doesn't reflect that very well that
was an interesting revelation that
modularity is not just something that
you have you know when you separate apps
when you split out applications that are
like little services that handle that
handle specific responsibilities
modularity is something that is
reflected in code but it's also
something that is reflected in your
dependencies if you have one big ball of
mud like we do you will have less you
will have less confidence to ship code
because there's always a fear of that
one change and that particular code
could affect something in this up FJ
other piece of code and static analysis
all that kind of stuff unfortunately
wouldn't even help with that so we're
working hard on breaking all of these
dependencies out fully but it's a lot of
work unfortunately and yeah one of these
components is worth a deeper look
because it turned out to be the biggest
culprit of us being able to scale out
because breaking all of this stuff out
for char Messiah was not really just
about having a distributed system it's
it was about being able to grow we
started you know we started protein
another magnitude of builds per day and
we could see back at the time that the
the architecture that we have wouldn't
really cut it locks in particular is the
more interesting example because lots of
the component that handled lock turns as
they come in from jobs it is
traditionally was one processor and that
single process was basically responsible
for updating the database and sending
the lock chance to pusher and in our old
architecture updating the database
basement updating a single row and a
column which contained the entire log so
whenever a new chunk came a new part of
the lock came in we would basically
stayed an issue an update statement to
the database and basically concatenate
two strings together it was rather
efficient but it meant one thing it
meant one it meant that this particular
part of the coat was depending on the
order that the messages came in it will
always rely on that the messages came in
the right order when they were out of
order suddenly you would have a broken
the broken door lock in the database and
that was not very pleasant and the same
is true for a web user interface um
pusher again you know this is the other
responsibilities for the locked library
it sends the messages to pusher which
then in turn forwards them to all the
people that have basically a browser
open and want to look at the built lock
is it tails in life and how that usually
works is we have this one large
processor and we had all these single
build jobs they were basically pushing
messages onto a revenant q.q lots
presses it would take them one at a time
and would update the database with it
and would send would forwarded to push
her and this process actually did fairly
well it at peak times it would do it
would process a hundred messages per
second it was slightly it was already
scale out a little bit so it is used to
use threads to be able to do things
concurrently but it was not doing very
well and 100 messages per second was
started being our daily average of
messages that we did so we could see
that this is going to be a ceiling that
we're going to hit at some point and
even when we have the cube backing up
because there's always a cube backing up
when the queue suddenly would get full
and we would have a burst and lock
messages that processor would not be
able to catch up with that with that
burst so that started really turning
into a problem and the court problem of
this of this entire process was that all
the lock chunks were required to be in
the exact order you know the locked
processes are relied on the lock chance
to be in the exact order that they would
appear basically in the in the built
output and relying on the correct order
in this case turned into a scalability
issue for us if you need to rely on
ordering in a distributed system you
come into the realms of coordinating
coordinating several several nodes to
find the right ordering of anything but
it turns out that we could actually do
something fairly easy about it and
unfortunately doing that had a lot of
consequences for the rest of our system
but what we ended up doing is we made
the ordering a property of the message
itself so rather than relying on
RabbitMQ on the message to be pulled off
the rabbit and QQ in the right order we
basically determined the order in the
message itself every message got got a
clock basically a counter that would
identify its position in the in the
india in the complete build lock and
that may be fairly easy to assemble
locks later we only had these chunks
suddenly we started storing instead of
updating a single column in the database
we would store the these small chunks
and at a point of time we could
basically take these chunks and it
simply reassemble them again into a
complete log just based on that counter
and it was quite a beautiful beautifully
simple thing because the jabra a job run
is always just responsible for a single
job so we could make sure that we don't
have we wouldn't have any any conflicts
between multiple job runners from
updating the same counter which is
another or of a problem I would like not
to have so we could keep this counter or
a around and it may it suddenly
made things really really easy rights
were actually faster because we only
wrote single rights which in Postgres
which is our main database means you
just append something to the transaction
log and you're done with it basically
independent only right and that was
fairly beautiful and i gotta say the
ideas from this um and this is where the
Renaissance fits and I got the ideas
from this for this it sounded like a
fairly obvious idea but the ideas for
this are the inspiration for that came
from a paper from Leslie Lamport which
it was called um what was it called time
clocks and the ordering of events and
let part in that paper talks about using
a clock um which in this case is can
just be a counter even a counter can be
a clock in this case and whatever a
message is passed from one from from us
2 receiver the center would be commended
what increment o'clock so you could when
they were in the receiver forwarded the
message again to another process that
would again an increment that clock
there are some certainly introduced
intricacies involved than that but the
basic idea is just that as a message
passes through a system that counter
would just being committed it's very
simplest idea and we can actually use
this simple interpretation of what lamp
or got about an hour set up we just
needed this one clock to being able to
identify a message later on so we didn't
have to worry about just leaving one
locks press is running we now have five
we can process that too I think our
Peaks are 300 messages per second I we
don't have to worry about reassembling
the logs because that lets simple
counter is basically what it allows us
to do that at any point of time it had
big effects on what our web user
interface was doing but it was worth the
trouble and that's another thing for
distributed systems one of my many
learning is simplicity you can break if
you can make things as simple as
possible they will be easier to scale
and there are actually be easier to
reason about reasoning about having to
reason about the ordering of messages
you know pro having to process them
without having an explicit order is for
me a matter of complexity and I think
Richie key would actually agreed in this
case he dislikes ordering we're having
to rely on order he called it one of the
comp like one of the things that complex
and he did he does use complaint
software systems and so this little this
little digital means just putting a
counter into the message allowed us to
actually scale out this beautifully it
forced us to update the user interface
because the user interface was also just
taking these events and impending them
to the dumb you know as built were
trailing alive um but it worked fairly
well it was we had two approaches
originally in the web user interface to
do that one was just we would just
buffered messages you know if messages
were arriving out of order which is
buffer to buffer the later ones wait for
that message that was
in order to arrive and eventually flush
all of them out no matter you either
after a timeout or when that message
finally arrived so we could make sure
that the bill block would always be
showing something and not not being
delayed because a single message was
missing because sometimes a single
message can actually just be one
character when the bill when it build is
running and it just outputs a dot it can
just be that character that is actively
running it made our user interface a
little bit more complex but it made the
rest of the implication a lot simpler
and large the locks processing is
actually the one process where we've
broke out all the dependencies and made
it a separated process and I think it
has like 30 lines of code in total which
is a lot better than having it rely on a
big ball of mud that shares the same
dependencies so in 2013 we're now doing
45 thousand bills and per day which is a
pretty good pretty good cut but we're
still paying a lot of we're paying a lot
of debt for the initial decisions that
we make the architect for once but this
brings me back to uncertainty these
architectural decisions they may look
bad in ayia site but you know when when
trying to say I was gold who knew that
eventually it would go from just a few
hundred bills per day to almost bowl a
forty five thousand votes per day no one
knew that it was an uncertain chapter no
one no one realized that it would
eventually turn into into a commercial
platform as well but we still somewhat
pay the price we're still have to handle
that we're a small team but it's just
how it goes it's like I talked about
these barriers I talked about bears and
treating external API is like you know
your enemies or treating them like like
something that could break anytime but
that's really hard to foresee when you
have when you have no control over you
know the other parts of the system and
it's really hard to you know make the
trade off between doing it put all of my
time and handling all these failures in
this uncertainty upfront or do I do that
you know as the system evolves as i see
it break in production because I you
know I gotta be honest as sad as it
makes me to see brain hear things break
in production I always get excited about
it
I can usually I can learn from it now we
have better means to actually learn from
these incidents and see where a system
still needs to be improved and one of
the thing one of the things we're
actually need to improve and this is one
of the last less puzzles that we still
have is all of these components still
use them a the same database which you
know if you have some if you have a
diagram where everything points to a
single component that's not good because
when that component breaks everything
will break and we just had that last
week our main database fell over because
of an ec2 issue and whole nothing Latin
and this is one of the things we're
working on because our locks processing
is now doing up to 300 visitors per
second at peak times and that actively
effects the reads from our API so we
have this problem where we can easily do
300 300 writes per second but being able
to doing that in our current
architecture effects you know people
using our API people basically looking
at our user interface and that's not a
great user experience but it's another
one of these learning experiences at
some point we will have we will
hopefully have multiple databases in
this setup and it will all be a little
bit easier to separate to isolate these
components from each other because if
you look at locks processing it's a very
isolated process it doesn't do anything
doesn't do anything that's related to
the rest of the system locks process is
basically just put all these lock turns
into the database flater aggregate them
into a big lock and then eventually
archives them to s3 that's always logs
doing it doesn't need to right through
the main database so we have fairly easy
means to make our system you were more
distributive but making by making it so
we'll have to deal with you more
uncertainty about which the which things
will break and we'll have to have more
monitoring and we'll see more failures
but in the end we will have more
simplicity and these components by
itself will be easier to manage on their
own then as one big ball of mud thank
you
we have questions that came in so thanks
for submitting questions if you have
more questions getting their ass and
live or submit them through your app
also remember to vote on your app vote
for the talk the right answer is green
by the way and tell anybody I told you
so first question what is the best book
or resource for engineers and developers
getting into distributed system X
pitfalls patterns best prep let me know
when you find it it's a really hard
question yeah it is a hard question i
Rihanna slee I have yet to find one
there's not really a good book entry
introducing this kind of stuff I found a
lot of inspiration for how well not even
just how to they have a single algorithm
to you know handle increment encounters
but I got a lot of inspiration from web
operations books to be quite honest but
I actually started reading papers like
Leslie LAN ports paper there is some
fairly nice reading lists on papers for
system engineers out there I think if
you google them you will actually find a
few blog posts and these are actually
the best resources some papers are not
very approachable but Leslie Lamport for
example he wrote you wrote a lot of
stuff that guy I don't know he'd be we
were like a hundred papers and then to
go to you can find his website under
microsoft research and he's got all his
papers listed it's an enormous web page
yeah unfortunately their papers like
pack saws you know that where you have
distributed coordination with sir not
very well at least for me that we're not
easy to understand but I don't know for
me these papers were the best source of
inspiration so far and one of one of the
one of the actual things that about
distributed systems in particular how to
handle you know when you have parallel
processes that talk to each other over
the network is and there's going to be a
talk about this is commutative
replicated data types it's basically
data types that can be coordinated you
know like incrementing a counter
that can be coordinated by multiple
notes that you always have an atomic
atomic infant increment in the end and
there's talk about that tomorrow I think
and that stuff is fairly interesting for
distributed systems because it gives you
a good actually idea of the problems
involved with distributed systems you
know there's a also a page under MIT
distributed systems research group so if
you googled esrog papers there's a page
with a whole bunch of papers listed
there that's a pretty good resource as
well next question is are you using
circuit breakers kind of like Michael my
guard superiors in addition two timeouts
and exponential back-off we're not using
circuit breakers yet but it is something
that I I've been thinking about quite a
lot I love Michael my guards book is
actually one of the better books that
you will find on actually running
anything in production run it Michael my
guards release it it is if there's any
book I would recommend it is that book
it mentions circuit breakers and it
mentions bulkheads and circuit breakers
is basically the idea when you have too
many failures hitting the good of API
you trip the circuit and you will stop
and you stop making requests and after a
while you will basically you will start
making slow requests again to the API
seeing you know gauging out if it's if
it's working in and then at some point
you can you can reconnect the circuit
that all of the requests will come
through I thought about that a lot we
don't have that yet it's one of these
things where I what really would be nice
because we have two API is that we hid
like hundreds of thousands of times per
day and the unfortunate thing is that
when they when one of them goes down
we'll just keep hitting them we'll just
keep hitting them a lot and the circuit
breaker would definitely be handy it is
something I want to implement for that
particular code to isolate these palers
a little bit more definitely there's the
third question but before I ask it I
have to give a little intro to it
because one of the problems in
distributed systems is network
partitions now partition can be the
failure of a network it's more generally
the lack of an arrival of a message
so message just might not get to where
it's going or there's also the fact that
a message could be duplicated depending
on what protocols are using so this
question was asked three times how you
handle multiple processes which need to
share the same temporal clock I think
Willie wouldn't have that problem it
makes it it makes it fairly easy i was
very happy about that it's um what i
mentioned these these clocks that we
have it's always only one process that
increments the clock that's we shaped
off that problem so we don't we don't
need to have a distributed a distributed
 that needs to be incremented um it
makes it a lot easier to implement that
goes to your simplicity yes that's all
will you refresh Oh more questions okay
would it be obvious to use a no sequel
database instead and I didn't ask that
question to be going honest I'm even
though I wrote a little book on react
I'm fairly fond of postgres it's an
awesome database I'm always surprised
that it's actually we could throw 300
writes per second edit and it's it's a
32-bit 1.7 gigabyte ec2 instance and it
can still handle that but to answer the
question yes it would be obvious it
would actually be obvious to take our
locks process and stored the China be be
able to store the chunks in for example
react instead of postgres but the beauty
of isolating the logs processing and
isolating the separate services is that
it doesn't really matter what we use we
can so we can swap it out without
affecting the rest of the system that's
the important about part about
modularity you can I can change the data
store for one system and it doesn't
really affect the rest of the system
that's where I would like to go and
eventually maybe we want we're going to
use a no signal data store but in the
end it's just another distributed system
we have to take care of it it's going to
bring more problems so it's it's always
a trade-off what kind of monitoring are
you doing and how do you correlate
events across the many component
Oh too painful question um yeah it is a
whole talk in itself so we track a lot
of metrics in our system as against
happen well we're using that similar to
stats d would just basically process
where you all of your process can send
metrics and it's going to forward them
to the system like graphite or something
that we use is called librado metrics
which is a hosted service for metrics we
basically had a grenade all these
metrics in our applications and we also
have built a custom collector that pull
that Paul's things like rabbitmq or
database for information like when was
the last bull running how many messages
are like how old is the Q how many
messages are currently backing up in the
queue because usually when r cube X up
there's a problem somewhere and how we
track the events that our system is
actually a problem that really currently
that I've currently working on fixing as
we're not doing a really good job at
that right now the problem is that well
what what what we're fixing on what
we're working towards is that we each
request basically gets a sign something
as simple as a UID it gets assigned to
you ID and then you can use that uuid to
trace in the locks what it is what is
happening in this system we don't have a
very a very elaborate tool that allows
us to visualize all that stuff it's more
about just being able to retrace to
these things because we don't usually we
don't spend a lot of time looking at our
locks we just need to investigate when a
customer sailors hey I pushed something
to github and no bill was running on
Travis CI and we need to be able to
trace that step back to did actually to
the reel web of notification actually
arrived and then what happened to it and
that is one of our biggest problems and
the solution is a good guess it blazes
hiding a UID to the request and then
using that you I need to trace
everything through the system how do you
test your own system for these kinds of
problems so open enrollment another
painful question we don't we read it we
run it in production that is how we test
it
well you might have or evil there's a
question about thoughts on replacing the
central database you kind of covered
that yeah what is gatekeeper doing
gatekeeper is it's I just it's it's
bright from Ghostbusters but gatekeepers
basically when when a request comes when
we get it notification from github that
a new commit was made we basically
pushed that onto a queue and gatekeeper
takes these commits and turns them into
something that is runnable by the system
basically takes a JSON payload that
contains information about it you commit
to or about a serious limit and turns it
into a set of a set of build jobs that
can then be run on any of the over / job
schedulers and it also does things like
synchronizing user data with github but
the main purpose is to be the main
entrance point for handling build
requests and one last question are you
using AWS as your exclusive platform oh
we're not where our entire app stack
runs on AWS it runs on Heroku just as
always somewhat surprising to people but
we're actually quite easy we're quite
happy with it our application stack runs
on ec2 but our this stuff that is
running the tests which is now 550 test
service built service that runs on a
different virtualization architecture
that actually runs a dedicated hardware
but it's still using we still have an
API to talk to to talk to that
infrastructure which is kind of handy
it's better than having to do all of
their provisioning manually that was all
the questions that came in anybody else
has but anything back there supposed to
rabbit in queue Oh
le vulnerable component of us what do
you do to work on that I'm so the
question is rabbitmq is maybe a fairly
well a breakable component in our system
what we do to fix that um yes revenue q
is a component that used to break a lot
but the problem with that is our initial
problem I pitch a lot about rather than
Q but um but I think it's more about my
dislike of amqp it is a very complex
protocol rabbitmq itself actually worked
fairly well for us but the copy that was
that we as we run our stuff on Heroku we
use a lot of others add-on services
basically that other people hose for us
which is sometimes handy sometimes
painful with rabid mq it was initially
was very painful because what these
providers usually do is they set up a
cluster which a lot of customers that
use and we've been relying on getting
consistent throughput through every
thank you if we just see a dip the tip
and throughput performance and RabbitMQ
our messages will start queuing up
immediately because we rely on that and
in a multi-tenant system RabbitMQ
unfortunately is built in a way that if
you have one producer that is producing
a lot of messages it can block other
other producers or even consumers in the
system so whenever some other customer
on that calculor cuddle on that cluster
came up and started sending like
thousands of messages per second that
immediately would affect us as soon as
we move to a dedicated cluster that we
just had to ourselves revenue Q was not
has not been a problem anymore and the
high-availability features and RabbitMQ
three-point o are actually quite nice
it's still I still have my doubts about
running it in production it's it has it
has its ports basically than Hank a
little bit hard to handle but when it's
running it actually runs fairly well
they did a lot to make it easier to
operate a rabid mq in production and but
in the end to us it doesn't really
matter anymore whether we use red and
kyra if we use something else because
we're not really tied to it anymore now
removing this removing the audit the
requirement of ordering of
implicit ordering as the messages i sent
through the bus through the message bus
allowed us to think about actually using
other message buses and the future at
some point but so far i don't think
rabbit and q wall it will probably a lot
easily allow us to do a thousand message
for a second and if we don't have to
worry about that which i like to not
worry about these things for a while so
makes it a little bit easier okay with
that it's lunch time so everybody thank
our speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>