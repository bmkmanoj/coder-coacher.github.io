<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Consequences of an Insightful Algorithm • Carina C. Zona | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Consequences of an Insightful Algorithm • Carina C. Zona - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Consequences of an Insightful Algorithm • Carina C. Zona</b></h2><h5 class="post__date">2016-11-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YRI40A4tyWU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I love that we got to squeeze in a
credit for Sasha londy who did in fact
found women who code so that's really
cool Diana forgot to thank one person so
I guess I'm gonna have to do that Thank
You Diana
okay SI anna said i am karina sedona i
am the founder of call back women and i
am developer evangelist for would be
together you can find me pretty much
anywhere in the internet as CC zona this
talk is a tool kit for empathetic coding
and we're going to be delving into some
really specific examples of uncritical
programming and really painful results
that come from doing things in ways that
were very been nine lee intended but
really go astray and because those are
some really sensitive areas i want to
start with a content warning because
we're going to be dealing with a variety
of subjects here there are certainly not
typical of a tech conference so
infertility sex shaming miscarriage
bullying stalking grief PTSD segregation
racial profiling white supremacy and the
holocaust just say you know throw that
one in there if you would rather not end
your day thinking about these things it
is completely fine to go and enjoy a
beverage right now i will not be
insulted and i think taking care of
yourself is actually a really good thing
and these are not the the whole point of
the talk but they are subjects it'll
come up starting in about 10-15 minutes
you got a little time to think about
that here we go algorithms impose
consequences on people all the time
we're able to extract really remarkably
precise insights about an individual so
there's this question that we have to
ask do we have a right to know what they
didn't consent to share even when they
have willingly shared data that leads us
there and how do we mitigate against
unintended consequences of this when we
talk about the word algorithms usually
we're thinking in terms of mathematics
and computer science right patterns of
instructions articulated into code or
mathematical formulas but this isn't the
only way you can think about it okay
like here's our friend bubble sort right
and this is the typical way that we do
think about algorithms but there's also
so algorithms as a more generic concept
are just step-by-step set of
instructions for predictably arriving at
an outcome and outcome that's all so for
instance in everyday life we have all
sorts of examples of algorithm senators
patterns of instructions articulated in
different ways for instance a recipe is
an algorithm directions on a map is an
algorithm making hirsch a pattern and I
don't ever want to hear about assembly
being too hard to read the algorithms
that we're going to look at today are
focused on the exhaustive rendering of
our conscious and unconscious patterns
into data sets and algorithms this is
the core of predictive analytics deep
learning is something we've heard about
in some other talks this week it is the
new hotness right now obviously from
mining data and essentially you can
think of it as algorithms for fast
trainable artificial neural networks and
it's a branch of machine learning that
actually has been around in academia for
decades at least since the 1980s but as
far as really being able to use it in
production at scale particularly has
really only been theoretical up until a
couple of years ago and there has been
rapid change because of those
breakthroughs right now it is becoming
possible for industry to extract
considerably more sophisticated insights
than anything imaginable even four years
ago out of the vastness of big data in
production at scale today in a such a
wide variety of applications so here's
just a really generic way to think about
the basic process you have inputs that
are some kind of collection of data that
might be words images sounds objects
even some sort of extraction and then
this is training data that doesn't even
have to be sort of labeled or
categorized like we usually think of it
right we have to mark everything that
like that's a dog that's a human that's
a sound no you could just throw data at
it and then
execution is just running a series of
functions repeatedly in a black box and
that word phrase black box is something
we're going to come back to and then
outputs are just a prediction of
properties that will be useful for
drawing intuitions about similar future
inputs and this is really important the
training data set has to be similar to
the production data set if it's not
you're going to immediately have subtle
biases that will change the intention
and results so we're going to get back
to that later as well so deep learning
relies on artificial neural networks
their automated discovery of patterns
within that training data set and then
it applies discoveries to draw those
intuitions about future data and for our
industry what that means is that it's
presenting us with breakthroughs for how
we even relate to big data and the
things that we can do with it and I
that's exciting but notice what it means
also its premise on a black box the
neural network has drilled down to tens
of thousands or even hundreds of
thousands of incredibly subtle factors
that it believes have predictive value
these are not things as a human that we
would consciously even notice let alone
say this this and those of other seven
thousand things are what make a
prediction here so this is technology
that's driving major advances in so many
areas including medicine pharmaceutical
research in motion detection text
prediction face identification voice
activation like you know Siri type of
stuff fraud detection sentiment analysis
translations video content recognition
even self-driving cars like Google
Google auto and Tesla are using this
stuff right now so today we're going to
be looking at just a few concrete
examples of those including ones using
ad targeting behavioral prediction
recommendation systems image
classification and facial recognition
but before we get to sort of those
really practical ones I want to start
with a whimsical example this is Mario
it is an artificial neural net that
teaches itself to play super mario world
it starts with no clue whatsoever it
hasn't been given rules for a game it
simply been presented with the
opportunity to alter values and notices
that sometimes something happens as a
result and so in a series of iterations
called layers it just keeps circling
back trying new things and noticing
patterns and each time noticing more and
more subtle patterns until by the end of
24 hours it can play the game it has
taught itself all from that cumulative
observation and nothing else so it's
learned that movement it's learned that
gameplay purely on its own and then it's
using that right to come up with
insights not rules but predictions so
having looked at one game let's play one
of our own it looks something like bingo
and it's called data mining fail it
looks like this are you ready to play
are you ready to play
and today I know I understand a little
thank you all right insightful
algorithms are full of pitfalls so it's
by looking at case studies that we are
able to better explore some of the stuff
that's on this board some of the ones
we're going to play with target target
is a essentially department store based
in the US they have you know anything
you could possibly need for your home
and sometimes for your work as well
everything from groceries to automotive
parts and clothing and household
supplies and guns worth America in the
retail sector overall the second
trimester of pregnancy is called the
Holy Grail and the reason it's called
this is because pregnancy is a moment in
time one of the few moments in our
lifetime where all of our spending
habits our loyalties are brand
selections all come up for grabs again
everything we've been taught every
pattern and habit that we have is
suddenly fair game for retailers and so
of course they're very interested in
determining when is somebody pregnant
but you need to compete right with
everyone else is doing that if you can
find that person in their second
trimester rather than when it's becoming
obvious in their 3rd then you are the
competition that can lock in that
customer before any other retailer even
has an opportunity so target of course
one day had some marketers go over to
one of their programmers and ask a
really simple question if we wanted to
figure out if a customer is pregnant
even if she didn't want us to know could
you do that so just pause and think for
a moment what would you do how would you
solve that problem what factors would
you look for to come up with that
prediction it's an interesting problem
it's a challenge and we loved challenge
so he took the challenge to and in fact
he did come up with a pretty reliable
algorithm for detecting pregnancy in
that second trimester before the
customer had disclosed it so they
started sending out flyers with coupons
for maternity targeted at exactly these
people that they seem to be doing really
well with until one day a man came into
a store and he demanded to see the
manager and manager came out and the man
said how dare you send this to my
teenage daughter what are you doing are
you trying to tell her it's okay have
sex are you trying to tell her to get
pregnant the manager didn't actually
have responsibility for a national chain
of stores and their advertising but he
apologize anyway and the man went away
the next day the man comes back he
demands to see the manager again and the
man says I'm so sorry I had a
conversation with my daughter last night
and she is pregnant is it funny though
this is a moment in a person's life that
is big and announcing it is particularly
big what they stole from her is her
timing her opportunity to have a calm
conversation with a parent instead of
one that is angry being able to choose
when that happens and how a retailer
trying to move ahead of the competition
stole one of the most important moments
of her life so haven't gotten other
feedback similar this target decided to
shift its strategy a bit the new to
strategy is to send out to the same
people a flyer full of coupons but to
couch it in completely unrelated things
so they don't know that they have been
targeted for maternity advertising so
you know your ad for baby stuff might be
wrapped up with some ads for
or cologne and you know tire care
whatever it is and the rationale for
this is as long as a pregnant woman
thinks she hasn't been spied on as long
as we don't spook her it works this is
an algorithm disrupting a person's life
in profound ways and is this the ethical
stance that we want to take is that it's
okay to surveil someone and distort
their life as long as they don't know
who's doing it shutterfly shutterfly
basically sells various printed goods
right cards photos etc at one point they
send out this message saying to people
you know as a new parent hey now that
you have had a baby you over everyone in
your life apparently a card you send out
thank you cards birth announcements all
that other stuff some people were not
actually the target audience and some of
those people laughed you know like hi
you think so huh it was not funny for
everybody though thanks shutterfly for
the congratulations on my new bundle of
joy I'm horribly infertile but I am
adopting a kitten so I lost a baby in
November who would have been due this
week it was like hitting a wall all over
again shudder flies response to this
kind of criticism was that the intent of
the email was to target customers who've
recently had a baby yes we know your
intent that's not the point
Mark Zuckerberg became a father last
year and he and his wife in announcing
that upcoming birth with obviously great
excitement also revealed a series of
miscarriages that had preceded it and he
wrote you feel so hopeful when you learn
you're going to have a child you start
imagining how they'll become and
dreaming of hopes for their future you
start making plans and then they're gone
it's a lonely experience facebook
speaking of Facebook probably most of us
are familiar with year-in-review it's
been around for a number of years in
December sometimes it's been algorithmic
sometimes it's been more curated but in
some way I presenting you with hey in
the past year here's some things to
remember what they fail to take into
account with this is that our lives are
constantly changing things happen like
changes of jobs and relationships and
the way we felt about them at one point
in time may not be anymore how we feel
about them now what was something once
joyous may have become very different
for us emotionally along the way Eric
Meyer coined the term inadvertent
algorithmic cruelty and he defines this
as the result of code that works in the
overwhelming majority of cases but
doesn't take into account other use
cases so why does he get to be the one
who makes up this name because he's one
of the people that happened to this is a
picture of my six-year-old daughter who
is dead who died this year the year of
you add keeps coming up in my feed
rotating through different fun and
fabulous backgrounds as if celebrating
her death and there's no obvious way to
stop it Eric calls on us to increase
awareness of and consideration for the
failure modes the worst-case scenarios
the edge cases and that's what I'm
trying to do here today and I hope that
you will carry forward to others so with
that in mind here is my first
recommendation for all of us to take to
heart be humble we cannot into it
interstate emotions private subjectivity
not yet anyway Eric's blog posts was in
December of 2014 and it garnered a
tremendous amount of attention not just
within the industry but also in
mainstream media how does Facebook or
anyone else avoid blindsiding someone
with unpleasant stuff annually this is
something obviously two years ago they
must have given a lot of thought to they
certainly had to answer a lot of
questions and so three months later they
did make a change they introduced a new
feature called on this day and basically
just reminding you of trivial fun stuff
five years ago you became facebook
friends with a person here two years ago
you went hiking a year ago you had dim
sum and it's accompanied by this you
know really nice message we care about
you we get it this time you know here's
this nice memory from three years ago
that we think you're really going to
like puppy who doesn't like a puppy I
know it's not a cat but come on it's a
puppy on this day you posted this really
adorable picture thanks Facebook for
picking today to hit me with this dumb
feature and remind me that my dog died
three years ago sometimes Facebook's on
this day sends me memories from high
school and it's triggering I did not
enjoy high school and I need to forget
you do not get to decide what parts of
my past I keep fresh in my mind and
which parts I walk away from
off this has impact for people they care
we have to learn from mistakes Facebook
chose to take those lessons and do the
same thing we have to learn from our
mistakes and we have to learn from
others we have to decide that harmless
and harmful are not consequences that
balance out Fitbit commonly known
activity tracker right social dimension
you know compete with your friends on
how many miles you jog oh sorry meters
right American you know how much weight
you're losing all that kind of stuff
right what Fitbit started out it also
had another thing it had a sex tracker
another one to compete on the difference
is it was default public
yeah you're reading closely aren't you I
am also a sex educator and you know I'm
both really happy for you and really
concerned public this was a failure to
think about the data itself treating all
data as equal if you want to compete on
one thing surely you want to compete on
every data point possible we need to be
thinking about context and we need to be
thinking about what data were collecting
and the personal meaning it has four
people and certainly to be thinking
about what data is private if you're
applying the same algorithm to every bit
of data there's already a problem so
most of us have some kind of internal
ops tools right like that's inevitable
might be for monitoring performance
tuning business metrics whatever it is
Uber's is called God view in their
infinite humility uber didn't limit
access to just add mins or restricted
just to operational use employees could
freely identify any passenger in the
system monitor that person's movements
even drivers used to have access to God
view records to even a job applicant was
welcome to access these private
individuals records meanwhile managers
felt free to abuse God view for
completely non operational purposes such
as I'm not kidding stocking celebrities
rides in real time and showing that
office party entertainment these
algorithms were created for legitimate
reasons but algorithms can be abused and
it's up to make sure it's up to us to
make sure that abuse is not possible and
that what we are making that we're
checking on what is this for
so okay cupid is a dating side and they
used to be really into their metrics and
they blog about you know all the things
that they were learning from looking at
aggregate trend data and that blog
really was focused on sharing insights
into simple ways that as an OkCupid user
you could use the site to achieve the
goal you're there for right to date
better to date more effectively uber
also used to blog about their data a
crucial difference is in how they
approached it because this was not about
improving customers experience of uber
service in fact if you look very closely
uber can and does track your one-night
stands this is purely invading people's
privacy for no operational reason no
legitimate business reason at all but
purely for the sake of judging and
shaming this is not a predictable
consequence of signing up for a
rideshare account google adwords so
several years ago researcher at Harvard
was googling her own name and she was a
prize to see that when she did so that
there was an ad on the side of the
results that said something the effect
of would you like to see Latonya
Sweeney's jail records her arrest
records as someone who doesn't have a
rest records she found that interesting
and was curious to see why that happens
and so she did a study in which there
were two sets of names once highly
correlated with white people one's very
correlated with black people and divided
them by gender and assigned these to the
names last names of real academics and
then Google done and saw what ad words
were displayed a black identifying name
was twenty five percent more likely to
result in an ad that implied an arrest
record ads like these the algorithm of
AdWords if you recall focuses purely on
predicting what will click on it has no
knowledge of the real world that's not
the point the point is tricked what will
click on
and to motivate us to click the real
world is absolutely irrelevant and it
bases that prediction on what it knows
about us and what it's observed from
others in aggregate so what we see here
is our collective bias being both
reflected back to us and reinforce to us
data is generated by people it's not
objective it's constrained by our tunnel
vision and it replicates our flaws it
echoes our perceptions Twitter
jo-ann McNeil coined the term accidental
algorithmic run-ins and as you can guess
she was inspired by Eric's definition
she wasn't giving a formal definition
but you can roughly summarize this as
classic classifying people as similar
where careless prompts create scenarios
that are harder to control and prepare
for in there a variety of different ways
that you can see this manifest for
instance being trapped by a similarity
algorithm that's determined to associate
you with someone who is similar in some
respect but who you would never actually
give conscious permission to associate
what you with so for instance being
associated with the white supremacist
true story um there's more than one
person the wire alton named william
johnson linkedin has this really nifty
news feature where it just pops in and
announces to all your links here's a
news article that we believe is about
you wouldn't you like to congratulate me
william johnson on your great new
success as a white supremacist if you
look really closely in the little grey
print is wrong person the burden is on
you to belatedly catch their error and
correct it and what he learned is it
takes a while it's not even click on it
and it goes away and of course the
announcement has already gone out to all
those individuals I don't even know what
to characterize this as I think hubris
is the minimum that we can call this an
accidental algorithmic run-in is a type
of false-positive often that falsity and
harm cannot easily be detected
algorithmically if you're stalked by a
former coworker Twitter may reinforce
this connection algorithmically boxing
you into a past that you
trying to move on from your affinity
score where that harasser just grows
higher and higher with every person who
follows at Twitter's recommendation that
person this is a common problem that
people have reported things like their
ex that they no longer want to deal with
and the fact that they have had these
overlapping circles becomes a circle
that you're trapped in and becomes the
person that you at least personally
recommend is the person that Twitter is
most determined to recommend and notice
it much like AdWords that these things
just are really doubling down on that
false certainty every time a third party
takes action you don't have control
random people with no knowledge of the
impact are the ones who inadvertently
have that control notice that just like
Eric's experience there's no escape
Flickr and google photos facial
recognition
at its finest right um this is I photos
and you know what it was starting out it
was teeny bit flood and you know like we
saw along the way lots of humorous
mistakes right it took a while for this
to start getting good and this obviously
is a really harmless mistake nobody's
going to be harmed by the fact that
their biscuit has been misjudged a false
positive like this is really easy to
chuckle at here's another kind of that
same category you know this is a site
from a couple years ago Microsoft call
that how old dotnet and it's using deep
learning to just take facial recognition
to you know kind of next level by
drawing intuitions based on a face about
age and gender so it's assigning these
tags that you can see here and notice
this is the same person who apparently
is simultaneously a whole lot of ages
and genders not nearly enough genders
but hey it's inevitable that as these
technologies are growing and progressing
that there always are going to be some
mistakes along the way right that's
accepted and and these are the kind of
mistakes that when you look at it it's
it's more intriguing right like how did
it come up with that particular choice
then rather than something that you're
thinking about as something to worry
about and then some false positives are
not funny at all such as this next one
flickr classified this as children's
playground equipment Dachau
and I want you to notice something else
here because those great acts are the
photographers the white tags are
flickers this is a consequence of
algorithmic hubris treating human
understanding the photographer's own
knowledge of where they were and what
they saw and throwing that out in favor
of an algorithms belief system treating
data as inherently neutral when clearly
there will be many circumstances in
which it isn't flickr likewise tagged
him as an animal originally it tagged
him also as an ape that's a comparison
that is a particularly ugly history it
is not merely wrong it is deeply
offensive so I want to be clear from out
here that this is not about picking on
any of these individual companies or a
coder behind them or even a methodology
you know I mean this is about more than
say just artificial neural Nets this is
big picture there will always be new
breakthroughs and we will always make
these same kind of mistakes fall into
these same pitfalls over and over again
unless we take different approaches here
is google photos one month after those
last two examples again the comparison
to a gorilla it's accidental but it
couldn't be worse so does this even
happen it's really doubtful that there
was some engineer who is like hey let's
like all the black people and label them
as guerrillas that'll be hilarious no
for one answer you can go all the way
back to the 1940s Kodak Eastman's was
developing a color film emulsion and
we're talking about America in a period
where racial segregation was still the
norm where they were making conscious
choice about whether to
get to white people or black people who
is the preferential market for our
product and unsurprisingly they chose
white people so long before computers we
had algorithms and film developing is
grounded in an algorithm every day lab
technicians had an image like this
called the Shirley card and every day
you would take a photo process the film
and make sure that the results were
registering the colors the same the same
amount of detail and if you didn't then
you needed to recalibrate your machine
so this becomes the reference for
exactly how your film should be
performing it treated black skin as
irrelevant the important thing is to
produce fine detail in white skin light
tones all lightness all of which means
that our development processes have to
respond to this legacy that's
generations old because even when
digital imaging started being developed
in the 60s and 70s it followed this same
pattern and when you had digital
photography emerging likewise it
followed the same pattern and if it
didn't think about what it would have
happened consumers would have seen
images look completely different than
they're used to taking pictures side by
side it would have said wow this
technology sucks so we keep carrying it
forward every image set that we mine is
inherently compromised by this lossy
data that's produced even now under the
weight of deeply flawed algorithms 70
years old the tools used to make film
the science of it are not racially
neutral and that presents us with a hard
problem a really hard problem so it's
really easy to decide to avoid thinking
about it at all and so it continues on
and on
a firm is a credit lending startup it's
from one of the founders of PayPal is
not micro lending but it is lending for
specific products rather than here's a
credit card buy whatever you want and
it's special feature is that it's all
algorithmically deciding who is a good
credit risk rather than conventional
methods where you know like you
demonstrate various things that prove
you're good at paying bills right
algorithms are gatekeepers to all sorts
of stuff not just credit but also things
like housing and jobs what's new in this
is not like those those previous methods
are also algorithms don't forget right
like we say if you look at this this
this and this we know something at the
end we have an outcome these are just
different algorithms that we're now
applying the big difference though is
now we're applying algorithms in a black
box we don't know anymore what the
algorithm is it's what's deciding who
gets access to jobs and housing and
credit traditional banks and their
regulators have been really paying close
attention to these new models affirms
and others they're evaluating how to
adopt same strategies and so this is a
really good time to look at what those
strategies are and the impact that they
have a firm makes an assessment of
creditworthiness based on just a few
factors your name your email your mobile
number your birthday and your government
ID number and sometimes I mean usually
they make a judgment right there snap on
those few things but sometimes they feel
like they need a little more so they'll
ask for something like your github
account and use that as the seventh
criteria how many people here have a
good have account right how many people
generally in the world have a github
account yeah okay so this is already
selecting on privilege and it's
selecting on a really particular niche
of the world it also looks at things
like the
averill factors like how long the person
takes to remember those Wow all six
factors behavioral it's a really
interesting way to phrase it remember
Salas talk about neuro diversity there's
all sorts of reasons why someone may
need to take a pause we're all riding
those things down some of them are
because of ordinary distraction some of
them are because you're Stephen Hawking
and maybe it takes you a little time to
type apparently he's a bad credit risk
these are algorithms all reinforcing
privilege let's go back to that original
description an algorithm is just a
procedure for reliably arriving at some
outcome what is the outcome here to
reliably predict that people in a very
narrow niche with a heck of a lot of
privilege are going to get credit you
know what they were already going to get
credit who's locked out of this it's up
to us to take into account impacts that
our algorithms lead to the outcome here
is just identifying reliably people who
can pay their bills not people who will
and it's reliably excluding most people
who just simply don't have an abundance
of privilege to tap into Alan Turing
reminds us that if a machine is expected
to be infallible it cannot also be
intelligent the immense power of machine
intuition is irreplaceable that is no
doubt we have moved far beyond wanting
to and I'm not suggesting that we should
but it's not a replacement for
comprehension a firm analyzes applicants
social media accounts as I said they're
not the only one so did some other
companies for instance in 2012 you may
recall that Germany's largest credit
rating a credit rating agency considered
evaluating applicants Facebook
relationships as a credit criterion
Facebook friends
a personal friend isn't necessarily the
same as a facebook friend right like we
know this apparently they didn't but
what about when a facebook friend is
generally a friend friend Facebook
defended a patent a few years ago that
pushes even further than all these other
models do because its patent is for
making credit decisions about a person
based on the completely unrelated credit
history of their Facebook friends not
just Association but completely what
someone else does here's an algorithm
with potential to deeply intrude on and
alter your real life relationships
simply to avoid being financially shamed
and punished by an algorithm the founder
of firm says it's important to maintain
the discipline of not trying to explain
too much adding human assumptions could
introduce bias into the data analysis
what dude date is not objective data
always has bias inherent minimum and how
its collected and interpreted very
beginning those inputs and then every
flawed assumption that training data set
and the original functions all are of
course going to have unrecognized
influence on algorithms and those
outcomes that they generate a firm says
that its algorithm assesses 70,000
traits out of remember those six data
points how many of those have potential
for discriminatory outcomes how would
anyone even know it's not like someone
can tell you what criteria led to a
decision rationales for the algorithm
can only be seen from inside the black
box so I took a photo of the inside of a
really really black box now we know
making lending decisions inside a black
box is not a radical new business model
it's a regression what is disrupting is
fairness accountability and oversight
Sarah Mae says many regulations are in
place to correct for systemic structural
bias against minority groups ask
yourself ask yourselves do you want to
be part of that disruption the realities
algorithms always have underlying
assumptions of course about meaning
about accuracy about the world in which
the Jado is generated about how code
should assign meaning to it at all
underlying assumptions influence
outcomes and consequences being
generated and yet Thoreau reminds us the
universe is wider than our view of it
our industry is in an arms race right
now major players are rolling out
products that make big bets on black box
algorithms and those opaque intuitions
and the pace of implementation is moving
so fast it's imperative that we make
decisions right now as an industry about
ethical and practical processes for
going forward with this well for the
moment outcomes are clearly varying in
quality but recall that these
innovations are all about drawing
predictive intuitions at extremely fine
grained levels fast and what that means
is that they're growing ever more
precise and correctness and ever more
damaging in wrongness so that's the
dilemma for us we care about getting
this stuff right we wouldn't be in this
room if we didn't we want to be
empathetic coders so how do we fill the
paradigm here are some starting points
consider decisions potential impacts on
others how might a false positive affect
someone's such as those shutterfly
customers or Twitter users how might a
false negative effects someone for
instance being
denied that loan or being declared a
white supremacist how many ways kind of
algorithms intuition be superficially
correct and yet deeply wrong about human
context in which it's drawing from like
that photo of Dachau or like that
reminder of Eric Myers daughter project
the likelihood of consequences to others
minimize negative consequences to others
did you hear me keep saying others
others others were so good at
anticipating how things can go wrong or
well for our mission for our company for
our product for our features what we're
not so good about is remembering to
think about impacts on others that we
need to bring into the room I think the
medical professions philosophy of do no
harm is one that we can adopt it's a
framework for thinking about this stuff
another we need to be honest and we need
to be trustworthy and this is not just
about hey we want to be nice people it's
because it's the right thing to do
period yes but also because it's
necessary as a business issue we will
make big mistakes on this stuff if
Google and Facebook and all these others
can be assured the rest of us can too so
we need to be able to essentially build
currency to be able to say when that
happens this was an honest mistake and
we are genuinely sorry we are fixing it
now and we have earned your trust I hope
to be able to recover from this and move
forward and that's why we also need to
make sure that we're building in
recourse for people to be able to easily
correct conclusions that were wrong what
if Eric Meyer would have been able to
say simply click not a happy memory
provide others with full disclosure of
limitations of what we've made what we
do call attention to signs of risk of
harm there's always limitations in what
we make and we're okay with that right
it's targeting a particular problem in a
particular context with particular
assumptions and thar
an audience and is accepting that
there's always going to be some sort of
amount of flaws and imprecision all of
that's expected and okay and I'm not
challenging that whatsoever but we need
to make it clear to consumers what those
limitations are that they have the same
understanding of scope that we do this
is a chicken breast this is an activity
tracker on a chicken breast this is an
excerpt from a great video by Emily
gorsinski giving a talk that is on
similar issues related to internet of
things this activity trackers heart rate
monitor is detecting a nice steady pulse
of 119 beats per minute from the chicken
breast come on funny uh-huh until you
know that a woman was recently convicted
of a crime based on the pulse rate data
evidence from her activity trackers
heart rate monitor these devices were
never designed to be precision
scientific instruments not even close
right essentially those algorithm
centers were more or less designed to
capture some data to give some sort of
sense of trend that doesn't even have to
be objectively you know compared with
others it's just about the level from
day to day using this sensor does or
does not change within a reasonable
bound right that's really all it is in
some respects they're an entertainment
product more than a science product they
were certainly never meant to be used as
criminal evidence yet the public has
taken to heart our industry's conviction
that algorithms inputs are always
precise their outputs are always precise
and beyond question that what we do is
science it's important that we
communicate with them about each
algorithms realistic limitations because
they are depending on things that are
not real we need to be visionaries about
creating more ways to counteract bias
data bias analyses bias impacts and when
i say x is i don't necessarily mean
discrimination
I mean skew things that are different
from the reality that they expect to be
covering we need to anticipate diverse
ways to screw up this we will do as long
as the teams are charged with defining
data collection and data use and
analysis are less diverse than the
intended user base we're going to keep
on failing that is a certainty we have
to have decision-making authority in the
hands of highly diverse teams not just
diverse but highly diverse teams culture
fit is the antithesis of diversity it's
superficial variations being allowed to
exist no more their unique perspective
is suppressed because hey the actual
point of culture phit is to avoid
disruption of groupthink you know
dimensional is a variety is not
diversity either this is variety it's
wildly varied on as many different
dimensions as you could possibly find
differing origins ages assumptions
experiences backgrounds where there is
no longer a majority to identify at all
do we have our teams looking like this
because the target audience when we say
we want a global audience sure does
audit outcomes constantly because again
outcomes are all we can see so what do I
mean by that in housing and job search
this is a pretty common proven technique
used in a variety of fields basically
what you do is you make two identical
applications of you know something like
say that application for a fellowship or
a job or a mortgage whatever and you
just change one attribute just one thing
sometimes that's been things like just
change the name or the gender or you
know the implicit race whatever it is
and you send those two applications into
the black box and in the past obviously
the black box is essentially like that
that landlord or whoever it is would be
evaluating this case the one evaluating
is the black box right and then look at
the outcome because if bias doesn't
exist these are identical applications
right
they should have identical outcomes if
they don't then we have discovered that
somewhere in this system bias exists it
gives us a starting point for
investigation so while we clearly can't
audit every possible contingency there
are some we're certainly we do know to
check things where there's like
regulatory issues where you know we
actively know that these are things you
can't discriminate about certainly
always audit for those anything is
protected by law is the bare minimum of
what we should we look for but you can
also look for settler clues what is the
system implicitly describing meaning to
for instance is say a difference of
family name or personal peril being read
as something else as an attribute of
character for instance rule of thumb
here is unless you can draw a complete
decision tree for every possible
combination of inputs leading to every
possible output audit that's your that's
your bottom line if you can't do that
then you must be auditing outcomes it's
all we've got so what does that entail
earlier remember that google adword
study that's that is an example of what
I'm talking about that's outcome
auditing here's another one it's a
Carnegie Mellon University tool called
ad Fisher it generates completely for
seeing google histories and then let's
google respond with job ads and it's
interesting because it showed that high
pay jobs were shown to six hundred
percent more men compared to women crowd
source as much as possible our teams are
always limited in our knowledge and
perspective take advantage of the fact
that the crowd has far more imagination
to offer and knowledge these case
studies really highlight for us that
mining social data is full of all these
pitfalls and they just can't all be
solved by technology alone and so we
don't have to we could take advantage of
non-technological ways of gathering
knowledge
and ends you know like there's this
crude approximation of the brain's
ability to constantly develop
sophisticated algorithms we're doing it
every day so why not bring the real
thing in this is an excerpt from a talk
by Heather rivers who at the time was
that Yammer the talk is called
linguistic potluck Yammer are for a long
time has been crowdsourcing
internationalization and translations so
you notice here you have just a standard
display but at the bottom you could
choose to translate provide a
translation to Yammer and then what's
important here is that that translation
is not immediately accepted it's for the
rest of the crowd to then evaluate the
truth or falsity of this so you
constantly have this refinement of data
and you also have some checks on people
trying to game the system or just being
wrong google has a similar app called
crowdsource a highly original name it's
being used to do do checks on things
like Street View google keyboard
translate the important thing here is
this is a data gathering thing right so
if i said that data gathering has
potential for biases one of the ways
that this one has problems with bias is
that they are in precise in
communicating what it is they're looking
for and so you have a variety of ways in
which you get skew for instance some of
the users believe what they're
essentially doing is providing a
dictionary definition so they're giving
me like the whole list rather than a
translation it's here is a reference
manual other times they sort of treated
as like they're talking back to someone
the k engineer this is approximately
something like my loved one sometimes
they're just trolling
sometimes they're really trolling
and sometimes the trolling is about
deliberately injecting prejudice think
for a moment about a sentence
construction in which immigrant becomes
Mehmet this can really be poisoning the
well which is why you need to have those
checks cultivate informed consent ask
for permission with the default being no
focus on all those people who do want to
share themselves who are enthusiastic
about giving consent to know better they
want to be known they want to be able to
get the best features and best discounts
have the greatest experience of our apps
be served best there's lots of people
there's no reason why we have to start
by coercing people facebook has been
making a lot of adjustments recently on
this day and your interview now they ask
questions like do you want to use this
feature hey that's a good one do you
want to edit its choices which people
don't you want to be reminded of which
dates which pose which sounds pretty
good right it's better than what we saw
earlier its effort it's well intended it
misses the big picture anyway because
these newer gentler features are all
promised on list everything you don't
want to think about think up a list of
every single thing ever that you don't
want to ever think about why should I
have to dig up what hurts me to fend off
an algorithm at all what they're doing
is still fundamentally opt out not
opt-in examples of opt-out are things
like having to block what we have put
out there or having an ignore or a
filter or a disabled in some way the
default is on and it's up to us to
change it opt-in means you start with
something like I will choose to follow I
will choose to view this I will choose
to select or enable this why not just
let something like Twitter viewers
select the people that they recommend
you follow we do this anyway it's called
follow friday right like a lot of people
have already given their actual
recommendations why does Twitter need to
come up
with this stuff as a guesstimate when
precise data is available another way
you can do it is have a checkbox right
like all those pregnant people actually
I think probably be excited about having
some coupons why not ask people and the
great thing is if you ask people you get
not just a pregnant person but other
people who have a stake in the pregnancy
right like there's probably people who
want to buy gifts and stuff this is a
better method a more effective business
method instead of trying we can get
people to come to us it's a win and this
is why we also need to commit to data
transparency and algorithmic
transparency and this is the one where I
know it's the most hard conversation to
have internally because so many
companies think that for taya cherry
everything is the secret sauce it's the
win if we reveal stuff then you know oh
no and I see the sort of comparison is
that you know it wasn't that long ago
that we were fighting for legitimacy of
using open source in our work and we
made some of similar arguments right
about transparency is part of the value
of this and we pushed hard we won we
were right we can do that again were the
professionals we know that transparency
is crucial for drawing insights that are
genuine and useful so at least start the
conversation we don't have the Linnet
today but at least start it argue for
increasing transparency because it's for
the sake of a better product it's for
the sake of cleaner features it for
fewer bugs and for stronger tests and
for happier users and for public trust
and to be able to make more money
because we're building stuff that
matters Amy hoy is really harsh here but
I think she's really right if your
product has to do with something that
PDP affects people either care or quit
go live in a cave don't hurt other
people it is so easy to unthinkingly
build an out pull of this data mining
fail because building differently
requires awareness critical thinking
and requires most of all deciding as a
team to take a stance to say hey listen
this is the deal we don't build things
here without understanding consequences
to users this is just our way this is
our process it's a good one we are hired
for more than just a code we're not code
monkeys or hired as professionals in
problem solving our role is to be
opinionated about how to make code serve
a problem space well this is our actual
refreshin applying expertise in judgment
about how to solve problems this is the
argument we can make when we're asked to
write code that presumes to intuit
people's internal life and act on those
assumptions as professionals we have to
be users proxies be their advocates say
no on their behalf to using their data
in ways that they've not knowingly
enthusiastically consented to say no to
uncritically reproducing systems that
have bias built into them say no to
writing code that imposes unauthorized
consequences onto their lives in short
refuse to play along thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>