<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • Following Google or don't follow the Followers, follow the Leader • Mark Madsen | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • Following Google or don't follow the Followers, follow the Leader • Mark Madsen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • Following Google or don't follow the Followers, follow the Leader • Mark Madsen</b></h2><h5 class="post__date">2014-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HdToqoAsiOk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">one of the questions is you know why
this particular topic in a wife
following google for example one of the
things that i would put forth is that a
lot of our industry especially as it
revolves around data which is where i'm
keeping things we're following things
like say hadoop which has been
developing but the MapReduce core of
Hadoop is more than ten years old and
we've been here before and we're going
to some of the same places again and so
when we talk about why database or why
not database or why this persistence
layer or that persistence layer there's
a lot of history and there's a lot of
useful old patterns that aren't worth
reinventing or are worth capturing and
carrying forward as we kill some of the
old stuff so you know one of my posits
is that a lot of times we're not
following the right lead you know we
talk about Yahoo Yahoo is trailing
Google and every single metric on the
market and yet we're following what
Yahoo's doing rather than paying
attention to the research papers coming
out of Google and looking at what
they're doing so I find that we get a
lot of bad architecture advice and bad
architecture advice is stock-in-trade of
vendors trying to sell you stuff so you
have to kind of pay attention to that in
a way it's like looking at constantly
revising things the database industry is
telling you everything is just fine
don't worry the Big Data industry is
telling you that the database is dead
and the truth lies somewhere in between
those as far as my background I've held
every job in IT I've been a janitor in a
data center and I've been a CIO and
there's a happy medium somewhere in
there that hopefully most of you are
finding and that history has led me to
make lots of mistakes so I'm going to
talk about some of the kinds of things
that we've learned earned that I've
personally learned because I'm really
good at making mistakes maybe not
learning from them so first thing
history is a rabbit hole the more you
read about the stuff the more interested
you'll become finding out the abstract
patterns that people struggled with and
so what I thought I would do is I would
start with some early history where did
all this data stuff come from well the
problem statements go back pretty far
you know and you look at say 1985 with
Neil postman who was a follower of
Marshall McLuhan he talks about
information becoming a form of garbage
because there's too much of it and you
can't answer the questions you can't get
coherence because there's so much crap
out there but if we jump back to 1685
you know going back a couple hundred
years the multitude of books which grows
every day is going to basically you know
lead us into a state of barbarism and if
you go back a little bit further to one
of my favorites Anton Francesco dhoni so
many books we don't have time to read
the titles right there's just too much
information and so these complaints
about information explosions go back a
long time they go back even further than
you think so you'll have to bear with me
I'm going to take you back to the
beginning so that we can fast forward to
where we are but it really starts with
writing now for approximately 5,000
years we used other means than writing
counting sticks this is called a bola
bola as a contract so a contract is this
round thing that holds these counters
now the counters are not precisely
symbolic I mean there's symbolic in that
their representations with different
denominations just like coins but they
are hard counters if you want 15 sheep
you have to have exactly the right set
of counters for that 15 sheep so you
can't really do easy math with this but
in part this is the origin of things
like an abacus now when you get writing
you get something new and the reason
writing was invented was because
civilizations got complex processes got
complex transactions got complex and you
needed to do something like this this is
one of my favorites it's a clay tablet
that's a contractor all of the
ingredients for a beer maker and so you
have numbers you have the symbols that
represent things you're beginning to
capture things this is about 3000 BCE so
about 5,000 years ago
and you have the first information
explosion which is clay tablets right
cuneiform Sumeria and so forth and you
know this this carries on in a number of
different fashions and spreads the
minoans the Minoans about 1700 BCE
developed formal mathematics and a bunch
of other things and what was fascinating
about them is that the first use of
writing that we know about and i'm not
talking about cave paintings but actual
you know symbolic writing is not
religion it's not history it's not
stories it is accounting all of writing
goes back to accounting for stuff like
how to make beer now when we look at
this clay sticks things that fit in the
palm of your hand the first palm
computing we have an information
explosion these things were all over the
Middle East you go through most parts of
the Middle East you find this stuff
because it's persisted for that long and
this was the the sort of beginnings of
the information processing nightmare now
once you have enough of these things you
start to have problems a few you stick
them in clay these clay tablets the way
you store and retrieve the data that's
on them as you put them in baskets and
you store the baskets on shelves and the
shelves sit in some room somewhere now
if you flash forward to say the library
of Ashurbanipal which is one of the
great libraries of the era you end up
with so many clay tablets you have to
figure out how to organize them and so
now you've got all kinds of problems and
so you make these little holes which you
see up at the very top there that little
hole up at the top was so that you could
fix a piece of papyrus with other stuff
written on it essentially tagging
infrastructure so now you have a clay
tablet with a bunch of numbers it might
be tax records it might be contract
whatever it is you start to put tags on
it you classify you organize things in
essence you're imposing schema so you're
taking specific records and sticking
them into baskets which you put in
particular shelves and then you tag them
this way for quick sorting and retrieval
within that table and so what's fun to
look at is the
abstract patterns for how they dealt
with this and small piles of information
are always easy to deal with it's when
you start to get lots of stuff that it
gets complicated and so we end up with
metadata this is a much bigger tablet
than the little small handheld tablets
that you saw which were for inscribing
records and so forth this is when you
have to worry about the categorization
and you have to start worry about
taxonomy and schema and so at that Great
Library you had things like this which
is actually telling you what's on the
shelves and what's in the baskets and
when you look at all of this they came
up with a pretty robust classification
scheme that's very similar to the way
Dewey Decimal System and others
categorized bodies of knowledge and in
this case though first they were
categorizing simple things like tax
records who produced what information at
what location how much did they produce
did we get the taxes that we were owed
on it that sort of thing and what's
interesting too is that you know what
you retrieve is basically these whole
big blobs of stuff you know oops sorry
metadata version 2 i'm meant to go
backwards these things all right which
have a whole lot of information so
you're not retrieving one row you're
retrieving a whole table it's sort of
like reading 64 megabyte blocks in HDFS
and when you do that you're beginning to
make use of some of the schema and so
forth now when you jump ahead to version
1.1 you don't have just tablets about
where other tablets are stored
essentially the schema or the indexing
mechanism for this stuff goes there
instead you have tablets about what's
inside of tablets right the second level
of metadata so if you think about
something like H catalog in Hadoop but
it's really telling you is what's stored
where the problem is that it's
optimistic about what is actually inside
of those things because people scratch
all kinds of crap on to these clay
tablets just like people put all kinds
of crap inside of your theoretically
pristine schema and you hope that it
matches when you retrieve it later and
so what you're worried about now is a
little bit different it's not just where
things are but it's also what's inside
of those things and how you can crossref
rinse them and so you know this is an
interesting little cinnamon synonym list
and why is a synonym list interesting
because I need to know that the common
point of reference works so just like
dealing with customer identifiers zor
product identifiers that are distributed
across systems are we talking about the
same thing well the way you do that is
you use things like synonym lists and
you start worrying about what is where
now clay Tech has limitations right this
is still the very first version of
accounting and bookkeeping and so forth
and in doing this you have little
problems like making mistakes or updates
well update ability is kind of a problem
because right here you can see the
process of an update what we had is the
very first worm drives so right once
read many but once it's in there and
it's baked on to that clay tablet ain't
going nowhere so you chisel it off now
the other way to do it of course is
delete marking but back then delete
marking didn't work because you'd have
to redo and reconstruct the rest of the
clay tablet so what else can you do well
you can move to papyrus right or other
types of tech paper tech version one
point I was a huge advance because it
was lighter weight it was much denser so
you could fit a lot more information
into one small space and it increased
the rate of change significantly and
there were variations on this for a long
time flat sheets and scrolls and so
forth now scroll tech was interesting
because scroll tech was in advance over
these little Leafs in that you had this
nice dense medium where you could spin
these things up you could pack them on
of course the the pinnacle of this was
between 300 BCE and about 47 BCE at the
Great Library of Alexandria which was
actually constructed to be not just a
tax records repository but also
information repository now you have
things like content catalogues they had
to start making Scrolls about Scrolls
now scrolls are a problem alright
scrolls are linear they can be written
on both sides but in order to get to
stuff that
at the front or the back of the document
you have to go to the front or the back
of the document it's like an indexed
sequential access file you start at the
beginning you start at the end you try
to move around to move that redhead your
eyeballs to find the information that
you need in this and then read that
stuff off of there it's kind of
inefficient it's like reading 64
megabyte blocks of unindexed data and
then scanning through them are dealing
with CSV files and you know when you
have cereal reads and writes that's fine
but when you need random access you have
problems right it's not really much
different going to tape systems which is
what I did in my first job back in
serkan 1983-84 tape systems linear
sequential takes a long time to get to
the beginning hope you don't have to go
back and forth between the beginning and
the end to get information that you need
now we did discover a trade-off problem
there are always trade-offs one of the
trade-offs between clay and paper is
that paper burns and disappears clay
tends to get fired and makes it
permanent which is why there are so many
of those clay tablets lying around so
you're always going to make technology
trade-offs and one of the things about
information recording it on clay tablets
recording it on papers that you start to
record who owes you taxes and who owes
who money and the best way to get rid of
that stuff is of course to set it on
fire and so every revolution everybody
burns down the tax offices and that's a
solution and a sad side effect that gets
rid of those things now if we jump
forward again paper tech continues
because it's just so damn useful now
with these it's a lot more like not
sequential file processing good true
indexed sequential access because with
the Codex you now have the ability to
flip open that book and random fashion
to any particular page what does that
mean that means you need to invent new
things and you don't think about a table
of contents or an index or colophon as
inventions but first they just stacked
up the paper then they started to write
page numbers on page numbers were an
invention you can trace the history of
page numbering schemes and when you look
at them what that gives you then is the
bill
to look at an index and jump into that
point much more efficient access for a
denser storage medium encyclopedias
start to come about in part as a
response to the fact that we've got so
many of these things if we keep moving
forward we look at the discontinuous
storage right this is kind of paper tech
mark 2 or mark 2.1 increased storage
density bound books they're more
persistent they're durable we can pack
them up hi res RGB graphics jump ahead
we finally get to movable type movable
type and typesetting changed a lot
because of course you went from carved
blocks and block printing where you have
to carve each page by hand to movable
type that you can shuffle around and
change the lettering in the numbering
and so forth which means you can
reproduce the rate of change goes up
perfect reproductions it's a lot faster
it's a lot cheaper it's commoditization
now what happened with printing presses
is that churches typically control them
and of course they produced the things
that churches wanted but one of the
things that made them a lots of money
was indulgences indulgences for you know
you committed a sin here's a piece of
paper give us a coin and we will be very
happy to Cleary of all of those sins
you've just committed and so that
changes the landscape because very
quickly you had an overcapacity an
oversupply of printers and we started
printing other stuff religious stuff
secular stuff Flyers newspapers it goes
on and on that destabilizing influence
carries forward but the printed becomes
more important than the printer at this
point about 200 years down from this it
really doesn't matter what your printing
you're focused on the wrong thing the
books are what's disrupting society not
the printers first they tried to ban the
printers then they tried to control
things like ink supplies and paper
supplies and when that failed they
eventually tried to ban books that never
really worked either because it just
disseminates too quickly and so it
changed civilizations well we're kind of
at the same point with data right we've
been generating data and generating data
and we've been focusing on making bigger
and bigger calculators Hadoop is
interest
for some of the things that does but for
the most part it's just a bigger
calculator and that's not really an
interesting problem to me now if we move
forward into sort of the modern era the
Elizabethan era so we're talking 1500
1600 s production automated printing
presses steel movable type data
management technology we now have
perfect copies of things you know
perfect copies are great because it
means that you can standardize for the
first time and you don't worry about
weird changes leaking and polluting
things fonts tattered ization was huge
there were different fonts in use that
people couldn't read if you go back to
early latin texts what you find is that
there were different types of latin font
for the same language depending on
whether you were a religious scholar a
secular scholar somebody dealing with
contracts and if you didn't standardize
on the font then you couldn't read
somebody else's Latin it was like a
different language when you have movable
type it's kind of silly to have all
these different typefaces that other
people can't read and so you have font
standardization which is very much like
character encoding you know we've sort
of standardized on unicode after coming
from a ski and episodic and so forth and
the other thing is it was so many books
out there taxonomy ascends how do you
group and classified categorize things
information explosion in 1500 we had
eight million books in circulation 200
million books a hundred years later so
multiple orders of magnitude increase in
the scale of available information it
becomes a commodity people start talking
about overload like that book that i
mentioned from 1550 that talks about one
of the you know early cases of
information overload so what do we do we
respond by coming up with techniques to
deal with the new production of
information metadata title pages kala
finds tables of contents page numberings
all these things that allow you to deal
with a book and cyclopedia arise
specifically because rather than going
to eight different books on eight
different topics somebody has parceled
that stuff out into the important bits
and stuffed it into an encyclopedia
that's organized in a totally different
way based on a topical index
that allows you to randomly go in and
find in one book a whole collection of
knowledge that stuff continues for
another hundred years or so until the
Georgian era right the explosion of
natural philosophy the beginnings of
science Francis Bacon 1700s to 1800s
we've got you know in the early 1600s or
sorry late 1600s the beginnings the
Royal Society and this dissemination of
knowledge and the roads that carry the
books to people creates another set of
problems you're sharing knowledge or
sharing data across a broader community
you're taking measurements of bird
migrations or elk migrations or some
other thing and you're in Italy and I'm
in England how do we communicate this
stuff how do we know that we're talking
about the same things so that creates
classification schemes and so forth and
we think about linnaean biology right
what does biology have to do with data
everything because Linnaeus was one of
the first people to really sit down and
try to get a group of people to codify
knowledge in a particular way there were
lots of people doing this linnaean
taxonomy is what stuck right family
genus species hierarchy top-down
orientation it's statically structured
but what this gives you is the ability
to describe some species stick it
somewhere in a tree and move on and then
everybody else if they all agree with
the classification system for how you
put things into that tree well therefore
share a taxonomy share and ontology and
that's how we know we're talking about
the same stuff and so he came up with
the taxonomic classification system and
the interesting thing is that it's
descriptive it's not an explanatory
system because hierarchical taxonomy is
rarely are what you do is you take stuff
which exists and you try to shove it in
and make it fit there was another person
that we never talked about his name was
Buffon Buffon came up with a totally
different mechanism for doing this he
said we need to do this bottom-up why do
we need to do this bottom-up well for
example there are mountain sparrows
mountain sparrow
those are just smaller cousins to the
valley sparrows but look they breed at a
different time they follow a different
migratory pattern they feed on slightly
different things and they typically
don't interbreed he said species are not
fixed Linnaeus is still stuck in this
world of fixed biology where God created
everything nothing changes after you
create it and so all species are the
same all we have to do is catalogers is
put them into the taxonomy in the right
place very top-down in hierarchical this
was very bottom up this was like faceted
classification system that said look at
the components that make up the
organisms including where they live what
they eat the habitat that surrounds them
he says context is key rather than just
the static structure of the data this is
one of the reasons that a lot of you
probably argue with data warehouse
people because data warehouse people
think of linnaean taxonomy if we could
only make the perfect model everything
would fit into it the reality is that a
lot of stuff doesn't fit into a perfect
model because context is what's
important the context of data creation
has to be captured or you can't make
sense of it but that's a very
non-relational non-fixed model of things
that's more in the realm of faceted
classifiers which is what Buffon was
doing and so you have a flexible
structure you find a flightless sparrow
well flightless becomes yet another
attribute you stick that attribute into
the system and you go forward so it
begins to explain and demonstrate change
it's mutable over time rather than fixed
in time at one point what that means
then in the case of the days that you
have an argument between sort of the
strictures of sequel and the loser
structures of no sequel the idea of
fixed schema versus variable schema and
so forth and think about why that
happened because I'm going to get back
to that in a minute but we're going to
move on to the Victorian era the
Victorian era 1800s 1900s
industrialization this is the next wave
of information production and in
particular engineering knowledge we're
not now just looking at absorb
servations in science but observations
in industry why things are the way they
are technology gets created we have
we learn it we move forward the rate of
change increases card catalogs now for
libraries mass reference systems random
access metadata card catalog is nothing
more than a metadata catalog decoupled
from the things that it's indexing you
now have the concepts or ideas of
universal classification systems you
have the ability to to manage
information we have lots of debates
during the industrial era about how to
organize things because you're producing
trade journals engineering journals
every engineering group has a
specialized set of knowledge steam
engineers metallurgists chemists
biologists people studying medicine they
begin to refine their own taxonomy zuv
information their own classification
systems and they begin to create this
hyper specialization that we're stuck
with today and what we're trading a lot
of times is how to store and retrieve
information of different types of
different fields that use information
differently engineers use lots of tables
biologists don't and so how do you deal
with that stuff and the powered printing
information that is the brilliant idea
to take moveable type systems and
stereotyping and hook them up to a water
wheel or a steam engine and run that
thing ten times faster means that you
can print the crap out of everything and
so we have another explosion the problem
is that eventually we produce so many
books we need to come up with new ways
to classify them so along comes melvil
Dewey does anybody remember learning the
Dewey Decimal System yes which nobody
uses anymore sadly but notice the
spelling of this man's name because I'm
going to get back to that later too but
what he has done is he's done taxonomic
structure top-down static structure
these are the bodies of knowledge very
much like the Mesopotamians did it's
descriptive rather than explanatory I'm
not going to tell you why this is put on
the shelf next to that thing it just is
that's the way it's going to be as long
as you learn this indexing mechanism you
can stuff things into here as quickly as
possible and so he's come up with the
top down taxonomic classification for
all bodies of not
College cutter as anybody learns the
cutter classification system absolutely
nobody Library of Congress type systems
use this it was called the expansive
classification system he and do we would
argue about this bottom-up faceted
classifier here are the attributes of
this particular book how do you deal
with things like history of philosophy
is it a history book or a philosophy
book how do you cross-reference the
stuff flexible structure is explanatory
it's descriptive because you can keep
adding attributes it's faceted
classification scheme of course same
problems as before but for a different
set of knowledge and a different scale
sequel beat no sequel in the sense that
cutter lost to Dewey now why did this
happen that's one of the fundamental
questions it's the same fundamental
question is why databases for so long
beat out non-relational systems good
enough wins the day and it's not solving
the problem you think it is basically
problems of pragmatism when these guys
were dealing with they were dealing with
books and paper when you have books and
paper and you need to insert new pages
into that book you're stuck so if you
don't leave enough blank space in the
Linnaean in the faceted classification
system of Buffon you have a problem you
can't update it very effectively and
because it's all on paper you end up
having to constantly rewrite the book
this is like doing index rewriting in
databases so this works really well and
it also jives with the philosophy of the
day which is a very top-down model much
like data warehousing and bi fit with
the management theories of top-down
hierarchical management the other thing
is that sometimes it's not solving the
problem you think it is this is solving
a totally different problem Dewey was an
efficiency expert the reason his name is
spelled mal VI else because the extra L
and e on the end of melville took that
much extra time when he was signing his
name so he removed those letters he was
a hyper efficiency expert in what he was
worried about is
kind of what Adrian was talking about
the Cassandra problem of how do I write
stuff as fast as i can because they
produced books faster than librarians
could shelve them he wanted to solve
that problem he was still working on a
retrieval problem of how do I find those
books as well as how do i shelve those
books interestingly dewey decimal system
does not tell you where to find a book
it tells you which shelf to find a book
on you can have 16 different books on
one shelf with the same Dewey Decimal
number so your job as the reader is to
get to that shelf and scan the shelf
spines finding the book you're after
whereas cutter system identified not
only a book but a particular version of
a book so you could go back to previous
versions of that thing in essence giving
you the idea of time travel for example
if you're using something like say HBase
the ability to go back through the time
stamps because you never delete old
stuff you can time travel in these
things if you want to whereas you know
he was just dealing with shelving books
and so there's this problem of get data
in and get data out now summarizing all
of that history basically there are
scale changes throughout clay tech to
paper tech version 12 version 2 2
version three masts printing all of
these things are dealing with ontology
taxonomy and schema problems they're
dealing with what happens when you
produce a lot eventually you have to
come to grips with dealing with all that
crap and so every time there's a 10 to
100 fold increase in either density or
production you come up with new
mechanisms for coping with it digitized
data is interesting because for the
first time it's different for the first
time you have the ability to use
alternate schemes of organization that
can couple both retrieval with ingest so
you can begin to use the systems that
were devised by cutter or Buffon
hundreds of years ago to categorize and
manage information and that gets down
into the detail but part of this details
that there are always trade-offs right
it's top down versus bottom up top downs
a lot easier than bottom up it's much
easier to explain
such eat much easier to build systems
and processes around Authority versus
anarchy when stuff is all over the place
we have to come to grips with anarchy
people don't like loose ends they like
the ability to stick stuff into buckets
to put things on their shelf and keep
them in their place we don't like
messiness and so to the extent that you
can deal with messiness which is what a
lot of what this Web 20 stuff versus web
10 stuff was all about was how do you
classify and manage that bureaucracy
versus autonomy when you can move to
faceted classification systems you don't
have to go to the central authority to
get clearance that this is the schema
that we're going to use and dynamic vs
static data warehouse models relational
database models are a static
instantiation of a point in time of data
model that has to evolve versus static
models and so you've got these ideas of
what you can do up front versus what's
postponed what you postpone in dewey
versus qatar going to the shelf to find
the book qatar gives you the exact index
this gives you a location it's much like
reading that 64 megabytes i'll sized
chunk and scanning all the rows that you
can fit into that to find your data so
what's happening is you're making
trade-offs what it teaches us is that
there are organizing principles scale
requires different principles and we are
now at a point where scale changes are
causing these things to shift again that
changes architectures if you look at
library structures they change during
different eras because of how we had to
get things you used to go to a library
and ask a librarian who got you books
now you get the books ourselves it's
like dealing with microservices at key
points emphasis shifts it shifts from
collection to use and curation and so
when you're going from one side to the
other moving from ingest to consumption
the problems become different for the
last probably eight to ten years most of
what we've been dealing with is the
ingest problem and most of this no
sequel stuff is focused on ingest
problems but unfortunately doesn't
really solve consumption and
mination problems are effectively so you
have to think about recording and using
and then sharing the summary of the
summary is that new technology
developments create these new methods to
cope which allow you to move up a level
of scale by encapsulating what's below
it which moves you to new information of
hville availability and it just keeps
feeding itself and that leads you to of
course Big Data because every single one
of these things is about scale increases
in data which gets us to actually
dealing with big and this is where the
technical part starts and yes I actually
have worked on databases and processing
kernels parallel processing code and
stuff and so this is where it gets fun
because we've spent all of our time
consuming all of the cycles and wasting
them so why doesn't your database scale
when you're building a website well the
hipster would say my sequel
does not scale therefore relational
databases don't scale therefore we must
use no sequel and the real problem is
that my sequel isn't a very good
database doesn't solve a whole lot of
problems and it does some dumb things
like for example the schema and
flexibility of my sequel means that
every time you add a column you have to
dump and reload the table which doesn't
really work well and in most modern
databases that doesn't happen but it's
not a modern database so until there's
enough information you're going to have
these problems of just organizing it and
getting it in but once you've solved
that right problem then you worry about
read problems and that's where you know
we come to Moore's law which was throw
hardware at it by throwing hard we're at
it we're going to solve all problems but
Moore's law died circa 2006 according to
Moore himself so you can go read his
paper if you don't believe me basically
though data has continued to grow
because we've solved capture problems to
the extent that it's once again
outstripping our ability to manage its
consumption and use and so we're in this
how do we solve consumption and use
problems and that's where a lot of the
new data architectures are coming from
so if we look at the history and this
will be the history of my career
basically client-server was a starting
point moving off of single system big
mainframes we separated the application
from the datastore we decoupled the
persistence layer from the code and
transaction processing ran against a
database you pointed to the database you
issued your queries initially pre
relational we were using things like ims
or or some other database that was
non-relational how did we fix this weld
a scale client server we realize that
you know one of the problems is that
when you have a single server like that
connection database connections consume
all the memory on the server we don't
have enough memory blah blah blah let's
just do some connection pooling put an
application so remove some of the logic
out into another layer we end up with
the whole three tier architecture and
how do we scale this well the bottleneck
is no longer up in here the bottleneck
is now back there in the database the
relational database you throw money at
it you ride the hardware curve because
that's the easiest thing and in you know
on the database side what we were doing
was breaking locks refining the
granularity of parallelism from one
global lock sorry MongoDB people that's
a really bad idea to finer and finer
locks to get node and processor and
thread efficiencies now make the
database bigger scales for a while but
eventually it's not going to work
because you can't write all these
transactions into one big database and
so we start to split things we create
these stateless models right web 10 web
20 was about building stateless
architectures that let you load balance
out here load balance in here and
hopefully multiplex into one big
database right circa nineteen
ninety-seven 299 Oracle was king they
were killing it my sequel was on the
rise because you know it wasn't good
enough you could still get by with the
simplicity of operations by making the
database bigger and bigger until you're
spending two million dollars a quarter
trying to scale your Oracle database and
eventually that big thing starts to
become a problem in a bottleneck it
limits performance because of the
contention issues it's got scalability
problems availability reliability I'll
start to creep in and so what do we do
we replicate it because replication
solves all things now when you're trying
to do this well you create these
read-only replicas to do all the read
work so that you can just focus on the
right or
into that big database and you can
separate the concerns somewhat deal with
your transaction processing but you may
have solved some of your scalability
problems at the expense of now having to
do back and work to muck around with
operational overhead and other problems
and eventually you can't scale that
anymore and so what do we do we shard so
sharding seems like a fine idea you just
take your database and then you create
some sort of balancing scheme it might
be hashing it might be something else to
take chunks of your data and throw it
off to different servers your database
is now many databases and the big
problem of course is if you need data
from more than one shard and so the art
of design is all about how to place
these things appropriately and then
rebalance them as it grows and
eventually it doesn't really work
anymore there's only one difference
between shard in that other word so what
do you do with increasing traffic and
how do you scale that further well start
dealing with latency issues by caching
data so we create caching layers up here
eventually you get to the point where
most of your data that you're frequently
using is cash and you hardly ever touch
those things on the backend now cashing
it the service tier is great but it
increases the complexity of the
architecture again because now I've got
various things accessing I have to deal
with cash timeouts and I have to deal
with the fact that maybe an update hit
on the back and that isn't reflected on
the cache and how do I have the traffic
going back and forth between the cache
glares and those servers and it starts
to get even more complex more software
more things to go wrong operational
complexity so more Hardware more things
to break more servers increase me time
between failures start to think about
what Adrian was talking about this
morning designed for failure don't try
to design to make everything as robust
as possible that runs counter to the
last 30 40 years of engineering which is
make things as bulletproof as possible
different design techniques apply
management and administration in this
world is a mess software complexity
between you know memcache sitting on top
of shorted my cycle with load balancers
at every layer multiple layers of app
servers talking to each other Layton
sees because of all of these layers
become problematic and so you start
pushing
hashing layers further and further I
mean CDN sit on top of caching layers on
your service tier which sit on top of
caching layers inside the actual
database servers and data administration
making it consistent managing and making
sure you doing the right stuff becomes
virtually impossible and so problem
solved well distributed no sequel
databases things like Cassandra or Mongo
or choose your poison why because you
have a single instance to the program's
a single API a single service layer for
persistence but underneath that it's
hiding all of that other crap the
infrastructure as a service argument
Adrian made I now have a distributed
system that can scale and it actually
matches this architecture because by
sharding my sequel you were trying to
take an architecture from the last
decade and apply it to a stateless
architecture of the present and that
didn't work this finally matches that
and so you have something there you have
caching typically captured and managed
within that layer but you've also
jettisoned some stuff and so you've
really simplified things and made it you
know much easier but there's no such
thing as a free lunch yes you can put
this new thing in but it's not going to
solve all of your problems and sometimes
you won't notice the problems you've
created until several years down the
road which a lot of people doing the
early no sequel work discovered
including myself which is that you don't
get perfect replacements when you
jettisoned things like a sequel
relational model so what have we done
well we scale transaction processing but
not query processing query processing
went through the same evolution right
transactions went into a database and
then we built reports on them we pointed
the reports at the same system and
typically what you're dealing with is
aggregate and low selectivity queries
these what were my sales yesterday wrote
was the sales trend for the last three
quarters those kinds of questions hurt
the performance of this because of the
shared infrastructure so what do you do
you make it bigger bigger and bigger
mainframes which costs more and more
until eventually the economics changed
because we got small UNIX systems
later Linux systems to replace those big
servers in the early days what we did
was we offloaded a replica database so
it was the same schema as whatever this
thing was and then you report it off of
that so you separated outbound data from
inbound data and that works great
because now I can tune this to take
transactions as fast as possible deal
with locking I can jettison locks over
there I can jettison non-used indexes I
can make that thing better for reporting
but typically i am running off of that
same crappy schema design for
transaction processing when i want is
something that's designed for reporting
purposes data warehousing comes along
1988 and at that point you have a
different model because you don't deal
with just reporting off of one app but
reporting across many apps you want to
be able to integrate the data back
together to see a picture of a process
from one end to the other which means
talking to multiple systems how do you
do that pull the data into that well
that data model can't be the
conglomeration of these three things so
you transform it into that thing by
doing that you can then put query tools
on the other side and this solves a
second problem every time we wrote a
report we wrote a program that is a
nonhuman scaling problem ad hoc query
tools get around this by creating a
generic data model that lets you spit
out sequel and get the data that you
need it decouples the reporting
consumption layer from the actual data
when you do that you get a whole new set
of things you get a whole new set of
scale you've got a hundred x headroom
now that worked great for a while but
eventually response times became a
problem as the data grew and the
complexity of the schemas grew so we
started doing things like cubing
architectures if you're familiar with
tableau it's nothing more than a 1969
OLAP cube married to a really fancy
interface but those things you took a
subset of your data you build it into
some sort of dimensional hypercube for
efficient storage for sparse data you
jam that thing out there you put query
caches so that when you fetch data it
stayed cashed out there and you move
forward eventually ending up at the
teradata zor the verticals or the other
kinds of relational databases but
distributed relational there multi-node
multi-server data
hashed out across a grid of these
servers with one sequel pipe onto it
populate it back here you query it out
there problem solved in the data
warehousing world now these two
architectures look virtually identical
because in many ways they are load
balancing front-ends distributed caching
layers a distributed set of systems
scalable distributed parallel databases
and they both work and they both work at
scale you've got petabytes of web app
out there you've also got petabytes of
query database in relational models out
there the thing is the nature of the
work that's done inside them is
different transaction processing to
queries which gets us into the whole you
know big data is unprecedented nonsense
of the Big Data industry there's a big
difference between having no past and
having to invent something new out of
whole cloth and just rejecting what you
don't realize is out there and that's
what most of the no sequel industry has
done if we go and look at the timeline
IMS and pea can database and ids
multivalued hierarchical databases these
things existed in the 60s and 70s
there's really not much difference if
you understand the design models between
designing for ims and designing for
something like Mark logic or even
MongoDB you can go back and rewrite
those text books to fit the new world of
no sequel as we moved along we realized
that having schemas was actually a good
thing because multiple applications
could share the same persistence layer
but that created a problem and so we
started to decouple schema from app and
schema from the underlying physical
storage architectures because when you
make your physical storage architecture
map onto the objects that you're storing
you have tightly coupled your code and
the persistence layer which means unless
you hide that access through a service
API with a with a mutable model you will
never be able to share those things
properly just one of the mistakes a lot
of us made so you move forward and
basically you're at this this period of
having no sequel to us in the 80s having
to learn sequel to well no sequel is bad
all through the 2000s too well we mean
not only to the point now where
everything is getting a sequel layer
slapped on
of it for different reasons the
trade-offs that you make tend to be
around things like query optimization
which on the surface seems easy but the
reality is that it's not rocket science
rocket science is what you do when you
fail at query optimization you may not
realize that there's more than one kind
of joint out there and that's the
beginning of the unraveling of much of
the code when you build a service to
query some data because you have to keep
parameterizing and dealing with edge
cases and it gets uglier and uglier
because a no sequel and the optimizer is
you you have to figure out a physical
data model to solve what is not a
physical data modeling problem in Mongo
or in a document store of any type how
do you deal with the simple many-to-many
relationship nesting one thing fits
inside of the other whenever you do that
you blow something up somewhere down the
road and so the disaggregation of that
you do what we did in ims you build join
indexes which allow you to deal with
this stuff but then every single access
path becomes an index and eventually you
end up in indexing hell because what's
really happening is that you've stripped
off things that a declarative layer up
here give you into a non declarative
layer down here solve storage management
very fast quick data access dealing with
each is dealing with a set of unique
things all the same versus dealing with
all kinds of different stuff in
arbitrary complexity transaction and
consistency acid versus based standard
API query layers in the in the form of
sequel and anything not done by the
database becomes application problem and
Google was one of the companies that
recognize this in the last couple of
years and said you know what that
actually isn't worth it we need to do
something that solves these problems in
the same way that relational solved
those pre relational problems the thing
is what trade-offs did we make well
relational made some bad trade-offs
global static schema model one model to
rule them all really type limited things
that relational operators can work on
forget about time series sequel the
impedance mismatch isn't so much the
relational model it
that sequel only gives you d normed sets
back and then you have to map them back
into your code and that sucks and you
shouldn't have to do that what I didn't
list on this there was scalability and
performance relational scaling problems
have solved query don't worry about it
if you are trying to shift to
non-relational because you're solving
repetitive query and join operation
problems you're solving the wrong
problem this is a performance trajectory
curve that's already four years out of
date down here in the hundreds of
terabytes range this is the sequel
servers db2 s and Oracle's of the world
notice how flat that line is when you
look at the green curve here this is MPP
databases the biggest one that I know of
is 40 petabytes that's 40 petabytes
trillions of rows sequel on top acid
compliant database for querying up in
here we of course have the really big
stuff which is well beyond this but I
would say most of you are right in here
you're probably not 40 petabytes you
probably do not need to move beyond that
and in fact relational query problems
have been solved get over it and move on
you can even make a relational database
look like a key value store by basically
mapping out your key setting up a
payload and using what's called a
user-defined function which allows you
to decode any arbitrary stuff you might
shove in there and in fact that's how
ebay does a lot of their work which i'll
show you here just really quickly i'd
say here's one of these 40 petabytes
systems i mentioned you stuff the data
into there you're right quarries against
it but you have things like query
strings everybody says click stream and
log structure data doesn't work
relationally well stuff it in there is
text it works fine have a UDF that can
actually take the delimiter like the
name value pairs that are a query string
chop them up so that you can get things
like this half the problem solved the
other thing is called a table function
it's a many in many out function it
allows you to take one row and turn it
into multiples so what you're seeing
here with two sequels statements is
taking a table with stuff that's stored
in a query string shoving it in like
that so the ingest is very quick but
what you really want to know is
how many times were these items
displayed on pages all across my entire
website well any page has a record it
lists that stuff you d couple that you
run this query right here it's running
against a trillion rows of data it comes
back in about 30 seconds and it gives
you the counts of all of the items by
day so you get this kind of thing Hadoop
and its ilk solve different problems
storage and processing problems not data
retrieval problems and if you're trying
to solve data retrieval problems in this
kind of an architecture you're making a
mistake the real thing that it's good at
is the processing and the storage which
are in order or two orders of magnitude
cheaper you're letting people at that
end of the spectrum set the taste
profile for what you're trying to
consume that stuff will leak back
through our architectures eventually but
you don't need to jump there for the
bulk of the market anybody familiar with
the Microsoft study of their analytics
cluster they went through 174,000 jobs
figured out that the median size of the
data set fed into a model after the
model had been built was 14 gigabytes so
you have petabytes of data but that's
how much you're actually running through
the number-crunching at the end of the
day now to get there you probably had to
filter through and sift that so the
model building is actually a pretty big
data problem but it's not the processing
of it and so what makes this stuff big
hierarchical structures nested
structures encoding time series this is
the stuff you tell the data warehouse
bigots because they can't deal with any
of this stuff these are the things that
you need to focus on in the architecture
and you know when you talk about
unstructured data what you're really
talking about is not unstructured but
unmod 'old data all these things are the
same thing structural formulas images
these kinds of you know mass formulas
all of this stuff in a database would
just look like text Hadoop can turn that
stuff or Hadoop and its ilk processing
spark choose your poison can take those
things and treat them all as data that
can be tabular
did consumed that's part of the magic
that you can do to this stuff and so if
you look at computational and data
workloads we solved transaction
processing we're their state-of-the-art
Cassandra and so forth for back-end
persistence layers we've solved analytic
processing and sense of query and
retrieval the analysis problem is
different its scientific computing first
we dealt with transactions the principal
element of the architecture here in
analytic systems is queries in
scientific computational systems it is
an algorithm or a computation there's
actually three different workloads no
sequel works really well except for the
acid compliance problem here query
problems and retrieval problems your
standard bi and look at data eyeballing
stuff and your algorithmic require three
different engines maybe over the same
set of persistent data Hadoop is trying
to promised the Holy Grail we aren't
even close it's five to ten years out
but storage retrieval and analysis in
one box so how do you carve this up for
your architectures well think about
computation and whether you're doing a
little or a lot think about data volumes
and whether you're doing a little or a
lot and what you end up with is small
analytics and small data do it in a
database you can do in database data
mining in sequel server it's not that
hard if you don't have a lot of data if
you have a lot of data you can start
moving up into other spaces but if it's
trivial computation go there it's still
relational if you're doing big math over
lots of data now you're talking about
interesting shared nothing architectures
that are non-relational if you're doing
big math but small amounts of data
you're probably talking about you know
supercomputing GPU systems sharks park
things like that that can solve it but
don't have to solve it at scale okay so
basically you have three axes but you
can only solve two with one system it's
another corollary of the cap theorem and
so what you end up with is looking at
where we are today Hadoop and no sequel
the real question is what problem does
this
oh jeez solve and how does it solve it
you're looking at what will happen in my
posit is no sequel will fail unless it
comes up with mathematically derived
data models and declarative languages
over top of that underlying sub
structure because without those things
you don't get an ecosystem you don't get
competition because each thing is its
own special snowflake and you have
fragmented development environments
fragmented ecosystems and nothing can
evolve as an industry around this so
right now everybody plays
winner-take-all Wars but maybe these
aren't the databases we're looking for
the thing to look at is something to
papers from google this is basically the
big conclusion spanner and f1 2012-2013
papers from google that tell you
basically all those trade-offs were
making in no sequel all those things
where we said relational was bad well it
turns out that they're actually bad so
we have to move on stuff that stuff back
into it and marry these things together
and so f1's evolution is acid compliant
synchronous replication cross data
center data distribution and MapReduce
to underlying data as well as sequel
layers and that's really where we end up
and so what we have then is if you wait
long enough most problems will solve
themselves and so Google will solve them
for you and hopefully open source it at
sunset at some point so thank you for
your time and I'm turning it back over
to you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>