<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Secrets of High Performing Teams: Science Edition • Jez Humble | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Secrets of High Performing Teams: Science Edition • Jez Humble - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Secrets of High Performing Teams: Science Edition • Jez Humble</b></h2><h5 class="post__date">2016-11-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EmDtmYwDs50" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so let's welcome chairs thank you very
much it's on my bucket list to be
introduced by Linda rising so I can
cross that off and have thanks very much
Linda and in fact a lot of this is
inspired by work that Linda has done so
thanks very much so my name is Jose
humble among other things
I'm CTO of this small startup devil's
researcher and co-author of these three
books devoxx handbook is coming out this
week in fact which is very exciting for
me because it's taken very long time to
be written divorces basically at the
peak of inflated expectations now and
the only way is down so please review
the session and feel free to ask
questions and we have a microphone so
you can ask questions in real life so
what I'm going to talk about in the next
50 minutes is number one how to make
your data suck less and in particular in
the context of writing surveys how do we
do surveys in a scientific way we have
been using surveys to find out about
DevOps and good practice and what makes
high-performing teams and organizations
and I going to talk about that today how
we what the science tells us about how
to build high-performing teams in
particular I'm going to talk about
continuous delivery because that's my
thing and also it turns out to have a
big impact on building high performance
teams and I'm going to talk about
management as well so in order to
understand this talk you need to know
about statistics my partner my business
partner Nicole fullscreen is she is
professor accounting at Utah State and a
statistician and also in a previous life
a Systems Administrator for large IBM
mainframes so she knows all the stuff
about stats I will tell you everything I
know about stats in this talk please do
not ask me questions about stats please
follow her on Twitter and ask her
questions about stats instead because
she is the stats guru so surveys who
thinks the surveys a sheet
fair enough who loves the data from
their log files and who has seen shitty
data in their log files right so you
know data lies and the job of Statistics
despite its reputation is to find truth
in data and you know this is useful in
all different kinds of fields but what
we're trying to do is find out about
sentiment about what people believe
about things like culture and those are
things that are difficult to measure
it's easy to measure something like
temperature you can get a thermometer or
measure temperature measuring what's
inside people's heads is much more
difficult but it turns out that we can
in fact do this we have something called
a latent construct which allows us to
measure things that are psychological
and there's a whole field devoted to the
study of this called psychometrics
and so what we do is use psychometrics
to enable us to measure things like
culture in a statistically valid
scientific meaningful way and what this
does is gives us what it says here a
reasonable assurance that the data is
telling us what we think it's telling us
and by the way some of these same
techniques can be applied to for
instance your log data so what we do in
psychometrics firstly we create what's
called constructs constructs are what
allow us to measure a thing in the real
world even if it's in someone's mind
what we tend to try and do wherever
possible in science is use constructs
that people have previously created and
validated reuse is very important in
science because if you reuse stuff you
don't have to prove that the measure is
reliable there's a bunch of papers a
bunch of books in the literature we
drawn those wherever we can typically
the wording of those measures has been
validated we use things like card
sorting to look at the questions we ask
to make sure that you know people sort
cards into groups based on whether
I think the question is asking the same
thing that allows us to tell us whether
the different questions we're asking and
measuring fundamentally the same thing
or something different and then we use
statistics to evaluate the results of
the surveys and validate that our
constructs are actually good we look at
things like a discriminant and
convergent validity say that's basically
making sure that our results are
consistent and that the things that
should be related are in fact related
and then not unrelated and we establish
the reliability of the constructs we
talk about prediction in the reports
that we put out and prediction is a very
important word because it implies some
level of causation in science there's
only three contexts in which you can use
the word prediction one is if you're
doing a longitudinal study so you're
doing a study where you evaluate the
system at multiple points in time those
are quite hard to do and we are not
doing longitudinal studies the other way
in which we can talk about prediction is
in the context of randomized controlled
experiments we can't those are very
difficult to do and one of the reasons
why there's so little science in the
study of high-performing teams in the
context of software development is
because it's so hard to do randomized
controlled experiments in the context of
teams because there are too many
variables and we can't control the
variables you try and compare two
different teams and there's it actually
Steven foster you once told me a story
about looking at a team that was
estimating and they couldn't they found
that the historical estimates were
totally useless in predicting the future
performance of the team and they were
looking at what's different between last
release when these estimates were good
and this release when they're not good
and eventually what they found is the
team have been moved and had been
sitting in a room with Windows and then
the team would be moved down into a
basement where there were no windows and
the scene wasn't getting any light and
this is the only thing they could think
of as a reason why suddenly the team was
much less productive so things like this
that we don't expect to be variables
very often you know actually very
important isn't almost impossible to
control for those kinds of variations so
it's very hard to do randomized
controlled experiments in the context of
teams so we use the third method of
demonstrating prediction we use theory
based design and what that means is we
have a theory based ideally on the
literature on existing studies and if
not on our hypothesis and then we go and
run the data and then we get the data
back and we validate if the data shows
that our hypothesis is true or false so
you know this is very basic kind of
baked Honea in science if you like but
it works and it does allow us to talk
about prediction and we found some
interesting things and crucially we've
had a bunch of our ideas disproved as
well which is one of the ways that you
know you're doing it right if some of
your theories are in fact false so I'm
going to talk about the things that we
found that weren't true as well as some
of the things that we found that were
true where none of these conditions
apply we don't talk about prediction we
talk about correlation so I'm going to
talk about an example of using
psychometrics to measure a culture which
is one of the things that we measured
when we were looking at how to build
high-performing teams so culture it
matters to our study but there's many
different kinds of culture people talk
about for example national identity and
differences between behaviors from
people from different countries people
talk about adaptive cultures there's
this concept of value learning the one
we actually ended up using was looking
at culture in the context of information
flow which is from a scientist called
Ron westrom who's studying safety
outcomes actually in healthcare and
aviation and he built a model to predict
safety outcomes in healthcare and
aviation based on information flow
throughout the organization and so we
used his model as our basis for
measuring culture so this is Wes trims
typology he basically divides up
organizations based on whether they're
pathological bureaucratic or generative
and he has these six different acts
for looking at how people behave in
these different types of organizations
whether they cooperate effectively how
we deal with messengers how do we deal
with people when they bring us bad news
do we train people to bring us bad news
do we ignore people who bring us bad
news or do we shoot people who bring us
bad news how do we deal with
responsibilities and risks how do we
deal with bridging between different
parts of the organization between
different silos and then two things that
I'm particularly fond of how we deal
with failure does it lead to inquiry to
learning does it lead to justice or does
it lead to scapegoating and punishing
one of the things that is very important
about organizations is that
organizations of people are complex
systems and there's a whole theory
there's a whole you know fields devoted
to the study of complex adaptive systems
but I think two things are very
important about complex adaptive systems
number one nobody has perfect
information especially not management
and the other thing that's important
about complex adaptive systems is that
you can never predict the outcome of
your actions there will always be
unanticipated side-effects and often the
side effects will be bigger in impacts
than the intended effects so in a
complex adaptive system where no one has
perfect information and you can never
predict the consequences of your actions
failure is inevitable in any complex
adaptive system failure is inevitable
and so when something goes wrong our
instincts as humans is to find out who
did it and to punish them or fire them
but in a complex adaptive system that's
entirely the wrong thing to do whenever
that happens you should always ask
yourself if that had been me doing that
thing could I have made the same mistake
and the answer if we're honest is almost
always yes I could have done and the
reason is because the person there
didn't have sufficient information or
didn't have the necessary tools at their
disposal to test what would happen if
they did thing X and so if you perform
an investigation into something going
wrong and
the end result of the investigation was
it was Steven let's fire him you have
failed that should be the beginning of
your investigation
why did Steven do that thing how can we
improve the information steven has how
can we give Steven better systems to
enable sis Steven to test the results of
this action and safely fail quickly if
that thing is going to lead to some
disastrous consequence that Steven can
possibly predict it and then finally how
we deal with novelty is it implemented
does it lead to problems like scope
creep or my favorite is novelty crushed
I think probably intuitively everyone
knows where they are on this spectrum
and what's fascinating to me is as I'll
show later the same factors that predict
safety outcomes also predict innovation
outcomes the same factors that predict
safety outcomes also predict innovation
outcomes and the reason for that is
you've got to be in a system if you're
going to innovate where you can try
things out and get things wrong and
where it's safe to get things wrong the
same factors that enable us to build
safe complex adaptive systems also the
same factors that enable us to innovate
so we're trying to write questions for
this trying right questions to find out
where people are I'll give you like a
minute to think of a way to write a
question to test one of these things so
just think of what you would ask
what we want to do when we're asking
questions is these strong statements
with clear language things that you
could agree or disagree with strongly so
these are the questions that we actually
asked so you know the second one on my
team failures are learning opportunities
and messengers in the of them are not
punished that's something you can
strongly agree or disagree with so when
we did the analysis we found these
questions to give results that were both
valid and reliable and turns out their
predictive of IT performance and
organizational performance which I'll
define shortly so when we ask these
questions what we do is we use what's
called a Likert type scale where you can
go from strongly disagree to strongly
agree and you know this is this gives
you one and this gives you seven and
then you just take the numbers from
these different questions and you
average them and that gives you a number
for culture so we can quantify culture
in this way however sometimes
sister-girl analysis shows that the
measure is not valid and reliable so we
for example wanted to ask about failure
notifications we thought that you know
in teams are notified about failures
that would tell us something about their
IT performance so these are the
questions that we asked about failure
notification so it turns out this was
not a valid and reliable measure when we
fall in statistical analysis this this
turned out not to be a thing basically
any idea why that might be
what the statistics doesn't tell you is
why it's not a valid or reliable thing
you have to guess so what we ended up
doing is we kind of thought about this
and we thought well actually this is
kind of asking two different things
the first two things are kind of asking
us about notification from outside of
our team and the rest of the things that
asking us about notification of failure
from data that we can get from within
our team and there are actually two
different things and when we split the
results from these questions into two
separate things those things
independently did in fact turn out to be
valid and reliable measures so this is
an example of the kind of thing you do
when you do statistical analysis you
find out well actually you know these
multiple things or you know this whole
thing isn't a thing at all or sometimes
you can combine things and take a bunch
of things that you thought weren't
exactly the same but actually they all
move in the same way in the data so
there's a bunch of other stuff that
Nikol does you know comparing early
responders versus late responders to see
if they're statistically different
looking at survey drop-off rates looking
at biases and there's some statistical
test she does surround color method
variance I have no idea what that is
so once you've done all this work then
we can look at the data and this is what
the data tells us first of all we found
that IT performance matters we've been
told for years the IT doesn't matter but
it can't provide you a consistent
strategic advantage in terms of
innovation we found that's not true
we use the standardized measure of
organizational performance in terms of
profitability market share and
productivity and what we found is high
performers were twice as likely to
exceed those organizational performance
goals as low performers in IT so we
found that IT performance does have an
impact on your organization's bottom
line secondly we were able to find a
statistically valid way to measure IT
performance so he was at Kevin's talk
this morning great so he talked about
that study the what's it called the two
axes yeah the alignment track right so
an alignment trap on the right bottom
right there's the bit about high
highly efficient IT what we've found a
way to measure is that basically how do
you demonstrate that you're highly
efficient how do you measure your
efficiency and that is obviously where
you want to go before you go for
alignment so if you want to measure IT
efficiency this is what we believe you
should use an RIT performance metric
actually is two separate things which we
found were actually one statistically
valid measure so the top two things are
throughput how fast can we deliver how
long does it take us to get changes from
version control to production lead time
and deploy frequency and then two
measures of stability how long does it
take us to restore service when there's
an outage or a degradation in service
quality and then change fail rate when
we push a change out to production what
percentage of the time does it fail or
do we have to do emergency fixes because
as we all know no one ever rolls back
with just emergency fix until it works
so two different measures through prints
ability traditionally we've been led to
think that these two things are a trade
off if we go faster we will break things
the most interesting result for me from
the whole study and the one thing you
should take away from today is that that
is not true what we found is that high
performers do better on both of these
things high performers achieve higher
levels of throughput and high levels of
stability this is not a trade-off and
this is what develops is a thing in my
opinion is because we found a way to
change the game we're no longer
operating the trade-off which is where
as engineers we usually operate we've
actually changed the game and we're now
able to move to a different paradigm
where we're improving by throughput and
stability and the data clearly shows
this out what we find is that high
performers are performing deployments on
demand they're achieving lead times of
less than an hour they can restore
service typically in minutes and their
change fail rate is not 15 percent
whereas low performers are achieving
deploys frequencies typically on a
monthly basis they're achieving lead
times of between 1 and 6 months they can
typically get lead times
restore service in less than a day for
certainly not in minutes and they're
achieving change fail rates between
sixteen and thirty percent so big
difference between the high performers
and the low performers and you can think
of organizations like Amazon and Google
on this axis and then you know much more
traditional waterfall organizations on
this axis but crucially the same
capabilities that enable you to move
fast if done right also enable you to
restore service and fix problems fast as
well we've lived in a world for ages
where we've been focused on how do we
prevent things from happening in complex
adaptive systems you can't do that you
can't prevent bad things from happening
complex adaptive systems will naturally
drift into failure so the question
becomes not how can we prevent failure
but how can we recover from failures as
fast as possible how can we detect them
how can we fix them as fast as possible
when the next CVE comes out and there's
a flaw in our ssl libraries hack scene
can we get a patch out to all our
servers these are the questions we need
to be asking how can we restore service
and the same capabilities that allow us
to move fast also allow us to detect and
respond to failures faster as well
creating more secure safe resilient
systems so if you look at the things
that give us the highest correlation
with IT performance this is what we
found number one having your codes
application figuration and system
configuration in version control and
what was interesting about this was that
it was more important to have your
application configuration and your
system configuration in version control
than it was to have your code in version
control so who actually can recreate
their production system purely from
information and scripts in version
control okay who relies on manually
logging into consoles and changing
things in order to configure their
production systems okay that's about
half half which is actually pretty good
I think but that's a great example of a
drinking problem parity and that's a
great example of a capability which
allows you to
faster and create more resilient systems
being able to do this being able to have
this capability allows us to create
testing systems much more quickly so you
can create test environments on-demand
in this way also if there's a failure in
production and we need to restore the
state of our system to a previous good
known state we can do that
programmatically in a deterministic
amount of time if you're logging into
consoles and editing configuration
settings that is non-deterministic
because humans make mistakes whereas if
you can do that purely in an automated
fashion we can restore service in a
predictable deterministic amount of time
that allows us to restore service faster
so that's a great example of a
capability that increases throughput and
also increases resilience getting
failure alerts logging and monitoring
systems not from say Twitter something
that's very dear to my heart
developers merging their code into Trunk
daily not working on long live feature
branches where we merge into the feature
branches regularly but actually working
in small batches on trunk and merging
changes into trunk on a daily basis
that's very important probably the most
important thing I look for in a
developer is that ability to break down
big problems into smaller problems and
check those problems into master into
trunk on a daily basis or more
frequently also this bit which I just
talked about developers break up large
features into small incremental changes
so these these two things are essential
to continuous delivery and then finally
the DevOps peace when development and
operations teams interact the outcome is
generally win-win so these are the
things that have the highest correlation
with IT performance when we want to look
at prediction the biggest predictor of
IC performance from the 2014 study and
I'll move on to the more recent studies
in a minute was having a peer review
change approval process what do we mean
by this
so there's a couple of ways you can do
change approval one is a peer review
process which is basically pair
programming or code review by people
inside your team and there's also
external change approval processes like
having a change approval board or some
other team testing your changes or
reviewing your changes
so he only relies on internal peer
review approval in order to go live hey
who has to have approvals for an
external team in order to put their code
into production like a change approval
board okay that's about a third of you
so it turns out that doesn't work it
produces much lower throughput as you
would expect but having an external team
do change approval has negligible
positive impact on the stability of the
systems you build so in terms of
actually improving the safety of the
systems you build it has a negligible
impact but it massively reduces
throughput and much of that kind of
thing is is what I like to call a risk
management theater I once I had I had a
colleague when I was working at Thor
works who worked at a large European
electronics manufacturer and they they
had to change approval process which
involved a spreadsheet with seven tabs
with the details of the change that you
would email to a change manager in
another country who would read those
changes and the change manager was
non-technical so they didn't really
understand what was written there and
they would phone up the dev lead and you
know ask questions and if they liked the
sound of the answers they would approve
it otherwise they wouldn't and the
developers knew that the change manager
didn't understand what was in the
spreadsheet so they would just copy the
last one and change a few things and
then send it on and the change - who
knew that developers knew the change
manager didn't know was in the
spreadsheet and at this point what we
have is risk management theater not in
fact a process for effectively managing
risk and this is exactly what our data
tells us to be the case for active
monitoring when we relationship between
Devon Ops and then this thing which I'm
going to talk about high trust
organizational culture this is what we
measured with a Western typology and we
find that the culture of your
organization predicts both IT
performance and organizational
performance so this result has been
published in a peer-reviewed journal you
can go and download the paper I'll have
a link at the end of this slide we have
real science that shows that culture has
a measurable impact on IT performance
and organizational performance and this
study was kind of replicated by Google
so
Google did some research into what makes
a good team and they had all kinds of
hypotheses you know to know developers
and the database administrator and a
manager or let's have no managers and
see how that works and what they found
was really interesting was that the
composition of the team in terms of the
skill balance was not actually useful in
finding successful teams what they found
was this the most important thing in
determining whether a team was going to
be successful at Google was
psychological safety team members feel
safe to take risks and be vulnerable in
front of each other kind of looks
similar to this and then dependability
structure and clarity the meaning of the
work they were doing and the impact that
it was doing and what we actually found
in last year's study we had two other
measures that we measured because we
were trying to break this down and what
we find is that actually in terms of the
statistical analysis when we analyze
this this in fact doesn't work as a
single constructs it's not a valid and
reliable construct when we measure the
results it's actually two separate
constructs these two I can't remember if
number three went with one and two or
whether it went with four and five but
four and five or one thing one and two
or another thing and three either goes
up or down and I'm not sure which but we
were able to identify two separate
elements that were constructs that forms
this thing that Google found told them
whether a team was going to be
successful so I'm going to move on and
talk a bit about continuous delivery
what we found is that continuous
delivery impacts IT performance and this
is the math version so there's a couple
of things that we look at when we do the
math one is the beta and beta tells you
about the strength of the effect and
then the other thing which is probably
the most important thing is R squared R
squared tells you what proportion of the
variance in that measure is caused by
the things that point to it so what this
tell us tells us is that twenty-eight
point eight percent of the variance in
IT performances explains by continuous
delivery
6.2 percent of the variants in IT
performance oh sorry no that that's a
betta not an r-squared okay let's look
at this one
14% of the variation in organizational
performance is explained by both IT
performance and organizational culture
and all these things were statistically
significant so what we found by looking
at the survey data is a we had a way to
talk about continuous delivery what is
continuous delivery well it's these
things version control these are test
automation the use of deployment
automation and the use of continuous
integration all these things were
statistically significant and these
things together we called continuous
delivery and we show they impact change
fail rate IT performance and also
burnout so what we found is that teams
practicing delivery had lower levels of
burnout and again using psychometrics we
can measure a burnout again using
previous models that have been studied
and found to be valid also reduces
deployment pain very very significantly
it explains 46 percent of the variance
in deployment pain and this is the paper
that you can download where we publish
this in 2016 this year we extended this
model and we added some new things in
and these are all the things that we
found to be valid we also added
effective taste test data management to
our definition continuous delivery also
trunk based development so trunk based
development means you don't use long
live feature branches you use feature
branches that's fine but you're merging
them into trunk on a daily basis or more
often I've been banging on but this is
still the most controversial thing that
I speak about I've been speaking about
this for 15 years about trunk based
development and not using long live
feature branches and then get came along
and get flow came along and everyone's
like you should do get flow and like
never has something been more poorly
names than gets flow you know it's great
for developer flow in terms of flow from
version control to production it's the
worst thing you can possibly do because
guess what we don't actually
to optimize for the rate at which a
developer can declare something they've
complete on a feature branch what we
actually want to optimize for is the
rate at which we can get changes from
version control to production so
toronto-based development I've been
saying for years is an important thing
the data shows that this is in fact true
this is why I love surveys there's such
a rich source of confirmation bias and
then we looked at security as well
incorporating security and security
Susan's delivery process also impacts
icing performance and organizational
performance we found those a predictive
link between CD and culture and then
this is the thing that moved with the
Google culture stuff identifying with
the organizations you work for we also
wanted to find a way to measure quality
and of course quality is very hard to
measure because quality is subjective so
we were looking for some proxies that we
could use to measure quality the one
that we found actually worked and gave
us statistically reliable and valid
measure was the amount of rework that
got done and what we found is that
because you know slivery reduces the
amount of rework that gets done and
these are the full results what we find
is that high performers spend 29% more
of their time on new work than low
performers and 22% less time on
unpacking and plan but and rework so
high performers spend less of their time
reworking stuff because they have faster
feedback loops and hence they can spend
more time on building new features and
this tells us what we kind of
intuitively know that if we spend time
on things like testing automation
deployment automation refactoring
producing higher quality code bases that
means we can move faster and this is the
argument we give to managers you know if
you just let us do this we'll be able to
move faster oh we don't have time to do
that because we've got to deliver the
features well there's an obvious fallacy
there right which is the reason it's
taking us so long to deliver the
features is because you have a shitty
codebase and no automation but it's hard
to make that argument and so here are
some numbers that you can use to
actually make that argument using
science however
some surprises if there were no
surprises we wouldn't be doing science
we wanted to look at effective test
practices so these were some questions
that we wanted to see if these things
impacted IT performance which of these
things do you think doesn't impact IT
performance or negatively impacts IT
performance have a guess
outsourcing all of them so it turns out
that the things in red negatively
impacts or don't impact IT performance
this is a bit of a surprise developers
creating on-demand test environments and
again we don't know why the statistics
doesn't tell us why it just tells us no
these things are kind of I mean
outsourced parties it should be obvious
we have I work with a large retail
company and where they got an outsourced
party to build unit tests and they said
we want 100% coverage and they got a
hundred percent coverage they were like
yes fabulous now you looked at the unit
tests and the unit tests all had a
search through hundreds ten test
coverage well done this is interesting
for me
QA primarily create and maintain
acceptance tests again all this stuff is
predictive so we have hypothesis behind
this my hypothesis that I was testing
here is that when developers build and
maintain the acceptance tests it exerts
a force on them that makes them develop
code that's easier to test I think
having the developers create and
maintain the acceptance tests working
with testers is actually very important
because otherwise there's no feedback
loop acting on the developers to make
them write more testable codes if you
try and add acceptance tests to software
that was not designed in a test-driven
way it those acceptance tests are
typically very painful and expensive to
maintain if the developers are
responsible for building and maintaining
the tests that exerts a force on them to
write more testable code which is what
makes the acceptance test easier to
maintain so here's some science which
provides some backing for the idea the
tester and development is important as a
design tool to help you write more
testable code which in turn makes the
tests cheaper and easier to maintain so
a change management again we kind of
talked about this what we found is that
we thought that this would actually
we thought we've been quite clever here
only high-risk changes so as database
such as database changes require
approval we found that actually doesn't
correlate with IT performance at all
so I want to talk a bit about the
management results that we found so we
all know that managing working process
is important right working process
limits imposing working process limits
that's going to create high throughput
this is you know well-known facts about
about you know the lean Canon and Kanban
and so forth what we found is that
that's actually not important at all in
terms of correlation between management
practices and IT performance having work
in process limits has no correlation
with IT performance that's a huge
surprise this is something that should
obviously be true what's going on here
what we found is that working process is
important but only when you combine it
with other practices the model that we
ended up actually that actually produced
statistically valid results is this one
so this is called a structured equation
model which is a statistical technique
that you can use to create you know bags
that show prediction work in process
limits together with these other things
oops
sorry I've given you the wrong diagram
okay I'm going to be very agile about
this
okay this is the actual diagram never do
something after a gin on a flight so
effective work in process limits that
dry process improvement predict IC
performance in combination with the use
of visual displays to monitor quality
quality productivity in work in process
and the use of application performance
and infrastructure monitoring tools to
make business decisions these three
things together which we're calling for
the sake of argument need management you
know it's short and snappy those three
things together predicts high
performance low levels of burnout and
improved culture but these things on
their own do not predict that so you
can't just add whip limits and expect
higher IT performance you also have to
use visual displays and you also have to
use data from your tools
I have feedback loops from what your
tools are telling you and the decisions
you're making so that was a really
interesting thing that we weren't
expecting
this year we also wanted to look at
product management so this is what we
came out with this year in terms of some
factors that are important in product
management in lean context gathering
broadcasting and implementing customer
feedback and spacing work into small
batches and making the flow of work
visible through the delivery process
both things together
impacts IT performance lower levels of
deployment pain generative performance
oriented culture and also identifying
strongly with the organization you work
with which was one of the parts of
Google's measure of effective teams and
as we saw earlier IT performance impacts
organizational performance as does
westerns culture so that's a snapshot of
some of the results from the last four
years of the surveys we've done they
have really big studies and we've had
over 20,000 responses to our surveys
that's about 100x what you need to
demonstrate statistical significance a
lot of statistical studies are based on
very small sample sizes about the
smallest you can possibly use and get
away with in terms of statistical
significance we have an enormous data
set very very large so we have a lot of
confidence in our in our results we've
published for state of devil's reports
that you can go and download and we have
two peer review papers published based
on these results the conclusions number
one even if you think it's obvious test
with data managing web limits the fact
that didn't correlate with IT
performance that was a huge surprise for
us so actually testing things with real
data very very important if the results
been surprise you you're doing it wrong
it's just confirmation bias
however if everything you thought was
wrong you're probably also doing it
wrong some of the things that you expect
should indeed prove to be true unless
you know you're highly deluded but hey
we all work in computer science the most
important thing from these results that
I took away is that throughput
instability is not a zero-sum game we
can improve both throughput and
stability and the people who are doing
it right are getting better
and finally we found a statistically
valid way to measure culture and we
showed the culture and the practices
that you do on the ground have a
measurable impact not just on IT
performance but also organizational
performance so we have some time
questions and if you pick up your phone
and send an email to this email address
Jay Jay's humble at send your slides
calm with the subject DevOps that's
Jay's humble at send your slides calm
with subject DevOps free stuff I've
written these books I work for this
organization we can help survey your
team thanks very much for your time
questions
questions for jazz nobody has a question
okay I finally get to have more exercise
today please vote on your fangs and give
feedback um why do you think that
developers creating on-demand test
environments negatively impact their
performance yeah why do I think that
developers create an on-demand test
environment negligibly impacting
performance well I think there's there's
two strong candidates one is that that
on its own won't give you higher IT
performance it's probably that in
combination with something else that's
one hypothesis the other hypothesis is
just that we asked the question badly so
often you know the way that you word the
question can cause cause you to get bad
results and it's kind of surprising like
some of the questions that I thought
were quite badly worded I actually turn
out to give us valid reliable results so
you know look at the question we asked
about culture and someone pick me up on
this the other day yeah on my team
information is actively sought well
that's a horribly worded question it's
in the passive voice but it turns out
it's a reliable valid measure and it may
be that the question that we asked about
on-demand test environments actually was
badly worded or I am maybe it just on
its own doesn't do anything and you have
to combine it with other things so those
are our that those are my hypotheses as
to why that didn't work out I'm just
related to that is it possible the did
you check the scales of testing so on
demand test environments might encourage
end-to-end rather than unit testing yeah
I mean that's another hypothesis and
again we go and look at the data and
test that we actually didn't ask many
questions about integrated testing which
definitely an avenue to explore in
future surveys so yeah that's a good
question have you validated your
statistics using other methods and then
quantitative data for instance
qualitative methods by participating in
teams high-performing teams observing
them validating their practice
ya know this is a purely quantitative
study and and I think you know we kind
of fetishize data and data definitely
doesn't tell you the whole story data in
particular can't tell you why people are
doing the things that they're doing so
data is limited it gives you one
perspective it tells you whether or not
your hypotheses are true doesn't tell
you why it's happening so I think you
know qualitative studies are very
important John all sport is a talk at
craft conference which I believe is
online this year where he talks about
looking at qualitative data Dave Snowden
has a whole sense-making system which is
based on qualitative narrative data I
think that's very important we didn't
look at it at all this is a purely
quantitative study okay so you already
mentioned confirmation bias and I was
wondering are there any other advisers
or logical fallacies that you see a lot
in the software development world yeah I
mean there's a there's a huge range of
biases and some biases you can detect
using statistical analysis the the main
bias that we're somewhat concerned about
is sample bias just because the kind of
people who answer these surveys the kind
of people we can reach the typically
more high-performing so we are bit
worried about sample bias in as much as
we think the population of low
performers in the real world we guess is
much bigger than the proportion of root
of low performers that we found in the
sample data however that doesn't mean
the results are invalid because we're
still able to break the population the
cohort who wants the survey into those
populations it's just that we think the
relative sizes of the Martin
representative but we do think that the
the characteristics of them are in fact
valid so you know biases are a thing we
can correct for many of them
confirmation bias to be correct for
basically by making sure you know it's
kind of joking about that but but you
know it's confirmation bias is a real
thing the way we correct for
confirmation bias is you know the theory
based design which
that we've come up with the hypotheses
before we gather the data what you can't
do is like get the data and then you
know mind the data and look for things
which look interesting in terms of
correlations and say well that's the
thing like that would be invalid so you
can't look at interesting correlations
and then announce them as results the
demonstrate prediction what you have to
do is come up with a hypotheses first
and then gather the data otherwise it's
not science so that's what kind of
corrects for confirmation bias to some
extent yeah that's the limit of my
knowledge it is a very interesting
question which biases we do correct for
or detect using the statistics would be
a question finical Thanks hi are there
any indicators about environments where
continuous delivery is delivering to
internal customers versus external
customers great question we didn't look
at that so you mentioned the drunk based
thing is the one that causes most
controversy yeah and what do you have to
say to someone who's done drunk based
and now does something else and thinks
it's proven much better um is it isn't
it I mean the statistics don't tell the
complete stories an average of all the
people you asked but they're still
impossible to be a unique snowflake
which has you know other yeah I mean you
know everything I'm presenting is a
model and models are necessarily
necessarily limited models will only
give you one perspective models can't
tell you everything they do allow you to
make predictions but those predictions
never have 100% certainty predictions
always come with an error bar if you
like so yeah I mean in real life
everyone is different we're all facing
complex adaptive systems there are
situations in which
you know the this advice will be wrong
for you and that's why you know we're
all human beings who are you know
trained professionals by using our
experience you know I'm not going to say
that you should mechanic mechanistically
follow the things we say here absolutely
not what I can tell you is that you know
in aggregate this is true based on the
limitations in the model real life as we
know does not always follow the model so
yeah I mean I absolutely accept their
situations in which this advice may not
apply and you know the only way to find
out is to try different things and
experiment and find out what works best
but you should do that based on setting
out Europe measurable objective first
and then coming up with a hypothesis and
then testing that hypothesis and that I
think is really a big part of what's
missing in the way that we practice
software delivery today is doing it in
that kind of scientific way people have
retrospectives they like we should try
this and they try and they never come
back and find out if it was better and
they never define in advance what better
would look like in measurable terms and
I think a lot of what we should be doing
in software development is doing this in
the small so every time we try an
improvement let's define what we're
going to measure to find out if that
improvement was the right things that
have done and let's come back later and
see if that actually made things better
and another thing this is useful for is
as a source of things to measure to see
what good looks like so you know you can
use this ID performance metric I think
it's very valuable for exactly that
purpose do you have any information
about using DevOps practices in high
regulatory like HIPAA environments or
high business risk PCI DSS environments
yeah absolutely if I did a talk on this
yesterday where we're doing exactly that
so one of my fat my day job is working
for the u.s. government where and I'm
going to borrow from my talk yesterday
it's it's the most highly regulated
environment I've ever been in I actually
went to work for the government because
I thought you know if we can make it
work here we can make
make it work anywhere boy did I get what
I asked for
you know this is the complete list of
controls that have to be true for any
federally approved government
information system there are and that
those are just the high-level ones that
each of those controls is beta split up
into sub controls so you have to
implement hundreds of controls before
you can put a system live in the US
federal government these techniques
absolutely apply in that environment and
they are perhaps more important in a
regulated environment because that's
where we actually care about creating
more resilient systems so yes you can
apply these my co-author on my
continuous delivery book Dave Farley
worked at L max which was at the time
the highest throughput financial
exchange in the world based in London
they implemented deployment pipelines
and the auditors loved it because
instead of a piece of paper you know
when we actually in the federal
government and other places when you do
this what you end up with is a big stack
of paper that says that this is what
you're doing does that bare relation to
what you're actually doing who knows
if you have a deployment pipeline that
says exactly which changes ran had which
tests run against them where they were
deployed to what scripts to run as a
result of that deployment what the
output was that's fabulous for auditors
it's really useful information so
absolutely these techniques apply in
regulated environments they are arguably
even more important in that context so
please remember to vote thank you jazz
it's wonderful science
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>