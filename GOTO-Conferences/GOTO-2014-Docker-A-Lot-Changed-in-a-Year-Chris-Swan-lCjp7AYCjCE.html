<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • Docker - A Lot Changed in a Year • Chris Swan | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • Docker - A Lot Changed in a Year • Chris Swan - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • Docker - A Lot Changed in a Year • Chris Swan</b></h2><h5 class="post__date">2015-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lCjp7AYCjCE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm Chris Ron I'm CTO at a cloud
networking company called cohesive ft
i'm also a cloud editor at info q where
i've been writing a little bit about
docker so this title of the talk seemed
like a good idea in March when I was
asked to come here because docker was a
year old then and it seemed that a lot
had changed in a year and eight months
on things have changed some more and
that's what I'm going to be talking
about so this sort of too long didn't
read version of this the summary of what
I'm going to talk about is docker yeah
one the DevOps truck here docker I think
is important for DevOps because it
tightens iteration loops I think
tightening iteration loops is a kind of
key objective for moving faster and
delivering quicker the core project
developed really fast and I think back
in March when we were talking about what
had changed in a year the core project
that just about reached a sufficient
level of maturity that people were
starting to build things around it but
what's changed in the last eight months
is I think we can see the docker
ecosystem so things that people are
building a round docker are now moving
even faster than the core project itself
and that means that there's a whole ton
of great stuff arriving that we can do
useful things with and and then if we
ask ourselves the question how did all
of this amazing stuff get delivered in a
year and eight months I think the answer
turns out to be they did it with docker
so because docker is allowing us as
developers and engineers to tighten our
own iteration loops it's also tightened
the iteration loops for the people
making docker and people making things
with docker in the dock or ecosystem and
so it's kind of it's become evidence of
its own success that docker changed a
lot in a year because docker is
effective at changing things so some of
you probably asking why me so this guy
who does cloud networking and writes a
little bit about docker well you know
where does my own experience of this
stuff come
from so going back a bit I used to work
in financial services industry and one
of the things I did in the financial
services industry was I built grids and
grids were kind of yeah the edge of
scale at that time so if you were
building something with a few thousand
nodes then that was pretty large scale
and if you're building something with
tens of thousands of nodes then that was
as big as stuff got apart from google
and i think grids to a certain extent
evolved into what we now call cloud
computing and yeah i started using cloud
computing really as soon as it emerged
on the scene was one of those weird
people who had an amazon web services
account before there was Elastic Compute
cloud because there was actually a bunch
of web services before those the web
services we think of today and cohesive
ft was born around about that time as
well so a cloud native company initially
delivering virtual machine management
and then the networking stuff came about
because as soon as people could move
things from one cloudy environment to
another then you needed some way of
connecting that stuff together and
connecting it back to enterprise systems
of record and that kind of thing and
that's a journey I've been personally
involved with for a little over a year
and a half now which is roughly round
about the time that docker came out so
as docker was coming out I was starting
my new gig and last summer we decided to
put docker into our product so we'd had
customers kind of coming to us and
saying I've got all of these things that
I'm running in itty-bitty VMs on the
edge of my application can you please
just put the functionality into your
core product and we were faced with this
sort of exploding test matrix of doom
because it was okay for each individual
customer I think that their request was
reasonable to put engine X or H a proxy
or varnish or whatever it was that they
wanted into our core product but if we
did that for every customer then we just
wouldn't be able to manage the
complexity and so what we did was this
we put a docker subsystem into our
product and that lets people take what
they would have
otherwise run as a standalone vm and
instead container eyes it and they can
have their SSL termination or their load
balancing or the content caching or
their network intrusion detection as
things running in containers rather than
as things running as standalone VMs and
that cuts down on cost and management
complexity and security surface area so
it's it's a it's a big win all around
now when you end up incorporating
somebody else's fast-moving open source
project into your own product you get to
know it very well and especially I've
got to know more than I ever wished to
about docker networking in particular
and Linux networking underneath it in
general and that's a completely
different talk so I won't be talking
today about docker networking so this is
a DevOps track why on DevOps do we care
about docker well I already said in the
summary i think it's about moving faster
and who in the rooms already heard of
John Boyd's doodle-loop yeah Adrian
cockcroft talks about this but there's
there was only a smattering of hands
there so i kinda I'll go over it for
those who haven't so John Boyd was a US
Air Force pilot and he saw active
frontline duty in the Korean War so he
was flying savers the things on the top
left and him and his comrades were
getting their asses pretty much kicked
by the MIG 17s being flown by the north
korean pilots which they got from the
chinese because the mig-17 was more
agile and when he subsequently went on
to become a sort of Top Gun type
instructor within the Air Force he came
up with this thing that's now known as
his odor loop an odor is observe orient
decide act and he said when you're a
fighter pilot engaged in a dogfight
you're going through these processes of
observe where your enemy is orient
yourself to them to orient your aircraft
to him decide what you're going to do
I'm going to go in there and engage with
guns or am I going to fire a missile and
try and take them out from a long
distance
and then act upon that and the pilot
that wins the dogfight is the one that
can go around that loop the quickest
that can iterate fastest and I think
when we come to think about software we
see a very similar sort of thing going
on in software so yeah we we build
software and then we need to test
software and when we test the software
we find that there's bugs and we need to
go back and change things and we're
going round and round these loops and
organizations that sort of win in the
competitive environments are the ones
that can move the fastest middle rate
the fastest so this has been kind of the
heart of the netflix story they decided
to become a company that was designed
for speed and adapted adaptability to
their marketplace not a company that was
designed for stability and as a result
they've moved faster and they've been
able to be very successful and I think
from an IT perspective we now learn a
lot from netflix this aircraft on the
top right the f-16 john-boy became known
as the father of the f-16 so what he
said was yes since we need to go around
these loops faster we need to be more
agile and one of the things that's
distinguishing about the f16 is it's an
inherently unstable aircraft so the
aerodynamics of it means that it's not
very good at flying in straight lines
but it is very good at turning and so
because it's inherently unstable it can
turn very fast which makes it agile
which means that it can win dog sight
the problem with not being able to fly
in straight lines itself for us by
computers so the f16 was one of the
first fully fly-by-wire aircraft and
it's the computer that keeps everything
stable by making tiny adjustments to the
control surfaces of the aircraft as it's
flying through the air so that in
regular flight it can appear to be
stable but as soon as the pilot pushes
the joystick over then he's going to
break into its turn so it's these kinds
of agility that we're looking for in our
software environment and it's my
assertion that docker helps us go round
these loops
very much faster now sometimes i do a
demo at this point of docker file so
who's seen docker file and and how to
play with that pretty much everybody so
I'm going to skip past the demo but the
whole point of the demo is if I go and
change something in a docker file say a
single line of configuration here I've
done my test and something isn't quite
how I like it and I need to go and
change a single line of configuration
because docker file is caching
everything that worked before I'm just
kind of expending my effort I'm just
rubbing on the piece that was wrong i'm
rubbing away on the mistake so i'm not
doing all of the grunt work of
reinstalling everything that worked and
i think many of the mechanisms that we
had prior to dhaka meant that if i was
changing a single line in a single
config file i had to do everything else
that worked first before the world would
let me change my single line and with
docker file because of all of that
caching it's doing in the union file
system then I get to change my single
line and the only thing that I'm waiting
for to change is my single line so I can
move really really fast and I can take
things out i would have been waiting you
know some cases as much as ours for if
i'm building new VM images and I can
have those essentially instantly if I'm
building docker container images instead
it does result in a phenomenon that I've
heard described as dockerfile jenga
where certain things within a docker
file can take some time and other things
within the dockerfile are really quick
so it's kind of some of the early stage
stuff when you're doing installation
from unix repositories that can take an
age and it's kind of you want that stuff
at the top of your docker file because
once you've figured out what your
dependencies are you probably going to
leave that alone and then later on in
the dockerfile you get to the nub of the
software that you're working on and you
just want that bit to be the bit that
you're changing because if you change
something too early in the process then
everything after it has to be changed as
well and so it's like when you've got a
jenga game and you push something too
low in the the whole pile of wood and it
all falls over
alright so let's have a look at docker
itself and I'm going to go back to the
beginning so the first release of docker
was described as point 10 it came out in
March last year 23rd of March and here
was a platform that was based on elexi
Linux containers I'm right from the very
beginning there was a registry therefore
that the system could push and pull
images and I'll return to why I think
that that's so important it in its of
latest stage in the talk but that was
there from the very beginning of the
release of docker so title of the talk
is what changed within a year so let's
look back to where we were a year ago so
actually there was a release a year and
a day ago who wants me to read out all
of these different change notes nobody
yeah it's an eye chart and I've got no
intention of going through it but you
can go into the github blog and you can
see what was there and this was not
really an epic release this was a whole
bunch of trivial stuff that people were
fixing up and at that point we still
didn't have a whole bunch of things that
I think have come along later and are
actually really important so we didn't
have a regular release cycle so docker
was changing in sort of fits and starts
and kind of nobody knew when new stuff
was coming or could be expected at that
point you really did have to kind of
bend things out of shape with your linux
distributions in order to install docker
so you had to get exactly the right
kernel release and that involved a
little bit of poking and fiddling in
most cases there was no linking between
containers so there was no kind of
metadata mechanisms by which you could
describe a relationship of one container
to another and so really you were kind
of on your own building things in single
containers and figuring out how to get
them to go together and kind of
associated with that there was no
concept of naming a great deal of those
things came in the point 7 release
which was the end of november last year
so as of just a little under a year ago
we started to get these things I think
many of these things if you're using
docket today you've kind of already
taken those for granted so the watershed
release I think point 9 was the release
that kind of made docker what it is
today and one of the major changes that
came in point 9 was lip container and it
was at this point that lxiii got through
under the bus so the docker had been a
wrapper for Alexei and the docker
project decided that actually they would
rather do the things that Alexei does
themselves and so lib container the
default mechanism now for daca is it's
not configuring control groups and
namespaces and capabilities and
mandatory access control and all of
those things through lxiii because
actually some of those things can't be
done through LXE such as mac and instead
it's doing it itself using lip container
which is a native golang library the
other thing that's kind of important
about lip container is it's essentially
a pluggable architecture so lib
container can drive lxiii and it can
drive to route and it can drive openvz
so you can take all of these different
approaches to containment and have a
unified interface to all of those things
but you're getting mixed results so I in
true is going to give you nothing like
the containment capabilities of native
lip container itself but what it does
mean is that there's now the opportunity
to have higher strength container
mechanisms which can be driven by the
same API and the same tool chain and so
VMware's been working on that with their
project Fargo we saw Ubuntu announced
last week LXD which they're describing
as a hypervisor for containers and
that's causing quite a few sort of stirs
within the industry of so what exactly
is all this stuff going to which even
ubuntu forking the industry and nobody
seems to quite know what's going on
there there was a patch 2.9 a couple of
weeks later so point nine one fixed a
bunch of bugs and point nine one is the
docker version you will get in most of
the distros at the moment so a bun to
trusty tahr here got point nine one
Center seven and quite possibly even rel
seven are going to get point nine one as
well this is unfortunately I think a bit
of a mess that yeah we've got to dhaka
10 we've got to dhaka in fact 1.3 we're
at now and if you just go into regular
linux distro and up get installed their
thing that gets you docker and it's not
called docker because there was a
project called docker before there was
docker and so it all starts to get a
little bit nasty and confusing i think
in a bun to its called doc I oh and they
don't even call themselves doc Oh doc I
owe anymore they've gone to Dhaka calm
and if you do that with a bun to then
the command line is called docket do
rather than docket and so that's not
good if you're installing docker I'd
suggest not actually using a native
package library from a distro just go to
the docker home page and see what the
district specific instructions for
getting the latest version are because
you probably don't want to be on point
nine one which was sort of a March
release from this year of a project
that's moving quite so fast as it is so
and then came Dokic on so June broadest
acha kaun and docker con brought us
docker wonder oh so weekend before dhaka
con they done one of their regular
monthly releases which had been point 12
and you know first thing morning of
dhaka con this appeared on github and
it's kind of nothing changed so point 12
to wonder Oh production support so
nothing changing the code base they
anointed that is the one that they were
willing to look after in a production
supportable environment there was a
bunch of other kind of super important
stuff came along at Daka con I'm so lib
container itself wasn't a new thing we'd
had lib container since March but there
was a whole bunch of announcements about
partnerships based upon lib container as
yeah the industry was kind of saying
okay we're going to do more interesting
things with lip container or we're going
to make lib container part of our own
services what was new was lips warm so
lib swarm is a set of api's for
orchestration I'm going to talk a little
bit more about orchestration towards the
end of the presentation but one of the
the kind of problems that was starting
to emerge was everybody was doing their
own orchestration and yeah in principle
that meant that we were going to face a
situation where it was just going to
become an integration nightmare and I
think it was quite precent of the docker
team to get in front of this and say
let's have an orchestration API that
everybody can interface into and at
least then we've got a single point of
convergence for orchestration the other
sort of big announcement from Solomon
hikes on day two was lips warm celebs
form he described this as kind of the
networking thing for daca and there's a
whole bunch of interesting things
happened subsequently with networking
such as we've and the past week there's
quite an interesting sdn proposal built
around open V switch appeared in github
but lib Chan is it's in my view not
actually about networking it's about
communication between different
processes so it's taking the go channel
mechanism and it's making it work across
the network and I think this is fairly
and squarely aimed at making the
internals of docker work across a
distributed environment and so what we
can expect to see grow from that is
rather than docker being something that
we use on a single host to control
containers on that single host it's
something that will be able to use a
cross a group of district distributed
hosts and so this becomes a sort of
underlay channel mechanism and we can
expect that to be used and distributed
things that aren't necessarily being
driven by a higher level orchestration
so in terms of the docker core what's
happened since wonder oh well not a
great deal actually and I think that's a
good thing so yeah it got declared as
stable and stable things shouldn't
change a lot so in terms of incremental
functionality we've seen really these
two new commands come along so exec and
create exec addresses the issue of
basically container maintenance so
there's this sort of religious debate
been going on of should containers just
be a single process really the purpose
of a container in life is to run a
single process or should a container be
like a mini vm and have an ssh daemon
and you can log into it remotely and if
you kind of went with the religion and
said it's a single process then you had
a problem because if your single process
wasn't attached to a terminal then how
did you communicate with your container
after you'd made it and if you bought
into the whole immutable infrastructure
thing and it's kind of you built
special-purpose containers and then you
threw them away when you needed a new
one that was fine but lots of people
aren't doing immutable infrastructure
and need to kind of go back in and do
maintenance activities so there was a
project called NS enter a namespace
enter which came along and allowed you
to get a terminal inside of your
container once it was up and running and
serving its purpose and running its
process and docker exec from the command
line is basically taking that same
capability that NS enter gave us and
making it part of the official toolchain
docker create is a little bit more
mundane so docker create is basically
saying I can make a container without
running it straight away so when we did
doc a run we would create a container
and run it and some people said well I
want to create a container but I don't
want to run it I just I just want to
make it be there and I'm going to come
along and run it or do something else
with it later and that's what create is
therefore so moving on to the ecosystem
and kind of other things that have been
happening around dhaka i kind of
summarize this as shocker Flocka clocker
and what else can you think of that ends
with acha that you're going to name your
project or product after so shocker this
isn't a great example of the ecosystem
but it's really important this came out
in a matter of days after the Wonder Oh
release and it was a proof of concept
attack against the containers so it was
basically showing that you could break
out of the containment mechanism and the
response to the shocker is very
important so Solomon hikes who I guess
is our benevolent dictator of docker
came along and said when we feel
confident for you to run untrusted root
programs inside of your containers will
let you know so my opinion is that that
day will never come or if Solomon does
say that we can do that he will be
proven wrong shortly afterwards but yeah
shocker was a good example of you know
the community kind of working on at
least evidencing that where we are with
containers and security you know i'll
come back to security stuff flaca so
Flocker is about state management and
developed by the cluster HQ guys they're
really pushing hard on ZFS on linux and
building something that can give you a
synchronizer ball state between
containers and i think this is possibly
one of the more important projects
that's going on because if we look at
how people are using containers a lot of
it is kind of third-generation
application architectures 12 factor and
all of that and i love all of that stuff
but it's kind of assumes magic state
management in the sky and
so state is still a real problem and I
think dealing with state using something
like Flocka is potentially a real
solution for that and then clocker so
the the guys from cloud self to have
been doing the Apache Brooklyn project
and stuff like that figured out that you
could take what they've been doing which
is all about packaging builds of things
and you could apply that docker
containers as well and so they called
that clocker and actually I think
clocker is just an example of
composition so clocker comes from a
heritage of composition that existed
before we had docker but docker has come
up with all of its own composition
frameworks and toolkits so first and
foremost amongst knows is fig so fig was
developed by a startup called orchard
labs orchard were one of the first bunch
of guys to kind of produce a docker
service somewhere where you could just
show up and run containers but as a kind
of value-added think that the community
they develop this thing called fig which
was a way of taking multiple containers
that formed an application and
describing in a single yamel file how
the containers interrelated so without
fig to get a simple three-tier
application running I'd have to have
multiple single docker run commands that
would start individually each of the
containers and contain the associated
metadata to describe names and links and
what fig does is it takes all of that
metadata encapsulate sit in one thing
and then I can just have that yamel file
and say fig up and it's going to launch
all of my containers for me so it kind
of greatly simplifies some of the
complexities of multi container
deployments so I figure were not the
only people doing this so red hat I've
got their thing they call that gear D
and that's become part of your red hats
project atomic and and some of the other
Red Hat initiatives going on around doc
and then there's Panamax so panamax came
out of CenturyLink labs they describe it
as docker for humans you could almost be
sarcastic and say it's docker for
dummies and so Panamax is a visual
compositing tool driven out of a web UI
so I'm actually showing a screenshot of
the the Clyde that you get with panamax
but the clydes really only used for sort
of lifecycle management of the Panamax
environment and once you've started that
up you've got a web interface where you
can essentially generate from the web
these yamel files that you might use in
something that looked a bit more like
fig but it's also got our repository so
you can take other people's yamel files
for multi container deployments you can
start manipulating those to yourself and
then you can contribute them back to the
community and they had quite an
interesting competition associated with
the launch of that trying to get the
community to build templates and
offering map no not Mac Mac Pro's you
know the cool dust bini super powerful
pcs as prizes for that so has court
docker so we ended up with docker based
passes and so you got ya day is the
doctor host and Doku and Flynn and so
people were kind of taking things that
had been done with paws elsewhere and
doing them with docker but I think more
importantly we ended up with docker
inside of the mainstream passes so
pretty much all of the the big past
projects now have got some kind of
docker involvement so Collins not in the
room I don't think but Colin Humphreys
did decker and that was one of the first
sort of proof of concepts really of
taking docker containers running them
inside of a pass but subsequently app
Sarah search Derek Collison's company
where he'd been involved in founding
cloud foundry and then said ok I'm going
to do essentially cloud foundry 20 but
I'm going to
a separate thing so absolutely a
continuum has got docker container
import capabilities in it and then Along
Came Diego which is basically the
official mechanism of putting docker
containers inside of cloud foundry so
you know pars and docker are now kind of
working together and all happy and I
think if pass has a problem has has a
problem with developers because its
opinionated so pads pretty often makes
people have less choice about their
languages and less choice about their
frameworks because they're kind of being
inflicted by the pass and I think docker
might just be the anti pass so docker
doesn't inflict any choices upon you
you've got a completely open book of
choice you can just essentially with
docker file go back to entering in stuff
on the command line just like you would
do manually which means you can run
anything and you can have any framework
and so if we think about taking
opinionation' away from Paz if we're no
longer making pars be about a language
or a framework then what is up has about
well the residual pieces it's about
operationalization it's about
configuration management it's about
control and monitoring of your
environment it's about the ability to
achieve scale without having to jump
through a whole bunch of Hoops so I
think this is where orchestration comes
in because when we start looking at
orchestration frameworks they are
basically giving us those operational
pieces that we get from Paz but without
inflicting choice of language and
framework and being opinionated so this
was the news from doc akan it's kind of
everybody has their own orchestration
framework and I love this tree it's kind
of what I lent today doc on everyone is
building their own orchestration
platform seriously everyone and yeah we
had
those guys showing off me Zeus and also
mesosphere which is a different thing
but based upon measles we had facebook
talking about Tupperware so tupperwares
the container management platform that
they have inside of facebook facebook
hasn't moved to docker containers yet
but they have been running containers
across their entire infrastructure for
some time and then coo benetti's so kuba
Nettie's has been sort of born out of
google's use of docker the whole linux
containers story started with google so
if you go back to the sea groups Colonel
changes that originated out of Google
because Google wanted to run containers
within their own data center they've now
got this awesome platform called Omega
which they use for scheduling and
orchestration across their containers
and COO benetti's is basically a golang
baby brother of omega and there's a
whole bunch of stuff that they've
already done with omega that they've not
baked into coo benetti's but they have
gifted Cooper Nettie's to the world and
so I think coo benetti's is becoming the
one to watch because it's already been
adopted by Microsoft so you can get a
coup benetti's based service off of Asia
it's been adopted by IBM in their
softlayer cloud so there's communities
based services there as well and then as
of just this week we have google
container engine so there's an alpha
product being launched by google based
upon KU benetti's allowing you to get
orchestrated containers on demand on
their cloud and that's kind of cool but
I will say that you know caution this is
not yet the containerized cloud so I am
expecting something to happen where
we'll be able to go along to services
and get containers on demand and it will
move the decimal on price and because
containers can move the decimal on so
many other aspects but this is not it
yet so if we look at the pricing of
google cloud engine then it's still
based upon VMs under the hood and
they're still billing you for VMS under
the hood and so nothing fundamentally
has changed so
they've just put the cube Nettie's
orchestration in there so why does all
of this matter well I think it matters
because in the end build and ship are
more important than run so people say to
me our talks boring containers been
around for decades what about zones what
about L pars right yeah sure containers
have been around for ages and you know
lxiii came along on Linux and I think
actually still then it was sort of
boring and what docker is brought to us
his ability to have a unified build
environment for containers and a way of
moving those containers around whereby
you know we can do that within our own
organizations and we can share them with
the rest of the world and that's much
more important than containers
themselves containers themselves just
become the runtime substrate for that so
docker hub is now this kind of public
resource that we've got we're not only
can i upload my containers or upload my
dakka files to make containers out of
but anybody else can come along and
share from that as we've had with source
code for a long time now with github and
what this essentially gives us his
bottled software so I love beer I love
brewing beer but I when I booby area it
takes months for my beard to be ready so
if I want to drink a beer then I would
rather go and get a bottle of beer
because I can drink it right away and
this one fluorite clini the elder that
is the best so you can get hold of that
stuff I highly recommend it so what this
means now is if I'm starting to use a
new tool I don't have to do the thing
that I did before of installing that new
tool and figuring out how that new tool
integrates with a whole bunch of other
tools that I might be using around an
ecosystem because it's almost certainly
the case that somebody else has already
done this and it's increasingly become
the case that not only have they done it
but they've made a docker image out of
it or they've made a docker file to
build one of those images and I can just
go
get that out of docker hub and so
instead of spending hours figuring out a
new tool and installing it and
integrating it I can just get somebody
else's bottled tool that I'm off to the
races I'm immediately productive so
that's saving a whole ton of time that
has happened for real for me personally
with the azure toolkit and I recently
spent far too many hours of my life
wrangling the wrt node SDK but if you
want to do development and wrt node you
can just go and get my version of the
SDK from docker hub because you can get
that in a matter of minutes now there's
still a whole bunch of work to be done
yep so docker hub already has stars and
docker docker hub already gives you how
many times an image has been downloaded
so you've got that kind of
coarse-grained reputation system in
there what they're working on now is
provenance so where did stuff come from
and can you trust it and org is it even
reasonably dependable and there's also
already problems with bad actors in
there so there's this random guy in
Canada called pill one who decided to
register the name docker file in both
github and docker hub and that looks to
official and lots of people think that
it is official including the Amazon Web
Services guys who originally used dakka
dakka file / engine X as their example
for a docker an elastic beanstalk so
yeah this stuff needs to be cleared up
right problems containers don't contain
so I'm not going to do the full security
talk dan walsh did a great talk at dhaka
con and everything he said is still i
think is relevant now as it was then so
this was the illustration you used which
i think pretty much sums it up I think
there are methods by which we will get
runtime environments that offer
containment I'm so I already touched
upon fargo and lxd i don't think
containers ever will contain so if you
look at what containers are made out of
see groups namespaces colonel
capabilities mandatory access control
for
dimensional problem with dozens to
hundreds of configuration options in
each of those dimensions I challenge
anybody to come along and tell me oh we
figured out the right ones this is the
secure settings yeah we got the knobs
and dials to exactly the right positions
and that is secure because there's just
too much variability there and so that's
why I think containers never will
contain but I think people will figure
out ways of working around that there's
also what I call the manifest problem so
if we go back to a docker file each
entry within our docker file is building
a lair in the Union filesystem and then
we take all of those layers and we bind
those together in it into an image and
that image becomes our unit of
deployment and yeah if we're just taking
something like engine X then I could say
that that's my engine X example and I
now don't know what version of engine X
is inside that I've got no idea it's
just whatever version of engine X got
apt-get installed at that point in time
and that turns out to be bad because I
don't know which version of open SSL
went into that engine X either so when
something like Hopley comes along I've
got no idea whether I need to patch it
or which bash so shell-shocked becomes a
problem for me as well and so this isn't
a docker problem it is the fact that
docker is causing a proliferation of
containers and container images which
will probably be an order of magnitude
worse than vm sprawl and this is really
a Linux package manager problem and we
have not been very good paying attention
to what package managers are putting
into deployable artifacts and I think
we're about to cross into a sort of
scale where it really will get
completely an ugly out of hand and we're
probably going to have to fundamentally
reappraise how we think about dependency
how we think about how things get into
deployables a sets and how we keep track
of what we have in production
and so you know that there are ways
where I see see some hope so there's
things like nicks which is a functional
package manager and Nick's docker which
zef hemel made around that so I said I
would talk about a docker wish list it's
kind of how we going to make it get
better so back in February I did a
presentation at q con and this was my
daugher wish list so I wanted a docker
cly I get sick of doing sudo docker blah
blah blah I just want to do sudo docker
and jump into the docker cly and then
say run and kill and do all of my
dockery stuff and Solomon said at the
time that that was super easy and good
idea and it still hasn't happened disk
quotas are a big issue so one of the
potential problems with running a whole
bunch of containers is one of the
containers can swallow up a whole ton of
disk with logs and whatnot and there's
really no way of stopping that and back
in February I was concerned about root
propagation but that was possibly
because at the time I still just didn't
understand the doctor networking well
enough so here we are in November I'm
still waiting for my daugher clay I'm
still waiting for somebody to do
something about disc workers and docker
have changed the way that they deal with
restart behavior which is making me want
to have finer control over life cycle so
i can now set flags about how i want a
container to be dealt with when it exits
but i don't find that they're actually
dealing with what what happens when the
demon is restarting so if I'm bouncing a
vm that's got docker inside of it I'm
not really having the control over the
life cycle of those containers as it
comes back up the message actually is
that if you want to have a better docker
get involved with the project and make
it better yourself because this yeah
although there's a commercial entity
around it this isn't like the old world
where we're customers and we make
feature requests and
people go off and do that you know for
sure you can throw something onto github
issues and say I wish docket did this
and that will probably be ignored
whereas if you write some code and do a
pull request somebody will do something
about that so if you want a better
docker learn go because that's what is
written in and and start submitting your
pull requests so there's a whole bunch
of other things that are happening which
it would be sort of rude of me not to
mention so window who's just going to
get docker so Windows 10 and the server
vers based version of it there's been a
partnership announced with Microsoft
Windows is getting a package manager as
well I mean it's like it's almost like
it's catching up with the modern world
so last week was the first meeting of
the docker governance board at the so
they've set up a foundation around it
and they've sort of got Solomon as
benevolent dictator but they've got a
whole bunch of people trying to keep him
on track and these are the four things
that kind of came out as the key points
of the first meeting so they want to
have a more mature tooling and
continuous integration process for the
project so it can scale a bit more they
need to deal better with long-standing
pull requests so you know figuring out
how to unclog the pipeline for those it
has been decreed that there shall be a
clear separation between Daka ink and
docker the open source project and there
was also clarification on the
relationship with Microsoft so they're
not going to have kind of special access
everything that Microsoft's doing is
going to be out in the open and visible
by all and a clear articulation of the
roadmap I think if you look at other
large-scale open source projects that
have maybe not satisfied people and
OpenStack I'm thinking of you you know
one of the things that's been lacking
there is a vision for people to work
against oh yeah what happens when you
don't have the vision is people just do
all kinds of random nonsense and I think
articulating a vision for Dockers where
it might will be a good thing
Solomon tweeted this just this morning
so they need more maintain errs and I
think he's trying to elevate people's s
team of maintainer 'he's to try and
encourage more to come along that a
maintainer is a contributor of decisions
and not just code and yeah this right
now i think is the bottleneck to the
core docker project getting better
quicker is yeah there's just so much
going on that they don't have enough
maintain is to go around so nearing the
end how did we come so far so fast well
i think we came so far so fast because
the people that have been involved with
the project they use docker so they've
benefited from the output of their own
ecosystem and so wrapping up then I'll
kind of return to my summary of earlier
docker is important for DevOps because
it lets us iterate faster it's
tightening those iteration loops the
core project has developed very fast but
the ecosystem around it is now kind of
outpacing it and yeah i think that is in
part because the core project is
satisfied a whole bunch of objectives
but there's also you had some sort of
governance and maintainability aspects
of that but whether you're looking at
the core project or whether you're
looking at the broader ecosystem it's
evolved so fast because they did it with
Daka thank you very much for listening
and we've had one question already but
I'd love to take some more anybody ok no
questions I've completely oh you've got
another one
so the question is how's docker going to
get a business model and there I last
talked to their investors about this in
June so the story may have evolved a bit
more since then so in June they
announced commercial support so they're
going to start to make some money from
that it's a drop in the ocean so they're
taking at the moment much more investor
money than they're taking revenue from
selling anything you know one of the
orchard guys who's now part of the
docker team said we want to be spending
customer money rather than investor
money so at some point that commercial
model has to come and yeah I think
they're still in the kind of business
model discovery process so yeah just
like Twitter built an awesome service
and grew it and grew and grew it and
didn't have a financial model for quite
a while they're building a company
around an awesome open source project
and they just want to grow it and grow
and grow it and see what might work I
think there's all kinds of options so
I'm support is just one of them I don't
think that that's actually going to be
enough yeah an enterprise version of
docker might be another VMware's already
taking a swing at that and I think
VMware might be ideally positioned to be
the enterprise tollgate for the run
piece of build chip and run and yeah you
can then say okay if somebody else might
take run what can docker Inc do with
build and ship so they've got docker hub
another way that they're making a little
bit of money at the moment is people
having private repositories in docker
hub that could scale over time and then
there's other stuff that I think we just
can't conceive of at the moment I kind
of expect them to eat a lot of their own
eq's ecosystem and so things like if
locker and weave and whatnot might end
up coming together into making a more
enterprise-e sort of docker offering a
lot of the the investment community that
I talked to at the moment kind of going
back a decade with where VMware was and
saying okay what made VMware acceptable
the enterprise and can we make these
same qualities happen to docker
containers
and what's it going to take to do that
and how do I build a company around that
and I think at the same time docker ink
is probably looking at exactly that same
list and thinking you know can we build
that ourselves or do we want to make
early-stage acquisitions to to deliver
that more quickly so it's it's still in
progress and yeah there's every
possibility that containers might just
turn out to be a zero billion dollar
industry and all the monies and services
so you know Google might be the ones
that make the money out of it anybody
else well thanks for listening
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>