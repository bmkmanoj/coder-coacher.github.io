<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2013 • Riak &amp; Dynamo, Five Years Later Presented • Andy Gross | Coder Coacher - Coaching Coders</title><meta content="GOTO 2013 • Riak &amp; Dynamo, Five Years Later Presented • Andy Gross - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2013 • Riak &amp; Dynamo, Five Years Later Presented • Andy Gross</b></h2><h5 class="post__date">2013-03-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AxG9DROsnqg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">no sequel search road shows a lot of big
names from no sequel all throughout this
year Munich Berlin San Francisco
Copenhagen Zurich Amsterdam London my
former boss Falco well smart guy -
whoever for me o4j and my mother
luminaire is speaking so I suggest you
check that out I'm Andy gross I'm chief
architect at vacío technologies you can
get a copy of these slides that link
down there quickly slash no sequel RV's
it's actually hosted on react so if you
download those slides my talk today is
about the dynamo paper and what's
happened in the years since it was
published how many people are familiar
with the Amazons Dynamo all right well
no need and this is actually an
interesting point because every talk
about Dynamo or any complex system for
that matter I've spent I spent the last
five years giving talks about the
database that my company produces which
is inspired by Amazon's Dynamo it's
always the same a few over and over
again and it's - information dense - to
really walk away with too much knowledge
from it so I'll give a brief overview
and mainly talk about how I'll react
work it was our clone of this system but
anyway back to dynamo dynamo was a paper
published by Amazon in 2007 so it's a
little over five years ago now
and it describes a collection of
techniques distributed systems
techniques that Amazon is to create very
high availability in low latency key
value storage system so by key value I
mean basically the API is that of a hash
table you give it a key and you get an
Associated value back Dinamo didn't
necessarily really introduce any new
concepts rather it took a bunch of
techniques that had already been in the
literature that had already been used in
different areas of computing and combine
them together for the first time and if
you haven't read the paper yet it's one
of the most accessible sort of computer
science inverters that I've read it's
really about how they built this system
it almost reads like an implementers
guide but it's not bogged down with with
unnecessary theory and I think that's
why the paper was so influential because
not only is my company's database react
inspired by dynamo but Apache Cassandra
Voldemort from LinkedIn and several
other lesser known databases are
inspired by this paper along with a
couple other papers that came out at
roughly the same time and this is the
mid 2000s the Google BigTable paper and
the Google file system paper and other
two papers that sort of spawned this no
sequel movement this this you know so
big collection explosion of alternative
databases that we've seen in the last
five or six years one interesting fact
that I didn't know and if you're a fan
of reading these research papers you'll
notice Amazon doesn't publish a lot but
the author's nearly got fired for
publishing this because it was giving
away in the opinion of their CEO too
much of their secret
sauce and I don't think they published
interesting papers since so how many
afraid of really alright so a little
mark that's cool actually usually it's
the other way around more people so
react is a dynamo clone there's this
legend that I wrote it on the plane on
the way to my job interview which is
only after I wrote something on the
plane and it didn't work at my interview
but uh-huh I got a kiss I got an A for
effort because they hired me anyway
we called the code that I write - okay
okay so I mostly talk nowadays um mostly
Erlang eager Lang fans in the house
that's fine
okay but it's mostly Erlang and I'll get
into some of the reasons why we chose
our lab to write the database it's sort
of an odd language choice but with a
little bit of C and C++ to interface
with the very lowest levels the stuff
that touches the file system
database drivers its Apache 2 license
it's been around for a while as I say
so the first started on it in 2007 but
we first released it in 2009 and we just
released version 1.3 a few weeks ago and
nowadays it's had you know we have a
commercial entity around it and we have
some pretty big customers of the fortune
50 companies use it chances are if
you're active on the web or on your
smartphone you're you're using it in
some capacity it's a little bit about my
company was founded late 2007 the
original people all worked at Akamai the
content delivery CDN company and now we
have about a hundred and twenty
employees with offices all over the
world and the sponsors of the open
source Apache 2 questions project and we
also sell the enterprise which is react
open-source plus the ability to
replicate among many data centers not
just to the customers
master master master bus rather
five six days and we also so clone of
Amazon s3 that's where the slides and
that little bit Lee link are stored but
if you can buy it as a product running
behind your own firewalls for cases
where Amazon is too expensive or you
have security needs or latency needs
that it was on king satisfied so the
principles in the Dynamo paper were to
have a store that it's a data store
that's always writable and this is sort
of a business need that that didn't
exist for a lot of companies until the
mid 2000s before them their needs were
largely handled by relational databases
but leading up to the Amazon publishing
Dynamo implementing Dynamo there were a
few very high-profile amazon.com outages
in the holiday season that were all
traceable back to some failure of a
relational database or a goal or Mike
sequel or whatever and the example of
what they were storing in dynamo when
they published the paper was Amazon
shopping cart so when you think about it
for a shopping cart it's much more
important that the database accept a
right than the database be consistent in
other words if you have a clustered
database and there's some sort of split
brain syndrome going on and databases
can't necessarily agree when you're
putting something into the database
about what the correct value should be
it makes sense for Amazon to accept that
right and not drop whatever you were
trying to put in the shopping cart on
the floor because that would be losing
revenue for them right you always want
to be able to write to this shopping
cart even if you can't get the exact
consistent state of it all the time give
you machines failing or network outages
whatever so as long as the shopping cart
can always accept rights when you go to
checkout then you use a more consistent
mechanism where you get make sure all
the machines agree on what's in the
shopping cart at the time of checkout
but while we use our shopping you never
want to say no I can't add this to your
shopping cart because one of the
database services down that is a revenue
loser for Amazon and since then Amazon's
not the first to have that problem but
since then a lot of business on the web
is like this if you're in the business
of displaying ads you don't want to
display an empty advertisement because
of some problem and some remote machine
in your datacenter you always want to
display something so being able to
always write to this database even if
half the nodes are down is one of the
key sort of requirements from the
beginning
businesses on the web nowadays can get
popular very quickly virally as they say
and you don't necessarily want to lay
out a bunch of money for Hardware ahead
of time you want to be able to
incremental e scale as the growth of
your website scales so being able to add
machines to the system one at a time two
at a time is very important just to
emphasize this is this is a
fundamentally clustered database we
don't sell react and or people don't
usually comply react in nose counts less
than five so there's always five nodes
here
symmetrical decentralized heterogeneous
this means that all these sort of at
least symmetrically decentralized means
that each node has the same task there's
no master node there's no slave node
there's no special configuration node
every node is exactly the same and if
you've ever operated a business or data
center at scale this becomes very
valuable because when you got people
messing around in a data center the
chances that they spilled a coffee on
some machine gets higher and higher over
time or something bad happens to one
machine and if one of those if they
spill their coffee on the master that's
a big problem so why not just have no
master every machine in a cluster of
react machines or dynamo machines if
we're talking about a paper that's the
same exact role so that means you'll
have to monitor them in any sort of
different way you don't have to be more
concerned if one goes down and the other
from a client you would the database you
can talk to any of them as if they were
the same it just makes things much
simpler and cheaper
oddly heterogeneous meaning you know
machines get more and more powerful
every year and if you start one year
with you know a group of five machines
and then you buy five more machines the
next year the database should be able to
recognize and take advantage of the fact
that these new machines are more
powerful and send more work to them or
recognize that they have more resources
and treat them accordingly
and finally these kind of data stores
are typically the the things that sit at
the bottom of the large sort of
service-oriented architecture stack when
you go to the front page of Amazon that
page
I think it's last I read 175 different
internal services that get called to
render the front page of amazon.com all
of which have some persistent datastore
underneath them so predictable latency
it's very important if the latency of
your database is all over the place it's
not a suitable foundation to build a
very large service-oriented architecture
and this is accomplished with several of
these techniques all of which have
appeared before in the literature and to
try to explain all of these in depth in
the short amount of time I have would
would not work so I'll cover them pretty
briefly consistent hashing is one of
them that was a technique pioneered at
Akamai and that's how they that's how
Akamai manages to get the right data
cached in the right places so everybody
has a nice fast web experience without
having to catch everything every word or
without having to move tons of data
around when new servers are added to the
network
Akamai used it for web caching in react
and in dynamo we use it for figuring out
which machines a given object should
live on one thing I should state up
front is that in react in addition you
know you gonna have at least five nodes
every object is replicated for three
times
so it's a database for critical data
that can't be lost
vector clocks are a way to figure out
how objects relate to each other since
this database as I said before will
accept a write even if even if the
network is partitioned or there are
machines down means that it favors
availability over consistency favor and
consistency is something that
traditional databases do if you have a
traditional clustered Oracle system or
my sequel system and you want to write
something to it if the database in that
case can't be guaranteed that every
member of its cluster agrees on the same
value then it will refuse to write
because the chance then it opens a
chance that you know some node in that
system that's partitioned off will serve
all data and traditional database
systems value means dr. clocks since
there could be diverging copies of data
in react are basically just a way to
figure out how objects are related if
one is an ancestor of the other or one
is logically concurrent I guess the good
analogy here is if you use git or
mercurial and you try to merge a branch
into your code you've already merged in
it knows that and it's just in a lot or
if you try to merge a branch in
sometimes it merges cleanly because the
changes don't reflect but sometimes two
people have edited the same thing and
you get a merge conflict and you have to
go
fix it doctor clots provide the same
sort of infrastructure in the same
properties for different copies of the
same data in a distributed database like
this we prepare anti entropy and did
handoff gossip protocol I have slides
for and oh I want explaining here
because I want to respect the time
limits so it depend off so but we're
dealing with a cluster database here
we're dealing with many nodes this
database is exposing a key value API how
do we decide where data lives it's a
technique called consistent hashing and
what this ring represents here is say
sha-1 hash functions the hash function
we using react the range of that
function is fruit zero two two two the
160th which is a big enormous integer
that's the range of numbers that that
function can produce giving whatever
arbitrary input
the way we decide where data lives over
here for a second to illustrate this is
if we envision that space and integer
range zero on this side two to the 160th
over here as a circle so zero is at the
top as is two to the 160th so this is 2
to the 160th over 4 to the 160th over 2
etc right so just basically take an
integer range and wrap it around like a
circle then as you hash keys
so just keys in a hash table they're
gonna end up somewhere between that
range is that's the range of the hash
function and then you visualize where
that is on the circle so passionately
the key you know go to its some
arbitrary large integer and same ends up
here these coloured slices here
represent different nodes in the system
so if it ends up here say oh thanks the
primary node for it and then we walk
around clockwise and we get green and
orange so that's why it's consistent
that's what I call consistent is because
you can add nodes in here and they just
claim different little pieces of this
and that means that there's not going to
be a complete rebalancing of data when
you add nodes but rather only little
pieces where the new machines come in
claims-based a complicated concept but
that's roughly how it happens so say a
node fails say the green node blows up
right and then you want to hash this key
block / this big UUID well when you hash
that the value wins here and again we
replicate 3 times so we're going the
normal places isn't this data would live
would be on this one this one and this
one but this one's down very simple no
problem we just skip over it and pick
the next one on the Ring
normally the data wouldn't live here but
any of these node
can take requests for any any of the
other ones so they sort of compensate
for each other when things go down and
then if you were going to read that data
again the read request would see that
this guy's down and it would read it
from here instead this all works out you
know sort of very nicely in failure
scenarios those can go down other notes
pick up the rights for them it's sort of
like when you go on vacation your
neighbor picks up your mail for you
right so your mailbox doesn't fill up
and then when you get back and use the
mail back to you this is exactly what
happens in react so if the note comes
back this guy knows that he picked up
the mail for this one and he hands it
off and then as requests come in the
normal operations resume and it goes
back to the original replicas so this is
a common theme I think it's always
replicated many times notes can fail and
we still maintain that three replicas
count so you can lose two notes and
still have a copy of your data left very
quickly the this how a request sort of
plays out in react and this exposes some
cool things about the Dynamo concept how
we've implemented in react and some kind
of cool things about the ER landing them
as well so react has clients you can
talk to react over HTTP you can talk to
react over protocol buffers interface we
ship clients in dotnet Ruby Java Scala
Haskell go and a bunch of yeah pretty
much any modern language you can
client or if you want to use the HTTP
interface any language that can speak
HTTP can be a client for you so you can
were in the clients process space the
client says in this example it looks
like we're storing blocks of some file
or whatever the client issues the
request to react the cool thing about
Erlang is that you can seem on many many
processes they call them process in
Erlang they behave like friends but
they're not as heavy weight as an
operating system thread so on my laptop
there I could probably spawn a million
Erlang processes in one VM without the
machine breaking a sweat so what we can
do because of that architecture is
actually represent individual concurrent
events or processes as individual
threads without the need for thread
pools or multiplexing different logical
events across you know smaller pool of
threads it'll be a very clean
programming model so the clients
requesting this key we spawn a process
to handle the request and we do this
hash lookup where it finds out where on
the ring the data lives and that results
in 10 11 and 12 so then you have the
cluster here and that's a sort of zoomed
in on one part of that ring and we have
10 11 and 12 these the ones responsible
for it so this little request handling
process broadcast out to all those three
replicas say give me blocks / big new
UID this is a feature of Dynamo and
react that I won't get too far into but
you can specify in a per request basis
how many of these replicas you want to
respond before you consider the request
successful so by tweaking that you can
tolerate more machines going down if you
were to
they only asked for one it would respond
with the first the first replica that
responded as you say - which is the
default it'll wait for - and do some
reconciliation on the values in case
their divergent to make sure you get a
more consistent ears they're not a stale
one the lower that value is - higher the
chance that you're going to get a stale
response because we don't insist not
everyone agree in your right time so Ken
responds with version 112 responds with
version 2 when I was talking about
vector clocks before
consider these I'll make I'll use the
git branching analogy what these vector
clocks can tell this process is that v1
has already been merged into v2 it's
like if you try to if you were doing a
git merge and you merge the same French
twice the second one would be a no op
and that's what the vector clock data
structure lets us do so here we know
that v2 is up-to-date version the other
node had a stale one because maybe it
was down when it was last written and
it's just come up and given us the stale
answer back so we know we choose the
correct one the clients told us that it
only wants to wait for two responses out
of the three that that it could wait for
if it asks for three then it couldn't
tolerate any notes being down because
three we replicate three times and it's
a spec so now we reply with v2 to the
client and it goes on its merry way it's
doing something with this result or is
you know processing to have a request or
whatever but recall that we have this
thing called need repair so remember
this 10 gave us version 1 it was on a
date for whatever reason 12 years 32
this process even though it's returned
to the client is still sticking around
because it has work to do
react and dynamo are eventually
consistent databases not immediately
consistent or a strictly consistent it's
a scary word but a vegetable doesn't
mean when we get around to it or we may
never get around to it it means that as
soon as it's possible to get around to
it we do it except we just don't stop
the world if we can't do it at a certain
time we always favor giving you the
latest value or accepting the right even
if we know not everybody's going to
agree on it so this is a very front of
if you're under the cap theorem this is
a very this is the essence of the cap
theorem here is that you really have to
choose between consistency you know
being up to date all the time and
availability being able to accept
requests all the time
so we've already returned to the client
but we have work to do
since we're eventually consistent we
take every chance that we get to bring
our database towards consistency we've
applied the v2 and say the other guy
then we always wait for that third
response so here even though to
replicate on a date we could use the
vendor clocks to determine that this was
the correct answer and then since we
know that v1 was already merged into our
new object we opportunistically and
asynchronously the clients not waiting
for any of this to happen
repair the snail reference so now we're
consistent as a side effect of doing a
read of the data
just to go into the architecture a
little bit I know I'm going to really
fast this stuff our website is bachelor
calm you can read all about this stuff
there yes what if no eleven had returned
version freezed a certain time version
true but you still have no Version three
if they got yes yeah that's eventual
consistency for you I'm going to update
the to our version 3 even though we yet
we've given the client version two but
the next client is gonna read version 3
so let me give an analogy to that say
you update your avatar on some sort of
social network site right and then it
takes you back to your profile and you
still see the old avatar that's a normal
that's within the realm of acceptable
user experiences or you add something to
your Amazon shopping cart and it takes
you back and you look at it again and
it's not there and you refresh the page
you've seen this kind of thing happen on
the web or you change you change a DNS
entry and it takes a couple minutes to
propagate these are all examples of
eventual consistency in the real world
that we've come to deal with it's not
right for every scenario but in
situations where availability is
important being able to tolerate failure
is important
there's there's ways to make it work
so reacts written in Arlen there's two
client API HTTP and protocol buffers
you're allowing VM is really cool
because it allows us to special
concurrent activities as actually
concurrent processes if you're writing
in Java you're probably not going to
you're not gonna write a scalable web
server by when the request comes in
spawning a new thread for every request
you're gonna probably have a thread pool
and put request in some sort of queue
and then the queue handles it and then
you have another queue to get the
response back etc we can actually just
launch a thread for each one and not
care too much about it and there's a
different kind of state machine model is
a process for every different request
type so it's a key value store the API
is pretty simple there's get and put and
delete just like it's having a hash
table in your favorite language but we
also have extras like MapReduce and some
other ones that I'll get to you later on
the real cool thing though is that this
is a clustered system there's we have
react clusters doing you can make one as
small as two nodes we have customers
with react clusters that are 150 200
knows large and all the complexity of
handling that clustered stuff and
there's a lot of it all that logic
around taking responsibility for other
nodes data when they go down the whole
notion of just membership how do the
nodes know about each other or how the
notes know how to route requests etc is
handled in this library called react or
which is the whole Dynamo model this
loosely coupled eventually consistent
distributed system but this layer here
has no idea that it's participating in a
database it just implements the sort of
data structure is an outward
some stuff that that the Dynamo paper
talks about but doesn't really insist on
a use case that under so the code up
here on this side doesn't really have to
be written in a complicated distributed
systems manner it's this says hey react
or you know about the cluster and all
these nodes here's a request distributed
in the right way to the right nodes in
the system react or handles that and
then on the bottom it's an interesting
way to use the library because you sort
of have to plug into it on the top and
bottom there's a process running for
every little piece of the pie can that
ring and those carry out the actions
locally on the cluster so from the top
you say here cluster do something the
cluster said that this where all the
complexities hidden says okay this has
to go to nodes baby and see here notes a
B and C here some work and then note
AB&amp;amp;C just carry out that work locally so
neither part each part is sort of
ignorant in fact that gives this
complicated distributed systems going on
the way we factor difference and there's
a JavaScript runtime in there because
not everybody likes Erlang and you can
write your MapReduce jobs and all the
custom stuff in in JavaScript as well so
react or there's actually several
companies most of them sort of
adventurous Erlang shops that have built
custom non database high-performance
reliable ads just using the react
our connection so this talk is really in
about ten minutes I'm gonna sort of
blast through this stuff but that sounds
great but I saw people get visibly
uncomfortable when I talked about some
of the concepts here and you weren't
wrong to do that because some of them
are extremely uncomfortable they're
necessary compromises if you want that
extremely high availability but
sometimes it's worth giving up a little
bit of that just for some sanity and how
you develop against this thing eventual
consistency can really suck what can
happen with eventual consistency that
example where we had version one two and
three or you suggested version 3 we had
versions one and two was the easy one
when we could pick a clear winner we
didn't cover the case we're just going
back to the git metaphor there was a
merge conflict if that were to happen
you would ask for you know key K and
you'd get two values back like what if
you want to do like select one row from
a database you normally get one row
right what that indicates is that say
half your nodes were split from each
other and you've had somebody update say
it was a grocery list the nodes were
fine initially and you added milk to the
grocery list ok cool everybody agrees
then a switch dies and half your nodes
are on this side half your notes are on
this side but you have clients talking
to both sides and then someone on this
side says add eggs so now this side has
milk and eggs and some on this side says
I had cereal to the side as milk and
cereal right if we insisted on
consistency both of these sides when you
went to add in the eggs to the cereal
would say nope I can't talk to the other
guy who has the rest of the list and
this is going to lead to an inconsistent
situation but instead we say no we
always want to be able to add to this
grocery list both sides except the right
so then the partition heals and you go
read the contents of your grocery list
and what you get is two values one of
them says milk and eggs one it's just
okay cereal this is actually a really
easy conflict resolution scenario
because you can just Union those two
sets together and get milk exit cereal
but that's a simple case it very quick
thinning it's difficult to model more
complex data structures than that using
this and more importantly it takes all
that consistency responsibilities and
gives it to the client you have to
resolve conflicts I mean on behalf of
the database and the key value model is
limited in what you can model with it
right there's if it doesn't look like a
hash table then you got jumped into
Martins to model it on a database who
exposes the API I won't go through some
of the other ones just a time but uh
talking a little more about the just the
limits of the data model the the good
part is you don't have to do that along
with all that nice vector clock
information we also record a regular old
timestamp and the default behavior is
just to return the one that has the
newest wall clock time standard so in
that case if the person got a serial did
it last according to the physical time
the result would be eggs and cereal but
the milk or whatever was written on the
other side would have gotten lost right
and if you go through this in your head
there's no way about it we could have
either denied the right with that number
of partition going on or lost some data
in the case of making it a little easier
or dealt with the conflicting versions
it's even more of a pain if you're a
shop that uses many languages because
then you have to write this logic many
times and there's several solutions that
have been proposed to this bump which is
a iPhone contact sharing app they were
at a proxy that implements all this
logic and an older
clients talk to that proxy you know read
it rewrite that conflict resolution
logic over um one great use for this
kind of system though is for immutable
data it's data that only gets written
once and never gets changed again Pat
Helland big distributed systems sort of
pioneer says accountants don't use
erasers when people want to shoot down
the notion of eventually consistent
databases they always say well you
couldn't implement a banking system with
this and that if you've worked with
financial systems you know that's wrong
because when you debit money from an
account it's not a destructive it's not
like there's a row in a database with
balance and we forever just change it no
you have another event or another object
that gets created that's a debit and
then everything gets reconciled at some
point in the future accountants don't
use erasers I know that's quote to talk
about this if your ATM machine can't
talk to its home base what it actually
does a lot of times this is alright I'll
give him two hundred bucks or I'll you
know I'll feel safe giving you some
amount of money and debit it later if
you know if the financial world didn't
use eventual consistency then it would
be impossible to bounce a check but if
the data that you're writing never
changes then you don't have to worry
about those conflicts you're never gonna
get two values back because nobody's
ever changing the data and you don't
need to wait around like we did in that
example for all the versions to come
back because there's only ever going to
be one version is so one technique for
dealing with for using these eventually
consistent stores is just don't change
data simply
create a new copy and have a pointer to
it right
the problem then becomes how do you get
rid of the old pointers that point to
data that's now stale but that's at
least that's the databases problem now
not yours you have to make a distributed
garbage collector which is very hard but
at least we've taking that problem on
the user's hands so in the several years
since these kind of databases became
popular people have recognized these
pains and they've come up the research
community has come up with some
solutions one of them is CRD cheese
conflict free or commutative depending
on what paper you're reading replicated
data types all that really means is it's
doing on the server what you would
normally have to do on the client to
resolve these things and how this
usually looks is basically instead of
treating a value is just a materialized
value you treat it as an operation log
and you run it if you're familiar with
CQRS or event sourcing this is going to
sound familiar
you basically to arrive at the value you
start with some sort of cache snapshot
of it and then you apply the operations
in the log and arrive at what the newest
value is and to change the value you
don't just destructively change the
value you add an entry to this operation
log and then with some metadata about
who you are and when you changed it and
then at points where the system knows
everything's in sync you can sort of
prune that log and create a new snap
continue on so this is coming to react
and that would give us things like
counters and sets we're instead of
reading a value changing it writing it
back and all this conflict stuff going
on you can just say hey increment the
set or increment the counter at this key
or at deciding to this set this key I'm
gonna skip by this by that so what we've
come to know is that eventual
consistency is great for the stuff that
can handle it
the your avatar in a social networking
site right it's fine if it doesn't
update right away when you change you
the like button on Facebook right it
doesn't have to be precise lots of
things in this in the web world are fine
with eventual consistency but usually
for anyone comprehensive use case there
is maybe 50 to 80 percent of it that's
fine and eventual consistency and
another you know 20 percent or so that
actually needs either requires strong
consistency or be a real pain to write
in that eventually consistent model so
coming up in a su - version of react
there is going to be strong consistency
so that's what you'd normally expect
from a database where you write
something to the database and that value
is either accepted and it's true forever
and ever or it's denied and the diet
change is the same none of this multiple
versions it's just going to be important
one of the add this since it's so much
easier people are going to probably want
it use it all the time
and they're gonna lose some of the
availability benefits of the other or
the other way of doing things if this is
the kind of stuff that you're into we
had a conference in San Francisco a few
months ago called recon recon 2012 comm
has lots of interesting talks on all
these kind of subjects sort of an
interesting aside here how eventual is
eventual consistency right like how
often does this really happen how often
am I going to get those two values back
and have to do something with it
it turns out you can predict it some of
our friends at University of Berkeley
made this model and proved it that if
you can measure the different look
latencies in different parts of your
system you can model how likely you are
to read the latest value and it turns
out if save this is just a simple
example but if you have 0.1 millisecond
latency at certain parts of your system
that you have to instrument that if you
wait 25 milliseconds
you have or if you wait 100 milliseconds
here you have a 99.9 6 percent chance of
being strictly consistent so the thing
about eventual consistency is that it
just doesn't enforce it it doesn't mean
that you're not consistent all the time
if you're reading the database only
every 200 milliseconds chances are
you're never going to see any of the
eventual consistency it only happens in
further conditions or under very heavy
concurrency when you you know constantly
writing a reading
one other sort of operational problem
TCP and caste so the best explained by
my co-workers grandfather is that you
can't pour two buckets when you were
into one these are lessons we learned
from running this kind of database and
this happens in any system really where
there's replicas have any where you have
a coordinator and then that coordinator
sending requests to many other nodes so
what happens is and he saw that sort of
broadcast in the earlier slide the
coordinator sends the request all of the
replicas responded roughly the same time
with a large value and when that hits
the switch the switch has if it doesn't
have space to buffer it and buffer space
buffer memory on switch ports it's very
very expensive it has to drop packets
and all the TCP flow control mechanisms
kick in and you can see like a 10
gigabit line only getting about 25
percent utilization and it's very very
very hard to even notice what's
happening because you have to really
grind down the window of that year of
events it will get this puzzled us for a
long time I didn't even know what TCP
MCATs was like we discovered it if you
ever see unexplainable like bandwidth
non utilization and systems like this
Google TCP in caste and you'll get some
some good
that got talked about back to clocks
so yeah just a little bit more about
react we added a bunch of things dynamo
in the paper was just the key value
store we added the ability to do
MapReduce in either JavaScript or Erlang
full-text search in a manner that looks
kind of like you're using solar or a
blue scene you can put secondary indexes
on data because having to look things up
by just the primary key all the time
isn't easy you can add what we call pre
or post commit hooks what traditional
databases would call triggers so you can
run custom code after certain events
happen this is the thing that we sell
which is the ability to have a data
center in an RO sadena Center in London
in the data center in Japan and in one
in San Francisco and have them all
talking together for applications that
really need a global low latency
experience there's a distributed
computation framework for sort of
real-time analytics called react pipe
and we got CS which is our Amazon s3
clone again the slides bitly slash no
sequel are hosted on one of our test
clusters for Amazon s3 this is sort of
the future of react nobody really wants
to code against this eventually
consistent database that's all
complicated with all these options but
it turns out it makes a great
foundational layer for building much
simpler database like things on top of
it and one of these is react CS so s3 is
a much more constrained simple
understandable API put large files and
Twitter large files out of it they're
organized in buckets everybody knows and
loves us three and this really was the
first time we tried to I mean we've
already
Bachelet we've all written apps against
react and experienced the pain of
- right against a database like this but
this really made us realize that so the
way to go in the future is much simpler
databases written on top of the big
feature phone complicated
and we're almost running out of time
react or if you're interested in
distributed systems super systems
research this has all really the hard
stuff it's it's own separate github repo
with its own separate project all the
actual applications that you deal with
when you're talking to react are
basically plugins to this framework
called react or open X is a very large
ad-serving company they have three or
four hundred nodes of custom react core
app running for doing the impression
counting stack mob is a mobile backend
as a service company there's a lot of
their stuff is written in react or
problem with it is that needs to be a
lot better documented you have to sort
of be in Erlang cattle boy to play with
it now multi data center replication
we're basically out of time but the
multi Civic multi datacenter replication
that we sell is much more optimized for
when links that are going to drop a lot
more packets than your local data center
so react actually comes with two
different replication sets of
replication not rhythms one optimized
for in data center or low latency I
throughput one optimized for long-haul
unpredictable lossy links and then
finally Erlang it's a great language for
this stuff I started my career naka my
writing and distributed systems and did
more of them at Apple and I didn't
discover early on until I got to bash
show and I know for a fact if we had
Erlang at Akamai back in the day we
could have saved a couple million
dollars in develop
I poorly reimplemented stuff that
airline provides first-class support for
distribution the transparently across
mobile
notes lightweight processes very easy to
just saturate a multi core box by
modeling your problem as a bunch of
independent processes and if you're into
Erlang github.com slash bash it's not
just tool and meet a lot of cool open
source stuff there that's where we are
so quickly for the future react has no
security model built in it's up to you
know administrators to firewall it off
we need to add a security model to it
support market clusters
everybody wants to have hosted services
now react has no way of implementing
fairness somebody can run a MapReduce
job that uses up all the clusters
resources and these are two ms starved
is their standard set of distributed
systems problems and the real answer is
use react as the sort of hidden
foundation for much simpler cloud
services and that's a quick version of
this presentation I wish I had time to
go through it all but to the extent that
you have any questions on that I'd be
glad to answer them the gun these are
our lengths I'm at RB 0 on Twitter we're
hiring all over the world if this is
your kind of thing you don't have to
know Erlang and you can read much more
about rocket Docs that passion thank you
rakesh way oh yeah we have some examples
on our blog I think there's an example
of Twitter clone on our blog somewhere
there's an example of image store or URL
shortener thing usually you know it
maybe maybe I'll try this way um have
you ever have a table that you've
denormalized so much that it's always
that it's just a primary key and then
maybe some compressed XML and a blob
column or seen or heard of these types
of things you know maybe it started out
oh yeah yeah yeah those are the type of
things that you know the I've seen them
a couple sites if you're only doing
primary key lookup on something and
that's becoming a bottleneck and that's
a good target for react the truth of the
matter is is if you're getting by with
your relational database use it because
this stuff is much more complicated and
if you're not getting buying your
relational database like by some SSDs
and buy yourself some time eventually
you know our long-term roadmap I was
asked today where react would be in five
years and I think in five years you'll
be able to talk to react with the sequel
interface because will gradually build
back to where we were you know when we
when all these databases sort of broke
from the tradition
but until then you know if you can get
more gas out of a relational database
definitely do it but you'll know what
one of these problems hits you when they
really do run out of gas then it's you
know then the first tables that people
tend to migrate are those just look up
tables of keep out invaders but there's
a for more sort of relational
holiday-related to react issues there's
a on our website there's a case study of
a company that was from my seat Bowles
relatively complex application it wasn't
a blog it was like a cloud monitoring
app so it wasn't me some data model that
we're all sort of familiar with but they
talk about how they moved from that to
react you know from relation to react
whatever we have on our Americana
website anyone else I know that some
exercise you smoke ODB
what is what are you solving that they
assure um so what Malgudi be does well
there's a couple things
MongoDB you still have this problem
where you have a master and a slave it's
very hard to scale rights out in MongoDB
because all the rights have to be
funneled through a master right so if
you have a workload that you can't
funnel all the traffic through one
master then you have to do shards and
that becomes sort of complexity that's
leads onto your app not the react
doesn't bleed complexity onto your
application but then you shard and you
choose a shard key say the first letter
of a person's last name is a simplified
example
machining for the A's and the bees and
the seasons the bees well unless you
pick a sharp key that has a nice uniform
distribution then you have like the
Justin Bieber machine right or some
popular pop star right getting way more
traffic over time and you have a hard
time minimizing hot spots I'm not I'm
gonna be diplomatic about the other
issues with Mongo and and data loss but
if you like search for MongoDB database
versus react data loss draw your own
conclusions there mom going to be is
great because you can fire it up and
right get really quickly up to speed on
an app with your laptop without all
sorts of object relational mapper z--
and crap middleware in the way right but
when it starts to hit very very high
traffic is when we get a lot of
customers I can only really speak my
experience coming from
to
Margot's definitely more friendly for
developers reacts more friendly for
operations is another way you can sit
it's anything like a caching layer or
anything like that Thor in English now I
don't think the race or is anything like
that
yeah I actually I actually wrote and I
don't know what I do that it's on get up
somewhere a layer on top of the app that
actually implemented the memcache
protocol there's no explicit cache in
Rio though it can be use pretty
effectively as a big distributed cache
because we Chuck we opted for our
simpler design where we basically just
leveraged the operating system buffer
cache right so for the right kind of
workloads you're gonna be getting those
reads served out of memory anyway
because we have a very simple on disk
format that's very friendly for say like
the Linux page cache to just read in and
we we give the page cache hints like if
if you delete a piece of data to me say
hey effective so we sort of used the up
the file systems VM cache as our cache
and so and the API is very similar to
that of like a memcache D the things
that we didn't when I wrote the memcache
dealer and the things that we don't have
somewhat obviously are things like
compare and swap because that's
something that needs strong consistency
that we don't have but when we add the
drunkest didn't see stuff
possible to probably implement a
faithful then cat you know future
complete memcache yeah the other thing
is that the HTTP API is very cache
friendly we generate and honor all the
right etag and conditional headers so
you can just put a squid or a varnish or
an engine X or something in front of it
and it'll just naturally sort of play
nicely with
cool well thank you guys for coming out
again check out the no sequel Search
Roadshow and give Tony Falco my old boss
a hard time I do have a question sir
thank you
all right thanks a lot</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>