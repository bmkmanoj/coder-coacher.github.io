<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • HTTP 2 • Daniel Stenberg | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • HTTP 2 • Daniel Stenberg - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • HTTP 2 • Daniel Stenberg</b></h2><h5 class="post__date">2015-10-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/r2f8tloYJY0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm going to talk about HTTP - it's a
mouthful to say sometimes we just call
it h2 because it's easier less speeding
when you're talking about it but anyway
I'll try to kind of mix it up and just
to make sure that you read something in
this plan and this is what I intend to
talk about I want to mention a little
bit about it should be HTTP how it works
today a little bit about the problems
how we solve them or how others have
sold them a little bit of what they
should be - how it works why it works
like that something but its status how
it runs today how many are using it and
how it looks going forward a little bit
about the future or guessing about the
future this is me I work for Mozilla I
work with networking I've been involved
in the IETF so I've been kind of
fiddling with this protocol stuff for a
couple of years
I'm perhaps most known for being the
main developer of curl I do all sorts of
network stuff
yeah then primarily on Firefox on on my
day job so the Internet today has sort
of turned out into a reality where we do
HTTP for everything right we don't do
anything we don't do new protocols these
days we do everything on top of HTTP and
HTTP as an oldie was created sort of
back in the nineteen ninety-six when it
should be 1.0 was released there was a
1.1 update in 99 but basically it's the
same protocol that we started out then
it's the same protocol today and as you
know websites the internet everything
has changed quite drastically since
those days the single page animated
moves under construction it's sort of
not the same thing anymore
just looking at the last couple of years
we have this great resource
called HTTP archive at org they just
collect stats about the 300,000 biggest
sites on the Internet
and they just check how things develop
over time so the bingo they've been
doing this for the last four years so we
can take this as sort of just check out
how things are going the number of
resource the number of resources you
need to get to show the front page from
these sites on average it has increased
from 80 to 100 in four years then the
total data the total amount of data you
need to show this front pages has
increased like almost three times in
four years sort of the trend is
aggressively growing and to do this to
just render these front pages we need a
lot of TCP connections actually we need
240 on average just to show this page 40
TCP connections on average and as you
know when we're looking at HTTP what is
sort of a ping-pong protocol so we have
what I call it a round-trip bonanza we
have this guy and he's talking to the
cloud and this is time so we do a
request and we get a response right
that's how it works and we don't do
another request until we have got the
complete response and I will do another
request so we don't get any response
while we're sending here the request
bam-bam-bam ping-pong ping-pong that's
how we do it there are many moments in
time here that you can see that nothing
is happening in the other direction you
can compensate that with more
connections so that the apps
kind of overlap so this sort of
ping-pong it makes it really slow if you
increase latency because one request it
waits for the response to come back and
you know the speed of light isn't going
faster so sometimes we have a site very
far away you need to send a request you
need to get the response back doesn't
matter how very big bandwidth you have
so just the same site today if you have
a very low latency to the site or very
high light and to decide to decide is it
going to drastically affect the
rendering time for you to justesen seed
it in this page just adding latency will
just make it slow and since we have
these TCP connections that we're doing
the HTTP requests on ping-pong ping-pong
we can add TCP connections to the same
sites we can do many TCP connections
perhaps there's a limit to that of
course but either way since we're this
ping pong protocol we have this
interesting head-of-line problem you
know this is the lying in the shopping
mall or you know which one do you pick
there's always going to be the wrong one
right the guy in front of you he's the
slowest but for sure I mean you can pick
another line possibly in the beginning
or possibly you can start a new line you
can do that to some extent even in TCP
but there's a cost reading a new line
but this is a big problem because you
don't know how how long time the guy in
front of you is going going to take so
this is why we in it appeal and
introduces a lot of TCP connections
because then you can sort of make better
guesses or estimates or whatever this is
sort of how it should be kind of implies
this is the rule and the rules of the
game so since this is how it works we
introduced a lot of fun workarounds yeah
this is how it works so but we can work
around this for example we're
introducing spriting right instead of
sending many many many small images
images we send a huge image with all
those flags in one and then we use CSS
and JavaScript to just show the little
flags we want to show on our particular
page great one transfer many images just
to kind of circumvent the latest problem
of course you get a lot of data when you
just want to have one flag or if you
want to update one of those flags you
get a lot of refresh
it's crazy for caching it's really you
need a lot of tools to handle this and
so on but of course that's one way to do
it and then another
fun invention is to just cram in the
binary blob in your CSS these read data
stuffs that's the base64 encoded image
huge thing in your CSS impossible to see
for a human eye but it similarly just
gives you one blob to get to circumvent
the latency problem great and how do you
do it if you're done a programmer on the
website you use the concatenation you
put every single javascript file into
one monster right to just get it all at
once and if this isn't enough mentioned
you want to have many TCP connections so
just to kind of distribute your requests
on so that you can kind of suffer
equally on many connections when when
HTTP 1.1 was written in 1999 it was said
that we should only have two TCP
connections per hostname but two TCP
connections per host times am later on
that kind of the limitation was removed
from the spec so nowadays browsers do
six connections per hostname but six
connections you know we wanted 40 right
we're 100 200 yeah we just add new host
names right every site now has it has a
lot of host names so you can have six
connections per host name so that's like
in this case easily 45 connections just
to that single site that's how you do it
or did it
this trendin all those sort of
workarounds those hacky things on top of
the protocol not really scaling very
good and as a browser it becomes really
hard and I mentioned a lot of
connections but we can't maintain all
those connections all the time right if
you go to a site and then you move on to
the next site and then you come back how
many connections do you still maintain
you can't really keep
worthy over 100 connections to each site
when you have 200 tabs in your browser
we need to kill off a lot of them so
anyway it doesn't really scale it
doesn't really work like that going into
the future and we need to remember that
kind of the trend that's where we're
going so we introduced this guy or
rather we didn't really introduce this
guy but the HTTP to protocol we started
back in 2012 in the IETF it started
earlier than that because that work is
sort of based on them
speedy protocol that Google sort of
experimented with even before that
anyway in the IETF we worked on the
hiv-2 protocol during a couple of years
it was actually I would say royal Swift
working for for being in IETF protocol
but still 144 days ago we sort of
published or of c75 40 which is the it
should be 2 protocol them is as our of
see from the IETF so we're sort of done
here and I am now when I explain to you
a little bit about how how it's going to
change how we make sites and how we
experience things and I should clarify
that a lot of the work in hb2 has been
sort of from them from this angle that
I've explained which is a rather I would
say browser web server centric sort of
approach not as well and not as well as
much as program and programmatic it
should be or API uses or so this is more
like how you get a lot of resources but
to make a new protocol like this to kind
of make it possible to deploy at scale
on the internet we need to maintain a
lot of things just we can just replace a
lot of things just check out a lot of
URLs that were being using for 25 years
20 years No
we need to reintroduce HP to us just a
new framing layer it's just a new way to
send stuff over the Internet or over the
wire really so we have all those fun
paradigms left or whatever you want to
call them they're basically we all have
a request response protocol still it's
still a request then you get a response
it still headers and you get a headers
and body and you send headers and you
send a body the same thing the same way
we've sort of learned that HP works for
since forever and we maintain a shippi
colon slash slash URL so we maintain the
HTTP colon slash slash URLs like the
same it's gonna they're gonna remain
that they're always gonna be either I
doubt anyone will ever break those and
we also sort of recognize that HP one
that one is gonna go into linger it's
going to exist for a long time going
forward we may introduce new protocols
today or tomorrow or whatever and we may
do new fun things but the long tail on
the internet it's really long and there
are go there are going to besides there
are just contempt and happy and they're
providing things and they'll just stick
to what they have so we're this this was
a rather big factor for us going into
this work that we're focusing really
hard so that we can actually proxy back
and forth between HB 2 and 1.1 so I mean
ideally one day in the future someone
will put a front proxy before in front
of your 1.1 server and you'll serve ATP
to to the world but you could still
leave your own you rolled the server in
the closet and it just run there when
when doing new protocols today I mean in
the in general when doing new protocols
in the modern era we sort of come to the
conclusion I think you all will agree
with me that this old concept of being
liberal in what you accept it's a pretty
stupid thing to do right so we shouldn't
be liberally in what to accept anymore
when we do protocols we're pretty
conservative in what we accept we only
accept
exactly what the other end is supposed
to send we don't we don't just try to do
things that's how we did it before it
has turned out to be a major pain and
just miserable people all over we're
stopping that right I know you all agree
with me that's sort of we have in the
definite complete mess in a lot of areas
because of that so in a should be too
and this is really a mantra that
originated from the from the Google work
of speedy is that we're really
conservative in what we accept we only
accept exactly what we're supposed to
get otherwise we're just cutting down
the connection or we're refusing to
serve it and that is also that also goes
along then we'll have less optional
person protocol HTTP 1.1 is riddled with
a lot of a lot of fun options of things
that you can support or you cannot
support you can you choose which sort of
the result of that is that nobody
implements it in the beginning with
deploy protocol five years down the road
we start implementing the optional parts
doesn't work because then suddenly would
you realize that didn't really work and
people are doing in different ways and
stuff breaks it should be pipelining
perfect example doesn't work
I mean deployed in the in the big ugly
Internet so we have less optional parts
you do it like this or you don't do it
and that also some done sort of mirrors
to this that we we don't have any minor
version of HTTP - it is HP - there's no
dot 0 here 1.1 yes but there's an HTP -
this then translated into exactly what
we sort of made out of all this is this
it is binary so no it means that you're
a lot of less this is sort of the get
Direction I guess from a lot of people
my telnet I can't do telnet the server
port 80 and get slash to be with anymore
no that's not possible and more with HP
to know more telnet tricks I know a lot
of people they just think about that
because they haven't really done it for
a long time but you could do it of
course I mean these days you have TLS
and stuff anyway so you wouldn't do it
anyway but doing it a protocol binary is
really it's it's really the way to do it
if you want to have it easier to
implement from both servers and client
side and anyone who ever tried to
implement an HTTP stack in either side
they would know that it should be is a
perfect example of really horrible ways
to to phrase things as sizes in ASCII
whitespace is everywhere plus and
minuses can you have a comma between the
zeros like or can you have spaces in the
number or whatever and also is to be one
that one they have like I think there
are at least four different ways that
you can sort of end the request
depending on what the contents within
the data so it's really really hard to
figure out exactly when the request ends
and where the next starts and so on so
it's in HTP - there's a size there's
data and then there's a size and their
state on their size and data much easier
and when it's wrong you disconnect
basically so yeah that I sort of enjoyed
that I had these Lego images in here
I've had it for a long time so I see it
as a salutation for this country so this
is how there is the
your framing of the package is basically
just a frame type there's a stream ID
and there's some other minor things all
right you're these days you should still
be doing TLS anyway so you shouldn't
read stuff you can declare when you're
sort of checking the wire what they look
like and there is a very good Wireshark
inspector already that you can use to
inspect hb2 traffic just fine right
and there's there's a handful or two
handful of frame types and the most
important ones are the ones sort of you
know from from old HTP requests sort of
the headers on the body and they're
called headers and data so it's not
complicated even when reading the
Wireshark inspector output but anyway
then going back to the fact that this is
binary using binary and using frames
like this it means that we send a lot of
frames for each request and response and
that introduces a new interesting
capability done we don't need to do with
text wise we don't need to send the
entire thing at once we can introduce
multiplexed streams so multiplexed
streams basically means that we're
sending more requests and responses over
the same physical connection we don't
need many connections anymore so
basically we sort of negotiate with a
pair how many of these streams we can
have on the connection otherwise there's
really no limit we can just open more
logical streams over this connection
which then basically if I wanted to
illustrate it means that we have one
request then we have another request
they're going over the same stream no
sorry they're going to the same
connection and we don't have to limit
ourselves to just two but just to
illustrate the third one of course going
over this same connection
so ideally this means one disappear
connection per host and actually it
could be actually been one to speak
connection to multiple hosts to if if it
manages to coalesce multiple hosts over
the same connection
in reality though we won't really get
down to exactly one because we have a
loved one you know if you go to a site
with a browser you have a lot of ad
networks and you have other sites and
you have scripts from the air and wave
everywhere so it'll decrease the number
of connections quite drastically but
there will still be like 10 or 20
connections in a site today I mean the
effect of having these streams and one
connection it solves this problem with a
ping pong pretty good that as I showed
you before we have this guy is talking
through the cloud the cloud pillow and
this is time and now when we have these
independent streams over the connection
it will send a request and it'll get a
response like before but instead of
waiting for the response it can just
send another request while waiting or
while actually while sending the first
request it can send them at the same
time basically and maybe the other
response is to be slow it'll take while
that itself doesn't kind of stands in it
doesn't stand in the way for there's a
third request to also sort of get in
same speed and that response that might
be just faster so the third response
could very much very much just finished
before the second response it makes it
much better to utilize the bandwidth it
can really really saturate the bandwidth
if you're just asking for a lot of stuff
you will get a lot of stuff back as fast
as the server can deliver them it's sort
of offloads the responsibility from the
client to the server a little bit the
client can just ask for more stuff at
once and the server will deliver them
as I mentioned before when you had a lot
of different connections and you have
this head of line problem you you
browser would always be very careful to
put any requests in one of those lines
because as soon as you put the guy in
the line
you couldn't remove them so you would
always do it as late as possible and
that would be then that ends up really
hard and to saturate your pipe properly
now we really work on avoiding the
latency problems by doing this and since
we now have these logical streams over
that single physical connection we have
some other fun things that we can
actually set the dependency this stream
depends on the completion of an other
stream as a sort of a signal to this
server which stream is most important
here is the HTML more important than in
an image is the image more important and
CSS or whatever is this image more
important than another image maybe that
depends on the other image that can work
as a hint for the server if there's a
race here if you have two equal
opportunities to send both resources
which should you send to me first and
that also goes to these the fact that we
can now dynamically tell the server oh
sorry this stream I asked for it's not
as important anymore
maybe for example if you have this image
site when you have a really large amount
of images on and you know you pull the
scrollbar down to the bottom really fast
do you think it should continue loading
all those images at the top of the image
well the page for a long while no you
better just tell the server that no
suddenly now we want to see these images
at the bottom first so we can just tell
the server no no now these resources are
the most important ones send me these
first and also we have independent flow
control on each stream actually which
makes us that we can actually pause
individual streams in case we have
different consumers or producers of
those streams I would say that that is
is that is going to be most useful and
most notable when you're doing when
you're using it should be to as a proxy
protocol because
you have more independent producers and
independent consumers that is sort of
what we get when we have streams but
since we introduced by binary anyway and
we have another sort of interesting
nature with a Chaput HTP in general as
this we're keeping HTP paradigms here we
have headers you know cookies and
authorization and whatever and cookies
can be really really big and they're
sorry
heather's can be really really big
cookies in particular but headers in
general and and that they're really
repetitive
since we have HTTP is a stateless
protocol that means that the server
isn't really keeping a lot of state the
client is keeping a lot of state so it
sort of sends everything every time
everything every time that means in
every request ill send the same cookies
again and again and again and again well
maybe it'll update them a little bit but
mostly they're the same and you know a
good chunk of data for k8k
sometimes 64k almost the same every time
that sort of begs for compression not
the least because since we're talking
about TCP here we're doing HP over TCP
it's really important especially for a
new connection you want to have a lot of
requests going into the first initial
window of the TCP connection since you
have this slow start period when before
the TCP connection has gotten up to full
speed you want to cram in as many
requests as possible in the first TCP
winner and if you can compress the
request you can get in more you can
getting more of them in that first
anticipate window which then results in
more data back and you'll be faster up
to speed again sort of defeating the
latency problems and even the latency in
in growing the TSP for slow starts
period and right HTP 101 has no had a
compassion at all in
and one that one has data payload
compression so that the content
encodings and it can actually compress
data but it can't compress headers which
it should be - now Camden we introduced
a new compression algorithm for this
which why not right
no but it's most letters and attempt to
circumvent the compression related
attacks that you can do called bridge
and crime I think which basically it's a
way if you can man in the middle and you
have like JavaScript you can put in
known clear text just to see how they
tell us changes and then make guesses on
the contents so this is an attempt to
make it less it's an attempt to make it
harder to exploit it in such a way it's
in its separate or FC called
seventy-five 41 and as some news that in
HTP - that has not been in it to be one
that one even though they should be
proxy proxy ball back and forth we
introduced server push which is a fun
thing this is a guy again is talking to
the cloud right and it's in a typical
browser way he'll ask for HTML like yeah
that's the common thing right and then
the server I would yeah here's some HTML
for you and since it here the server
knows that usually if you're asking for
HTML you want something else as well so
it'll just deliver something else as
well while you're talking without the
client asking for it a push
you didn't ask for this but here it is
basically then that skipped the latest
you need to wait for the request because
the server thinks that the client might
need this this is a new thing as I said
and you know so how exactly how this is
going to work out and exactly how we're
going to make the best use of this I
think that's still
subject for experimentation and research
but you can clearly there are some good
initial tests and the measurements on
this that for some sites especially for
high latency connections you can really
gain a lot of speed for this I mean
speed until you actually render the page
for a client or browser in HCP to while
working on the protocol we of course got
to that situation when we were talking
about TLS HTTP should that be mandatory
or not right
it's TLS a good thing or a bad thing do
we want HTS everywhere or don't we yeah
those were the two teams and we were
fighting for a long time really fierce
actually I would say and a very long and
boring discussions that really never got
anywhere in the end so hb2 is now a
protocol where we sort of this side FTP
us or not exactly the way we did before
but so anyway if we're doing than TLS if
that is it has a whole section that says
yeah but if we're doing a GP - over TLS
over HTTP style we have a certain number
of restrictions or cannot we're trying
to raise the bar here we're doing TLS we
they should be - we only use TLS 1.2 or
later we don't do anything before that
sort of trying to leave the old stuff
behind a new clean slate at least
nothing earlier than that we have so
many mistakes so many bad stuff so we
should just forget about that at least
we're trying to so there's also a sort
of set of cipher requirements - that are
actually more blacklist you should never
use these ones and there's no
compression which also goes back to the
earth
there are numerous attacks on TLS
compression sure we should just not do
compression at all and there's also no
renegotiations in TLS which
renegotiations is more of a when it is
required to do client-side
authentication or client authentication
of a connection in TLS and that
basically doesn't really rhyme when you
have a lot of streams over the
connection because now suddenly there's
a lot of requests and doesn't know so I
want to then just go back and emphasize
was thi b2 is not and it's not then man
Authority less even though there's not a
single browser out there today that
supports non-https it be to sort of the
reality of the specification world and
the implementation world and the
implementation world where the browser
people are I mean the browser part of
the implementation world both Firefox
did I work for them and and chrome both
teams
very early on said that we only do it
should be to over HTTP and the there was
some other people that said different
but in the end now today if you're
looking at the browser's today there's
only it should be - over HTTP and there
are two reasons for this I mean days
that we're so going your GPS and that is
of course that the primary thing for me
personally I'm not speaking for for
anyone else so M HTTPS Everywhere is a
good thing we should go there anyway but
also then there's the practical side of
things that deploying a new protocol
over existing ports you know like doing
it should be - over port ADA them in in
the plane that is a riddle with problems
because we have a lot of stupid boxes
between us and decides over there and
they they think that if you can see
something that looks like HTTP over port
ID we can fiddle with it right can
change improve it fix it so it turns out
that if you're trying to do new
protocols over these old existing ports
you're up for for a tough time and a lot
of stupid things along the way there
break things for you so adding TLS to
connections actually helps protocols
succeed better so by encrypting things
people in the middle have a less way to
ruin the traffic and happy faces all
around and we're not changing the HTTP
headers so we have the cookies we have
these immensely huge authorization
headers still around this has the upside
on that if you're writing in application
you need the site you're writing a
client you're writing a server app or
whatever it's basically the same thing
you won't really need to care about
hep-2 you're sending a request and
headers you're getting your requests and
headers doesn't matter in most cases so
for users done it'll lead to faster page
loads it should be more responsive
loading like if you move the scroll bar
or do things you should also sort of in
the case I mentioned when you have a lot
of images you can actually start
downloading of more images earlier which
also kind of helps your experience of a
web page and there is going to be more
htps since you needed to be s to provide
you to be to more service I think are
going to provide a space to be able to
serve it should be - and the other way
than random how does it affect you or
anyone who develop stuff for the web
most things are just gonna be either so
and a lot of these funny things that I
mentioned earlier or in the beginning
you know spriting and inlining in
concatenation and stuff that's just bad
for HTTP to you rather want them as
individual components it'll make it a
bit too much better and also sort of
initial tests I've done with just
converting things from HTTP one to hb2
world they're in it should be one we
have a lot of different things when we
have loaded scripts that load scripts
that load scripts in a sort of bam bam
and one step at a time
and that turns out to be also be really
bad it's better to just load everything
at once we there should be - you don't
to just to defeat the latencies much
better so we might need to do separate
tweaks than for HP to because I imagine
that there will be people that are still
wanting to provide HTTP 1.1 tweaks it'll
take some time there are a bunch of
existing implementations already for
Asia Picchu and this is just a list
actually it's quite an old list by now
but there's like 40 or 45
implementations you don't need to even
bother about that list there's a link in
the bottom you can see if if you want to
study this in detail and browsers there
are a few browsers that have said and
are implementing this I would say that
this covers most of the browser's you're
using right now oh right I'll get to
some numbers soon so I'll just mention
that it yes it will be wide Aleppo
deployed just no questions asked no but
I'd like to see this when I when I talk
about the new protocol people always
throw this at me right
what about ipv6 right deploying new
protocols on the Internet
this is a completely different paste
this is a protocol on top of TCP it's
much easier to deploy done in a
completely different way it's not fair
to compare to ipv6 at all so let's not
do that all major browsers are on on
this a lot of sites already did speedy
this is quite a similar thing and we
have Apache 2 dot 4 dot 17 is going to
be released they said within two weeks
last week so I would guess that is like
in a week 10 days with one week a few
days days they told me and that is going
to provide the mod HTP 2 which is the
Apache version of supporting upto and
nginx already released a version like
two weeks ago or something like that
with full release version of their HTP 2
module so then that makes it so if we're
then looking at what we're having right
now on the Internet today all browsers
only doing this over H
yes even though someone said that they
wouldn't act Firefox in Firefox we have
this telemetry system that if you can
opt in to delivering data to Mozilla and
we cannot crunch that data just to see
how people are using all sorts of
different things and we gather HTTP
response data 14 percent of Firefox 41
requests get HTTP 2 responses back over
that is on the total HTTP traffic that
Firefox uses fair amount that's like one
in seven right almost
that makes it thirty percent of all HTTP
is already it should be too because
roughly 47% of all requests or yes
already I would say that is quite a
large degree it should be two is now 15
times more popular than the 1.0 version
which to be fair is really nobody is
using it well somewhere we don't really
know we don't gather URLs or host names
in this telemetry system for privacy
reasons so we don't know who these are
but they exist and these are I believe
that the total number of requests were
looking at here or in the in the region
of 100 billion requests or so so there's
a fair amount of data here it's not
really an active data Google servers
well I've asked them and this is a
fairly old number so they said 18
percent awhile ago and I would take that
with a grain of salt because they have
also have a lot of other experiments so
for them HTP 2 is just one of the
multiple experiments so that might be a
bit I talked to a guy that runs the
infrastructure for Apache org and the
work in the the the Apache traffic
server
server which then is a server and they
have there is that there are now over
40% HP to traffic so it's going there we
have now it should be to support in
roughly 70% of all browsers in use right
now which is I would say it's a pretty
good number right and the unfair
comparison try to b6 they are on 9%
right 20 years after they are see the
first are see we are 144 days away from
the first RFC away should be two that is
sort of where we are today so if we're
going to if I am going to make some sort
of wild guesses about the future about
this this being HTTP or transfers for
the web or something like that I would
say that with this I didn't really get
into exactly how we negotiate HP to over
connection but figure that was a bit too
technical we can take that over coffee
after this but hb2 has laid the
foundation for negotiation negotiating a
new protocol version with this existing
URL so it makes it really really simple
to now introduce a new protocol I would
say that this has an excellent
foundation just to build upon and now we
can do experiments and we can do it it's
right much faster and better going
forward there might be some time soon
now later to drop some of those really
crappy is to p1 legacy things I don't
know the cookies - Heather's blah blah
blah there's a lot of things that we can
work on we have a lot of we have a
pretty good community within the IETF
and HTP working group and we have I
think we could produce a list with 150
different things which should work on
could work on going further so there's a
lot of ideas and a lot of brains
spending a lot of time and effort to
take this further so I think HP 3 might
can will happen much faster than HP 2
did
fifteen years or 16 years or 19 years
that's just too long I think number
three is going to happen much sooner
than that from now on
probably this I would say http/2 it has
the effect that some people would say
that we're merging layers in it in the
networking stack like we're now doing
the multiple streams of the connection
and we're doing the transfers and that
is sort of as quick in which is another
Google experiment they're doing that
even further and I would say that is
sort of a clue of where we're going next
I think we're going to do more of that
sort of experimentation and I suspect
that the future of hp3 is going to at
least contain bits of that that style of
thinking or style of designing pearls so
just to wrap things up done this is
binary and multiplexed many connections
at one and we're primarily going to see
this over TLS we're seeing it already
over TLS primarily and there won't be
two anywhere nobody will see it - we're
already using I bet most of you are
already using HTTP - already at least on
those big sites called Google and
Twitter right it's already here that's
about what I was going to say I wanted
to mention also that ilya grigorik has
written a great HP - section in his book
high-performance person networking
that's it goes into far more details
about HP T and I've written this
document with which is available online
which also sort of gets into all these
details slightly more than I did right
now thank you
okay
I caught a few questions during the
session so there's a question about H
Peck so H pack introduces some state on
the server how could the server handle
the state when does it expire
how about scaling and stuff like that
the yeah each pack is compression on the
connection so it is a kind of a
connection related context so yes it is
added state in the server protocol power
connection I don't I'm not sure what
more I should the answer to that yeah
they're they're state but you're
removing basically five connections to
so that's a lot of left state in that
aspect another question
doesn't the flow control feature of HB
to have a negative impact on net
neutrality and might be misused that
would be really strange since I mean
we're talking about point-to-point
communication here so your your client
and your server and we're delivering
stuff whoo I don't see where net
neutrality comes into the picture here
so I mean net neutrality would be
someone someone saying something is less
important but these are trying to server
they were talking to each other there I
mean it is not like this isn't already
going on this is now just more made
explicit and more specified we were
already doing things in the background
we're trying to do things more important
less important so I don't see how this
is related to natural neutrality at all
good maybe I'm stupid and I think one
last question through to place our
connection and data push
sounds like WebSockets so tools HTTP to
complements or replace reps
did I mention WebSockets no no there's
no web sockets for HTTP to and really
that's an entire different separate
argument but WebSockets I don't know how
many people know about all those funny
things about web sockets but WebSockets
was sort of a workaround to address a
problem that you really couldn't do good
with HTTP 1 and and going to HDPE to you
can really go back to do the old-style
how you could those do those long
polling gets we there should be two
there's really no extra cost for that so
the the primary thing to do with it
should be to is go back to how you did
it with it should be one before we
introduce WebSockets so know if you're
if you're if you're married to
WebSockets today you don't use hb2 until
you have a solution for it ok I have one
more question actually from the from the
crowd can hb2 be made to run on UDP to
avoid the TCP handshake a bit like no
hdhp to is made to be run over TCP then
we're talking if you're talking about
UDP we're talking about something else
so yeah you could of course make your
own experiments and as I mentioned quick
already quick is basically that is an
attempt to increment everything over UDP
so yeah there and there are other
experiments to do protocols the entire
stack basically over UDP so I'm sure
that we will see more of UDP experiments
going forward but that is not I should
be too as if ok thank you
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>