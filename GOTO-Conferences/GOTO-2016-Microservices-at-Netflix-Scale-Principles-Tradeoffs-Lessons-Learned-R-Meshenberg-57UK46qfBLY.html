<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Microservices at Netflix Scale: Principles, Tradeoffs &amp; Lessons Learned • R. Meshenberg | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Microservices at Netflix Scale: Principles, Tradeoffs &amp; Lessons Learned • R. Meshenberg - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Microservices at Netflix Scale: Principles, Tradeoffs &amp; Lessons Learned • R. Meshenberg</b></h2><h5 class="post__date">2016-09-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/57UK46qfBLY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you and thank you for coming to
the stock talk is titled micro-services
an atlas scale we'll talk about the
various definitions of micro-services
and we'll talk about netflix scale but
first I'd like to start with asking you
a question did anyone make a large
purchase recently a house expensive car
alright Jeff you how many of you made
that purchase just looking at the
features the benefits and not looking at
the price alright good so the analogy
here is microservices is clear it's in
many talks and many seminars you go
micro-services pitched purely for their
benefits but that's only one side of the
story these benefit comes with certain
costs and so today what I'd like to talk
to you about is Netflix journey to micro
services benefits and reiterate some of
them costs lessons learned basically the
best practices and anti-patterns we
discovered through our journey to micro
services and some resources that I'd
like to make available to you
first by way of introduction Netflix
with over 81 million subscribers all
over the world we're probably the
largest internet television network and
we allow people actually people enjoy
over 125 million hours of TV each day
that translates to a lot of bandwidth
we'll talk about it later
what I do at Netflix is I run several
teams under the umbrella of platform
engineering if we effectively make the
Lego blocks that all the rest of the
engineering teams at Netflix use to
build their own application so we create
that common layer to enable teams to
move fast
Netflix rounds and microservices that's
our own in fact but what is not like
scale so 125 million of ours watch per
day translates to over 4 billion our
streams each month
in other terms that translates about
one-third of North American downstream
traffic on average that's a lot of bits
at any point in time all this traffic is
supported by over 500 micro-services the
reason I say over 500 simply because we
don't know how many services are running
at any given time at any given day we
have maybe between hundreds 2,000
production changes new services of being
deployed existing services being changed
some services of being retired there is
no central system that gates it all
because we want people to to be able to
move fast
all these services we have a
aspirational goal of four nines of
availability the more more closer to
reality will be three and a half nines
of availability and we'll talk about
that and that runs across three AWS
regions two in North America and one in
Europe across nine availability zones
and we'll talk about why all of that on
a people side that so effectively
represents a company of about 2,400
people about half of them are technical
so about 1200 engineers working all the
services so let's start with the journey
how how did we get to this point because
we weren't always in the cloud weren't
always in micro services first of all
our journey took seven years it didn't
happen overnight we started back in
August 2008 and just finished earlier
this year so it was a long journey and
our journey to micro services was not
really a necessarily journey from
monolith to micro services per se it was
triggered by our need to move into the
cloud and for various reasons we blogged
and talked about it at numerous places
are not going to reiterate there but
effectively the only way for us to take
a very big complex system and take it
from datacenter into the cloud we
couldn't just forklifts we couldn't just
take this big complex fragile system
move into a cloud and say we're done
we had to chisel it piece by piece
effectively micro serviced by micro
service and then deploy it in the cloud
we architected redesign it reimplemented
and
in a cloud so what did it look like
before in our own data center we had a
monolith there was this application that
all the various teams contributed jar
files it would get baked into a war that
war file would go through your normal
bi-weekly test release cycle or the
release train cycle and eventually get
deployed and on on a database side it
was a pretty big our DBMS Oracle DB in
particular that would take all the
traffic and you can clearly see what the
problems with this are besides the
velocity we'll talk about velocity
separately but even from reliability
perspective this is very fragile any
issue that you introduce to your
application whether it's instant or
latent will be homogeneously represented
throughout your forms so if you put a
bug into one machine you put a bug into
all machines in addition this database
does represent a single point of failure
so in a previous slide I mentioned
August 2008 this is exactly what
happened in August 2008 we have a
database failure our main production
database got corrupted and all of the
Netflix users most of them were not
streaming at the time they were still
mostly getting dvd-by-mail got this ugly
message sorry we're working on fixing
our problem and we'll get back to you
that took four days
imagine having a four day outage that's
that's not pretty so that was sort of a
triggering event that led our
transformation into micro services the
leather transformation and to being in a
cloud only and I'm really happy that
transition is done before we get into
the application and practice let's talk
about the first principles effectively
the assumptions that we all made once we
started chiseling away those micro
services from them monoliths application
we had to make certain assumptions we
have to prioritize for something so
let's let's quickly go through what
those assumptions were and we'll also
talk about why we made those assumptions
because again depending on what you're
trying to optimize for DSM
Asians could be different and I didn't
want to pitch you a particular solutions
or frameworks without understanding
first what those assumptions are so
first of all we really don't like NIH or
northern method here if we faced with a
choice between building something or
buying something and just to be clear
buying included it doesn't necessarily
limit search to a vendor software it
means just using off-the-shelf
open-source software we prefer not to
build it if we can leverage and
potentially contribute to something from
open-source we would rather do that and
so we we only reserved building
something in-house for the cases where
there is no other solutions for that
that will work for us particularly with
our scale we've tried many solutions
that unfortunately ended up not working
and ended up being building a lot of our
own software anyway but at least we
tried not to most of your services as
you breaking out your modulus should be
stateless with the caveat of course
except for persistence and caching layer
what that means is you shouldn't rely on
any sticky sessions if you operate any
particular instance or node dies you
should be able to simply retry it hit a
different instance and proceed on with
your execution it's not enough to say
sure okay well we'll do that you have to
prove it and the way we prove it at
netflix is by chaos testing case monkey
and alike we'll talk about that scale
out versus scale up you have two choices
when you need more resources one you can
go to instances or nodes with more
resources whether it's CPU memory
storage network you can always get
something that has more until you can't
until you're running on that instance
that has the most cores available that
has the most disk available and what you
do done you architectural II paint
yourself in a corner so what you'd
rather do is when dealing its scale you
want to scale out it's so much easier to
add new instances and especially in the
cloud because
nice benefits of elasticity not infinite
elasticity but else this is nonetheless
redundancy and isolation we'll talk
about it when we even we talk about
resiliency but it's really important
redundant it and these two principles
are really common sense but redundancy
you make more than one of anything you
simply don't want any single point of
failure
even though you make more than one of
anything you still need to isolate the
blast radius for even given failure even
if you have 150 nodes in your cluster if
one of the failures can result in
cascading failure throughout the whole
cluster you haven't isolated that
failure and so it's important to do that
and of course the distractive testing
what you want to do is you don't want it
to be a one-off you don't want to just
do it once a quarter once a release or
once whatever cycle like you want it to
be constantly running you want to
constant constant ly prove that your
system can withstand the failures that
occur now it's not if the failure occur
failure will occur it's when it will
occur and chances are Murphy's Law still
working last I checked
failure will occur in the middle of the
night on Saturday night where most of
you engineers are asleep or drunk or
both and that's not the time when you
want to wake them up and have heaven to
deal with a failure and so we actually
put a lot of thought and practice into
running destructive failures all the
time during the office hours where
engineers are there they're caffeinated
the fully alert they can deal with
problems right away so it started with
chaos monkey probably our most famous
member of the seminary but it actually
grew into much bigger thing it was the
whole principle of being able to
destructively prove that our systems are
resilient so how do these first
principles all these assumptions work
work in action well we talked about
stateless services in our ecosystem in
order for a stateless service to be a
good citizen effectively it just needs
to do a few things it needs to register
into service discovery
and depending on your ecosystem service
discovery may be anything it could be
DNS it could be your own service
discovery anything it needs to implement
a externally callable health check again
whether it's your service discovery or a
load balancer or other external agent
can verify that your systems are not
just there but they actually up and
running and they're functional
and in order to call other services it
needs to utilize that information it
gets from service discovery of where the
other services are located simple right
right until it fails so when it fails
all you need to be able to verify is
that the calling services of this
particular diagram service a just needs
to retry that request get a different
instance of service B get the same
response and then move on in order to do
that you have to kill an instance of
service B or introduce a network
partitioning event between a service a
and instance of service B and then
verify that the results are the same you
have to do it in production that's the
catch I've seen tons of instances where
developers and companies claim that yes
I've done this testing I've done this
testing on my development machine or
environment have done this testing in
test or QA environment and yet in
production the system's failed in
different way so if you really want to
prove to yourself that you're stateless
services are truly stateless do this in
production you don't have to do it
overnight you can build confidence but
eventually if you really want to do this
you got to do it in production chaos
monkey I mentioned it already that's
what it does it effectively a monkey
running around datacenter a virtual
datacenter in our case but datacenter
nonetheless and killing instances
randomly we run it Monday through Friday
9:00 to 3:00 and each application is
subject to it initially it was just
stateless only now it's stateful
applications as well and you can tune it
basically you can you can tune it from
doing it infrequently or maybe once a
day to if you feeling more bold more
frequently and see
your system reacts data so I mentioned
in our data center environment we had
this big our DBMS instances running
Oracle DBS when we started migrating to
micro services to to the cloud VM brace
Cassandra as our main key value storage
and for several reasons one it it
actually runs at scale it's one of the
biggest scale and no sequel engines it's
open source and that was important to us
because very early on we need it to and
we in fact contributed quite a bit to
Cassandra in order to make it
multi-regional multi-directional
replication ready and in terms of the
storage engine from perspective of cap
theorem it's available it tolerates
partitions well and it has to loop the
tunable consistency now in many talks
about Cassandra you'll hear that its
eventual consistency engine that's
actually only a part of the story with
Cassandra per request whether you're
reading or writing you can tune the
consistency of that request from CL one
which is pure eventual consistency to
local quorum all the way to the global
forum I don't recommend global quorum
when we'll we'll go in in those reasons
so this is actually how Cassandra
multi-regional replication works on one
side your client let's say write with a
local quorum that means you're going to
get three replicas locally within the
region before getting an ACK before
confirming that the write has been made
but what you don't want to do is
tolerate that confirmation over the long
distance network across the region's
your region right simply will be too
slow it's not going to scale so what you
want to do is you want to replicate the
data across regions asynchronously and
that's actually a change that one one of
our commuters contributed to Cassandra
very early on many years ago and now
it's default in the distribution so we
talked about stateless services we
talked about stateful services there is
also issue of billing services which
probably composed of both but the deal
with money
so you have to be a little bit more
careful and that's actually that's the
service that took us the longest to
migrate to the cloud to break into micro
services and for a good reason we don't
want to overcharge our users we also
don't want to under charge our users we
want to do it just right so you know
also in order to be compliant with Sox
and PCI and
in u.s. we had to put these services
into a separate account with a limited
access it's not as fully open as all the
other production systems that Netflix it
should be fully logged and auditable and
and the key key there is is just just to
be a little bit more careful about what
you're doing okay so I will go very
quickly through through the benefits of
micro services because I'm pretty sure
you've heard many of them already many
times but I think some of them worse
repeating but before that what are
Netflix priorities what are we trying to
optimize for well first and foremost we
want to optimize for velocity for
innovation for us it's really important
that all of our teams that are working
on Netflix product move fast and because
of that we're actually willing and so
far been able to to sacrifice a little
bit of reliability that's why you see
reliability is priority number two SSL
is may sound in order to achieve that
high velocity and efficiency is only a
third on that list so when we were
making the straight A's when we trying
to decide which way to optimize we
always optimize for innovation first and
that's exactly the challenge that you
have in the typical model of systems and
release cycles of modular systems you
have your teams that are producing
various components they're all
developing eventually they submit their
development results into a test cycle
for train release and eventually after
the QA signs of that gets released that
creates a lot of tight coupling between
the teams and that just doesn't work if
you're trying to optimize for velocity
it's too slow loose coupling is when you
have each team working independently
that means each team needs to now work
all of the parts of the cycle they
develop the architect the design they
develop they test they deploy and they
support in other words its end-to-end
ownership now in this sometimes it's
called DevOps model I'm trying not to
use this word because it's getting
overused to the point of being a
buzzword but if you think of end to end
ownership your team is responsible for
everything from cradle to the grave and
this way their motivations are set up so
they write quality code but they also
write it fast because engineers
especially great engineers are motivated
by the results that motivated by the
impact that they make and if each team
owns the full cycle you'd get something
like this where all of them are
constantly making progress
all of them are constantly doing
something if you think of it in
technical terms imagine H team as a
thread and all of them are running
independently in parallel without a
single gate to block them to impede the
progress all these teams that Netflix
runs they very loosely coupled we don't
have a central place to gate or approve
their releases they each move on their
own cycle at their own cadence you also
get the benefit of separation of
concerns for example the lowest layer
the infrastructure teams that I work
with the mostly concerns with
availability scalability security the
fundamental qualities that each
application should have so then the
teams who build their applications out
of a Lego blocks that we provide don't
have to reinvent that feel they can
build on top of it and leverage what we
already have done enough about the
benefits though let's talk about the
cost nothing is free first and foremost
microservices and of the nor change if
you want all your teams to embrace the
full cycle that means you no longer have
necessarily a QA team you no longer have
an ops team
now who wants to hear that the job is
going away or changing right these are
the harder things these are the people
things emotions are involved work
changes are hard and so what you what
you need to do is you have to evolve the
organization over time gradually you
won't be able to do it overnight and
just to give you a couple of examples of
how your practices may change you can
depending on what works for you in our
data center environment many years ago
we had a centralized NOC that was
basically approving and driving all the
releases right we had a IT group that
was responsible for capacity and
budgeting and planning and executing
that capacity we had DBAs who were
effectively gatekeepers to this big our
DBA message that we had we no longer
have any of that
now if the developer our team needs
resources the self provision it through
the tools now because they do it through
the tools it's all transparent
everybody can see what's going on and
the people who concern themself is
capacity they can see a real-time signal
of where that capacity is needed we
don't have an ops team we have an SRE
team that builds centralized set of
tools that everybody can use leverage to
operate their own systems but they're
not gating anybody and the DBA is they
work with other teams to work out what
to see what schemas for their databases
would be optimal but we don't operate
anything for anybody so it's a lot of
centralized teams who build a support
models who build the frameworks for
other teams to enable them to own the
full full lifecycle again it's
effectively building these Lego blocks
but in order for you to reap that
benefit you have to invest you have to
build the teams that will build these
Lego blocks here comes the catch you
have to do it while still running your
old stack and because migration doesn't
happen overnight you're going to be
living in this dual world for for a
while and at Netflix with curvy
this term that if this image represents
Roman writing that's when you're on the
right two horses one leg at a time
this guy looks very uncomfortable you
think about it you supporting two texts
X let's double the bugs double the
maintenance sometimes you have to
propagate new features in two places you
have to replicate data and multi master
data replication especially at scale is
no picnic no matter what what great
engineers you have and how much testing
you do it's not going to be foolproof
and only once you switch over the source
of truth to the new state and stab that
multi master data replication you will
be able to breathe a sigh of relief
until then you're going to be constantly
fighting some kind of battles so what
kind of lessons we learned through that
through that journey first and foremost
that there are a few pieces that are
critical to ensure this loose coupling
now you talk about loose coupling
between the various teams developing
micro-services there are really two
pieces that are needed when I covered on
the slide which is IPC or IPC RPC again
interchangeable terms
but you effectively want to establish a
contract or a language between any two
services to talk to one another
this way when Tom comes to develop
develop a new system you already know
which language to talk to all these
systems and if you have 500 plus micro
services running you don't want to know
500 different languages it just simply
doesn't scale so IPC is one of those
pieces the other one equally is
important you want some homogeneity you
want some consistency in how your
applications are being deployed for us
initially it was Asgard that we had to
develop in-house and then we recently
replaced it with another tool that we
also open source called spinnaker that
automates all of our deployment
workflows the benefit of that being
again the same central tool is that
people don't have to reinvent this wheel
already and we also get a source of
truth point of view from what
applications were deployed when and how
n
provides an invaluable insight in a time
of crisis one of the first things you
want to see when something goes sells is
what was changed most recently databases
it's one thing when your database gets
called by one or two monitors
applications it's completely different
when suddenly it's get pummeled by
500-plus micro-services you want to
protect your databases and so a pattern
that we evolved into over time is that
for most heavily hit databases even
cassandra has its limits we protected
with a layer of cache and so we use
actually two different cache caching
technologies within Netflix one is the
key value add on the build on top of
memcache called Eva cache and the other
one is a richer schema that you could
use Redis for it is called dynamite but
the idea is the same what you want to do
is on the read path you want to hit
cache first and go to database only on
the cache miss and if you go database on
the cache miss on the way back you're
going to backfill the cache you also
want to make sure that in your call
graph right at the top of that call
graph you will have a certain request
cache so then for example if in the
beginning of the call you had to fetch
the user information to understand some
metadata about that user you will
propagate it downstream so all the
subsequent services don't have to go and
call that database again to finish the
same metadata over and over and over
again you will find that you will reduce
the load in your databases by at least
one order of magnitude maybe two maybe
three depends on your call graph depth
and current interconnectedness telemetry
operational visibility matters a lot if
you run your monolith you probably have
some good metrics of what kind of
application and system metrics you look
into to to understand your systems
health when you run hundreds of micro
services will your telemetry scale how
many grafts are you looking per server
multiply it by number of micro
services that you have will you be able
to see forest from the trees per server
how much individual human action is
needed when something goes wrong how
much of that are you able to automate
just to give you an idea at Netflix we
generate over 20 million metrics a
second on average that translates to
about 1.7 trillion a day that is simply
unsustainable for anybody any human to
look into all of those metrics and
actually be able to get some signal from
that noise and so most of these metrics
never get looked at that is by a human
they get piped into automatic sorry
automatic error detection algorithms
they get piped into automatic
remediation algorithms that will detect
if there is an anomaly and in many cases
will correct it without human ever ever
being involved the same goes to your log
analysis tools right if you generate
logs like like us a couple of years ago
my ex-colleague coined the term that
Netflix is a log generation service that
also allows you to watch movies because
we just produce such an obscene amount
of logs and so you need to be able to
find and hone in tools that will allow
you to to get that signal out of all
that noise and preferably automate as
much of it as you can so this is what it
looks like once you get into this scale
of microservices you simply don't have a
luxury of having architectural diagrams
because things change all the time and
so what you'll need is a runtime at
runtime you need to be able to discern
who's calling who how where are the
errors various traffic flowing is there
any congestion into a system it may look
something like this it may look
something different in this particular
case you see that traffic starts at the
elastic load balancer for us then it
gets spread through our front line of
defense called Zul which is our front
end proxy
then your calls may go into playback
systems API back-end and this area
comprises our edge systems this kind of
front line of defense
from then on you have middle tier
services platform services like caching
and databases and so on and so forth but
this type of insight this type of
telemetry you have to generate at
runtime because if you just create a
static architectural diagram with the
next service persons going to change its
going to render it obsolete reliability
it matters a lot especially at scale
failure happens in distributed systems
and the rate of failure is proportional
to the amount of change that you're
pushing through in a scale that which
you're running for us bows are massive
and so we strive aspirational we strive
for four nines of availability
unfortunately that only leaves us 52
minutes of downtime per year up to now
we haven't been able to consistently
achieve that now in the Netflix these
days when Netflix is out that causes
some disappointment outrage or
withdrawal
luckily some people actually do retain
their sense of humor and react a lot
more positively
but availability is important as much as
we'd like to focus on innovation and
velocity only you can't do it at least
for long if your system becomes
unavailable people just stop using it
now in distributed systems especially
when you have large number of micro
services you have to face the fact that
you'll see some cascading failures if
you test each individual service micro
service that you're right and each
individual service reliability is two
nines well it's not great but it's not
bad but if your graph is fairly
interconnected and you have over 500 of
them the total availability is not going
to be very good in fact you're going to
be out of service most of the time
that's not acceptable so what you want
to do is you want to detect and correct
these failures as fast as you can thus
comes the discuss the whole concept of
circuit breakers because circuit
breakers detect the fault in your
electrical systems and trigger a
fallback in electrical systems you
should simply switch the power of but in
software we have more options if you
detect a failure in a downstream system
what you don't want is that that same
failure to be propagated eventually to
the client because that that's not very
helpful even if you propagate that
failure fast what you want to do is you
wouldn't you want to detect that there
is a problem and figure out whether the
problem is with a critical service which
case gave us over or non critical
service and if it's a non-critical
service like for example it Netflix it
could be a personalization may be for
whatever reason we cannot give you the
level of personalization that you like
to do well you still can probably would
enjoy browsing and selecting a from a
set of movies and TV shows that's not
necessarily personalized to your maximum
liking probably we'll still find
something good to watch so what you'd
like to do you want all those failures
to turn into fall backs as immediately
as you can in this way you can still
operate well you're the team responsible
goes to debug and fix the problem
so a few years back we actually open
sourced a library that enables us to do
just that it's called hysterics and it's
been used by many companies we've been
getting really good feedback and
contributions of that I can't emphasize
this enough you have to do distractive
testing failures will happen and you
only know how your system will react to
the failure if you inject it you got a
do it in production I get not to start
with but eventually if you want to prove
it you got to do it in production
because failures happen not just at the
system or cluster level but across those
clusters in the network level some
failures are more insidious than others
if something fails it's actually a lot
cleaner you know it's broken you know
you're getting error or mangled response
back you can deal visit it's a lot more
insidious when the response is returned
just too slow too late right at the
point where it's going to trigger your
timeout imagine good cause cascading
timeouts that's just not a not a pretty
scenario and so we actually develop a
system internally known as fit where it
will be able to inject per request in
individual call paths certain faults and
the faults could be errors or they could
be Layton sees or combinations thereof
and when all else fails you got a
trigger failure on more massive scale
it's not enough to trigger failure an
individual instance or zone or even full
cluster at Netflix we actually trigger
failures exercise it in production on a
monthly basis at least for a full
regions so all of our services run at
three AWS regions across nine zones at
least once a month we randomly select
one of the regions and we evacuate out
of it we basically simulate that if that
region would fail what would our users
see in our goal which happily to say
right now is the case when we evacuate
out of a region as a user you will not
see a thing
Netflix services will continue working
as if nothing happened it wasn't always
like that the first couple of times we
run the simulation we call it chaos Kong
things were not as fun we had about 40
engineers
in a war room scenario debugging for
about four hours trying to figure out
various things that went wrong and that
we fixed those things and they tried it
again then it was 10 engineers debugging
for about two hours and now it's
effectively a piece of animation a piece
of script that runs at monthly intervals
most of Engineers don't even know when
it runs because the results are so
transparent it takes a while together
but once you get there the rewards are
pretty damn good so that's that's the
graphs of us in this particular case
evacuating us east into us West and EU s
regions you will see that as the on the
bottom as the traffic went out to the
other regions the overall traffic did
not change or as this much cooler
illustration shows this is what a lot
faster timescale replay what exactly
happened the traffic that you see
emanating from the middle circle is the
traffic from our users from the internet
the traffic across the edges that's the
traffic between the regions and you will
see that right now we triggered a
failure in this region right here in
u.s. West - it turned red and we just
started proxying traffic to the other
two surviving regions and as that
continues to to ramp up speed and scale
at certain point we will flip the NS
switch and you will no longer see any
traffic hitting us is us worse because
all the older traffic has been fully
redirected to the other regions this is
the big hammer takes a long time
together but if high availability is
your goal you're going to have to
exercise something like this not
necessarily this it's whatever works for
you ok no token micro services would be
complete without mentioning containers
its latest and greatest version II but
let's get down to basics containers
don't make microservices containers
change the level of encapsulation of
isolation from virtual machine to a
process containers bring you great
benefits specifically for developer
velocity you can iterate on a second
length cycles you get the same artifact
that you round your development machine
the same as in production you can do a
lot of really cool magic with containers
is actually the first stock this morning
showed but it's not a silver bullet and
so make sure that you're using it
because it's a right tool for the job
besides to run containers its scale
would require something like this
this slide is intentionally meant to be
an information overload I'm not going to
dwell on any details here but it does
require a very significant and complex
systems now Google has done amazing work
of scoober natives it's matured quite a
bit over the last couple of years if you
think enough containers use a
foundational blocks that are available
to you don't write your own
unfortunately for us we started a bit
earlier where a lot of these blocks were
not available or we're not available at
our scale and so again our preference is
not to build something we can buy in
this case we actually ended up building
a lot of pieces in our container run
time called Titus we had to write our
own custom scheduler and a whole bunch
of other things simply because the the
ecosystem were not Ray was not ready yet
for our scale so let's talk about some
resources that are available for you or
you can think of it as a commercial
break from the regularly scheduled
presentation because we by being pioneer
into the cloud so early we ended up
writing a lot of things anyway and
because we ended up writing a lot of
these kind of foundational pieces we
open-source them and so they're
available to you if you go to Netflix
that gets a github comm you will find
most of our infrastructure pieces the
tooling pieces that are available to you
to use and contribute if you choose to
it and they separate into major category
so it should be fairly easy for you to
find anything that you're looking for
they don't try to read the small small
print you can just go to the site and
it's all it's all there but the main the
main pieces that you could benefit from
spinnaker is the tool that the open
source that works for continuous
delivery workflow deployments
consistently throughout any your
stateful or stateless applications for
the common runtime sharing libraries if
you need service discovery for example
we have Eureka that you could use there
are many other alternatives as well and
there are a few IPC components there as
well for data persistence we open source
and contribute components to Cassandra
ecosystem components to read this
ecosystem if you need to make Redis
distributed we have the component
dynomite Eva cash for the memcache
client replication and such for insight
again if you need the telemetry for
distributed systems at scale large
pieces of it are available you can plug
it into your ecosystem security matters
a lot and probably the last thing you
want your teams to keep reinventing is
security pieces it takes some skilled
security software engineers to write
good crypto to write good secure systems
that will not fall down on their basic
attacks and what you'd like to do is to
provide these either services or shared
libraries so your application engineers
can focus on the application logic best
their business impact it's not
reinventing security so to wrap up micro
services are good they bring great value
to development velocity availability
many other dimensions but they're not
free micro services at scale first and
foremost requires organizational change
and centralized infrastructure
investment if you want to do it right
and it's up to you when you want to make
that investment just keep in mind that
if you have 10 teams working on 10 micro
services and you want to introduce a
certain centralized change there is this
much amount of tax to eat all 10 teams
will have to pay if you want to do it
later and perhaps this 200 teams writing
200 micro services your centralized tax
becomes a lot larger and so optimized
accordingly also I can't stress this
enough don't do something just because
we did it what worked for us may not
work for you
so be aware of your situation and what
are the right trade of September the
optimizations for you we're happy to
share our tools but the only work for
you if you're making the similar
assumptions and similar trade-offs as we
do I hope this was helpful I can take
any of your questions now
thank you very much there are some
interesting questions from from the
audience and actually one of them caused
a little smile on my face analog to the
falling tree in the forest if chaos Kong
kills stuff and nobody notices it notice
is it how do you know it actually
happened excellent question
so just because just because our users
may not be aware that chaos Kong killed
something our telemetry is we have our
telemetry and logs and obscene amounts
that pretty much tells us everything
that's happening within our system now
of course the most obvious side-effect
that we never want to see but you've
seen it from time to time is when chaos
Kong causes user impact but this is
actually a failure that we can learn
from and make our systems more resilient
the tougher situation is when kill
something and users don't see anything
it actually takes discipline from all
the system developers involved to look
at their log of their system health
understand how the system's fared under
the evacuation situation under the
latency situations and make sure that
there is no bottlenecks that were close
to reaching that there is no failure
that we just degrade it out of in
everything seemed peachy but it wasn't
so there is a little bit of follow-up
that happens internally but first and
foremost of course when we run Kong is
that there is no user impact and who or
how do you decide what services and the
steams need to be created or go away is
that a central architect or excellent
question so we don't have a central
architect they don't have architectural
committees we really try to embrace
culturally this whole concept of loose
coupling and individual or in this
particular case team freedom and
responsibility so each team is free but
also responsible for making these calls
when new services need to come into
operation when certain services need to
be retired and how to deal with the
migration strategies
I think someone is afraid to do the
chaos testing in production so I think
that's why this question came up would
you also recommend destructive testing
on production on more critical
applications for example financial again
it depends what you're trying to
optimize for it's a really good question
but at certain point you need to embrace
the fact that failure will happen and if
you don't test it it's going to happen
anyway what you're what you're gaining
on running the destructive testing and
yes in production is your understanding
of whether your technology is ready to
deal with these failures but more
importantly are your people ready to
this to deal with these failures for
example will they have run books or will
they have to scramble do you have enough
people who understand your systems do
you have enough people who know who to
call who understand these systems these
readiness you can do simulation you can
run the drills but nothing beats running
it in production and if you want to do
it in production but on a copy of that
let's say because it's your billing
system in a shadow mode that's fine too
but you want to get as close to the real
thing as you comfortable with please
don't do it just because Netflix does it
we get comfortable with this over years
so to do a fire drill you don't really
need to put the building on fire that's
what you're saying probably not now a
question that has popped up more than
once so I really have two asset where
can we get those awesome service
visualizations so the service
visualization that I used here in videos
are called flux and flow they haven't
been open sourced yet but I use the
keyboard yet because our philosophy is
generally we want to open-source
anything that's not proprietary critical
to the business which these are the
pieces of infrastructure or not we'd
like to share them at the same time I
can't commit on behalf of those teams to
any particular time line because
business priorities always come first
all I can tell you that we'd love to
open-source and we'd love to share it
with you and actually get your
feedback and contributions I just don't
know how soon or how not soon it's going
to happen I guess we have time for a
wrap for one or two more what kind of
discipline do you ask of your team
members oh that's a loaded question
nothing really in particular but if you
haven't seen our culture that I highly
recommend you go see that's available on
SlideShare on our site if you've seen it
this whole concept of freedom and
responsibility the type of developers
that are hired and become successful at
Netflix really embrace it and it doesn't
mean that these people don't make
mistakes it means that they'll learn
from their mistakes mistakes are fine I
mean that's how we learn you just don't
want to keep making the same mistake all
over again okay last question services
belong to teams how do you handle new
functionality that needs change in many
services is there something to manage
this smoothly yes and it's it's one of
those hard situations where there is no
magic bullet there are cases especially
with security let's say you want to
respond to a newly discovered threat or
you need a critical piece of
functionality to be spread throughout
all the services where I campaign a
centralized campaign is necessary and
you need somebody to coordinate it most
of the time for us this is basically the
team that is pushing to change the
championing the change they go and cross
functionally interface with whoever
required and make that change get into
production because most of our teams do
implement some sort of CI CD where even
centralized library changes gets pushed
on a regular cadence automatically it
really becomes a question of long tail
there will be no matter how much you try
to embrace continuous delivery there
will be a subset of applications that
are not pushed as often and those will
require special attention okay that's it
all right
thank you very much and please make sure
that you vote for the session
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>