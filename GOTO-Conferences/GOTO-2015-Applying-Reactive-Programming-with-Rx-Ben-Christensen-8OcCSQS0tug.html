<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • Applying Reactive Programming with Rx • Ben Christensen | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • Applying Reactive Programming with Rx • Ben Christensen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • Applying Reactive Programming with Rx • Ben Christensen</b></h2><h5 class="post__date">2015-07-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8OcCSQS0tug" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I don't know about you but it's
pretty rare that I get an opportunity
filled of a nap like very rare that any
of us get to do this maybe it likes
small stuff but any of our big systems
maybe you think it looks like that maybe
that's how you think your app looks but
probably most of you your app is more
like this it's actually very
sophisticated it's very powerful it's
doing a lot of things well it's running
business in my case we've got systems
built and running a multi-billion dollar
business globally and this is a picture
of New York
okay so New York has been built up over
decades and the infrastructure of it
serves a purpose but as we as it's grown
up around it any major city like this
has the challenge of how do we start to
modernize certain aspects of it in a
cost-effective way without ripping out
the city because that's just not reality
now if your app looks like that you can
get up and leave you don't need me I've
never actually seen an app like this if
you've seen those those comics of like
an engineer and architects it knows like
this time I'm going to do it right and
they've got this vision and then like
six panels later it's the all we've done
it again thing my goal is to maybe take
a few of the aspects of this and bring
it back into New York City okay
so a few years ago we had a very simple
use case that we were trying to achieve
we wanted to have a single Network call
come in from our devices and then
leverage the low latency network of our
server-side infrastructure and the
concurrency that was available to us on
our powerful servers as opposed to
letting little mobile devices or cheap
like $30 boxes and because Netflix
deploys to anything that attaches a
screen to it and so we've got everything
down to like the 20 and 30 dollar
silicon and all the way up to the multi
hundred or that multiple thousands of
dollars of equipments we've got
everything in between and the cheaper
devices they just can't handle all of
the concurrent composition of all the
web services so we wanted to push all
that server-side so along the way we
realized that to do this effectively we
wanted to be able to embrace a async we
needed to do that in the most natural
way the obvious ways
let's do call backs call backs
effectively is what everything boils
down
- at some point you put lots of
obstructions on top of but at some point
the work finishes and it tells you when
it's done and it calls you back in
reality of the use cases we were trying
to solve for with service composition so
we want to compose multiple services
together without giving up on error
handling and developer productivity and
in reality most solutions that they go
down the async path end up sacrificing a
lot in these two areas and so there was
enough of us on the team that had
experienced that pain that we didn't
just quickly move down this path I was
lucky enough that I had lots of
different engineers around me from
different backgrounds and one of them
was Jeff er Hussain who had come from
Microsoft who spent a couple months
arguing with me in beating me down until
I admitted that he was right and that's
how I was introduced into the reactive
programming space so we chose to adopt
async but to do it without raw call
backs so if I just look at Java as is
most of Netflix's is running on the JVM
and most of that is still just playing
Java and more and more of it is Java 8
thankfully the future and future of list
of teas with those two things we could
actually address a lot of our use cases
but honestly that list of T in a future
is not exactly right because we still
have to wait for the entire thing to
complete before it returns and also at
the time when we started down this path
Java futures they just saw that you
actually can't compose them you end up
blocking threads on them I'm not gonna
get into that in depth today the lists
are actually something we deal with this
is the stereotypical example a list of
lists of movies and this is for making
like the grid on the home page and so we
iterate over the list of lists and then
within that we iterate over all the
movies within a list and then within
that we go and fetch a bunch of stuff
and so if you do this naively you end up
with two things
hundreds of Network calls and you also
end up with a step function like a
waterfall that just keeps on going on
forever and so that was exactly we
didn't want
have happened there's also this case
that you would actually never do with a
normal future this future list the
future of tea but semantically that's
actually what we wanted we want to kick
off this work asynchronously that would
be able to send back to us individual
items down the pipe as they came and be
able to work on them as they came in
so we already knew semantically that
that's what we wanted to do just this
wasn't right and we ended up there's
different ways of handling this but we
ended up adopting the the RX observable
type so our X stands for reactive
extensions in our prescription that
might be a prescription for Java I don't
know but this the reactive extensions
for Java gave us this observable type
and we ended up treating everything as a
stream so just all data coming into this
is just a stream of data whether it's
one item many items or an infinite
stream of data we just started treating
everything as a stream so what is what
did our API look like at the time
everything was blocking API calls like
that now we don't really deal with
orders products and shipping status but
I decided to use something other than
the than the Netflix isms that I
typically use and in this case you fetch
all the orders and then for each order
you fetch all the products and for each
product you fetch the the shipping
status so when I go to my amazon.com
page I see similar things like this okay
this is very common like virtually every
commerce site I've ever used has some
page that is being rendered behind the
scenes I can envision this flow
happening and the API is underneath it
that we're using these are typical
blocking API is that we've all been
using forever okay the problem is that
if you just put this together naively
you end up with stepping through this in
a waterfall style in each shipping
status calls let's say 100 milliseconds
I get it here so you can just visualize
it and you step through it and there's
absolutely no concurrency going on
whatsoever
now I'm all good with the Java
concurrency in practice book that train
book is sitting on my desk it's one of
only two books I still have in paper
form and it's a great book but honestly
most of us shouldn't have to be reading
it and even though I've read that thing
twice and still refer to it I still
screw everything up and
we had dozens of Engineers and we even
had a user interface engineer stepping
into these code bases and we just knew
that we can have semaphores in the new
Texas and like launching threads
randomly and all that kind of stuff and
expecting it to work the environment we
were working with in this is what our
stack dealt with all of our inter
process communication was predominantly
being done with Apache HCP we have
memcache D and here's the sad thing is
even though its client actually is
running on an event loop had a blocking
API over that didn't actually expose the
asynchrony BIA underneath it and so
you're still sitting and blocking on an
async call hack Cassandra was blocking
in Tomcat and servlets like that's the
world that we were in and most of our
stuff still is all blocking so how on
earth did I bring in a reactive model
into a stack like that and so the
reality was that we can go run to
rebuilding everything especially for
something that was unproven engineers
were still not buying into the idea but
we could change our service layer and so
there was a layer within this stack that
we were able to go in and and change we
had full freedom to do it it was
isolated to just our team and we could
iterate on it and so we created this
async facade and we stuck it right in
between the parts that we couldn't
control at the top is the servlet and
Tomcat infrastructure that we weren't
about to go change is that it's a whole
different thing I'll talk about that
more at the end of the presentation and
underneath all of the code that runs all
the business logic and all the IPC and
all that stuff which we couldn't in any
time soon swap out so what we did is we
went from a blocking model blocking
api's to asynchronous api's which we
ended up calling observable api is
because they all end up looking like
that instead of type T it's an
observable of T and so all of our entire
service layer was switched over to this
model where all the method calls
returned and observable of something and
what that allows this is very trivial
example but just to show that the
shipping status now that was
synchronously being invoked you still
look the same nesting so
the semantics are still there I still
see that on two levels down like I
wasn't before but now it's sitting
inside an async observable stream which
is inside another async observable
stream and the way that this behaves now
when I hit play on this it runs and then
it all snaps in because what it does is
by up making it so that it's all async
it allows me to declare what I want it
to do and then the runtime can choose to
the concurrency that it's going to
execute at and so I can start to
leverage the the the concurrency
capabilities of my machine and the i/o
that I'm interacting with so that the
actual user experience latency now can
be significantly improved so that's all
stuff that we all know about well this
starts to give me the tooling to get
there so how do I actually get from that
get shipping status method which is
blocking to one that is async so Rx is a
tool that we use to do this kind of
stuff so a very basic way is we
literally just wrap the blocking calls
and so there's a mechanism in rx an
operator called defer that we're
literally saying defer execution of this
until I come back to it later just
declare the work to be done when you are
run execute this block of code that's
sitting here in a lambda and then when
you run it this is the this is an
important part when you actually are
subscribed to subscribe on this
scheduler in this case you put it off on
an i/o scheduler which is going to put
it on threads that are out there
intended to be blocked with blocking il
okay so there's no magic going on here
we're just adding threads through our
system as some of you are gonna wince
and cry and I'll get to that later yes
we have hundreds of hundreds of threads
in some of our systems and it's a sad
sad state of affairs but it's better for
the use case that we had where it's
almost all i/o that were sitting on when
it runs this it then invokes get
shipping status in a blocking way and
blocks that i/o thread and then when it
eventually returns it says just take
that value of T and return it as an
observable
so what if I have a list of orders I
actually don't want to have this turn
into an observable list of order because
that we can defeat the point of what I
was trying to achieve and so that that
list of order can become observable of
order so when I subscribe to this it
will emit each item in the list to me
now I'm not actually achieving any
streaming yet this is just getting me
into an async programming model the way
that you can do this is exactly the same
the only difference here is I have defer
subscribe on and get orders I just use
from instead of just and from just
converts an iterable to an observable
nothing all that special here so at a
basic level we literally just ran around
our system wrapping everything in
threads and we were able to just with
that ability improve our user latency
but in a way that we could actually
still reason about because the tool gave
us the ability to compose it let's go a
little bit further if I actually am
working at JDBC which I imagine at least
some of you still have to deal with it
thankfully I haven't had to deal with
JDBC in four years I'm very happy about
that the JDBC though is still the really
the only legitimate way to get access to
a lot of databases and so if your code
actually is interacting with JDBC you
can start to do some different things so
in the blocking one you can actually see
that we accumulate all the data into a
data structure as it's coming back so
we're pulling it back from the database
and as it's coming back we accumulated
and this means that were allocating
memory and we're also holding the user
up until we have finished pulling
everything down putting it into a data
structure we give them the data
structure they iterate over it again and
then they have to throw that away and
you garbage collected instead if you
move to the observable stream model you
put that same code inside of create and
as you're receiving the responses back
you're actually just on next thing them
out down the stream as you receive them
and so we've completely eliminated the
need for that extra layer of object
allocation and buffering this alone is a
big game because now I'm just letting
the data flow through and I can
react to it as it is passed down the
pipeline
rather than buffering it into steps
along the way the terminal states for
both successful and erroneous completion
are there and this is showing the bridge
between a blocking and non blocking
world because JDBC is all blocking it
will throw exceptions and I have to
catch them
it also supports unsubscribed because in
my trivial example here I decided to
select star from the enteric table and
so I probably actually don't want to
pull the entire table down and so this
will allow me to actually unsubscribe
gracefully and it will shut it all down
so that if I only want to take five
values it will then terminate itself
cleanly and clean up all the resources
instead of sucking the entire database
down and just throwing it all away
and of course because JBC is blocking I
have to put it off on an i/o scheduler
now I'm going to show you a more
complicated example and I want to do
this so that you can see the the power
as you that is there and this reactive
pull style back pressure has evolved out
of collaboration in the industry with
like type safe and pivotal and Twitter
and others and has resulted in the
reactive streams specification which
just went 100 recently and the great
thing about that is that we're getting
the same semantics for how to do
streaming with back pressure across many
different companies and libraries so Rx
java has been involved in that right
from the beginning and so we support the
the semantics and so in java for those
of you who are lucky enough to work in
c-sharp and other languages with
async/await
it's just shield your eyes we have to do
it all by hand so we have to put a state
machine in place to do this like
async/await style stuff and so i'm going
to show you some code that is like the
in-between I'm not going to show you the
actual state machine but I'm going to
show you the pieces of us and we're
going to create an observable in this
particular abstraction we use just to
drive the state machine for us the
actual state within it is the result set
that we're going to get back from the
jvc okay
when this state machine starts the first
thing it does is a constructed state and
so you'll see that within it we get the
database connection we create the
statement the correct
configuration so this configuration is
basically you do in JBC to tell it to
not buffer everything for you so years
ago when I was working with my sequel a
lot it's always fun when you ended up
like bloating your your memory space to
like 10 gigs because you tried to load
the whole database in this tells the
JDBC driver to if your database supports
it to stream the data to you instead of
pulling it all down buffering into
memory and then giving it to you which
is when you're streaming large amounts
of data you want to be doing this
especially when you're doing something
like that so the state is created then
what it does is that it actually has a
bi-directional negotiation between the
consumer and the producer and when the
consumer expresses ability to receive
data it requests data the state machine
will then invoke this lambda as many
times as it needs to fulfill that and so
when it steps into that it retrieves the
state and then it says if the result set
has data and that's a blocking call
because it's JDBC then it emits on next
otherwise it all completes if there's no
further data because JDBC is blocking it
will throw an exception so this is the
bridge between the blocking and non
blocking worlds and then I would capture
that and emit the error and then the
importantly when it when the state
machine shuts itself down it gives me
the opportunity to terminate and clean
everything up and that happens whether
it's a terminating a successful
termination an error or if I have
unsubscribed from an asset to shut down
prematurely it will always go through
the cleanup step so I share this because
this is important for a sync streaming
systems that if you don't have this kind
of a state machine behind the scenes you
you end up with buffer bloat or out of
memory data loss different things like
that back pressure is important in a
sync systems the same way it is in
synchronous it's just a little trickier
to get right in an async system and so
this is a mechanism for dealing with
that and of course because we're
bridging between the synchronous and
asynchronous worlds here we have to put
the all this blocking code off on an i/o
scheduler so the first round of things
that we looked at doing effectively and
by when we started we didn't have any of
that reactive pull back pressure stuff
that is only in the last year and a half
that's come about so about three years
ago when we started down this path we
weren't doing any of that these tools
are available to us to easily bridge
between the blocking and non-blocking
worlds so that was the bottom half of
this diagram what about the top half the
top half was when the requests are
actually coming into the system and it's
a threat for request model and this is
honestly the hardest part of most
infrastructures to get rid of because
your entire company is set up to deploy
into something and changing that
infrastructure is pretty non-trivial in
most places and so we weren't gonna try
and change that up front we're starting
to explore it and I'll get to that and a
little bit but we've had to deal with
the servlet model for the last several
years and so this is the typical servlet
request response loop well within that I
still want to do that a sync service
composition and so I can kick off that
work and this is calling that
asynchronous method we made earlier with
the observable that kicks off the
shipping status asynchronously and none
of the covers is doing everything async
it's all the composition we looked at
later so this is going to achieve all
those goals of flattening out that
waterfall so that now everything is able
to happen concurrently and I get the low
latency for the users the difference
here now is that I need to bridge back
from async to sync because otherwise the
servlet thread will just run right
through and I'll have spawned off the
work in the background and then I'll
return some empty response to my user
and so I could go and like do countdown
latches and all that stuff by myself but
that's not fun
and so the RX observable offers this
very explicitly named method to blocking
it is called that so that it is in your
face that you know what you're doing so
that you know that you're going to break
all the reasons of why you just adopted
rx in the first place because otherwise
that people end up defaulting back into
their imperative synchronous mode easily
because it's like easier when you're
first getting started so this is like in
your face if you see two blocking in
your code you know that you're stepping
out of the async world it then allows
you to the for each over it and it's
going to block the servlet thread for
you and then you can bridge back into
the
world and synchronous blocking i/o and
you write out your output and also
because to each to blocking if for each
have converted back into a blocking
world it will then convert the error
messages back into thrown exceptions so
you can use the try-catch model around
the entire block of code and so in this
way we bridged back from a sink to
synchronous so that we can live within
the this servlet ecosystem that already
existed so two blocking is the mechanism
for bridging between asynchronous and
synchronous this is another just
practicality of the library it by
definition it's virtually impossible to
have anything pure in a platform such as
the JVM it was never built to assume
100% asynchrony and so these types of
things are there in the library to allow
for bridging back and forth between them
so I want to get into like how it
actually went as we did this and the
first attempt we actually tackled Apple
TV Apple TV was the first device that we
we chose and we rewrote it three times
and the first two were horrible the
third one still is bad but it worked and
the experience felt like this okay that
it was myself and one other engineer who
doing most of the work together paired
on it and especially the other engineer
was just like why the hell are we doing
this and I had I had some a bit of a
head start on him and learning the model
and we honestly had no idea what we were
doing and what we found is that the tech
worked that actually moving to this
async model did actually achieve the
goals and that the the reactive
extensions model created by Eric Meyer
Microsoft actually did work even in our
horribly
bad implementation in Java that existed
at that time but we learned a few things
along the way and these are important
for any one of you to understand if
you're going to start going down this
path in your own environments we had to
relearn a lot of idiomatic solutions
things that we take for granted that we
just assumed the there
the way to do things when you you know
spent your entire career with the
imperative you know standard style
programming things as simple as
iterating a loop so a for loop we just
kind of take for granted most of us
unless you came up in a completely
different path than I did and you know
came from the functional background most
people I talked to have come from
backgrounds of C and Java and c-sharp
and Python and Ruby etc and we just
assume that you write programs of for
loops the thing is is that actually
doesn't work when you start to move into
the async and functional composition and
stuff like loops have to go away they
just they're the wrong tool for what
we're trying to achieve and so we had to
move our thinking to instead use like
map functions and you declare go apply
this to everything in this in the
sequence and the stream and so we had to
everywhere where we would naturally go
to think about a for loop we had to
eliminate that error handling is
completely different in an async world
too so everywhere that we would have a
try-catch it's funny I've seen code
where people would actually write
everything async and then they put a
try-catch block around and they would
wonder why their errors never got caught
and it's because the code is long gone
and off somewhere else and the try catch
is like over there and has no idea the
exception was thrown and so we had to
completely stop doing any of our error
handling like that we had to move to
these models of actually having error
handlers that are in the pipeline's in
the streams that took a mental
reconditioning we also had to invest in
documentation and so we spent enormous
amounts of time and getting this right
and we actually hired a technical writer
David gross one of the best decisions we
did in the project and he eventually
elaborated into documentation like this
where we've got all on the website we
started it all internally and it
migrated and evolved as we open sourced
and I'm telling ya it is awesome working
with the tech writer we're engineers
right they're a horrible bad description
in the Javadoc or wherever your
documentation is and he would then see
the commits happen and he
swoop in behind us and like make it
actually like make sense and readable
and flush it out and put it in examples
that's been incredibly important as we
spread this different model across
dozens of Engineers because we actually
have to relearn all those things we also
had to allow for the time and mistakes
that were guaranteed to happen I found
that once an engineer was tackling a
project like this too you had to assume
about two months you basically had to
consider that two months of time for
them to wrap their head around it and
start to feel natural writing this type
of code and be willing to sacrifice good
chunks of that time once they get over
that hump
then it's smooth sailing for the most
part I also need to talk about unit
testing and debugging these things
always come up and so the unit testing
actually is pretty good async unit tests
actually can work quite nicely so I want
to show some ways so rx actually
supports testing right out of the box
and with the test subscriber that block
of code underneath it 100% async if you
run a normal like J unit or pick your
whatever framework you prefer you notice
they're always they always assume
synchronous execution because if you
were just to run that without blocking
something your unit test would pass
while the work is actually off happening
somewhere else okay and the unit test or
it would fail because there has no none
of the work is there but it's not gonna
be right
so you subscribe to it with this test
subscriber and then it just provides
helpers to be able to say I want to
await a terminal event so block the
current thread until a terminal event
occurs either successfully or
erroneously assert that there's no
errors and then do the assertions of
everything you accumulated so the test
subscriber takes care of accumulating
the data that's all a synchronously
firing the test subscriber also allows
you to pass in like mocked observers
like if you want to use maketo or
whatever and so it allows you to combine
all those tools together so you can
actually mix and match async and
synchronous in unit tests gets even
better though with virtualized time so
same test subscriber except I'm going to
add this new test scheduler so something
interesting that Eric Meyer
when he designed reactive extensions is
he made it so that all concurrency
anytime you add concurrency of the
system it's always parameterised and so
all time is virtual it never actually
asks the run time what the time is it
always asks a scheduler and the
scheduler always can be injected and so
what that allows us to do here is that
that time the time unit milliseconds
that we're incrementing by all the
sudden allows us to take control of time
and so we can advance time in a relative
manner by 200 milliseconds
deterministically we can then assert
that it has emitted one item as we would
expect when it ticks and then I can also
do absolutely increase in time advance
time to a thousand milliseconds and
assert that I've received the five
values and now by a unit test I've got
two really strong benefits out of this
one I no longer have to have like crazy
long unit tests where I'm adding like
seconds of sleep time just to try and
like cater to all the different machines
in the world I might write run my unit
test to try and get it so it's not racy
and so that it's deterministic but it's
also so so it runs as fast as the CPU
can crunch through it and my tests are
always deterministic it will always do
exactly the right thing because it's
incrementing time in a deterministic
manner this is an incredibly powerful
tool if you're dealing with asynchrony
this same behaviors also it allows us to
actually look at historical streams of
data and then crunch over them in as
fast as we can applying the same
temporal operators to them as if we
would have done in real time on the
other hand async debugging it just sucks
this is just the reality of where we are
in the industry right now let me just
show you a few examples of how bad it is
so stack traces lamb does just destroy
everything they're just completely
garbled you have no idea what's going on
when you schedule something
asynchronously the native thread only
knows that it was scheduled it has no
idea what scheduled it so you've
completely lost the chain that got you
to this point
stack pollution everywhere all you see
is the library code for the most part
hardly any user code in there so Rx
tries it tries to tell you here that hey
I got an error when on next thing this
thing is the
types you can like sort of kind of find
your way your way but this is a just a
mess your step your debugger stepping
through code it probably won't work for
you up at the top there if you actually
set a debugger and try and follow that
you're actually going to step through
the outer layer as you declare but it's
never going to actually get into your
user code so to get into the user code
you actually have to put a breakpoint
right in the lambda where you want to be
but that's it like you actually can't
step it up and down that's just that is
the nature of async code today and so we
end up doing a lot of debugging via
logging and this kind of sucks but it's
the reality of where we are now this
actually isn't that far off from where
we were anyways because distributed
systems that's just we work in
distributed systems and so we kind of
given up on all the debug tools for the
most part anyways but this really does
suck when you're doing local development
and that's just the reality that I don't
hide so we're actually exploring work
here like we've actually conceptually
built logical call stacks that are
really nice that hop threads they just
kill performance by like two orders of
magnitude does you generate stat traces
everywhere there's a company we're
working with his done some prototype in
a space that we're exploring being able
to do some things like deep inside the
JVM to stitch together logical stack
traces that flow across through our
boundaries if that work matures and
becomes useful we'll definitely let the
world know about it I would really like
for the platforms to start to solve this
problem that's really the place where it
needs to be solved so how did this all
look once it was done the observable API
has worked well in that async facade and
we were able to move from calling
patterns like the top where we had
everything synchronous and it was all
the concurrency was really only because
our devices were calling us lots we were
able to collapse down the amount of
Network calls and push most of that work
to the server side and have it be same
the error handling was correct developer
productivity after that that hurdle of
training ourselves was good and we moved
from a blocking API to an observable API
and all Netflix traffic
the control plane goes through
observable traffic like this absolutely
every web service request that powers
our you eyes goes through this this has
nothing to do with the actual video
streaming part to see DNS that's a whole
different beast that's just fetching
bytes off of a CDN but the control plane
all of our web service calls all go
through these kind of api's there were
challenges alone along the way though so
one of them way too many threads okay
like I've got one system that's like
over 1,400 threads on an eight core box
just think about that now the benefits
of outweighed the cons for the base upon
the code base that we have okay
however it's made it it was already hard
to tune the thing in shedload we've made
it far harder we're actually pretty good
at it but I feel like we're only eighty
percent of the way there and we don't
think we can get the last twenty percent
it's like this magic black art to figure
out at what point the box falls over and
dies because it's never 98 percent CPU
it's like 73 percent some days 62 other
days 84 percent it all just depends upon
the way the wind is blowing and which
threads are doing something and that's a
challenge of this model it's also really
easy to accidentally block asynchronous
code and so in this example we've got
two concurrent model on the top a
sequential one on the bottom the one on
the bottom will do this step waterfall
and it was just a user error because the
difference a flat map and map completely
breaks everything because in the bottom
one whoops and the bottom one I'm
invoking a blocking call and so what we
actually did is we made something we
made it so that we just eliminated all
the blocking calls in our API so that we
made it really hard that you actually
have to consciously go out of your way
to do it the thing is is it's there's
there's no proper modularization in java
developers make mistakes it's really
easy to slip into the old model so it's
just a simple block and call it returns
T and we've seen in production code like
this get out and all of a sudden we
destroy performance on something and our
Canaries have to find it or if it's a
small thing we don't find it too later
this is a problem with mixing things in
a environment that is not built for it
so along the way we start asking or so
what if we did make the effort to go
like do a green filled rewrite of this
layer and what I found was myth and
legend everywhere it was like near
impossible to get straight answers on
anything and I go to companies that ask
so why did you adopt something like
Metis or whatever and I get this answer
I'm like really because it's better like
that that's an answer like that that's
not helpful or I get it's worse don't do
it like absolutely don't do that
completely destroys everything or it's
the wrong tool for the job all this
stuff I'd also get it doesn't matter
like just whatever you prefer and so as
I researched it more and more and more I
basically found that all these different
approaches to dealing with concurrency
theoretically they should all do the
same thing so if you boil it all down at
some point you've got the same CPUs the
same memory the same like physical
resources if you if all the scheduling
and whatever abstractions you use were
to slice and dice it in the same
efficient way your abstractions should
theoretically all be equivalent the
reality though is not that but this is
where a lot of the confusion I face
would come in like for example there's a
paper about user threads that you know I
shouldn't have to use callbacks because
of my user threads it's the right
abstraction it's like that that dead be
great if my runtime actually gave that
to me in a way that worked but it
doesn't the only agreement that I got
from anybody is if you're trying to
solve the sea 10k problem which is
10,000 concurrent connections on a box
and that was a decade ago or like now
it's like a hundred K or a million I've
even I know people who are definitely
running a million concurrent connections
on a box in production today they're now
starting to tackle ten million which
just baffles me I have no doubt we'll
get there this is about the only place I
got agreement that yes go use event
loops in like non-blocking i/o for those
things it's the only place I could get
people to like not argue with me and so
I had this problem I had to like what am
I going to bet the future of a pretty
important system and Netflix on and I
didn't want it to be based upon myth and
legend or gut Jack's or opinions or like
you know
all that stuff and so we did what every
bad engineer does we wrote a benchmark
and disastrous of course but I will tell
you why in a second why this actually is
interesting so we've created this thing
where we miss there's like the
stereotypical web service at Netflix
where something comes in you kick off
some work and then as that work comes
back you kick off some more and then
some more and then you compose it all
together into the totally inefficient
JSON and I purposefully use JSON so that
I was actually realistic because try as
I might we don't actually use a whole
lot of efficient binary encoding type
stuff I'm very much trying to get some
of our systems there but honestly most
of it is this kind of stuff it's the
reality and we're not a hyper latency
driven company and so most of the time
it's like good enough one and so this
whole thing should take 150 more 154
milliseconds in a perfect execution of
it and most of it is just waiting on i/o
very little of it is actually
computation and honestly very little of
any of the services that I see are doing
anything computation other than our own
inefficient serialization and
deserialization of swill that we send
across the wire and so we tested this in
the cloud and all that kind of stuff but
eventually we beta we did just like bare
metal so we can actually get some
interesting data out of it and the only
reason why I have any trust and
confidence in these numbers is because I
got to work with Brendan Gregg on it and
Brendan Gregg has written a book that
that thick it's like two inches thick
assistant performance and I was really
lucky to have him at Netflix to work
with on this and it took months and
months and months what I learned from
this is if you're actually gonna go and
try and answer these kind of questions
plan to lose a lot of time if you're
actually gonna go and try to do do
benchmarking and we used just Tomcat
nettie like nothing fancy or interesting
really and so here's the first of the
slideshow on the data we were actually
able to push both of them up to almost
all CPU utilization now when we first
started we couldn't get Tomcat anywhere
near that we actually had to spend two
weeks just tuning Tomcat to be able to
max out the box that's the first sign
actually Neddie like
we had configure nothing it just works
that actually is a good sign but Tomcat
we were able to get them both to that
point here's where it starts to get
interesting Nettie has lower CPU
consumption per request and it keeps
getting faster under load I'll explain
that in a little bit so why so
Nettie definitely achieves higher
throughput noticeably and we were able
to prove that this is primarily because
of lower CPU consumption per request we
were able to track that in all the
metrics we were using flame graphs and
stats and different things average
latency now average latency sucks it's
totally useless don't use it it's just
interesting on this graph to see even
how the average works out the average
latency here for Nettie is better than
Tomcat but this is where it gets
interesting you start looking at the max
latency and they're like the 99 9th and
99th 90th etc we so we don't only have
so much time today so I can't show you
all those things but when you start out
both of them are actually right in line
and what the interesting thing here is
it actually starts to confirm the
premise that theoretically these two
should behave and achieve the same thing
and when the system is not under a load
actually it holds out both of them
effectively behaved the same as far as
the user experience latency on them when
the system is not under load as you ramp
up the Tomcat just falls over and so
let's look at what's happening the
actual instructions per cycle for Nettie
which is an event loop model one event
loop procore actually gets more
efficient as you put under load this was
fascinating to watch Nettie gets faster
as you push it harder okay and the
thread migrations was really where it
started to pop out what was going on and
so at the beginning we chose not to pin
threads or anything like that like
that's just not the world we live in
we're exploring it but we don't optimize
to that level so we let the operating
system just schedule things and when
it's under light load each time an event
loop or off thread per request was
invoked it was still doing scheduling
stuff and so both of them were migrating
threads and always good when you start
to get under load though the event loop
start to get hot and they actually start
to do the right thing
and just start to pipe line everything
and so it's thought the the scheduler
stopped migrating stopped moving them
around where's the Tomcat model it's all
everytime it does an event asked to
schedule a thread it picks a core puts
it on it and so the thread migrations
becomes a major thing with Tomcat so why
was Tomcat faster and more efficient and
why did it scale better all those things
the first one just Nettie's better code
than Tomcat like we could actually see
in our profiling that it was far better
optimized for object allocation and
those types of things I just smaller
stacks they went out of their way to not
allocate memory those types of things
the event loop architecture though we
were able to demonstrate actually did
impact things reduced thread migrations
which in turn affects the cache warmth
memory locality instructions per cycle
and ultimately the thing that we were
able to track right from the start lower
CPU consumption per cycle and then we
were able to pinpoint in the thread poor
architecture non-trivial amounts of time
that were spent on lock contention and
thread migrations
there's far far more data behind this
than Brendan Greg could talk about this
far more interesting than me what we
were able to determine from it though is
that despite all the theories with
current JVM and Linux implementations
there is a benefit for us to adopt the
event loop architecture and we also
found through this all it was
interesting to me
I found it far simpler to write and
operate the event loop model than the
thread mall not easier though easier
it's easier for me to do the Tomcat one
because that's where I've got 15 years
of history doing it so I have to like
actually think more about the event one
and also our tools suck at dealing with
that like we haven't we don't have 20
years of tooling around event loops in
fact I found that it kind of worked like
this exactly the same way as the latency
curve is that the effort to get Tomcat
going it's pretty easygoing and then
when you actually tried to scale the
thing and get to tuned well and like how
did shed load all that stuff because the
difficulty spiked exactly like its
latency curve so we found that a fully
async architecture does have benefits
sufficient enough that we've kicked off
a pretty massive effort to go and
rewrite our front-end
and migrate off of the the tommcatt mall
over in Eddy and the interesting thing
was is actually wasn't the the
throughput gains that ultimately drove
us like that we could throw money at
solving we can scale horizontally for us
the reason that drove us was the
operational benefits the fact that I can
push that thing up to 98 99 percent and
still trust how my machine was going to
run the availability and user experience
benefits are far more important to us
than just we can throw money at machines
that's cheaper than the engineers for
this but availability and operations is
important so reactive programming can be
incrementally applied but it does come
with the challenges when you do mix
codebases
concurrency by a sync service
composition has been in our environment
at scale over many years of running this
with dozens of Engineers of varying
levels of experience with it it's worked
really well for us we're very happy with
this rx Java schedules and threads can
be leveraged to bridge that gap in
reality though we we just use districts
for that so history is our fault
our fault handling layer it basically
does bulk heading and all that kind of
stuff it actually we leverage it to do
all that stuff we wrap around blocking
i/o it deals with the it adds the
threads and concurrency and bulk heading
and metrics and all that stuff so we
actually don't directly do any of that
code that I showed you earlier so we
just put it all in one layer in
hysterics rather than everywhere in our
code repeating the same thing over and
over again in closing concurrency is
still it's non-trivial okay that's not
going away our X does not try and
trivialize it every attempt I've ever
seen to completely abstract that stuff
away and make it trivial and easy gets
about 80% of the way there and then you
have to punch out the side at some point
and completely break the system and I
have found that thus far I have not seen
a solution out there that solves
concurrency and asynchrony in a way that
you can completely ignore what's going
on they are leaky abstractions you have
to still understand what's going on
Rx is a library not a framework so you
apply it where it's needed it doesn't
take over your ecosystem and
it's been important to us we're not
there I don't ever expect to actually
see a cell for system like that but I we
believe that we're on the process of at
least being oh maybe pull those little
spaceships up there and like bring them
into New York now maybe I look like
Avengers and there's a lot more
information here oblique done and it's
most of all I've talked about is all
open source and I'll take questions if
you have any
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>