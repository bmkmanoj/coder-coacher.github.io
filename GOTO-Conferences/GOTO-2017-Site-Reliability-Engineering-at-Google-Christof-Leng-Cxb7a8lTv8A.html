<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Site Reliability Engineering at Google • Christof Leng | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Site Reliability Engineering at Google • Christof Leng - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Site Reliability Engineering at Google • Christof Leng</b></h2><h5 class="post__date">2018-02-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Cxb7a8lTv8A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and urine has mentioned a lot of things
from the essary book a specific to
monitoring
thanks for the sales pitch I will talk
more about the philosophical background
the idea of site reliability engineering
in general so site reliability
engineering it's kind of a weird name so
what is this site hard come from well
essentially been trainer had a team of
seven people running all of production
and their depth mission back in 2003 was
to keep this side up where the site was
google.com and if Google comp is down
even back in 2003 that was big news and
was very bad for the company so that's a
critical mission and and that was the
mission statement nowadays site
reliability engineering is much broader
and there are lots of different services
that are run by a three teams internal
services internal infrastructure many
different things so maybe service
reliability engineering is what you
might like to think about it but then
there's that second word in there
reliability well I guess everyone agrees
that reliability is as a nice property
but is it really that important I mean
as a developer you can probably come up
with a huge list of properties and
things that you want to see in your
software and reliability is is one of
them sure but is it the top priority
well you can could argue about it but
let me prove it to you so you probably
all know a service from Google called
Gmail it's a web man thing you probably
have at least seen or you maybe you used
it
and if you look at that gmail 2016 it
has like tons of nice features it like
sort stuff into social and promotions
automatically it has this very powerful
search and lots of things under the hood
that you don't want to miss now imagine
I would strip all of these nice features
away and would give you a like five year
old version of Gmail that would be
really disappointing you've got used to
all of these nice features you don't
want to lose them but what if the
alternative to a gmail 2011 or so would
be that of a product that I call Gmail
500 would you prefer that because it
does not matter which features you have
which features you offer if they're not
available if your users can't use them
so it's better to lose any kind of
feature if you can make your system
available and reliable so that is the
top feature that you want but the
problem here is reliability is a bit
like oxygen it's all around us and yes
everyone knows we needed to survive but
because it's a little bigger at us it's
easy to take for granted you quickly
forget about it
and the problem is when you notice that
it's gone it's kind of too late and you
are in a very dangerous situation then
and it probably will take you a long
time to work yourself out of that
situation again so when a system
systematically fails there's probably
not just one thing that went wrong
not two things probably three four five
things in parallel that went wrong one
after another and then they caused this
perfect storm that takedown took
your whole website or something now to
get it up and running often it's not
sufficient to just fix one of those five
things often you have to fix all of them
and until then everything is on fire so
the problem here is you have to plan
ahead to make sure that you stay
reliable and not it's not something
where you can just react and fix things
when they break now the problem here is
as I said earlier as a typical developer
you have so many things on your plate
and reliability is one of them but there
are other things that people pressure
you to do things to implement things to
take care of you have to launch there's
a deadline
that's the monitoring later right we can
add that people will let us know when
the service is done so the real
important thing here is that you have
someone in your organization who is
dedicated to reliability whose sole
purpose and mission is to make things
reliable and to keep them reliable and
that person can speak up that team can
speak up and say look we need to take
care of this and we will take care of
this we we own this topic and that is
why I reliability engineering inside of
Google is its own organization up to the
SVP level and that makes it very
autonomous and it gives us the voice to
speak for reliability now what do we do
this talk is about site reliability
engineering at Google but there are lots
of other companies that do site
reliability engineering some of them do
it slightly different you can read a lot
about it in the book that was mentioned
multiple
times and one thing that is very typical
for essary at Google is that we work at
a large scale systems at Google tend to
be pretty huge so that gives us a lot of
opportunity but it's also a huge
challenge because as our system grows
from a million users to ten million
users to 100 million users to a billion
users
you cannot just increase the size of the
operations team to keep that running you
cannot increase the size of that team a
thousandfold you don't want to pay for
that many people and also your
organization wouldn't scale that way
that said even if you run a much smaller
organization it's it's very important to
look out for these things early on and
they can pay off a lot especially when
you do become successful because then
often you come into a success disaster
situation where it's like you set up
yourself for failure at the moment you
all the users come and love your product
and want to use it it fails and you get
the bad press in the worst moment now
cyber liability Engineering has
reliability in the name but it is not
the only thing we do is also about
efficiency if you run a lot of service
saving a few servers can save a lot of
money and and we work closely with the
development teams on how to build new
systems and make them reliable from the
start
make them scalable make them efficient
make them fast and make that a very
painless process for the development
teams because they want to focus on the
features they want to to build the this
crazy new thing and then I want to
bother
were with the infrastructure so much and
I guess the key to site reliability
engineering is that is an organization
that was designed by software engineers
so we treat operations as a software
engineering problem and that
fundamentally changes the perspective on
how we treat those things but I said
okay so I reliability engineering runs
operations so it's an Operations team
and if you have been in the business for
a while you probably have learned that
dev teams and Hobbs teams they kind of
always fight you can trace this back to
the sixties to probably before the times
through our computers like probably the
people who design pyramids and the
people who build pyramids they were
always fighting I guess now witnesses
come from well they're fundamentally
different incentives a development team
I mean if you are a developer and I've
been a developer myself you have this
awesome idea how to build something new
that will change the world make the
users happy and make this planet a
better place and you want to get this
thing out of the door and you want it to
be successful on the other hand the ops
team they want assist system that
they're responsible for to blow up not
on their watch not on anyone elses watch
for that matter but definitely not on
their watch so you don't want to get
paged in the middle of the night that's
like a terrible situation nobody likes
that and
do you know what the number one reason
of breakage is us it's change and change
is exactly what developers want and
change this exactly what off Steve want
to prevent yeah so you have to find some
middle ground right so traditionally
what happens is the ops team says yeah
sure launch this thing but first let's
have a look at it do a large review you
have to school your thing
we will you will presented to us we will
do a review we want design documentation
and well there were a lot of things that
went wrong in the past so we created
this checklist here and let's go through
that checklist make sure that you don't
do any of those things that have gone
wrong in the past in this company or any
other company we've ever heard of and it
will keep adding things to their
checklist until this checklist is a huge
spreadsheet and you need a project
manager to like administrate the process
and it's very painful and then you're
like it isn't enough really you have to
look at the system itself like the white
box approach you have to do technical
deep dives and stuff like this and it
really slows down the dev team and when
they think they're ready to launch an
awesome system you ask them to do this
this this this and this and isn't this
which is probably another six to twelve
months of work it's basically like a
post submit code review everyone hates
those but developers are smart people
right so they have to counter tactics
it's not really a launch it's just like
a maintenance release small risk Qi
change flag flip
20% experiment right we don't need to
review for that and now the problem is
they will always outsmart the ops team
because they have built the system no
matter what clever engineers you have on
the ops team they are always at a
disadvantage
they cannot never know the code base as
well as the people who have just written
it so the team which knows least about
the code is the one who will suffer if
things go south now you know where you
have to finis Krampus acceptance is that
an inevitable No
so this kind of traditional very typed
launch checking does not work so at
Google site reliability engineering
typically does not try to do that
try not to avoid all outages in the
first place or even tell the dev team
what to do what to release there is a
responsibility of the dev team now you
might wonder how it is that Google cells
are still available there's a very
simple rule but to explain that rule we
first need to explain Aero budgets to
explain our budgets we must think about
SLO there's a lot of service level
objectives we have certain metrics say
how available your service is or how
fast your service is what the latency is
you measure them you need monitoring for
that and that is why monitoring is at
the lowest layer of of that parameters
we saw on the last talk and then
yeah a bunch of thresholds where you say
okay this service needs to prove you
provide two nines of availability you
measure that you check the threshold and
if you are not within the threshold
you're out of SLO you haven't made your
service level objective and you put all
of those s ellos int or a contract
called a service level agreement and if
that is not met typically something like
your customers give your money back or
so so it's very important to establish
that culture of SLA s and s ellos in the
first place
and to tell to have everyone in the room
agree that this is the metric by which
we are judged and money trading hands is
typically very very good consensus
building now what is the right SLA for a
product obviously a hundred percent
right things should not break that is
true for certain kinds of products so if
I ever need a pacemaker I certainly do
not want something with 99.5 percent
availability also if you if you heard
the keynote this morning if you're
sending a 2 billion spaceship to Mars
you better get it right the first time
because even if they give you two
billion dollars again it will probably
take you another five years or so so
those are cases will I saw at stake
where you deal with a once-in-a-lifetime
chance but you might want to get 100%
but that comes at a cost and the
question is is that cost justified and
if you look at devices like like this
one or like this one
and those other things that we typically
use to access such services they don't
come at a hundred percent reliability
themselves if I pull out my phone or my
pocket switch it on and access a random
webpage chances that that request will
reave step where page are what 99 out of
100 times probably in other cases maybe
something is wrong with with with the
phone something is wrong with the cell
signal with with the isp with something
something happened and your service
didn't even see the request coming in
and so if one out of 100 requests will
fail anyway it doesn't really bother any
user if one out of a thousand requests
fails because of you because your
service wasn't available at that time
that is just background noise that no
one will even notice so of course you
can increase it to 4/9 to 5/9 but the
cost is exponential it's getting harder
and harder to make systems that
available both from the engineering time
and from the resources that you need to
spend and Excel essentially at the end
of the day it buys you nothing because
he uses don't notice they have upper
problems so the question is what is the
right goal of an SLA for your product
and I can't give you a general rule of
thumb that is like up to the product
management of your system what is the
right percentage that you users are
happy and typically what every task is
we look when we got alerted because we
were out of SLO but you use us
our users were happy then probably if
that happens a lot the SLO is too tight
we have a way around the SLO is too
loose so it is a bit of an iterative
project a process to find the right
strike the right balance coming back to
the arrow budget now imagine you have a
service that serves say a billion crew
is per month and you have set your SLA
to 99.9% availability that means you can
serve the million arrows per month and
no one will have any problem with that
so that's a budget that you can use to
serve arrows but how do you use that
budget well the one thing you can do is
you just build the shitty infrastructure
that breaks all the time you service
boom they take a few queries down with
them your air about to trickles down
over the month and 80% is just spend on
is just leakage of your infrastructure
or you could say the infrastructure is
fine everything is reliable we can take
risks by doing changes by trying
something new of which we don't 100%
know if it will work but it will allow
us to innovate faced faster now the rule
that I mentioned is as simple as that
if you services with finesse allow the
dev team can launch whatever they'd want
knock yourself out
if it's not with an SLA nothing goes out
because the asari team cannot tell was
that this feature or that feature or the
future from last month tips was causing
some slowly building up problem only the
dev team
and find out while the a3 team can help
them with that but the left he needs to
fix them so while your service out is
out of SLA is kind of an
all-hands-on-deck situation where the
dev team needs to bring the service back
into SLA maybe roll back releases maybe
do cherry picks - something like that
together with the s3 team with the sole
purpose on fixing things and not adding
more features to the mix that might
break other things and and the beauty of
that rule is it removes the discussion
nobody needs to argue is this the risky
feature or not can we do that is it
really that important it's just we look
at the dashboard it's a green launch
away it's a red yarn of luck and it's
everyone has agreed on an SLA and it's
it's not someone one side against the
other but both sides are working
together to keep the system within the
SLA and the other more subtle thing is a
developer team is many different people
with many different plans so it's not
like they act as one and they produce
one feature after the other but many sub
teams are working on several projects at
a time and now if I work on this
world-changing feature that will like
increase our market share so much that
the competition will start heavy
drinking and give up and eventually
change companies
I don't want your flaky under tested
feature to go out two weeks before blow
our era budget and I won't get promoted
so I will make damn sure that you only
release stuff that is good and I'm also
on the dev teams
that's why the dev team is polishing
themselves each other and they don't
have to fight with with with ops or
something because they know it's an
abbess self-interest okay so you created
a situation where the dev team and
yesiree team work together on building a
super reliable service that is meeting
just the right SLO that you want but
there are multiple ways on how to do
that you can like invest all the
engineering and design stuff that you
need to build an architecture that is
well tested very efficient and scalable
and and autonomous and everything but
that takes a lot of development time or
you could just like file hundreds of
thousands of tickets that the ops team
takes care of and with one heroic act
fixes the system and yes you can do that
there will always be someone on the up
side who feels very responsible for the
service and will be very heroic to keep
it running sleepless nights no weekends
a lot of exhaustion's just feed the
demon machine with blood tears and sweat
now the thing is heroes don't have a
very high life expectation and while it
might be very credit fiying
in the short term in the long term it
just leads to burnout and the worst
thing here is it is very repetitive work
it's just like go for the errorlog fix
this thing fix that thing look at the
ticket queue oh my gosh it's huge well
take it from the top
sigh reliability engineers are coders
are engineers
we are bored easily we don't want that
and our management doesn't ask us to do
that it's not about boring repetitive
work but then but what does the dev team
do I mean for them it's very tempting
everything they can like throw over the
fence to the upsides of things makes
their life easier and that sounds mean
and and and malicious but actually if
you're in that position and you're very
focused on your project and you think
you did a good job you put in a few lakh
statements and and and and and and add a
few unit tests it should be fine right
and then you launch it and then you
forget it because you work on the next
future and you don't actually see how
that plays out in practice because
you're not running the system and all I
see is all there is I don't see that
other stuff so I forget about that and
it's a number of those incentive
problems but unfortunately there is not
this one beautiful simple rule to fix it
but we have six different fixes that
work together number one is do not just
make it very cheap for the dev team to
shift work over to the ops team and it
works like this there is one headcount
pool that the Deaf organization gets and
they have to pay sorry out of that pool
so if you get ten headcount for this
year and you need seven headcount just
to crunch through the ticket queue that
means you have only three people that
you could put on the dev team it doesn't
sound good
so typically what dev teams try to do is
team try to minimize the headcount they
give to the SOE team based on how much
work they need to do an infrastructure
to make things reliable they find the
optimal balance them
selves because it's in their best
interest you could say but but wait a
minute that makes the s3 team kind of
small takes headcount away from that
doesn't necessary unhappy about it no
actually what it does take away is it's
the stupid work is that the repetitive
work that it is boring and we don't want
that we want to be left with the very
hard problems that the dev team cannot
just fix by having another hour of
designers gushing and essary hires
people from a very broad spectrum of
skills a lot of us have been some kind
of system administrator operator at some
point many different backgrounds but one
thing that everyone has in common is
that they can write non-trivial code
they are programmers and they want to
write code not everything we do is
coding a good chunk of it is but we do
know how to do it so first of all we
speak the same language with the dev
team it's on it's it's a discussion
among equals and we also understand what
they tell us they understand what we
tell and we can explain what we need and
we know what we can reasonably ask them
and what is like a software system
cannot do or cannot do easily and the
other thing is that thing with that
repetitive ops work if you have a coder
and you give them one ticket if they are
fine I'll do that second take ticket
they say I think I know how this works
third ticket they're like oh gosh fourth
ticket is I'm gonna replace myself with
a small shell script and it's exactly
what we want because what I mentioned
earlier was
we want to scale the systems and this
kind of repetitive work that typically
is based on on the traffic that the
system sees the more errors you get the
more requests you get you the more
errors you get two more tickets you
produce you cannot scale that but you
don't want to scale the number of people
on the team so you have to automate
yourself out of the job and that's what
we do all day long what the systems keep
growing so we're still in business the
other thing is if you want to be able to
do that if you want to have time to
improve the systems to write automation
to do all of these things you need to
reserve time for that so there is a very
simple rule that you do only maximum of
fifty percent of your time and the team
with what we call toil
it's like stupid repetitive Hobbs work
that can be easily automated it's very
hard to define precisely but they say
it's the stuff that you know when you
see it and the problem here is if your
system is successful and it scales and
you get a lot of traffic then use of
system dies and scepters before and now
the automation helps you to keep grow
but you need to reserve the time because
otherwise you will be in that downward
spiral and things will only get worse
now the fourth approach is to keep the
development team in be on-call
responsibility they will get tickets
they will get paged typically only for
small fraction of things every team has
a different approach on how to do that
and our certain fraction goes to them a
certain type of problems goes to them
the Azeri team escalate certain things
to the left team manually but the dev
team has to
be exposed to running the system to
fight that all I see is all there is
problem and it helps them to learn so
much more about the system so you might
see a lot of resistance from the dev
teams to do that because they don't want
to carry a page of the day I want to be
page at 3:00 a.m. either but be
persistent escalators all the way up the
management chain because it's super
important and typically when they have
done that for a few months they will
understand and they will be very happy
and they didn't you know no one will
ever go back because they learn so much
more about the things they have done and
in the coding part and design part and
how they play out in practice and how to
do it better next time and and another
thing is that on the dev team if one of
the developers has spent the last 3
nights trying to keep the system running
because something someone else like
pushed back and back in their backlog
fix it later I have more interesting
stuff to do well they will build
consensus on bug prioritization now I
said we have a 50% cap on the ops work
that yes ready team does in practice it
it is a cap it's a maximum we don't do
50% most teams do I don't know 20 30
percent really depends on the situation
can change over time but there is this
cap so where does the excess work go I
mean we can't just like leave it there
and let the system break down well we
can but that's not what we want so that
also goes to the dev team so if you
design a system where you don't spend
enough time on making it reliable and
you giving the SRE team a hard time well
it will get back to you you will have to
chime in
and help getting the system fixed and
that will help you to understand the
effects of what you've done and you will
probably try to avoid to do that next
time
and also it will keep you in the dev
team from writing more features that can
potentially reverberate the system
another self-regulating system isn't it
beautiful last but not least nobody
keeps me on my team if I decide this is
a terrible service and I don't want to
do this stupid work anymore I can go I
find any other SOE team or because I am
a developer I'm a software engineer I am
qualified for software engineering
position at Google I can switch over to
the dev team to some dev team probably
not the one that built us terrible
service some other left team but well
maybe to that dev team because I think I
can help them to do a better job and but
they will slowly lose their Suri's if
they are not providing a good working
environment for them and eventually it
might happen that no one wants to work
on their team anymore
then what dev team is like what happens
to our service I'll see yes that is the
patron gets the ticket kill go deal with
it now that is a nuclear option you
don't typically want to do that and it
doesn't really happen that often we make
sure that dev teams and SOE teams get
along very well and it typically works
out quite well but it does happen and
this threat has to be real you must make
sure that your SVP your CEO is backing
you on this they must understand
that they cannot keep other people doing
a job that they don't want just because
they don't want to fix their system and
then if they understand that it
typically will not have okay third part
so we've built a system it is designed
to be reliable and the dev team in the
s3 team are working close together to
keep improving the system but there will
be outages that is certain we did
specifically not aim for 100%
reliability because we don't want to it
doesn't matter to the user and it will
cost us a lot it will slow us down it
will keep us from writing cool features
competition heavy drinking you know the
drill so there will be outages and it's
not fun a little bit maybe but really if
you get paged at 5:00 a.m. in the
morning and you know your customers have
a bad time nobody is really happy about
that but it's okay now what we want is
we want to get the system back on track
as quickly as possible and the first
thing we can do is we minimize the
impact so your system has a certain time
between failures MTTF mean time to
failure and then it has an empty TR mean
time to repair how long does it take you
to get it back up and running and of
course we can increase the mean time to
failure to make the system more reliable
but it costs us when we can also reduce
the mean time to repair how long does it
take us to get it up and running again
and one thing that we do is the sree
team is directly in the loop there is no
naka operations under whatsoever the
esri team is in charge
of the service and we have very good
diagnostic information if you get a
general problem with your service down
message it's not very helpful you have
to figure out what's actually wrong with
the service where does it come from so
if you have good white box monitoring
and you've been told that you have the
light latency spike and RPC is going
from this cell to this backhand cell you
have somewhere to start you can look at
the network between those two clusters
you can look at if any releases were
rolled out in one or the other cluster
configuration changes things like that
quote our problems so that helps you to
get started very very quickly and the
third thing is practicing so if you have
a services very stable and and you make
sure that it's improving a lot you might
not have had a lot of yesterday or this
week so you might say ah I just need to
read up on that I have no idea what that
part of the system is it didn't have a
problem in the last two years so what
you should do is you should practice up
front and and studies have shown that
this decreases the time of an outage by
something between 50 and 70 percent but
practicing doesn't really sound like fun
how can we make it fun the approach then
we have at Google is called a wheel of
misfortune and it's basically like a
role-playing game Dungeons fragments if
you remember that so basically you come
up with a list of things that typically
break in your system you can make a pie
chart like that they won't allow me to
show you what actually goes wrong so we
replace it with
things you can imagine that would be
terrible for your system sharks with
laser sharknado shock avalanche
ever-popular bear cavalry don't want
that strapped into your service any you
pick one of those you can roll a d20 you
are D&amp;amp;D player and and and one person on
the team is the game master runs the
scenario says you get paged the message
says this and that went wrong and then
the victim the players try to figure out
how that happened they say okay I will
look at that dashboard what do I see and
so and so forth and try to find how to
fix the problem and what has actually
caused it because there is a very
important part we do not want to just
get a system up and running again
there's our very first priority but
after the system is patched somehow duct
tape but it's running we want to figure
out what was the root cause and if it
was a bad outage we want to write a
post-mortem and the post-mortem should
document what happened and why but
writing such a document is takes
significant amount of time doing all the
investigation for the root cause takes
time so you cannot do an arbitrary
amount of these all the time so we have
to rule that you should not have more
than a median of two events per
long-haul shift
you should typically will what was going
on magic
you should not typically have more of
these events because then you would
either not be able to really do a deep
investigation or doing other things like
project work so that is why we typically
have s area with at least eight people
on an on-call rotation we do prefer
having two sides on different sides of
the planet so you can have a follow the
Sun one rotation you're not getting
paged at 3:00 a.m. because the person on
the other side of the planet is
responsible and have at least six people
so you have six weeks to do project work
and so five weeks to do project work or
let's four peaks of project work and one
or two weeks of being on call as primary
or secondary and then you have enough
time to actually write a good postmortem
so what is the post mortem about a post
mortem documents what has happened what
we found out and what we should do about
them so it has a list of action items
and don't let those action items rot in
that document put them in your back
tracking system and make sure they get
prioritized but the most important part
about post mortems is to get blameless
one of the many companies that I worked
at whenever something was wrong and
there was afterwards the discussion the
first and only question was who's too
plain and you know who was always to
blame the person who was not in the room
at the time to participate in the
discussion so you better not take a
vacation that is not what you want
obviously what you want is that people
come forward and tell you what they did
how it went wrong and not getting blamed
for it so we look at the process and
technologies so if christophe press that
shiny red button that took
down the system the question is not why
is Christophe so stupid the question is
why did we have the button in the first
place
and what can we do about it and yeah
then we prevent that same thing from
happening again
so hopefully you will not get paged for
the same problem months and years over
and over again you will get new
interesting outages that are fun to
debug and will make a good wheel of
misfortune so how are we doing on time
okay so you hire only coders you let
them do whatever they want to improve
the reliability of the system you have
to have an SLA and everyone has to agree
that this is the right SLA and that's
the most important thing use era budgets
to launch to get your launches you have
a common snapping pool of 5% of your ops
work go to the dev team the operational
load on the SOE team is kept by 50% you
have to have enough people on your own
core rotation so they can actually do
deep investigation without being
exhausted you practice practice practice
and your post mortems are plainness now
shameless block the Bluebook again get a
copy of it or if you hmm
don't want to spend the money there is
actually a copy of it online of as
creative commons at google.com not sre
so you can have a look there and then
get your good old paper copy thank you
very much
Thanks I bought a book by the way it has
many too many pages I think so 800 or
something - I must admit I haven't read
all of them oh okay okay we have one
minute for questions or actually for
yeah let's try a few questions thus
delaying releases increase the chance of
errors for subsequent releases as these
tend to be larger you should release as
often as possible so the smaller release
gets the more reliable it is and the
easier it is to debug because the Delta
is smaller so if you think back at the
90s where your ship cd-roms
like you had a dev team to change
everything in the system over 12 months
so basically anything that you had
tested for the last release has become
worthless if you had only like an 8-hour
8-hour workday for them to change and
break things it's much easier to find
out what went wrong then again if you
are in a situation where you're out of a
salon you're not launching they should
not keep changing the system they should
fix the system and not just delaying
later releases by doing project work on
the side you want to keep that window as
short as possible
to get the system fixed so the dev team
can get back to future development but
only once the system is fixed
is there a conflicting incentive
regarding the diversity of technology
used by developers for example every new
language or platform used in the
landscape increases the work of the SRE
team that so that they in sorry
increases work of the SOE team more than
using something already in place I guess
there's a trade-off here of course you
don't want to like manage this like
crazy sue of things
then again the programming language that
something is written and doesn't matter
that much at the end of the day if you
set certain standards and
the best thing is to get the SOE team
involved very early in the design
process so you can agree on the
technologies that you use and if
something is just a better fit for your
service and will make the service better
go for it
if it's just because that's the fancy
new thing that you always wanted to try
out maybe a sari will not be thrilled
then again reducing the number of
critical dependencies that your service
has is probably the best you can always
ever do for increasing the reliability
and also the headache that maintaining
that code will give you okay a last
question
oops questions come in so quickly what
are typical SLA budgets on different
products no comment but there was a
public announcement this week that
cloudspinner now offers five nines of
reliability that is a lot okay thank you
christoph and thank you for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>