<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Apache Spark Beyond Shuffling • Holden Karau | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Apache Spark Beyond Shuffling • Holden Karau - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Apache Spark Beyond Shuffling • Holden Karau</b></h2><h5 class="post__date">2017-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4xsBQYdHgn8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm going to try a slightly different
format for presenting this information
than I normally do in part because I got
an opportunity to dress up my stuffed
animals in outfits and it's why there
isn't magic which is kind of depressing
but then we'll look at the magic and
it's really more about how the magic in
programming inevitably steals our lunch
money and makes our jobs fail so there
is some magic it just comes at a price
and we should understand that price so
yeah I'm holding my preferred pronouns
are she or her I work for IBM safar
Technology Center in San Francisco
I'm really glad they pay me money to
come and talk to wonderful people like
you and I get to do a lot of open source
software and I'm a petty spark committer
in part because they pay me and I get to
have this time to focus on spark so I'm
really happy for that I've worked as a
bunch of other companies like Alpine
data books Google and Foursquare and I'm
a co-author of several spark books my
latest one though is high-performance
spark and I do get the most royalties
for that so that is the best spark book
and I really hope you buy it if you want
to follow me on Twitter for a Canadian
who's wondering why she's living in
America you can find me on Twitter and
the slides from today's talk I'll give
them to Dean but I also put them on my
SlideShare and I've got some other more
sort of standard links if anyone's
interested also I want to be like super
clear I'm like trans and clear I'm
Canadian I'm here on a work visa I'm on
an h-1b and I also consider myself a
part of the leather community if you
don't know other people like me in
software you should come out to San
Francisco more often but also like we're
here and like we're nice people or maybe
not as scary as television implies I'm
not stealing your job I press the same
buttons that you do and you know maybe I
end my sentences in a more often but
we're fine we can all be friends so whoo
yeah this you slide I should not skip
the slide from my glorious employer who
can do no wrong the IBM scar technology
in San Francisco is amazing I actually
honestly really like it there's a lot of
developers who work with me there and we
all focus on open source not all of us
are directly on spark but a bunch of
people work on things that are built on
top of spark and we also have a design
team associated with us and they do a
lot of work on sort of related projects
notebook type things and experiences
they tend to work on things which are a
little more commercially relevant so you
know when the layoffs come I'll be off
in the corner and they'll still be
around hmm anyway sorry bad layup joke
um yeah if you buy things from IBM if
you have a mainframe if you have a
support contract keep doing that someone
told me that the system/360 stuff
essentially pays my salary so if you are
in any way involved in that in a bank
just keep buying that it's amazing right
okay so you've been laughing at my jokes
so you're clearly nice people or at
least willing to pretend to be I'm
really curious how many people are new
to Apache spark this is like your first
intro to spark oh wow that's a lot more
people than I thought this is really fun
and exciting okay um I am a little
worried though I hope this talk does not
scare you away from spark because we are
going to talk about the parts of spark
which don't often get mentioned because
they don't work so well um but most of
the spark works much better than the
pieces we're going to be talking about
today we're talking about a shortcoming
and in learning that you can hopefully
learn what the other parts are pretty
awesome too how many people are Scala
developers okay Dean yes thank you um
how many people will call themselves
Java developers good representation and
Python developers less than I thought
but we Python developers we can be
friends I work in Scotland mostly but I
do a lot of work with the Python
community okay cool thank you um there's
going to be a mixture of examples and if
there's something that you don't
understand because it's in a different
programming language stop me and we'll
go over it together it's fine
so for people who are new to spark
you're presumably here because like you
saw something
cool about spark but if you're not super
sold it's really awesome it's a
general-purpose distributed system it's
not just for like solving one particular
task you can do whatever you want with
it you can provide arbitrary functions
and do stuff
it's an Apache project which i think is
really important it means that it's not
exactly controlled by a single company
although there are components of it
which are more controlled by individual
companies and others but it means that
there's sort of this continuation if any
of the big companies that are working on
spark decide to go away they're still
found a ssin behind it and I think
that's really important for sort of just
you know trusting your software a lot of
people come to spark because they're
like wow my MapReduce jobs of taking 18
hours and so they go how can I make
spark fast how can I make my job faster
and then they see this thing which
claims to be a hundred times faster
which is obviously
but there go like well a hundred they
probably wouldn't lie that much and then
they try spark and then then we have ok
the other way that I think people find
spark a lot is they're like my pandas
dataframe is running out of memory
I want to distribute a data frame and
then they search for distributed data
frame on the internet and then the spark
people they're like come yes
do not look too closely um and then we
we sneak in the Python users and it's
delightful and there's sort of two core
abstractions of spark there's the sort
of legacy one called rdd's that are
still going to be very much maintained
and developed and they are they just
allow us to do sort of really raw types
of computation and then there's data
sets which is the new name for data
frames because software people love
changing the names of things so we have
data sets and they they give us a data
frame like interface so there's many
many asterisks after their word like and
possibly a you know very sad cat or two
so right yeah people come to spark
because they're like how hard could it
be to learn spark while I'm waiting for
my MapReduce job to finish and honestly
you can get the basics in the time it
takes for your MapReduce job to finish
right like you don't even have to ask
your boss for training just like read
this while you're waiting for your
memories job to finish our learning
MapReduce that's in the room
okay well please don't murder me um and
yeah this is the other one my data
doesn't fit in memory on my fancy
machine anymore and they won't let me
buy bigger machines or I there's not
into larger Amazon instance type how do
I solve my problems now and the other
reason they come is there's a little bit
of magic and the magic will bite us in
the posterior but it'll be fun until it
explodes right so what is the magic of
spark right the magic of spark all sort
of stems from this we call it the
directed acyclic graph which makes it
sound all fancy in computer science e
very good for writing papers so the
sparse paper actually one like a best
paper award gives a lot of instances of
the word graph very solid but we can
really just think of this as spark is a
a lazy system and instead of doing
computation immediately when we ask it
what it does is it builds up this like
idea of all the things we've asked it to
do and then once we sort of force it to
do work much like a teenager it
collapses all of that into the absolute
minimum required to not get grounded in
data frames instead of calling this a
directed acyclic graphs we call it a
query plan because it's more like sequel
and in some ways there's there's some
very important differences and we'll
look at those so there's this dag in
addition to being used for sort of this
really cool optimizer which takes all
the things we have to start to do and
shove it down into one lazy thing is we
use this to recover from errors in
distributed systems are a really big
problem is that failure is a first-class
citizen right like we can make
distributed systems really easily that
run on reliable hardware but it turns
out reliable hardware is expensive and
the reliable network does not exist and
so we have to be able to recover from
failures and so spark uses the same
information that uses for the optimizer
to go like I encountered an error I'm
going to recompute this piece of data
and this is very different than the
MapReduce approach which is when I
compute something I'm going to write it
out to three different disks distributed
across my data center and I always kind
of
and I owe on three remote discs is like
really slow so using this dag to do our
recompute is going to be really fast
awesome and then there's some other
magic it's not super exciting we'll look
at some times okay all right so RTD is
the Corbett the resilient distributed
data set notably
it is not the same it's not saying that
the data set is not resilient this is
just the wonders of software developers
allowed to name things data sets are
actually built on top of rdd's despite
what the naming would imply if there's
kind of users and you're seeing like
data frame you're probably really
excited and you're like I can port my
code super easily and no no that's not
going to work data frames totally have
some data frame like operations but they
don't have the same ones that you're
used to in pandas not not by a longshot
we'll talk about this but let's dive
into the first piece of magic together
let's let's look at our dag so in spark
rather than sort of manipulating
collections by mutating them our our
collections are distributed and
immutable and so what we do when we want
to like compute something we obviously
want to compute something is we get a
new collection back so we transform our
data and then we get back a new
collection and then this collection
doesn't really exist it's this
collection is just like this plan of how
to make this collection if we ever get
caught with the fact that we didn't make
our collection and both collections are
the ones where you never write them out
cuz I don't have to compute anything and
you won't find any bugs in my software
and I'm really excited about that but
let's let's look at what one of these
plans looks like so this is maybe not
the right font size I'm sorry about that
but we can see here that the dag is like
all of these different transformations
are linked together in a very just one
after the other and now this is this is
a really simple dag we could have an RDD
which was comprised of many different
input rdd's so we could join right joins
are pretty basic and then our Dex
to get more complicated and the stuff
that we can do by collapsing this down
becomes even more important but but the
nice thing to think about is that even
though we've asked it to do a whole
bunch of different things it's going to
compute this in sort of a single pass
over the data we can see these stage
boundaries are sort of where the
differences and we can look at query
plans and we can see that query plans
have a little bit more information not
information that you can read at a
distance from sorry um but you can see
that there is a little more going on
spark has a better idea of what's going
on when we're working like data sets
right so we're going to look at the
stereotypical word count example and
that is because a licensed Big Data
instructor I am required to include word
count in every talk which may have a new
student and I am definitely required to
include it if it has returning students
and this is this is why word count is
everywhere but more seriously where
count is actually a pretty good example
for for big data because of this reduce
by key step but we can we can see our
our first bit is just loading our data
and this in this case it's loading a
text file the text file could be
terabytes of data on HDFS or it could be
kilobytes of data on your local mac book
either way I get paid the same so I
don't really care then we go ahead and
we tokenize your data in this case we're
using flat maps so we're just saying for
each one of our inputs we could have
multiple outputs and this is our rule to
tokenize it it works really well for
English when I go overseas I pretend
that I don't know that other languages
exist and then it's almost reasonable
and then I've got this word and then I
need to make the word on the number one
and then I tell SPARC like hey what's up
sum up all these numbers one and save it
out and this reduce my key stuff is
really important because this is the one
step which requires internode
communication if we just had all of the
previous steps and we saved it out each
node could just compute the data locally
and they wouldn't have to talk to each
other and it would be really fast and
like that would be amazing
but sure we actually would count does
have something tricky in it because
not that tricky I'm important the
important thing is that until we get to
this action spark hasn't done anything
I could literally tell it to load a file
that doesn't exist from a input format
which doesn't exist either and it would
just go like okay cool and then it's
only when we get to here that'll be like
oh wait yeah you asked me to check out
the garbage but the garbage doesn't
exist so I'm done and so this is this is
the action which forces spark to
evaluate the RTD and this is really cool
because we could do a lot of different
maps and if you're used to working in
Map Reduce you're maybe used to spending
a lot of time thinking about collapsing
all of your things down into single
stages but in this case the computer
does it for me and it's mostly right so
that's okay that's good enough um
magic okay I'm not very convincing magic
yeah so what what about this word count
example is awesome our map stages got
combined together our flat map stage got
combined together if one of our nodes
fails in the middle it would just go and
be like that node was processing the
information of this slice of the file
I'm just going to go and have one of my
other nodes do this recompute and we can
even do like really cool things like
sort of if one of the nodes is taking a
long time we've got a straggler and like
maybe it's actually 386 that someone
snuck into my cluster as a prank it'll
go and like preemptively start a second
task on another machine and start
computing this for me so I think this is
pretty awesome there is there's some
limitations though that come from this
design so the biggest problem with SPARC
is that at the end of the day SPARC is
not a compiler it doesn't know what your
program looks like in telling it and it
sort of gets a view at each instance of
the word action and then we try and do
our best when we encounter an action
right when we try and save the data out
we we do our best to optimize it but I
don't know what you're going to be doing
right afterwards right if maybe you've
done this like really awesome complex
thing to train a model and then you have
another thing to extract all
the errors I don't know that you're
coming through and extracting the errors
afterwards so we'll compute all of this
and then promptly throw it away and you
might actually want to reuse some of
those pieces right and so we need to
help spark out and let it know when
we're going to be reusing data just
because we don't have this whole program
of you going on one of the other things
is combining these things together is
really awesome for performance and
really terrible for debugging so if if
my tokenization thing was actually
something complex and I screwed up in
there like I had a bad statistical model
for like Chinese tokenization and I
started throwing exceptions I wouldn't
find out until I saved it out and at
that point I wouldn't necessarily know
which one of my individual map tasks had
caused my error because I'm just getting
an exception from my save right and like
save should work this most often happens
with people going count and they're like
why can't I just count my data counting
should work and it's like yeah
accounting should work but it has to do
all the other work and so it's there's
some sad bits of magic in here but it's
ok we can we can work around much of our
sadness and the last one is it's sort of
the limitation with rdd's spark is has
this really awesome
optimizer but it can't look inside of
the functions that we're passing to it
right so if we if we jump back to slides
spark can't see that we're tokenizing
the data all it knows is that we're
doing something with the data and it's
going to return 0 or more results for
each element and then for the next one
it knows it's going to get a one-to-one
mapping but we doesn't know what's
happening inside of that it doesn't know
if it's going to be really complicated
or really simple or if maybe we do
something interesting so this is this is
frustrating and datasets went like I
want to make a more awesome optimizer
but I don't want to spend my life trying
to decompile Java bytecode
or Python bytecode right as far as
people were like I want to make more
awesome tools but I don't want to become
a compiler writer and yeah that's cool
the other piece of magic is that inside
of all of this our data is magically
distributed there are some caches which
happen inside
Saiki but when we're loading our data in
from HDFS we we have our data magically
distributed across our cluster if we
have a local collection and we want to
like sort of send it out everywhere
spark will take that collection and
slice it up and send it to all of the
different computers and we don't have to
think about this too much except when it
breaks and this normally happens inside
of the shuffle set the shuffle step is
the tricky part of word count and it's
the tricky part of most distributed
applications books that are building in
the same way and our shuffle requires
that we have an own partition on inside
of the spark and what that means is when
we're loading our data in spark doesn't
really have to make a lot of assumptions
about what our data looks like but when
it shuffles the data forward count for
example it needs to send all of the
information about everything which has
the same word to the same machine and
this could you know very quickly
overwhelm spark right so obviously
there's some solutions to these problems
but this is part of where the magic
breaks down our petitioners have to be
deterministic and key skew is
essentially the part which breaks our
magic automatic partitioning of your
data I like to think of it as the ante
rescue it could also be the Wicked Witch
of the West doesn't really matter but it
turns out most data that we have has key
skew in it or at least modis phases that
I have that I care about have key skew
in it and if your customers tell you
that your data doesn't have key skew in
it they're lying
probably not intentionally but they're
lying to you
if you have something with like no
inside of it ever you're probably going
to find out like 80% of those records
actually have not at one point in your
processing pipeline and and it's going
to be sad and you're going to start
sending all of your data to this one
machine and your magic distributed
system becomes as effective as that
single machine which is really not a lot
of fun that's not what we want to do so
inside of shuffles when we do something
like sort by key or group by key which
essentially just requires that all of
the data with the same key goes for the
same machine we can we can
as explode this is one of my favorite
examples it is about my intentions to
open a handlebar moustache wax shop in
San Francisco well I'm not convinced
it's in San Francisco yet but I want to
open an artisanal handlebar moustache
wax shop I think that it's time has
really come I see a lot of hipsters
although not as many in the room as I
expected I should maybe reconsider my
moustache wax shop um but you know maybe
I'm going to purchase some market data
and I'm going to try and figure out
where I should be sending my fliers for
people about these exciting once in our
lifetime opportunities to save 20% on
their moustache wax or where I should
open my shop right and so I've got the
code data because it turns out that
precision is pretty cheap and we can
pretend that it doesn't II anonymize the
user even though it totally does but you
know whatever people will sell me that
data for about five dollars and so I
shuffle it but then it turns out that
there's so many hipsters in San
Francisco that my distributed computer
fails and I send all of my data for like
this isn't even all of San Francisco
this is just the Mission District and it
turns out we have a lot of mustaches and
so red red is sad and this happens
because when we sort our data by key
spark says ok I'm gonna I'm gonna sample
the data I'm going to try and figure out
what some reasonable range intervals are
for my data but it can't break it if the
key doesn't change we can't send one key
to two different computers right now and
this is really useful from when we're
doing things downstream like join
because if I know where all of the data
for one key is when I'm doing my join
this can be much more efficient
I can do my shuffles more efficiently I
can maybe only shuffle one of my two
pieces and it's like one of them is big
and one of them is small that can make a
huge difference right so this is this
gives us some really cool performance
benefits but it comes out the cost of I
can't figure out where to open my
moustache wax shop which is a pretty sad
cough so there's a hack
I will did I fix it in this example
whatever we'll pretend it it um there
might be one record which is shuffled
incorrectly by hand I'm
sorry I did this by hand for for the
literally bytes of data but we can we
can sort of take our key and we can
append some junk to it and this is kind
of hacky but it lets us do our
distributed shuffles I can figure out
where to send my mustache blacks fliers
and you know everyone's going to be
happy my distributed system is still
going to magically work until it splits
one way that the shuffle can make things
sad and this is one way we can make it
happy but there's other ways we can make
the shuffle sad and happy at the same
time we were sort of talking about how
spark can't see inside of the lambda
expressions right and this can come
together with our good friend Keith Q to
make an even bigger problem so group
Ikey is the thing which sounds really
safe how many people think group bikie
sounds like a safe API to use okay only
two people how many people think it
sounds like a dangerous API to use
before I started talking on the slide
okay three people and a lot of laughs so
that's pretty good thanks cool umm so
yeah I mean it's clearly not safe
because this is a slide labeled more
sadness um but I think grouped by key
when I first came to spark I was like
yeah I want to group my data together by
key that makes sense
and the magic optimizer will look and
see what I'm doing next and understand
and be smart and it will save me from my
laziness or actually it will save me
from my laziness with laziness okay no
one like um here is bad word count um
and I think that if we hadn't talked
about why group bikie was sad or is that
we wouldn't necessarily this wouldn't
jump out to us immediately as non spark
developers as being a bad pattern right
I've got my data I tokenize it I do that
word the number one thing and then I
group it together for each word and then
I go ahead and I compute the sum oh it
seems fine and then I save the result
out well there actually is a difference
in this case I did this with literally
kilobytes of data because I was
on airplane but the the shuffle read
here is let's cryptokey yeah the
shuffled read here is like forty eight
point seven kilobytes and the shuffled
ride is pretty 2.4 and so that's like
alright for an input essentially for our
input we ended up writing out exactly as
much as our output was during the
shuffle stage and that's interesting and
that could be fine
but because we're going to be sending
potentially all of the records for one
word to one machine this could be
problematic a large portion of that
shuffled right could be going to a
single machine so instead what we did in
our earlier example as we use reduce by
key and reduced by key allows spark to
understand what's going on because
inside of our map values spark all it
knows is we're computing something it
doesn't know if there's a partial
aggregation of that something that
exists right it can't like compute that
for like the keys on one machine and
then send the closure to another machine
and then the new some craziness like
that's just not going to go very well
but we're deuced by key tell spark like
pays for all the data with this key this
is my combined rule I want you to
combine the data using this rule and
then on each worker it combines the data
for each key locally and then it
shuffles the combined values and so at
most we have number of workers instances
of each individual key being shuffled to
the different machines and unless you
have more workers than you have memory
on your workers in which case please buy
an IBM support contract last time I said
Clara and that was just awkward
um book please buy a support contract
from it I do vendor in that case but if
you have more than two gigabytes worth
of machines you're going to run into
other problems but reduce by key
essentially guarantees that we're going
to have a reasonable result provided
that our reduction isn't just creating a
list right like if we create a list
inside of our reduced by key there's not
going to be any memory decrease and
we're still going to essentially have
this path of sadness but if I'm summing
the numbers I mean integer plus integer
takes up the space of an integer
I'm pretty happy or even if I'm doing
like a hash set like I'm probably not a
duplicate value so I'm going to be
pretty happy to an aggregate by key is
like word count part two which we
normally leave to the advanced talks and
that allows us to do things that aren't
word count so that's that's definitely
the next talk but you can use it for
things where you're computing stuff
where the types aren't quite the same if
you're doing like an Indian to a hashmap
you can do it more easily with whatever
get by key and we can see the reduce by
key version you know the shuffles read
went down by a factor of two and you can
trust me because no one can read this
slide I'm sorry about that
but the slides will be posted and you
can double check that it's the very
least I used mspaint to fake these
slides yeah cool
so another potential fix instead of like
intentionally remembering not to use
group by key which to be fair isn't that
hard but instead of expressing all of my
sort of aggregate statistics as reduced
by key operations I could use data
frames and data frames allow spark to
understand what's going on so inside of
our instead of having this map values I
can get this group data structure and I
can tell spark what sums I want it to
compute and then spark can do really
awesome things for us I can even have
things where I do select statements to
narrow down the data that I'm reading
and then spark will only read the parts
of data that I need if I have a filter
or if I select only certain columns this
can make a huge difference if your data
is appropriately partitioned and this is
really important right the other thing
that I think is really cool about data
sets is that it lets me write in this
DSL that spark can understand which
looks a lot like sequel but at the end
of the day I'm a functional programming
nerd who isn't very good with sequel so
I still want to be able to put my random
functional programming stuff in sort of
the middle of these sequel like
statements right in this case I'm
loading some information about pandas
probably most pandas are pretty sad
honestly I think they live in zoos it's
kind of depressing but I can process
this data a lot faster since I only care
about the happy
assuming I've got my data are perfectly
partitioned and so I can have this
filter statement and spark can see that
I only care about happy pandas and it'll
just completely ignore all of the sad
pandas in the world which is depressing
sorry okay um here's depressing with
annotations right okay so let's let's go
back to something more fun let's say I'm
really curious about how fuzzy pandas
are in aggregate but I really suck at
writing sequel like statements and I've
got this array attribute and I just want
to sum all of it and so this is this is
a functional programming some the
underscore is should really you can
rewrite the underscore into lambda X : X
greater than zero when your life will be
fine I'm only interested in positive
fuzzy pandas if a panda is negatively
fuzzy I think that's a data error so we
can just throw that out or that panda
has some issues and I can keep my thumb
and I get back a data set and I think
this is cool because I get to still
write my like a nerdy functional
programming stuff and pretend that my
employer is getting good value for money
but I can intermix with sort of sequel
like expressions spark can really easily
optimize and do cool stuff with and it's
a lot faster right if I'm computing some
aggregates I mean we know grouped by key
is bad because I told you it's bad big
is bad here small is good unlike the
amount of money which people should pay
me where we will curse these two but we
can see group by key performs pretty
terribly and we can see that reduced by
key performs like pretty okay but if we
ask spark data frames do the same stuff
they're able to do it a lot faster
because it's able to understand what
we're asking it to do and it can
pipeline a lot of these things a lot
more efficiently it also gets a much
better understanding of the types data
frames has this sort of restricted set
of types that we can work in pretty much
any native type that you're used to
working with it's fine but our Eadie's
have to take sort of arbitrary java
objects and arbitrary Python objects and
we use the built-in serializers for
those things
and that works about as well as you'd
think at least it's not XML hmm and so
this is this is or JSON so this is a lot
faster because the optimizer is able to
see what we're doing and also our stuff
just takes up a lot less memory while
we're doing it so this is pretty cool
and you should definitely consider using
datasets which relatedly is only covered
in my new book coincidence I assure you
right so we're going to do one final bit
of magic so they're like seven or eight
Python people in the room and we're
going to make spark work with Python I
mean we're going to someone else did
this already well let's figure out how
they did it they drink a lot of coffee
and they said to themselves I know how
to do inter process communication I use
UNIX pipes and then they were like okay
well let's kind of flow but how you can
just buy twice as many machines and life
will be okay and that's the state of the
world but let's look at it let's look at
how this works did you do kind of like
this playground um if you send your
process down here it will probably
succeed somehow but if it dies it's not
my fault and you signed a liability
waiver or you know by a support contract
so on our driver side program which is
sort of where we define all of our
transformations and we define all of our
selects or whatever it is we're doing on
our data sparse needs a way to
communicate from Python to the JVM so
that the JVM can schedule these jobs for
it and we do this using something called
PI for J and PI for J is amazing and
terrible it gives you delightful error
messages like object is not callable
which is it's just like that means
something has gone wrong from anything
to like object is literally not callable
to package does not exist to my class
pass it got deleted by accident all gets
do the same error message which is
delightful because when you google it
you can see all of the pun potential
flaws but you know it's great because
it's
code work and there's really nothing
better in high spark what we do is we
take our data and we pickle it and we
store it in an RDD
I don't work so well but it works kind
of surprisingly it turns out that trying
to figure out what key a pickled object
has in Java is kind of expensive
so it's this is exactly true but it's
close enough
we do some magic to make it so the JVM
can understand the pickled objects about
as well as I understand French which is
to say I can say hello and the number
four oh also I can assure you that
Europe is a pineapple no banana um
anyways so yeah so the JVM can sort of
understand these pickled objects just
enough to figure out where they should
be going the JVM is responsible for all
the network communication happening so
it has to understand what the keys are
of our data and this top quality
architectural diagram which shows you
that I have not managed convince the
designers to work on my slide it shows
you pretty much how this works on our
driver program we use PI for J it's not
amazing but it works and on our worker
programs we copy the data from the JVM
into Python we do all of our magic
computation and we copy it back this
does have some unfortunate implications
so no waiting for you to finish taking a
photo and okay cool ah so how does this
break essentially this talk is all of
the bad things that we've done in spark
you don't have to deal with and for the
Python users the biggest one is that we
have to serialize the data on multiple
times and we essentially serialize it
send it to Java send it back from Java
to Python in case you realize it and it
just gets kind of sad because we spend
all the time simply CC realizing this
data and it's a little depressing the
other parts aren't super important one
thing that I left off here is that the
error messages make no sense
has anyone ever understood a PI spark
error message and no one okay delightful
yeah so the error messages in PI spark
literal
are things like my iterator is like
invalid and that can be your lambda
expression in one of your maps is wrong
and that's just great
if you're interested in sorry I should
leave you with a solution rather than
like everything is terrible and on fire
oh yeah I keep forgetting to put the
happy putter in um now now thankfully
there are ways to understand the error
messages but that is a talk unto itself
for Python people and so I have a Python
focus talk on how to read Python spark
error messages and it's on YouTube I'm
sorry that it's like a 40 minute talk on
how to read your error messages but you
should really use data sets of your
implies on because it saves us from the
biggest downside of this delightful
architecture that we've chosen and that
is instead of taking the data and
serializing it back and forth all the
time with data sets when we write our
things in this DSL that spark can
understand spark just compiles a DSL
with Java and just runs up there and
we're fine
right we don't even have to execute
anything in python except for you know
our nice little driver program which
tells the JVM what to do notably this
all comes crashing down once we actually
need to use numpy and then we go back to
the double serialization cost because it
turns out that the java version of numpy
is called does not exist and yeah
anyways so I think I'm getting close to
the end there are some delightful SPARC
videos for the people who are in new
this part if I didn't scare you off too
much Paco has a really wonderful
introduction to Apache spark if you have
an expense account please buy it from
O'Reilly it is $100 but if you're
deciding between buying that in my book
he does have a free version of this talk
on YouTube and you should buy my book I
keep talking to people who have not
tested their spark code and this is
terrifying just because it's in a
notebook doesn't mean you don't have to
test it not looking at anyone in
particular but like please do not kill
me or lead to the next financial
collapse then I end up in one of those
nearing textbooks in 20 years about
someone who was killed by her own
software and that is not what I want my
legacy to be so here's a bunch of
testing libraries don't screw it up I'm
one of them is written by Jesse well
there's a blog post on how to use it
written by Jesse who is amazing Jesse
raised your hand yeah Jesse um and you
should definitely read it if you're a
Java user I think it's really good
maybe he tells you to use my testing
library so hmm there's some symbiotic
relationship there how they were here
our spark books I receive royalties for
many but not all of these if you're new
to spark learning spark is great
learning PI spark by Dennie I don't get
any word any on Tomas I don't get any
royalties from but it's also pretty cool
and I Oh Danny a beer so check out
learning PI spark and drop them a line
that said you bought it from me and it's
like six people do that I'm pretty sure
I don't have to buy them Bud Light Lime
oh no I forgot my Bud Light Lime okay oh
sorry um I have some post conference
traditions which you know but most
importantly buy several copies of this
book my my corporate compliance training
was very clear that I'm not allowed to
accept $20 bills in a brown envelope
anymore and they said it didn't even
actually matter what colors an envelope
was but I am allowed to ask you to buy
my book which I receive royalties from
not $20 mind you but please please buy
this it will be finished soon but do not
let that stop you from giving me your
money if you're looking for some places
to come I will be doing office hours
tomorrow at 11 a.m. at intelligentsia
these are unofficial not go to sponsored
or anything like that I just like
drinking coffee and if other people show
up I can put it on my after mentioned
expense account if no one shows up my
boss is getting suspicious that blue is
not a real person so I probably can't
expense it if anyone is from Europe I'll
be in Lisbon in Barcelona and
in Israel for the rest of this month I
am sure I will go home in June when
spark summit happens in San Francisco so
come say hi at one of these fine events
and I can buy you coffee on my expense
account especially if you rate this talk
really highly remember this talk was
amazing and the stuffed animals were
super cute so thank you for for coming I
I guess I can do questions is there time
for questions okay does anyone have
questions great that's that's a
wonderful question and so essentially
the question was I'm not shorten this
I'm doing something normally when I'm
doing it I have to start as a sparse
matrix but inspark
like I have this magic distributed
system do I still have to do sparse
matrices and the answer is no but you
will be buying a lot of machines and
that is fine from my point of view I
work at a hardware vendor from the point
of view of like future Holden who may
not work as a hardware vendor and will
work at a cloud service also fine from
the point of view of someone who may
eventually pay the bill
oh god please use a sparse matrix um
there's a distributed row matrix format
and SPARC it's not great but it has a
built in sparse vector representation
you can use and that will probably give
you the right level of sparsity and that
all probably work but like don't hold me
to it okay awesome I think that's it for
questions so thank you all for coming I
hope this was useful and I didn't scare
you away from using spark if I did
please don't tell my boss
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>