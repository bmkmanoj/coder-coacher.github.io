<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • Migrating Applications to the Cloud in a Cost Effective Manner • Andy Bennett | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • Migrating Applications to the Cloud in a Cost Effective Manner • Andy Bennett - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • Migrating Applications to the Cloud in a Cost Effective Manner • Andy Bennett</b></h2><h5 class="post__date">2016-03-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0suGyZsdG6w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so i'm i'm andy bennett I'm the I'm the
CTO at a company called skipjack and
we're a ref le felling newly funded
company last year and we're looking to
use machine learning guided techniques
in order to work out how to deploy
existing workloads into into cloud
services so I'm going to tell you what
what what it what it is we're doing and
how we how we're doing that and kind of
some of the learnings that we've come
across while we've been doing that I'm
going to going to be quite a technical
talk hopefully you won't feel like I've
I've pitched you at all and that's not
the intention so the underlying problem
that we're solving is that it turns out
that big a big enterprises have even
even much smaller companies these days
have a lot of apps they um they were
they write an app for something commit
and then in it never goes away so people
tend to even even though the DevOps
movement is making a lot of traction and
still there's kind of this attitude
where the developers will build an app
and then they'll throw it over the wall
to operations and then it will stay in
operations for a very long time the UM
often the workload will change over time
and ops basically end up with a lot of
poorly understood apps or apps that they
don't understand very well and apps that
are kind of doing things that they
didn't weren't intended to do when they
when they were first delivered and
despite all of this they need to have
kind of flexibility to deploy this app
in the right environment and so they
still would be effective in their work
even if they even if they haven't got
the developers on hand who built the app
and to understand the app so you know
they need this flexibility to control
the spending like the cost of cost of
running the infrastructure and the vast
majority of the stuff they're looking
after is legacy apps so
lapsed into appear and then they
disappear at a slower rate than the new
apps are appearing so that it turns out
that any one point in time whatever
technology you're using the vast
majority of what you have is is older
stuff and legacy stuff that you have to
keep running for some reason so they
have to as I said they have to UM still
be effective in their work and it turns
out that the best way to do this is to
over provision over for visioning is the
norm it costs a little bit more money
but it allows them to allow them to get
it done quickly and move on to the next
thing so ops ops departments in
generally they kind of want to be agile
so to the things at which we're trying
to help them be is more agile and more
lean I'm not really going to cover any
rugged stuff this afternoon but they're
armed they're interested in being agile
and what they want to do is get their
apps that they've been sent into
production quickly without manually
tuning them without a lot of manual work
and automating as much as possible and
the other thing that helps want to be is
that they want to be lean so in general
the ops departments are still considered
as a cost Center in the business and
they're kind of a necessary part of the
business but later they're not often
contributing to the core the core
product of the business and the core
business value that the the company's
product is offering so so they want to
reduce their costs and keep their costs
down as much as possible so we've been
we've been trying to apply some machine
learning in this in this area and our
customers tend to have lots of Java
workloads in the JVM a few Food Network
loads in the common language runtime
they're deploying on Windows and Linux
and they've got a ride variety of app
servers and in containers and servlets
and this kind of thing that they're
using so i'm going to talk i'm going to
illustrate what we're doing with the
example of of a small micro service that
you might deploy in amazon ec2 for
example and what we'll do is we'll take
your micro service and we will we will
send a test workload to it and
characterize it for performance and then
we will work out how much it costs to
run on different sized cloud instances
so here sir is the kind of output that
we might produce and
each one of these bar graphs is is 11
different type of instance type maybe an
Amazon ec2 like a tea too small or m3
large or a as yer a one basic or maybe
even a private cloud in vcenter or
something and the the darker bars are
the what we benchmarked your app app on
that instance with your with your test
load at the other base line as you as
you provided it and then the blue bars
on top of that are the extra performance
that we managed to get out of your app
after we tuned it with with with with
our software so what we do is we're
choosing we're doing a black box
optimization so we don't know much about
your source code or your workload we
don't know we don't change anything
inside your app but what we're trying to
do is offer tuning settings to for
example the JVM or the OS or the
platform in some way and so we vary
these in different ways based on the
feedback from the test and we also vary
them based on what we know about
different type how different types of
app respond to different amounts of
resources in different areas so the the
blue bar is what happens when we apply
the best settings that we found on that
particular instance type and so you can
see this this guy here this guy is a
it's a t2 micros or a t2 t2 medium
running in Amazon and you're getting
approximately a performance rating of
432 which is it's kind of an arbitrary
measure of relative to all the others in
that in that test so there's no
particular units for that but it's kind
of transactions per second whatever your
transaction happen to be and that costs
if you get a reserve instance pricing
for Amazon one at one of those instance
will cost you about three hundred and
twenty-six dollars a year to run check
the price this afternoon to localize
their current price and it's
approximately this after optimization is
approximately the same height as this
bar which is actually an AWS m3 medium
which costs costs about twenty-five
percent more to run and you only get a
15-percent uplift in actual performance
so depending on where your performance
target actually is you might be able to
move that workload from an m3 medium
into a t2 medium and still
meet all your essays and requirements
and if you can do that without really
any outlay of effort on the part of the
ops team then then then you've gotta win
the idea with the micro service is that
you can deploy lots of these so you can
reflect that twenty-five percent saving
across all of the all of the deployments
of that particular microservice in the
cloud so what we do is we do these tests
for all the different types of instance
and in order to get a optimal
performance reading for each one of
these gravel bar charts we're going to
do a series of a series of different
tests under different conditions on that
particular instance and so this is what
a search for one of those bars looks
like each one of these red lines is a
test that's currently running and it's
going to have a performance metric
depending on how high the bar is and
we're going to be varying things like
maybe the garbage collector settings or
or maybe maybe the way the TCP stack is
configured in the kernel or something
like this so we deploy the tests we do
all this all the settings and then we
take the best run with the best thing to
see is the best optimal configuration
for that particular platform now it
turns out that it's really difficult to
do this this um or to automatically I've
had a lot of conversations with a lot of
customers and they're pretty excited
about this when you go and talk to a
developer about this they get less
excited they they will tell you that
well you know I design my algorithm and
I know exactly how much memory uses so I
know how to choose the heap sizes in the
gravy airmen and I know their access
pattern and like and so I understand how
it interacts with the garbage collector
and they they believe that they have a
really good understanding of of their
workload and I think this probably true
in the case where they did they designed
their workload and send it to op so what
tends to happen after the after the work
that goes into production is that their
workload changes over time maybe it
grows over time maybe the visitors are
doing slightly different things with it
over time and so as the as the lifetime
of the app continues the way it was used
when it's
first deployed in this amount of
resources when it's first deployed is
actually quite a lot different from what
it can end up using over time and it
turns out that actually even if they
have a quite good feel for the parameter
bait is actually massive we will this is
just an example we can be consumed most
of the jvm parameters there's about 18
or 20 of them maybe like tom cat or
websphere or something has a handful of
parameters that you're going to try and
earn you that you're going to tune as
well maybe your underlying operating
system has like TCP queue sizes and and
swap space and different cpu and IO
schedulers that you can that you can try
out as well and it turns out if you work
out how many different configuration
permutations there are then there's even
with those there's there's just 28
different dimensions there and there's
there's a lot of a lot of scope for for
exploring this space and even if this is
the example of the characterizing the
performance on just one of the types of
instance at amazon for example here
we're doing about a hundred tests and
even that isn't going to possibly cover
it all or anywhere near to that my
marketing guy tells me we can do we can
explore a quintillion quintillion
different possibilities and so it's just
not reasonable to try them all out and
it's not feeling like it's not
cost-effective in time or money to do
that to do that so how do we do that
then so if you I'm going to do an
example here with just a pair of
parameters if you have a parameter two
parameters with two values each or well
you have you have two parameters with a
range of l-e-d chand you want to sample
just two different values of each of
them and so that involves four tests and
if you want to do two more tests if you
want to sample two more parameters for
parameters are two more values for
parameter 1 you end up with four more
tests that you have to do and each to
each time you add a dimension you you're
kind of having this explosion of
combinations and so there's there's two
things we do one is we can kind of take
quite a course coarse granularity over
the entire search space
and another one is we can kind of limit
that further by kind of cleverly
choosing the intersection points between
the different dimensions so that we can
work out one dimension against another
as an independent variable and then work
out the correlation between all of the
different parameters that you can set so
this is this here is what's called a
Latin hypercube and it's a little bit
like the eight Queens problem I don't
know if anyone's familiar with the eight
Queens problem but the idea is that on a
on a chessboard you have to put you have
a eight rows and eight columns and you
have to place on there eight Queens such
that none of them are attacking any of
the other ones and there's that there's
a whole bunch of different solutions for
this and what it what it means is they
you end up with at least one queen in
every row in every column so you get in
an intersection between those two
parameters at that point and that's the
without any other training data that's
the that's the bare minimum number of
parameters that you can use in order to
kind of get a rough idea of the extent
of the performance over that entire
entire space so we do a bunch of tests
about we try and we try and limited to
100 or 200 per per instance type and we
plot them for performance so we work out
what what they how well they did on your
on your on the customers metric of what
they're trying to do with them with
their workload and then we fit a wii fit
a curve to that to try and work out how
it changes over over the area that you
can you can perform the tests in and
from that we can kind of predict other
points and maybe run a couple other test
two and a verify or falsify hypotheses
we have about how the performance
changes in certain areas and on top of
that we then can apply some hill
climbing algorithms to try and maximize
the performance that you get for for
that particular instance but it turns
out this there's no there's no silver
bullet here the the hope was that maybe
you know if you set the heap size the
maximum maybe everything will go faster
always it turns out this isn't the case
we did some one of our one of our
statistics guys did some regression
and we found that for most most
workloads in most situations you can
come up with five or six parameters each
of which contribute a little bit there
maybe ten or fifteen percent to the to
the uplift that you can give on a
particular on a particular workload and
so the the bad news is that it's not
really possible to to do this manually
it's something that you have to automate
and the good news is that you can you
can actually automate it so it also
turns out that setting things for the
big numbers doesn't always get you the
best result like isn't it there's a
number of counterintuitive results which
turn up I mean perhaps there perhaps
they're kind of a bit more obvious to
people who have done a lot done a lot in
this area but often you have to know
quite a bit about the the app
specifically to work out the
implications of certain certain
parameters so not it's often the case
that if you increase the number of
threads available you'd think that it
would be able to do more stuff at once
and it turns out that often if you if
you decrease the number of threads under
high load it brings in the amount of
time that it takes for each each
resource and so the system ends up less
busy and I can actually get more work
done because it's doing it's doing less
concurrently so it's not sharing as much
stuff and um there's also quite a lot of
easy wins to be had without with a lot
of applications because most of them has
never actually been optimized at all
often they're they're written for /
developer and then they deploy it for a
very specific use case and over time it
grows into something else it becomes it
becomes an important appt in your
business and people start making demands
with that that they never did before so
it becomes a it becomes used in a case
where it's never that it's never been
optimized for and so you can end up with
a lot of apps that you've never never
optimized and so there's often quite a
large gain you can make it by doing
anything at all they'll let alone
guaranteeing that you have the optimal
result and it's also the case that that
the that the scalability of an app is is
limited
and so there's a there's a theoretical
maximum that as a particular app can
scale up to honor honor on a machine and
if you give it a bigger machine than
that then it won't get any faster
because of inherent limits in in the
architecture of the app and the way it
was written and there's often there's
often theories about where there is
where this limit is but it's kind of
often unknown until you reach that limit
and so if you if you're able to do some
automated testing you can discover where
that limit is before you reach it and it
helps you to do capacity planning and
and and cost management and do and
prioritize the right kind of development
and I'm frightened so you don't end up
in a situation where your where your
fire fighting so that's the kind of a
quick overview of the of the machine
learning site and how we how we apply it
and the kind of benefits that we can get
from doing it obviously some of us SAT
there thinking well he's running like
200 tests that's that's not a small
that's not a small number to run in a
conventional way and this is where we
can really take advantage of some of the
things that the the kind of the clep
more cloudy offerings have to offer so
kind of the software-defined
infrastructure where you can spin up and
down serve servers with with api's and
this kind of thing and i'll also go into
a bit more detail about how we how we
actually run the test itself so this is
a graph of of one particular test the
horizontal axis is time we test run for
just over an hour and the vertical axis
is the on the vertical axis on the left
is the transactions per second in
performance and the vertical axis on the
right is in bytes so this red curve is
the important one that the red curve is
the number of transactions per second
that our app was doing or a micro
service was doing at a particular point
in time and it's got its got three main
features one is the little ramp at the
start and then we're onto onto the
plateau and then there are four main
features then the ramp at the end and
then the decline at the end is the
fourth feature so this is a um this is
this is a driver workload and what
happens at the start is that the the
green line is the load
we're presenting to it from the from the
thing that's generating the test load
and the red thing is the number of
transactions per second the application
under test is doing and what you can see
is that it takes some time to kind of
warm up and this is a well-known
characteristic of jit compiled workloads
in the what the Java compiler does is it
as the as it worked out what the hot
pass through the application are it
compiles them into native machine code
for that particular platform and it
makes them go faster and so this can
really this can really hurt your measure
of performance if you measure it at the
wrong time because if you just leave the
load stable for a bit you can find that
your app suddenly gets faster without
having done anything so that the JVM
itself is quite good at good at
self-tuning and this this blue curve is
showing you the size of the size of the
region that the JVM is storing the
compiled code in and you can see that
when the region fills up it's about set
to about 100 megabytes here when it
fills up the performance levels out and
and that is the that's the optimal
that's the optimal processing rate for
that that particular load that was given
in green and so when we're sure that
that phase is complete we can move on to
the next phase which is to gradually
ramp the load up until we can find where
the peak is so as I as I alluded to a
little bit earlier if you if you
overload the system actually there the
the response will go down again because
things end up stuck in queues and
waiting for things and so the the
contention increases and then the the
response of any of the latency of any
particular response goes down so as we
increase the load we find a we find a
peak point here which is which is the
which is the optimal point for this
particular machine with those particular
settings that's as far as it goes and if
you continue to increase the load still
it then it starts to tail off again and
what you find is there's a bit of a
there's a bit of a cliff there we're
basically it when you when you get to
the machine to saturation point is it
performs really poorly and also has
quite a lot
jissa you see when it was in the stable
point before it was nice and flat and
got the variance in the blobs was quite
small that's quite a nice place full of
machine to be running whereas if you're
running it up here then you're not
really that sure what's going to be
happening to your workload it could be
some things could start to take a really
long time and what what you'll find is
that these are averages I'm plotting
here but what you'll find is that
probably if we difficult because this is
a normal mean average fifty percent of
your your users are getting worse worse
worse experience than that so if you
keep that variance nice and small you
can make better guarantees about the
fact that you're delivering a good
service to everyone so the idea is to
pick pick the optimal point and not not
give too much load this graph here goes
into a bit more detail about that the
greater that the red again is the
performance the transactions per second
that though that the service is doing
and the green is the the latency the
average latency for that request so the
the length of time it takes for a
particular request to happen so the the
red is the number of the number of
requests that are finishing per second
and the green is the number of that the
length of time the average length of
time that each request takes so it
assumes that there's more than one in
flight at a time so that if you take the
point where you're getting the most
throughput you can find that the latency
is not the smallest so you can you can
actually trade this off you can say
actually I want to I want my service to
be more snappy for my for my user and so
you can pick an arbitrary point that you
that you want for your your service
level agreement on the latency curve and
you can move the you can move the bar
back so that you say the optimal
performance is not just how much
throughput I'm getting but it's it's a
certain amount of throughput with a
guarantee that most of the requests are
completing in 50 milliseconds or
something and so you can move it move it
back to their if you pick a point
roughly there you can move the line back
to there and then your your maximum
performance for that particular
configuration becomes that and so you
you're trading off a little bit of
throughput for much better response time
for each of those and kind of much
better quality of service for each of
those users any questions so far you've
been very quiet am I going too fast or
too slow okay so that's just the aetna
that's kind of the life cycle of a
single test what when we were actually
doing a test you have to get a machine
from somewhere and it turns out that if
you get one from for example Amazon you
can you can buy an instance on demand
and then they will bill you one hour w
in one hour increments for that so
you've got to spend some time starting
your machine up and that costs you money
and you've got to spend some time
installing the thing you want to test on
there and then you need to actually do a
test and the rest of that time until the
billing tick happens at the end of the
hour is wasted so if you if you only
spend 20 minutes doing the test you're
playing for 40 minutes that you didn't
use if you spend 61 minutes doing a test
you go into the next billing period and
you waste 59 minutes and there's there's
there's various ways of solving this you
can maybe you can reuse the machine for
another test there's basically the green
bits the way you're doing the testing
and that's that's the actual value for
the application and all the rest of the
time that that machine is doing things
is it's just overhead and cost for for
the testing and like I said we're doing
approximately 100 or 200 tests on a
particular machine with different
configurations at each time and you can
trade off the length of time it takes to
do that entire search against the cost
of that search because if you you can
you can spin up a hundred of those
machines individually and you can do one
test on each and you get your result in
an hour because that's approximately how
long we test for and you waste thirty
three fifty percent of your of your
money on compute power that you never
used or you can you can basically double
the length of time it takes and and due
to tests per machine one after the other
and then you can you wait two hours but
it costs you about half as much I mean
there's quite a lot of quantization here
because the billing period on Amazon is
an hour you you ends up being quite a
cliff when you go over into the next
hour you you have this kind of big extra
overhead on other clouds such
some such as Microsoft Azure they have
much finer granularity of billing so
they tried you for the first hour and
then after that they charge you only for
the minutes that you use in the in that
period so part of what we've built is is
some some schedulers to to deal with
this kind of thing if you go and read
the the literature quite a lot of people
like Google and Facebook and people are
talking about how to schedule where how
to share your workloads on kind of what
they're calling warehouse scale
computers where they where they own all
the computers they can place the
workload where they want and they they
have control over everything the kind of
scheduler we've had to build is is is
about placing different workloads on a
variety of different clouds in both
public clouds and things like like visa
into that customer zone where other
other users are on the hosts so we might
have noisy neighbors and also we get we
get billed per in this some example I've
just given so so it's kind of a slightly
different type of schedule ER and the
example I've just given it's just for
one assumes just one host in one test
but actually what happens is often use
your spin up a a host for the micro
service and you'll spin up a host to
deliver the test load and you might spin
up a host for the database server or the
message queue or something like this and
so you end up with quite a complicated
scheduling scenario where you might have
a host ready that can be the database
but you don't have the host that's can
be the the message queue and you have to
kind of try and pack the work in as
efficiently as you can and try and
optimize for the conflicting things
which are links of time it takes to do
the test and price of the test but it
works pretty well for micro services
because you can you can typically get
quite a good saving overall and the cost
of the test is generally significantly
less than the cost of one instance and
with a microsoft teched you tend to be
scaling out a number of those over a
longer period of time so the cost of
test ends up being being quite efficient
and quite small so that's the material I
have and hobby interested to take some
questions and I like if you could go on
the app and let me know where you're
sort of a talk thank you
okay so I'm going to look on here for
some questions in the meantime does
anyone have a question I would like to
inject do you always use yours just you
put random parameters or do you ever use
things like profiling or garbage
collection log analysis that sort of
thing to help inform your sort of
initial set of parameters so at the
moment we we can do two things one is
it's like I said of a course a course
examination of the entire surface so if
you've got a continuous property like
heap size we might we might try it from
0 to 4 gigabytes in 100 megabyte chunks
or something the other thing and that's
quite a quite naive the other thing we
do is try and fit these in using the
Latin hypercube to pick points the work
well the other dimensions and that's not
particularly amenable to forcing a lot
of choice on a particular parameter you
can typically choose one or two and then
the others kind of tend to fall in in in
particular places or requiring an
orderly large amount of computer time to
work out where to put them another thing
that we we're trying to do as we collect
data is actually mine the data for
different apps and so classify the apps
into you know this looks like an i/o
kind of workload this looks like a cpu
kind of workload this is a database this
is a message passing system and if we
can classify the apps like that then
it's we can apply the learnings from
optimizing those apps and kind of a more
ties the cost of exploring the parameter
space across a kind of a whole suite of
apps so the number of tests that are
required to X cubed R do you know to get
to a certain level of optimization is it
is it totally dependent on the number of
parameters that you that you have or
there is some kind of optimization over
there L they're also like you know if if
I have ten parameters ten into ten it's
like
in fact 10 to the power of 10 yes we are
talking about hell lot of guests right
yeah how do you so yeah every time you
add a dimension or so a parameter is one
dimension or add a new value to the
parameter you basically have a power
relationship so you explode the
parameter space and so it's infeasible
to test that many number of parameters
using a lot of this kind of search
methods with this kind of search method
trust us distribute the point evenly
throughout the space and when that's
basically it's bigger you need a lot
more points to fill it up the hypercube
method actually worked the other way
around you tell her how many points you
want and then it tries to put them in
the best place and so you can fix the
number of points and it will try and
distribute them fairly regularly
throughout the space but also try and
get this specific point where it thinks
are going to be interesting based on
parameters how parameters might indirect
essentially nice time today x 800
Thank You Mandarin violet there's again
a mute so yeah yeah in the screen the
count of the square the course method
that's the case in the in the hypercube
method you can say don't put the points
closer than 10 megabytes megabytes apart
buddy might not put them that close
it'll probably spread them out further
if it sinks that's not a very
interesting dimension I typically you'll
find the heap size kind of isn't isn't
very smooth so like bit varying it
between 0 and 100 megabytes or 0 and
half a gigabyte isn't very interesting
but varying it between 3 gigabytes and 4
gigabytes might be quite interesting for
a particular workload and so what you
can do is you can kind of put the point
fairly costly and very fairly sparsely
to start with and then go back and and
when you fit the when you fit the
surface like this you can see a
discontinuity and then you can do
another set of tests to have a look in
the place where you think the
discontinuity is to see what where the
inflection point is so I've got some
some questions from the app I'll come to
you in a second yeah so that there's
nice qualifier with this so how do you
handle predictions if you have things
like cluster management software like
humanities or me sauce that have maybe
moving workloads around while you're
sampling and then immediately followed
by it if this is NDA please ignore so
you don't have to tell us it's tough it
is tough like part of the stuff in the
scheduler at the end was to do with the
fact that we're trying to place
workloads on public clouds and when you
have a when you're a Google or Facebook
doing the warehouse scale schedulers you
can place those things based on I mean
it's kind of a hard problem anyway but
at least you have all the data that you
can you can solve for the things you're
interested in there's a lot of things we
can't solve for and noisy neighbors is
something that we try to minimize and
certainly the different clouds give you
different abilities that Amazon a kind
of offering some of the best features
that on some of their larger instance
you can now select to be on the same LAN
segment so that when you're squirting
the test load at the machine you're not
going to be subject to as many
as much jitter in the network as if you
just got a rapper to random placements
in the same region and so that yeah
that's there something we have to
control for and it's it's part of the
work we do with me when we write a new
driver for a new type of cloud part of
the kind of product ization process but
there's a how does it work question just
curious if the machine learning part of
this is using genetic algorithms or
something smarter to find optimal
configurations so what does it do what
does it do I think that's a good
question how long have you got so the
genetic algorithm works by a kind of
quite a quite a small number of tests
and then quite a long lot of iterations
we try and work we try and work the
other way around so that will do more
tests upfront and then only maybe a
couple of rounds of iteration 2 to mine
the the local areas we think are
interesting because it turns out for our
customers that time to time to
gratification is quite important so
they're good that they're happy to wait
maybe two hours or three hours they're
not happy to wait a week so there's kind
of a limit to the amount of feedback we
can have between tests so most of our
most of our machine learning is on the
analytic site where we're kind of mining
mining the information in that we
already have and kind of doing some big
data analytics on the on the telemetry
that we have from kind of lots and lots
of tests over time cool excellent i'm
gonna go running over here the
microphone so my question is have you
ever had any really big surprises when
you initially had a fairly sparse test
space with their some broadly separated
parameters and then tighten that up by
say in order of antigen and suddenly
found a major optimization point on
something where performance is radically
improved which you wouldn't have found
had you not tighten that up it turns out
that's nothing to really answer the
question your wife to ask me again where
it's important but turns out the best
wins come from when you discover bugs in
the application so a lot of the stuff
that's not been tested before when you
test it in this kind of a man this kind
of intensive manner you end up finding a
concurrency but quite a lot and all you
end up finding a scale
bugging some some minor code that
generates the HTML table or something
and you pull that out and you get you
know thousands of percent and you've had
your big wins after that often often
we've got some quite good after all some
we've had some quite good wins by kind
of making compromises like the often
you'll find a workload is provisioned
for over provisioned or provision for
its peak load and what that peak load
may be a completely different kind of
workload that happens overnight and so
you can it completely ignore it
optimized for the the common case which
happens during the day and then either
reprovision at night or add something if
possible maybe add a swap space that
allows it to complete in the night time
period but maybe it takes a bit longer
than it did before or something like
that say that the big wins are more more
lucky than you think they're not they're
not that technical smaller winds are
kind of more gradual and you kind of
tease more out of the data and have to
look at the cross correlation between
the parameters and how they influence
each other fantastic and they went one
more do you see the same cost savings
for services using non JVM stacks is it
specifically in the JVM the order most
of our work is on the JVM and on the on
the common language runtime they're both
pretty similar um brought both pretty
similar and that's we're almost to our
data is at the moment so what about
things like if you had something like go
there's a statically linked binary and
carries its own vm with it I don't think
we've tested any go I'm afraid okay
thanks very much indeed that's fantastic
we're going to take a short break and
then it ones back here at five o'clock
for the closing keynote so
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>