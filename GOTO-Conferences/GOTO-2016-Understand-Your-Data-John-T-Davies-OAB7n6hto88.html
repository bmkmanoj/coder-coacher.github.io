<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Understand Your Data • John T Davies | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Understand Your Data • John T Davies - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Understand Your Data • John T Davies</b></h2><h5 class="post__date">2016-12-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OAB7n6hto88" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I
I'm going to go through some technology
which I've been working on for the last
23 plus years it's some ideas which it's
really history repeating itself I
started programming back in the 70s
assembler and then got into C C++ in the
80s obviously I was at school at the
time and in those days we had machines
with very very little memory and every
little tiny bit of memory you had to use
you have to use it very very efficiently
anybody remember the BBC micros
Sinclair's at X 80s 81 s all those sort
of things they had ridiculously small
one today's standards ridiculously small
amounts of memory so you literally had
to use for example the the screen
buffers you'd have double buffering and
and if you need to do some computation
you'd often use the screen buffers for
these things and if you wanted to run a
game where you want to have something do
some computation you had to use
literally everything so what I did was
working in financial services started to
go and see a lot of clients and these
clients were putting hundreds of
millions of messages and in many cases
billions of messages and they're busy
using Hadoop and they're spending a lot
of money to do this and spending a huge
amount of resources not just people but
equipment as well and and I thought this
is a little strange because you look at
Moore's law Moore's laura's just
identifies what's happening it's not the
actual sort of reason why it's happening
but these machines are getting faster
and faster and faster we're getting more
and more RAM and I was thinking okay the
date is not increasing that much it is
increasing a lot but something's wrong
with this anyway so we got into looking
at we have the way we can improve prove
this so ironically this beginning of
this week started off a little a little
strange for me because I've been working
at a company which i founded in
co-founded with some colleagues in two
thousand we sold
2007 we bought it back in 2011 and at
the beginning of this week that's Monday
sold it again interestingly this company
now has the technology which we
developed over this so it's they bought
other things as well but it's sort of
proof proof in the pudding that it
actually works so let me give you a
little bit of background why do we
distribute as we get larger and larger
we need more and more computational
power we need to start to use other cpus
any of these machines we buy we can buy
some really powerful machines today we
can buy machines with 48 cause we can
have multiple CPUs in the in the box and
you get these things which would run
entire industries just a few just a
decade ago in fact but you get to the
stage where you need more CPU power and
to do that you've got to distribute and
share the load and to do that you've got
to go across a piece of wire we often
need memory as well why do we use memory
well it's a lot faster than disk disk is
very cheap but it's incredibly slow and
the next slide I'm going to sort of
point out just how slow that is so some
of these numbers I'd like you to try and
remember these not not the actual
numbers but the ratios between them now
the reasons this is something you can
just grab off the internet it's fairly
well well known i've read done this side
they used a different ratio now the
ratio of these this is to sort of put
for people that don't or can't
comprehend nanoseconds picoseconds
microseconds etc this is to put it into
sort of real-life numbers that we can
sort of understand the way i work this
out was a round-trip San Francisco to
New York is about six hours one way five
hours back etc and on an aeroplane so
I've basically sort of put it down into
aeroplane standards the difference
between the speed of light and an
aeroplane is about a million times so
these are 1 million times slower than
these so let's look at this if we're
writing data to to disk where we're in
the millisecond realm it takes a long
time for the disk if it's constants
obviously constantly spinning but the
head has to seek to the position where
your data is that's extremely
time-consuming now you'll have multiple
heads multiple platters on the discs etc
but it's very very time consuming you
can you can obviously speed this up
somewhat with with different levels of
raid then we get to SSD and we're
getting a lot faster there's no physical
movement of the disk and we're down to
sort of milliseconds micro second times
when we get into ram and this is the
reason why something that doesn't write
out to to SSD or to disk is so much
faster so we can see there's quite a big
improvement here but look at the
difference between the memory here and
the l3 l2 cache that is the cash that is
in your CPU now that cash is actually
quite large for the average even a
laptop for the l3 cache which is the one
shared by all of the cause it's about 16
megabytes 24-32 you can get larger these
l2 caches they get smaller this is
shared by the just each core we're down
into sort of 500k but there's a lot you
can get into 500k that's still typically
ten times larger than the BBC micro
computers that we were using just 30
years ago so there's a lot of leeway in
this the problem that we have with this
distributed technology is is not the
serialization that's that's actually
putting it on the wire it's the process
of taking it off the wire it's the
process of decoding it any data that we
send to a computer that we send for
processing whether it comes from another
computer whether it comes from a human
or whether it's come across the internet
needs to be understood by the CPU it
cannot be processed properly unless it's
understood by the CPU otherwise it's
just a document or a random pile of
bytes so this the big issue here is d
serialization let me let me show you
what this looks like this is just a the
this is the post mortem using this
rather clever tool that I use a lot from
J clarity of a very simple program
that's generating comma delimited files
or comma delimited lines so it generates
as you can see five million of them here
it generates them writes them to to
memory or two discs and
then reads them back so the time to
generate them is here the time to write
them out is here and the rest of this is
the time to read them back why well
because the way that they're stored the
way that they're see realized whether
you serialize them to memory to network
or to disk is not correct or not
understandable by the CPU so as the
read-back they need to be put into a
form that can be used or it can be
useful if we look back at this who's
programmed in c or c++ before there's
quite a lot of you so you'll remember
the days of if you're trying to make
something efficient you bit fields you
can easily you got used to doing this
the whole point of java was too abstract
us from the different size of integers
the the different big Indians Little
Indians and you get this language all of
a sudden which is the actual language is
standardized it's not the hardware and
depending on the hardware depending it
then dictated the size of the integers
and things you'd get that made it very
difficult to have portable c and c++ if
we were to take three letters we want to
store ABC and moving into the financial
services worlds ABC would obviously
represents in this case not the currency
but it's gbp/usd EU are we want to store
this or serialize it or read it back if
you're doing this in most low-level
languages you just need four bytes to do
this we have two choices we can either
stick the 0 at the end this means that
this ABC has ended or we could put a
number at the beginning here which says
three characters now if you know it's
always three characters because its
currency you could actually say fine i
will just reserve three three bytes and
we can make this far far more efficient
we just have three bytes knowing that it
will always be three bytes and when i
write it it will always fit in the same
space so if i got three currencies it
will be nine bites nice and easy when we
put this in Java we have this many bytes
or bits 48 minimum so every single
currency is
that we store takes 48 bytes now we can
Kate take a copy of one of those and
make a reference and it will be smaller
it'll be 16 bytes but remembering that
the data is coming off a piece of wire
off a feed or we deserialized it if even
if you get the same currency twice you
get GBP twice it doesn't compare it and
say oh this is the same as the previous
one I had it just reads it in so each
one has 48 bytes and this is what we get
and it's not just with strings it's with
pretty much any java objects some are
more efficient some are less efficient
but every single java object that you
read and write into these things is
going to take at least 48 bytes we could
optimize this we could say okay we could
put we only need to bite to store the
year the month the day we could optimize
it we get a little bit less and we could
probably get this down to about 16 bytes
but 16 bytes when we only need three
seems quite a lot remembering now if we
were reading this off the wire the
difference between reading 16 and 30 s
16 to 32 40 48 it's already quite
significant so we would make some
improvements however if you take let's
just take a date and I'll can i'll come
back to the currency but if you take a
date this represents it's not the real
stuff this represents the amount of bits
you would need to store a date in java
if you actually think of you you all
obviously if you've been programming in
c c++ and even java you'll know the
epoch date first of January 2070 and
everything is counted from there if we
take that date and it's very easy to get
hold of that date divide it by the
number of seconds in the day we can get
the number of days since first of
January 2070 that number is less than
65,000 which means we can store it in
just two bytes which means we can store
any date between now and about 2160
something rather as two bytes so we need
just two bites like this as opposed to
48 bytes like this if that data is now
stored on the cpu as two bytes we can do
date comparisons we can we can say get
everything great
than the state less than this date equal
to this date and we can now start to do
analytics and it's in the right format
imagine now when we see relies this
instead of serializing and when we do
Java serialization we don't just see
relies this we serialize the name of the
class as well so we get java.util.date
and the content of this so the actual
serialization of a date is actually
about 80 bytes is quite significantly
larger rather than this if we know we're
serializing a date and we see relies
just to the bites when we read it back
in its going to be vastly more efficient
because we can already use this it's an
it's in a cpu efficient format so extend
this a little bit further we take
something comma delimited file this is
the one that I used in the example we've
got dates so we could now move that down
to two bites by cell anyone want to
guess how many bytes we need for that
come on it's not that early in the
morning one bit yep so we need one bit
bad bits are inefficient so we'll use a
bite for this bits we can use if we had
lots of them we could use lots of bits
and eventually we'd make a bite but yes
a bit as the is all we need typically
we'd use a bite for this currencies now
we said three characters so we'd need
minimum three characters but if you
actually look at the iso currency table
though are only in fact 163 currencies
defined so 163 currencies less than 256
therefore we enter need one bite to
store every single currency what if we
want flexibility what if we want to put
precious metals in there we have x au
for gold x PT for platinum ok we can add
a few more in there there's not that
many precious metals that are traded we
could also add an escape sequence so we
could put an escape sequence so if it's
zero or 255 we could say that the next
three bites are the currency so we have
plenty of flexibility in this numbers
there I Triple E standards already
pretty efficient so we're not too far
off with this we do need some
flexibility we need to make sure we
don't get rounding so we're probably
down to four bytes for these so
with exchange rates we can store
exchange rates are only five significant
digits therefore we just need a two
parts the five significant significant
digits and the the multiplier 10 to the
power of back to the currencies back to
the numbers and back to the dates again
so we can store this very very much more
efficiently this comes in in in this
format that's roughly on average about
70 bytes per line this is using ASCII
naturally if it's you you encoded it's
it's twice the size but let's just stick
to ask you for the moment when this
comes in it's not in a cpu readable
format so it has to be parsed this
character here the two is ASCII code
actually character 32 the next one is 3
6 30 34 35 33 etc so those have to be
computed x Tanner's or / 10 as they come
in so that we actually take this number
which is in binary encoded decimal in
fact nosing it's not it's not even
binary coded decimal especially in in
ascii and we have to read that in and
that's the the thing that is taking the
time to pass this file so if I ask
someone to anyone of you or anyone else
in an interview question to code up a
class for this this would not be
something that you would fail the
interview for it's for the data that
we've got up here that we can see this
looks reasonably good possibly fail them
in an interview if they put an engine
here because it's very quickly to very
easy to wrap around the int in some of
the telcos stuff that we've done we've
actually wrap around the the lungs as
well we found after a couple of days the
the lungs of wrap round because we're
the amount of data we're getting in
millions per second anyway trade dates
strings but think how much spaces is now
taking not just in memory but think of
the serialization so we got obviously
eight for this and the way that Java
stores it we've got forty-eight
forty-eight 48 48 etc all the way going
down when we see relies this it's even
worse and I'll come to that very shortly
so we can write this class we write
getters and setters and with the getters
and setters
we could do something as simple as pars
the line we use a line split so we just
say line splits on the on the comers
it's not doing anything particularly
fancy and we just simply use the getters
and setters in here to say set trade
date we get the format we get each of
the fields 1 2 3 4 5 etc and everything
works it's nice it's pretty quick we
could pass as you seen several 20
million lines very quickly and that's
classic java the problem is when we
start to scale this and this is this
this is the problem that we've had since
Java came out in the mid-90s we've gone
we weren't globalized in the mid-90s
because although the internet did
technically exist no one was using it we
didn't see the internet coming in and
messaging taking over from databases
we're in a database centric world in the
90s as the Internet's come in we've
we've now got a message centric world
and it's quite normal to expect to get
messages coming in from from everywhere
on the planet and all of a sudden the
scale that we need has just gone has
ballooned out and our simple java now
starts to show performance problems so
this becomes very very verbose who here
thinks that XML is a sort of a both
standard for data if you get quite a few
of you what i can tell you java is worse
java is actually worse than XML so if
you put in your data from Java into XML
you're actually saving space believe it
or not this 70 lines as roughly 70 per
line comes out at 328 bytes in memory
that's a lot so reading in a million of
these is not 70 megabytes as we now need
328 megabytes this is half a gigabyte
just to read in a million of these and
that's assuming you haven't got any
inefficiencies in the way that you've
you've stored this so we've already
bloated it about four times larger and
that also has to be read from the disk
or read from the network that's bad to
start with but we haven't got into
civilization yet so Java is really
bloating things I like to think of it
like this for those of you that program
in C C++
everything if you want to to process
vegetables it's all beautiful crisp
really nice and fresh when we working in
C++ it's pretty hard to to get
everything right and the memory right
and yes this all could get crushed and
distorted if you abuse your memory but
when we work in Java we work in a much
more protected world every one of those
things is is wrapped up its wrapped up
with garbage that we don't need those
two bytes that we actually needed for
the date yes I've stripped off the hours
minutes seconds and milliseconds but we
could add those with another few bites
this is even this isn't even correct in
Java each one of these would be
individually wrapped inside the
container so we'd have individually
wrapped things inside wrap two things
which would be inside more plastic bags
and that's what we get and that is
another problem so we got two problems
now one is that the data is inefficient
and we're not using it correctly so the
CPU is working overtime seriously amount
of overtime but we've also now got to go
back and collect all of the crap that
we've left as we process this so
everything that gets left around this
needs to be cleared up it's not just
Java for anyone that thinks ah i'm using
the latest wonderful language that makes
java look silly I'm really trendy so I'm
using one of these things actually that
some of them are pretty good Kotlin is
great but it's the same problem you're
working in a JVM and if you think okay I
mean I doubt many people who works in
Microsoft here with Microsoft languages
okay few of you I mean you have the same
problem you're working inside a virtual
machine the virtual machine is
abstracting you from the hard way you're
getting the same issues with this if
you're putting things into memory rather
than using a map or something that comes
with the language you're probably often
using one of these in-memory data grids
this i would say solve but it makes your
life easier with distributing the data
so if you're putting data into these you
can write into gemfire coherence neo4j
hazel cast etc you can write it in it
will distribute them for you but what
you're not seeing
is is all of the stuff that goes on
underneath and all of the serialization
now some of these have optimizations for
this but it's it's still a lot of work
that needs to be done so how can we
solve this let's let's go back look at
the the Java work if we were taking this
classic and we're putting it into
standard Java code we could generate
this and this is Java binding as those
of you that have used anyone used Jack's
be jib X these sort of things so you've
got perhaps a database you've got you've
got XML and it will generate the Java
code for you that's relatively simple
and easy to do but what it's doing is
generating this from this and we just
said it sort of it's acceptable it's
okay but it's starting to use a lot of
memory so what happens if we just store
the row well all of a sudden we've now
improved the size of the memory from 324
down to just 70 bytes so we've made a
huge improvement but now I've got a
billion of these rows they're much
smaller which is great how do i search
through them if I want to search through
these I've now got to go back and pass
the damn things so when i'm reading this
line I want to say get everything which
is above a particular date which has a
buy with euro against dollar I have to
pass every line and every line that I
passed I then have to pass the data in
it to understand it I could build an
index ah but I'm storing objects and the
index so it makes a complete farce of
having to store it in this way so it's
very good for storage but it's not any
use for actually processing the data so
i need that cpu needs to understand
what's in there so original data would
go in this is quite useful for for XML
because it's you've got to parse out all
the XML if you take a standard XML
contract like f pml in in the banking
world it's it's up to about 25 k just
for every single contract and you could
have hundreds of millions of these
things so it uses up a lot of memory
there are banks around here in the city
which have tens of terabytes of in
memory storage and that's a lot it's
expensive
what about compression or compaction
okay what's the difference well if we
were to compress them we have two ways
as anyone that's tried to compress a
file there's two different ways you can
do it you can either either compress
individual files and you get a
relatively okay compression ratio but if
you compress everything together you get
a much better compression ratio okay so
now we've got a million of these rows if
we compress them that's pretty good but
if we want just one of them we have to
decompress the entire 1 million of them
that's not so good and not only that but
it generates and needs a lot of memory
to do that so now we can press
individual lines instead well hang on
that also takes a long time not only
that but I've got to decompress it
before I can even understand the content
of it so that makes it even slower that
we've saved even more memory but it
makes it even slower if we compact it
let's go back to the Java date so if we
compact it we've now only got two bites
and we don't need to decode the entire
message to be able to read those two
bites we just need to know where they
are so we need to do is calculate where
those two bites are that we need we can
read them into the CPU and we can
immediately do the comparison so move
this into binary if we store every one
of these rows simply as a byte array and
that byte array is not the ASCII that
was in this but the binary that we have
replaced with this with so we have maybe
four bytes at the beginning for this for
the or eight but in what we want to keep
it as we have two bytes for the date
there is no time in there so it's just
we need two bytes for that we have one
bite for the buy-sell we have one bite
for the currency so we need etcetera as
we go across we can get this down much
smaller hi you say well now we've got to
change all the code well you're working
in an object-oriented world we have
encapsulation so if we have gets trade
dates and set trade date which is the
typical way we would do with the class
if we replace that with get trade date
and I'm just using the simple example
here so
this is getting the date from the data
so we we go and find this is just
something that goes off and finds the
date in this particular cases could be
complicated if it was something larger
it could be simple if it's a fixed
length format so in this particular case
it'll it's at offset eight and we read
the word back now we don't have words in
Java so we read this back into a long
the rest of it is zeros or whatever you
want it to be and Java has this rather
convenient ability to actually take
along which is the number of
milliseconds since first of January 2070
what we do is multiply this number by 86
million four hundred thousand which is
the number of milliseconds in the day if
we do that we pass in a new date we get
back the original day so we can go from
two bytes to a full-size date why now we
just said we really want to avoid
objects because we want to avoid garbage
so what have we done here while we just
we've used this this word new new crates
and other objects so every single time
would be calling this method we will be
getting a new object surely that's not
good well Java the the way the
allocation works is divided the JVM is
divided up into separate sections these
if they're used very quickly because
they used we get the datum we use it as
a comparison for example as soon as it's
been used its garbage collected is from
the eden space so this is like I feel
like a ring buffer it's incredibly fast
we tried to do some some clever things
with this by by using static fields on
this so that they'd be reused and we
went to talk to the guys at Oracle about
it and they said Bob just use just
ignore it just use new it's actually
faster it will be garbage collected even
quicker we did and with the whatever it
was 1.6 Bush underscore sort of 70
something rather onwards it worked
incredibly fast this is effectively it's
free it's almost free it's it's comes
down to sort of single single
instructions same with the trade date we
take the date we get the long we divided
by the same number
then we we use these things that C++
programmers will remember very very well
we use them all the time or C
programmers these are rotate right we
have rotate shift etc now I won't have I
don't have time unfortunately show you
the code but if anyone's interested I've
got the code you can if you've got a
memory stick you can walk away with it
or give me an email i send it to you
it's just a simple example of how this
works and in this I've got two
comparisons I've got the same piece of
code because we're using effectively
like an interface we're using the
getters and setters I can run it with
both classic java objects than i can run
it with binary objects looking at the
results and all i do is run it as the
demo so looking at the results this is
the amount of bytes used in the classic
java object this is what we use with the
binary version i could compact that down
a little bit further as well because i
used an entire bite for the for the
buy-sell when we see relies it as i said
the serialization is the name of the
class and then the content of the class
is tag value pairs as we serialize but
if we see relies one object that
contains multiple other objects you get
name content and then inside that you
get name content name content name
contact so this each row started off at
70 bites now see realized is nearly 700
bites 10 times bigger that's why Java is
slow not only that but it has to read it
back and still decode it the binary
version is 85 now it's gone up because
it's actually the class name that I used
something like binary demo or something
like that so I've got John Davis binary
demo or something and that's where these
extra bytes have gone now we can now
because in a Java object if you wanted
to store a string you don't know where
the memory is for the string that's the
whole point of the encapsulation you
have a point or a reference for the
string you can get the data from the
string and you can actually take it out
and you can write it that would be
customized serialization so we could
avoid it but one we've got a byte array
we know exactly where the memory is
because it's a we can basically iterate
through this byte array so we can now
use custom serialization because we know
it's 39 bytes in size in this particular
case not in
case it doesn't vary so let's just round
it up to an 8-byte boundary and we get
magically 40 this means we can now write
if we've got a million of these we can
write out an array of a million times 40
so forty megabytes 41 million of these
things in one go interesting advantage
now we can seek to any one of those and
we can read out the data almost
instantly not only that but when we read
it back in we can actually if we know
we're reading in a million we can read
in 1 million rows as using Java niño in
one go directly from the disk directly
back into memory not only that but it's
immediately understandable there's no
pausing to be done now look at the
performance now so we take this standard
serialization 41 microseconds typically
so we're getting what 20 250 23rd is it
back to 2.5 million a two point five
thousand a second on this one we use the
batched I writing out a thousand or
writing out a million of these as binary
we're now down to 44 nanoseconds so
we've got almost a thousand times
increase in performance and that's
typically what we do find out of this
let me just show you some comparisons
these two graphs I will enlarge the one
on the right in a second this is the
post-mortem heap usage of the ordinary
Java object version this is 200 seconds
so 200 seconds down here this is eight
gigabytes of heap this is creating five
million writing them out and then
reading the same five million back into
a second array so we end up with ten
million in memory hence they increase in
size so eight gigabyte heap this one is
a 2 gigabytes heap and this is 40
seconds and I just sort of scaled them
so you can see them roughly side by side
what's happening here is exactly the
same as what's happening here the fact
that the slope looks dif just because
the scale is different it's it's so this
part here and here is the same this is
the reading it back so this is the D
serialization
so the D serialization we're writing
them out civilization sorry creating
serialization d serialization and as you
can see just in this very simple example
five times faster look now at the heap
usage so this is the GC pause time GC
pause time is what kills most
applications particularly if you're
working in and as we say real time or
set a real time or something where a GC
pause will kill your machine one of the
best ways of avoiding this is to have as
little memory as possible and you have
multiple machines if you have little
memory then you've got to distribute it
and you're back with the same issues
again if you have a very large amount of
memory you get less GC pauses but when
they happen there into the sort of
seconds and tens of seconds and it shuts
your machine down and given the way Java
works it will also even if you're single
threaded it will grab every single core
on your machine it will kill your
machine at I'm sure you've seen it
occasionally when it goes haywire it's
quite quite easy to replicate and
sometimes it gets to the stage where it
doesn't actually catch its own signal to
to throw the out of memory exception in
which case your machine will just go
into a horrible die and if you're using
a laptop all the fans will come on and
it'll start getting pretty hot and I'm
sure you've seen that so let's just look
at this again 200 seconds here this is
half a second to half second pauses in
this this is 40 seconds now the very
bottom rung here is 50 milliseconds so
I've got 50 100 150 200 etc half second
here that's the top of this one is 45
milliseconds 45 milliseconds maximum
pause time and one-fifth of the time to
actually process it this is a really
really simple example this is something
that if you've got data that you're
using distributing and you can make very
very small modifications this is the
sort of performance you can see when we
did this to more complex things like XML
we got performance increases by and the
realms of in this case it was five we
got in the realms of 50 times faster 50
times less memory and that was very very
significant
so the it's a serialization and
deserialization which is causing the
problems with this and this is it there
are there are many many frameworks in
fact probably 20 30 different frameworks
because civilization is so slow it
needed to be compatible across so
anybody worked with any of these Avro
cryo thrift protobuf etc at all these
are significantly better than the JVM
because the JVM is just so bad so you
can you can do pretty much anything and
you're going to improve on the JVM you
can write your own serialization
effectively that's what these do but
these also store tag value pairs they
store a token to say this is I'm going
to send you a string I'm going to send
you a date I'm going to send you a
something else if you're sending data
from A to B typically you have a
contract be knows what I is sending so
why bother to send all of the details
about what you're about to send if you
send XML so if you go to a rest based
service or something and get json or or
XML you've typically got a schema that
defines what that is and because of that
it's well defined what's been sent now
if you're sending serialize data or
you're storing data sir in a civilised
formats you don't actually need to store
the contract or the content of the
message with every message because it's
it's not going to vary you need to store
some tags and some tokens to say this
will either be one of these or one of
these and you can put that into a bit or
a bite but you don't need to do the tag
value pairs so when we compare this with
with these if we just do straight binary
writing out it is very significantly
faster than these as well anybody use
jmh before another one of these things
really highly recommend if you want to
walk away with something something
useful to use this is an amazing tool
jmx just search for the java micro
benchmark harness unbelievably useful
does some really nice stuff draws
creates all of these very very quickly
the downside is it uses may
which I hate because I spend so much of
my coding time on planes and of course
maven doesn't work on planes because
they haven't got nexus repositories on
them yet still waiting for that day but
when you put in this micro benchmark
what you do get out of it is for example
you can see this is nano seconds but up
we can see 171 nanoseconds for binary
civilization and with cryo in this
particular example quite quite a bit
slower it is still very significantly
faster than Java than standard Java
benchmarking it across all these
different ones thrift pro 2 baths etc we
can see we got some pretty pretty good
performance here the other interesting
thing is this is serialization and
deserialization is virtually
instantaneous because it's niño this
was an open-source benchmark that we
used so as realistic as this particular
benchmark was but it did mean that we
got pretty much instantaneous read back
on this if we're doing analytics this is
where the CPU comes in if you're
analyzing very large amounts of data and
your data is stored in Java objects
every time you use those java objects
and you've got an array of maybe a
hundred a thousand a million because
you're searching through an array so
he's saying okay of my oh right i want
to get everything which is between these
dates and between this amount of
currency or between these things and i
want to process it so i'm going to do a
lambda here with a MapReduce where I'm
getting the the currency amounts for so
I'm basically doing a MapReduce across
each type of currency creating a
starting with a bigdecimal from zero and
I'm getting the amount and I'm basically
adding this up so I'm basically for each
currency get me the total of each of the
currencies if it's in Java objects it's
very fast if it's in these binary
objects I've now got to do a little bit
of calculation so you might think it's
actually slightly slower because I've
now for example with the dates I've now
got to do this manipulation of
multiplying by 86 million four hundred
thousand for example you think okay it
should be slower what actually happens
because we now have an array of a
million of these or 10,000 or whatever
it may be that array is read into the
cache because it's in the cash I'm not
doing memory fetches so I'm going for my
200 hundred and twenty nanosecond fetch
time from memory I'm going straight into
the cash now in Java I'll get a lot of
it in the cash but when it's stored in
binary I can get even more in the cash
and we actually find that the binary
objects are even faster than Java
objects which is sort of weird because
you're actually doing more work but it's
actually faster so we have the ability
because we've encapsulated it even
although it's stored in binary we can
still do everything that we could do
before we get some very very high
performance and interesting results but
we can do it a lot faster anybody use
Hadoop here maybe yeah don't bother
anyone used spark yeah a little bit more
intelligent way of doing it one of the
problems with spark or Hadoop is that as
soon as you've gone past the
capabilities of the machine you're
working on you've got to distribute you
have two choices you basically run out
of CPU power and memory you you can
either go to disk so you put everything
on the disk and use the existing cpu you
can you can distribute you can go to
more memory which is faster but you hit
serialization and but you get the the
extra CPUs the problem when you do that
is let me just show you here this is
this the speed that it takes in standard
spark when we're on the same machine but
as soon as we hit what was this one
cache sterilized memory or distributed
memory we hit this cliff anytime this
you distribute you hit this massive
cliff because we have to serialize it so
sparks comes quite standard as one of
the optimizations you can use is cry oh
it's a it's the optimized serialization
which means that you as you use spark
it's faster to use the civilization you
can see there's an improvement ok these
are benchmarks so don't necessarily take
them as sort of fact or figure what I
would
ask you to sort of really notice even
though it's just a benchmark and all
benchmarks can lie there is a massive
difference here between these two
because we've now improved the
serialization because we're reading this
stuff directly back in a format that the
CPU can use using binary we can now
distribute and we don't have this cliff
as we we hit the distribution boundary
the network where basically it's just
like a little allege so ready just to
wrap this up if you can rewrite a little
bit of your code keep encapsulation you
can get some incredibly fast performance
out of your data you also don't need to
distribute quite so much because you've
got more data in the memory that you had
in the first place quite significantly
more it's incredibly fast and from not
just from the benchmarks and the test
but from actual customers we install
this with and some some very large
customers using this going from this is
the object and again these particular
numbers are just benchmarks it cycling
through I don't know what this was a
100,000 FP ml messages these are very
complex XML but these are binary eyes
java objects took 22 milliseconds but
the binary version only took 17
milliseconds so we're starting to
already see some of the performance
gains from hitting the cash we have more
work to do and the XML was more
complicated because you have to work out
where the data is and because you've got
recurring elements in the XML it's much
more complicated because you've got
things that vary in size and you you're
stepping over data to find out whether
the data is that you want despite all of
that extra work that had to be done we
still got performance increases over
standard objects and the data was a
fraction of the size transport friendly
the other advantage when you run out of
memory you just write it to disk you've
got something like SSD because you can
write it to disk and read it back into
memory with no pausing that's important
there's no D civilization to be done you
read it directly from this directly into
memory you use it instantly so to write
something out to do
is only a sort of fifty percent slower
than actually using it in memory so a
typical machine with say 16 gigabytes of
ram and a terabyte plus of disk you can
actually use that terabyte almost as if
it was ramp it's slower naturally but
almost at the same similar performance
which is just absolutely impossible
using standard java objects so what I
hopefully this is the last slide what
I'd like you to walk away with here is a
little bit of there's a lot of things
you can do to improve the code that
you've got to improve performance to
make small changes to make small tweaks
but this is something that I've taken
stuff that we used to do 20 years ago
was just standard practice this is
something that you can go back and I've
done this with many initial clients
where we go in we look at the data
they've got and within half a day you
can start to see some incredible
performance increases by taking the
objects that you've got making a couple
of small modifications from half a day
you can go to a week you can say okay
we've seen some performance improvements
we'd go to a client for example that had
massive amounts of data we'd take it
away from the evening rewrite the class
put a few little improvements in there
they would see a five times performance
gain five times less memory and all of a
sudden in the space of one day that gave
them something they were about to do a
entire six man month plus project on and
we've done it in one day so if there's
anything you can walk away with here it
is this is a very very small change you
can make to your code that makes a
massive difference both to the
performance and the the the way that
your CPU uses the memory I've hopefully
got a little bit of time for some
questions other than that I'd like to
thank you very much any any questions at
all yep
c++ program here well I'm from my
Microsoft background so don't be
nevermind so it sounded to me unless I
misunderstood that you've kind of ruled
out compression as a way to improve
performance but I don't know if you're
aware of Microsoft's columnstore indexes
that they they use i'm just wondering
whether that would be a good way to get
a really really good compression factor
from your data and also another their
technologies in-memory OLTP where they
use hash functions so hash buckets I
wondered whether combining those kind of
two methodologies would maybe make that
a viable you know addition to yeah
absolutely I think the hardest thing we
found is we were sort of we thought okay
this is the this is going to change the
world in the Java world and then you go
to clients and they were use so many
than we're using existing optimizations
to get around this in similar sorts of
ways a lot of the in-memory data grids
already had pre optimization for certain
things and you're effectively not
competing with that because you do
complement it but there's an extra
little bit of work we had to do to to
make that complement complementing work
with the existing optimization so in
answer to your question yes it can be
used because there's optimization and
you're adding more optimization it's
sometimes a little harder to get the two
to work nicely together we did manage
that in every case we did manage it if
you're going for something completely
unsupervised a few going for something
already pre optimized you have a little
bit of extra work to do but it did make
a very very significant performance
improvement and one of the I mean I i
was using microsoft for back in the 90s
before before Max and Linux became quite
so available so I've got a long
background in the Microsoft world as
well and a lot of the clients use
Microsoft as well Microsoft is not the
issue is just the fact you have to pay
for the licensing and if you can
basically reduce the amount of machines
you're using you reduce the licensing
another question I saw a twitchy hand I
thought when someone just coughing any
more questions okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>