<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • The Inner Workings of the Chakra Javascript Engine • Steve Lucco | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • The Inner Workings of the Chakra Javascript Engine • Steve Lucco - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • The Inner Workings of the Chakra Javascript Engine • Steve Lucco</b></h2><h5 class="post__date">2013-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ouSoyWj_lSI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's great to be here this is my
first time at this conference and I've
been having a great time too it's a
really terrific conference and so I'm
really also delighted that I have a
chance to to talk about the chakra
engine mostly I'm just walking on my
treadmill coating on it and and don't
don't get out much but this is a great
pleasure so you'll notice that from the
talk abstract in your little booklet
I've changed the metaphor from inner
workings to under the hood that's a
shout out to our home team the the VA
team and all the great work they've done
using the engine metaphor okay so this
is the slide that we showed the Internet
Explorer team two and a half years ago
or has been no it's been longer than
that more than three years ago to
express the principles of what we were
going to do to try to to reinvigorate
the Internet Explorer JavaScript engine
and I bring it up here now mostly to
just remind everybody in the audience
that it's not all about perf there are
other things and and and actually before
you can really dig in and work on on
perf like we are now you have to make
sure that the thing is secure and and
compliant and then by balanced
performance I'm just referring to that
there's a lot of dimensions to to the
performance of a language runtime system
you know in addition to throughput which
we often talk about there's also things
like how fast is the application load
how much battery power are you consuming
and so on and so that's why put balanced
performance there and then transparency
that's up there because it really
matters that you get predictable
performance so when your authoring
something as a JavaScript developer your
authoring a big app you want to know
okay 15 minutes into the thing is the
form is going to degrade can I predict
that if I use a certain style like a lot
of closures or you know a lot of long
prototype chains what's that's going to
do to the performance of the thing and
so one of the key goals in addition to
just getting great performance was to
make the performance predictable for app
developers so that's the main design
principles that we had and i'll start
with security so when you go to do
Justin code generation you introduce an
additional possible attack vector and so
I'll talk a little bit about Jude
security but before I talk about that
I'll talk about some of the other things
so a key thing that we did for the shock
ranjhan is is do a lot of security work
even in the native code of the chakra
engine and one of the things we did is
that we hired an outside vendor of
people who like to go to the black hat
conference and had them attack us for a
period of time and got quarterly and
reports and so we had a lot of fun
actually hardening this thing and it was
it was gratifying that after a while
they had a really hard time finding
attack vectors nothing is perfect but we
keep working on it so one of the big
things we worked on because it was new
in what we were doing is is this jit
spray attack some of you may not be
familiar with that term but it just
refers to a clever adversary trying to
figure out some way that they can send
you JavaScript source code a lot of it
and then you will emit sequences of
native instructions that they can then
exploit from another exploit that they
already have going in the native part of
your code so if for example they can
jump to an hour arbitrary address in
memory then they can
or even some region of memory they can
do things like put in a thing called a
no-op slide which is then you follow a
bunch of no ops and you get to the
attack code so this type of attack is is
out there they're been real exploits
that have caused millions of dollars of
damage from this so so we did our best
with it and so the first and foremost
thing we did was we run our our engine
with data execute protection on as far
as I know where the only script engine
does that the reason we chose to do that
is that for a lot of other programs
we've seen a huge decrease in the amount
of exploits revealed over time on those
programs if we have data execute
protection on so there was a lot of
solid evidence that it would work it
does make certain performance things a
little challenging because you have to
do things like well you can't write the
code because because once we eat once we
output the code we take a checksum a
cryptographic checksum we then turn
right permission off and then turn
execute permission on and so but so the
pages that are executed cannot be
written and are checked to make sure
that they're exactly the code that the
Jake compiler emitted and because of
that it's a little trickier to do things
like polymorphic inline caching some of
you may not be familiar with that term
but that's geeky term from dynamic
language implementation where you take
objects that come through a code site
that might be of different hidden class
types and you have to be able to cash
how to get at a particular property
that's on the object and I know for
example the v8 engine will just write in
the the value of the of the current type
right in the code but we can't do that
because we've got data execute
protection on so makes it a little
harder but it's worth it okay so code
base alignment randomization that
is to make it so that it's not very easy
to know some offset that you can jump to
that will work for you random no up
insertion is for the same purpose
constant blinding that's because people
will write whole programs in a little
constants especially on the x86 which
has a profusion of instructions where
you can stick big juicy constants right
in there and then they'll jump off
alignment to these constants and it'll
be an actual sequence of executable
instructions so constant blinding is
where you do x or tricks and then also
there's a large but present heuristic to
avoid a big blitz of code allocation and
finally there's there's page
randomization which means we don't just
open a giant buffer and then generate
code into sequences of pages this is
just a little bit of comparison that a
third party did and I think the really
the only thing to take away from the
slide is that it's a good thing that
Firefox doesn't have a high probability
of being attacked because at present
they're not doing a whole lot chrome has
just about caught up to where I E is in
doing the JIT hardening I think this
this third party took this for chrome 14
and I know chrome has added added some
more of these rows since then okay so on
the compliance so this is a screen from
a recent run of the test 262 which is a
suite of tests conformance test that a
group of people from the industry
including Microsoft are collaborating on
that it's like over 10,000 tests and
it's just to make sure that everybody is
running the same language and you can
see here I 10 right now has the highest
pass rate and we're missing one test and
I was really sad to come here with the
one that's missing but it turns out that
there's this new little Unicode test and
there's a bug actually in the Windows
operating system so in fact all the
browsers right now have this test
failing
but we had one hundred percent I was
ready to show you one hundred percent
but it's not to be oh and actually
before I get off this at this this slaw
it reminds me of a particular day in the
history of this project it was a great
day was that it was I think january 6
2010 which is the day that we turned the
chakra runtime engine on by default in
the internet explorer 9 browser and
that's not that easy to do because they
run a suite of 4000 web pages that just
goes across every possible compatibility
range so it's like corporate things that
use ancient assumptions about what the
browser does and up to very modern
things like acid three tests so passing
all that was a great day and the
Internet Explorer team actually sent us
krispy kreme donuts which was great and
they gave us love for about 12 hours and
then we started getting email messages
like hey did you know that for this web
page you regress twerking set by half a
percent that's not acceptable so then we
had a really good two months of fixing
all that before we're ready to go so so
let's start with a little stuff on how
we do page loads so so basically source
code comes in and it's parsed I can get
that far we make an ast as abstract
syntax tree for those of you who don't
write compilers and then that goes into
a byte code generator why do we generate
bytecode instead of just going right to
native code well because we want to
interpret why don't we r on interpret
because as far as we can tell by running
experiments that's the fastest way to
get code actually running and to get
interactive response of a page so get
the bytecode out put it in the
interpreter and that's it that's all
that you have to do for to say load the
New York Times and the important thing
is that the the parsing is deferred so
there's a prominent cable news site that
I'm not allowed to say who it is but
I was amazed to find and this is very
common that they download 70,000
functions for their front page and they
they use nineteen percent of them during
the page load so we certainly we soon
learned that if we wanted to cut page
load time we had to parse as little as
possible and so we you know we just
parse curly braces and we fill in later
and there's all kinds of tricks to that
and and there's ongoing challenges so
for example the ACMA script six
committee there they're working on
features like lead and const and one of
the specifications about lettin const
which by the way are new ways of
introducing variables into local scopes
in Echo script six is that syntax errors
are supposed to be reported
synchronously with the load of those
function bodies and the problem with
that is that it's very hard to do
deferred parsing and then report errors
about local variable declarations so
we're working with them on that so it's
a it's always a struggle to keep getting
performance through the design of these
things okay so but we try page load is
important because it's it's one of the
most visceral parts of the user
experience you know a lot of times
especially with the old school web
before we had web applications you know
you're just navigating sequentially
through a series of page load hits so
it's very important to get right and you
know there's solid evidence that
Internet Explorer is is doing pretty
well with it both for i9 and ni 10 most
of the independent reviews that I've
seen and a lot of the recent reviews who
say that Internet Explorer is loading
pages the fastest certainly does pretty
well so what do we do we interpret so we
start execution almost immediately
that's very helpful there's deferred
parsing there's startup profile caching
so a lot of you guys that will have
heard the term hot
which was you know done by a lot of the
v8 crew and other people for sun
microsystems for java execution so one
of the things that you can do for very
little memory on stored is is just save
where the hotspots are and so what we do
is we don't write out any code or any
complex information we just like say
what what the hot spots are and we say
what heuristics work the best and so
like for example for that big cable news
site with the 70,000 functions in one
and a half K of output we can capture
everything that's needed to know exactly
what we need to generate code for and
also we generate that code in the
background so so instead of generating
code on the same thread that we're
trying to give user responsiveness what
we do is we generated on another core
and we interpret right away and then we
use the interpreter to gather
information that's going to help us
generate accurate code for the thing so
and also the garbage collector is
concurrent I'll have a few more slides
on what the garbage collector does in a
few minutes but concurrent garbage
collection is really helpful so one
wonderful fact about page loads is that
page those are do not follow the
generational hypothesis so for those of
you who don't like to read about garbage
collectors the general internal
hypothesis is that most objects die
young but in in a in a pate and that can
be exploited by doing a little copying
thing where you have a nursery of these
young objects but in a page load what
happens is all the objects are created
and almost no objects are destroyed
because you're setting up for people to
click on things and try everything and
so of course you you just create a lot
of live state and then leave it there
and so it's important um
that your garbage collector doesn't take
up time during the page load and mostly
how we accomplish that is by making the
garbage collector concurrent okay so now
on the throughput and and two
interactive response which full largely
in this talk mean GC pause times and
making them small okay so more
powerpoint animations you saw this part
now we have the heartbeat interpreter
and that heartbeat is generating profile
information so whether we store that
profile information to disk and get it
back or not what we do is we use the
profiler is kind of a data gathering
machine so as it's running the code and
giving you interactive response it's
also measuring where the hotspots are
and measuring what loops might benefit
from further optimization and what
object accesses are monomorphic without
object x's are polymorphic what function
should be inlined and what code don't
you totally don't care about and you
will never get out of deferred parsing
so the interpreters gather all that
information and then there's a set of
heuristics that determines whether you
have enough good information to try to
type specialized something or generate
good code for something and then you
take the runtime profiler and you feed
it into core number two where there is a
just-in-time compiler and the
just-in-time compiler produces machine
code and right now it's the same targets
I think as v8 you know arm I'm and X x64
and x86 so we've got machine code and it
too has a heartbeat and then finally the
garbage collector is mostly running in
core three now this picture is a little
misleading because it's not like three
cores or blazing all the time the actual
CPU utilization is about 1.2 cores
because most of the time either the code
generator isn't running or the garbage
collector isn't running but if they have
to to deliver you good interactive
response they'll run at the same time
okay so a little bit of more detail
about the garbage collector it's
conservative this is always something
that people find controversial or
interesting but it works out very well
for us and the the reason we decided to
make it conservative is for good
interoperation with native code for
example a native host like Internet
Explorer with its dom implementation or
a native host like node with its
basically native code that does things
like talk two sockets so we did a number
of things in the in the chakra engine to
to interoperate well one is this
allowing pointers to objects to beyond
the native stack and still finding them
and that relieves the native code of the
pressure of having to work with whatever
kind of add reference release or smart
pointer or I'm in a scope now now I'm
out of this go various disciplines that
you would otherwise have to do in order
to interact with your script engine we
don't have to do because we just look at
the stack there's also the capability of
doing what's called a guest heap or
guest arena where the native code can
just allocate some space using the
garbage collector and then any object
javascript object pointers that it
stores there are automatically scanned
by the garbage collector and that makes
interrupt very simple you just have to
follow a simple discipline so it turns
out the conservative choice we've
measured it a lot because we get a lot
of questions is that okay aren't you
going to get spurious references that
well no there's an average of 0 point 0
to spurious references / collection so
that's like to spurious references over
100 collections and one of the reasons
it's so low in practice is that we have
tagged integers in our JavaScript
implementation and so it's impossible in
the JavaScript part of the stack which
is most of the stack to mistake an
integer for a pointer because the
integers
tagged so it so it's also generational
there's no separate nursery space but
you you can do partial collections and
you can ten your some of some of the
objects because sometimes the
generational hypothesis is it is
followed like in things like gmail or
outlook web access so it's mark and
sweep but pretty much all garbage
collectors these days are small objects
are stuck in size buckets to reduce
fragmentation there's the large object
heap has another defragmentation
strategy but right now we aren't copying
or evacuating anything which is also
surprises people but we read a number of
studies that were done in the late 1990s
and early 2000s that showed that the
impact of copying garbage collection is
actually much greater on your memory
system than just keeping everything in
place you actually go through more pages
go through more cache lines and so on if
you're copying all the time then if then
if you're not now it turns out that we
could copy over ninety percent of the
objects in a typical heap because only
the object headers can't be copied
things like you know the the front of an
object before it contains the slot that
pointed all the properties of the object
or the string header but the payload of
a string or the payload of an array or
the slot array that holds all the
properties of the objects all of those
things could be copied because there's
only one pointer to those things and so
if we did need to do evacuation and
copying we could but we haven't found
any smoking gun in any of the apps that
we try to run that makes us do it and
that's the way we run a lot of our
performances that we no one is allowed
to speculatively add anything to the
chakra engine there has to be an app
that is agreed to be important enough
and and what we call a smoking gun like
a measurement that focuses in and says
okay to make this app run well enough we
really do need to take this optimization
so this little diagram here is not to
scale the blue parts so the bottom part
is supposed to be the concurrent garbage
collection thread and then the top part
is supposed to be the UI thread and the
little blue on the top where we were
scanning roots and then rescanning those
are actually smaller but I couldn't fit
the text in there and so basically what
happens with concurrent GC is you get a
little pause and you scan the registers
and the stack and then you put all that
on the mark stack and then you go back
and run user code and give them good
interactive response meanwhile the
background thread is marking through the
object graph then once you have marked
through the object graph and you're
given an opportunity you do a thing
called rescan which is that any objects
that were written during this middle
phase of the program right here need to
be checked to see if they have any have
any new references and then you also
have to rescan the stack normally this
results in a very modest amount of new
objects being stuck on the mark stack
and so we go ahead and mark through
sequentially and finish so we you can
also not um not finish what you can do
is you can say oh you know what I got
too many objects to mark through
sequentially so I'm going to go back and
current mark the object graph again and
then try again the finish okay so what
that leads to is very small predictable
pause times and so we measured some
here's some examples of what we measured
out look Bing Maps being imaged search
and then a couple sharepoint so we've
measured this on a lot of things both
Microsoft things which this mostly is or
other things like gmail page loads and
and and it's pretty much this is what it
looks like the one exception is html5
games which is kind of a frontier for us
we're trying to you know height get high
frame rate games to work really smoothly
and we
we still have some challenges with that
about working on it hard but for things
like this big web applications and web
pages we're doing quite well so this is
this is the max time ever spent in
garbage collection across a big session
about a 20-minute session of using these
programs with a script that the person
goes through to test it and so you see
for for outlook and SharePoint the
background GC used about 15 milliseconds
for a collection but but the foreground
GC at most used seven milliseconds or in
the case of SharePoint up to 10
milliseconds so these are short pauses
that aren't going to interfere with your
interactive response so if you're busy
typing in something like in office 365
PowerPoint or whatever you're doing
something under 30 milliseconds is
probably not going to interfere with the
interactive response and then another
thing to note is that the average time
is a lot is somewhat lower like under 4
milliseconds for all but still it's
pretty consistent so so what you're
getting is you're getting consistent
predictable pause times which is what is
great for delivering interactive
response and these in these big web
applications and so that's mostly due to
the concurrent GC and how it's
architected ok so now the obligatory
throughputs slide so here we go and I
have to tell you that being in our host
that v8 is a fierce competitor so when
when when we were working on I 9 we
passed chrome seven times and seven
times they passed us back again before
we could ship a preview and so for a
time it was you know quite difficult and
we worked really hard on it and of
course different benchmarks are
different but this is the benchmark that
people paid the most attention to and
had the most diversity of little
snippets so we focused on it a lot and
we managed to hold the lead between the
shipping of i 9 and now the the the
shipping of i-10 but you know I'm sure
v8 will surge past us at some point or
it's it really doesn't matter that much
because honestly you have to take these
kinds of benchmarks with a big grain of
salt because um you know like the press
loves a horse race and there are a
number and people can focus on the
number and be like oh yeah this number
but I think what we try to do instead is
think about classes of applications so
what do you want to do on the web
application platform that you can't do
because i think we all share I know from
talking to Lars that that did his team
feels this way and I think we all share
that we just want to make the web
platform better we want to make it more
applicable to more classes of apps and
so what we try to do is we try not to
get drawn into the benchmarks too much
and to focus more on okay we want to do
cut the rope and can we make sure that
it doesn't glitch at all or you know or
in the looking back in the past it was
more things like well you want to run a
word processor or PowerPoint or a mail
client and you want to make that smooth
interactive response and can we do that
and and sort of forcing there to be a
smoking gun from one of those things
that applies to this but so we make
steady progress on benchmarks we use
benchmarks we think you know they have a
place but we're hopeful that we can all
work together to craft a benchmark as an
industry that would be more like the
spec mark and and less like the the
current benchmark so back when we were
all doing you know C and C++ performance
there was a thing called the bite mark
and other little sort of code snippet
based benchmarks which is kind of like
what sunspider is and some of the other
vendor-supplied benchmarks that there
are what we'd like to see is that evolve
the way it evolved for C performance to
this thing called the spec mark which is
basically scenarios from a bunch of real
applications that people really use and
using that to measure the performance of
the language runtime system we think
that that would be a really good
direction okay so that said what kind of
games do we play to run code really fast
well so these slides are highly abstract
so I'll try to interpret what they're
trying to convey so basically what this
is trying to convey is that the orange
is bad and that the the green and red
are better the orange was bad because in
I 9 we just generated polymorphic code
so when we weren't interpreting we would
just generate code that was pauly that
that that no matter what the object type
was that came through a particular code
site everything was cool you know so if
you're doing a loop over something and
you're doing a plus plus well if it's an
integer grade if it's a floating point
number grade if it's an object great if
it's a string and your incrementing it
you know great we'll try to do that you
know so obviously some things are
supposed to produce runtime errors
recording acma script but most things
don't and so but generic polymorphic
code only runs so fast especially on
applications that use a lot of object
access so we want to do better and how
you do better is you play this game
where it's kind of a game of chicken so
you run the thing in the interpreter for
a while and figure out what the types
are usually going through the particular
code sites and then you say okay now I'm
going to bet that that code is that
object is always going to be that type
and i'm just going to generate code that
just believes that and type specialized
it as much as i possibly can for that
type but oh wait a minute the echo
script committee says that i can't just
generate that tight beautiful code
because it might not be that type that's
coming through there and so you always
need to do extra checks and this is
where the rub comes in like even when
you're generating the much better
code that you'll see in a minute it's
still not as good as native code because
you still have to do those checks of
course you can try to hoist the checks
out of loops and that is helpful but you
still have to do them okay so that's and
basically how that's done as we check
and if it isn't the type we think it is
or the set of types we think it is we
bail out to the interpreter we collect
information more we so actually it's
polymorphic and then we try again we say
okay it's either this type or this type
and generate code now you might be
thinking this toot audience member might
be thinking oh but didn't he say he
wanted predictable performance and
people could think about it and get the
right performance well i guess i should
say predictable performance up to what
the semantics of the language are and
we're always trying to figure out how to
make it more predictable but basically
if you don't make these leaps that you
can bail out from there's only a limit
to how good of code you can generate
okay so for example here is type
specialized integer math so basically
this is a thing where we noticed that
some of the variables like these
variable B and the variable C and the
variable M are integers every darn time
through the loop and the interpreter
runs the loop 10,000 times and it's like
it was always integers so then we
generate this code and we think okay
it's going to always be integers and
this is more or less the native code
except for this little sad line in red
where hey if it wasn't an integer we go
to the bailout we go back into the
interpreter and we try to get smarter
about that loop similarly here's
floating-point math and i-10 and i'll
show you comparison 99 in a minute this
is from the SunSpider benchmark and if
you sort of gloss your eyes over this
assembly code you'll see that there's
mostly sse2 instructions in there and
that's mostly good because that means
you're mostly doing what the code
intended you to do and not much else and
then there's the evil two instructions
that you are forced to do because you
might be wrong and something other than
a floating point number might come
through there okay so fast property
access to 99 this is actually sort of
just a retrospective slide but I
included it because it's so pretty I'm
sure you guys have seen a million times
how people do pipe polymorphic inline
caching and but what you're going to see
here actually is monomorphic inline
caching because that's all I e9 had okay
and so basically what this is is that
the little bubble thing on your right
there is the hidden class that's what
people typically call it when you sort
of make up types for objects as you go
along and the runtime and then you can
see that b1 and b2 are two objects with
two different types and well in this
monomorphic case they're the same type
and that's good because x and y came in
in the same order but then in this case
here unfortunately they're not the same
even though if i were to access x or y
everything would be cool but the sea is
kind of messing me up so and an i-9 we
didn't have any provision for that so
code was slow so what did we do in i-10
we did object type specialization we
made polymorphic property caches we did
field hoisting which means if you access
a field of an object more than once in a
loop we will try to stick that access
outside the loop if possible we did copy
propagation we did streamlined object
layout which for example means that
instead of actually having a slot array
and all that you just stick the the
slots right in the object header for
some fixed known of known amount of
slots and function inlining which is a
big deal so as a result this is the code
on the left this is 99 code for that
little JavaScript snippet at the top
total plus equals 0 dot X plus 0 dot y
plus Z and then this is the i-10 code
you see the odex is you know still
pretty nasty and there's still the test
of the type of the object and the
bailing out to the interpreter if we
need to but the code for accessing o dot
y no se lot cleaner so we made a ton of
progress there in in a loop it's even
more dramatic because basically you only
do the loop header one time you do the
loop body say a hundred times and you
can actually hoist some the checks out
of the loop as well and then you get
very tight code so this is how that ends
up looking is that the the loop here in
blue is very nice and small only one
bailout check and then also this is
showing in lining so we took calculate
and we in lined it there this is just to
say that that we worked a lot on
throughput as well as load time and
power consumption and all those other
things and we've made a great deal of
progress with it we are at the point
where we're running large applications
that that you know use a lot of objects
with a lot of properties on them very
well well enough that you know big
applications 50,000 line applications
are delivering very good compute intends
to perform is in very good interactive
response but now we're we're trying to
tackle additional application classes
for example like html5 games and they
bring in new challenges and so we're
always trying to broaden the class of
applications that the thing can actually
do okay so speaking of broadening
there's another thing we've been working
on and there's a few things that the the
script engine does that are related to
that and that's that there's a thing
called Windows 8 it's coming out October
26 is when everyone will be able to buy
windows 8 machines everywhere and um but
it's actually done and a lot of
corporate customers already are using it
so
and and one of the things that made me
really happy about Windows 8 is that we
were able to apply the web development
model to building desktop apps so a lot
of the key applications like the music
player and the mail client that just
come with windows 8 were written using
exactly the same development paradigm
that people use to build say gmail news
you know HTML CSS JavaScript and so we
think this is great because then people
can leverage their skills to do yet
another thing and so when there are
windows devices you know competitors the
ipad for example you you'll be able to
build a reach client and then you'll be
able to use that same client to do the
device specific stuff hopefully
leveraging most your your your code so
there are a few things we did just
specifically to make Windows Store
applications great one is that we did
bike code cashing in addition to the
profile caching and that's because we
wanted to make sure that with the
Windows Store applications most of the
JavaScript code they're ever going to
run is known at the time they're
installed up on the store and so it's
really nice just to be able to create
the bike codes for that thing the first
time it's run on the box and then store
the bike codes away so that so that you
can avoid because by cogeneration is
actually a huge component of application
load time and then another thing is we
added a way for hosts to tell the chakra
garbage collector when they are idle or
when they're going to suspend the
application to disk so that we can run
GC at more opportune moments so that
when the person is sitting and thinking
what they're going to type is when we
run GC and not when they're actually
typing and then probably the most
extensive work we did was fast
marshalling the native code this was
made easier by the fact that chakra in
the first place made some decisions for
great interop with native code which is
that we use native calling conventions
so the same exact calling convention
that if you wrote a little snippet of C
code would be generated is what chakra
is generating for its JavaScript stack
also we use native exception handling so
all of the exception handling machinery
that's put on the stack is exactly the
same this also helps by the way about
with debugging infrastructure it's
easier to debug the stack because
there's already existing tools that know
about these stacks and know about the
exceptions and know how to deal with
them so so that was a big help and then
probably the biggest one thing we did
was the generation and caching of method
entry points so one of the things that
Windows 8 comes with is like a big
surface area so you can work with like
the touch screen and the GPS and you
know all the other device features no
shaking it whatever that's called the
diorama door and so there's a P is for
that and we wanted an easy way for
JavaScript programmers to just program
an intuitive normal JavaScript and see
events that were coming from this in a
JavaScript ways and call the api's in
just an in just a natural way so we
wrote this marshalling layer that makes
all of the hosts api's and it doesn't
just have to be the windows the new
windows api any host of chakra can
create an API surface area like this and
we'll custom marshal to it basically by
we read metadata that that says you know
what are the interface is what are the
types of the the method calls and then
we generate marshalling stubs for them
on the fly so so that's kind of the work
we did for sort of the more desktop
application of the thing and then
there's always more work to do so array
operations and like you were asking
about the polymorphism we're going to
get a lot better in the future now that
we kind of get what we have to do
there's always you know the ESX standard
is in progress and there's things that
are already pretty much agreed on
that we're already working on to comply
with that and to support it we are still
working on things from the last standard
ratification like access or performance
for ES five and then like I said we're
trying to find new application classes
that did customers are coming to us and
saying no we want to run this on the web
we want to run cut the rope on the web
we don't want to have an iOS client and
everything we just want to have one
thing so we're trying to make that work
and then Newport work always comes up
for example we have a conservative GC
but it turns out that very very often we
know precise information about what's
the pointer and what's not a pointer in
a particular object and for example all
the things that are allocated by the
native runtime system those things we
know exactly where the pointers are and
so taking advantage of that precision is
one thing that will help html5 games and
log running applications and then also
doing an even more aggressive amount of
iterating between sequential and
concurrent phases of garbage collection
so that doing a heuristic where you say
you know what if I've paused for more
than 10 milliseconds i am just bailing
and going right into the concurrent
marking we don't do that very
aggressively right now I think that's a
thing we'll look seriously at in the
future so yeah I mentioned games and
then the other big app category that
we're trying to expand to right now is
long running applications you know so
pages you do a little bit and then you
navigate but you might stay on facebook
for you know an hour and so the GC
requirements of that are pretty
different and we're working on that okay
so finally big picture last slide what
are we really trying to do I think all
of us who you know many of the people
who do the most work on this kind of
thing are here at this conference and I
think all of us
really you're just trying to make the
web platform better you know trying to
find a way that more and more
interesting killer apps can just be on
the web and we don't have to have like
little square bracket programming for
iOS and all that other stuff but instead
we can just develop the darn thing was
and it runs beautifully everywhere
obviously that that's a dream that will
never quite be realized but to the
extent we can do it i mean the the
person who is you know in sri lanka with
you know a web browser like a on a phone
it would be great if if you know they
could do everything they could learn the
program they could edit documents may be
on a you know a tablet or something
whatever they have you know the pc at
the library the web is what gives that
promise and so what we're trying to do
is you know fulfill that promise by by
making great infrastructure for the web
and I kind of think there's there's
three parts to that one of which I don't
have anything to do with and that's the
part that's in in the middle and blue
they're like AP AP eyes device
capabilities and secure component model
I think those are all things that we
need to do better on to make the web a
truly great application platform things
that that I and other people in in
Microsoft work on are the great j/s
engine performance using all the
resources like multiple cores and GPU
continuing to optimize so that's what
we've just been talking about in this
talk and then finally I think the other
piece of the picture that has to be
there is that we have to build tools
that enable construction of large scale
JavaScript applications because it's
hard it's hard to take a language that
you know puts everything in the global
namespace by default and so on and
actually build large-scale JavaScript
applications and so I think to really
support an app ecosystem where the next
killer app is going to be a web
application we need to do a better job
with tooling and I am really excited
about that one today because in about
an hour in 10 minutes I think Microsoft
is going to make a pretty big
announcement so 6pm Denmark time in the
tooling space and and I feel great about
it because I feel like it's it's it's
it's something that I'm really going to
be able to join with the my colleagues
who built this and offer it to the open
source community into to everyone trying
to make the web platform better so I'm
really looking forward to all your
feedback not only on this talk but but
on that announcement that's coming in
about an hour so that's that's it thanks
any questions so the question was with
long running JavaScript applications do
you see sort of creeping keep remnants
over time as the application runs longer
and longer and those creeping heap
remnants that you know they can be due
to fragmentation you know which means
where you are allocating little objects
out of bigger pieces and then you get
little holes where you can't put
anything and it can also be due to
people just holding onto references like
one of the speakers earlier today
mention so we sort of expected to see
that we haven't seen it enough to do
anything about it like opportunistic
evacuation for example is something we
could do which basically means that when
you know you have all the pointers to an
object you go ahead and move it into a
new page to try to line things up and
reduce fragmentation but the
fragmentation strategy were using seems
to be working well we actually we did
comparative studies of similar size
applications running for similar lengths
written all in C and written in the web
development paradigm and actually
there's less fragmentation right now in
the ones we measured so we're looking
for a smoking gun there haven't seen it
we have a set of work items we can take
if we do there
is there's great interest in running
chakra on the server side in the azure
community unfortunately right now chakra
is only available on the Windows
platform and so it's not really
something that could compete with with
v8 to be a note for that reason because
I'm sure more than fifty percent of the
node applications are running on
non-windows platforms but in in our
little world yes we've got multiple
customers that will be shipping servers
this year that use chakra on the server
side and we did a lot of work both
initially with chakra and since then to
make it scalable and hardened for the
server environment so things like memory
leaks things like the Avs and what to do
with them you know we can run about
12,000 script context simultaneously in
a process you know with isolation so
that's a pretty good number
competitively and it's proven to be
quite good for for the people who are
using chakra in this context and I think
a lot of the work we did to do good
native interop like native calling
conventions conservative GC native
exceptions really benefit people on the
server side because it just simplifies
everything but the stack looks a whole
lot more like the native stack and so
when you go through gymnastics like you
were describing in your talk it's
actually a little bit simpler because we
it turns out we already have code to
traverse the native stack and pull out
all kinds of information like that so so
yeah we care a lot about that in our own
little world of Windows customers and
and we've been pretty pleased with the
initial results of it okay so the first
the first question actually I'll take
the second question first so the second
question was great work when are we
going to open source it and
I think unfortunately you have to ask
more people than me that question I
think if you ask me and i just spoke my
heart you might like the answer but I
think that Microsoft as a as a whole is
gradually waking up to the fact that
there's a software Commons out there and
that the software development community
has committed to the idea that we're
going to share a lot and we're going to
compete and build our own stacks but
we're also going to share a lot of
things we build our stacks on and I
think that's super healthy and super
interesting to be interacting in that
way not just writing papers about things
like we used to do but actually sharing
code working together on code it's
terrific and Microsoft has to understand
that that's not going away people are
permanently going to do it and that
Microsoft has the biggest producer of
software in the world ought to
contribute to that so that's clear and
that meme is expanding at Microsoft so I
think that's that's all that I can say
about that the the the first question
was do we when we store profile
information what about input changes how
stable is the profile information under
input changes because if we're going to
say hey that this worked then what if
the input says if if new input then make
it afloat otherwise making an int right
so what we do is that we have actually
primary and secondary heuristics so
there's actually heuristics that say
what is the likelihood that this thing
will be stable and therefore it won't
change under input and what is
likelihood it won't and so far those
heuristics have been good enough that we
can run things faster by guessing and
bailing out then by bit then by not
following the advice of the of the
profile okay thank you very much really
enjoyed it and have it won't great
conference</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>