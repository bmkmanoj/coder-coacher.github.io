<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • Polyglot Data • Greg Young | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • Polyglot Data • Greg Young - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • Polyglot Data • Greg Young</b></h2><h5 class="post__date">2014-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hv2dKtPq0ME" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we're gonna go briefly through event
sourcing today and then we're gonna
start looking at a bunch of different
problems and inside of these problems
we're gonna start seeing massive amounts
of accidental complexity come out
because we're using the wrong models and
then we're gonna come back through we're
gonna focus on how do we use the right
models now I'm sure you all know I don't
actually live in America anymore but I
am American and Americans are known
worldwide for their their subtleness and
reserve please do not feel bad about
asking me a question when I'm talking I
will not bother me that much
now before we get into polyglot data I
want to go through a very brief overview
of what event sourcing is just for the
people that may not know what event
sourcing is this talk is not about event
sourcing but we all need to at least
have some level of shared understanding
now it's interesting for me is and going
around and looking at business problems
almost no business problems that are in
mature industries have the concept of
currentstate inside of them think about
finance think about accounting think
about insurance they don't have the
concept of current state current state
as a transient concept what they do is
they store facts that occur at points in
time how many of you have a bank account
do you think your balance is a column in
a table what would happen if it were
so now you disagree with your bank about
how much money is in your account and
they said the almighty table says this
amount
therefore it's correct and you go I need
a new bank know your balance is an
equation your balance is a summation of
all the transactions that have occurred
inside of your account I may store it in
a table but that's a cash nothing more
it's not a Thorat ativ the same thing
holds true with any business problem
that we want to look at most people
today they work with structural models
and this has been my quintessential
example of a structural model where we
have a purchase order with n line items
and shipping information and this could
be a document this could be stored off
in a sequel database this could be an
XML file how many of you have worked
with a structural model like this before
this is not the only way of representing
this information I can also represent
this information as a series of events
so we have cart created three items
added and shipping information at it now
those three items added those are
actually three events but if I'd made
all three events on this slide the boxes
got really little and you couldn't read
them anymore
I can at any point in time take these
five events and I can replay them and I
can give you this piece of structure I
can always replay this set of events and
give you back the structure of what it
actually represents but I don't store
the structure I store the series of
events and there's a lot of benefits to
storing facts as opposed to storing your
perception effects this is a perception
of the given facts
the main difference you start finding is
that over time your perception changes
now the real reason why almost all the
mature industries are using this as
opposed to this is because this does not
lose information this I don't care what
model you pick you are losing
information
as a CTO I had only one rule for my firm
we would not lose any data the reason we
would not lose any data is because I had
no freaking clue how to value it how do
you do cost-benefit analysis if you
don't know how to value some of the
future we were storing over 100
gigabytes a day of data like this and
this was years ago
this model is guaranteed to lose
information how many of you have an
update or delete statement in your
system how many of you had a c-level
meeting to talk about the value of that
data in the future so you just decided
that this data is worthless and we will
just get rid of it and a lot of problems
come from this now this slide it took me
forever to find and the reason why it's
because accountants don't do that the
same thing happens well at maybe at
Enron but accountants don't race in the
middle their ledger when we talk about
these events source systems event source
systems are append-only there is no
concept of delete and there is no
concept of update this has a nice side
effect if I were to go through and set
an HTTP cache on an event what would I
set it for infinite all of our data is
completely immutable and we are an
append-only model now what does can lead
to is we might for instance say that now
we've got cart created three items added
one item removed and then shipping
information at it cool is this the same
as cart created two items added shipping
information at it and I see some people
saying yes and some people saying no one
that that's great because this is always
a confusing question for people the
answer is it depends on your perspective
if I were using this model and I'm
projecting this model off the two event
streams they're going to come up and be
the exact same what if I were doing a
different model though what if I were
making a model
would tell me which items were removed
from a cart would they end up the same
and this is a nice trick that you can
play inside of your systems today if you
can find any two sets of use cases that
leave your structural model at the same
end point guess what you just proved
you're losing data what's the value of
the data that you just lost depending on
my perspective these will end up with
different results now this is best seen
with a use case and I've been using this
use case for years let's go back to this
model and let's imagine where amazon.com
and they say you know what I think
people that remove their items from
their carts within six minutes before
they check out I think they're more
likely to buy that item in the future
than the other items that we show them
based on their preferences why when do
you remove an item from your cart six
minutes before you check out you looked
at your cart and it's gonna be like
three hundred and seventy dollars and
you're like my wife's gonna kill me I
can't do this I need to take two or
three items out of the cart it's not
that I don't want them I'm just
prioritizing them lower than the other
items that were inside of my cart so
what do we do here we add a new thing
called removed line items and then we
write a report that looks for removed
line items and then does nested
sub-query to see if you bought that item
in the future roll it out to production
user runs his report what does he see
nothing that report will work from this
moment in time forward let's do it in
this model so what I'm gonna do is on a
write a projection now where I talk a
lot about projections today projections
could be in memory projections could go
to a sequel database projections could
go to neo4j projections could go to
MongoDB not that I highly recommend that
and what the projection is going to do
is it looks for the item removed and it
says I am now searching for this item in
the future then it sees the shipping
information which is the equivalent to
the checkout and it says was this within
six minutes of that it was mark
searching for this
item in the future found equals false
and then as I go forward in the future
if I find that item in someone's cart I
marked true for found now what I haven't
told you about projections is when you
run a projection it always has to start
on event zero and come forward so it
will go to the very first event this
system has ever done and come forward
all the way till right now and then it
will run into the future I then write a
report based on that output State what
does my business user see when he runs
that report he sees the report as if
it's always been in the system in fact
he can even go back and he can look at
what that report would have told him if
he had that report on July 14th at 2:13
and 22 seconds in the afternoon this is
where the real value of event sourcing
is coming from it's this ability to look
at things with new perspectives and to
go back in time and look at history with
perspectives that you have today in a
real world example imagine being able to
take your brain as it exists today and
send it back in time to when you were 12
years old at your first dance and you
could perceive the facts then as you
understand them today it's actually a
really scary scenario I wouldn't
recommend doing it there's all sorts of
cool things that we can do because we
can move forwards and backwards through
time we can fire up new ways of looking
at our problem and it all comes back to
that we're not losing any information
and the only thing that you can do to
not lose any information is to store a
series of events there's no other model
that will do it any other model you pick
will lose some information it's just a
matter of what information are you
losing now how many of you have heard of
some of these before all of these are
wonderful databases they're all the hot
new things if you're not cool if you're
not using these except for one of them
how many you have heard that sequel is
dead no one will ever use sequel ever
again
new sequel no sequel that's the way to
go
yeah sure there is no best storage every
database on the planet sucks every
single one of them whether we're talking
about neo4j because there's a couple
guys in here on whether we're talking
about Cassandra whether we're talking
about Oracle whether we're talking about
events store they all suck every single
one of them and they all suck in their
own unique ways how many of you remember
these does this remind people of the no
sequel movement today years ago they
said that no one will ever use sequel
again because we have these new things
called object databases and object
databases are the best thing ever
object databases they Rock we can use
yaja with spring XML configuration and
store our data in XML and if you didn't
know llaves a language to convert xml to
stack traces if we can store our data in
XML we'll make it even better
but everyone found out that these things
actually suck when I used an object
database behind my domain model how many
of you hibernate before hibernates
really nasty because you got this giant
impedance mismatch between you and your
database these don't have that impedance
mismatch when I go to get an object I
walk from one object to another object
that that operation has done Big O of
one inside of that object database then
why did everybody stop using them how
many of you use an object database today
occasionally I'll get one hand actually
they're normally working in financial
systems a lot of the old backing
financial systems are still an object
databases why did they suck why didn't
they take over the world what happened
was people would go through and they
build their domain model up on top of
their object database and everything
would be hunky-dory maybe just getting
ready to go to production and then that
stupid business person would come over
and say something like you know I need a
report and what I want is a report that
takes all the sales from the cost
I want to roll them up by customer and
then I want to get an average and
standard deviation for their their
ticket size and I want to get the
summation total of how much they've been
spending you like that sounds like a
group buy or you could try doing it in
an object database and it's going to be
horrific ly painful to do you need to
load up 800 million objects into memory
and go through and basically deal with
this
and so everyone said you know object
databases suck we should never use them
perfectly rational thing they they suck
at doing OLAP behaviors so we just won't
use them anymore when we start getting
into these kinds of systems it's
extraordinarily rare that a single data
model will actually work to do
everything that you want to do and do it
well and we can go through lots of
examples of where people pick the wrong
models and it causes lots of problems
how many have heard of this small
startup so when they started going you
remember the fail whale what twitter is
at its core is a topic based pub/sub of
course there's no prior art inside of
this problem and they ended up
implementing it in Ruby on Rails on top
of my sequel until they had hundreds and
hundreds of my sequel instances you may
know that they've they've since changed
all of that but they ran into the
problems because the model that they
were trying to use underneath things
wasn't appropriate to what they were
trying to do another example we have a
table it's got an ID a parent ID and
then some data how many you have
implemented this before come on you can
admit it this is for a menu bar and then
you push this off to production and
production comes back to you and says
why is this report taking nine minutes
to run and of course you say well works
fine in my machine what's wrong with
your production environment well what's
wrong with their production environment
is that instead of having the 20 rows of
data that you had
they've got a hundred and seventy
thousand rows of data and what you've
created here is known as a recursive
query now you guys are all smart
developers so I imagine what you did
next was this where now I'm going to
have id parent ID zero parent ID one
parent ID two parent ID three and you'd
do the parent IDs all the way out to the
depth that you want to be able to query
because that way I can just do a query
and I could say where parent ID equals
or parent ID zero equals or parent ID
one equals or parent and then I can
rearrange them in memory and do the
recursion in memory this is very similar
to another problem I've seen I have seen
this particular problem at least three
or four times over the last six months
with real clients many of them in
America so what they have here is
they've got a table for person and a
person has a name or a type and I don't
know why it's named person and there's a
type company and it doesn't make any
sense and then they have a relationship
table where they have a relationship ID
a person ID a person ID because I'm
mapping two people together and then
they have a relationship type how many
have implemented something similar to
this now the business problem that
they're solving is they start saying you
know I've got Joe here and I want to see
if this person is related to Joe up to
three levels based on these relationship
types like they can be a caregiver or a
parent but they can't be an uncle so
show me whether or not this is true and
what's the quickest path to get between
them they had a roughly ten engineers
working on this problem for about a year
we actually fixed this for them and in
about three days what we did was we
brought in this database called neo4j
it's a graph database this is a graph
problem that they implemented inside of
sequel and it requires recursive queries
they were running nearly a half-million
dollar sequel server to support these
queries we were outperforming them on a
laptop
this is common and I have seen this
problem happen
over and over and over again now I'm not
going to say that you guys are incapable
of writing a graph database but how many
of you have a PhD in graph theory
no one ok I trust the people at neo4j
more I was actually funny I was doing
this talk in budapest about two weeks
ago and I asked that question there's
actually a guy in the audience he's like
well actually I do have a PhD in graph
theory and it's like well what
we're seeing here is that the wrong
model causes massive amounts of
accidental complexity we're not talking
like adding 20 minutes or 30 minutes to
your day because of the wrong model
we're talking about taking things from a
year of 10 people working to three days
of two people working all the rest is
massive accidental complexity other
examples of this can be seen how many
have seen sequel server full-text
indexing what do you think does better
job that or leucine Liat Lucene blows it
out of the water but we always try to
find one solution to our problems and
what we really want to be doing is we
want to actually be separating and using
multiple models inside of our problems
now I've been talking about one form of
this separation for a very very long
time and that is CQRS
command and query responsibility
segregation because reads and writes are
different we should separate based on
reads and writes this is not the first
time I talked about event sourcing this
is actually the second I believe and it
was hilarious because I wasn't really
talking that much and I don't mention
died was very young back then I mean
we're talking what it's like seven years
ago so I'm like 27 years old hadn't
talked much at conferences I come out I
probably had 20 cups of coffee my front
row people I've never met Martin Fowler
Gregor hope-a and Eric Evans and I'm
gonna be talking about extending their
work I think I went through my entire
talk in like 22
it's and afterwards Eric comes up to me
and he's like you know that was a bad
talk it's like well crap thanks Eric I
appreciate that but reads and writes are
one way that we can start distinguishing
and separating now for me the reads are
the interesting part of systems writes
are really simple and we'll talk more
about them but if you look at most
systems the interesting ways that you're
going to be putting your data it's about
how you want to read your data not about
how you want to write your data and when
we look at the reads it's usually reads
that need to scale not writes I bet most
of you could run the rights of your
system on my mobile phone how many of
you do more than 50 transactions per
second couple hands I know there's a lot
of finance guys here so I should be fair
and say that in financial systems this
is completely backwards and you get lots
and lots of rights for very few reads
but most systems the reads are what I
need to scale and I get brought in for a
lot of scalability problems and one
thing I always find funny is you go and
look at their problems and their
problems are scaling their reads off of
their sequel database I bet most of you
are somewhere between one and two orders
of magnitude more reads than rights
my guess is most of you are somewhere
between 90 to 99 percent reads now how
many of you have a third normal form
database or right around third normal
form this is what I typically find you
built a model that was optimized for
writing and then wonder why you have
problems scaling your reads it's the
same thing we were talking about before
but reads are almost always the
interesting side of things
rights are almost never interesting if I
go look inside of neo4j if I go look
inside of event store if I go look
inside of sequel light if I go look
inside of Cassandra you know what they
all work the same way for writing so
does oracle so too sequel server they
append to a log and then internally
after they append to a log they will go
and update whatever the thing they're
going to be querying it is whether it's
a b-tree is it a stored string table
they all work almost identically now in
general storing events is a good
lossless transactional model that's what
the database does internal e you can do
the same inside of your code why not for
writes just store an event that's what
we are talking about all the way back
when we're talking about event sourcing
I'm just appending events to a log now
reads and writes are one form of
separation that we can talk about but
there's other forms of separation that
exist another very common form of
separation is time time will allow us to
separate out different models now if I
go into finance this is actually a
domain concept it's the concept of
intraday data versus inter day data
today is treated differently than any
other day in the history of mankind and
this is common to see what I've been
seeing a lot lately is big multivariate
problems things that people are doing
large map produces across and what I'm
finding is they will end up having this
massive MapReduce job let's say using a
t Amazon extra large nodes and then they
do that once per month and that will
give them perfect answers to their
multivariate problem but now the
business wants to get that every day how
much does it cost to ramp up 80 Amazon
extra large nodes for a month
it's prohibitive but what we can do here
is we can separate based on time and
what you'll normally end up with is
you'll end up with the large Hadoop
let's say batch job and then we'll put
in a little tiny speed complex event
processor now what the complex event
processor is doing is it's going through
knowing what this thing said and what
you cared the most about because let's
say you're bidding on things because of
that information and as it's going
through its invalidating thin slices of
the big problem and then doing little
tiny MapReduce jobs to try to end to
update just a very thin slice of the
overall MapReduce job this is very very
common and it can get you to the goal of
having near real-time information
without taking the expense of having 80
Amazon extra large nodes running
constantly this is a really common form
of separation of models between data
that's over one day old versus data
that's current you will see these
separations start happening all of the
time and almost never do you want to
have a single model single models will
cause problems you're always going to
end up because every database sucks in
its own unique ways now when we start
looking at how to do this a lot of
problems come up how many you have tried
running with two databases in your
organization before did they go out of
sync what happens when I run three
models and they all tell you three
different things
it causes a big problem and this is
typically what ends up happening so this
is my stereotypical way that I see
people trying to implement multiple
models in their system so basically
they've got a client that's talking back
to a domain the domain might be backed
by something like hibernate talking to a
third normal form database the domain
model will then publish events off to
this magical thing called a bus and the
bus will then deliver the events off to
the things that are going to be reactive
so we may write into an OLAP database we
may write into a graph database we may
look at things from a stream processing
perspective this is doomed to failure by
the way has anybody here ever watched
martin fowler's talk does my bus look
big in this I would highly recommend it
but in production this will not actually
work well one of our first problems has
to do with this so if my domain model is
going to write into a third normal form
database and it's going to publish a
message into a bus what does it need
well what happens if I go to write to
the database and I get a success and
then I say now publish to the queue but
someone unplugs me right before I say
that so now we're going to need a
distributed transaction of some type
stay away from distributed transactions
avoid them like the plague if you can
but it's a more sinister problem here I
know none of you would put a bug into
software but you have juniors on your
team what happens if we have a bug here
let's say there's a bug and into the
domain model we set the state to be a
hard-coded Connecticut while we take
whatever our clients command was and
correctly put that onto the event to go
off to everybody else so now this
database says it's in Louisiana and that
database says it's in Connecticut how do
you handle this this is a very very big
problem now let's just say that that
occurred how would I detect
that occurred how do you reconcile
between a sequel database on a graph
database this should be fun right
what is right a reconciliation script of
course which is more likely to have
happen that you actually found a bug or
that you've got a false positive because
your migration script is broken or sorry
your reconciliation script is broken
okay let's imagine that we found out
that this occurred how do we fix it do
we go back from the OLAP model to the
third normal form model and open up a
distributed transaction through linked
servers or something we've got a really
big problem here but there's a more
sinister problem and that's what happens
when I try to bring out a new model and
we talked about one of the beautiful
things about event sourcing and this
keeping of streams of events is that I
can have new perceptions that I come up
with at any point in time and I have a
new one now and I want to start saying
okay I need to get back all of my events
so I can bring up my new model how do I
do that with this bus in the middle do I
do that what's happening here is I've
got a bunch of cues I would have to send
a message from the new model back over
to something on the other side to say
hey I need you to resend me all the
events I haven't seen before in other
words I need a control channel very much
like an FTP can you imagine if blogs
work like this so you've come to my blog
and you want to subscribe so I say okay
cool if you want to subscribe to my blog
send me an email I will create a queue
for you and then send you a link to your
queue and then you say but I don't just
want the blog post in the future I want
to get your historical blog post too so
you send me another email that says I
want your historical blog posts and then
I the historical blog posts into your
queue this doesn't sound very scalable
this is actually a huge problem and it's
a source of accidental complexity that
people don't see when they start
building this and they find out at the
end once they've gone to production and
thing is it's really hard to undo this
once you've done it this is a fairly
typical problem but there's other ways
of dealing with this particular
situation and in order to think about
them we need to think about how blogs
actually work and if I have a queue here
what we have is known as a a producer
driven subscription the producer
remembers the state of the subscription
if you are following my blog it doesn't
make sense for me to know the state of
your subscription you keep the state of
your subscription you remember how far
along you've actually come and when you
come back up you ask me for anything
after that point in time and that allows
for a lot of interesting things to
happen here so now instead what we're
going to do is we're going to have a
client that's talking to a domain and
behind that we're just gonna store all
of our events and we're gonna expose
them over a protocol let's say Adam Adam
would be perfect for this now all of
these subscribers remember their own
State they don't have queues they all
just let's say listen to an RSS feed of
the events that are happening now if
this guy needs to reset and go from
event zero what it's going to do is it's
just going to forget its own
subscription information and go all the
way back to the beginning and say oh I
will I was at 120 - but forget that
give me the next 20 events after
negative one each of these controls
their own subscription on the other side
we no longer have two sources of truth
we don't have the events and the third
normal form database we only have the
events by the way if I'm writing to an
event store and I'm writing to an event
store do I need a distributed
transaction now because you're only
writing to one store at this point it's
a single local
there's no need for distributed
transaction right into that event store
is the equivalent of doing a publish
it's this at the same operation it's
save and publish as part of the same
operation and these are all just
following check pointing off of what is
available over in those events but where
the real beauty of this model starts to
come into play is when we start saying
that we're going to add a new model to
this before when we were back here a new
model is going to need to talk to
something on the other thought side
through a control channel to say I want
you to come up and start sending me over
events by the way this is always an
interesting question so if I talk over
the control channel to you to tell me to
resend my entire history and there's
currently things that are happening
right now
do you interleave them into my history
do you cache them until you're done
sending me the history how do we
coordinate when I get those new events
compared to when the old events were
being sent across and it's actually a
really really hard problem and it
requires generally that you're going to
go off and build some infrastructure
here this model controls its own
subscription it controls its own state
of its subscription it can just throw
away that state start from event zero
and come forward again the easiest way
to think about these protocols is
imagine that every event is given a
number 0 1 2 3 4 5 when I first come up
I say give me the next three events
after event negative 1 so I get 0 1 2
once I've processed two I remember I
have saved 2 and I say give me the next
3 events after 2 by the way I crashed
right now so I come back up I say what's
the last thing I have processed event
number 2 give me the next 3 events after
event number 2 and what this does is it
simulates at least once messaging now
you can actually simulate transactional
messaging with this as well if you store
atomically your check point with the
work that you
actually doing and it's a really nice
cheap way to be able to simulate doing
this kind of workflow here this is just
going to blow away whatever its state
was if if I want to replay this
something I just say forget everything
that you've seen before and start back
at negative one and come forward there's
no coordination that needs to happen
between these two it's all going to
happen on this side and this will help
to remove a lot of accidental complexity
inside of these kinds of systems another
benefit that we can have here could I
have two OLAP models that are identical
if we start seeing we're getting too
much load on our OLAP model people
running reports could I just spawn off a
second one right next to it and then put
a load balancer in front of these two as
to which one gets the query providing
that my all that model fits into one
silo yes and this is very often how we
scale in these kinds of systems now and
where's my slide there it is
and this is supposed to be an animated
jiff but I've switched it to PDF and
apparently PDF doesn't like animated
gifs very much what this is is this is a
route Goldberg machine and it's an
animated ship it's so much better it's
actually going all over the place and I
don't want people to walk out of here
saying Greg told me that we should use
ten different databases we should use a
different database for every problem
that we have we saw that leucine is the
absolute best at doing C at doing
full-text indexing and you know we've
got a full-text indexing problem that we
do over here where people start typing
in this one thing and we do autocomplete
on it so we need to get leucine and we
realized that we've actually got a graph
in our system it's the 28 items that
make up our menu so we need to bring in
neo4j in order to handle our menu this
is not what I'm saying
when we talk about this kind of stuff
you need to remember there's two sides
of this the other side of this is
accidental complexity and sorry
operational complexity if I for instance
bringing MongoDB somebody in operations
needs to know what the hell MongoDB is
they need to know how to configure
MongoDB they need to know how to monitor
MongoDB how do you do a backup for
MongoDB somebody is going to need to
know all of this information in order to
be able to get the stuff working and
that has a cost as well in terms of
complexity from operations perspective I
am NOT saying that you should go off and
get customized database for every single
thing that you want to do instead I'm
saying more understand the trade-offs
you will take accidental complexity
because you're choosing the wrong model
if I choose lots of models I introduce
operational complexity the answer sits
somewhere in the middle now if I were to
come to your company and you were to
show me that you've got a small
full-text indexing problem and you said
you know we do it inside of sequel
server I'd say oh okay
best and then if you said you know and
we do it inside of sequel server because
we were looking at bringing the scene in
and the cost of bringing in Lucene from
an operation
perspective was much higher and we don't
really have much data we've only got a
hundred thousand rows of data we're
doing it over anyway so based on the
trade-off we decided that we would
actually use sequel server indexing I
would have absolutely no problem with
that that is an informed conscious
decision if you're bringing me in for a
performance problem with full text as
full text searching inside of sequel
server and I were to say well have you
considered using something like we've
seen or elasticsearch you went Lu what
that would be a bit more of a problem if
we're going back and we're looking at a
system that is built like this and when
I mentioned a graph database you go oh
what this is we really need to be
getting in and understanding all of the
different kinds of databases they're all
valuable and they all suck if you use
them in the right places everything will
work well if you use them in the wrong
places it will really suck operational
complexity needs to be taken into
consideration when looking at this we do
not just say that we're going to put in
a custom database for every single thing
that you want to do but understand that
you can put in a custom database for
everything that you want to do I
normally in systems will see at least
three models I'll normally see some kind
of key value store document store that
represents data that's just being shown
back up on screens
I want quick query access to this to be
able to show it very quickly up tutorial
user on a screen there's normally going
to be some type of searching inside of
these systems and almost always there's
going to end up being some OLAP system
that people use for reporting almost
every system I see will end up with
about these three graphs are another
really common one that we'll see start
coming into the problem event processing
is another common one we'll start seeing
in the problem but keep in mind that
there is two downfall of this which is
operational complexity I am NOT telling
everybody to go off and grab ten
databases and put them in production
understand there are costs although to
be fair the cost of running ten other
databases will be probably cheaper than
one Oracle Isum
now some takeaways just appending to a
journal is a really really good model to
sit behind the writes inside of your
system this is what your database most
likely does internally anyway it's just
writing to a transaction log and then
it's internally going off this and
updating its internal structures there's
absolutely nothing wrong with you just
saying you know what I'm gonna do that
and then I'm gonna give the chance for
other databases to read off of the same
journal that I'm writing to absolutely
nothing wrong with doing that and by the
way you can get this to be really really
really fast handling tens of thousands
of transactions per second is absolutely
no problem doing that it's relatively
simple to reach and I'm not talking on
like fusion-io boards or something like
that on a fusion-io board I've actually
seen them get up over a half-million
this transaction log can be events and
this kind of brings us back to the idea
of event sourcing the general idea is my
book of record is this log of events
that I'm writing my operation of a write
is just to append an event to a log and
then I'm considered done this append
only immutable data works really really
well for a lot of systems how many of
you have needed to communicate something
to your UI before as well could my UI
also listen to one of these atom feeds
that we were representing events over
them and put update saying by the way
this person's also looking at this data
or updated it be relatively easy to do
and having that event model makes it
relatively simple CQRS is one way of
finding different models so saying I'm
gonna have my rights on this side of my
reads on the other side but it's not the
only form of being able to segregate out
models
and if there's one single thing I want
everybody to walk away with it's that
every database in the in the world it
sucks every single one of them it
doesn't matter what the model of it is
there are problems that it will suck at
neo4j I love neo4j try doing a set
operation in neo4j and you will realize
that it sucks
I love sequel try building a graph in
sequel and you'll realize that sequel
sucks
I love events tour try doing a query
like I'd like to see all the customers
whose first name starts with the letter
G and you realized that it sucks why
because we're gonna have to replay every
event inside of your system from all
time in order to give you an answer to
that question it's an absolutely
terrible terrible database you should
never use any of them now understand the
different models go out and start
checking out all these different
databases what are they good at what are
they bad at object databases are not
crap they have their uses and there's
certain types of problems that all of
these systems will do extraordinarily
well on and there's certain types of
problems they will all do
extraordinarily poorly on and remember
the other side of this with operational
complexity you don't want to just bring
in ten databases and say yeah we're
we're done there are trade-offs to this
as well now with that I will thank you
for coming out</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>