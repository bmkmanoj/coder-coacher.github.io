<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • The Verification of a Distributed System • Caitie McCaffrey | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • The Verification of a Distributed System • Caitie McCaffrey - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • The Verification of a Distributed System • Caitie McCaffrey</b></h2><h5 class="post__date">2016-10-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kDh5BrqiGhI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so yeah let's get started I'm going to
talk to you guys today about the
verification of a distributed system and
the point of this talk is not to like
belabor formal methods and academic
proofs because people in industry don't
use them the school we're going to talk
about some real ways that you can do it
and I have to mention hormone proofs
just because like their thing and there
is some interesting uses in industry
that are happening right now so let's
get started I'm kidding me
there goes I'm Katie McCaffrey like I
said I'm a distributed systems engineer
I've spent my career building mostly
large-scale backends at power
entertainment experiences and video
games so for Microsoft on the Xbox so
halo series Gears of War HBO and now I
work at Twitter where I'm at I was the
tech lead of observability and I've
recently branched out into distributed
build tools which is super exciting this
is me on the internet my DMS are open if
you have questions or want to chat
afterwards totally talk to me cuz I I
like doing that it's great to hear from
everyone ok so we're gonna start this
talk off with a quote from Leslie
Lamport because what distributed systems
talk doesn't need a Leslie Lamport
reference
so Leslie defined back in the day that a
distribution is one in which the failure
of a computer you didn't even know
existed can render your own computer use
unusable so back then maybe that was
fine this is not OK today if like one
computer goes down and totally breaks
your whole system then we have a really
big problem because your users are most
likely being affected and that impacts
your bottom line so the point of this
talk today is sort of like how do we
increase our confidence that our system
is doing the right thing so that this
one pesky computer going down or
multiple computers going down or a data
center going down doesn't affect our
system and it can keep running I also
going to take a quick aside to say we
are all building distributed systems
even if you aren't building Google scale
or Twitter scale or whatever
insert company scale we're all building
distributed systems and we all have been
for a while our clients talk to our
services which talk to a database and
this is even when we were back on one
single database and
this may be a small simple system but
it's still distributed because there
we're talking over our network
components can fail and so all of these
tools are that we're going to go through
today are applicable to any size system
your system might be simple it may be
really large and complicated like
Twitter services this is a trace
created by Zipkin which is our
distributed tracing system at Twitter of
all of our micro services these have
sometimes been referred to as Deathstar
architectures so like this is kind of
crazy and like testing all this is very
important because we want to keep
Twitter up and running so you can use it
and then you know sometimes we build
these systems that are a little crazy
and you might like sort of scratch your
head and be like what the hell have you
built but we still want to test these
systems and make sure that they work ok
- ok so a quick overview of where we're
gonna go today we're going to talk about
formal verification very briefly this is
and then just because it is sort of like
the gold standard of how we verify that
our systems are correct and then we'll
spend the bulk of the time talking about
how you can test your things in the wild
and while these don't give you a gold
standard like six gold star approval
that it's probably correct it does give
you a lot more confidence you can sort
of mix and match which ones work for you
based on time and budget and investment
in tools and resources that you have and
then we'll briefly touch on some
research that I think is really
interesting that's giving us like a new
hope that maybe building and testing
distributed systems is not so terrible
I'm gonna go through a ton of
information in this talk and don't feel
like you have to take notes or anything
I have a github page where this talk is
listed and all of the references are
also in there if you want to dive deeply
into more don't worry about trying to
read that there's a bigger link at the
end and I'll tweet it out also if you
want to look at it okay so let's get
into testing what when we do testing and
formally what we say is that we're
trying to prove certain properties or
about our system and we're trying to
prove safety properties and this is the
idea that something bad will never
happen in our system and we're trying to
also prove liveness properties and this
is the guarantee that something will
eventually happen in our system that our
system can make progress you can
obviously build simply build a system
that has the either one of these
properties like exclusively but it's
generally not a useful system right you
can also think of safety properties as
the idea of like what is the system
allowed to do basically like this could
be a guarantee that all committed data
so all right acknowledged or acknowledge
rights are persistent and correct and
like durably stored and will never be
lost equivalently you can also say a
lightness process property is this idea
of like what the system must eventually
do so this is an idea of like when I
receive a request I will eventually
respond to that request so those are
sort of very like formal terms of what
we're doing when we're testing like I
said formal verification is this idea
that sort of came from academia and this
is how we prove that systems are correct
this is really great because we when we
build like you know programming
languages and things like that we kind
of have an idea that they actually work
or a consensus algorithm and this gives
us this gold star like our system is
provably correct we have done like we
have math that has told us that the
system is going to do what we think it's
going to do and that's really useful for
certain things formal specifications
generally use there are two sort of ones
that people generally have heard of
there's TLA plus and then there's coq
we're going to go through a quick TL A+
example just in case you've never seen
it and you're totally like I've heard of
this but and people talk about how
terrible it is and boring just because I
think it's interesting to to see um and
this is a quote that once again Leslie
Lamport
invented TL A+ at Microsoft Research and
he writes in his book specifying systems
which you can just read a PDF online
it's cool it's a good idea to understand
the system before building it so this is
the idea that we should like maybe
design something before we start coding
it but he also goes on to say that it's
a good idea to write a specification of
your system before implementing it this
is saying like let's define what
properties our systems are going to have
and that prove that these properties are
going to hold and actually produce the
outcomes that we think they're going to
produce so this is an example from
specifying systems of TLA plus and the
our clock specification so he basically
writes a really simple specification of
an hour clock this is just the idea that
it displays the hour from like 1 to 12
and so you'll write your proof and
you'll define it like right this one
line is saying that like the hour can be
one to the number 12 like those integer
numbers and then it defines how it
updates if it's 12 it becomes more
otherwise it increments by one hour plus
one and then you take this this is like
you know it looks like code right and
you take it and you run it either use TL
TL C which is a model checker or TL ApS
which is a proof assistant and then you
put your specification into one of those
and it will tell you whether your system
is correct or not and and then you have
a verifiably provable system so that's
neat industry has actually used this
this is not just an academic thing so in
2014 amazon released a technical report
on how they use TL a plus to verify 10
plus core pieces of their infrastructure
including s3 this is a super
approachable paper so if you're
interested in maybe using this to verify
key pieces of infrastructure I highly
recommend reading it so we'll go through
some of the highlights but basically
they declared at the end of this paper
that formal methods have been a huge
success and they're now in their annual
planning actually budgeting time into
building systems so that they allocate
engineering time to use TL a plus and
write specifications for their crucial
pieces of infrastructure like s3 when
they did this they found two various
serious bugs that they say they couldn't
have found in any other manner of
testing and then it also allowed them to
increase confidence to make all these
sort of like very complicated perf
optimizations that were changing the
underlying and implementation and the
algorithm quite a bit and they felt that
they wouldn't have had this confidence
to make these optimizations in these
core pieces of storage because that's
kind of risky without having a formally
correct proof so so this is sort of cool
like if you are building something that
other people are gonna use heavily rely
on like a storage system or a database
or you're offering something as a
service like maybe this is something you
want to invest in especially if you have
strong consistency guarantees around it
they do call out also in this paper that
one of the and this is typically one of
the biggest criticisms of tli Plus and
formal methods is that they generally
you write this logic or this like your
formal specification code and that's not
code that generally runs your system and
so you could have a totally correct
specification of your system and your
code that you write is still wrong and
that's like a challenge right it's worth
noting that which I didn't have
time to go into actually can generate
Haskell or objective camel for
the specification and that is runnable
first subset I don't believe it does the
whole language I believe it only does
the subset of the language because other
like you have to be able to prove
properties that like it terminates and
you know I can't do that with language
most languages so that's like something
that sort of helps bridge the gap but
obviously we still aren't all the way
there even if you have a formal
specification you probably still need to
test the code that you wrote so let's
talk about how we're going to test the
code how US practitioners are going to
do this in the wild and so while we may
not get this idea of a formally correct
or provably correct system you know we
get this idea of it seems pretty legit
so I'm gonna put it in production ok
this is super basic
I don't wanna belabor unit tests but
like write them please um typically this
is like the idea that it's implemented
by the developer who is writing the code
it can be ran locally with CI or no
environment needed it's just your CPU
and this is to verify basic
functionality and error cases this
increases your confidence that your
codes actually doing the right thing
typically I like test cases for it's
increasing my confidence that like you
know six months down the line my codes
are doing is still doing the same thing
when someone is like you know done a big
refactor or change an implementation and
then I would like to break for a brief
rant a Festo because if you're going in
the programming languages tap track or
anything or you used a typed programming
languages types are not testing types
prevent you typed languages are great
I actually prefer to use them so I want
to totally like talk about my bias
because this is a perspective um but
like when you're writing in a type from
language it only gives you so much it
can only prove as much as your type
system is as expressive as your type
system is so this is like a brief
example in Scala I define a method add
and then like I make a dumb typo and I'm
actually multiplying the values on
return so like obviously the type system
has not made the system correct this is
gonna return the wrong thing a unit test
would find this and and I sort of break
here because I think it's sort of a
there's a lot of debate in the
programming community but we don't have
expressive enough programming languages
and the ones that were typically using
an industry to like rely on this there's
actually research going on that may May
this better but we're not there yet I
also sort of want to call out that you
know this is sort of this is a this is a
thing that happens a lot
there was a story of a younger developer
that I was working with at one point who
he was very enamored with type systems
and the mathematical proofs behind them
and type theory and that's great but he
believed that you didn't have to write
test cases and if you're on my team
you're gonna write test cases so like
caveat if you ever want to work with me
so I wrote in a code review that like
hey you need to add some test cases to
this and I like went through and did a
thorough code review and I actually
found a bug which was his recursion
algorithm never would never exit it
would actually recurse forever and could
blow up her stack and so if you deploy
into production that's gonna cause a bad
time and a simple unit test case would
catch this right so and then I you know
very nicely wrote back because I wanted
to use this as a teaching moment that
hey I think there's an impairment
curtain bug here could you please out a
test case specifically for this and he
did and then we all moved on and our
lives were better right I also want to
point out that like TCP doesn't really
care about your type system so when you
start going over in the network like all
bets are off and your type system isn't
gonna help you here yet right once again
programming language research people who
maybe in the audience you know like
please solve this problem for me but
like TCP doesn't care so this sort of
brings me to my next topic of you need
integration test and I know there's a
lot of different testing methodology in
industry but I sort of defined
integration tests if you're gonna stand
up a small some kind of environment
you're going to write these tests so
that you're exercising sort of the the
network boundaries between your systems
so this is talking to your database or
testing your network protocol testin e
inversions of your network protocol also
important there's this fun sort of story
that I had when we were shipping Halo 4
that we caught a fairly large bug that
would have really hammered our system at
launch because we had proper integration
testing so I worked on the statistic
service of Halo 4 you uploaded your
stats at the end of a game and we
process them we noticed during
integration testing that the game was
consistently uploading the stat blob
three times and this thing is big and
it's expensive to process so like doing
three times the load that we projected
would have been a really bad time in
production I'm so I'm going back and
forth with the network developer or the
game dev who
responsible for this feature on his side
and I had written API on our side and
I'm like look look at our logs we're
returning HTV 200 like why are you
thinking we failed and keep sending it
to us three times and still think that
you failed to upload stats and he's like
I don't know like you guys must be
sending us something wrong and we had
this nice little banter back and forth
and eventually what we found out is that
because of legacy code what the game
code was actually looking for was the
words done in all caps in the payload
exclamation point to mark a thing as a
success so androgen tests are really
useful because it's testing the
breakdown between your interfaces right
and and once again like very smart
developers do very different things and
we all have to deal with legacy code so
we should you know exercise these
boundaries and prove that they're
correct before we just sort of you know
throwed code out there to our users to
test also to support unit and migration
tests there's this really lovely paper
that's probably one of my favorite from
2014 we're called symbol testing can
prevent most critical failures and what
the authors did is they studied 198
randomly sampled real world's failures
reported on open source software so this
included things like Cassandra HBase
HDFS MapReduce and Redis and then they
have a ton of information in this paper
but I'm gonna go through some of the key
highlights that I think are really
interesting but also like go read this
paper too so one of the key findings
that I think that was super interesting
because it debunks a popular myth that
you need like a whole giant staging
environment that looks just like
production to reproduce some of these
like nasty production level bugs is that
out of 98% of the failures that they
analyzed three nodes or less camera
reproduce this you can stand up three
nodes on your laptop and run it right so
like we can see enough free nodes in a
dev environment that's super
cost-effective it's super cheap so
there's really almost no reason we
shouldn't be doing this level of testing
is really effective in finding failures
in our systems and and when I say
catastrophic failures what they're
talking about is like data loss system
crashes from these core systems that are
actually supposed to be very stable and
store our data and be very reliable so
this is like do integration tests right
another thing is testing error handling
code could have presented 58% of
catastrophic failures so this is once
again we didn't
right unit tests that tested anything
besides the golden path or we didn't
write unit tests at all and so what this
says to me and as a practitioner and one
thing that I took to heart is use a code
coverage tool
I know code coverage tools are super you
know like it doesn't tell you if you
have a hundred percent coverage that
your code is correct but it gives you an
idea where you have a gap and if there's
like all these big like oh we didn't
test the error handling cases like 58
percent of catastrophic failures were
caused by error error handling right
yeah so so so so do this it's super easy
like I know writing test cases isn't the
most fun thing in the world but like you
know we get paid for this also what I
think is interesting here is that like
you know and we've all done this right
like in Gears of War we were like
investigating a bug after launch and
we're going through the codebase and we
find this like you know if do this else
if do this else error please fix kay
thanks bye in a comment and there was
like no code there so that actually
wasn't the bug but you know like I think
we all have these in our code bases so
just actually handling them will help us
a lot
finally one of the other the last point
from this paper that I'm gonna leave you
with today is that certain 5% of
catastrophic failures were caused by
very very basic things like internet
error handling code is simply empty or
only contained a log statement they
actually found in the paper that most of
the catastrophic errors were logged so
that's at least good right you have
information to go fix them but they just
weren't handled error handlers abort
cluster aborts the cluster on an overly
general exception so this is like Oh
catch exception and then like you know
tear down the cluster which is bad so
maybe like do a little more thought
about how your systems are supposed to
fail or this idea of like error handler
code contains comment lines like fix me
or to do this is laziness right and this
is causing catastrophic failures in our
system and like so just like we talked
about how terrible and hard distribute
systems are to build we can fix a lot of
those problems by just doing unit
integration tests and this paper sort of
showed us that so I really like this
paper I'm actually gonna be talking
about it at Q Khan or at New York's
papers we love in June if you're there
come say hi okay so let's move on to
something that maybe you haven't heard
about and it's not just me ranting about
write test cases this is property based
testing so this is sort of inspired by
life model checkers which are
actually like a formal method of
verifying where you go and actually
exercise the entire state space of input
into into your system on that formal
specification so what property based
testing does is this is you know made
more industry friendly and they're gonna
execute
you're gonna write properties about your
system that you want them to hold and
then it's going to execute randomly over
that state space so while it doesn't
prove that your system is correct it is
exercising more the state space than
just a single unit test case can there
are a ton of tools to do this quick
check is the it was invented by John
Hughes and that was sort of the first
one and there's a version in Haskell and
Erlang that you can use and so what you
do here is you just define the
specification instead of a test case or
you can write both if you really want
you use a tool that generates many
inputs to test the scope and the
specification and what's super nice is
that if they all pass and you get a
green you know thumbs up and if one
fails it'll actually tell you like this
specific use case that caused the
failure so that gives you a lot of
information to go in start debugging
this right it's not just like oh like
something failed in the wild and now I
have to go scrub through like you know
miles of logs to figure out what
happened across my system that's
annoying this is really nice and then
one other quick note from that previous
paper about testing and catastrophic
failures is that they sort of found that
basically like three inputs or less
could reproduce more most catastrophic
failures and the order was deterministic
so if you can define your property's
broad enough you may catch them with a
tool like this so quick tricks the
original one I've used Scala check
that's written in Scala and Java and on
the JVM and then if you want to use it
like all these languages down here also
have quick check ports and I link to
this from references because if you
can't read all that but basically
there's one of these for almost any
language that we're using in production
today which is cool it's also worth
noting that quick check is really great
because it's been used by companies been
sued successfully used by companies like
bash show which has react which is a no
sequel eventually consistent data store
and they have a great talk on how the
sort of model eventual consistency
properties that's in my reference
section using quick check they've used
it to find quick check to find bugs in
Volvo cars and most recently there was a
talk in a paper published about how they
use
to find bugs and Dropbox and so these
are gonna discover some of your more
gnarly bugs because it's gonna exercise
over a larger set of inputs and you just
have to like think about um just a super
quick example so you can basically see
it like this is not I'm not asking you
to write a crazy specification this is
the Scala check one because that's what
I've used so this top one is basically
saying I'm gonna define a type small
integers and then this is a basic level
property you can like combine properties
together to make more complicated
statements but it's basically going to
say that like the number should always
between 0 and 100 and so anytime I sort
of see this then you know that that
property will hold this second one is
how to reverse a linked list or reverse
a list and so it says you know the
reverse of the reverse of a list should
be equal to the list and then like what
Scala check will do is go and generate a
bunch of inputs to this and ensure that
holds so you didn't have like not find
one error or like you know whatever like
some bug in your system and it'll return
you a counterexample so this is this is
pretty easy to do and um you know I
think I think for the basic stuff is
very similar to writing unit test cases
on the investment time and you could
obviously do way more with this so I
like property based testing I am
encouraging a lot of my teams to use it
more because I think it finds a lot of
bugs and it's it's sort of low
investment for a hybrid ward finally I
want to move to our fault injection
which is another way that we can test
distributed systems I think this is
particularly useful for distribute
systems because we're basically forcing
our systems to fail and then we're
observing what happens I fully believe
that if you do not force the system to
fail all the theory and like proofs in
the world like besides formal
specifications are basically not going
to give you you shouldn't have any
confidence that it's gonna operate
correctly in failure modes so you should
stand it up and force it to fail and see
what actually happens and prove out your
design one example of a fault injection
system that you're probably familiar
with is netflix simian army I'm not
gonna spend a ton on time on this but I
like to like show it as like this fits
into that classification of tests so
this is like they have chaos monkey
which kills random instances latency
monkey which introduces network lag and
in between packets and then they have
chaos gorilla which takes
an entire availability zone to make sure
that there are multi DC tolerant and
things like that obviously this is a ton
of investment and we don't have to go
this far with fault injection testing
you don't have to build something of
this scale to get benefits of it
another popular fault injection test is
Jepsen so this is a tool that's
open-source that was written by Kyle
Kingsbury and when it goes as it does it
simulates network partitions in the
system under test and then after the
test operations and results are analyzed
it will basically say like did your
claim consistency guarantees hold were
they upheld he's used this to test a
bunch of different systems including
like MongoDB elasticsearch Kafka there's
a whole list on his website if you want
to go read them and basically what Kyle
sort of showed us as he started doing
this a few years back is that the
distributed systems we rely on or maybe
not as reliable as we think they are and
this isn't because people are bad people
are bad developers it's just because
distributed systems are hard and
predicting all of the failure cases and
dealing with partial partial failure and
a synchrony is hard and so what's really
great about this project I think is that
he publishes these results he files bugs
on you know github and open source
projects and a lot of these things have
been like fixed right and the systems
are getting better the goal is not to
like make fun of people by finding bugs
the goal is to like make the system's
better that we use although like this
image is hilarious and was these were
drawn by Kyle and he let me use them
also you can like now pee Kyle - Geoff's
and tester system so if you're building
like a database or something that's like
a piece of infrastructure maybe do that
I want to pause really briefly to be
like passing none of these tests I've
shown you while they will increase your
confidence that your system is doing the
right thing passing them does not ensure
that there are no bugs in your system
there still could be bugs but we
probably have more the more of these we
use and the more that these tools that
we use the more confidence we have that
our system is doing the right thing
another final method of fault injection
tests is this idea of game days so this
was developed by Jesse Robbins at Amazon
in 2000 mm and he actually had the title
there of the master of disaster because
basically what he would do is he would
just go break production and he would he
would do this in a responsible way where
he would tell engineers that there's
gonna be a major outage in three to four
months get your systems ready
they wouldn't know exactly what the
major outage could be it could be you
know we lost an entire data center
I guess one time they mimicked that
there was a fire in the data center or
you could lose a rack or a series of
racks and in reduce your capacity
significantly and so the idea here is
that a we're telling developers like you
know your systems are gonna fail plan
for it and make them more reliable and
then the other idea here is that they're
testing their people processes in
addition to their systems so the system
failed and now we have to our DevOps
people and our on-call people trying to
fix it and how do we ensure that it's
fixed right like are those processes
also good where is the breakdown in
communication there so you're testing
your people and your processes in
addition to your software so this has
been used in a bunch of different things
it also helps you if you have a ton of
data centers or pops or you know
instances installed around the world if
you become more global figuring out
where you have these weird hidden
dependencies I think they found a bug
we're basically the data center they
took out contained the only it was the
only instance of their paging system so
like no one got alerts that the data
center was down and that's kind of bad
but like this is how you find these
things that are you know maybe more
operational maybe more config driven
error those are still very difficult
things to test right so if you want to
run a game-day at your company how do
you do this and it may not be like as
involved as like an Amazon one right
you're gonna notify your engineers that
a failure is coming like you probably
shouldn't just like pull the plug one
day and be like surprised you're gonna
induce a failure then at some future
period of time you will monitor these
systems that are under test typically
there will also be in observing only
team so they're sitting there and
they're more their job is to monitor the
recovery process this is how you find
the bugs and the people processes and
where the breakdowns are the breakdowns
are you don't want the the devs who are
sort of like oh my gosh like we just
lost an entire rack the systems on fire
and trying to fix that to also be trying
to evaluate like the processes this is
the idea of having an objective observer
and then when you're done with that you
need to sit down and go through the list
of like here's where everything failed
you and a prioritize those bugs and get
buying across the teams because
typically where they found failures in
this especially in people processes is
across team boundaries right because
that's always where the breakdown in
communication is
okay just to sort of point out like how
simple this can be and like how critical
of bugs that can find stripe wrote a
blog post about how they ran a game day
and all they did is they basically ran
kill9 on their primary Redis node and
then it went down and like the other two
like started handling requests and that
was fine but then it came back up and it
had no data and it decided it was still
leader and it propagated the fact there
was no data in the cluster to everyone
in the cluster and they lost all the
data in the cluster so that's bad
luckily they had done a backup because
they were doing they knew they were
doing controlled failure and so right
there's this tweet that Kelly Somers
made at the same time once this blog
post was published because like I don't
know if you guys were on Twitter when
this happened but it was really
entertaining to watch Twitter while this
was happening but they did this idea of
like you know an industry we need to be
better about this just running Hill nine
just like literally like shooting a note
in the head in a test environment or
like in production can like lead to
really disastrous results and you know
it's worth noting that like what they
did is like the Redis offer came back
and said after this outage that he had
not expected the configuration that they
were using and that's why this failure
happened this is sort of harkens back to
especially in this open source world if
we're going to pick up a dependency and
use it then it's sort of on us to vet
that dependency right if especially if
it's a critical piece of infrastructure
because you know especially with like
these crazy config databases and things
like that and like the tributed queues
and all that kind of stuff if you're
gonna stray from the normal like you're
you may have a problem because that path
may not have been totally thought out or
you may be doing something that the
authors never thought you were gonna do
and therefore was not well tested okay
we're going to take another quick little
rant assessed over AIDS because I had
some thoughts about testing in
production that was ran in production
that test case it needs to be ran in
production to prove that that failure
happened so I think there's a lot of
value in testing with production data
I'm a big believer of that I tend to use
in a lot of my systems have ways to fork
in production data to test things and
test in staging and we're going to talk
about some actual testing and production
code but you're impacting your users at
this point and
that's problematic because your users
are having a bad time when your system
fails in production so it's this risk
versus reward thing if it's not a super
critical system maybe it's fine to test
in production but if this is like a
critical backbone piece of your
infrastructure or like core component of
your service the risk is pretty high to
just sort of like you know send some
antastic code out there and be like it's
fine or not and then roll back so I just
I just wanted to say that I think you
know there's a lot of like hype around
testing and production but that doesn't
mean like testing in production should
be the final step we should have a lot
of confidence and have implemented some
of these other things in staging
environments or in unit tests and
integration tests to prove that we think
our system works I also have to say that
like monitoring is not testing people
say this to me lacks I ran the
observability team at Twitter and I'm
like monitoring's not testing monitoring
is super critical to your system and
understanding what's happening in your
system and recovering from failure in
your system it's not testing all its
gonna do is like tell you that if you
have a graph that happens to show a
failure that like maybe you'll see the
failure if you have an arching the graph
or maybe it'll get a page right but this
is a reactive approach this is not
verifying that your system is correct
it's just sort of telling you what's
happening so have monitoring but like do
more things than just monitoring to test
your system finally another way to
verify in production I call it
verification right because it's like at
this point it's already impacting your
users is it really verification is that
you know and I think this is actually a
really good process I'm not like trying
to say don't canary but do other things
first this is sometimes called like
red-green testing or whatever but the
idea here is that you gradually
introduce new code into production and
this greatly reduces the risk of doing
deploys and is actually super powerful
from like an Operations perspective
though Canaries have a lot of
limitations so Canaries are only gonna
tell you that the Golden Path typically
is working right that you have it like
broken user functionality that they're
using all the time because Canaries can
only tell you that they perform as well
as assist the old version at this exact
moment in time and so unless at this
moment in time there is a network
partition happening or you know a data
center is down or some other failure is
happening you don't actually know if
like the canary is just as good as the
old version it just it verifies the
golden path and that's all
gives you and so I think super powerful
tool use it but understand the
limitations people were like joking
they're like why isn't there a canary
bird this is the canary island butterfly
for people who are using my slice so I
tried okay so we just went through a lot
of things about verification in the wild
you know like obviously I'm very
realistic about the fact that there are
deadlines and that we only have so many
resources and so like maybe we can't all
build a simian army or don't have the
time to like Jepson test every system we
build but we do have the time to write
unit integration test that should be
like base level engineering effort and
like that paper that we went through
shows that they're incredibly incredibly
valuable property base testing I think
is also a fairly low investment and high
reward so I would HIGHLY encourage if
you haven't used it try it out on some
of your projects both Injection also can
be very low or sorry low investment and
high reward as we saw with the kill9
nodes
maybe do it in staging first and then
and then write Canaries are a nice
process to sort of like reduce the risk
of deploy and then prove that this new
code hasn't totally broken user
functionality as we roll it out and and
realistically I like Canaries for
testing that I didn't screw up my
configuration somehow that's at the
level of what I'm actually testing at
that point okay so let's take a quick
moment to go through some research i'm
not going to go through all of these
today these are just things that i think
are particularly interesting that are
currently happening within the last year
or so
and i have links to all of them in the
references section if you want to
explore further but we're gonna go
through i'm one that i think is really
cool and also ties into industry because
it's called lineage driven salt
injection so this is a paper by peter
alvaro from UC berkeley he's now a
professor at UC santa cruz and so the
idea behind this paper is and this this
tool that he builds to go along with it
called molly is that a lineage driven
fault injector is going to explore the
state like try and explore the state
space of like all inputs and failures
that could happen but it's only going to
do that for the ones that actually
matter so it starts with a successful
result of like you know I stored
something and it's durably stored in my
database right and then it's going to go
look at like all the the
the graph the Cole graph to figure out
what happened and start like axing like
requests or things that happened along
that path and start injecting failures
only along that path so then we only
test the things that actually could
affect the system and this gives us
provably correct so that's nice right
and you can run through the state space
of failures in a you know a smaller
amount of time than some model checkers
can so this is an example from the paper
he uses it to replicate a bug that had
been in Kafka a few years back that I
think Jepsen actually found but there
was an issue where like how do if a
network partition happened this this
node on the far upper cut a becomes the
primary and it's also the sole member of
the cluster because you know B and C
can't talk to the zookeeper that is you
know determining membership a then
acknowledged your right from the client
I'm said like you know I've durably
persisted it but then it crashed and so
that right is lost because it wasn't
able to replicate it to anyone else in
the cluster so like right like this
paper didn't find this book but is
proving like this is how we would find
it and this bug is fixed now right like
this is like an old old bug but it was
an important one to sort of show because
like this is a very what's happening
here is very complex there's multiple
systems involved and multiple sort of
like protocols that are doing like
reliable broadcast so this is kind of
cool because then like when you run
something through Molly it'll give you
the exact use case where something
failed and this is much easier to
understand like we can all look at this
and be like oh like a shouldn't have
acknowledged that right right like
that's clearly the problem but it's much
more easy to reason about the failure I
think in this model I think Molly and
Laneige urban fault injection is super
cool because Peter actually worked with
Netflix and implemented this in their
sort of fault injection model so Peter
and Colton Andres from networks Netflix
partner to like make a prototype of this
and there's a lot of interesting things
that happen when you try to take like a
research project and then like put it
into production and there's a really
amazing talk that's linked in the
references that they give about it and
there's an article as well but some of
the key findings were like right this is
like the Netflix Deathstar o
micro-services diagram that I borrowed
from Adrian and and so like right like
maybe we don't have a whole
but what's happening this is Seminole
this is they're talking to each other
because like that's not discovered a
priori that's defined via code and so
they use distributed tracing and they
have a tool already called fit which is
their fault injection system to inject
failures and they so they know opponents
where they can inject failures into the
systems they use that to construct the
call graph so they're doing it sort of
like live right they use the metric
system to determine if the call is a
success or a failure
because they have all these like an HTTP
200 wasn't enough because you have all
this sort of like weird certain clients
you know behave badly if you send them a
500 back and they have to support a
range of clients and like whatever the
world of like the internet is terrible
so fine we do it and we use metric
systems to determine if the calls a
success and then like Mali would sort of
like process this call graph and stuff
like that and then determine where fit
should inject a failure and then they
could figure out like how'd it happened
and there's a couple other interesting
things that they had to go problems a to
go solve and so I highly recommend like
reading talk but it's really cool
because they ran and they found some
bugs and that's a that's that's a good
use of like how do we start thinking
about integrating some of these things
into production environments ok so in
conclusion use formal verifications to
test your critical components if you
have something that is super
mission-critical that maybe you were
selling and making money off of it's not
a bad idea to write a formal
specification and invest in that
software
I think unit tests and integration tests
should be like they find a multitude of
their errors they should be bare minimum
for any software that you're writing and
then write we can increase our
confidence using property testament and
fault injection in our systems and I
think these are highly underutilized in
this last bullet point is sort of where
we can see a lot of gains for fairly
minimal investment and then if you know
you're a giant company you can go in and
build these sort of like really large
tools and finally I'd like to end with a
quote from my friend Camille enjoy the
ride have fun and test your freaking
code thank you to all of these lovely
people who helped me with this talk and
the article
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>