<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Stream All Things - Patterns of Modern Data Integration • Gwen Shapira | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Stream All Things - Patterns of Modern Data Integration • Gwen Shapira - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Stream All Things - Patterns of Modern Data Integration • Gwen Shapira</b></h2><h5 class="post__date">2017-06-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Hjae0Cw9oew" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so I'm going to talk about oh
thanks I'm going to talk about three
more things because it's how everything
in the world is streams and also how
every problem could be solved with
strings or almost a lot of them could
and about modern ways of parsing data
you'll see it would be good and I want
to start by reducing my co-presenter a
rainbow courtesy of a Holden who notice
that I'm speaking without a stuffed
animal which is apparently no longer
allowed and so I'm very grateful for
that i'm gwon Shapira I'm working for
confluent which is a company that well
I'm not longer allowed to say that it's
their company behind the project after
because it's all a community but we do
have a suspiciously large number of
patrica for commuters working for us
working on Apache Kafka and contributing
to the project I do a lot of stuff so I
wrote a nice book on Kafka so is Kafka
is your thing you should totally read it
getting reviewed by Jesse who's here
yeah earlier and I'm ports management
manager right now which is weird because
the team didn't no longer accepts my
pull requests or like I don't know you
do the PowerPoint will write a call and
I used to write code I used to be a real
engineer and I used to do a lot a lot of
consultants like if you ask me what to
do one thing you did most in your career
is go around to taco companies help them
will do stuff with your data and which
means that I've seen how Casper
how data management evolved over the
years and I can kind of I'm going to
talk a lot to that and then I tweet way
more than I should I even got a book
from go to Chicago for tweeting so much
render and I will do what while I'm not
supposed to code I still do and a lot of
said stuff I'll talk about here I wrote
examples for and you can go to my github
account and kind of look up Kafka
streams examples
you'll see a bunch of them so what
happened to data management like that
kind of over the course of my career
which started somewhere around year 2000
psi mold and until now so we started out
I started as a DBA and I started as a
data warehouse DBA and doesn't work DBA
was kind of a difficult job and the
biggest the most difficult part was
getting the data model right if you got
it right everything was supposed to be
perfect but nobody ever got it right
ingest was a real pain there was those
huge batch jobs loading data as they
would break every single night and we
always had to scramble to still get the
business reporting time even though
those huge batch jobs kept failing we'd
had a lot of techniques about optimizing
those nightly load jobs that was like
99% of the work and then because
modeling the data and loading that it
was so hard we never caught up with new
applications so some data was always
missing and in every single time someone
wanted a report and he couldn't really
do everything he wanted to do a limited
throughput was a pretty big issue like
you could only load that much data in a
towers window you had every night and
once that you want to tune the hell out
of that it was a like we didn't even
know what to do and obviously nobody
even thought that you could do it not in
large batches and then fast forward 2009
2010 we had Hadoop's coming along like a
brand new way to do it so I don't feel
bad batch only but stuff was a lot
better now we could keep the whole data
modeling which was totally a pain in the
ass and skipped most of the interest
pains in just to Hadoop is easy you can
just dump all the data in there and
someone will figure it out which is a
DBA was a huge improvement in life
quality but then I was a Hadoop admin I
had people set up their results and you
immediately run into whole new host of
issues which is that nobody can actually
use the data and I now demoed that
modeling is without understanding how to
use it and what to do with it in writing
those fantastic mark produced and later
Spock jobs was left for every single
engineer in the company and it turns out
that they weren't any better than I was
the DBA it is still just really
difficult problem and it was still
missing and also lost 10 people just
couldn't find the data like they knew
the data is somewhere in Hadoop but how
do hell do we know what it is worthy is
what to do with it it is just easy the
pains were still there they just moved
from one thing to another as often
happens and now I'm looking at where the
companies that I'm talking to now and
the rather lake is still there people
definitely still do this kind of
processing but because of its still
patchy nature people are also moving a
lot of the data processing to something
that's more real-time and obviously with
much lower latency but still very
scalable like the hooks used to be and
the thing that scary there is at least
two completely different models like
streaming and real-time and doing all
the data processing a life is just
pretty neon pretty scary for everyone I
met someone it's a very large online
cloud service that's pretty much
probably everyone in the room is using
or has use at some point and he's like
I'm using Google data flow and I'm like
fantastic how is that working for you
and he's like well but I'm using it in
batch mode I'm like huh okay why are you
doing that
like what because I'm scared of going to
extremes and I'm like you know I can't
give you I mean it is scary like how to
debug it like it's Rito it always goes
on you don't have the 16 hours of day of
looking at stuff and fixing things it's
really scary so I kind of get the
holiest really scary part and we are
kind of still fitting our ways around it
but it is good to see that companies are
doing streams not everyone is totally
afraid of it so I'm basically going to
talk about the patterns like that we
discovered so it's not like silver
bullets this is how we do things
everything works that way like I've
talked to all those companies I've seen
what people are doing here the patterns
that emerge that seems to be things that
people find useful and didn't burn
anything yet and one of the things I
noticed throughout this whole transition
is that some of the not just the way we
do things very changed but who does
things changed
so as you remember I said my career as a
DBA but we had the ETL engineers using
form attica's and data stage
and then at some point totality and
pentomino Lo and we all did
drag-and-drop and build the whole ETL
pipeline and that sort of continued into
the data lake and in people like you
know informatica for the data lake
fingers I guess and but then sometimes
we disappeared and now I pretty much a
human someone is building a modern pipe
data pipeline that user stream I don't
see ETL engineers anywhere anymore and
at first I thought it's just because the
job went away right like if you dump all
the data into Hadoop what the hell do
you need an ETL engineer for which is
absolutely correct but then I looked
around to notice something even scarier
it's all engineers now everyone is gone
I can't find all the people I used to
work with when I started my career I had
those QA people and people doing data
modeling and the season means and the
network admins in the storage admins
I used to hate the guts of my storage
admin I think he's working for Amazon
now because those are the only people
who still higher storage admins in the
world so pretty much everything changed
so I'm not surprised that software
engineers are doing those data pipelines
and data processing because they're
doing everything in the organization so
when you have a software engineer doing
the job it's not just that it's a
software engineer who is doing the same
things that the ETL engineer did earlier
like the whole thing changes like the
whole perspective it's a different
person and everything is different so if
you're a software engineer you are not
into let's get data from here to there
you are into I'm building an application
I usually have a business goal in mind
like that's the thing I need to do and
you're also not really into drag and
dropping like you are an engineer right
I know how to write Java or Python or
SQL or whatever I have a debugger like
what the is those pointing clicks
like so so a lot in their space just
completely changed and those patterns
that emerge that we will look going to
look at it is first of all the fact that
we're now streaming everything right
like that's the whole transition that we
are doing everything in streams and in
real time it's about how do we keep
compatibility that's a program that kind
of kept popping up starting from the
keynote yesterday like marching forward
and mentioned that compatibility is a
big issue when you were
building applications that move data
around and it's still a big deal I'm
going to talk about how to ask some for
some thing is just ridiculous simple and
now we can do it at huge scale I'm going
to talk a bit about that and then also
how to solve the really interesting
problem of doing streaming data
enrichment and everything is going to go
through an example of it's kind of a
merge of two customers I worked within
the last few years and they're both
large hotel chains each doing its thing
slightly differently but a lot of things
are in common because it's the same
problem domain and then I don't know
like hiatus here in Chicago but those
are actually competitors or not the same
thing but I hope it will look familiar
so let's start with something that when
I wrote a slide I thought it's a bold
claim that all the data is advanced
dreams I didn't expect that the starting
from the keynote and every single
presentation afterwards will also repeat
every single event stream and by
everything I mean that okay your dog
hotel chain and you have the website
right and people look at the pages and
you want to know what people are looking
at definitely events right so you can
capture those events and people look at
specific rooms those are definitely
events and people have emailed and they
click on links to see your promotions
those are events searches on the website
are events someone booked a room it's an
event
when you get to suddenly the loyal
customer and it changes is their email
address on your database an event if the
hotel is under maintenance it's an event
if you get to the hotel and check-in
it's an event if you go to your room and
open the door you know how you do it I
bet that's an event to if you are going
to the minibar and take out a beer it
has to be an event otherwise how in the
world will they bill you so how much
internet do you use in your room it's an
event
it's a bunch of events they have to
measure it otherwise it can't really do
any kind of capacity planning etc
everything is measured everything is
tracked that's the kind of the kind of
world we're in today I bet that they
also collect information from your
phones through the Wi-Fi somehow but I
don't want to begin to that so I hope I
convinced you everything everything
that's happened in your database is an
event because you can do a change data
capture on the database everything with
the sensor everything in the log
everything an application all events
and we look at one of them we start by
looking at one of them which is the page
movement pretty much every every company
in the world has a website everyone
carries about what do people do everyone
cares about pageviews and if you look
inside the page reasons you kind of get
something that's pretty similar like you
have like a session identifier usually
from the cookies to kind of track how
the person moved around you have a time
stamp because everything in the world
should have a time step you have kind of
water viewing right now he's viewing the
property itself reviewing property we
also have the property ID so we know
what he's looking at and because he's
logged in to our website we also has his
loyalty ID number so we actually have a
lot more information we we offer you a
rewards and things so we will get this
all this extra information associated
that's super important if you're a hotel
and then we know that he's on a website
because he clicked on a link from a
promotion that we sent in by email as
well in terms of those meta data what do
we do with those events well a lot of
services are interested in them right
that's the event driven micro services
architecture that's what it's all about
you have a lot of people interest
interested in those events you want to
have a picture on the website that says
people who looked at this room also
looked on those five other slightly more
expensive rooms maybe you're interested
in those or twenty other people are also
looking in this room please book it fast
because who knows how long it will last
right you want to do a/b tests you want
to know that people who search for these
ended up booking the other thing you
want to know how relevant your search
results are from if your email promotion
is actually working affiliate programs
these days you know how someone may blog
about the fantastic vacation I don't
have a bunch of links to the place I
stated if you click on the link the
blogger and you actually book the
blogger will get paid that's very
important to track page your events just
for that kind of she said never believe
anything written blog so that's
definitely not mine but yeah that's a
guy it always scares me how much of the
ongoing gay lifestyle blogging ends up
being marketing so we have the all those
events we have all those services who
wants to get these events and if you
kind of lively every
time you have someone generating data
and you have someone who wanted that I
have them talk to each other kind of
HTTP request/response kind of thing that
gets really messy really hard to
operationalize I usually dig a lot into
it but like I feel like this entire
event was all about how hard it is to
debug troubleshoot operationalize a
bunch of events who talk to each other
with the ephemeral messages that nobody
can really track so pattern number one
the way I really like doing it is
basically publish everything into Kafka
and Kafka was written originally as kind
of a published subscribe message bus on
steroids just open source and meant to
scale really really really well which is
why companies like LinkedIn can actually
afford to publish everything into kapha
because it is the only thing that's
capable of scaling and they tried active
in queue earlier and that's been turned
up very well and then you have all those
subscribers who can read from Gotha
the other cool property of course is
that it scales really well with a high
number of subscribers that's another
thing that MQ tend to not to do that
amazingly and it also kills with
different type of subscribers so we have
bachelors drivers who just read stuff
every hour every day
Kafka will actually handle them pretty
well as opposed to message queues that
has to keep stating memories for all
those subscribers that's why we can get
a tattoo herd open to the data warehouse
as well as to stuff that's more
real-time so that's kind of the idea if
we look slightly inside Kafka we can see
why it's a good idea so the basic unit
of information in Kafka is the logger
and if you haven't read my co-wrote a
really nice book called I love logs and
before that there was a really nice
O'Reilly blog post everything a software
developer has to know about log and he
talks about logs is an obstruction
because really logs is at the basis of
everything right it's not just
application logs where you write how
this happened this happened or exception
it's also if you are a database every
database in the world every no sequel
sequel whatever they have the writer
headlock right to keep providing
everything that happens every single
update insert delete commit rollback
everything
goes into the slope everything in the
world is a log of events so kapha is a
very good abstraction that allows you to
model pretty much everything that's
going on in your organization so the way
this log works is that we have this
serious sequence of events and you keep
writing and every time we write it comes
to the end and Kafka's guarantees that
it preserves the order of writing across
this one log and then you have readers
and the way it scans at each reader
basically remember that I am now at
message five which means it's a red
message zero to four and now in message
six and now I'm message seven so Kafka
doesn't have to keep track of those
messages work knowledge but those
messages weren't which is most of the
load on traditional MQ thingy so this is
pretty useful it's fast it scales it's
easy to understand which is important as
someone who debug Kafka easy to
understand is important I like simple
and but obviously one log can only fit
on one disk
maybe if we try to keep the whole
sequential thing and we actually wanted
to scale so the way up category test
topics and topics have partitions and
each partition is a log so you scale by
saying okay all the major events go to
this one topic and this one topic has or
a 100000 whatever partitions and then
each we have maybe 10 workers each
broker gets ten partitions and that's
how things are kind of load balanced and
distributed and so on order is still
maintained in each partition so you kind
of have to make sure that all the
vendors are relevant together for
example all the page views from one
session all the page rules from one user
they go to the same log so you can
process them in this with the same
reader and the way we do it is basically
we have keys and we use them as
partition keys so that's pretty
straightforward like the way most
systems would work and so back to our
page view in all those consumers and we
now know that we have capture page we
events going to Kafka all those things
our consumers reading data from cusp and
processing it and this is kind of the
white usually looks you have those
events going into Gaza and then you have
all those services so maybe someone is
getting the events into
nice he does create table we have the
timestamp it's a number fantastic you
can see the timestamp is indeed along
number of seconds 1970 the usual you
have a searcher something it's a it
needs to pass a date no worries new day
to give it a timestamp everything is
fantastic that's how it works
and then one day a developer says that a
number of seconds since a 1970s horrible
to debug it's unreadable
we need an easier format let's go with
something more readable and he makes
this change and immediately everything
across the organization breaks that's
the way life is yes it's a really sad I
know so and that's it you can think I'm
making this specific example with the
timestamp format up but I've actually
seen it in one of my bigger customers
how this small change blows up
everything and they ended up having to
reprocess something like four terabytes
of data to overcome the progress I've
then I think that ok my customer is
uniquely crazy but I've been a strata
talk buy over and they talked about the
exact same problem so it's not just my
customer who is crazy this actually
happens to people who are supposed to be
really good engineers so I guess if it
happened to you don't feel ashamed it's
a thing but we still need to prevent it
from happening so which brings us to
partner number two in fact that we need
to somehow maintain compatibility right
you write an event you need to make sure
that people are capable of actually
reading it after you wrote it and that's
something I keep feel very strongly
about because all this pair conference
I've heard people talk about a
compatibility and they talk about
compatibility in terms of services
talking to each other but if we take
accepted premises services talking to
each other via REST API is actually not
that good of a way to scale and not that
good of a way to process data and then
compatibility at that level is not what
it should be worried about you should
worry about it but that's not the big
thing the big thing is that if you're
doing event processing or stream
processing your the messages in the log
are the schema right that this is the
largest Keima and they are them
API between the different services right
that's the contract you produce an event
it has a certain schema - it has a
certain type you everyone has to read it
the contract is what the event contains
so it's super super important to me this
is your API this is the thing you need
to control compatibility for and it's
even worse than just those api's
messages because those events can stick
around for very long so if you decide to
save data for a month in Kafka or three
months or three years which has been
known to happen
you are now committed to allowing people
to read data with the schema that you've
used three years ago so that's pretty
crazy and we need to make to at least
know when we did bad things and one of
the important things about compatibility
is people tend to miss is that you need
to do it as early as possible like a lot
of people talk about keeping
compatibility in production and having
safe guards around production and not
allowing stuff to the breaks to write
events in production which is great but
if you're an even if you deploy to
production three times a day it's
probably fine but I'm not all
organisations deploy data 3 times a day
some of them deploy data once a month
for them like it changes so you really
want to hopefully be able to catch
schema changes when someone makes the
first commit to his branch or if it's
not on the first commit to its branch
maybe when you do emerge and if it's not
only to emerge well you do have nightly
test running via something like Jenkins
right so that would be a good time to
verify the schemas are still compatible
and if you don't even do that maybe you
have a staging environment that would
also be a really nice time switch get
real comfortable one of those don't wait
until production because that kind of
like last resort that kind of things
like if you have to roll back a change
from production that's kind of ugly and
so you kind of want to integrate with
all those wonderful development tools we
all have you know get plugins and maven
plugins and Jenkin plugins a lot of
things make sure you care compatibility
there so Coughlin's has an open source
schema registry and we have maven
plugins and we also have the things that
keep it safe in production which can
also be used in tests and staging etc I
usually don't
actually running this kind of Kimmel's
and computability checks on the
development environment itself because
usually when you just develop in your
own environment you want to be able to
iterate and test a bunch of things like
a non worrying about it at the stage of
after the first commit or nightly test
seems like a reasonable compromise for
me anyway you don't have to use control
and schema registry but do something
that keep things compatible that that's
the key point here third pattern and
having ridiculously simple sometimes you
just have to want to do really simple
changes to a messages so for example you
have that you happen to cry to have the
credit card transaction this is
completely a kind of a made-up right you
wouldn't really have in a pager even a
credit card number but suppose you did
then you wanted to mask it just imagine
this is not a page view and there is a
bad this is pretty simple right you read
an event you change one field you write
an event that back kind of thing and
this is really simple
so the could be a bunch of ways to do it
and the easiest is basically to run a
kafka consumer and it just you know
cover consumer is basically an api is
that you do subscribe to a topic paul
you get an array of events you in a way
to make the changes and now you need to
also do a produce because or do
something with the data that you just
got to make sure that the credit card is
gone but it's very straightforward
and the thing that makes it taste very
very ridiculously scalable is that you
can basically write one cup of consumer
and if you just apply one of them it and
say read data from the page view topic
it will just read from all the
partitions in the topic but what if you
have a thousand partitions in the middle
of this event per second and this one
poor consumer can't handle it all well
you just start another one and then
magically and by magically a means
through a complex Kafka protocol and
we'll talk to each other and figure out
that hey I'm doing those three
partitions you're doing the hazard and
997 petition or they try to do it 50/50
and you say Oh still doesn't scare you
start another one okay I'm doing this
partition you're doing the other
partition and this goes on until you
have one consumer / partition you can't
have two consumers of partitions because
then you can't you don't go into order
and any of that but and so that's why I
kind of like heavy lot of partitions but
until you exhausted that part then it's
a it scales really really well and as
you can see you can do consume from a
topic do stuff producer topic it scales
really well this would be the hipster
stream processing pattern and the reason
we call it a hipster stream processing
is that over in San Francisco land where
we don't drink Bud Light with lime we we
have those and hipster people and they
ride em 60s which are bike not just
without gears but without a freewheel
you just have to keep pedaling they have
no brakes it's really really really
weird and it is and then you think that
they love about those buses are super
simple which is true and that's a thing
that I like about hipster stream
processing is that we just have a
consumer and a producer very small app
it scales well super simple I love
simplicity and this works great until
you hit the first big hill in some of
these go you hit the first big hill
really really soon and that's where
simplicity that no longer works right
like you don't want a fixie on a large
hill if no matter if you goes up or down
you're there and what you and that spree
might so what are the hills when stream
processing the hills are things of the
stateful things of the stateful would be
either joins which are kind of hard they
would be a aggregates like window
aggregate you need to maintain a state
if you crash you still need to somehow
restore the state and figure out what to
do with it that's really hard and the
other thing that's kind of hard and
people don't think about it is a dynamic
configuration is really hard in
distributed system because you have all
those consumers and now you want to make
change config somehow and you need them
to all basically start together isn't
unless you specially built for it
half-and-half is really difficult so
the constriction is another hard problem
so carefully so this you both hips are
simple thing skills really well for
simple problems not so much for harder
problems and then there is also like
what do we do if there is a data store
involved like we actually show later why
but you happen to need to get some data
from a database or to get data to
elastic can we do it in a simpler and
the answer is that there is cover
connected exists to make it even simpler
and the reason it's awesome and so
simple is that you don't have to write
anything basically there's just like we
have like 50 plus connectors already out
there so of course the easiest to write
software the fastest with the least bugs
is the one you never had to write
yourself and so that's kind that's not a
bad option at all like there's a lot of
data system out there but I'm betting
with something like 80% certainty that
you have MySQL or Postgres or Oracle or
Cassandra or MongoDB or elastic like one
of those six you probably have somewhere
around so it's like the 8020 rule and we
have 50 connectors so some one of those
you'll probably have around and how does
it work why do I claim that it's
massively scales well it kind of runs on
its own a no worker nodes which can be
docker containers or whatever and it has
a website it's only sold a bunch of
those nodes and you have a REST API and
you can say hey please get me data from
a log and from an MQTT this kind of an
IOT thing service and it goes to the
respective data stores and say hey how
it doesn't say give me the data yet
because I'm good in scale right I only
have one of each it says how much does
what do you have for me
how many indexes how many logs how many
files how much data and then it looks at
the data and says who are so I need to
have that many tasks and you kind of
configure how many tasks you can have
maximum but idea that you will have like
a task per CPU so each one is one thread
single thread and now it's all paralyzed
and you don't have to do any of it but
it's fully power power light and another
nice benefit because of our magic
coordination protocol if that one dies
the work will the task will magically
move its way to an available worker so
that's very convenient and by the way I
mean what 90% of the time work on the
presentation was animation is I hate
PowerPoint a tough crazy because the
streams is super easy PowerPoint is hard
so that's kind of cool but ok so now we
have data from databases we have those
simple processing we got rid of credit
card numbers
now our tailor is a new business case we
want to do a promotion we have platinum
members and we have a new bridge site
property in a way that we want to
promote and we decide that if Sutton OMA
a member looks at any kind of bridge
property anywhere in the world we will
send him an email about the awesome
deals you can get in a why because we
want him to book hotel in a while ok
fair enough so that's pretty normal a
lot of hotels do it all the time and
this means that we take a page of events
that have session and timestamp and
loyalty at the in property but what we
really need to know is things like in
this platinum is it a bridge property
like we need an enriched event we need
more our service that has to decide has
to get more information so our
architecture excuse my drawing ends up
being a bit like that you have those
events going to custom you have your
steel super simple consumers that get
them but then you need to enrich the
event and then you need to pass it on to
decide whether or not it's a promotion
suppose that you also write your
decision to elastic so the other
scientists can do a/b testing and make
decisions and then you have a producer
sending an email to the notification
sending story a message in an event to a
notification service saying please
notify the user and set an email etc so
you can't have this pass through singing
and now the question don't question is
how do I write the enrichment service
how do I get I have this event and
ending an event with more data so our
first attempt is to say well we have a
database with all the information let's
go and get information from the database
or if you are really really nice you say
oh we have a service that provides us
API that give us information let's go
and talk to the service and have you
talk to the database and give us all the
information which is slightly better but
not that much because we still have a
pretty big issue and the biggest issue
is latency that just takes time the
second biggest issue is throughput like
just because Kafka scales to gazillion
events per second doesn't mean that
every servicing organization is going to
scale the same thing so you have a huge
firehose trying to go into a pipe that
big especially important if you go
directly to a database databases can
scale 10,000 events per second like
without killing production accidentally
like your DBA will kill us we try to do
a hundred thousands or millions in per
second
Kappa can do a lot more so it has to be
very careful not to DDoS your own
database extending of course if you are
using Amazon something just go to town
you pay for all of the events they will
of you and the other problem is
availability like if you just talk to
one service no big deal one of my
customers needs to tune reaching each
event he needs to talk to eleven
different services and I heard that's
not even the worst thing he has to do so
so if you have eleven different services
one of them will have an issue at any
given time like 90% probability one of
them with having issue what do we do now
I can have an event I have to enrich it
like I okay so I have to skip through
trying and I have to manage all the
retries it's a pain we can do better
than that guys you can do a lot better
than that enter the streams table join
one side we have a table with the state
of all our members and what is their
account status and on the other side we
have a stream of events with ideas that
we need you somehow need to join both of
them and what the way we want it to end
up is that we have a local copy of
database that's a pattern that everyone
in this conference kept talking about
right you have a local with only
in-memory cache of the data that your
application is and the way we created
meeting obviously caches has the problem
of getting stale and we don't want it to
get stale we want it to stay alive and
active so we add another consumer his
entire job is to get the
about all those updates to the table
from the firm Kafka and keep on
maintaining this a cash fair enough but
how does those events from the database
even and I think after in the first
place
let's call change another capture you
take stuff out of the database and
basically that you read a commit log
update insert update insert delete
delete and then you turn it into a log
of events basically turn a state into a
log that that's a super super important
pattern a castle connect has a bunch of
connectors specifically for that or if
you're into Oracle you have Golden Gate
you have eternity there's a lot of ways
to do change data capture super
well-known patterns from the ETL days so
we use all those things to take a state
in the database into a stream of events
in Kafka and then we turned back into a
state in the application so we kind of
do this back and forth pretty much what
it looks like we take the state turn to
a stream back into a state of course now
that you have a stream of events you can
also do a lot of stuff you couldn't do
before right because now you have the
entire history of your database at your
fingertips you can see how many people
book properties and then immediately
come to them you couldn't before because
database only has a specific State State
at any given time so you can do a lot
cool stuff with that am I completely
running out of time
I'm good cool ok and ok so this is one
of the things that is a it kind of
sounds good when I'm waving my hands all
over but it turns out - doing a stream
to table join in a reality is not always
trivial that just you have to maintain
this table in one thread and you have to
have those things they coordinate and
then you have this clash if you fail
over you have to recreate it from the
stream and you if you want to be fast
about it you need to somehow maintain
snapshots which is kind of where its own
kind of challenge so there's a lot of
stuff that you have to worry about if
you implement it like it's always like
it's all fun and games until you have to
think about what happens if something
crashes and then you find out that it's
actually an hour down time until we
manage to recreate the state from this
stream of events and nobody wants that
so oh we actually need an in-memory
database oh that mean number does this
has to persist so we will rock the beat
to persisted and then we have to say
okay but if these clashes and starts on
another machine what do we do like there
is a way to make it make it stateful in
kubernetes which the way anyone knows if
there's a way to have straight full
containers in kubernetes because yes
I'll find out before next time I'm doing
this presentation and so Kafka streams
tries to make it easier by hiding a lot
of the oh we have a local and it ought
to be instance and we take snapshots and
we try to make recovery faster we just
give you a very nice API and we verily
it's called K table we're saying this is
a stream that represents a table and
then we say okay do a left join and the
example is on our website and you can
kind of take a look and there is a blog
that just explains how it works in
actual delicious or excruciating detail
depending on how much you like little oh
okay
so I have a bonus content on how to join
streams to each other which is kind of
cool
and but I can also take questions in the
remaining the ten minutes or so and
anyone has preferences like if you were
burning with questions I'll go to
questions and if you're not then I can
just keep on talking anyone who really
really ok question sorry oh you want to
go on scones okay like anyone has
burning questions that he feels will you
not sleep if I don't answer okay we're
good bonus content so we have another up
we want to decide on search relevance
the way search works is that the user
searches for hotels in Philadelphia and
what we really want him to have most of
the people just click on the first
result and book it and then maybe few
more on the second result and the rest
kind of in the first page we don't
really want anyone to keep looking
because that means that our search
absolutely sucks in reality well how do
we even know right like we need to
somehow measure what do people click on
when they search so now
have two events we have pager events
which are the usual pageviews and we
have search events which is what people
search for session timestamp user IDs we
have it and the search term so now we
know what people searched on the search
term and we know how often they search
for and we know what people viewed but
we need to somehow put it together right
like we have those two stories of events
that we need to get together we really
want to say that this search cause this
click right so it's a kind of we're
trying to infer a casual relationship
and obviously it's just in first but a
good infers would be that if the click
comes within let's say 20 30 seconds of
the search by the same user then it's a
match and yes he searched and then
clicked if the click comes half an hour
later it's publicly he searched through
Google and somehow found something in
another way maybe Expedia or something
so we have this kind of inference which
means that we have to do a join on two
things right the first is the session ID
to make sure that we're actually looking
at the same user and the second is time
we need to do a joint based on time
right we have this ten-second window in
which stuff still counts I say I will
study optimistic this is a five minute
window but we basically what we do is
now we have two caches one cache looks
at five minute window of one stream the
other cash keeps five minute window of
the other stream and we basically keep
updating those 2-way caches and trying
to join them so we have kind of a hash
table with events from both that we keep
matching and also keep aging stuff out
which is a very nice example of you know
the kind of how the kind of data
structure that you really hate in job
interviews end up being really really
useful in the real world like if someone
asks you to maintain a hash table that
allows you to do hot joints and also
allows you to expire eventually get
older you'd probably say those crazy
interviewers nobody would ever need it
in real life and here we are and so and
as time goes by you keep just
maintaining those hash windows and keep
joining and keep omitting results
join fairly is it the only pro question
is that time is obviously really really
critical here if everyone been in the
beam I'm sure you've heard it before but
time is really really critical and
there's a lot of ideas of four times
it's that existing streaming system it
can be the time that the user actually
clicks something is browser like the
user really did something or the web app
got go to the click and loaded the page
and created an event or the producer in
the window web app created an event to
send into Kafka that's slightly later
than when the web app first created even
probably when the producer actually did
send we may be doing some batching and
then the tenth event to really go to
Kafka which can be a long time later who
knows what network partition will exist
and then it can be the time that the
stream processing app is reading the
event which can be months over years
later for that matter because who knows
when we decided to add this kind of
application so when it could be a bit
careful about it the best time is
obviously the one that your application
put in in lieu of that Kafka producers
always put in a timestamp just in case
and if you have an all producers the
doesn't value it
Kafka itself will put in the timestamp
also just in case so you have a lot of
places just don't do the whole if now
it's 9:30 in my web application my
streams application just bought the
event then the event is at 9:30 this is
pretty much never ever correct don't
wait and then if we do two network
partitions you've gotten late events our
system can is actually capable of
joining to an earlier hash table as long
as we actually keep them in memory so
you can even say something like please
keep all the hash tables live in memory
especially if they're small enough for
maybe half an hour because some of our
applications and app reporting stuff
would be later so that's that's a nice
thing that you can do right you can just
basically take an event say oh it's all
let's join it with an older a hash table
it's all super complicated anything is
actually very challenging to implement
ask me how I know and so you need those
two buffers and then again maintaining
state failover all those things makes it
like fun
again Kafka streams it makes it easy by
the way I'm saying the traffic to it
makes it easy
I know Kafka streams really well and I
wrote all my examples in kappa stream
I'm not saying that oh it's really hard
and fling or oh it's really hard in
sparkle oh it's really hard in BIM
because I wouldn't know I don't know
things I don't know this I know spots
for next three years ago we definitely
didn't have it then but who knows what
they have now nope I guess nope
a cynic may have it that's slightly more
stream for and maybe Google dataflow
probably has it because they keep
talking about it so I'm guessing they
have it so this is an example don't take
it as we have it and they don't kind of
think we we love all stream processing
systems that's kind of the thing with
Casca right because we work with
everyone and then we can you can see an
example of how I do it in my a github by
the way a Kafka streams is still at the
point where I pee I sometimes change and
so at the time you look at the examples
with some chance it wouldn't actually
work pull requests are really really
welcome it's just small API changes okay
going back three more things into Kappa
keep compatibility if you listen to
nothing else from this presentation
please keep compatibility nothing else
will screw you over as hard as breaking
compatibility in events and use whatever
you can to make simple stuff simple and
then if you need to do complex tasks
then there is this business enrichment
patterns please don't write stream
processing applications that have to
query 11 services
this will bike is slightly less hard
than compatibility but not zero pain if
you collide this kind of stuff we're
doing cough the summit and it's in New
York which is kind of close by to in
Chicago and you have this nice discount
and it's next week so it's kind of last
minute if you don't register yet but
it's going to be a lot of fun and
instead is going to be there by the way
so so he's a totally good person to meet
even if you if you happen to be in your
fee and yeah so I'm done I notice fast
we have signed expectations
earlier there was a talk I mentioned it
as well as the idea of all of these
extreme intentions supporting beam it's
like a top-level API is different any
discussion about that Nakata stream I
think we've gotten questions and but
we've got questions about a lot of
things like can we get question about
one will we support Scala and when we
support Python and when we support go
and one will support dotnet and one
support OGS so we have a lot of like
just prioritizing the whole thing is
really hard and so right now we don't
obviously in the open document II the
easiest way to highly prioritize your
requests if you have someone go ahead
and just contribute a nice bunch of code
to the project and so that's a also
that's a way to get high so I know
that's for Python a bunch of people kept
asking and we kept like well we'll get
to it but we're not release area and
then they just went and open the Gita
project Kafka streams Python words are
doing their stuff and so I'm guessing
that at some point if enough people are
interested there will be a Kafka streams
beam kind of showing a problem beam
right because they host most of the
integrations right they don't really put
it in the other project like there I
think the host called a spark streaming
one and so on so yeah and getting one
day either we will gather first or
someone else will get either first but
yeah I'm guessing that it will happen
I'm very excited about the idea that
again I max VBA I wrote a lot of sequel
and I moved from Oracle sequel server to
MySQL basically almost no hitch except
the how the hell do I take backups
because a lot of my scripts in sequel
still worked and I kind of expect beam
too and I hope it will end up doing the
same thing for us yes
oh that's an excellent question because
earlier this week the idempotent
producers has been merged into Apache
kafka which means that you can actually
avoid duplicates in Kafka itself you can
have a producer that basically checks
for lack of duplicates and does sequence
numbers it does pretty complex stuff
before a device data to Kafka so we
think and then streams itself is using
those commits and checkpoints to make
sure that it's exactly one so we think
that we reduced drastically the need for
the duping at least we hope so and the
other cool thing about the example and
producer is that we added transactions
so you can now write to a lot of
different topics and partitions and you
say begin transaction write write write
write write
commit and you can have read committed
consumers that only read the stuff that
was committed and if something got
aborted they will not see partial
transactions which is absolutely mind
breaking Lee changes everything in my
world kind of thing and I am super
excited about it a capita summit this is
the one we always with you will have a
talk about exactly how it's done and how
to use it and all the Gotcha which
apparently are gotta the biggest one
people keep asking me is it slower if
you use transactions because everyone
knows transactions are slower it's
actually significantly faster
not because transactions are faster we
didn't break physics or anything but
while we implement reductions we also AM
optimize the data protocol of cop club
by a lot so basically if you produce
more than two events every three
milliseconds or something it you're
saving your you spending a lot smaller
batches even if you don't compress
anything you just send you smaller
budget and that magically makes
everything faster because of personally
it's always the networks which is the
bottleneck or it is a network or disk
you save either way
I'm here tonight and then flying out
really early I have Twitter one shot if
anyone feels like chatting about data
integration stuff I'm very much into it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>