<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • Some Considerations for Scaling: What We Did At New Relic • Bjorn Freeman-Benson | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • Some Considerations for Scaling: What We Did At New Relic • Bjorn Freeman-Benson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • Some Considerations for Scaling: What We Did At New Relic • Bjorn Freeman-Benson</b></h2><h5 class="post__date">2012-12-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fU02st8cfTY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">some considerations for scaling sort of
what we did at New Relic and you could
sort of subtitle this talk 17 things we
did so if you're keeping score at home
you know when you get to 17 you'll know
the talk is done so just a brief this
isn't trying to be a sales pitch but you
got to know something about what we do
to understand some of the things that we
did to make this work so we do web
application performance monitoring so we
monitor the performance of people who
have websites typically people who are
selling something so they want to pay us
money to monitor it we monitor the real
user performance by injecting javascript
in the browser and it sends data back
for every single page at every single
one of our customers customers views we
monitor servers you know at the server
level we monitor applications and the
interconnect connectivity to
applications you know it monitors the
full stack it has short-term reports and
long-term reports and alerts and all
that kind of thing the the more
interesting thing for the sake of this
talk is what's the architecture
underneath that and what we've got is
we've got your causes our customer out
there puts a little agent that's the
little logo they're sitting on the
servers into their application if it's a
job application it's a jar file if it's
a ruby application it's a gem etc that
could instruments automatically
instruments and collects the data and
sends it once a minute over to our data
center where we collect it and store it
on lots of databases and then we have a
couple web servers where you can then go
to our application and view that data so
it's complete software as a service
solution so you don't have to do
anything or our customers don't have to
do anything on their servers other than
install this little agent so but you
know it looks in in the sort of
marketing speak here like a fairly
simple architecture we use Ruby we use
Java we use my sequel pretty pretty
standard stuff the interesting thing for
the sake of this talk is that in the
four and a half years the customer that
the company has been existence we've
gone from zero accounts obviously when
we started to 30,000 accounts out there
and every account has between one and
our largest account has 17,000 servers
reporting data to us and they report
data to us every minute from every
server and actually actually every
application on every server reports data
to us every minute
so we're collecting over 40 40 million
metrics a minute which is over fifty
eight billion metrics a day or five
terabytes a day now this is much much
less data than the Large Hadron Collider
that we learned about this morning
collects in one instant of time and a
collision but that's ok now another
thing you'll find in this particular
talk is I've put footnotes on my slides
because I'm tired of going to talks
where people make arbitrary claims and
don't have anything to back them up so
if you have a computer handy you could
type in that little bit ly link and look
up the the data that I'm talking about
in every one of these slides um so
anyway we we grew you know the 30,000
accounts and we're continuing to grow
rapidly and so how do we do this the
interesting thing is you know it's New
Relic has been a startup and so if you
know anything about this Steve Blank
customer verification stuff or the lean
startup and so on the the way that the
the model of a start-up is first you
prove that you've got a business model
and then you scale that business model
so at the beginning New Relic was over
there in the scalable startup thing
where we're trying to find a business
model and now we're transitioning over
into the cash flow breakeven profitable
we're actually a company and growing
side of the business but since we were
started over there in the scalable
startup we had no customers we had no
revenue we had nothing we needed to
build a system that was we're able to
build very rapidly and get out there and
get people to start using so we hosted
our initial system at Engine Yard on
their their platform as a service system
which they had a bunch of virtual
machines we ended up being on 8 physical
machines there we wrote everything in
Ruby are our web application was ruby on
rails are collector application that
collected data from customers which was
Ruby on Rails we had a homegrown load
balancer based on top of H a proxy and
we had separate processes for each one
of the activities that we did and so
this was a great way to quickly build
this this solution so that we could
search for that business model we could
find out whether people would actually
pay for this service that we
thought was valuable and that perhaps
they thought was valuable now of course
this wasn't going to be able to grow to
the size we are now we're we're
collecting five terabytes of data a day
but it was a great place to start so
then how do we move from a great place
to start to something that works today
with the size that we are today well a
couple of the characteristics that it's
important to know about our software is
well first of all like I said every app
instance of every customer sends us data
every minute and sometimes more often
than that so that's not just every
server but it's every app instance so if
you're running for instance unicorn on
your as your Ruby service you're sending
you know 20 or 30 packets of data to us
per server and if you have 50 Apache
workers on a PHP process you're sending
50 data packets to us every minute etc
now that's the data that's being sent to
us only a subset of our customers view
the data on any at any given time right
so some of them log on to our website
and view the data but most of them
aren't you know here we are in Denmark
at the moment most of the American
customers at the moment are still asleep
right and so they're not looking on at
the website but European customers are
our data has a very steep half-life the
most interesting data is mere seconds
old people really care about how their
website is performing right now you know
is it slow at this exact moment they
care a lot less about whether it was
slow an hour ago or two hours ago or
last week now we keep the data for long
periods of time so that you can look at
the data from a week or two week or
three months or sometimes even a year
ago depending on you know which
subscription plan you're willing to pay
us for but for the most part that the
customers are interested in the data
that's right now um and they also care
that the accuracy is essential what's
interesting is we find our customers
write to us and say things like I was
looking at your chart and I don't
believe that our web response time was
250 milliseconds I was timing it with
some other stuff and it's 247
milliseconds I'm like cheese guys three
milliseconds but they actually care to
make it there's really really accurate
measurement of these things and so we
take a lot of care to make sure that our
agents are measuring thing
accurately but it also means that we
can't use an eventual consistency style
database where we for instance had all
the data feeding into some sort of no
sequel database that would eventually
get all of the data in one place and
then query it and tell the user what the
numbers are because they care about the
data being accurate right there right
there in that short period of time so
let's talk about the basics that we're
going to go through I've got five basics
that you go through and sort of we went
through in putting this together many of
you who run big web applications you
already know this basic stuff so number
one is you want to reduce the number of
connections to your servers so typically
when one of these processes one of these
agent processes or a website goes and
connects to one of our servers it may
take a while to make that connection and
have the response come back and that
that holds a socket open or a worker
open on the on the application server so
especially since you have lots and lots
of people trying to do that we use front
end servers we happen to use f5s as our
big switches to handle SS a ssl and it
buffers the requests and it just makes
sure that the you know it the f5s are
very well designed for doing that kind
of thing and then they buffer up the
request and send them off to our to our
application servers and our collectors
and reduces the number of connections as
servers which means each of the servers
can handle more customers pretty basic
stuff we also moved off of the platform
as a service at Engine Yard onto our own
bare metal in our own data center we
found that virtual machines didn't work
particularly well for our type of data
throughput we found that when we had a
virtualized set of servers which has
lots of good benefits but for the type
of collection we're doing didn't work
particularly well because with these i/o
latency problems when we're doing shared
i/o in a virtualized machine we saw
bandwidth jitter and some of the
virtualized instances that we that we
tried to use we tried putting some of
our stuff at amazon for a while which is
great for lots of things but it's not
good for high-throughput can rights to
databases sort of thing and ruby on
rails which is what our applications are
written in his very memory have
and we believe that the virtual machine
infrastructure's just doesn't handle
memory mapping as well as it could and
so you get a lot sort of lower execution
speed on your rails on a virtual machine
or so we found so we went to bare metal
we use direct attached storage in our
data center instead of a San we found
that my sequel which is the database
that we use depends on a really fast
right commit which means that you want
the cash to be as close to the CPU as
possible and you're thinking well yeah
in the sand the cash is out in the disk
array and it's only you know a few feet
away from the cpu but it turns out the
speed of light matters and we got a huge
amount of performance gain by using
direct attached storage instead of a
storage network even though these are
all in the same rack in the same data
center and we're talking about wires
that are just a couple feet long we got
a big benefit about going with the
direct attached storage now when you use
direct attached storage you you have
this problem where you end up with silos
of data because you can only access the
data from the machine that is attached
to but our architecture is such that we
don't have that problem we're able to
shard our customers very effectively to
separate shards so that when for
instance Sam is accessing the site we
get he only goes to the one server and
doesn't have to look at the data on the
other servers because we don't mix the
data from different customers across
servers like that so all of his data
would be say on server number 4 so the
director touch storage worked pretty
well we don't use app servers a lot
actually so we have you know remember in
architecture diagram we had our
collector tier that collects data and
then we have our app server tier that
actually serves up the application when
you go to the pages and the collector
tears just native Java applications as
an old assembly language programmer it's
sort of weird to say native Java but it
is NATO Java there's no web server in
front of that it just opens a socket it
has an embedded jetty in there so that
it does the HDP stuff but really it's
just native Java very high throughput
you can see I took a snapshot to put it
on slide here we're looking at a million
requests a minute on our beacon which is
the
collector that collects data from our
customers customers web page visits so
that's that front end data with the
JavaScript every time one of our
customers like groupon has a customer
who goes to a page that that's too many
levels in direction anyway it sends a
hit back to our beacon and so we're
looking at a million page request there
and these are just actually the sort of
average requests of when I snap shouted
that the peak requests are about fifty
percent higher than this so we need to
be able to handle a fairly high
throughput on these things we have one
beacon machine we have eight aggregator
machines and they're look you know
you're looking at 3 millisecond or 170
170 micro second response times on those
processes so that's pretty fast on on
the Ruby on Rails side for our main
application we switched to unicorn if
you do Ruby on Rails development you
know that unicorn has this advantage
that it doesn't have a dispatcher that
dispatch is the request because they all
share a common socket all the workers
share common socket so that takes a
little bit of the delay out of the whole
process unicorn also happens to be
easier to do live deploy of code to you
don't have to take them down quite as
much and restart them the way that we
did previously and so it helps with with
continuous deployment so those are the
basics so now once you've dealt with the
basics let's go and deal with the usual
suspects that aren't quite as basic but
things that you can do a little bit more
they take a little bit more knowledge
about what the application actually
looks like to tweak so the first one is
let's look at the agent protocol now the
agent is the little piece of code that
it sits in your application and sends
data over to our aggregators and so our
first agent was really simple the first
agent protocol was really simple because
we were originally a ruby on rails
monitoring shop and we had ruby on rails
in our data center doing the collection
and so we just used Ruby object
serialization to send the date / send it
over no problem allowed us to get up
fast and quick and prove whether we had
a good business model now as soon as we
extended from just doing Ruby to doing
Java we could either duplicate the Ruby
object serialization in Java code to get
that or
we could come up with a better protocol
and so since we now realize that we
might have a business in place when we
were going from Ruby to Ruby and Java we
said okay great let's make up a new
protocol and we designed a new protocol
that was more optimized for sending the
data over and it happens to use JSON but
a couple other things that did is it
reduced the round trips uh previously
when we sent we sent metric data in one
round trip and error data in one round
trip and transaction traces in one round
trip and configuration data one round
trip and so the agent was making four or
five different requests to the collector
every minute and that was fine when we
had 16 customers but you know when we
got to about 10,000 customers we're
starting to see a significant load of
round trips on to the server so we
changed the protocol so that it packages
all up into as few round trips as
possible does it keep alive on that so
that the round trips aren't actually
closing the socket on every round trip
and so on of course we are also very
careful in our protocol design to put a
version number in there so that we could
continue to support old versions of the
agents out there as we continue to
evolve didn't have to insist that
everybody update all their agents every
time we're currently supporting 12
different versions of the protocol out
there as we've evolved it as we've grown
the company so another thing we did
which we found very important in scaling
the business is that if one of our
services is temporarily unavailable we
accumulate the data and retry we don't
double up and retry but we accumulate it
so for example if if the agent is trying
to report to the aggregator this minute
and it finds it the aggregator is too
busy to take the data it says okay well
too busy this time and it continues to
agric ummul eight data from the from the
customers website and then the next
minute tries again with two minutes
worth of data it doesn't try again with
two separate requests in that second
minute it tries with two minutes worth
of data and we do that at every one of
the levels at the agent level we
actually do to a greater itself so that
if we're trying to write to the
databases and the databases are
temporarily too busy to take the data we
accumulate the data for a couple minutes
or you know a couple interation which
might be 30 seconds and then write the
accumulated data the second time it's
really important to do that that
accumulation because otherwise you end
up swamp in the service when it comes
back up again and especially in a
situation like ours where you're running
at a very peak load inside the service
already I mean you know we have a very
constant high throughput rights to our
databases and to our services you can't
sort of double that load a minute later
and expect the service to be able to
handle that I suppose you could if you
put a lot more Hardware in but we didn't
want to do that so anyway we've got that
at all the levels and this was a good
win for scaling the system up another
thing we did is you know originally when
we started the company our customers
were all small companies like ourselves
in fact to be honest they were all
friends of ours who thought that might
be nice to try out what we were doing
little by little we started getting
bigger and bigger customers you saw my
slide earlier that our biggest customer
has over 17,000 servers we have for
instance atlassian is one of our
customers and they have 20,000 different
applications each running on a subset of
their servers and so when they would go
to our web page that show listed all
your applications running it was great
when our customers had three
applications didn't work so well when
our customers had a thousand
applications it was just too much data
showing up there so again a design
decision that we made early on to make
the company to give the company a start
we need to modify and so we did things
like paging and smart sorting and
searching smart sorting is just you know
if you have a thousand applications that
you're trying to view you obviously
don't care about a thousand of them you
care about the two that are slow so we
sort those to the top and that sort of
thing we use Ruby on Rails which you
know for our main web application which
has certain advantages it's it's a great
system for building certain types of
applications but since it has this
built-in ORM you know rails it can be
very scary so there's your halloween
pumpkin from the united states with
these scary rails on it the scary part
of the ORM is that if you're not careful
about how you design it you can bring a
lot of objects when you try and trend
walk over the database using what seems
like innocuous pieces of code and so
what we did was you know first of all
wrote the code in rails and it worked
fine when we're looking at small sets
and then as we got more and more
accounts and more and more applications
on those accounts and more more servers
on those applique
things started to bog down so we
cleverly used our own software to look
at what the slow transactions were in
there so we'd find something in there
low that page is taking 2.2 seconds to
load that's pretty unacceptable we want
everything to be less than half a second
so we could dive in with our software
and if this were a product demo I could
show you exactly how you dive in with
the software and find out how many
incorrect objects are being loaded and
then we would go in and we would fix the
ORM mappings or we would add an index or
something so that it would be faster and
but you know the lesson here is that if
you're using something clever like an
ORM to make your development faster
there's a certain cost to it and at the
beginning of the company's life that was
the costs were willing to pay because we
had a very small set of data and as the
company's gotten bigger and our data
sets have gotten bigger we've had to
move away from that automatic mapping to
objects and do more and more of the
queries manually ourselves okay so those
were the usual suspects now let's go on
to what I would I consider the clever
things I've got six of these starting
with one of the clever things we do is
we precompute some of the queries now if
you ain't went to Nathan manses talked
yesterday about the way Twitter does
things and calls it the lambda
architecture this is a very similar sort
of thing although we're not nearly at
the same data scale they are but for
example since you can send your data
from one of your applications to
multiple named applications on our site
so you use this to do roll-ups so for
instance in our case we have two servers
a hot one and a stand by one they
collect beacon data and we report them
in our dashboard as beacon one beacon
two and a rolled-up one called beacon
which is a combination of those two and
so in the database this is stored again
remember we're using my sequel just
straight native my sequel this is stored
as lots of different rows by server and
by application and by customer and so to
do a query on beacon one we have to
query a bunch of rows in a new query on
beacon to have and beacon we'd have to
query about your rows so instead what we
do in the aggregator is we know that
these are standard queries that occur in
our application and we pre compute those
on the aggregation
side because we can do it faster as the
data comes in then we can by writing it
to the database and then querying it out
of the database and so by knowing some
of the characteristics of the way our
web application accesses the data we can
pre-compute some of those rows and write
them into the database and then go get
the pre computed row and show it and by
robustly building the software if we
can't find the pre computed row we can
actually though and then go and compute
it the slow way in our application so if
for some reason the pre-computer row
didn't get written we're still in good
shape we a similar problem with our
background jobs we have we have our data
in a number of different tables at first
it's in we store it in tables that store
things by the minute and then we roll
those up and store things by the hour
and then we roll those up and store
things by the day so that when you look
at data farther in the past you can look
at things at a different granularity and
we can get that data more quickly and so
when we first built the the software we
had each of these background jobs was
written in Ruby and once an hour there's
a background job that took all the
minute data and rolled it up to our data
and it all made sense and it worked
great except that we eventually
discovered that the roll up process was
taking more than an hour to complete and
we were starting to fall behind and
rolling up the data it's like okay well
we got to fix that problem and so we did
the same sort of thing that we did with
the pre computing the data except in
this case what we have is we have the
collector which writes to both the
minute table and it collects data and
rights to the our table so once an hour
it does a few extra rights to write to
that our table and essentially pre
computing that background job and again
if that collector were for instance to
die because it's just a Java process and
it you know you might kill 9 it or it
might run out of memory it might die and
restart it would lose that our data that
it was starting to write so we've still
got that background job sitting around
and if it notices that the that the roll
up for that our didn't happen it'll
trigger it and start to do it and so as
long as the collector is doing most of
the roll-ups we're fine because the
background job even if it takes more
than an hour to do one hour we're still
in good shape another thing we did is we
use different databases for different
types of data now we actually are using
my sequel underneath the whole
system so it's not that we're using you
know my sequel for one set of data and
and say Cassandra for a different set of
day we're actually using my sequel in
all these cases but we're using a
relational tuned my sequel for our
account data like have you paid us this
month and what's your account name and
who's all the users who can use it and
we use a a right only tuned or write
once tuned database for our time slice
data because all of the time slice data
is effectively immutable right it comes
in once the data is recorded it has it
doesn't change again and it has this
characteristic where you can you can you
can change the way the tuning parameters
on the my sequel the different buffer
pools and so on you can get the maximum
speed there and so you know if you know
something of way about memory allocators
are implemented in c you know Malik and
so on there's a an algorithm called
buddy memory allocation where you you
make blocks of different sized blocks so
that if you're allocating small lots of
small blocks of memory they come out of
this chunkin of the lots of large blocks
that come out of this chunk this
basically what we're trying to do with
our database is here by separating the
databases according to the
characteristics of the data that goes
into them and if you're if you were at
the Large Hadron compiler Collider talk
this morning that they were basically
doing the same thing of course again
they're not using just straight my
sequel but they're separating the data
by the different characteristics that it
writes out another thing we did is we
use a non garbage collector garbage
collector solution for our databases so
you know when people are sending us data
we're writing rose into the database
we're writing 5850 billion rows a day
into the database and since we write it
in a date on a minute basis and then we
roll it up to an hour basis after
sometime the minute data is no longer
useful and we can throw it out so how do
you garbage collect that well a standard
ways you'd go into the database and you
just delete all those minute rose but it
turns out if you do that in my sequel at
least using some of the standard my
sequel backends it gets pretty slow
because it locks up the table or locks
up rose you're trying to use and just
jams the whole thing up so instead what
we did is we said well let's just make a
different table for every account for
every x
sighs and then when that time that's no
longer used we'll just delete that table
just do a drop table so instead of
deleting the rows one by one we're
deleting the whole table now I worked in
Amazon 1999 2000 2001 and we had a big
executable there that was written in C
this is before the Amazon switched to
doing a services model and we had a real
memory allocation problem because people
would allocate memory all over we're
trying to side well should we make a
garbage collector or whatever and we
said no they simplest solution is just
run a hundred requests through the
through the executable and then restart
it so the memory collection consisted of
using the operating system to zero out
all the pages because it was extremely
efficient at doing that and just start
all over again and we're using a similar
model in our database here we're just
using the operating system dropping a
table as effectively as simple in my
sequel as deleting a file from the disk
and the thing works very quickly that
way another thing we do is we do
computation in the database instead of
in rails ruby is a is a nice language to
work in for making changes quickly but
it's not a particularly efficient
language in terms of execution speed
this is why we moved our back-end clever
front end collector from Ruby to Java
because we could the the Java process
was much more efficient and the Jetson
in Java are very efficient and we got
really high throughput response time
there but we we don't we don't want to
take the the cost of writing and java if
for our main web application we're all
happy writing in rails so we want to do
as little computation in rails as we can
because it's just it's not that
efficient as a computation language the
my sequel back-end actually computes a
lot of the things that we need very
efficiently if you look at you know
again the sort of data that we gather we
gather time slice data for your
application and then we provide you
averages of that over periods of time
whether that's a period of a minute or a
five minutes or ten minutes or something
well average is a computation that my
sequel can compute very efficiently over
rows in the table and so we do that in
the database and then send a few rows
over to the Ruby side to display on the
screen now this is opposite of sort of
the classical advice of doing nothing in
the database and there's a there's a
famous paper from ebay and
2006's d forum where they explained
about how they managed to scale ebay out
so well and one of the things they said
is don't do anything in the database
don't do anything at all in the database
and so we're not doing that we're
actually doing stuff in the database we
just happen to be doing computation
we're still not we don't have any
business logic in the database what we
do do some computation there and then we
move to SSDs and the interesting thing
about SSDs and hard and it's just in
spinning rust in this case for for our
data set is that when when we get data
from customers we get every minute we
get from every customer a piece of data
and so we could either write those all
sequentially into the database and then
when you came to view your data we'd
have to go pick your piece of data from
this page in your piece of data from
this page and your piece of data from
this page so we can only get sequential
writes and random reads which of course
would thrash the database to bits
because that random read means it's
gonna have to fetch basically every page
across those tables or we could do it
the other way where when we do the
rights we right here we right here we
right here and then when we do the read
we can read it sequentially but again in
the rights case we're going to we're
going to be writing all over the disk so
we were constantly running our disks
very hot and we tried to do things like
put in our aggregators where we would
pre chunk huge writes all together and
then write them out all at once so they
could do as much sequential writing as
possible even though we were gathering
it randomly and so on so then we decided
well let's try switching to SSDs now
SSDs have certain characteristics
they're they're supposed to be very fast
and they are typically very fast they're
actually a little slower on the right
side at least the SSDs we chose they're
a little slower to write than the hard
disks but there's no seek time on the
reeds and so we can do random reads and
sequential writes on the SSDs which we
couldn't do on the hard disks and we got
a huge performance wind out of the
characteristics of our data because of
this so we can write the data as it
comes in just straight across the disk
and then when we go and do the read we
get the random read with no seat cost at
all by using the SSD so that this is a
good win we actually continue to use
hard disks for the longer term storage
because people again don't access it
that much so we have
process that takes the SSD data and
moves it off to the hard drives on a
regular basis and then we get it from
whichever place it is in so a couple
more things here's a three-dimensional
bin packing problem which i think is is
great for the two further optimizations
that we're doing to continue moving
towards becoming a larger and larger
company one of the things we did was
when we first started out we just
plopped all of process type X on one
machine and olive process type Y on
another machine and it was great because
we had lots of machine resources we
started we started with zero customers
at eight physical machines so we had
plenty of machine resources to handle
everything but each one of these
processes has a different characteristic
in the amount of memory it uses the
amount of i/o uses the amount of CPU it
uses and so on and we've discovered that
by moving a few of the exes here and a
few of the wise under the same machine
we can optimize the characteristics so
that we're using as much of that machine
CPU and as much of that machines memory
as possible without causing any
thrashing this at the moment is a manual
process where we just sort of go well
looks like we can move a couple of those
onto this machine and look at the curves
and make sure it works but we're getting
more and more performance out of our
same hardware because we've gone from
eight physical machines to ten physical
machines when we've gone from zero
customers to five terabytes a day of
data right so we really haven't added
any machines we've just been continually
optimizing it the other optimization we
do is that we move customers around
between our shards we made an early
architecture decision when we're
building the system to allow customers
to be switched between char to be split
between shards now obviously we made an
early decision to have shards in the
first place but once you've made that
you know you normally put all of a
customer on one chardon all of another
customer another shard but we thought
that you know we might want to move them
someday so we have the ability to put
some of some customers data on one chart
another so that we can gradually move
them over to another shard without
having to do a giant copy of all of the
data that customer because there's quite
a bit of data so we've been balancing
the customers and this is automated
between the shards so that all the
shards remain about the same level of
load regardless of the size of the
customers so in effect each one of our
big sharp shards has one of our big
customers on it and a bunch of our
smaller customers right so that you know
as we're growing we continue to do that
so a couple lessons takeaways from this
you can sort of go back over the talk
and think what's he going to say well
number one do the basics right there's
there's basics things you can do about
you know putting switches in front and
setting up your hardware correctly so on
number two when you're building a
business like this design in some
scalability you don't have designed in
all the scalability you don't have to
make a magic solution that has all the
possible you know no sequel buzzwords or
whatever it is I mean we've built this
business that does fifty eight billion
metrics a day and we're still running on
my sequel just straight my sequel and
we're running on you know generic Linux
and we're running on you know root rails
192 you know there's there's nothing we
didn't do anything magic to get here we
just designed in some scalability and
then and then iterate it to get there
what we did do is we use the unique
characteristics of our application to
optimize what we're doing for instance
when we have our database tables we've
got two types of data like I said we've
got our rate relational data like our
account data and we've got our time
slice data we actually designed our time
slice data so that all the information
in the time slice is actually stored in
the keys of the index so much like when
you use a hash table where you have the
keys have some value and then you don't
actually care what the value in the hash
is because all you care whether key
exists or not we're doing the same thing
in the database for the time slice data
so that means that in my sequel what
we're using is we're using the b-tree of
the index and not the on disks data
storage and as a consequence you never
actually have to seek the head over to
where the data is stored you just look
in the index and so we're using some of
those unique characteristics of our data
to optimize the way our application
works and how it stores and access is
the data so I know that as the company
continues to grow you know we intend to
double this number in the next year
we're going to continue scaling
horizontally we'll add a few more shards
we're also going to probably have to
make some architecture changes so this
isn't the end-all be-all where we'll
probably have to change that using just
my sequel to using my sequel and some
specialized database
underneath and you know there's there
some other changes will have to make but
that's what I wanted to say and I'm
happy to answer questions yes well they
can they just have to do it indirectly
through some service so it's not going
to be as efficient but you know we have
our background processes move and you
know as long as they don't have to be
real time access then we're still fine
right I mean we suffer a little bit
because we don't have the story the
storage is direct access so you have to
go through that machine but it turns out
not to be such a big problem yeah the
computation we do the basis database is
really simple right it's just what you
can do with sequel statements right and
again it's a characteristic of our
application which lets us do that we're
not we're not doing sophisticated
computation is just that rather than
just selecting the raw data and doing
the computation on the on the app server
side which is sort of a standard model
we chose to do as much computation as we
can get away with in the database layer
to reduce the number of rows we have to
bring over reduce the memory churn of
the Ruby garbage collector and that sort
of thing
you starting UN in today would you pay
tribute as your powerful religious
services well it's an interesting
question because we've managed to get to
where we are today using Ruby right so
the question is is there a way to have
gotten here without using Ruby and yes
but not with the people that we have
right I mean we've got a bunch of people
who are very good at doing Ruby and so
continuing to do Ruby is a good model
there now you know as we've grown and
we've specialized certain services the
new services we are choosing some other
language and we hire people who are good
at that particular thing to do to do
that you know you know would I if I was
starting over what I use Ruby um yeah I
guess it depends a lot with the people
that I was starting with because at the
beginning of the company's existence the
real question was would anybody pay for
this right was it was it a reasonable
business model and so optimizing for
scale or performance and so on at that
point would have been a stupid move we
needed optimized for getting the idea
out in front of people and finding out
whether people would pay now you know
like when i worked at amazon we had the
whole thing was in c and it's now you
know a bunch of services that are in a
wide variety of languages and they've
done a good job with that and so i
expect that some time a new relics life
in the not-too-distant future we're
going to have to do a similar break up
where we're breaking the services up to
be not just a ruby system but a number
of different systems we just haven't
gotten to that point yet yeah
question is what should be not much
about some things that you did that you
think we're good parts of businesses
scaling what what kind of like early on
hmm well there's certainly some
personnel decisions we made then that we
wish we hadn't made but technically you
know again I'm going to sort of waffle
on this question of what things wouldn't
we have done technically because again
we wouldn't be here if we hadn't done
things the way we did so so the question
you know could we have gotten here by
not making some of the decisions the
decision I think early on to use my
sequel and only my sequel has cost us a
lot more development time then we
probably would have preferred right but
once we'd started down that path the
sunk cost have continued you know of
continuing was better than redoing it
again but perhaps if I did it again I
you know I would choose to do a
different technology for the time slice
data perhaps but but overall I think
we're pretty happy with the technical
decisions that we made and there's
there's nothing out there that's really
causing us a lot of pain technically in
the product right now maybe you
below is that is considering our
children tech because they actually turn
sequential access into yeah I think what
we'll end up doing is writing our own
because a number of the people we have
in the company have done that in the
past for exactly this data problem in
previous companies and so we we really
have a lot of experience in exactly the
piece that we need there hey I think
that's what we'll end up I don't
actually know at the moment so VMs were
hurting you personalization where was
that may be running an elastic
environment like Amazon or was that when
running virtual machines on top of
physical hardware you actually do know
yes so the the virtualization was
hurting us with our particular
performance characteristics whether it
was our own hardware or whether it was
Amazon's hardware or whether it was rec
space or you know any one of these
virtualization providers there's just a
number of characteristics that just
don't fit well with this high throughput
data right model right and if you look
at some other people who've done this
they've used a hybrid model where they
have a raw metal version of their write
throughput data and then they have a
virtualized layer to do the application
server and we could easily do that
except that we you know we just just put
it on raw metal and said we I mean we
still use virtualization for a number of
the things in the in the company but not
the the main application and it didn't
matter whether it was our own or
whatever position especially he will
packet interested the next step and is
acting fingers you can use it choose for
your iOS going to the CPUs and GPUs are
consuming that you can see colors and I
need two different
we didn't try that um good but that that
would be something yeah yeah I mean you
know it's it's not that it's trivial and
we spent a lot of time building this
stuff and and optimizing these things
and and looking at the characteristics
of our application and tuning my sequel
and so on but you know there's there's a
lot of solutions to these problems and
you don't you don't have to go and build
something custom you can sort of evolve
your way there ya know so so your your
question there and I'm repeating it for
the video is in SSDs you know don't you
have a care don't you have a problem
where there's an only in a certain
number of cycles you can do before the
disc dies and the answer to that is two
parts one is it's the number of writes
that you do that costs you not the
number of reads and so one of the nice
things about our data is it's right once
and read many times and so it fits very
naturally with the SSD model and the
second is that the the cost trade-off in
the business of even let's say the SSDs
only last us for a year before we have
to replace them it's still such a win in
terms of not having to do other things
in the application that we would be
willing to pay for ssds that only lasted
a year it turns out it looks like from
our modeling and our testing that the
SSDs are going to last us about three
years each and by then you know who
knows what the technology will be
quantum discs or something so so your
question is do we maintain decent
performance in the back end when we have
these large customers like seventeen
thousand pages I guess it depends on
what you mean by the back end right so
right so we've we've spent some time so
the nice thing about being the customer
who has 17,000 servers is that you're
paying us a lot of money and so we spend
a lot of time paying attention to you
and so we've spent a fair amount of time
optimizing our application to deal with
the case of like 17,000 servers so
things like pre
shooting some of those queries so that
on the disk instead of having our on the
database instead of having to go to
17,000 rose to get a minutes worth of
data we all have to go to one row to get
a minutes worth of data because we've
pre computed the fact that we know we're
going to have to do that particular
query and so a lot of these these
application optimizations have been
because of our large customers and you
know we monitor our own site and we go
gosh looks like customer X is starting
to have a bad experience what can we do
to go in and and fix that up and make
the performance better there you know
it's not just adding an index to a
database table it's well you might
actually have to modify the application
to do things differently like move a
computation to a different layer of the
of the service or something so we
actually just store all the data in the
same database tables just with the
different types of keys that you know
sort of magic keys that we've said well
this is a rollup row and it rolls up the
following types of information and so
it's just all start in the same table
and so then it's easy to do that non
garbage collection garbage collection
when you just no longer rent needed and
so on it's just oui oui je actually just
start in the same table ii don't we
don't build special because our
aggregations are just producing
aggregations or pre computations of this
the same data output right it's not it's
not an aggregation that looks different
it's just a pre computation of the same
result that you would get if you done
the query live well thank you very much
appreciate your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>