<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Video Transcoding at the ABC with Microservices • Daphne Chong | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Video Transcoding at the ABC with Microservices • Daphne Chong - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Video Transcoding at the ABC with Microservices • Daphne Chong</b></h2><h5 class="post__date">2017-08-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Pn_Op4zUKwQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so have you ever sat down in front of a
streaming service like Netflix and you
thought hey you know I wonder what needs
to actually get done to prepare this
content for streaming to my TV and okay
Sam has but I totally like I never did
the reality is though that there's
actually quite a lot of stuff that needs
to happen to video content to actually
make it available to you and that's what
the talk is about today something that
the stuffs that needs to happen to your
video in order to make it available for
you and what we actually did to make
that happen for the entire ABC catalog
so again full disclosure I work for
Amazon I'm actually working at Amazon
fresh now which is grocery deliveries
and it's available in Chicago if you
want to try it but I'm not here
representing Amazon today I'm
representing myself for a previous role
that I did at the ABC so I really want
to introduce you to the ABC first it's a
public broadcaster in Australia and it's
actually a really great national
treasure it's really well known for TV
we've got a lot of different own
broadcasting services and shows that we
create it's also hugely important source
of news radio regional programming and I
like to think of it a bit like an
old-school PBS it's publicly funded
there aren't any ads during the shows
and it's actually one of the rare
channels that appeals to both kids and
you know senior citizens so it's kind of
a got an audience that's a really big
spectrum and one of the iconic shows
that we have is a show called at
playschool and if you're a kid in
Australia you grow up with these
characters
you seriously know everything about this
show and it's kind of our equivalent of
Sesame Street and in my first week at
the ABC this was me getting to meet my
childhood heroes in giant form I just
wanted to show you this to just show you
the love I have it
here and how hugely excited I was at the
opportunity to work on a system like
this for a company like this so the ABC
has a product called a via and it's the
front end of the system that I'll be
talking about today that's what we
produce all the video is for and it's a
TV on demand service it's free
everything that gets broadcast on the
ABC gets made available on ibew
and it serves millions of viewers every
single week so to make these shows
available on iview we have to process
them right we have to transcode them
into formats that are actually suitable
for streaming so today's talk is a
system we built to do that our scale in
a cost-efficient way we called our new
system Metro so things that I would like
to cover today one is what do I actually
mean when I say video transcoding right
what why do we need it what does it do
I'm gonna walk through a transcode to
kind of show you the whole process of
what that's actually doing to is what
did we end up building to handle the
volume of content that we produce at the
ABC how did we take one transcode and
actually repeat it reliably so I'm gonna
give you a higher high-level overview of
the architecture that we built and AWS
services that we use here language
choices that we made and also the
benefits of arts using small services
and dividing asset our system up this is
my favorite part of the system it is how
do we scale our transcoders transcoding
is the most expensive part of our system
so how do we actually scale to handle
fluctuating demand but also optimize for
cost how much does Metro actually cost
to run right how does it compare to the
commercial transcoding systems and you
know what does it actually cost for us
to run for a typical day or a typical
year of content for the ABC
and last this is kind of a bit of fun
these what kind of optimizations could
we actually do in future because this
project actually started as a really
small team of like an average of three
people that ran for about six months
this is the resulting system that we
ended up with and so there's definitely
some things that we can do in future
that will help to improve the throughput
and optimizations so first off what is
transcoding sorry I just realize I
hadn't started my time off thank you
so our transcoding is a process right we
start with an initial input file which
is either an audio or a video file of
some kind and we convert it to a variety
of different output formats which we
call renditions I'm going to be
referring to renditions throughout this
talk quite often because it's a really
important part of our system so we
create a lot of different renditions for
each video and in tradition is slightly
different so what changes are we
actually making to these videos we can
do things like adjust the resolution
like the size of the video we can adjust
the actual format of it so that you know
the encoding of it we can change the
bitrate so the quality of the video
higher bitrate is a higher quality and
we can add things like watermarks so
like logos on the bottom of the screen
that kind of stuff and our goal is
really to take a video that could come
from any source and produce a standard
set of renditions from it we want a
predictable set of renditions for us to
use on iView and so this video here on
the left is the single video that will
receive and through the transcoding
process we're going to end up turning it
into the multiple standard renditions
that are on the right and we do this for
every video so why would we have so many
sizes one is we want to optimize the
video for your device so if you're
watching it on a
machine or a TV we want to give you a
better quality video versus if you're on
a mobile you really don't need something
that's so big and the other thing is
that we actually want to adjust it so
that it suits the network speed that
you've got so if you are what so every
rendition has a different bitrate and
that's the amount of data that you're
downloading per second in order to view
the videos so if you want a high network
connection that's great we'll give you
the highest bitrate that we've got
otherwise if you're on a lower version
like I mean if you're on mobile on the
go we'll give you a low quality
rendition so that you can actually play
that it won't really affect your viewing
quality the player the video player that
you're watching it with is actually
responsible for selecting which one of
these renditions to play so our job is
just to ensure that we produce the full
set of renditions so that when it comes
to your player it can select whichever
one is the most appropriate for you so
we convert everything to h.264 mp4 we
receive a lot of different input formats
but it makes such a lot easier to output
this in one format and that's because
it's a standard format for HLS streaming
the player doesn't have to worry about
you know different codecs we don't have
to do QA across a whole bunch of
different formats and also you know
we're not doubling up on storage of
multiple different renditions so I'm
going to walk through an example here so
you get a good idea of what we're doing
first we have to prepare the watermark
that actually gets put on the bottom of
each video so we make it transparent and
then we put it resizing it down and that
size is a relative relative to each
rendition size so you know you'll get a
larger renditions we're just going to
have a larger logo but when you watch it
back on your screen it'll look the same
relative we then take this input video
that we've got this is an episode of a
show called utopia that is an Australian
show produced by the ABC and the format
that we've got this in
something called gxf it's a broadcast
format and you can see it's got this
black bar at the top here and and that's
used during broadcast
it's got timecode information in it and
it's also got captions in it but the
thing is is that you know you're not
going to use those when you're streaming
online so we crop that off next you'll
notice that it's actually in 4:3 format
and we want to show it to you in
widescreen 16 by 9 so for historical
reasons it's actually more space
efficient to kind of store it in 4:3 and
there's a whole bunch of like systems in
the ABC they're good to producing
content and dealing with content in 4:3
so that's what I get saved in but we
adjust it the ratio and save it in the
header file so that you know the video
player will know yeah it's actually
supposed to be a widescreen format we
then add the watermark that we've
produced earlier and again you just want
to make sure that if you're switching
renditions in the middle of streaming
but the logo doesn't look like it's
moving around we also have to process
the audio of this video sorry this is
the audio track for the original
half-hour version of that show and it
shows you the audio intensity so the
higher the bars the louder the volume
and we go through a process of audio
normalization because we want to bring
everything to a consistent volume across
all the different videos and we do this
because if you have a show a and show B
here and they've actually recorded
different volumes or have different
volumes on their tracks
when you watch it back on your player
you don't want to have to keep adjusting
the volume but as you as you change
shows so we normalize everything so that
you don't notice the difference so after
the video and the audio of both
processed and the video is now ready to
stream and it actually gets split up
into a whole bunch of 10 second chunks
and so when you play your video you
receive a series of these small chunks
together and they form a you know smooth
continuous video for you
and the those little files can come from
any of the different magicians and
that's how your video player there gets
the ability to stream and to change the
quality you know dynamically without you
having to really worry about where it
comes from so that's a lot of different
changes that we've made to these videos
right we've we've changed it encoding
and cropping and bit rates and stuff
like that how do we actually keep track
of all of this stuff right we ideally
want all of those changes in one place
because we get a lot of different
formats the kind of steps that we need
to do for each of the files is going to
be different and we don't want to have
to think too hard about what we're going
to do there so we save the list of
changes for every single video type into
something called a profile and that's
it's a set of instructions that we can
look up whenever we get a new file in
our system it looks a little like this
stylistic use this so our metrics got a
source location where all the files are
dropped and to start with we just kind
of map a folder to a profile and that
serves all videos dropped in this folder
I have this set of instructions on them
and so this folder holds GSF gxf content
this is the stuff that will do to a gxf
file and we end up with a transcoding
example we just saw before but if we get
some other type of content like mouth
file Perez Apple ProRes format there's
actually less stuff to do we don't we
don't get that content with the black
bar we don't actually get that content
we in the wrong ratio it's already it's
already done for us so there's less
instructions that we actually need to do
to finish that and so to actually do all
of that to execute all of that video
copying the scaling etc we use ffmpeg
which is an open source media
transcoding tool it's incredibly
powerful and it's been around for about
15 years this is an example of a sample
command that to run something in ffmpeg
and it makes a lot of assumptions about
the input format about the quality you
want about the output formats that you
want
this is the real thing that we end up
using for hours and it's linked to the
profile so if we're producing six
different renditions we'll have six
different versions of this command and
the command will show you things like
what the encoding is that we want as an
output the cropping deinterlacing this
is the scaling aspect ratio some stuff
with the watermark things that are
controlling video quality and audio and
just some other options and there's a
lot of different stuff that's in here
I'm not going to be able to cover it in
detail but ffmpeg the community is
really active and helpful and it's very
well documented so it is an amazing
piece of software we really could not
have built Metro without this so that's
kind of an overview of how the
transcoding process works and our goal
is to really take that input video and
produce the standard set of videos
output videos that we will use for our
thrive you so we want to now take a look
at how we actually run that at scale
Metro is a workflow so all the content
that goes in will follow these
high-level steps so we get a file and we
transfer it into AWS we then queue the
content and we make sure that it's ready
for processing we'll transcode it by
running ffmpeg over it we move the
resulting renditions out to our CDN and
then tell live you hate all your
renditions are now ready and you can
publish and here's what it looks like
some an architecture point of view so
first the input file is uploaded to an
s3 bucket and when it's finished
uploading we use s3 xin built
notifications to put a message on to an
amazon SQS queue so we've written
several small services we call queue
Lissa's and one of them will monitor
this queue and it's single job is to
messages going onto that queue and to
tell another component that this has
happened so as soon as it as soon as the
message gets added there it will send
that message to and another component in
our system called the orchestrator to
say that if jobs been added and the
orchestrator is kind of the brains of
our operation it's the only component
that's actually state aware the only
component that has a data store it's
responsible for tracking where
everything is in the system and it
controls it also controls the number of
transcoders that we have according to
the number of jobs that we've gone in
flight so that gets saved and then while
it's saving the job it will also look up
that list of profiles and it will match
this video to a profile and that profile
will tell us how many renditions we're
trying to create and it will also tell
us the ffmpeg command for those
renditions and the input and the output
locations so that's like three really
important things that we get we actually
get more information that we send along
with that profile but for us to do our
transcode those are the three things we
really need and so at this stage we've
actually got enough information to begin
our transcode the orchestrator will in
queue a message for a rendition on to
our transmitting queue so if we say
we're producing six different renditions
then we into six messages here and well
the orchestrator will also calculate
whether we need more transcoding ec2
instances to actually meet the number of
jobs in the system so if we do it spins
up new instances an external I will
actually now spin them down so all of
our transcoding instances are marshaling
this queue with the trance code messages
on it each transcoder will take one
message at a time one transcode at a
time and do its work and this is where
the most intensive part with ffmpeg is
actually done now
system so that it takes the most time
it's like 90% of the time in the system
is inside this machine here and the time
to transfer is also going to be
dependent a lot on the type of content
that you're transcoding so if it's a
long piece of content they will
obviously take a long time also if it's
a high quality piece of content it's
going to take much longer than a lower
quality piece of content so once that
transcoding is done the file is saved to
a an s output s3 bucket that we also use
the orchestrator is told and then it
will save the status of this job now so
that we move those renditions out ready
for the CDN so the transfer process
takes the finished renditions and sends
them to the CDN and that means that they
actually become ready to publish on I
view sorry yes okay so the orchestrator
will mark that rendition as complete and
it will notify I view so that the videos
can be published at this point also it
says hey is there anything else in the
system and if not it's going to turn off
all the transcoders so that we're not
running machines when we're actually not
doing any work on average it takes
around 8 and a half minutes to actually
go through the pipeline and that's that
figure is everything from file upload to
s3 all the way to notifying I view and
as I said we've got a variety of content
from you know five minute videos that
are kind of like really short to
documentaries that can be 90 minutes or
two hours and so the timer system really
depends on how long it is here's a bit
of information on some averages of how
long it takes so the lowest quality
rendition we produce which is actually
an audio track and that tends to take
around 7% or 12% of the original content
time in the system and our highest
quality button tends to take around 70%
of real time so if a one hour piece of
content goes in audio will take around
seven minutes to come out and a higher
bitrate content will take 40 to
in terms of technology that we use to
pick to build this system the things we
built ourselves are actually written in
two different languages we use node and
we use go so the first of those is the
orchestrator it's the brains of our
system that's the only one that we wrote
in node it's because it's quick and easy
to write an API for it basically and the
team already kind of knew what was going
on with node so most in smarts lives
here we have what's in flight
how many transcoders do I need what's
the state of each job and you know
looking out information that I need to
know about each of the renditions that's
going through in the profile these
services the ones that are sitting on
the end of the queue and actually aren't
aren't very stateful they're written and
go and so they're really small they're
independent long live services and their
job is basically to monitor these events
that come in and do something with them
and so those steps there's three of them
those steps are to register a new media
file here to transcribe the content and
also to transfer the content out to the
CDN so they've been really stable and
really easy to deploy because they're
just a they get built into a binary and
we just drop that down and that's been
fantastic so we don't need a special
runtime or anything like that for go and
it's also been great for piping large
files out through to our CDN go can use
a ghost streaming you can use basically
a files of any size and it'll use a
constant amount of memory disk space and
send anything through so that's that's
actually being super beneficial in terms
of infrastructure our whole stack is
built on AWS so going to cover these
really briefly use them in a fairly
standard fashion and we've also just
just talked about in terms of
architecture we use MySQL on RDS just to
store all our jobs and allows us to do
some pretty simple stuff in terms of
analysis we've got some auto-scaling ec2
groups feature for four different
services
and that's great because we can scale
them independently and particularly here
our trans coders are the ones that we
scale in an unusual fashion so we
frequently changed the number of
instances running at any given time to
meet demand and I'm going to be
explaining a lot more about this in the
next section we use s3 for input and
output storage and keep content for
seven days and purge otherwise and we're
using queuing here as qsr queues for
reliable retry messaging between our
components so that's basically so we
don't lose anything in the middle of
processing so you know if something gets
dark or we can't contact the CDN to
actually put stuff on there and no big
deal stuff we'll just sit in a queue
waiting until everything's available
again the same thing applies if we're
actually redeploying one of our services
queues what messages we'll just sit
there waiting till there everything's up
and ready to receive them again so the
only kind of unusual thing for us is
that transcoders can take up to two
hours to actually do do their work and
that's kind of a long time to be
processing a message so we have to
periodically report back and say
actually yes I'm still working on this
still working on this otherwise we found
that some other transcode it would pick
up a piece of work and we would
duplicate effort and yeah that's not
really ideal so we have to make sure
that only one piece of content is being
handled by one transcoder at any one
time some of the advantages about design
as a small service new deployments are
really easy we like like I said we don't
have any loss of data messaging because
we're using their cues messages to just
wait there until everything's up and
ready to handle them another thing that
we've really found beneficial is the
ability they skew to scale things
independently
and you'll see that very shortly in the
transcoding section as well we can have
dozens of transcripts running hundreds
if you really wanted to and that would
not be a problem it's also really easy
for us to add new pieces without
affecting any of the existing ones we
have been talking about doing caption
extraction so that black bar that was at
the top of the video content before that
we actually crop off one thing that we'd
love to do is to actually extract that
ourselves and the thing is that it
happens to be in a very unique format
and for us to do that ourselves would
actually be no big problem create a new
service do that we can actually run it
in parallel along with all the other
transcoding machines and that would be
fine I also really love the deferred
responsibility to kind of individual
services so having one source of data
for our system has actually been great
because if we do any schema changes or
anything like that and the blast radius
for that is very limited right there's
only one services actually bothered and
the other ones it actually doesn't they
don't really have to have state they get
messages in everything they need is in
that message they do some work and then
they tell something else that I've done
so we really we don't have to worry
about synchronizing things if we're
deploying everything can can go
independently whenever it wants to
things that we found challenging
deployments making sure all the services
are actually wired together correctly
has been challenging so to that end
we've had to really concentrate on
automating stuff as much as possible we
use CloudFormation templates AWS
CloudFormation templates to make
everything repeatable and consistent and
95% of our system is actually templated
we and we also deploy by a CI we make
sure that everything is you know has as
little human involvement as possible
because we have staging dev prod and
there's no way that we wanted to
manually configure any of that by
ourselves
it's obviously more work to build and
develop small services independently
than to just put it all in one big thing
but for us they're really the big thing
that we gained out of this was the
ability to have independent scaling
particularly for our transcoders and
that's what made it worth doing in small
services so that's an overview of you
know what our architecture was on AWS
the fact that we're using ffmpeg and
using our small independent services
there so now we're going to look at one
of the really interesting pieces which
is how to transcode all the content in a
timely manner so as I said earlier we
frequently change the number of
transcoders that we're running and
that's because if we've got a lot of
content to process you know we can spin
up a lot of instances but if we've got
nothing then you know we don't have to
run anything so when should you actually
spin up or tear down these instances
it's not a simple question to answer
this there's actually a lot of different
factors that come into play so the first
of them is how quickly does the content
need to be available we get a lot of
content far in advance but live shows
are an exception so if you have the new
news broadcast
we're gonna have that in advance once
the show is done it gets sent to us
through our transcoding pipeline and we
need to try and make that available as
quickly as possible
we actually Envy pede with one cue that
was a conscious choice so if both high
priority and regular priority content
goes in this queue so if if we get a
high priority piece of content we kind
of need to drain the queue you know we
need to process everything in there to
make sure that that high priority piece
of content is also being processed we
also don't know when people put content
into our pipeline if there was a set
pattern
we could run transcoders at the same
time every day so you know they put it
in between 3 &amp;amp; 8 p.m. and their problem
yeah well we'll start 10 strands produce
20 transcoders just leave in there but
we don't get that so content comes
through at random and we really don't
want to leave them running well there's
nothing to process we also got a variety
of different content lengths so we've
got these you know five-minute videos
that are very short and we've got these
documentaries that are sort of 90
minutes long so you can actually process
a lot of five-minute videos in an hour
but a 90-minute documentary might take a
whole hour on a transcoder by itself in
terms of AWS considerations for scaling
it has hourly billing so as soon as you
started in the instance for two minutes
you've basically paid for an hour you
might as well kind of leave it running
but there's also a balance between cost
and speed so you might start off a very
fast instance and you could transcribe
this piece of content in two minutes but
then it will be idle for fifty eight of
them so you're actually better off in
that instance is to buy a slower
instance and pay less for it and you
know maybe take 40 minutes or something
instead to transcode Amazon scaling
metrics for an auto scaling group kind
of normally around five minutes
monitoring right I take around five
minutes to react so you know in a normal
scaling thing you can say hey if all my
instances here hit this memory threshold
I'd really like you to take an action
and spin up some more instances so that
I can spread the load for us and if
there's hot something high-priority
coming through we actually want to we
want to try and react faster than that
so easy to instances as well will take a
few minutes it's been up so the whole
whole thing going from zero to two this
is maybe around seven or eight minutes
we want to react fast enough
and at the time we built this we
actually didn't have control over which
instances in an auto scaling group got
turned off Amazon would pick the oldest
ones and shut them down and that would
not be good if it was actually running a
transcode at the time we'd have to redo
all that work it is worth saying that
they now have instance protection
available so you know you can say hey
this is this is busy doing work please
don't turn it off but this was done in
late 2015 and we didn't have that
available so instead of trying to
address everything here we focused just
on our MVP don't make things way too
long for a variety of reasons we chose
to have one queue so we have both
regular and high-priority stuff in there
don't make things wait too long in this
queue in order to make that that queue
drain - to actually flow we've got to be
proactive about supplying enough
capacity to handle everything in the
queue at any given time so we know it's
going to be processed within a
predictable time frame so proactive
supply is needed in two cases one if
it's high quality because that's going
to take a long time it's going to block
stuff behind it two is if it's
high-priority content so things like the
news that we really want to get up as as
as soon as possible
so if you remember this profile from
Elia it actually tells us those
conditions right it can tell us when
it's high-priority or when it's high
high quality so if it's high quality
content it's going to take a long time
to transcode start a new transcoder for
each high quality instance that we're
actually trying here if it's
high-priority content most high priority
content has only for lower quality
renditions this is a weird business
quirk but it's actually super fast for
us to transcribe those for lower quality
on on there in a single instance so we
only spin up one to handle it so there's
two cases they're high quality high
priority we're specifically starting out
new Transpo
to handle those situations if it's a
neither high priority nor high quality
we want to check the ratio of jobs in
the system - how many transporters we've
got running and if that ratio is fine we
leave it otherwise if the ratio is too
high we'll spin up a new instance so
that we can handle that if there are no
transcoders running then we want to spin
one up to ensure that the pipeline is
moving so there should never be a time
when there's a queue content without a
transcoded running one thing I should
also mention here is that we we do get
different numbers of renditions
producing from each individual video
that comes in so the reason why we might
only have two renditions coming from a
particular file is it might come from a
different source
so we'll look we can take to high
quality renditions from something and
for high quality or addition lower
quality renditions from a different
thing and stitch those together and
those are the six that will actually end
up putting live on I view so to adjust
the number of transcoders that we've got
running it is one line of code the
orchestrator decides how many instances
is the ideal number to have and it will
set the capacity on the group and AWS
processes that request and we'll have
our new instances starting pretty much
immediately so it's very it's much
pressure a little more preferable for us
to do this instead of waiting for auto
scaling metrics to kick in so the next
question is what ec2 instance size do we
actually want transcoding is pretty CPU
heavy and so we also want to think about
cost versus speed of these so we wanted
to look at the c4 series of instances
from AWS which have the best price to
compute power ratio in them and this is
a graph of the analysis that we did to
show amount of time to transfer to a
one-hour piece of content versus how
much those instances
we're going to cast and you can see that
inverse relationship you know between
cost and speed we wanted to find an
instance type that was going to suit our
requirements the best the majority of
our content isn't time pressures if it
takes a little longer to transcode it
that's kind of okay again this is kind
of a balancing what do we need for the
majority of everything versus how can we
produce stuff that's going to make our
high high priority content flow through
that I'm going to remind you that we are
publicly funded for cost efficiencies
actually really important so we actually
decided to go with the C for extra-large
since the majority of our content isn't
speed driven that we really wanted to
save save the difference the next is to
size up to double the cost the last
question is when do you decide to
decrease capacity so scaling up is
pretty easy if you get a lot of content
and you don't have capacity you can add
more instances but scaling down requires
knowing when the instances are busy
that's you know you don't want to stop a
transcode that's in process so at the
time we built this if you decrease the
number of instances Amazon would just
choose the oldest ones and kill them so
to start with we needed to actually wait
until the entire pipeline was clear
before we could shut everything down and
if you think that sounds really
inefficient it kind of is so here's an
example of that in action right this is
the number of transcode is running in a
four-hour window so you can see quite a
lot of scaling up and down activity here
but you can see the inefficiency of
waiting until all those content was done
before we turned off everything off so
you can see these these cliffs and you
can also see the effect of us tearing
down things immediately there are time
so we've spun everything down only to
start it up literally at two minutes
later and so ideally we really only want
to tear down
our was up to get the most out of our
instances this is another view of those
same transcoding instances that we
created earlier this is a graph of
lifetime instance in that four-hour
window so this is not just what's
currently running but this is this is
everything that was there so it's
heavily skewed to short-lived instances
that's that's pretty inefficient you
know we're paying for a whole hour of
compute time but we're in the using a
fraction of what's there so the ones in
blue are the ones using more than 30
minutes of an hour and everything else
is is underutilized this is this is a
worse than average picture all right I'm
kind of showing it to us as an extreme
example it's highly dependent on when
content gets put into the pipeline so a
regular consistent feed will produce
well utilized machines but a huge dump
of content in there is going to spin up
a whole bunch at once and then shut
things down so the auto scaling here I'm
gonna pause the content and it sounds
like we left things in a really weird
state here but we're gonna look at costs
and that will explain why we've stopped
so right now we've got a pretty minimal
approach and that's kind of based on
profiles and it roughly tells us how
long a time it takes to transcode a
piece of content or if it's a
high-priority piece of content and we
react based on that so it's an area of
the system where anything we do is going
to anything we tweak or change it's
going to have a huge impact on costs so
we're now going to look at how much it
costs to actually run this a common
question that I get asked is why would
you build your own system when there are
some commercial transcoders available
let's look at a few so if you are going
to use a full service transcoding
profile provider you're going to get a
nice neat front-end you're gonna get
some stuff you'll be able to call them
with stuff things go wrong you'll have
everything transcoded as quickly as
possible so you submit jobs the whole
the whole thing is geared towards giving
it to you as soon as you've dropped it
in the thing is we know for most of our
content we actually don't really need a
ASAP so we've started with this model
where we are producing everything ASAP
but we have the ability to actually not
transcode everything immediately quickly
so we don't necessarily need to pay for
a service that's going to give us
everything as quickly as possible
it's like quite a lot of money a
six-figure figure number that I don't
even know what it was but to transcode
the volume of content that we produced
at the ABC it was a large number there's
also some do-it-yourself kind of
transcoding options available things
like elastic transcoder from AWS and so
there's that's a service where you can
submit files you you have to do a bit of
configuration you still have to write a
bit of code to actually make this work
you'll probably be debugging issues if
they're going wrong and it still kind of
doesn't meet some of our strange
requirements so this black bar at the
top here with asymmetric cropping it's
actually kind of it's not a super common
requirement to want to crop a video in
an asymmetric way and when we were
building this it wasn't actually
supported by a lot of transcoders so
that was a reason why we couldn't do
this we also have our captions format
which is slightly obscure again it's an
ABC quark it's it's it's in a it's in
the format called Opie 47 and it's not
really used by a lot of different things
so it's actually not supported by a lot
of places so again if we wanted to try
and expand our system to do more things
we couldn't use some of these
roll-your-own or DIY systems to do that
through elastic transcoder it's about 45
K
from metro what does it cost for a whole
year of iview content remember we've
solved the problem of paying for ASCAP
content in a cost-efficient way and we
can handle these odd requirements like
asymmetric cropping and we've got the
ability to be able to handle things like
captions in the future so for Metro in
2016 our whole year costs we're 11 point
2 K and that's yet partly because we
skew to some lower end renditions we are
a public broadcaster we are cheap but in
2017 you know with where things publish
a whole bunch of high-quality renditions
and that means that our costs are going
to be higher but we're actually we've
paid for that we've already invested in
being able to scale for that by building
this new system so here's a sample day
as an example for our system we get a
variety of different usage patterns I'm
just going to show you a couple this is
a view of renditions completed per hour
it's 24 hours and so you can see most of
the works done during the day here but
we still have a lot of transcoding
running later into the 9th
so there's news there's shows like a QA
and this particular day costs $42 and 86
cents and the breakdown of it was that
90% of our cost is related to easy to
database is 6% storage file storage is
4% and because you're paying for sqs
messages by the millions you know we
know register on that scale it'll to
show you the varied nature of our output
this is literally the next day this is a
bunch of content that ended up being
scheduled at 3:00 in the morning by some
person or system and so it kind of just
illustrates the we don't know when
things get put in we we can't run things
at particular set times and so the
flexibility for our transcode is to
actually
scale according to content in flight is
super beneficial $25 so I said that you
know these costs are actually a massive
deal for us
the ABC is a public broadcaster we love
that we love the fact that we get the
flexibility that we want but we can
still do this in a really cost-efficient
way and we actually got to the point
where the costs are so small that we
couldn't justify spending more dev
effort on it and so that's why we have
left our now transcoding algorithm as it
is exactly as it is
so we withhold in efficiencies it is
still doing the work that we need in a
really cost efficient way so having just
said that we can't justify optimizations
it's super fun to theorize about what
you can actually do so I'm going to do a
couple quickly so an obvious thing we
can do is actually introduce a priority
queue here right in addition to our
regular queue and our priority queue you
could transcode on a much larger
instance that gets spun up immediately
and the regular content that doesn't
need to be transcoded immediately can
actually just sit waiting until there's
enough to produce yeah to produce a more
constant transfer and full transcode so
that we can avoid this graph where we've
got all these underutilized instances
another thing we can do is actually
transcode in segments so we can divide
the source file into those chunks and
then we can use ffmpeg to actually help
us do that and that means that our time
to transcribe for a particular file is
going to be constant it doesn't matter
if you give us a five-minute video or a
two-hour documentary we farm it out
somewhere and send it off we can do that
in a constant time so this was a prime
candidate for something like lambda
didn't exist in the Sydney region at the
appointment we were building it we
actually did a cost analysis to you know
ship everything over to the US
transferred it there in lambda and come
back but we ended up building it in a
situ so the last thing we can do is
actually remove our reliance
on hard-coded profiles so we can use FF
probe we you know we use profiles at the
moment kind of hard-coded it's not great
instead we should examine each file as
it comes in and actually see the
attributes on the file using FF probe
and so we can see here that we've got
information about the resolution we can
we know that that's not a standard size
so we know we actually have to crop it
we can see that it's got 50 frames a
second that we want an output at 25
frames a second so we know we have to
run a merge filter down to to fix that
and here if we get a stream an audio
stream that's our caption stream this
unknown stream here so ffmpeg doesn't
immediately recognize it but if we see
that we know that's captions hey we can
we can actually extract them as part of
our process so just to wrap up right
transcoding is a really important part
of preparing content for streaming on
services like eye view so our new system
was designed specifically to be speedy
but cost-efficient we have small
services that we ended up building in
node and in gold and we use ffmpeg to
handle our transcript in work and we
rely on AWS services for resiliency and
scale so in terms of scaling you know we
take advantage of different techniques
here so that we can meet changing levels
of demand but save on costs when we're
actually not doing anything with those
instances and we really aim to deliver
all of that within a very reasonable
time frame and with reasonable cost so I
really hope you've enjoyed this overview
of transcoding and how our transcript in
system works thanks very much for your
time I really encourage you to please
rate or ask questions on the app and
I'll be able to answer them in the
arrested Dibble section next Thanks
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>