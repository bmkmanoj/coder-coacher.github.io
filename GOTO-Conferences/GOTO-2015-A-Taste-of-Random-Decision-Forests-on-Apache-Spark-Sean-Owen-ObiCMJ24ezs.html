<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • A Taste of Random Decision Forests on Apache Spark • Sean Owen | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • A Taste of Random Decision Forests on Apache Spark • Sean Owen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • A Taste of Random Decision Forests on Apache Spark • Sean Owen</b></h2><h5 class="post__date">2015-10-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ObiCMJ24ezs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you thank you and good morning I'm
here to talk about two things and I hope
at least one of them is interesting to
you number one is Apache spark which I'm
sure everyone in the room is by now
heard of it's a the new hotness in the
Hadoop ecosystem it's a new processing
paradigm and has a lot to offer I think
for the data scientists in the room as
well but I also want to talk about
decision forest which is a common and
important algorithm and in machine
learning and data science and one I
think everyone should know a little bit
about so so hopefully you're here to
learn about at least one maybe maybe
both at the same time let's begin I want
to start by backing up a little bit and
talking about the the roots of
prediction or where we got this idea of
predicting some data from other data and
I like to go back to this picture this
is this was drawn by Sir Francis Galton
in the 18th century I think he was
studying the relationship between
parents and children specifically their
heights and he noted that tall parents
have tall children surprisingly what he
was really getting at though was that
very tall parents have tall but not very
tall children and he called this
regression to the mean and we still do
talk about regression to the mean
but interestingly he tried to plot his
data and he'd plotted parents I think he
might have flipped the axes relative to
what we're used to but parents versus
child heighth and found there was a sort
of linear relationship between the two
and the slope was less than one but I
think this is really almost an example
of an early predictive model the idea
that we could say something about the
child's height given the parent site as
input and this act of drawing a line to
establish that linear relationship we
still call that regression for this
reason even though it really has nothing
to do with regression to the mean or
moving backwards or anything so there is
something interesting about this
particular picture historically now if
you think about it it's it's a this
Dalton maybe unwittingly was positing a
black box there's this chain of
relationships core
relations maybe causations between this
input and this output we don't know what
they are I don't think he was trying to
explain exactly physiologically why
parents have tall children but he
thought there was a relationship there
and he showed the relationship he tried
to describe that black box in this
linear relationship and really this is
how most of machine learning or
supervised learning thinks of the world
more particularly there's a very common
data set out there called the iris data
set that people have used in research
for years and it's data set that
describes a bunch of irises or flowers
and gives their measurements their
people and petal lengths and widths and
their species and we think that there's
some relationship between the two that
excuse me they given some of these
measurements that's causes or correlates
with the the type of flower that it is
and we don't know why we don't need to
know why in order to model that
relationship or to describe that black
box that connects these inputs to that
output and in fact that's the magic of a
machine learning that we don't actually
have to understand the relationships we
can we can just try to model them and
learn them empirically from data this is
an example from the iris data set itself
the data sets not important but it's a
good example of just about every normal
machine learning data set out there you
have a bunch of examples of inputs plus
outputs in this case it's four columns
of numeric measures these are the the
lengths and centimeters of the sepal and
the sepal and pedal lengths and widths
and the species the outputs and this is
the thing that we're trying to predict
given the inputs and again we believe
there's this connection between the two
and we want to discover what it is not
discover it but describe it and model it
and so we given enough of this enough of
these examples of the inputs and outputs
we believe we can learn what how that
blackbox behaves and this is the essence
of just about all standard conventional
machine learning given enough of these
feature vectors we can build a model
this is a an attempt to plot this data
set now it's really four dimensional but
here I think only I've just plotted
three or picked up a plot of three of
them
engines so each point in space three of
its dimensions are termined determined
by three of its inputs and the color and
shape is the output or the species and
you and I can see that the iris setosa
exists in a fairly distinct area of
feature space intuitively we understand
that when the measurements are put a
point over there it's probably a zoo
Tosa and indeed a lot of classic
classification algorithms and machine
learning are all about drawing lines
trying to figure out what's the line
between this data of one type and that
data of another type because if we could
draw this line effectively we could
effectively say that stuff over here in
feature space is probably this type of
flower at this type of output and that's
a that's a form of predictive model so
if you've heard of logistic regression
logistic regression is really a linear
model it's trying to pick some best line
or really really hyperplane in space
that's dividing some types of the
outputs from other types of the outputs
if you've heard of support vector
machines it's also a linear model that
tries to draw maybe a slightly better
hyperplane that's maximizing the margins
here between the the classes and the
line and then if you've heard of support
vector machines plus the the kernel
trick that's an attempt to draw an even
better line that's maybe not linear in
the original feature space so we can
draw curved lines in space and again
each of these the idea behind each of
these is let's just draw a better and
better boundary between the outputs
we're trying to predict to discriminate
them and this is good and this is
powerful but this is not the only form
of algorithm or decision boundary we can
construct to try to discriminate classes
or outputs and indeed decision trees are
not of this form and but there's still a
interesting and powerful idea for
classification so I want to introduce
them briefly so to introduce decision
trees I want to give a quick toy example
so imagine you are a robot that works at
a pet store and you need to know which
pets are suitable for children and you
don't know why
pets are suitable for children or not
but the owners
told you the following you can you can
measure the following the attributes of
the pets in the store and you've been
told in some cases that they're yes
suitable for children or no not now I
think you and I can kind of intuitively
understand that a huge elephant is not a
good pet yes maybe a poisonous millipede
is not a good pet yes but the robot
doesn't know these things the robot can
only learn empirically on on this data
that we've given to it now one valid
approach if you are the robot is to try
to come up with some rules of thumb
based on this data that might
discriminate the yeses from the nose for
example you might indeed see that weight
seems to matter the you know very heavy
pet seems to not be a good pet in these
two cases so so maybe that's a good
place to start maybe we try to make a
decision rule based on weight so we
start there and maybe this is the first
decision rule we come up with if it's
over 500 kilograms it's not suitable if
it's if otherwise it is and that would
be right in about half those cases
actually that's not bad but it's not
really any better than chance so that
that doesn't seem like a good enough
decision rule so we can make this more
elaborate maybe maybe try different
thresholds maybe you know hundred
kilograms is a better place to begin
discriminating and maybe amongst the
smaller animals we need a further
discrimination and another rule that
based on color may be so you can make a
more elaborate decision process that
with several layers here and get more
answers correct
for example this decision rule while
still pretty simplistic gets seven of
the nine examples correct and that's
that's better right and you can imagine
that with a couple more steps here we
could get a hundred percent of the
examples correct which may or may not be
a good thing actually we may we may over
fit the data but I think the process is
fairly clear we want to keep chopping up
the making decision rules and layering
them in order to accurately reflect the
input we have and there's some process
here by which we could do that and the
nice thing about the output here is it's
it's fairly intuitive to read off and in
fact
as a classifier this is also a a model
of the relationship between three inputs
and one output so this is a valid
classification algorithm as well of
course the question is how do we build
these trees and that is in fact all of
the complexity of decision trees how do
we pick the right tree how do we know
when to stop things like that so for the
curious if you think about the rules
that decision trees create simple
decision trees end up drawing decision
boundaries like this in comparison
access aligns boundaries they keep
chopping up space into into halves in
this case and two via binary decisions
keep chopping up space recursively along
along certain dimensions so that's
qualitatively a different type of
decision rule you can get out of
decision trees then you can out of a
linear model and this has in some cases
this is more powerful this is a more
powerful way to to model decision
boundaries here for illustration I've
built two different classification
models this is just an R this is not
SPARC on the Left we have a support
vector machine model on the right we
have a decision forest model and in both
cases we're just going to build build
based on the IRS data set which is so
common and it's built into our as IRS so
very simple to do in R at this point in
both cases we built we build a model and
it's a perfectly fine classification
model they both have high accuracy on
this data set but interestingly if you
look at the model and try to interpret
it based on its string representation to
find that linear models are a little bit
hard too hard to reason about these are
the support vectors and they have a
meaning and we can we can understand
them and maybe interpret them but it
does take a little bit of time to think
about what this means the meaning of
this even change is based on what
exactly you did in the support vector
machine so linear models are maybe not
trivial to interpret whereas decision
trees are much more straightforward you
can almost read off the logic in in
plain language
at the top the decision tree starts by
asking whether the pedal length is less
than or equal to one point nine
centimeters if so I think this means
predict the the first class the first of
the the three species otherwise it looks
at the petal width other and if that is
small enough it checks the pedal length
and so on so you could you could easily
reconstruct the logic and explain to
anyone what the the logic is that led
you to one decision or the other and
maybe as a botanist you could evaluate
whether these rules make sense sure but
but at least we can all we can read them
off and explain how the model is
relating inputs to an output and that's
often also powerful especially in think
about finance or regulated industries
where you do need to explain why your
process approved someone from for a loan
or not so this is this is a non-trivial
consideration in modeling now of course
as advertised I want to talk about
decision trees as implemented in Sparks
machine learning sub-project which is
called ml live at least the classics
machine learning component is called ml
Lib and it has an implementation of
decision trees and actually decision
forests as well which we'll get to so
this is the part where we start to show
what it means to build a real decision
tree at large scale using spark for this
example I'm going to use a data set
called the cover type data set which is
a happy coincidence also about trees and
forests it's five hundred eighty
thousand examples each of which
describes a piece of land I think in in
Colorado in the United States has a
bunch of features about that piece of
land like its elevation and slope and I
think humidity and distance to water and
maybe sunlight and so on and it also has
the type of forest covering that piece
of land is it
no trees is Aspen spruce etc this may or
may not be interesting to us as a
problem but it'll make a good example
can we actually predict based on the
attributes of the piece of land what
type of forest is probably growing on
that lands this is again a reasonably
well known data sets and the machine
learning community so it's mimic a good
example
as a benchmark as well so let's look at
the data well the data is a bunch of
feature vectors a examples and as before
we have a bunch of inputs and one output
and I've printed it an example here and
formatted it in a particular way to make
a few points about it the first ten
columns or ten features are our numeric
and they're measures of things like
elevation and slope and I think distance
to water and maybe to the nearest
highway I forget what they are that it
almost doesn't matter the next four
features and the next 40 are actually
respectively just just two features two
attributes these four describe the
wilderness type is that desert arid or
scrub I forget it's there's four types
of wilderness that a piece of land can
be so exactly one of these four columns
is a one and the others are zero so
really this is a categorical or just
discrete value that's been encoded as a
series of numbers and we'll come back to
this point but likewise for these forty
this is actually one of forty soil types
so there's forty features here but
they're really conceptually one feature
because all of them will be zero except
for one which will be a one it's like an
indicator Vivint flag and last but not
least we have the cover type this takes
on one of seven values and these are
encoded as a number from 1 to 7 so
interestingly there's there's really
three non numeric values here encoded in
two different ways such as life this is
this is real data and to begin to work
with it we need to convert it into a
form and a representation that the spark
ml lip implementations can consume and
for spark that's called a labelled Point
which is really just a double and an
array of doubles the label is the output
and happily it wants even categorical
features encoded as a number and we've
already got that ok and then these are
the the rest of it is the feature vector
all the inputs encoded as as a vector
which is really just an array of doubles
so thankfully we this that should be
pretty straightforward
that's already how this data is encoded
actually so this should be pretty
straightforward to to make objects like
this out of the input let's do it may be
a little bit hard to read but I'll walk
through this this is actual spark code
this is using Sparks Scala API and if it
looks like regular Scala it's because
spark really is at heart distributed
Scala so a lot of the operations that
are actually spark API calls here are
going to look just like Scala collection
API calls and that's a good thing import
some classes first thing we'll do is
open or get a reference to this data set
which may live on HDFS or your your
clusters distributed file system and in
SPARC just about everything is an RDD a
resilient distributed data set that's in
a mutable distributed collection of
things and in the case of a text file
that mean spark can use that as a RDD of
strings so a text file is just a big
collection of strings see one per line
right that's R that's so raw data is an
RDD of strings we're going to transform
those strings into labelled points using
a map here for every line split on
commas so we make a tokenize it and we
map each of those to a double so now
I've turned my CSV formatted line into
an array of doubles I'm going to take
all but the last one and make a vector
out of it so this is my feature vector
I'm going to take the last one as the
output or label and subtract one from it
since SPARC wants them to be zero based
not one based and there we go I make a
labelled point out of the label and
feature vector and that's my
transformation process so you can type
this code into the SPARC shell directly
no need to compile a program and if you
type that and you type that you'll find
that nothing really happens because
there's been no action yet so we've just
described a process that would transform
a data on HDFS into this set of objects
in memory but we haven't asked it to do
anything yet before we go and do
something with it and build a decision
tree model however we're going to
actually put aside about 10% of the data
for
for testing purposes that is we're going
to train the model on most of the data
and hold back a little bit for four
reasons I'll show you on the next slide
in order to test the model last but not
least it's interesting to note that we
should cache the training data
this tells spark'd that once you compute
the RTD do you keep that result in
memory in the cluster so take those
those label point objects and actually
persist them across the distributed
cache that the sparse executor is have
and this will be important because this
process is going to access that data
over and over and if it's in memory
that's fast and of course this is why
people say SPARC can be a lot faster
than say MapReduce because it can use
memory so use more memory to in order to
gain speed and that's a good trade-off
in general these days all that done all
that set it aside then building the
decision tree model is just one method
call here this isn't she train
classifier past the input data we have
to tell it in this case that there are
seven possible label values and this
we'll cover later this is concerns which
values are categorical and we give it a
couple hyper parameters which again I'll
explain it in a moment
and the output is a decision tree you
can print it and again as in the our
code you see that's we can sort of read
off the logic here given this API it
doesn't actually know what these
features are we haven't given them name
so all it can do is call them feature
zero one two three but again this is
this is elevation for example so the
first decision is is the elevation less
than three thousand forty two feet and
so on we could read off the logic and
see this nested tree structure in the
same way so that's nice so we've gotten
a nice decision tree out of this but of
course we didn't want to just build the
tree to build the tree we want to build
the tree to make predictions and looking
at that decision tree certainly we don't
know whether it's a good model even I
think if we're an expert in the in the
field I don't know that we could look at
that and decide whether that is the best
model so we need some way to evaluate
whether we've built a good model whether
this thing actually will predict other
forest cover types from new data to do
that we need some
some theory of evaluation and that's why
we held out some of the test data sorry
some of the input data as a test set
because that's new data or data the
models not seen before for which we have
the right answer and so we could see how
often the model predicts the same answer
that is we know to be correct from this
test set so that's what we're doing here
I'm gonna make a method that takes in a
model I've built and a test set an RDD
of labelled points and for each one I'm
going to take that point and map it to
the result of predicting from the model
and the known correct answer so the
result of this function are two poles or
pairs of predicted answer and correct
answer and spark even has a little bit
of support for automatically analyzing
those results to get some metrics out so
that's what I'm going to return and I'm
going to use that on my model and test
data and then print out some of the
metrics that that come out from this
this evaluation process so one thing I
can get out of this is a confusion
matrix and a confusion matrix is
effectively these are correct answers by
row so here's all the seven possible
outputs and the seven possible
predictions the model could have made
and ideally the models always writes and
therefore as the count the number of
times that the correct answer was
predicted to be such-and-such an answer
so ideally the only that diagonal would
have the values and everything else
would be zero that would mean every in
every case the model predicted the
correct answer those the correct cases
are the diagonal counts here and that's
mostly true as you can see but not
always and the confusion matrix helps
you read off some attributes of the the
model like classified was never
predicted for some reason don't know why
but maybe that's interesting so in the
main it looks like the models doing
something reasonable but but it's not
always right you cannot see a number of
errors here and here for example but
maybe then one single interesting value
out of this is the the accuracy how many
times was the model correct and that's
that's about 70% that's effectively the
sum of the diagonal over the sum of
everything here
correct out of out of the total 70%
sounds all right it's better than one in
seven I guess but is that good could we
do better is this better than random and
that's always I think a good way to
think about these things when you get a
results it seems okay benchmarking
against something else something simpler
maybe after all if this is no better
than random guessing why bother with a
decision tree model let's just make
random guesses and do as well
so let's actually benchmark this against
random guessing and to do that what we
could do is look at our training set and
count up the number of times each of the
seven outputs occurred and then guess in
proportion to those counts for example
of class one was present in the data
fifty percent of the time then 50
percent of the time I would just guess
that the answer is class 1 and some of
the time that would be right just like a
broken clock is correct twice a day it
might be might be even correct quite
often who knows so we can implement that
pretty easily by counting here's another
example of sparking action counting the
number of times each label occurs so
that's us taking the label out of each
point and counting them up and then just
producing proportions or price are
probabilities if you will out of that so
this is just counts divided by the sum
of the calcium I do that for the Train
and test set and then put them together
so we know the Train probabilities
dictate how how frequently we'll guess
one class versus the other and the test
probabilities give us a distribution of
the actual correct answers in the test
sets and from that we could we can infer
how often we'd be correct with this
random guessing and even make a pseudo
confusion matrix here out of this
process which is going to be symmetric
of course and it is worse random
guessing I guess not surprisingly is not
as good it's right about 38% of the time
so okay so out of the box the decision
tree algorithm is doing better than a
very simplistic random guessing
algorithm so so that much is good so we
be doing something right but can we do
better and yes we can for sure so let's
let's do better part of doing better is
tuning the model and I told you there
were three parameters here I'd explain
later and these are these are hyper
parameters to the model they're called
hyper parameters since they're not
things the model learns but things you
have to give as parameters to the model
building process itself so whereas the
model will learn which nodes to put
where and which thresholds to choose
these are things we have to our inputs
to the model building process itself
that we choose maximum depth for here is
easy to explain that's the maximum depth
that the tree will expand to and you
want to limit that because as you go
deep build a deeper and deeper tree that
chops up the data into smaller and
smaller subsets eventually you're only
fitting noise and that's not helping and
it's only helping you over fit the data
you have in hand and is unlikely to help
you make good predictions in the future
that and it takes extra time and it
makes a bigger model which it takes
longer to score so this is the crudest
way and the only way this implementation
supports at the moment to know when to
stop expanding now you can also maybe
look at when expanding the node seems to
not help much because it doesn't gain
you much information or decrease in
purity much and that brings us to
impurity so actually maybe I'll explain
the minimum bends first in the initial
example I tried two different thresholds
for the the weight should I try a rule
based on 500 kilograms or 100 kilograms
and in an in a algorithmic process I'd
want to try many more possibilities and
pick the best one and that is indeed a
key part of what the decision tree
algorithm does it tries a bunch of node
expansions and and decision rules and
picks the best one and then moves on
through the tree but but what does best
mean I mean yes I can tell it to try a
hundred different values at every node
but but which one is best
how do I know which thresholds is the
right one to choose out of those hundred
that I try and that's where the impurity
measure comes in impurity is a is it
going to be the Krait
area by which we judge a split in its
quality there's two possible values for
the impurity measure one is Gini
impurity and one is entropy I put up the
formula is not - because it's so
important but to illustrate that they're
not actually that different even though
their interpretation is probably quite
different than Gini impurity I think you
could most quickly view it as one minus
the probability that a random guess is
correct within that data set so what
does that mean if the dataset is very
mixed and has a little bit of all of the
outputs then a random guess is unlikely
to be correct and so this value is high
and that's bad because we kind of want
decision rules to break up the data into
regions where the output is clear
there's only one type of label or output
presence in the subsets so we do want
lower impurity you want a decision rule
to take a high impurity set of data and
break it into two subsets with low
impurity we could also use entropy as
that criteria entropy is an information
theory concept that measures the in a
way the mixedness of a set of things and
again if you have a lot of a little bit
of everything in a set that has high
entropy in a sense you could say that's
because it's we predictability of the
outcome at that point is high the
unpredictability is high so again we
would love to make subsets from a
decision rule that have low entropy that
I have just one output or the other
output because then we'd be we'd be
driving to a clear decision from the
model so in both cases lower is better
and the formulas mean ignoring this
constant one which doesn't matter
they're both like a negative
weighted average of something in this
case the log of the proportions in this
case the proportions themselves so
they're there in a way related but this
is a bit of a sidebar the question is
which one's the better measure for this
data set and we don't know we actually
don't know just looking at this so what
do we do I mean how do we know which
measure to choose which maximum depth is
right or which number of bends to try
and this is a general problem in model
building because models will have high
program
and we don't know necessarily which ones
are correct because that the best value
is going to depend on the data set so
the answer thankfully in Hadoop is just
try them all or try a lot of them and in
spark and and it's specifically the
Scala API this is really easy to do so I
can write a triply nested for loop here
that that's what this is in Scala that's
going to try all possible combinations
of two values for these three hyper
parameters so I'm going to try both Gini
impurity and entropy I'm gonna try a
depth one which should be very shallow
and depth 20 which should be very deep
and maybe I'll try building 10 bends
versus 300 bends and so there's eight
possible combinations here and I'm going
to build a model for each one of them
here and evaluate its accuracy just like
before and return the three values I
used and the accuracy at the end and
then I can just take those and sort them
descending by this accuracy and print
them out so if you run this I found when
I ran it that's uh I got about 91%
accuracy with entropy as an impurity
measure depth twenty and trying three
hundred bends that's not bad I mean
clearly that's better than this
combination which is only 63% and it's
better than my initial guess of geni for
100 right so just I think it's kind of
nice that just running this process
going and getting a cup of coffee you
can let this expensive cluster do the
work for you and improve your
predictability and in fact you could do
this all day
you could try many more combinations you
could be smarter about this exploration
of the hyper parameter space but the
point is that you can we can often throw
Hardware and raw brute force at a
problem like this and have it do some
work that's otherwise very very complex
or hard for us to know or guess and
that's that's the premise and promise of
Hadoop so I guess we might observe that
generally speaking and trying more
values more bends is better which makes
him sense right if I'm trying more
possible thresholds I'm more likely to
get a better
best answer and the expense there is
simply that takes more computation it
takes 30 times longer to try 300 values
versus 10 so that straight ups more
obvious but the max depth and the
information measure that is not nearly
as clear which one to pick so it's nice
to be able to appeal to brute force on
with it sparking with a Duke and the
nicer thing still is that I mean this is
a parallel process this will build in
parallel across your cluster but if you
wanted to really make your cluster heat
up you could stick a dot par in here
somewhere in in Scala and ask the driver
program in Scala to evaluate all of this
at the same time in parallel so in
parallel build eight models that are
themselves parallel processes on the
cluster so you could easily fill your
whole cluster with this process all at
once which is a good thing if you've got
the hardware it's nice to be able to
type a few characters and make full use
of it to get an answer faster
parallelism within parallelism can we do
better yes we can do better to get
better from here we might need to not
just apply brute force but but think a
little bit through the process and make
some improvements to our data and a
representation so that will also give us
a win and I want to come back to this
idea of categorical variables so as I
say the really the outputs a categorical
variable it's a soy it's a cover type
there's no ordering to it there's no
such thing as spruce being greater than
Aspen or spruce being one less than oak
but we've encoded it as a number and
also there's two other values in the
data set the soil type and the
wilderness type that are also not
ordered values but they we have to
represent them as numbers or what we've
had to and the way they've been encoded
in this data set is called one hunter or
one of it encoding the practice here is
to take a set of a categorical value
that takes on n possible values and
encode it as n indicator variables that
are zero or ones so for example if I had
a categorical value that took on three
possible values cat dog and fish I could
encode that as three features at exactly
one of what
was one and the rest of which are zero
so you can think of this as like an is
cat is dog is fish feature and it turns
out that's a reasonably principled way
to encode non numeric data as numeric
for most algorithms purposes that's a
pretty good encoding the only downside
is if this has a lot of distinct values
this blows up and becomes pretty hard to
manage in this case it's pretty
reasonable
we had one value with 40 values but that
wasn't so bad now the thing is this
process is necessary if you're using an
algorithm that can't consume categorical
values directly for example the support
vector machine decision force can
consume these categoricals directly or
they have a principled way to to
evaluate rules based on categoricals
without this encoding so in a way this
is this is not optimal for a decision
tree so we can undo this encoding and
then try again and see if that actually
helps with a different representation so
conceptually we're going to collapse
these four and these 40 features down to
two this is going to become the index
that was a 1 in the set of 4 and this is
the index that was a 1 in this set of 40
so here we're again still representing
these non numeric values as numbers so
we have to be a little careful to to
realize these aren't numbers but we can
tell the decision tree algorithm to
treat those as categorical values and
and proceed accordingly so that ETL
process is still pretty straightforward
let's split the line we're going to chop
off the 4 and 40 values figure out where
the 1.0 is and add those back as
additional features on our feature
vector and proceed as before to return a
labeled point easy and then try again
the only difference here is we've we're
telling the input that that tenth
feature in that eleventh feature are not
actually numbers they're categorical
values taking on 4 and 40 different
values respectively so you have to tell
this particular implementation about
that ahead of time other than that it's
the same we can run the same hyper
parameter selection process and come out
with new answers
so in this case the best result was
about 94% accuracy with similar settings
I'd say I tried some different values
here depth 3,300 bends so that's not bad
I mean it's clearly seems clearly like
we're getting better results from from
this process and I guess the reason is
that we're losing less information by
collapsing conceptually one value down
to one value and letting the algorithm
treat it accordingly rather than
encoding it and sort of obscuring the
the data so data representation does
matter and this is a another argument
for decision trees that they can consume
these non numeric features directly can
we do better yes we can still do better
than this now this is where decision
forests come in so as you've probably
guessed that this isn't forest is a
bunch of decision trees put together and
in particular we're going to talk about
random decision for us now to set the
stage for this I want you to think
take a guess don't don't don't don't
tell me don't tell your neighbors
how many black cabs operate in London
think for yourself I don't know I took a
guess like you are right now and I was
wrong of course but then again most
people are wrong but we've we all
probably made some educated guesses
so I the real answer is is about 19,000
I guess about 10,000 maybe you got
closer than I did I asked around our
office and got some really starting ly
bad estimates of the number of taxis in
London but interestingly if I took the
the mean of all the guesses the mean was
well closer to the correct answer then
my answer was and in fact it was closer
to the correct answer than most answers
were in fact so even though lots of
people were quite wrong on average
people were reasonably right this is the
so-called wisdom of the crowds if we
average combined some partially educated
guesses we end up with a better guess
overall and so this is the phenomenon we
want to leverage in machine learning
when we talk about ensemble techniques
where we're actually putting together
many classifiers that may have different
opinions about the right answer and
producing one one answer out of that
that's maybe better than in the
individual classifiers output so let's
see if we can use this phenomenon to
improve improve on decision trees yes so
this isn't for us leverage the wisdom of
the crowds that's the key takeaway here
now the problem here is everything I've
described so far as deterministic so if
you give this algorithm the same data
and same hyper parameters you'll end up
with the same tree every time and that
doesn't help it doesn't help to have 100
copies of the same tree giving a
prediction because they're all the same
it's as if I have just one tree what we
really need is a diversity of opinion
here we need a bunch of slightly
different trees giving slightly
different opinions good guesses good
predictions but maybe slightly different
ones that we can average together into a
better overall prediction so how do we
do that how do we actually get different
answers out of the same algorithm and
same data two things number one we can
candy cap each tree a little bit by
showing it only part of the data so for
example you could make ten trees each of
which look at a different 90% of the
data and for which a different 10% of
the data is held out so each individual
tree is going to be a little bit
different since it's seen different to
input it's going to be a little bit
worse off than a tree that looked at all
the data since it has less input but
strangely the the average guess over all
those 10 cheese is going to be better in
general than one tree built over all the
data that's powerful idea so and this is
where the the the idea the motivation
for random forests comes in we could
also do the same thing but with features
you could have different trees look at
different subsets of the input and make
different predictions based on based on
that
for example this tree could look at just
the elevation and slope and so on this
one might be looking at soil and
wilderness type and they would get
different answers indeed but maybe
overall the
the combined answer is it any better you
can do both you can slice it up both
ways in fact you can slice it up to some
degree randomly which also generates
some degree of difference in the the
outputs of the trees and this phenomenon
makes decision for us the building
process especially nice for Hadoop
Hadoop is fundamentally a data parallel
paradigm where we have to live with data
distributed it all across a cluster and
in a world where it's relatively
expensive to move data around so we need
to do small operations on small subsets
of the data and combine the answers at
the end and that's exactly how decision
forests work the idea is to build weaker
trees on small looks at the data
independently and then combine them into
one ensemble or forest at the end so
that's another reason decision forests
are nice in a data parallel paradigm
like Hadoop so if we do something like
this we will find we get different trees
not just different thresholds throughout
the tree but totally different
structures trying different features
making different predictions etc so we
will get a diversity of opinion from
these from these trees and we can make
hundreds of them or even a thousand if
we want and then to make a prediction
out of all of them we are simply taking
a majority vote for example asking each
of the trees for a prediction and
predicting the one that occurs most
frequently pretty simple stuff we can
thankfully do that in ml lab as well
this is also implemented in in SPARC
very similar except that there are too
many parameters now one is the number of
trees to build and another is called the
feature subset strategy this is really
the means the how it chooses how many
and which different features to randomly
try at each level and to some degree it
also controls how the how the examples
are apportion to the trees so in a way
this controls how how different pieces
of the data get fed to different trees
for purposes here we'll leave it at Auto
but there are some other settings and
the rest is the same nose same hyper
parameters as for the decision tree
model and if you go through the same
process and try a bunch of hyper
parameters and trying
different numbers of trees you can get
pretty easily better accuracy than than
we had last time I got about 96 percent
accuracy with the same process with just
a couple hours of tuning on my cluster I
imagine if you tried a little harder you
could get 97 percent I don't know I
think the the best algorithms I've seen
for 4-cup type have an end up with that
maybe a 2% error rate or so so this is
pretty close to state-of-the-art for
this particular data set that's not bad
for a simple algorithm that's implement
for you that you can use brute force and
a bunch of cluster power to tune for you
and get nearly state-of-the-art results
and indeed this is random forests are
generally a a pretty competitive
algorithm for a large range of problems
so definitely worth knowing about
definitely worth learning about in the
context of Hadoop and spark now that's
not the end of it though let if you've
gone to all this trouble of preparing
your data and making labelled points you
can easily try other algorithms in ml
lip for example there is an
implementation of the support vector
machine or logistic regression and they
take the the input as the original input
we showed is something you could easily
feed into these algorithms as well and
you could easily put those into an
evaluation loop as well and see what
their accuracy is like so that's another
nice thing about using ml lib it does
have a fairly standard and cohesive set
of implementations with the same sort of
input in same sort of outputs so you
could easily compare and contrast them
another fun thing about SPARC is that
it's not just for ETL it's not just for
machine learning it has a streaming
component called spark streaming which
is spark as applied to a stream of data
coming off of a message queue like Kafka
for example and whatever you do in spark
ml lab can be used within spark
streaming so that decision tree model
could be directly reused in the spark
streaming job that would maybe score
data make predictions on data as it
arrives in the cluster now maybe that's
not so useful for forest cover type but
it might be useful if your model is
predicting for
if it's detecting anomalies and sensor
data for example so it's nice to be able
to just directly port that code move
that model into a streaming job with no
with no change or rewriting and maybe
it's also worth noting here at the end
to tie it back to the beginning that
random decision forests are not just
classification algorithms we've been
using it as a classifier here to predict
a a discrete categorical value but they
can also be used for regression which is
means predicting on continuous numeric
value so you can predict numbers as well
as categories with this exact same
technique and that's also implemented in
Sparky ml lip worth knowing that's all
from me thank you very much are there
questions objections sir you shout at
them and decision tree in normal
decision tree is more explanatory than
for example support vector machine or
logistic model how about random forest
because it seems to me that it's not
really explanatory anymore right so a
random decision forest is really just a
bunch of decision trees so it's more
complicated than that you're digging
through a bunch of these trees not just
one but underneath they're all just the
same decision trees inside a forest but
do you then have any it doesn't have any
implications for example using this in
practice because for example I've work
for HR company in man melons and I know
that when we have matching and searching
models then people find it have
difficulty mental difficulty to use it
when they don't know what the results
are coming from do you it is harder for
a decision forest then this then you
know this simpler decision tree model I
think it's probably the interpretability
of the models about the same between a
decision tree and decision for us since
the forest is just many trees but you're
right that's exactly the kind of case
where the trees are nice because often
people either have to or just want to
understand how the models thinking and
you can read that off directly in the
case of a forest or trees isn't then one
final question how does the
how do decision trees and random forests
in general compared to say for example
support support vector machines in in
accuracy precision recall things like
that
how does in general how there's this
perform versus SVM's
I don't think there's one answer to that
I guessed on interpretability aside
decision for us have some advantage in
that they can do multi class pretty
easily and consume categorical variables
that said I generally speaking decision
for us are a brute force approach which
is okay in some cases not in others they
tend to struggle a little bit with a
very large number of features and SVM's
have some tricks that make a very large
number of features work well so there I
think they have just different
properties there's no one answer thank
you question here yes sir so the
question is when I build a decision
forest am I using the same hyper
parameters for every tree yes so I'm
making one call here to train the
classifier and I'm giving it one set of
hyper parameters I can build different
for us different hyper parameters but
within one forest that's constant the
the features and examples they look at
are different across the trees though
okay and will there ever be an option to
change hyperparameters within a forest
or isn't that no would you ever want to
use different hyper parameters within
one forest it's not crazy I think it's
probably not that important you would
generally want to have one consistent
measure within a forest although I guess
the decision for us an example of
ensemble learning where you don't care
what the the the individual classifiers
are you're just averaging their results
so it wouldn't be crazy to combine two
forests that were built in different
ways with different hyper parameters and
average all their results
maybe not not as important or not as
helpful but there's some use for it but
that but note there this doesn't exist
in this API okay there's a question in
the back
right so how its performance affected by
the size of the forest so certainly the
prediction time it grows linearly with
the number of trees since you have to
run each example through all of the
trees you can't do that in parallel on
the plus side it also of course depends
on the depth of the tree but that's
that's less of a factor since that's
kind of you know that time grows
logarithmically with the number of
decision nodes and that's that's not as
generally a problem
so generally I would describe decision
for us is fairly fast to score they're
not as fast as a linear model which is
just doing a dot product maybe but I've
generally not not really a problem I
have a question what a similar approach
could be applied to not two trees but to
a linear model so sure that deliver some
advantages in fact could you put
together an ensemble of linear models
yes definitely so decision forests are
really just ensembles for decision trees
you can make an ensemble of support
vector machine classifiers as well and
would see a similar effect yes I think
that's done as well I think the the
benefit of ensembles is maybe more
pronounced in decision trees or somehow
is a better a nice fit for the way
decision trees work but there's nothing
specific about that idea and that's
specific to decision trees yes one more
okay there is any relation between the
number of features and the number of
tree that should be used okay is there a
relationship between the number of input
features and the number of trees that
should be used that's an interesting one
so it I suppose it depends on how many
different features each tree looks at
because generally speaking each tree
will not consider all the features so
for example for a classification problem
often each tree will look at about the
square root of the number of features
that's I think that's the default in our
and most people have just followed full
of that so given that I suppose if you
only had a couple trees and a large
number of features you would probably
end up not ever looking at some of your
features just randomly because only a
few are chosen so may I suppose you want
to have enough trees that you suspect
you've used all your features at least
once
average was there any occurrence of a
deadlock because that can be a chance of
contradiction of when you combine
different trees there can be
contradiction of different subsets of
features so you know was there any
deadlock I mean it was running for long
and was there productivity
predictability reached or did you face
anything neither is there a possibility
of deadlock in the amphitheater did you
get any deadlock I mean because when you
compare between different trees that can
be a contradiction right because
different decisions by different
features in one tree can be different
from whether a tree so oh do the trees
disagree yes absolutely
so hopefully the trees generally
speaking agree on a prediction and
that's your that's your final prediction
but they won't all agree in general I
guess the nice thing about that is you
could return to the caller not just one
answer but a distribution of predictions
70% of the trees say it's class one
twenty percent say it's class two and in
fact even the trees themselves can give
you a distribution over the categories
and you can average those distributions
so yes there certainly the trees will
arrive at different answers sometimes
but that's fine that's to be expected
thank you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>