<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Applications in the Emerging World of Stream Processing • Neha Narkhede | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Applications in the Emerging World of Stream Processing • Neha Narkhede - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Applications in the Emerging World of Stream Processing • Neha Narkhede</b></h2><h5 class="post__date">2016-10-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WuBQBTET8Qg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello is this work okay post-lunch talk
I know how that works thanks Katie
okay I everyone I'm Neha RK day and the
co-founder CTO at confluent one of the
creators of Kafka and this talk is about
applications or micro sources though I
specifically avoided using that word in
the world of stream processing this is
important because unbounded on ordered
large-scale data sets are increasingly
common today whether it is mobile usage
statistics or sensor networks or merely
web blog data stream data is everywhere
and there is a huge push towards getting
faster results but for a long time would
have observed a stream processing is you
know sort of construed as something that
is only a faster MapReduce layer
something that is way closer to your
Hadoop's
and your data warehouse is something
that requires a whole cluster to run and
you know specifically for niche problems
like faster machine learning or faster
analytics but in my experience of
working in this area for the last five
to six years we have learnt is it
actually impacts a big area of the
company essentially the company's
business logic and applications that in
fact can leverage stream processing and
that is what this talk is about and
where Kafka and Kafka streams which is
Kafka's newest stream processing layer
comes in okay so without further ado
let's get started first off you know
let's start by answering this question
stream processing is this buzzword today
there are many ways of defining it but
today I'm going to use a very specific
way which is that it is merely a
programming paradigm for processing
unbounded datasets now if you look at
programming paradigms there are various
ways of chopping this pie but here I'm
going to use a specific way which is the
way an application gets its input and
the way it produces its output and if
you look at this input output IO
dimension then there are three paradigms
for programming one is request response
there are bad systems and then we'll
come to stream processing so request
response are really simple we are used
to these systems these are our PC
applications you send one input in to
your service and then you wait for one
output back and the only way of scaling
the services by deploying more instances
of it the properties of request/response
systems are that the it's tightly
coupled it's latency sensitive and you
want to scale it out by deploying more
instances now on the very end of the
other end of the spectrum are bad
systems here you send all your input in
you wait for systems to crunch all that
data and then they send all their output
back and the big difference from request
response systems is this loser coupling
and expectation around latency typically
these systems get maybe hours to run or
sometimes days and though the thing to
note about bad systems is that they
assume that data is bounded in nature
you know when you know you know how to
define what all means here okay so then
coming to stream processing systems here
you send some input in and you get some
output back and the definition of some
is left to the program it could mean one
input or it could mean all your inputs
the the output is available at various
times two you might get one output item
for every input item or one output item
for every n input items it is really a
generalization of the previous two
extremes that we saw the thing to note
is that stream processing it assumes
that data is unbounded in nature it is
never-ending and so it's designed around
that now for a long time you know stream
processing
is misconstrued as something that might
be transient or lossy or approximate
some systems have been that way but my
claim is that those are essentially
drawbacks of the way those specific
systems were designed but it is not an
inherent property of the stream
processing paradigm in fact swim
processing systems can absolutely be
made to generate perfect results just
like that systems do as well as do that
efficiently so the reason there is this
you know conflation of problems with
stream processing is because when you
process unbounded datasets there are
various tricky trade-offs involved and
those are around latency cost and
correctness and often times systems are
specifically designed to optimize along
some of these dimensions versus giving
the user the flexibility to pick the
trade-offs that are correct or right for
their application and this matters a lot
because one size does not fit all here
some applications might care about
correctness like billing while some may
not like lot processing some might care
about latency like alerts while some may
not like ETL while some applications
actually may not be okay with paying the
cost for fully optimizing along the
other two dimensions so this flexibility
is important while designing good stream
processing systems this talk is about
you know stream processing for
application development what I'm
interested in exploring is how the
entire company's business logic can be
designed as stream processing
applications and what might be required
if we were to do that so throughout this
talk I'm going to work through this
example this example is from a
brick-and-mortar retail you know company
or industry here you know for retail
companies there are two inputs that are
relevant or important at the very least
and these are sales of things and
shipments of things
and for healthy businesses these two
things are never-ending hopefully and so
these are essentially streams of inputs
they are unbounded they never end while
if you if you see that you know these
two inputs are useful for a whole set of
applications downstream you might send
it to your Hadoop or your warehouse for
analytics you might send it to some kind
of fraud detection system or you might
index it into a search system that can
powers the search feature through your
inventory and then there are two kinds
of processing that should happen
continuously but for several of these
companies today it might not be designed
that way and those are price adjustments
and inventory adjustments this is
essentially what Amazon does a great job
off which is they have a totally
real-time view of what is being sold
what is in demand and they have this
capability of doing both sort of a
adjustments and price adjustments to
take advantage of that demand and so
this is essentially what is what falls
in the stream processing bucket however
may not be designed that way today
and so we'll take a look at that if you
look at this picture a little
differently at the top are things that
fall in the you know what happened
category these are events that are
interesting to this business and
everything else is essentially a
response to that event so as a response
you might send it to your warehouse or
search you know index it in a search
system or might process it using some
kind of a processor now taking a step
back if this whole thing was designed as
one large monolithic application then
what I'm going to say is it doesn't
really matter but in reality
applications are not designed that way
you typically have several services each
service has you know some
responsibilities that it does and so
when you have you know several
applications that need to interact with
each other you essentially have a couple
of choices right if you don't have a way
to buffer these input streams then what
you're left with is you know the need to
tightly couple all
these downstream services with the
services that generate these rights and
if you do that then it has all the
downsides of tight coupling which is
cascading failures or sensitivity to
performance issues if on the other hand
if you had a log or a cue like Kafka at
the center which could buffer your
streams of inputs then it allows you the
flexibility of decoupling all the
downstream services so that they're not
tightly coupled you end up with a much
more resilient architecture and more
importantly what we are going to look
into is it allows you to do stream
processing some of these some of these
actions that should actually be
continuous actions or processing can
happen if you have Kafka to buffer all
this data okay so stream processing you
know tying that back to this
conversation this is it's actually just
a bunch of functions or actions on top
of this what happens sort of event data
and because Kafka has emerged to be a
sort of a de-facto system for storing
event data it essentially boils down
into writing functions on top of Kafka
topics or Kafka streams so you know
Kafka has been there for a while it has
been around for five years it's used
across thousands of companies so over
the past five years a couple of stream
processing approaches have emerged over
time the first one is really what I call
do-it-yourself stream processing here
what people do is they use the basic
Kafka libraries and they build stream
processing logic and code themselves in
their application and if you do that
then there are a couple of problems
related to stream processing that you
should know you would have to deal with
those are you know ensuring that your
data arrives and gets processed in order
ensuring that you can do that while you
know horizontally scaling your
application out fault tolerance so
ensuring that you have all the
guarantees as machines and processes
fail state management some operations in
stream processing like joins and window
dag Rockets it requires maintaining
state so
what happens to state management
reprocessing so the ability to you know
upgrade your application or fix bugs in
your application that essentially
requires you go back in time and
reprocess past results and finally last
but not the least the idea of time the
correctness in stream processing is
closely tied to correct treatment of
time and I'll talk about each one of
these problems and how kafka stream
solves these so the other approach is
you know instead of doing it yourself
you use one of the many stream
processing frameworks that are out there
SPARC has a streaming module which is
quite cool there is storm
samms which some of us built at LinkedIn
there's also an emerging system called
flink which is which works really great
the thing to note about these systems is
that you know they share a certain sort
of MapReduce heritage the idea of making
a faster MapReduce layer and this works
actually pretty well for some subsets of
problems there is a ton of innovation
happening in this space there are you
know because of this MapReduce heritage
there are some traits that these systems
share including the one we built at
LinkedIn which is called Sansa and those
traits are that you know typically there
is a custom way of configuring your
processing code which is the properties
of a job that runs on a cluster that
that stream processing frameworks are
operates then there is a custom way to
package and deploy your code much like a
MapReduce job there is the whole sort of
problem of resource management mapping
processes to machines and there are tons
of solutions for that today this works
actually pretty well for you know things
like you know code that already runs in
Hadoop that needs to run faster
long-running queries like you know
machine learning or long running
analytics kinds of queries that works
pretty well on these kind of systems
what we wanted to do is you know take a
step back and look at stream processing
for application development and if you
look at that then there are
a subset of problems when it comes to
these traits the first one is that for
config management and packaging alone
there are a whole slew of tools right
there's darker and chef and puppet and
salt and that that's a long list for
resource management as well there are
lots and lots of tools mezzos which is
what the other talk is about
there's kubernetes and that whole
ecosystem is exciting and it's moving
along and so if you think about what it
will take for application developers to
use stream processing primitives
you would have to make sure that it has
fewer external dependencies it is
lightweight it can be embedded in
applications and the second thing is
that you won't allow developers to use
the tools of their choice for config
management for application packaging for
deployment and so we thought that you
know if you come back and look at this
problem designing this as a library is
essentially the right way to go if you
want to approach application development
and so that is exactly what Kafka
streams is it is a Java library that is
built on top of Kafka and the reason it
is built on top of Kafka is because
Kafka provides foundational primitives
which are required for shrimp processing
and we look we look into what that means
so copper streams has essentially two
interfaces one is a processor callback
api and the other is a high-level DSL
and both are Java at the moment so
looking at the processor API just to
take an example it's pretty simple you
get a key in a value which is a sense
essentially a message in Kafka you
process it that might be your code and
while processing it you might update
some state we can talk about that later
and then you might send that message out
in some sense this encapsulate the
entire you know scope of stream
processing you have input streams you
have some processing code and then you
have output streams and this is the code
that you write to build one of your
processors you know you ask you
configure it to tell it where to connect
to Kafka you write your processor class
and then you just say start and you can
take this code and either run it
on one instance or one process or you
can take it and run it on multiple
processes Kafka streams manages the load
balancing of the data
now the second API is this might be hard
to read is the high level DSL this is a
simple application that does word count
you know this is essentially the the set
of API is that it provides are the ones
that you might expect filter join window
aggregate and and so on and so forth now
whether you use this explicit processor
API or you use the DSL internally what
stream processing operators typically
form are topologies and you know you you
with kafka streams you don't have to
worry about how this topology maps to
processes Kafka streams takes care of
assigning the partitions that your
topology needs evenly amongst all the
instances of your application what
happens is this topology is just
embedded in your application
it's almost some detail that you don't
know about so coming back to the
problems of stream processing and how
Kafka provides primitives and how Kafka
streams leverages those primitives let's
take a look at each one of these and see
what it actually means
first off ordering for those of you who
might be familiar with Kafka this
abstraction might look familiar this is
the key abstraction that Kafka provides
which is a log and it is really nothing
but an ordered or a structured array of
messages so data is completely ordered
it is immutable so once data is written
it never changes and every record can be
identified using a unique index or a
sequence number in Kafka this is called
an offset so essentially this you know
abstraction it makes sure the data is
both written as well as read in order
and this is what provides the ordering
abstraction in Kafka which is also used
by Kafka streams so coming back to you
know partitioning and scalability that
is the logical view of the log
physically if you wanted to scale this
log out you'd shard it into
suitable partitions and if you did that
then that is exactly the back end of
Apache Kafka where a log is a topic it's
a logical thing and physically the topic
lives in partitions that are deployed on
brokers as messages come in they're
appended to one of the partitions log
and there is some sort of policy for
maintaining a fixed window of the log so
you don't run out of space it could be
based on time or it could be based on
size or a special compaction policy
called law compaction which we're going
to talk about later okay
so you know part of the problem with
partition like scalability is you know
data parallelism which is what we looked
into which is how do you shard your data
but then part of it is processing
parallelism which is how do you now
process across a distributed set of
machines and processes for that Kafka
has a primitive called group management
which allows a group of processes
essentially your application to
subscribe to a partition resource called
the topic while Kafka transparently
handles the load balancing so that you
can have multiple processes and it knows
how to assign partitions amongst those
processes essentially consuming large
amounts of data across several instances
of your application that embeds the
Kafka consumer is pretty simple the same
primitive is available in Kafka streams
now if you look at this the consumer is
replaced by the streams topology and it
leverages the group management feature
so it has essentially the same effect
you can have your Kafka streams topology
run on multiple processes that
essentially are your application and it
handles the load balancing and
assignment of partitions evenly amongst
those Kafka streams instances so fault
tolerance our next stop you know we we
looked at how processing and data can be
scaled horizontally but then what about
fault tolerance what about what if you
restart part of your application or it
simply crashes now the thing is that the
same group management facility it
handles fault tolerance so if one of
your instance
of your application might die or is
restarted it uh you know it
transparently handles the rebalancing of
partitions so that the new set of
remaining live instances they pick up
the load of the failed instances and the
same thing is available for Kafka shrim
so if if one of your processes that run
some of your Kafka streams tasks die
Kafka streams automatically moves those
tasks or those topology instances to the
remaining alive instances of your
application so coming to state
management and why this is important in
stream processing if you notice some of
the common operations in stream
processing some operations like filter
and map they're essentially a record at
a time you know our record comes by you
either filter it or not a record comes
by and you map it and send it across you
don't have to maintain any context of
you know of a record as it passes your
stream processor but then there are some
you know operators that require
maintaining some state when you window
data you have to you know remember a
couple of messages that formulate that
window and so state management is
important in stream processing you
almost can't get rid of it
so there are several options when it
comes to state management the first one
is possibly the more common approach
which is you take your state and you
just stick it in some kind of key value
store that you know and trust
that actually works however there is a
performance impedance mismatch for
example Kafka can actually process
messages at the rate of hundreds of
thousands of messages per second porno
while an external key value store when
used in this manner which is for every
message or every set of messages you
need to make an external RPC to this key
value store and either get or set some
message or data it can only handle
possibly thousands of messages per
second so there is inherently this
performance mismatch the second option
is you push your state inside this
processor so it's available locally if
you did that then that's obviously way
faster because all the data that is
required for your processing is
available within inside your processor
the nice thing is is that it provides
better isolations so one fast processor
cannot take down the database that is
used by other processors and it is
actually flexible so this this state is
actually pluggable so it could be an
in-memory hash map or it could be a rock
city beast or it could be your own data
structure that could be read or write
optimize depending on your application
and so what Kafka streams does is it
provides the option of local durable
state this is pluggable at the moment in
the current version of Kafka streams
this is either a rocks DB store or an
in-memory hash map but the nice thing is
that it is a fault tolerant and you
might think how what it does is that
every update that is written to any
shard of your state store is
transparently written to a highly
available and durable Kafka topic so
even as processors come and go when they
restarted on the new machine they can
scan this compacted log in Kafka and
recreate the state store now this is
merely an option so if you choose to you
can still go back and stick your state
in an external database there are other
implications of that but this is an
option that is provided out of the box
in cough machines so you might think
about you know if you log every update
made to some kind of state store how
does it work
without running out of space and this is
actually where the third garbage
collection policy in Kafka comes handy
which is log compaction the idea is that
you know imagine every update that was
made to a state store is written as a
message in Kafka and recall that a
message in Kafka is a key and a value so
this is the primary key and this is the
value which is essentially the content
of your row what if you did that then
you know every row that exists in your
database it exists in this Kafka topic
in fact as rows are updated you might
find multiple messages that belong to
the same key
if the ROI is never updated you might
just find one message that never goes
away so the implication is that you know
everything that you have in your
external state store is available as a
message here so if you wanted to
recreate the content the current content
of your state store all you have to do
is you scan this Kafka topic from the
very beginning and when you reach the
end you have your state store the
compaction policy what it does is
periodically it deletes older values for
every unique key to only maintain the
latest value and this is what you see
the the row with key a was written or
updated thrice but what really matters
is the latest value and that is what the
compacted version of the log looks like
and this is actually exactly what
traditional databases do is you have a
redo log and every every update to a row
is logged into that redo log and so that
is exactly our databases mean handle
crash recovery ok so this law compaction
feature is actually really important in
solving another hard problem in
reprocessing and I'm going to attempt to
explain what reprocessing means with the
help of an example so assume that you
had some application that counts the
number of visits to a user website and
it updates some kind of dashboard and
that might work fine until you realize
that you're either fixing the logic of
counting or you're fixing a bug in your
application to essentially maybe you
know drop or geo code altogether so if
you did that then you know you know that
all the windows from now on have totally
different counts from the windows that
the previous version of the application
handled and so what you end up with are
basically incorrect results for a large
percentage of those windows and so what
you want to do is
you want to go back and reprocess the
past results the moment you will feel
like you have to upgrade your
application so the way you do that is
you know note that every update or the
entire history of stuff is available in
Kafka inside this compacted log so
reprocessing merely amounts to resetting
the offset of your application to zero
and rescanning from that point onwards
so this is how it works your application
might update some kind of state and this
is where it's running which is it's
fully caught up you now you want to flip
the switch and upgrade your application
what you end up doing if you want to use
this feature is you start a new instance
of your application with which sits in a
new consumer group and the offset of
that consumer group is set to zero and
the state that it updates is initially
empty as it catches up you know the
state fills up until it reaches the end
which is when it's fully caught up once
it's fully caught up you flip the switch
and you have the dashboard reads move to
this new state store you shut down the
old instance of your state store and
effectively have now upgraded your
application completely in a way that all
the counts are you know consistently
reflected across the board now this
feature are actually really important
because if stream processing systems
don't support the idea of free
processing you have to fall back to a
parallel Hadoop base pipeline which
essentially makes it pretty complicated
as you will see later to maintain okay
so last but not the least the idea of
time we spent a lot of time on this and
a lot of our work is inspired by this
you know this idea are shared by the
dataflow team and Google and this this
insight is basically that stream data is
never complete but in fact can always
arrive out of order and what that means
for stream processing systems today is
that
you want to design things in a way that
you always expect data to either arrive
late or arrive out of order and this has
a bunch of applicator you know instances
on how you handle some of these problems
so the key thing to note about time or
when it comes to time are that there are
two concepts worth paying attention to
one is event time which is when an event
is created and the other is processing
time which is when the event gets
processed and due to various delays or
bottlenecks in your pipeline these two
things can actually converge and diverge
over time and the source of the loss of
correctness in a lot of stream
processing systems is because they
totally conflate these two ideas leading
to incorrect results so here's an
example from windowing to convey this
idea now lets you know assume the same
application we have accounting user
visits to a website and we're
maintaining some kind of dashboard and
because we're counting we're probably
counting in Windows of data and let's
just assume that window is a 10-minute
window now if if I'm a mobile user if I
visit this website and right before that
visit event makes it to the application
servers I lose network coverage I come
back after 12 hours now when this event
makes it back to the application servers
you essentially have two choices you
know you do you want to count this event
in the current 10 minute window or do
you want to come this event in the 10
minute window
12 hours ago which is when it really
happened and if you notice if you if you
count using the former method that is
counting by processing time it's
essentially incorrect because of these
delays because of ten thousand reasons
why those delays might happen if you
count using event time that is correct
because that is that reflects the true
state of the world so this applies
broadly to you know a lot of downstream
processing inside your stream processing
topology and this is something that
Kafka streams as well as fleeing as well
as dataflow a lot of these more
systems incorporate the idea of event
time carefully throughout the system
okay so this is uh you know we've
rounded off sort of all the problems in
typical problems in stream processing
how you might get that in kafka streams
and Kafka itself next up I'm going to
talk about a novel idea in Kafka streams
that helps you model some of these
harder stream processing problems like
Windows and joins much more effectively
and how that enables you to build these
loosely coupled stateful micro sources
much more easily I'm going to start by
saying that streams and tables are dual
and Kafka streams fully integrates these
two ideas um now this might make a lot
more sense with the help of an example
so here goes this is essentially a Kafka
stream it has three messages every
message has a care of value like any
other Kafka message if you notice the
stream is a little different the third
message updates the value for a key that
you observed in the previous message now
as a third exercise
let's try converting it to a table where
you know with every message we add a row
to this table so when you read message
one you create the first row when you
read message to you create the second
row this is a new key and when you read
the third message you update the first
row so this is you know essentially what
we've done is we've converted this
stream into a table now let's go one
step further and try to see what a
changelog stream for this table might
look like you know for those of you not
aware changelog streams are our streams
that have a message for every update
made to any row in a table so if you
want to look at what a changelog stream
for this table might look like this is
what you end up with if you try to
compare it with the stream we started
with this essentially the same stream so
we've taken a stream we've converted it
to a table that gets converted back into
a streams
essentially streams and table are doing
this is not that New York van idea you
know sequel database systems or database
stream systems have introduced this idea
we're merely bringing it to the surface
through Kafka okay so so what this
stream table duality is helpful in
modeling some hard problems specifically
joins so we will go back and take an
example to make this a little more easy
to understand so back to this retail
example where I said that you know you
have a sales stream you have a shipment
stream if we wanted to join these two
streams we might get the inventory on
hand for this retail company so you have
this shipment stream and sales stream in
for simplicity let's assume that the
format of a message in both of these
streams is exactly the same it has a
dual key which is item ID and store code
which identifies an item inside a store
and then it has a count now this count
for a shipment stream it means the
number of items that are stocked up in
that store and this count in a sales
stream means the number of items that
are sold from that store so what a joint
operation looks like is essentially for
a message in this shipment stream you
increment the value in this table and
this table is essentially a concept in
kafka streams and for a message on the
sales stream you decrement the value in
the table and now if you look at what
this table is it's essentially a
real-time view of the inventory on hand
for this particular company now this in
kafka streams it isn't just an internal
concept I didn't get a chance to show
you know API but this is absolutely a
concept that is available to you as a
user so you know what the semantics of
that particular thing is so you have a K
table which is a table which is exactly
the output of this kind of join
operation where there are messages that
update values of previous messages and
then there are K streams where there are
just messages that
don't update the values of previous
messages in the stream now going one
step further you know there's an
emergent property of some of the things
that I shared that might not be as
obvious and that is that the stream
table D Valatie when combined with local
queryable State I'll explain that
shortly it allows you to build stateful
applications with ease
let's go back to this example if you
look at you know this joint operation
and what the application view of things
looks like let's assume that this was a
simple code that you wrote to join to
join these two streams inside your
application you have several instances
of this application every instance has a
subset of partitions of the shipment and
sales stream this is handled by a Kafka
streams under the covers and hence every
instance of the application has you know
subset of shards of this inventory on
hand table assume that this table which
is exposed to you as the user
this was actually queryable if this was
queryable then what that gives you a
chance to do is expose this sort of REST
API on top of your application that
allows you to query the count for this
inventory item in every store this is
actually pretty powerful because we've
not only built you know this application
as a real-time view but that view is
actually getting updated underneath the
covers because of this join operation on
the same application right this actually
may not make sense in every domain
oftentimes what you want to do is even
if you do this join you just want to
stick it in some kind of external
database you know and trust but if any
operation that you need to do it
requires accessing a ton of State or a
ton of data then having this queryable
state option is actually super useful it
essentially the state is fully isolated
it never leaves your application and it
is entirely isolated and hence resilient
okay so moving one step further if if
you try to put two in two together
Karim's is actually helpful for building
these asynchronous and loosely coupled
however stateful applications going one
step further on our application you know
building it out a little bit further to
complete that picture I showed earlier
we have this inventory on state
application it has that table and recall
that every state store is backed by a
change lock topic in Kafka that's
transparent it happens automatically and
this is what the change log stream looks
like crying every message is it
corresponds to some raw update that is
made on that table and that is like any
other Kafka topic that is available to
you if you had that then it actually
powers to downstream applications that
we initially said are typically not
built that way today and that is the
reorder inventory application so what
that does is it subscribes to this
change log stream like any other Kafka
topic with every message on this stream
it checks if that inventory count has
dropped below a certain threshold if it
has then it triggers the reorder action
this application is entirely different
from this other application which is the
price adjustment application what that
does is it independently subscribes to
this change log stream and it tries to
project the demand for this particular
item and if the demand is high or it
changes then it triggers price changes
right and these two applications are
independent there are real-time but
they're also totally loosely coupled the
failure of the reorder inventory
application has got nothing to do with
the price adjustment applications so
we've not only you know designed these
services as stream processors they're
designed as you know stateful services
that are loosely coupled and that is
what I meant initially when I said that
I this this whole idea of you know using
something like after shims it enables
you to do push stream processing inside
your application and
helps build these loosely coupled
micro-services if you might call it that
an emergent sort of theme across Kafka
or Kafka streams is you know the
simplicity is valued and and the reason
is that the more things you have in
distributed systems the more things you
have to operate and ongoing operations
are absolutely the hardest things you
know much much harder than building the
application or even troubleshooting it
in the development phase and so the
implication for this for stream
processing is that typically if your
stream processing system does not have
local durable state then this is what
you end up with you have Kafka which is
still your you know data pipeline but
you have the stream processing framework
if it doesn't support state you're
dependent on some kind of a database so
you have to operate that this is your
stream processing code which runs as a
job on this framework that really
updates your final view but then if you
if you think about it if your stream
processing system does not support the
capability for reprocessing or upgrading
your code then you're left with this
parallel pipeline which is based on
Hadoop so you have the same logic that
runs on Hadoop it updates in entirely
offline view of the same database and
not if you look at your application it's
it's pretty complex it queries these two
views it merges it's on the fly and then
gives you some of these results so that
is what I meant by you know for
application developers that is just too
many moving parts this might work for a
subset of a you know use cases that you
might depend on a fully managed cluster
for and it might work but for
lightweight you know stream processing
inside applications that is just too
many moving parts so that is what we've
paid attention to in streams which is
you know we've taken those aggregates
we've provided that as local durable
state so it is available out of the box
you don't have to depend on an external
key value store we've taken the idea of
free processing and it is actually
available as a first-class primitive
inside of Kafka and Kafka streams so
you don't have to depend on a Hadoop
based pipeline essentially moving away
from lambda architecture is what it
finally enables in this simplified
picture so you might think you know
Kafka streams is great but it is only a
library on top of Kafka which means that
it only works for data which is within
Kafka it is done that way for simplicity
for ease of operation but then the
problem you might think about is well
how do you get your data in and out of
Kafka in the online release which is the
previous release of Kafka we had
released Kafka Connect which is this you
know a way to build elastic scalable
connectors to other systems using Kafka
the idea is that this layer solves all
the common problems that connectors need
to solve which is scale out and
partitioning and load balancing offset
management since you know this is
exactly the layer that allows you to for
example ETL your data from a Cassandra
or Oracle database into Hadoop using
Kafka connectors it's been roughly
around four to five months since we've
released Kafka Connect and the community
has developed basically maybe 30 or so
connectors already on top of this to
various systems like elasticsearch
MongoDB Cassandra someone even I went
ahead and wrote a Bloomberg ticker
source so you can you can take a look at
some of these connectors or even develop
your own now tying this into the big
picture which is what I wanted to do
here which is this is what some of us
achieved while at LinkedIn and have
helped some companies move to which is
essentially the ability to deploy a
Kafka based stream data platform at the
heart of your data center this is the
foundation for building loosely coupled
microservices a building block for
stateful applications or stream
processors or enabling other stream
processing systems to exist due to the
data pipeline as well as the feed of
data that goes into your warehouse
now if you wanted to try this out
cough-cough streams was released today
in the latest release of apache kafka
which is the OU 10 release you can get
it through apache cough core the
confluent platform both connect and
streams are is brand-new there are a lot
of people were jumping on these ideas if
you need help we we do a bi-weekly ask
me anything call if you wanted to
participate or give us ideas you can
ping me on Twitter and that is basically
it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>