<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Log Aggregation • Kresten Krab Thorup | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Log Aggregation • Kresten Krab Thorup - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Log Aggregation • Kresten Krab Thorup</b></h2><h5 class="post__date">2016-11-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZlGZ0_mFlmQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi there my name is Creston I'm CTO in
this company called humor and I'm here
to talk about understanding luck data so
first of all I'll just it's a very quick
agenda of these next 40 minutes just a
bit who we are I'll do a super super
quick demo for you because this is
really a talk about this product that we
built and why we built it so a good
chunk of it is actually are our
experiences and why we went about doing
this and then a bit about how it works
and we'll see if there's more time we
can we can play around with it after
that so so we're fairly we just came out
public about our company last week in
Copenhagen we've been working on this
for nine months the core team is the
three of us we're Kieran has actually
been a global manager for the go to
conferences for the last ten years I've
been I'm co-founder and was CTO of try
Fork which is now a 500-person
consulting shop for the last 20 years so
we decided to kind of break out in a new
startup picking up some of our
experiences so I'll start off with a
short demo where I have in this in this
little demo
I have pulling in data the logs from
github because they're publicly
available there's a public feed of live
data coming out of github that tells us
all the pushes all the likes all the
comments are all in in it in a in an
event stream that you can subscribe to
from github so just as an example of
something that a lot of people can
relate to so dumb
the main API looks like this where we
have a time series over the last 24
hours
and then we have events that comp become
as Jason structures that go something
like this they're just a description of
what happened in this case this was
somebody forking a repository so let's
see so then the language up here is
basically it's a you could call it a
it's a grip on steroids where we can
really fastly grep and do basic
analytics on this data so let's start by
looking for anything that has to do with
docker alright so here we get an event
distribution graph over the things that
happened on the repository darker darker
I could go look at that or we could do
simple group by the types of the types
of events that happened at github the
last 24 hours so these are the number of
total number of events was about a
million and you can take these and then
sort them and we get them the most
events like this or we could do take all
the watch events that's them that's when
you like a github repo and say we want
to group those by repository name and
sort them so you get the most popular
github repos the last 24 hours the ones
that were the most watched and you can
take this over another time interval and
turn this into a live stream of data so
we just take this pipeline again over
the live stream from github and as this
24 hour this is in this case a 15-minute
window you can see the counter will
update as we go along
that's this time so that's the basic
idea so let's get back to the
presentation if I
it works well to just explain this
briefly but this is so this all started
the why in building 24/7 systems you
know most interesting systems today are
really composed of many servers many
services today's you know it's micro
services cloud distributed systems and
locks are really a time-proven lowest
common denominator for understanding the
system what's going on in a complex
distributed system so starting out with
locks you get a really low barrier of
integration very easy integration point
and by aggregating locks its we really
found that this is key to understanding
this kind of system so really a lot of
the experiences is this is comes around
this system we started building around
10 years ago which is the equivalent of
NHS spine so it's a it's a prescription
service central prescription services
medication services supervision of
prescriptions pharmacies etc a
centralized system that we've had in
Denmark for the last 10 years so this
was one of our big first big you know
24/7 systems which were really missing
critical distributed in many ways so
it's really important to have any
availability
you know you need doctors that the
emergency room to be able to to
prescribe drugs even in the middle of
the night I'm to be able to look at
people's prescription records there's a
lot of security requirements which made
the DevOps pretty cumbersome lots of
servers and many different kinds of
services running on these systems and
also a thing that complicated a lot is
this was part of a distributed system
where we only controlled a fraction of
it because there was like 40 ish
different systems at hospitals and and
doctors and various services that would
interact with this and generate data
that could flood and cause all kinds of
issues so in this environment and this
was you know with just when DevOps and
distributed systems was really fairly
new we we set out and actually picked
a tool for doing log aggregation and
what we found here's some of the
interesting things we found actually
even today lots of people are not using
log aggregation so so the the most
important thing you get when you have a
log aggregation system is to you want to
build as have an idea of what's normal
so we set up this place in in the dev
teams which would show dashboards of how
many transactions did we do today what
what kinds of errors happened
today and by having it visually right
there in front of you you get the sense
of normal where you take the AI of the
brain and x-ray a part apply it there
and so you could immediately see the
devs could immediately see if something
was different and it used to be so we
put up dashboards it could be something
like this is just a sample dashboard
it's not from the actual system but
these visual things did you put up that
makes it there's some information you
can your brain can can process and look
at and understand what's normal then one
of the most important metrics is
something like just you know just
transactions over a today what's
happening in the system so this could be
how many prescriptions are are being
prescribed to during the course of the
day or the course of the week this could
be maybe this is lunch break everybody
goes off for lunch break so it's normal
to have a dip around there and then very
a very easy thing to to to do is just
add a trendline for the same day last
week so it compared to compared to last
week that's as the super-easy that makes
it very easy to see what goes on and
then of course every now and then you
get weird things like the first day
after Christmas - the first day after
Easter break typically the first day
after after a break lots of people are
kind of they're aggregating their
illnesses and they all come to the
doctor under Monday or the Tuesday after
the break so you'll see these kinds of
patterns so they're off
every one every now and then this
doesn't this isn't right but but in many
cases that's a very easy measure to just
you don't need you know fancy AI you
just need people actually looking at
these graphs every day it's very very
very simple so another thing is because
of the security here you know there's
really dev and ops were really divided
right because this is like often in a
bank environment we had aberrations
completely isolated from dev and
everything Ops was supposed to do when
through the ticketing system so you need
to configure this you need to install
this there's no tinkering around with
anything because of course this is
healthcare information so we need exact
tracking on who does what in in the in
the operations environment so it turned
out having access to the locks in the
system of course with an audit record of
who accessed which locks what did they
do is is a nice read-only way of having
access into the operations environment
where it could still learn a lot about
the system as it's running without
having you know being afraid of touching
something or making breaking things in
in the operation system so a key thing
is to also be able to with this system
be able to iterate and explore so you
might already have a metric system set
up where you've count you know how much
CPU how much disk etc how much how many
how many transactions of various kinds
that you have but often you know the
this cycle for adding extra information
to this or explore things is it's pretty
slow so we often see that you know
there's some team they'll go once and
set up the dashboard and then you know
do you never go back and change it but
we find that it's really super valuable
that everyone all the devs
can go build their own variations of
dashboards to play around with it
and easily extract new kinds of
information out of out of the lock
stream and that might sound like it's
difficult but it's actually not so so
this whole idea of being able to iterate
and have an explorative kind of approach
to to your metrics to your locks is um
it's really important so you get a very
diverse and this are you know our
healthcare system was at least these
kind of different kinds of services and
systems and you can imagine they eat for
to get data out of these two those right
there's a different way of doing various
ways of things in these things
so by just taking the locks and I mean
the the flat files that come out of
these systems it's a very easy way to
just take these locks and ship them now
the interesting thing is how do you make
that into into some interesting data and
this is a place where various log
aggregation systems differ quite quite a
bit you know in some cases you have to
configure the system to extract relevant
information when the data comes in but
some of them also allow you to at hoc
and at a later point in time extracts
the data that you want and that's this
there's a super flexible way of doing
that so you want to that's there's
something we learned that being able to
go back and say I really wish I'd index
that attribute and have that be
something that's super easy to do has a
high value so orange anyone uh well it
turns out uh you know there's there's of
course two major players out there
there's Splunk there's elastic and we
found for many many of our customers
didn't weren't ready to pay for spunk
it's you know it's priced by volume and
for various reasons did we found it you
know especially and smaller accounts
difficult to for elastic we also spent a
lot of time you know deploying these
systems and there
they're also it's a bit tricky to at
least run your own because there's so
many moving parts so we wanted to do
something that's simple you know fast
flexible affordable simple to install
and use to get this to get all these of
these benefits so so human-like the
Corbett is um let's say it's a time
serious text database so it's an
in-memory stream processing things so
it's like advanced event processing or
bringing in these idioms and techniques
from you know from the financial
high-frequency trading systems kw+
ish things where instead of doing a lot
of indexing up front
we actually just very fast at processing
data at brute-force with some very very
simple high level indexing like time is
the primary index that we have it's only
index pretty much so so at the core the
engine is it it's P Jason and API thing
and then we have we build a front-end
for this so let's look at little at what
the queries look like so we built a
query language that kind of resembles in
a unix pipeline we want to do grep into
sort into unique into something so a
query that would look like this in SQL
select the average response time from
the back-end services where the URL
looks like this this could be a query to
stew and the time is bigger than this
and we want to group it by the method
for instance so you would write this in
in this way in in our query language the
first things are filters we filter on
tax and starting one day back we want to
filter on that the URL is this that
starts with this and then we just group
by group the data by method averaging on
response time
so we find this is very natural way to
think of it as as a stream processing
language you imagine kinda mentally that
events and to all the events of the
system enter through this pipeline and
the ones whatever comes out and the
other end is the response of of the
query and that applies both for like an
ad-hoc query and also for standing query
that just sits there as as a MapReduce
pipeline or an event processing pipeline
where events are fed into so the basic
way how this works is we just store
segments in many ways this is like how
Kafka stores its data so we built our
own back-end where you as you ingest
data it stacked with various attacks it
could be front-end tags and data center
tags like this and therefore each
combination of tags
enters becomes a stream of segments kind
of like a Kafka topic and is stored
compressed we can decompress it to
process it and then when a given query
comes in use you saw before there's a
there's a set of tax and a time range
that could be the backend in this time
interval from 200 C 3 that happens to
match these segments of the streams so
we just picked we just brute force those
and inside these we have a in each of
these blocks that adds up to depending
on the compression somewhere between 50
and hundred megabytes for each of these
segments we have a little time index so
we can also just select as a portion of
the data in one of these so other than
that once we once we do that it
we actually just brute force search and
I'll get back to how that's fast
but because we have the entire data set
in we can actually run interesting stuff
like at full speed run a regular
expression over the locks so this is
them this is the power of this
flexibility is this is how we extract
you could say we'd wanna take the back
end we want to run a reg X of there's
some conference ID at this position in
URLs and then I want to group by that
conference ID and select something right
so you generate data out of this so in
many systems you would this this would
be really difficult because you extracts
you decide what to extract or what to
index on when you ingest the data but
here we can actually at full speed run
this yeah so just a summary and it's
like a UNIX pipeline style reactive
event processing we find that you know
many queries you want to do in these
systems are have an aggregate component
anyway right so you often you have to
scan the data many realistic queries of
course it's nice in in a term index
based system that you can very easily go
to a specific request ID find a specific
record and of course elastic or that it
likes are faster at finding specific
things but what often happens is we want
to do a graph over response times or do
some some weird aggregation across our
selection so you have to scan the data
anyways and then it becomes actually
faster to just do let's make sure you do
serial reads from disk and are really
careful to stay on cache etc and it's
really super valuable to have the schema
on query you could say extract data out
of the locks as you're processing it
because it's like a time it's like
having a time machine right you can go
back to a year ago and say gee I wish I
indexed that attribute so how do we make
this um go fast
we apply a good dose of mechanical
sympathy so Jackie Stewart was this
British Formula One driver and it was
really famous in the 60s he went he he
won three World Championships in Formula
One and the special thing about him in
this context is he came up with this
name of mechanical sympathy arguing that
you know you can drive better you can be
a better race driver if you really
understand your car so he really spent a
lot of effort understanding the car
exactly you know he was mechanic himself
you could you understood every aspect of
it and you'd be one with the car so this
this idea applied in in software systems
of course has been you know Martin
Thompson has been walking around talking
about mechanical sympathy in terms of
computers mostly in terms of
understanding what your CPUs look like
understanding what the caching
infrastructure looks like what how to
best access your data put it on course
and because there's there's almost an
order of magnitude difference between
Nexus and data on a local core on the
cache and in main memory so if you can
have data we can have an active data set
on on a core that runs you know much
much much faster and the key trick to
making this brute force thing run fast
is to have it compressed on disk loaded
up onto a core in small chunks where we
doing uncompress it because the
bandwidth up is maybe 1/10 because we're
10x because it we loaded up compressed
then we decompress it on the core and
then you have super fast access to run a
rec X over this little chunk a run
extract or do comparisons on pieces of
data so of course that's some that's a
lot of there's a lot of engineering that
goes into that and also guessing because
you really it's really difficult to to
measure these things you just have to
kind of feel like you're art
today I am the CPU but having this
sympathy with with the course and you
can go online my Martin has done several
talks on how to employ this and put it
to use in various context so the it's
it's really amazing how it's an
interesting quote the most amazing
achievement of the computer software
industry is its continuing cancellation
of the steady and staggering gains made
by computer hardware industry you know
we've been so busy you know especially
the last ten years we're building
distributed systems scale out systems
and and really not caring about the
hardware it's you know hardware is so
cheap that we can just throw more
hardware at it and you know sometimes
this gets to a point where it gets
totally ridiculous because if you do
things right you can you know you can
save an order of magnitude on systems so
our current finding is you know with
humor oh we can also you can save an
order of magnitude on your servers right
you could turn five or ten servers into
one by just doing the right thing so the
the gist of it is to get the raw speed
in the code you know the active data set
of inner loops fits on cache
you want to make sure you don't do any
dynamic allocations in the inner loop
once you set up you can go because
allocations involve synchronization
without their cores want to bind your
stuff to core so that you don't the data
doesn't move around especially on on
multi suckered systems memory banks gets
associated with the CPUs on the course
hyper threads are just in the way if you
can actually utilize your machine you
know it's just it just happens you half
the cache size so the type of threats or
two threats that run on the same core
using compressed or it's actually the
biggest performance gained we see
constantly or the biggest difference
between different systems is how good
that's how well does the the data
compress the more we can compress it the
better bandwidth we get all the way
through up the Iowa system main memory
to CPU no it almost doesn't matter how
long time it takes to to decompress it
but of course fast decompression is
important and I think having fast
decompression that's going to be that's
going to be an important thing in new
hardware architectures in from Intel
going forward we I'm certain we're going
to see hardware supported decompression
directly in hardware because this is so
important to get everything out of the
machine sequential read from disk
it's an old timer so there's all these
tricks in the book that you can you can
listen to Martin's talk talks and and
and learn it so on ingest how does it
work we have a bunch of ACP Jason api's
or you can use various other tools like
we support elastics bulk API and a file
BJP is to reduce its existing beats from
from elastic to load data in a load net
flow directly from firewalls syslog etc
it's very easy for us to of course add
new kinds of things like this so in this
performance this is this is often one of
the pain points of others alternative
solutions so arriving data I would just
process it really minimally and
basically just compress it and it
persisted an append-only
fashion so we don't have an indexing B
table sorted on disks things alright and
you can easily do 100 gigs a day per
core on an ingest note so this is uh
this is often a pain point they do need
you know a lots of big machines to sit
there and take data in that you might
never look at
so the pipeline is quite simple you can
either go directly or via Kafka and if
you if you set it up through a cop car
this is actually showing that we we try
to stay out of the loop on these
complexities of storing three copies and
making sure that everything is
consistent across different copies and
bit rot and whatnot so block stores are
very good at saving extra copies of data
Kafka is also very good on taking this
in-flight extra copies of data so you
can have as data arrives we can load it
through Kafka humor just does the
processing and builds up these segments
and then pushes them off to a block
store so this block store could just be
a file system in a simple set up and
with that you know once it's a block
stored here we can act it all the way
through Kafka so we can release release
that those messages could be another
machining system so basically we tries
to stay out of the loop of being a
complex distributed system keeping three
copies of all your locks etc if case you
want that we can offload that to other
infrastructure surrounding it then also
so this is what we're working on
being able to run distributed queries
actually at this point we haven't we
don't have any any examples of customers
so we actually have some pretty large
torrents but we we don't have any where
it has been a big issue that we can't
query fast enough so that being able to
query fast enough it's gonna be it's a
next major feature we're building on is
being able to use a block store and then
essentially have a number of different
query nodes that can just read out of
the box store you can distribute a query
over segments so the product we have
today is just a single note that we
deliver you know online hosted or as a
container
for download on premise that's it's an
important thing to have this on premise
solution because many of our customers
you know they're in health care or
they're in banking or something want to
have their own Doc data and as the show
before we have a demo system and we've
spent the last two months almost doing
nothing but writing documentation kind
of wrapping up things but I think we can
spend just five minutes on trying to go
back and look at the at the system you
can still hear me so we saw here
this was the query to figure out what's
hot and not right now and get at github
so imagine we want to figure out
something more complex like like to give
an example of what a reg X looks like so
so these queries that go like this we
can you know we could you could
visualize them differently so if we take
just the 10 most interesting ones and
show that as a pie chart then we get to
10 most popular repos whatever we can
put these on a dashboard so a dashboard
it's just a collection of query results
like this so this is the just a sample
dashboard that shows um most-watched
today that's what we talked about the
actual inflow of events right now
it could happen here's the query that
shows the users that did the most
comments today so let's see we can open
the query over here so this takes all
events that are comment events and group
by the login counting how many of these
are and then sort them another example
here is
most forked so in here we have we take
all the events that are forked so this
is some if you just write any text then
it becomes just a free text grep if it
has an equals in it somewhere here then
it's a more directed search for those
attributes then we run a red X over
these four cabins like let's try to look
at what a fork event looks like I'm sure
forgiven and that's all this information
but most importantly it has the
repository that it pertains to the ones
that's fourth and it has kind of an
organization or user and then the
repository name so if we want to extract
that I just run a red X over all these
ones finding the repository name of
something that comes after it slash so
this is the name group in a regular
expression and so after doing this we
haven't an extra attribute available
that is is the rep name and then we can
group by that so this is disregarding
who owns this repo what are the
different kinds of what are the names of
repositories that get forked and we
could do this over you know a longer
time over the last week so this actually
depending on the complexity of the query
of course this easily runs 1.5 1.5 geek
of in desc data per second per core so
and this is a four core node we run at
you see five gigabytes a second so you
want to run a similar complex system I
have a bigger I have a have a bigger
server here
let's run in here go I want to lock in I
have the some github data here if I want
to run this here over this is somewhat
bigger bigger machine okay it should be
able to get close to ten weeks a second
- how for something like this so um so
being able to to play with these queries
and have have access for devs all the
way in for these kinds of things is is
super valuable so instead of you know
dashboards being something that we're
set up once now it becomes part of the
interactive experience when you're both
debugging finding bugs and general
monitoring your system so that's how
it's something we find that has a lot of
value and it gives you you know in total
insights in into your system of course
distributive systems are always
difficult and hard to understand but
just can just let you do it a little
more bit of help in that regard so I
think I'm going to end here and and take
questions from the audience in case we
have some yeah yeah do you have a way to
send alarm so that predicts predictions
as data doc not currently we it's one of
the first things in or in our pipeline
are things to do so alarms and alarm
system one of the first things I'll be
coming out with so this is the first GA
release that we had we've been working
on this for nine months mostly Christian
sitting there in me and now we've over
the last three or four three months
we've been building up the team with
marketing resources and and more to know
where is we're seven people but there's
that's that's obvious and it's quite
easy to to do something like alerts of
course but that's why it's one of the
next right we have some fairly large
customers we were running tests at that
we're letting those customers actually
drive the decisions these the first few
big test customers actually drive the
pipeline of features we implement okay
yes we you're most welcome to come and
and buy it so we're still in the process
of figuring out what does it cost to be
to be honest and and right now you know
we're interested in reference customers
so that's something that's a bargaining
chip you can come back with at this
point it's more important to get
reference customers and to get a lot of
money at this point so we're hoping for
a price point of 10k euro per note and
many in many instances you just want one
right so so we'd we don't want volume
pricing we want to fit I mean the
pricing model is something it's annual
subscription to run the supported
software and it's not open source
okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>