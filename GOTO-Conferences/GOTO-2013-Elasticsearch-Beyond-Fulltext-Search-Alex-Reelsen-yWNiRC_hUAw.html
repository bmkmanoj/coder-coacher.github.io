<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2013 • Elasticsearch - Beyond Full-text Search • Alex Reelsen | Coder Coacher - Coaching Coders</title><meta content="GOTO 2013 • Elasticsearch - Beyond Full-text Search • Alex Reelsen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2013 • Elasticsearch - Beyond Full-text Search • Alex Reelsen</b></h2><h5 class="post__date">2015-10-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yWNiRC_hUAw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm going to talk about elasticsearch
obviously I'm not going to talk about
the full text search capabilities but
rather how to use it to transform your
data into information before I start who
have used actually using elasticsearch
in production okay a couple that's fair
anyone already keep on and logstash user
also a couple but not too much that's
great so yeah if you want to tweet about
it I'd like you to use the elasticsearch
hashtag as well so we can basically see
what you tweet about it before I start I
may introduce myself I'm Alexander I'm a
comedian elasticsearch this means
features bug fixing I'm maintaining the
debian and the RPM package so if
everything is going wrong with packaging
it was most likely me as committing to a
github repo is not a sustainable
business model the company called
elastic search around the product is
offering development and production
support this means we hope you like to
get up and running with elastic search
and we are also helping you in case your
production system goes nuts
another thing is I'm doing trainings and
I get to talk at conferences and a
Templar so my interest mainly Java
JavaScript and web applications I've
been working with elasticsearch since
about two two-and-a-half years I've been
working for les Sixers the company since
a couple of months and we a distributed
company so we yeah scattered all across
the world if the world is northern
America and Europe and I'm based in
Munich but we don't have no for sale so
I do a lot of home office obviously so
what am I going to talk today about and
before I start let me start with a very
very common search and let me show you
the search it's more than a full-text
search today I pretty much hope every
one of you is going to recognize this
page it's a github search page and if
you enter a search query like elastic
search in the search box you'll pretty
much expect to get back results with
elastic search but what is actually
happening on this page is way more than
just executing a full-text search if you
take a look on the left you can like
select by language and this makes it not
only an unstructured search for elastic
search
but also it adds some sort of structure
to your search similar to a way a Korean
secret clause a search engine also is
able to enrich the search results it's
going to return to the client you can
see here that the search for elastic
search is actually highlighted whenever
it's been found in a repository name or
in a repository description sorting is
something which is quite obvious usually
a search engine is sorting by relevancy
but if you want to like take another
look at your data you obviously have to
be able to sort by numbers or by strings
as well pagination is also something
pretty obvious which needs to be
supported by any structure which returns
any kind of data and where it gets
interesting again and which we will
cover today in this presentation is the
aggregation of data meaning that you're
executing a search query like elastic
search but you actually would want to
know like what is the distribution of
results like the programming language
like Java Ruby JavaScript and so forth
and this kind of aggregation of data
will be covered today in this
presentation something else which is
pretty much obvious for most search
engines today as well as the capability
of offering suggestions and as you can
see it's not only returning suggestions
but also enriching them with
highlighting again because search query
has been Debian and Debian has been
highlighted in the results as well and
this is for all kinds of functionality
like did you mean correcting typos on
the fly before people firing up a search
which doesn't return no results which
you pretty much don't want so before we
go on I'm going to start with the
introduction into elastic search going
to start with this password bingo slide
so if I only have this kind of slide
pretty much best description for elastic
search is a schema free rest and Jason
Bates distributed document store it's a
distributed document store with
obviously extraordinary search and data
extraction capabilities and the schema
free is kind of a misnomer today because
it's more like schema less and if you
index data into it
elastic search tries to understand what
kind of data you're indexing like a date
or like a number and automatically
extract
this meta information from the data you
are indexing its rest in JSON based so
we are pretty much speaking HTTP and
JSON hopefully to protocol to a protocol
in the object notation everyone should
be like at least capable of
understanding of and working with
systems it's distributed I'm going to
show this in a second what we understand
under distributed document store and the
document store basically means you can
index any JSON object of an arbitrary
death and you can also search on this
kind of data it's open source it's the
Apache License 2.0 so you can do
whatever you want with us it doesn't
require a lot of configuration to get up
and running and it's used by a couple of
companies and if you take a look at them
they look like it's only used by like
tech oriented companies but these are
the early days they're really a lot of
companies across every kind of industry
which is using elasticsearch today zero
configuration is not like a part of my
marketing slide before but it's really
that easy you download the archive you
unzip it or you're on target and you're
running this one single command and
you're ready to run with the Search
Appliance basically the only real
dependency says is Java so you should
have most up-to-date Java 7 because Java
6 is end-of-life but it also works with
Java 6 if you like forced to use it in
your environment the other nice thing
about this is you can just run the last
command a couple of times in your system
and you immediately have a cluster on
your local notebook to play around with
the distributed features so how do you
index and search data with elasticsearch
and this is pretty much the first
command you have to execute I will not
talk about curl as a tool it's a command
line HTTP client it's kind of a Swiss
Army knife for everything so if you
don't know it use it a lot and what you
can see it first line is the URL which
is being referenced like localhost 9200
which is a port elasticsearch ones on
products is an index product is a type
and one is an ID and this is how you
reference a single document why the URL
and the rest of the document is
basically the body and again you can see
it's primary it's the same
J'son object it's a JSON object with a
death like it has a price object which
inside has a net and the tax value and
if you go through this for different
values which are inside of it
elasticsearch actually recognize that
that's a created at field is a timestamp
and is indexing it as a timestamp
without having you to configure this
upfront name is obviously a string and
the price fields are different because
net looks like an integer and text is a
float however when you enact this for
the first time the price dot net value
will always be an integer which is not
what you want on a life and on
production system so you can configure
this upfront but in order to get up and
running you don't need to
and yet text is a float and after you've
indexed the data with putting it to this
URL you could also just execute an HTTP
GET request to get it back immediately
it's basically a really a data store
which you can just put data in and get
it back if you want to search data in
the next step you can see that there's
another call at the bottom which search
in the index products with the type
product and execute the query for
Macbook and you will most likely get
back the first document so this is all
it takes you to get up and running with
elasticsearch
download unzip start and have fun I was
talking about the distributed features
and there are two things you like can
divide the distributed application into
the first thing you want to do is
replication this means you're actually
duplicating data and storing it on
several machines this offers you some
sort of reach scalability because you
now have actually more sources to query
this kind of data on and you can execute
more queries per second and the second
thing is removing a single point of
failure even if one system goes down but
you have your data replicated it's still
fully available and available to query
the other thing which is important in
distributed system is the charting of
data this means you take an index or a
collection of data and split it across
several machines
the core reason for this is the data
gets too big to be stored on a single
machine the other reason for this is
you've got right scalability
suddenly you have two systems you can
write your data to which is which
basically scares out and the third thing
might be that you want to control data
flows if you have a cluster with an S in
criminals strong machines and two are
very strong into a very weak you may
want to index your current data and
Clarry your gun to current data over the
current over the powerful machines so
what means distributed distributed
basically mean that you can fire up a
single node with the command digest
shield and you can create an index in
this one in this case it's an orders
index and the products index and when
you create an index you can actually
define the structure of an index in a
distributed environment in this case you
would divide the orders index into like
four parts which is the number of shots
and you want that each part is
replicated somewhere else in your
cluster this is some sort of contract
you're defining when creating an index
the second example with the products
index means creating an index with two
shots but you don't want to have it
replicated somewhere as a kind of backup
so if you're firing up a second note
this basically means that the contract
was having four shots and one replica
for each is going to be fulfilled on the
second on both nodes this means every
data is basically doubly kated now but
it's available it's completely available
and there's a notion of so-called
primary and replica shots the one with a
green background other one where data
gets index first and then it gets copied
to the replicas shot and the contract of
the products index basically means I
want to have two shots but I don't want
to have any backup so if in this
scenario the second node fails I still
have my full data of the orders index
but I will obviously lose half of my
data of the products in Ex if you start
up a certain node in this scenario the
products index will not change at all
because the contract is already
fulfilled but the other shots will
actually like distribute the clause
across a freshly added note and you will
you can now shutdown any note and all of
your data from the orders index will
still be available so talking about
elastic search as a product is really
nice but there's actually a whole
system around and since we founded the
company we try to create an ecosystem
like clients like you bond unlocks this
but the first thing you actually have to
know that it's very easy to extend the
functionality of the elasticsearch co-op
product it's basically just a Google
juice application and we provide hooks
for everything so you can create your
own plugins like your own leucine
analyzers like you NLP functionality
like your own aggregation functions or
whatever you want put it in a zip file
and this is it to install it as a plugin
in your elasticsearch node so it's it's
really easy to extend we also have
clients for many languages but the
client is basically just a very thin
layer of code between the HTTP layer of
elastic search and your application but
it mate makes things actually simple if
you don't have to care like for opening
HTTP connections and closing them and
last week we released a couple of
clients for Ruby Python PHP and perl as
official upstream clients which can be
used to create your own object mapper
for example and there's also a couple of
community-based efforts for JavaScript
client for scarlet line for closure
client and many more languages and even
there are more clients for the first
four languages written by the community
because they wanted to solve a certain
use case also kibana is something which
I'm going to show in a second which
basically allows you to like get a
graphical overview of your data which is
thought in elasticsearch it's a small
JavaScript application which creates
elastic search and creates charts and
allows you to create dashboards you can
share with other people I'm going to
show that in a second Laxus is lockfile
aggregator which basically takes a pile
of log files and indexes them into
elasticsearch but it's able to enrich
them like with adding geo based data for
an IP address so it's easier to query
and we also have a hadoop integration
which allows for faster queries in some
cases because we can execute them in
memory and return faster than a usual
how to drop wood so this was
introduction the next step is that we
are going to it index some data and I'm
going to show how to extract useful
information out of it
and
question is what is data and in my terms
it's whatever provides value for your
business and there's some kind of
obvious data which is a domain data like
your orders or your products if you're
an e-commerce shop this might be
internal but it also might be external
if I'm a social media company which
analyzes constant data streams of
Twitter or Facebook this is my core
product value and this is the data I
need to analyze same goes for Gmail and
emails they need it for spam tagging and
filtering but there's also less obvious
data which is in this case it's a sample
application data
there's log files which might tell you
when you need to like fire up your next
ec2 instance because you're getting too
much requests for your static files
there are also metrics like in-memory
values from your application which you
would want to monitor and see what
happens with them and react on certain
events if you walk through a company you
will see that there's tons of more data
like this not two main data not
application data but maybe something
which is there on paper and you would
want to have digital eyes or whatever
and you will see that many things of
data you would want to index and
obviously ask questions so you can ask
questions to your data and these are
simple questions but you still want to
know imagine we have this orders index I
just showed you with the four shots and
one replica and after some time you
might want to know how many are assigned
there how many orders we did get last
months how many orders are coming in per
day what is the average revenue for my
shopping cart inside of this order is
actually a quite a wedded question which
will come for your controlling
department and also the average shopping
cart size per order and you wanna maybe
know this by hour and order to find out
when if the peak shopping time of my
customers these are all questions which
sooner or later will most likely pop up
so what can we do first we are like I'm
just showing you this as a format in
order to like I understand the next
queries I'm going to show you
so which order is obviously make quite a
complex beast but in this example it
gets a creation timestamp and it has a
total and usually an order has like
items
and tags and the currency and
are getting really fun if you start with
item cancellation out of it because then
you have to recalculate your whole
shopping cart how many orders are
created this is actually something which
is really easy to answer with
elasticsearch because all you have to do
is an HTTP GET request on the orders
slash order slash underscore account URL
and you're getting back a number but
maybe this is not the most complex
crutch question you want to have
answered if you want to know which
orders were created in the last month
again this is very simple
you just call it order endpoint this
count endpoint again but you have the so
called range query which simply limits
all the results between the first date
and the last date greater than equals
and level then though the next question
might be how many orders were created
per day so you actually have a bucket of
like 30 days for the month of September
and you want to know like the chance at
25 orders so you still need to filter by
month but you also need to create
buckets and count per day this is where
faceting and aggregation actually comes
in if you remember the github screenshot
of the beginning the languages and the
aggregation information also was simply
a facet it was just a count for the Java
language it was just account for the
Ruby and the JavaScript language so it's
just some sort of pocketing which has
been happening there and you can also
simply count by day in this case you
create a so-called facet you give it a
name like created in this case you
specify a type like date histogram and
the field created at which was part of
the orders to be indexed and you have to
specify an intervals could be one week
this could be one month could be a
quarter of the year depending on which
scale or kind of scale of data you want
to look at and you're obviously gonna
need this kind of facet filter which
simply filled out the data for the
specific months so what is the average
revenue per shopping cart this is sing
which is slowly getting interesting for
a controlling department and what you
can actually do is you can create a
so-called statistical facet which
actually takes all the numbers inside of
the field adds them up create some
statistical values like min max average
standard deviation
anthro forth and you can basically what
I did she is I'm accessing the total
value field and I'm multiplying it with
one point zero zero point one in case we
have like 10 percent revenue in each
shopping cart it's it's most likely a
simple calculation because you will want
to have this per product if you have a
huge product catalog and I'm adding two
because most ecommerce retailers make
their money actually with shipping costs
and they can easily add a static amount
of money for shipping cost and this is
where most of the revenue comes at with
low budget it's orders so this is all
nice but this is not really complex
questions I can do this with a little
bit of sequel as well so where it gets
really interesting is to like apply
several facets at once like I'm taking
the input of one facet like the
statistical one and I'm marketing this
over time like per day so what is the
average order value or the average
shopping cart size per order per day and
this is where questions are slowly
becoming more complex and this is where
it's going to be complex for elastic
search because you need to combine every
type of a facet arbitrarily now you can
do this with the upcoming release of
less search 1.0 which is really flexible
in this case and I'm going to show you a
later on how this works and obviously
you can also do this by simply changing
your data and re-indexing it by like
already having an aggregated field in a
pre computed computation and putting
this into the index and then running a
statistics facet but this is a big
problem because every day your question
changes and you would need to change
your data as well and re-indexing this
doesn't scale obviously so keep this
question in mind and we come back later
to that and how to solve it pretty
easily another step you want to do like
when you now started to extract the
first kinds of information the first
bits of information and you you know
what's going on with your orders maybe
is yeah your boss asks you to create the
daily Axl sheet you would want to have
by email at 8 a.m. and you start hacking
away this wonderful XLI brief
Ruby and Java and no you don't want to
be the extended arm of your boss and
grabbing through log fights or like
order data and do this every day on a
different manner and Jason itself also
is not a management compatible notation
which is actually returned by
elasticsearch so you can just forward
the request to your boss and tell them
to be happy and also writing your own
visualization app for all kinds of stuff
this is not going to work in the long
term and this is where Cabana comes in
handy because Cabana is basically a a
small thing on top of elasticsearch it's
an own JavaScript application which is
able to query elasticsearch get back the
data and make wonderful graphs out of it
keep on itself it's you can basically
just clone the github repo put it on a
web server and that's it to get up and
running initially what you can do with
Cubana is to create this kind of
dashboards like in this case it's a log
file data from an apache web server
which is maybe interesting to know about
the distribution how many HTML files
you're actually sending back to your
client
how many PHP files how many images and
you can see at the top that there's a
query for file endings for HTML PHP PNG
gif on CSS then you can see a
distribution over time so you can pretty
much see that there's a traffic peak at
noon and this web server log so maybe
it's a lunch planner I don't know what
this kind of site may be and you can see
you can you can use a pie chart so this
is a completely configurable dashboard
so you can pretty much fit this on your
use case and you can save a dashboard
you can share it with your boss and he
can like drill down on the data further
if he wants if you like just want to
know how many people from China are
accessing HTML files this is very easy
for him because he just needs to modify
query and the query language is pretty
simple yet even as a dark theme now I
mean the important part of this image is
obviously that there are two problems
owns inside of this image and if you
have a web server lock spanning over 10
Apaches it's going to be pretty hard to
find this problem without the help of
visualization so this kind of tool is
not only an 8 for you
but if you're indexing Apache log files
it can really help you to like identify
problems find out what is going wrong
and at least know that something has
gone wrong at 5:00 a.m. this is not most
likely not the time you're going to be
awake and check all your webserver log
files another thing you can optionally
do in new releases is you can use geo
locations of the data and like draw it
on a map and you could see like where
other people are actually coming from
visiting your site or in in the context
of your orders you can actually find out
where other people are ordering form and
another nice functionality is that this
geo location not only works for for
specific location points but you can
also if you index the iso country code
like de for germany like DK for Denmark
you can use a special widget which
basically draws you this country map and
shows the distribution on top of it as
you can see in the in the widget below
and you can also show trends like you
can see that the tweets which are split
by language in this example that the
Turkish tweets are actually rising by
8.33% so you can see what is actually
happening and changing in your
application and this is a kind of
dashboard information you boss most like
these like you wants so this is will
what's a small overview of Cabana cabana
is a very nice tool you can create these
dashboards you can like let some cells
update every 30 seconds so you can just
wreck mount a huge LCD in your wall and
see actually what is happening in life
system but the thing is you're always
looking at data which is kind of in the
past and the next step you option you
would want to do is like to know stuff
now because they are definitely events
which can happen which you really want
to know in the moment they are happening
and if you take an arbitrary application
totally independent of how it's been
implemented these are events which can
absolutely happen like the first one you
have a checkout application and it
suddenly takes more than two seconds for
your payment provider to process a
response you send this is not a problem
per se but this might be a problem
because
your payment provider is offline and
you're not going to be able to process
any checkout and you might just want to
call them about that same goes for
credit card fraud detection maybe it's a
huge order of several 10k and the fraud
detection kicks in and cancel this kind
of order maybe you want to know about it
and processes manually you're sending
out television commercial your visits
like exploding you might want to spin up
several ec2 instances and if you have a
sort of voucher in this television
commercial and it's gonna after one
minute of this television commercial
being sent that might be a good good PR
for you another use case from an
operational perspective is the memory
usage exceeds the physical memory this
basically means that your process has to
swap out and is going to be slow this
doesn't mean that your application is
offline but that might indicate a
problem after all and these are all
events which are not like bringing your
application down but you want to react
on this kind of events and you want to
know it in the moment they're happening
and you don't want to check qivana from
the log files of 5:00 a.m. see maybe I
should have done something about that if
you don't know this is from Java volt
this is a library called metrics which
is a pretty awesome thing it's written
by Koda Halley who also wrote the drop
as a web framework and the most ya
likely library like that in Java volt is
German short for Java monitoring it's
library which has been around for 10
years and basically allows you to gather
live statistics inside of your
application and you can like have
several different types of metrics like
a meter which can measure events over
time like requests per second you can
like have a have a timer which can
measure the runtime of something like
the connection to your payment provider
API and create statistical statistic
data about it and this is a very nice
feature about health checks which
basically allow you to check if
something inside of your application
works common Hadj check would be to
connect to your database and see if it
works
usually people now tell me but I already
have my monitoring holes connecting to
my database and see if it works
but it's not a useful information if you
know that you're monitoring house is
able to connect to you
database it's useful information to know
that your application is not able to
connect to your database and this is
where whole checks come really in handy
and metrics can report to elasticsearch
and even if you decide not to use
elasticsearch at all after this talk I
definitely encourage you to check out
the metrics library because it's awesome
the metrics library from a Java point of
view is pretty easy to use you have one
huge class called the metric registry
where all the metrics are like
registered then it's easy to create a
meter or timer as you can see here the
meter was one to like collect
statistical values over time like the
requests per second the timer one would
be the one you use when you want to
measure how long your external
connection to your payment provider
duration is but the main problem with
the metrics library is this is always
life data so it's being updated in the
background and again you need to store
this kind of live data sometime
everywhere to like create statistics
about it and this is where lastic search
comes in you can basically configure the
metrics
library to report every minute most
likely to elasticsearch have this kind
of metrics start there and use it for
like pre-processing later on the metrics
elasticsearch reporter uses HTTP so it
doesn't need a huge elasticsearch
dependency it's independent from the
elasticsearch version so no matter if
you see most up-to-date one or little
older one and it has another really
really nice feature which is called a
real-time notification we our population
so real-time notification basically
means you can do something if you hit a
certain metric like the disk space used
for the while file system exceedance 90%
you may want to know about that the
reported stuff is available in the
github repo but before we go on I should
explain what percolation is except a
nice feature inside of elasticsearch the
normal process of indexing data is that
you index document and run a query
against it like an indexing MacBook and
I'm varying for MacBook and I'm getting
back this kind of data but imagine I'm
searching for MacBook with a price of
200 euros which is not that much I won't
get a document back but I may want to
register this query to get notified as
soon
this kind of data is indexed and this is
a classic use case for implementing a
price agent and this is where a
percolator really comes in handy and you
can if you take close the look of your
data you can imagine pretty crazy stuff
with it there are customers using it for
classification like getting back some
sort of text and adding this text to the
document automatically and obviously you
can use it for this kind of metrics
application as well so this is how you
basically create a population support
for it so you have an elasticsearch
reporter which basically runs every 60
seconds to report its data to
elasticsearch you register notifier like
a pager notify in this case and you
apply the names of the metrics in this
case every metric the population should
run against you have to sort of
registered and then you implement your
page and notify a class and whatever
should happen there most likely you want
to trigger a call or you want to trigger
RabbitMQ message which then gets
forwarded to another application which
takes some sort of action together with
the metrics elasticsearch reporter which
is on the elasticsearch github account
there is also a small sample application
called cockpit which is basically two
applications one is a java application
which constantly reads out an open
stream from the US government showing
which site is visited right now like
nasa.gov or whatever and it takes some
metrics about that and collects how many
lines per second are read from the
stream and the other application is this
one which is a small node.js application
which basically just can draw awesome
graph using d3 GS obviously and you can
see the green the green rectangle on the
left side and this is basically the
result of a notification being sent to
this node.js app and it's forwarded by a
WebSocket connection to all the clients
and this means it's basically real-time
the moment it happens in the metrics
reporter it's reported to the client so
if you want to play around with it check
out the small application and see how
you can again build your own dashboard
and wreck mount it to your wall in the
wreck moment where I see a panel - what
the next step would obviously be to like
go from data to insight so right now we
went from data to like extracting some
basic information but seeing the
limitation of this kind without having
to reenact your data we are able to like
get a first overview about it
we are visualization and we are finally
able to like get notified about it but
what we really want at the end of the
day is like we want to know what do we
need to do next so it's more like
getting to the predicting business and
the thing about predicting stuff is that
it's basically pretty complex the most
important thing is that you need
long-term data this is the only chance
of like being able to know what's going
to happen next
visualization is an awesome start
because Cubana will help you a lot to
find out when you reach the next
possible level of scale where you need
to add hardware to something but the
most important thing is you you have to
like sort of in sauce this kind of data
you can't just push it to some service
and hope it will get back to you with
useful information so you have to know
your data and you have to know your data
format and the last thing is that's not
a problem there are lots of people
inside of your company which have
questions about the kind of data you're
indexing in which which will have
awesome ideas what they always had
wanted to know but didn't dare to ask
because you didn't like show them a
solution to easily extract data from a
huge amount of data so there's a new
feature called aggregations which will
come up in the elasticsearch 1.0 release
the nice thing about aggregations is
basically you can take the output of a
facet operation like the data through
Graham the bucketing into values per day
and use it as an input for the next kind
of operation like calculating the
average value of your shopping cart and
if you remember what is the average
shopping cart value per order per hour
we can now pretty much and easily answer
that one those looks a little bit more
complex than the last one but if you
take a look it uses EGS aggregations
feature you specify a nice name for it
average shopping cart per hour you again
use the
date filter and the data to Gram facet
but you use the aggregations field to
like greater calculate a total of the
average and this is what you can now
actually have the drawback with this
solution is it actually creates a bucket
per hour and then average value this
means for one months you're going to
have like 30 x times 24 buckets which
makes it 720 I think or something around
that and this is not something you want
to draw easily so what you actually
wanted to know most likely is are the
most of my customer or the average value
of my shopping cart
is it higher between fifteen and sixteen
or four and five o'clock or five and six
o'clock you would have to change this
query a little bit and still use a
histogram facet or histogram aggregation
but using the script to access the date
from the created egg field and just
create a bucket per hour of the day and
this means you can like easily answer
way more complex questions with the
aggregations because you can like go
arbitrary deep obviously to the cost of
some performance because these are all
in memory calculations but it's at least
it's possible and this one would create
exactly 24 buckets each for one hour and
add up the average value for your
shopping carts so this is what I
encourage you ask complex questions and
try to extract them out of your data for
example you've got the product page
views which might be interesting but if
you can combine the sum of product page
views with the price range you can
actually find out pretty easily if
you're a low budget shop or if your
premium shop so you can actually see
pretty good what your customers are
thinking of thinking of your shop
instead of like having the perception
what you think what your shop actually
is another thing might be geolocation so
imagine a huge Furniture house like
sending out a newsletter twice a week
and they may want to find out</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>