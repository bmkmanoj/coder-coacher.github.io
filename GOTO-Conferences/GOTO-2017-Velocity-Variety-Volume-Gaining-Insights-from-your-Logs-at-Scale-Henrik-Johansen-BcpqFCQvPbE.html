<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Velocity, Variety &amp; Volume - Gaining Insights from your Logs at Scale • Henrik Johansen | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Velocity, Variety &amp; Volume - Gaining Insights from your Logs at Scale • Henrik Johansen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Velocity, Variety &amp; Volume - Gaining Insights from your Logs at Scale • Henrik Johansen</b></h2><h5 class="post__date">2017-11-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BcpqFCQvPbE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm here to talk about velocity variety
and volume gaining insights my name is
Henrik if you want to talk to me after
this conference please use my Twitter
handle because it's probably the only
school should meteor know that I'm using
at the moment and I'm a bit of an odd
fellow for go to at least because I'm
not a developer but I do write a lot of
code I'm not necessarily a system
administrator by it
I do have Minister a lot of systems and
I'm involved in architecture without
being an architect so I'm kind of a
generalist that does let lots of
interesting things when it comes to log
management my my first log management
solution was something I built sixteen
years ago give or take and it was
industry standard centralized is logging
a couple of years later we did something
that actually had a my sequel database
back-end and that thing failed rather
spectacularly when you imagine you have
a two terabyte database back in 2006
2007 and to try to run all the table
statement the worlds just ground to a
halt in 2000 2011 I discovered we
discovered elasticsearch that was I
think just before the 0.10 release and
locks - as well before 1 . oh and we've
been running that for a couple of years
in 2013 we switch to gray lock which
we've been using for a couple of years
and given the the experience that we
gained you doing all of that stuff we
essentially ended up building our own
log management pipeline because I love
locks and because I so often dig about
locks I'm sometimes called
affectionately called sir locks a lot
I work for a public sector entity the
region of South Denmark and for those of
you that are not inherently familiar
with what a region does Denmark is
divided into five different geographic
regions and the main responsibilities of
a region is the fertility facilitate the
health care operations within a a
specific region for us that means I
don't know 14 15 16 hospitals a huge
number of social institutions sack
record wards and while regions have
other responsibilities such as tracking
soil contamination and all kinds of
other stuff health care is what's
probably 99 point something of our
entire budget an organization this size
can generate a ton of data so we are I
think we're logging a couple of
terabytes every day we can ingest and
generate hundreds of thousands sometimes
even millions of messages per second and
we see a tremendous amount of good and
really really really bad logging formats
what are we going to talk about today
well I think that we should start with a
general discussion about why I think log
management is important for an
organization while it is important for
developers why it is important for ops
team security teams etc I will hopefully
you walk you through with a tour of what
it is we ended up building to manage all
of that complexity and I will show some
hopefully interesting use cases of what
we actually do with all of the data that
are a bit unusual so why the hell should
you care about log management I have a
fairly strong belief in log management
being a very good tool for managing
complexity if you look at an
organization of the size of ours we have
twenty-five thousand employees tens and
tens and tens of thousands of devices
and systems and workstations and
Printers and drug infusion problems and
all kinds of other stuff this entire
thing actually looks a lot like a
complex autonomous system we are so big
that I mean it's it's not unusual for an
organization that the left hand doesn't
really know what the right hand is doing
but in our size one finger on one hand
doesn't know what the other finger on
the same hand is up to and locks provide
a unique insight into what what is
happening in this giant machinery I mean
I've seen some of your faces go blank
after going all philosophical on you but
I mean it all really boils down to that
locks can help your organization lops
can help your development team you
administrative people people do running
your servers doing a security to
overcome some of the implicit challenges
of such a system and stuff is getting
more and more complex every single day I
mean how did we go from deploying our
app on a single machine to all of a
sudden having to manage kubernetes not
ditching on kubernetes but but
complexity is increasing every single
day like I briefly mentioned we have
tens and tens and tens in tens of
thousands of different devices some of
them pretty unusable
I think the minority of you have MRI
scanners and drug infusion pumps and all
kinds of other stuff but we have the
usual workstations servos phones and we
have a presence in over a hundred
different locations within our little
tiny part of them are we need data to
understand the complex interactions in
our distributed system sometimes things
can fail with no obvious reason and
sometimes an input to the system can
have a radically different output than
then you would realize data can come in
a lot of different forms metrics
monitoring checks documentation
guidelines but
logs are also a pretty significant form
of data what is a log really I mean if
you think about it it's actually pretty
simple a log is pretty much nothing more
nothing less than a record of activity
something has happened it gets written
down in a log file but logs are also
inherently multi-dimensional compared to
metrics if you if you collect metrics to
gain an insight into infrastructure you
normally have a number representing a
thing so CPU usage memory utilization
disk i/o logs are multi-dimensional
because if you think about something
really simple like a log line from a
phone from some web server it usually
contains 7 8 9 10 different pieces of
information within that log line the
request path status cooked bytes
transferred response times perhaps even
the user that was logged in doing this
so logs are an inherently
multi-dimensional logs are also data
data can become knowledge if you treat
it correctly knowledge can at some point
become inside and inside can provide a
fair amount of context in terms of what
what is happening in your infrastructure
what is happening in your application
so what what did we end up billing well
we had we sat down around around a table
and draw some stuff on him and on the
whiteboard and because we have we have
tried this a number of times I mean we
have tried different solutions and all
of them failed at some point due to
various constraints due to various
things that we didn't even think would
be would be interesting for us to look
at so if you look at the the purely
technical requirements they were
actually pretty simple we needed
something that could reliably ingest a
million events per second
we needed something that could reliably
ingest terabytes and terabytes of data
and they perhaps more interesting thing
is we needed something that can work
with an arbitrary amount of different
formats because we don't we don't
control the format of the data that that
ends up being ingested into our system
the architectural requirements were
actually a bit more interesting we
wanted something that was reliable and
ops friendly because we had some bad
experiences running a couple of the
previous systems that was something that
was was really really interesting to us
the entire pipeline was built and
maintained and run and operated by a
very very small team so the less time we
spent fixing this the more time we can
spend on our other duties and improving
this little system we wanted to
encourage a high degree of decoupling
make it as micro service architecture
like as possible because we would like
to be able to grow change and devolve
parts of this infrastructure
independently from each other we wanted
to utilize modern development and
operations that strategies which might
come as a bit of a shock given that I
come from from from the public sector
that's not necessarily something most of
you would would think of when you think
about a government entity and we
actually wanted this thing to function
as an asynchronous integration platform
so we essentially could take any form of
input do something to it and throw it
into any kind of output
so essentially what we ended up with is
a distributed stream processing platform
for contaminants we needed it to be
optimized for high velocity and high
volume we wanted the outcome of those
events to be optimized for certain
analytics
while allowing a high degree of
variability in the input data we wanted
it to be resistant to failure but we
also wanted it to encourage
experimentation so that we could
continue to evolve and play and and
figure out new interesting stuff that we
can do with all of our data and we
wanted an architecture that allows us to
change almost any component without
having a major impact on the platform
itself and it needed to be somewhat easy
to run and easy to scale so what what
did we couple together to build our
little little little baby well if you if
you know about Kafka if you're perhaps
even already using Kafka then you
probably realize that that what I've
been talking about screams Kafka how
many people here know what Kafka is how
many people run and operate Kafka on a
daily basis for those of you that don't
I will give you a very very short
breakthrough or breakdown of what what
Kafka is all about Kafka is a an event
bus a messaging system that came out of
LinkedIn a couple of years ago
conceptually and Kafka notes Kafka
servers are called brokers you can have
multiple brokers to form a Kafka cluster
data that gets ingested into Kafka is
put into a topic you can think of a
topic like like a database a topic is
then split into a number of partitions
which are the actual files that end up
being on disk and you have consumers
that can read data out of Kafka topics
and you have producers that will
actually produce data and push them in
to counter the
Kefka can be a bit challenging if you
can ecstatic with it but but I mean if
it's getting better and better and
better with every release and they're
adding more and more features that make
it more and more office friendly so it's
it's actually been not as painful as you
think
we always keep a number of days of our
data inside Kafka Kafka can do and per
topic you can actually configure Kafka
to say I want to store the last three
hours the last seven days the last two
weeks so everything that we ship from
our servers from our switches from our
endpoints from various appliance boxes
medical technical devices whatever you
could think of once it reaches Kafka it
is persistent on disk meaning whatever
else happens to our system as long as
Kafka is running and Kafka is happy we
can always recover the data because we
have data in Kafka we can rerun some of
the stuff that we've written some of our
services if we figure out we had in the
regression if we want to play with new
algorithms then we can simply run our
code and instruct the graphical consumer
client to start at the highest available
or lowest available offset and it will
actually read in a day a week a month of
production data so that the tests we are
doing I actually done against real time
a real live production data they the
theme that is has been has been evolving
over the last year it's kind of this so
we have different producers pushing data
into it into different topics we then
have a consumer group which is a group
of Kafka consumers pulling data out of
the topic they do something with that
data they then ingest that data into
another topic that get picks gets picked
up by another consumer group that does
something and ingest it into a new topic
and the list goes on and on and on and
on until we finally hit some form of
destination where those logs are getting
shipped to
the interesting part about this is
actually what's happening inside all of
those consumer groups because what we
ended up building is we wrote a number
of small services that can work
independently of each other and do
something to to all of the data
currently we have 29 distinct services
that do distinct things to our logs and
the interesting thing about about micro
services because again I'm not a
developer but but looking back at the
history I kind of think micro service is
a bit old I mean if you think about it
that command in itself could be
interpreted to be a number of micro
services working together to form a
function if you agree with a notion that
you should write services that do one
thing and do one thing well and if you
agree with the notion that you should
wide services to work together and that
services should be exposed to a
universal interface in this case tech
streams you have actually just
acknowledged that micro services are
dating back to the late 70s early 80s
when the UNIX philosophy was
crystallized by Rob Pike and Bryan
Cronin in a paper in 1984 which is kind
of interesting again we have 29 distinct
services and I thought it would be
interesting to look into some of them to
figure out what what are we doing to all
of the data that comes in in a
never-ending stream of different formats
we need to have some kind of event
passing and some kind of normalization
so that data makes sense when it comes
out the other end this particular
service is written in Ruby we utilize
some of the same libraries that elastic
locks - does to support croc patterns to
allow us to essentially not having to
reinvent the wheel every single time
because there are not a lot of
interesting lock patterns out there for
many of the normal lock formats that you
would see
this essentially allows us to met a
number of passing rules to a given topic
so every message that hits is this topic
or a topic is of the same kind and would
be essentially passed in the same way we
do have a few conditionals in there to
allow for small small variants in the
actual incoming data and to look at okay
if we detect this string then run this
passer if we detect this string run
another possum but but essentially this
is step one everything in a lock gets
gets broken down and converted to JSON
we do some interesting things if you see
this this actually tells you when I when
I started writing much lines today
excellent example this is a Cisco box
sending some obscure syslog format but
with the right rules what comes out the
other end makes a lot more sense you
could now break down connection IDs you
can look at devices you can filter on
Direction ports destinations which we
are if the traffic passed through you
can do a lot of interesting stuff as
soon as you start breaking them down
into separate fields to do searching in
analytics what we figured out pretty
early in this process is that we we
don't really want to mess too much with
the original event format so we we
practice the not a second I wouldn't
really call it non-intrusive but at the
little amount the smallest amount of
intrusion that that we can get away with
in essence you should always be able to
recognize the original format if you're
used to looking at syslog events from a
particular appliance box or a particular
network vendor or windows event logs you
should always be able to recognize
whatever you're seeing as such an event
but that also has some inherent problems
because I think in our environment we
have twelve or thirteen different field
names for fields that contain a source
IP address that gets really really
complex because you have to keep all of
those rules in your head about how to
structure your
depending on whatever data said you're
looking at so what we actually do is we
append a number of fields to each and
every log that use the same notation the
same nomenclature so we always have
source underscore IP regardless of what
is in the original event format we
always have source host etc etc because
that then allows us for easier searching
once data comes out the other end and we
always keep a copy of the original event
in a separate field and for various
reasons one of them is compliance the
next service that sounds a bit
ridiculous is actually counting we count
the occurrence of everything you see
every process every dll every user logon
every file hash every domain that gets
visited every IP address gets counted
and that is actually a pretty
interesting problem I've started
rewriting this in C because I I don't
know I felt sorry for myself one day and
wanted to be really really really make
things even more difficult but the
interesting thing is distributed
counting is an interesting problem we
have a number of we have a consumer
group doing this meaning we have 10 15
20 30 100 different services or instead
instances of the same service doing
counting and doing this reliably in a
distributed way it makes it kind of
interesting once we have a count that
actually allows us to give some context
to an event how many times have we seen
this particular thing in our environment
if I'm looking at a process that gets
executed 500,000 times in a week across
25,000 workstations that's probably ok
if I see an executable executed on one
machine and that's the only time that
I've seen it executable I start to get a
bit curious about what what is going on
this also allows us to implement
detection types such as first seemed
like the first time we see something in
our environment
a file hash an exception that is
inherently interesting to us lower
casing it's it's actually a bit
hilarious but but we have a dedicated
service doing nothing but lower casing
of the actual field names that you're
seeing and for some event formats even
the entire event again the backend that
we are using for for doing searching and
discovery and analytics and stuff this
case case-sensitive so sometimes you
miss stuff just because you forgot that
there are some log formats somewhere
that wrote something but a capital A if
it's the first letter in the event and
this really really helps with search and
discovery and the analytics part of what
it is we do DNS lookups we can we can
generate hundreds of thousands of DNS
lookups and a lot of the event formats
that we're ingesting contain an IP
address but not the actual name of the
machine doing whatever it's doing and
that in our environment causes a problem
because my work well laptop can have
four or five different IP addresses in a
single workday wired Wireless different
locations I start early and work from
home or I work late over VPN we need to
assign some common denominator and that
usually tends to be the name of the
machine so we have a pretty significant
part of our infrastructure doing nothing
but caching dns lookups so that we don't
we don't trash our our authority of DNS
servers with all of this entropy
calculation we have a service twin
shannon entropy calculation to estimate
the average number of bits needed to
encode a string of symbols this
essentially helps us to filter out names
strings that are inherently random in
nature perfect for looking at something
like DNS locks for looking at process
names to figure out the stuff that that
kind of stands out compared to the rest
of
the system we also do a fair amount of
vector selector lookups every time we
see something that we can identify as a
machine or as a user we look it up and
we amend a number of information to the
event something like where does this
person work what's their actual title
what do they do
where is this particular machine located
for reasons that I will talk about in a
minute we have 29 of those services like
the ones I just went through doing all
kinds of interesting stuff and the
trickiest part of our entire
infrastructure is actually this if we if
we if we need if we want to maintain
velocity in terms of passing a million
events per second we can't really wait
for external connections stuff just dies
we can also kill read as instances in
the blink of an eye so we need to be
really pretty clever about how many
instances rerun here we run them how we
shot the data over them because it can
it can be quite complicated actually but
this is essentially what helps us to
maintain a high velocity in orangist
rates and then there's this little gem I
mean it there is a reason for for me
losing my hair and this is actually one
of them time formats are probably the
thing that that has caused the most
amount of frustration when you're
talking log management I mean come on
the number of hundred nano second
intervals since January 1st 1601 UTC why
why Microsoft was it necessary to invent
yet another timestamp format I mean it
just doesn't make any sense at all
essentially what all this ends up with
is we built a multi-dimensional data
model that lets us inspect and look and
correlate data and basically on a number
of different angles
assumptions we can look at process
execution by title like this is the use
of this application is this process
normal foreigners working in an
intensive care unit yes or no we can do
a lot of really really interesting
things because we have added a lot of
metadata and every layer in our matrix
becomes another opportunity for
understanding and doing detection and
doing all kinds of interesting things if
you remember the the architecture slide
we talked about being being modern and
being ops friendly the team that builds
runs maintains this is three and three
and a half three and half people and
they have a lot of other stuff that they
need to do besides doing that so we we
decided to go all in and everything in
our infrastructure that runs and
facilitates this this our little
application is actually running
containers even though many of the
people that are working on this our
system administrators security people we
try to embrace continuous improvement
continuous delivery and and a lot of the
other stuff that makes application work
interesting nowadays in our particular
case we want docker containers we have a
github instance where all of our code
gets gets gets gets stored we have an
image registry we have a built pipeline
that that actually ends up deploying our
images to production so we can move fast
and be relatively safe as we don't go
breaking things and we actually have a
bit of code that we have written
ourselves because we have a scheduler
for our consumer groups that I wrote in
in Ruby that allows us to introspect the
consumer lag in Kafka and scale the
number of consumers up and down
accordingly and hopefully I'll finish
writing the next part of this which will
actually allow us to manage Kafka
partitions so the amount of petitions
the topic has is basically the amount of
concurrent reads it can support and it
doesn't really do you any good to scale
the num number of
of Khafre consumers if you're not also
scaling the number of petitions but
that's that's a tricky problem and we
haven't have time to fix it
we embrace kind of the DevOps
methodologies so we I mean plan code
build test Willy's deploy operate
monitor should and shouldn't be of any
surprise to any of you but it is
interesting once you start doing this in
a very very traditional organization
like where do you where do you do our
C's in there I mean how do you integrate
isolate processes against something like
this that's the thing some of the things
that that make this really really really
interesting from an organizational point
of view because you start challenging a
number of constructs and concepts and
processes and that that would otherwise
would simply disallow you to run stuff
like this when data comes out the other
end
we pump it into homeo which is a pretty
nifty lock management platform that is
developed here in Denver and we use it
for basically anything search discovery
archiving storage visualization alerting
everything we do has homeo at its core
so when Kafka is the backbone of this
application and our micro services of a
central nervous system homeo is kind of
the beating out that's all main the main
user into action towards the system we
will end up having 150 200 users once we
have finished our migration process and
the security crew the operations team
alone generates something in the order
of three and a half to four thousand
queries the week so it's it's it's
pretty substantial for for our security
for operation teams we're looking into
how we can make it compliant with the
different legislative comply
requirements so that we can actually
store some of our most sensitive data in
there that's
going work that that hopefully looks
like it's not not not such a big issue
as you noticed trending has little
estwick sides it because I don't think
that the hall ventus code has actually
been been released to the public but
there is something coming in terms of
doing forecasting which is really
interesting and I think we have
succeeded
250 300 different alerts that we run
just on top of all of it that nice data
I mean it's great we have a lot of
events we have structured them we have
normalized them we have added a lot of
meaning to them but what do we actually
use them for again we have an enormous
amount of stuff logs everywhere and I
would I would like to at least showcase
two very different examples of what you
can do with data like this one of them
is is actually using logs for behavioral
detection behavioral detection of
malicious unwanted activity inside your
infrastructure we run a pretty
substantial deployment of bro-bro is a
network security monitoring system that
is written in grow the language how many
of you are familiar with bro or bro the
language yeah my team the interesting
thing is bro is actually a
turing-complete language that uses an
event based programming mode it has a
lot of domain-specific data types like
IP addresses and ports and extensive
support for tracking and managing
network state over time the interesting
thing about bro compared to other
intrusion detection systems is bro has
no notion of good or evil
bro just reports what it has seen on the
network bro supports a ton of different
analyzers so what bro basically does is
it detects connections it detects the
protocols that are being spoken of those
connections and then it runs protocol
analyzers to basically tell you what
this protocol is doing so it's as if
it's a DNS request you get a connection
log saying
somebody connected UDP port 5-3 on some
machine but you can also see what was
the actual query and what was the
response to that crew that all gets
locked in some pretty nifty text files
that are interlinked which make them
pretty interesting to use the
interesting protocol for for any
organization would be SMB serve a
messaging block which is one of the core
protocols in Windows operating system in
Windows networks a lot of stuff happens
over SMB besides sharing files and
Printers and we have we have a
particular use case that I think was a
little bit interesting because let's
let's assume that that we would like to
detect ransomware most people in this
room should be familiar or at least a
programmer some of you perhaps work in
organizations that have been exposed to
ransomware it's a very very annoying
problem when we start to talk about
about detection there is there is David
Bianca's pyramid of pain which the
higher you move into the top of the of
the higher you moving the permit the
more tricky things get for an adversary
it is really easy to change hash values
it's really easy to modify a document or
file and make it produce a different
hash it's also really easy for them to
switch their attack infrastructure to
using a different number of IPs
different domain names etc etc what is
really really tricky for them is to
change the way their malware operates if
we look into some of the into the data
that that bro can provide this is a
pretty good example which is actually
wrong a raw block we have done nothing
to it where you can see the actual this
is a rename event somebody renamed the
file where you can see the name of the
file the previous name of the file you
can see IP addresses ports etc etc the
interesting thing is you can now start
to track different SMB events such as
file open file read file write for
rename delete etc per IP address over
time
when somewhere has some duties to
perform I mean it has to encrypt the
content of the file meaning it has to
read some files and says it has to
delete some files just to change the
file content simply by looking at the
rate and volume of those different
operations and comparing it to a normal
baseline is something that you can start
doing detection on there are also be
lots of false positives like people
starting robocopy or copying stuff
around or copying their holiday images
to whatever file share but essentially
it is it is possible already using this
single log file the lead example where
you can start looking at and this is
probably even more useful because not
many people go around deleting messes
volumes of data but that's typically
something you would see in that summer
another interesting thing is you can see
the actual file name so if you really
want to move into the typical signature
based detection thing you could probably
feed in a list of known file types most
ransomware families have a number of
file types that they use when they start
renaming the can cryptid content and you
can start detecting based upon that but
what's even more cool is bro has entire
file analytics framework build in so we
can pull out files out of the network
traffic submit them for sandbox analysis
but Brod does entropy inspection so
broken actually helped us decipher how
random is the content of the file that's
being written if you start tracking that
over time you can then compare this and
say this word document that's being
written has a very very different
entropy profile to the other Word
documents that we normally see in our
environment and by combining the
different data types different methods
different queries you can actually
respond pretty quickly to something that
isn't affected by rain summer by looking
at behavior and behavior
another thing that I find very
interesting and we are kind of back to
the using logs to manage your complexity
thing is you can actually use logs to
discover relationships most of the logs
that your applications your
infrastructure your servers you whatever
generates has some kind of notion about
who is talking to what or who is talking
to whom and by modeling that you can
essentially create a graph of your
entire infrastructure and of all the
interactions within your infrastructure
one of the examples that we've built is
using graph theory to answer a number of
interesting questions
we run a over a thousand different
applications that can be anything from a
small standalone desktop app to pretty
substantial information systems about
patients and treatments and all kinds of
other stuff and we can't remember every
one of them from the top of our head so
if we if if I have to work with within a
specific system wouldn't be great if I
could figure out how this thing operates
you can actually do that by feeding the
right logs into something like new Forte
so you can look at this machine the
clients connect to this web front-end
server this web front-end server talks
to the database and that database
actually also archives files to that
file share which is inherently useful
because you kind of create a
self-documenting infrastructure you
could then start to look at what impact
do actual changes to infrastructure have
how many people here are developers how
many people here do micro service based
architectures how many of you actually
know exactly the number of upstream and
downstream services that are dependent
on your code this is one of the things
where where graphs actually can help
because you can model the entire
dependency chain across your distributed
application by using the logs that
already exists and it doesn't have to be
as hot if you look at something like new
4j
it is it is inherently easy to express
some of those changes and what you end
up with is something pretty that can be
inherently useful this is kind of the
current state of affairs this is what we
have been been spending I don't know a
year on because we have a lot of other
stuff that we have to do also and if we
look into what what will most likely be
tackling in the future we will expand
our usage of of Kafka we have run some
interesting experiments around K streams
and K tables by manipulating the actual
partitioning keys that data gets
partitioned over we will most likely be
able to join disparate sources of data
and key them on a common field like
getting an event stream for everything
that a specific user or machine or
service application has done despite
where that log those logs are coming
from and we can then turn this into a
table to obtain a real-time view
essentially a aggregated materialized
view that we can update every 10 minutes
15 minutes 30 minutes that can help us
to manage some of all of our data and
exposing the new k sequel interface so
that some of the people that are perhaps
not as used to some of those technology
can actually work with some of that data
using something they're familiar with
like NC sequel and we will probably also
start to look a bit into machine
learning we build a highly highly
multi-dimensional data model and there
are some algorithms that are very very
good in terms of working with that
something like k-means which is
inherently useful when it's talking
multi-dimensional data models and that
can potentially help off with doing
outlier detection and other interesting
stuff
ai is not something we've dabbled with
because that machine learning is where
we kind of draw the line that's what
what what beyond that it is something
that is that that's not really territory
that we would like to venture and
because we really have no idea what we'd
be doing
that's essentially all I had and I'm I
hope it was useful I hope it was
interesting are there any questions you
have Mike there's one question here
through the app so have you made any
considerations concerning the
forthcoming gdpr that's the general data
protection regulation yes we have for
organizations such as ours that's
actually a really really big problem
because PII and pH I is pretty much
everything we touch isn't that category
and this is also why we have decided to
keep all of this on print so nothing
ever leaves our infrastructure it runs
on the dedicated infrastructure that is
only accessible by very very few people
in our organization we have made sure
that everything we do has an audit log
even if every query inside whom you
actually get locked so we have a
separate organizational role that can go
in and figure out who has accessed what
data and for what purpose
and we are working with them at the
moment to make sure that even the data
on this is something that that it can't
really easily be compromised so even I
as an administrator I can't SSH into
that thing and then starting leading
some of the queries that I generated but
because we run it on Prem because we
have a very very limited exposure
because there is a permission model that
controls exactly who can see what data
many of the GDP our issues are actually
not such an headache for us thank you
any questions from the audience
I how do you handle a sensitive data
type we thing looks how we handle
sensitive data do you mask it or and we
have certain we have certain we have
certain types of data that we run
anonymizes over we also make sure that
data that contains sensitive material is
removed within the required time frame
because we can actually store some of
this for a shorter time in terms of
debugging and other stuff so there are a
number of guidelines that we do but we
would actually rather remove data than a
normalized data because one of the most
of the key fields that we use for
analysis are the features that we need
to analyze so that it doesn't make that
much and then we'd rather expire data
and say okay if you only can keep it for
a week we only keep it for week okay so
you haven't haven't a need for encrypts
on our opening that incur up some tea
mainly this is about infrastructure log
security logs and helping to find falls
in different devices and appliances and
all that stuff we the next the next
giant leap will be offering this to our
business as a service so going out to
the clinicians going out to the people
running the applications and having a
discussion of both of them about how we
can help them achieve some of the I mean
even some of the GDP are things around
that you actually have to document that
you check the access logs for your
application and stuff like that where we
are in a unique position to offer a
shared service that can help with that
but it's only then when we start to hit
the the tricky issues okay I have
another question as well how long you
have been doing this you said that you
have 29 different micro services we
started this journey I think a year year
and a half ago when we started feeling
the growing pain and some of the
constraints of the trade offs we made
with the previous iteration of this
system we started looking at how can we
do this better how can we do this
smarter and how can we solve some of our
problems and I bet you that in a month
in two months and three months from now
this is
we'll have changed in some way and
that's an inherent thing that we built
into the system is the ability to change
to grow to learn to fail to relearn and
to do other interesting things that can
helpfully achieve helped us achieve our
goals okay thanks
any more questions
costs
we spend taxpayer money so our our
foremost obligation is to make sure that
we spend taxpayer money correctly the
majority of the stuff that I've been
talking about is actually open source
it's either open source or it's
something that we developed it was a
conscious choice even though we have an
operating budget of 24 point 1 billion
the majority of that goes into
healthcare and even even if we could go
out and buy a support contract for
something like Kafka and we are under
some very very interesting purchasing
rules as a public sector entity and we
kind of want the core of this system to
be free we wanted something that that
even if we have to do a public tender
and somebody else wins a bit we can
change different parts of infrastructure
without changing the core of it every
single time imagine if you need to do a
public tender if you have somebody
coming in and underpinning the
competition and you every 3 or 4 years
have to enchained the entire system you
have to port all of the rules all of the
alerts it gets really really cumbersome
and the stuff that we are paying for is
some of the stuff that we simply
couldn't do better ourselves so we have
some some endpoint telemetry sensors on
our workstations that log all the DLL
loadings process executions registry
riots faster getting access to all that
stuff and we have Fumio and both of
those are actually a lot less expensive
than youth and you would think it was it
politically enough answered any more
questions
what was the major driver to move from
elasticsearch to whom you that's an
interesting question over here like like
I said we have been we've been running
elastic for four five six we still run
elastic I mean we still have parts of
our world infrastructure that we have
moved into this pipeline yet because we
have actually 100 150 people doing
service that's work that use the system
every day to figure out why the hell
somebody's account gets
keeps getting locked out those people
need to be retrained we need to get them
into the new system those operations
take time essentially we move from locks
- to Greylock in 2013 because locks that
has some inherent issues around
availability there is no read wasn't
really no way to introspect the system
to figure out what the hell was going on
it keeps failing and keeps throwing away
data through place blocks - with great
lock and we've spent we spend more time
managing elastic over the last year then
developing this entire pipeline when you
start when you start to move into the
realm where you have racks of elastic
servers you often also need people that
are trained to handle that system if
elastic is part of the application
already I mean if you use it for low
latency search for something else then
you probably have people who have a good
idea about how that thing works but for
a logging specific purpose it was it was
simply because of stuff that I could do
too much because we inherent we inherit
all of the complexity because elastic
can be used for anything from doing logs
to chasing Ebola outbreaks in Africa the
other of the third thing that made us
change was the inherent trade-off in
most of this system the most of the most
of the system that that we look at in
terms of velocity and volume most always
centered around the notion of are you
doing schema on Rideau scheming right in
our elastic cluster and 80% of the
resources are being spent by maintaining
indexes for search who Mayo has a rather
different approach because they don't do
much indexing they do brute force
searching this allows us to have a
relatively small amount of resources
that can ingest even more data than our
gigantic three react elastic
installation I mean the switch alone I
think we saved something like a hundred
thousand euros in hardware and licenses
and stuff by making that switch so for
us and for our specific use case it was
the right choice
elastic is a great product I like what
they're doing
but it just wasn't the right choice for
us Thank You hammering unfortunate I
think we're off this time now so yeah
thank you very much for listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>