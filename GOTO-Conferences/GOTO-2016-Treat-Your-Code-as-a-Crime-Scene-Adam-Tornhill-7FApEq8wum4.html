<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Treat Your Code as a Crime Scene • Adam Tornhill | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Treat Your Code as a Crime Scene • Adam Tornhill - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Treat Your Code as a Crime Scene • Adam Tornhill</b></h2><h5 class="post__date">2016-10-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7FApEq8wum4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Thanks and good morning everyone and
welcome to code as a crime scene today I
want to give you a different perspective
on your code base I'm here to
investigate the traces we all leave
behind as we create our systems and you
will see that there's a lot of valuable
information of traces it's information
that will help us predict the code
that's hard to maintain the code at risk
for the effects and the code that
becomes a team productivity ball neck so
let's jump right in and have a look at
what we programmers actually do with
programmers we don't write code yeah
sure we do that as well but that's not
where we spend most of our time if you
look at the research you will find that
we actually spend a majority of our time
making modifications to existing code
and most of that time in turn is spent
trying to understand what the code does
in the first place and what that means
if that is that if we want to optimize
any aspect of software development we
should optimize for ease of
understanding that's where the big win
is this is a challenge here's why please
have a look at this visualization I will
claim that that's where most code bases
eventually end up it's pretty
complicated to reason about which parts
are hard to maintain
where will the bugs be or the run a team
productivity ball neck without more
context we just cannot tell and this is
a problem that gets harder with the
scale of it because today's systems are
developed by multiple programmers
organized into multiple teams and that
leaves everyone with their own view of
how the system looks no one has a
holistic picture and this is not a new
problem this is a problem that's been
around for decades and we do have a
number of traditional solutions to this
within the software industry so I'd like
to start this talk by having a look at
some of the traditional approach
to tackling those problems before we see
the limitations of those and then we
move on to something entirely different
so if you want to know which part of a
code that are hard to maintain or
complex why don't we just measure it
using complexity metrics so let me ask
you how many of you have been using
complexity metrics of any kind so few of
you maybe seven eight people great
thanks so we actually have a number of
different complexity metrics to choose
from and this is just one example this
is one of the most popular approaches
this is called mccabe's cyclomatic
complexity and what you do in mccabe's
cyclomatic complexity is basically you
consider your program or your function
as a graph and then you calculate the
number of possible paths through it and
then you sum up that number and the
higher that number the more complicated
your code and this kind of makes sense
because it ties back to one of the most
limiting factors we have in programming
it's a cognitive factor it's a working
memory there's really just so much you
can hold in your head at once so I would
say that McCabe definitely makes sense
why don't we use complexity metrics more
I don't know but I know the reason that
I don't use them the reason I don't use
complexity metrics is because complexity
metrics are pretty bad at predicting
complexity perhaps a bold statement so
let me support that with a quote
syntactic complexity metrics cannot
capture the whole picture of software
complexity and what I like about this
quote is that it's from a research paper
and what the researchers did here was
that they took a number of different
complexity metrics mccabe was one of
them and then they apply them to real
world code bases and looked at how well
they perform and what they found out and
this was really really interesting was
that as soon as the start to control for
the number of lines of code those more
elaborate metrics that didn't add an if
further predictive value Wow the number
of lines of code that's a lousy metric
isn't it is that really the best we can
do
let me read another quote view this is
one of my personal favorites because
this one is a little bit evil the use of
metrics to manage software projects has
not even reached a state of infancy and
this is a quote by Robert gloss one of
my favorite offers if you haven't
checked out these books please do so
he's great a word gloss means is that
software projects they are so
complicated and so open-ended so
multifaceted that we will never be able
to measure it
so what gloss claims is that we should
do something different we should use our
intuition now as some of you may know I
actually have a parallel career as well
I have a parallel career within
psychology and as a psychologist you
just have to love the concept of
intuition because it's such a warm
fluffy human subject but what is it
actually well let me ask you how many of
you have been programming for more than
a decade so half the room at least great
so during that time you have all built
up a lot of expertise you know a lot
about code and all that expertise is
stored away somewhere in your head now
intuition is basically just something in
this situation perhaps something you see
on screen something is in your code
editor the triggers are recall of some
of that information so intuition is
basically just recognition
however intuition is also an unconscious
automatic process and that means you
have no conscious control over what kind
of information that gets retrieved and
that's kind of what gives intuition its
mythical qualities it also like all
other unconscious processes in our brain
it also makes intuition quite thrown to
a bunch of different cognitive biases
and even if we could somehow control for
those biases which we can't we are
hardwired to fail for them but even
we somehow could intuition probably
still wouldn't do the trick and here's
the reason intuition doesn't scale
there's no matter no matter how good we
are as developers there's no way our
expertise and intuition is going to
scale to millions of lines of code and
the software word hasn't really come up
with a good solution to this and that's
why I suggest that we look into a
different field a completely different
discipline but one that faces similar
open-ended complicated questions as we
do so welcome to forensic psychology you
probably all know a little bit but
forensics already I guess you have read
about different crime investigations in
the papers and I'm pretty sure you have
seen a number of interesting things on
TV like numbers or CSI right text or
anyone I also bet that you all most of
you have probably seen one from a
favorite movies Silence of the Lambs how
many of you have seen Silence of the
Lambs virtually half of you great how
many of you agree with me that animal
Lecter is the coolest character yeah he
really is he's amazing and the first
time I saw that movie I was quite
influenced by Hannibal Lecter and no no
no no no no no no no I really don't mean
that in a scary way I was influenced by
forensic skills not the other stuff
promise you and I was influenced by him
because if you think about it it's quite
amazing what Hannibal Lecter does so
Hannibal Lecter resuts down locked up in
his cage and he receives information
from the crime scenes and based on that
information alone Hannibal Lecter is
able to deduce not only the motives but
also the personality of the offender and
when I saw that movie the first time I
was like wow I want to learn how to do
that
years later has actually started to
study criminal psychology I get terribly
disappointed I get disappointed because
it turned out that those amazing
techniques that Hannibal Lecter had been
used
they have quite a serious limitation it
turns out they only work in Hollywood
movies but fortunately there are some
more scientific methods that we can use
so please allow me to give you a short
introduction to geographical offender
profiling which is a modern forensic
method geographical offender profiling
is based on a very fundamental fact did
you know that criminals most of the time
behaved just like us they go to the
movies they go to restaurants they go
shopping they meet with friends perhaps
they even pick up the kids from school
unless they move around in an area
that's where the spot-on opportunity for
crime because for crime to occur there
has to be an overlap in time and space
between an offender and a victim and
what that means is that the crime scenes
there geographical locations are never
random they contain information about
the criminal and that's something we can
use to catch them and I would like to
show you how this is done in the real
world and to do that we need to travel
together in time we need to travel to
19th century London to the Whitechapel
area the streets that Jack the Ripper
hunted so what you see here is a map
over 19th century London and you see the
blue dots on that map with the labels
each one of them represents one of the
crime scenes of Jack the Ripper now in a
geographic offender profile here's what
you do you take each one of those crime
scenes and consider them as a center of
gravity and then you wait them all
together but you also add your knowledge
of human behavior and psychology into
that wailing so for example crime scenes
that are closer to each other get
assigned more weight because
psychologically all distances aren't
equal now if we weighed them all
together we end up with a new center of
gravity and that's the red area you see
there in the middle that's something
called a hotspot and according to the
research there's a 70% chance our
we'll have this home baster so that's
the area to patrol a supervised if you
want to catch the Ripper now before we
move on and look at this applies to
suffer I have to admit that Jack the
Ripper it's a really cheap case study to
make because Jack the Ripper was never
cooked so who was here I really wish I
could answer that question I can't but
what I can do is to take some of the
main suspects and look at how well they
fit the profile so let's start with my
number one personal favorite suspect
James Maybrick James Maybrick was a
cotton merchant from Liverpool and when
he went to London to do business he used
to rent the room in Middlesex Street so
let's have a look at Middlesex Street
there it is right in our hot spot if you
read in news like a year and half ago
you probably heard about some supposed
tiene evidence against another Ripper
suspect the hairdresser Aaron Kosminski
kozminski used to live at Sion square so
let's have a look at Sian Square hmm
it's a little bit east of her hot spot
so kozminski is not a good as a fit for
this profile as Maybrick is but this
just raises a really important point
that i would like to ask you to keep in
the back of your head as we apply this
or software because of geographical
offender profile never ever points to a
precise precision when a geographical
offender profile does is that it creates
a probability surface on to the
geography right so this is all about
probabilities now why do I think that
this applies well to code think about
what we have done what we have done here
is that we have taken a potentially vast
geographical area and narrow it down to
much much smaller part a much smaller
area where we can now focus our human
expertise and intuition and manual
efforts and still be pretty sure that we
will catch the offender so what if we
could do the same in code
what if we could take our code bases
with hundreds of thousands of million
lines of code and never numb down to a
few hot spots and still be pretty sure
if we actually improve that code we get
a real effect in terms of growth
productivity and quality how can we use
this in code well the first thing that
we need is a geography a geography of
code and this is my favorite approach
this is team sitting in team city
software systems are visualized as towns
and each module each namespace in your
code base gets visualized as a city
block and each class each file gets
visualized as a building and then larger
that building the more complicated that
code so this is something we measure
from the code alone now what I like
about code city is that it gives us a
pretty good overview of the complexity
distribution within a code base however
it doesn't really move us forward this
is just a plain old complexity metrics
visualized in a novel way for sure but
if this was all the information ahead I
would go after those large buildings and
try to refactor them so to get more
precise information we need to add
something we need to add the missing
dimension we need to understand the
spatial movement not of criminals but of
programmers and the good news sort that
stayed that that you all already have
we're just not use to think about it
that way I'm talking about the version
control systems our version control
systems are a great behavior log offer
we as developers interacted with our
code and it what you see here is a
simple example it's a single commit by a
single developer but you see that each
time we make a commit our version
control system records a lot of valuable
information for example we see who did a
commit when did a commit take place and
the most important piece where in the
code did a change took place so what I
suggest is that if you look at that
that's a single commit done by a single
programmer a large system contain
virtually thousands of different commits
so what if we take all the data
aggregated and project it unto the
static surface of a code here's what it
looks like you see those red areas
glowing up those are our hotspots in
code and in this case a hotspot is
defined as complicated code that you
have to work with often so complicated
code that you have to work with often
what does that actually tell us about
that piece of code to answer that
question I've done a bunch of different
case studies and I'd like to present one
of them to you here this is a case that
were studied a code base over a year and
a half and the code base was developed
by close to 100 programmers and it was a
fairly large code base a half half a
million lines of code here's what the
geographical offender profile looked
like my wife always claims this looks
like an alien but it's not this is
approximately five hundred thousand
lines of code and you see that I move
back to two-dimensional representation
and the reason for that is I find that
these gifts are so much better overview
of the relative importance of the
different parts so this is an
interactive visualization and you can
zoom in and out to the level of detail
you're interested in but even here on
the highest level view you're able to
spot a number of hotspots you see those
red circles now remember a hotspot was
complicated code that were to work with
often what does that actually mean to
answer that question I mind a second set
of metrics I looked at how good are the
hotspots at predicting where the defects
will be and a reason I looked at the
effects was because the facts have this
really fascinating property that defects
tend to cluster the facts like each
other and that means that in your
typical code base relatively few modules
tend to be responsible for most of your
defects
so looked at how good are the hotspots
at predicting dose defect dense parts
and what I found out was quite
interesting it turned out that the hot
spots only made up made up seven
correctly identified seven out of the
eight most affected parts even more
interesting the hot spots only made up
4% of the code yet those 4% were
responsible for 72% of all defects or if
we turn this around this actually means
in this system that if improved just 4%
of that code we get rid of the majority
of all the effects of course given that
we don't introduce any new ones the
important thing here is that the hot
spots help you take that large system
and narrow it down to the parts that
really really matter for both
productivity and quality but once we
have found a hot spot we want to
understand a bit more about that code we
want to know is this a problematic error
that we already know about or is it
something that keeps declining in
quality and to see how we can do that we
need to understand how our programs
evolve so have a look at this what you
see here is a picture of a piece of code
now I'm well aware that you cannot read
the details and that's not just because
I'm a lousy photographer this is
actually deliberate because I want you
to focus on the overall pattern here and
the reason I want you to do that is
because I'm pretty convinced that we as
an industry we know how to write good
code we know what good code looks like
we know about the importance of naming
testability the solid principles all
that other stuff so the first version of
a program typically looks quite good
let's see what happens next whoops get a
little bit stained not sure if that's a
good thing all right never mind let's
see what happens next
Oh No what's that well let's see what
happens after that oh no oh no that can
definitely not be a good thing what just
happened what just happened to her code
that started out so nice and clean and
beautiful and then a number of small
disasters occurred to it
what are those disasters well let me
tell you the first one here that's a new
feature the second one that's the
typical sign you see of a bug-fix done
later on Friday just before a deadline
and the final one here that has to be
when a project leader decided to cut the
scope and of course he changed his mind
so dis my friends this is basically time
happening to your program this is
maintenance and that's actually a good
thing because it means someone cares
about your code because remember each
one of those small disasters represents
something that someone Fox adds value to
your program so you actually want that
you want to see that happen however the
bad things start when we continue to
build upon that shaky foundation when we
allow our programs to continue to
decline in quality like that so what we
want to do is to devise a technique that
will allow us to catch offending code
like that here's one way of doing that
this is something I call complexity
trends a complexity trends works like
this that I take each of the hotspots
that we identified in a code base and
then scan through their source code
repository I pull out each historic
version of that hotspot and measure the
complexity of that historic revision and
that allows us to plot a trend in time
like this which in turn allows us to
predict the future and the data that you
see here is real data from real system
this is from a open source project mono
the initial open source CLR port and
this shows the data for a single hotspot
over four years of time and you see that
this code has just kept accumulating
complexity it becomes worse and worse
over time now remember it's a hotspot
because we
need to work with it often what do you
think happens if we have complicated
code that we need to work with often and
it becomes more and more complex it's
not a wonder that there's a strong
correlation between hot spots and
defects all right so far we looked at
the technical side but large-scale
software projects I would claim that the
social side of software is even more
important the larger the project and I
would like to introduce this segment of
one of the best observations and know
about software development code doesn't
lie if we want to know how something
really really really works we need to
look into the code because the truth is
there and it's described in where
precise technical detail so code doesn't
lie but it doesn't tell the whole truth
either and if we limit ourselves to its
visible in a static snapshot of the code
we miss a lot of important information
one key piece of information that we
miss is something that I call tempura
coupling and tempura coupling is
interesting because tempura coupling is
quite different from the way we
typically talk about coupling in
software because tempura coupling isn't
measured from code it's measured from
the evolution of code so let me show you
a simple example here what you see here
is a simple system with just three
different parts the first time I make a
commit
I changed the fuel injector and the
Diagnostics module together the next
time I make a change I change something
else the first time I'm back to changing
the fuel injector and the Diagnostics
module together again now if this is a
trend that continues there has to be
some kind of relationship between a fuel
reactor and the agnostics module because
they're changing together over time like
this what kind of relationship could
that possibly be well let's look at some
of the most common findings
but you will find quite often when you
investigate tempura coupling is this
that you have a piece of code over here
that uses a piece of code over here and
each time is changed at the API you need
to change the client as well so this is
basically or plain old physical coupling
and this is indeed something you can see
in the code alone but remember tempura
coupling is sent measured from code
tempura coupling is measured from the
evolution of your code and that means
you will sometimes come across cases
like this where you have a piece of code
here you have another piece of code here
and there's absolutely no dependency
between them yet they keep changing
together over time how can that possibly
be
how can two independent pieces of code
keep changing together over time is that
some spooky quantum physics thing most
of the time not but I will find in most
cases it's a dear old friend copy/paste
so you have some code over here you can
put pasted here here and here and now
each time you modify original you have
to remember to modify the compass as
well so tempura coupling is a great way
to detect software clones but we can do
even more with tempura coupling and I
would like to show you how by showing
you how you can analyze a complete
architecture so consider this simple
system here we have three different
subsystems here and we can measure
temperature pling on a subsystem level
as well we don't have to limit ourselves
to in video files and if you measure
temperature pling here we see that these
three systems tend to be changed at the
same time now do you think it makes a
difference if those three systems are
winded hands of a single programmer or
if they are developed by multiple
programmers organized into different
teams I was actually in that situation a
while ago so let me share a little story
with you this is something that happened
five six years ago at that time I was
working as a consultant and I joined a
new assignment and my first day there I
got to signed a number of tasks
so I picked one of the tasks obviously
the one that looked the most fun and
start to work on that one pretty soon I
noticed that in order to complete this
task I need to make a tweak to an API
that API was owned by a different team
so I walked over to their team leader
and I asked her do you think you can
make this change to this API and she
told me yeah sure it's a simple change
so I could starting with it right away
excellent so I went back to my desk and
started to work on another task and that
was a good thing because it took me one
week one week to get that simple change
I didn't think much about it at first
against something else obviously came up
but it turned out that we had to modify
that API a lot and each time we had to
make a change to it that change took at
least one week so one day I just had to
find out what's actually going on here
so I walked over and I actually asked
the team leader I'm sorry what's going
on here it is something unexpected
turning up all the time because it takes
such a long time to get the simple
change and what she told me completely
changed how have you softer architecture
because it turned out yes it was a
really simple change for them to make
but in order to do it they had to go to
yet another team and change their API
and that team in turn had to go to yet
another team and that team had to go to
the database administrators and as we
all know that's were all change for
Cresco to die so the takeaway here is
that if you have a system with
dependencies between the different parts
and those parts are developed by
different teams well what you actually
have is a dependency between people and
of course this goes all the way back to
one of those famous and well-known
observations on software Conway's law so
I guess you already heard a lot about
Conway's law at this conference I'm
going to keep this really really brief
who's familiar with Conway's law it's
cool so
Congress law is basically the
observation that the way we're organized
as an organization will be reflected in
a kind of software design and the best
thing with Conway's law is that there
are just as many interpretations of it
as there are blog posts about it
so we can basically pick our favorite
and my favorite is Conway's law in
reverse because used in Reverse Conway's
law becomes a useful organizational tool
so that basically means we start with
the camp system we want to build and
then we look at that system and see how
should we organize in order to make that
happen as efficiently as possible but
yeah new systems most of the time we
have existing code that we need to
maintain with existing systems that we
need to improve so how can we use
Conway's law on legacy code what I
recommend here is that we start by
understanding how well does the current
system support the way we work with it
and one technique that you can use is
something I call social networks in code
again social networks are measured from
the evolution of the code from a source
code repositories so what I do here is
that the scan the source code
repositories and each time I find a file
where two programmers have made
contributions those two developers they
get a communication link between them
now I continue to scan for history and I
find that another programmer has made
contributions to the same file well then
she gets a communication link as well
and over time this allows us to build up
a complete graph over their ideal
communication paths in our organization
and as I ideal paths because there's
obviously this is measured from how
their code was actually created there's
nothing here to suggest that some of
those lines represent communication that
actually took place so what you need to
do now is to take your graph your
communication network and compare it to
your formal organization of short and
any discrepancies have to be understood
so let's have a look at some typical
examples on what you can expect to find
when you do that
according to Congress law we should
expect to find something like this what
you see here is that we have three
different teams and you see that most of
the communication paths are between
members on the same team and again that
means that on each team the people work
with approximately the same parts of the
code and this is a good thing because it
means they share the same context
sharing that same context is something
that's just making communication so much
easier and cheaper so this is where you
want to be and you also see that the you
have the occasion online here between
different teams and that's probably also
a sign that cross-pollination of
knowledge is a good sign in programming
too so that perhaps just represents a
programmer that rotates teams now I have
a confession to make all data that you
see in this session is real it's from
real systems all the associations are
from real code bases except this one and
the reason for that is I'm a Spartan my
day job I've analyzed hundreds of
different code bases and I'm yet to find
an organization that are death aligned
well aligned with their code so I had to
make this up the next one I will show
you however is real so now we have seen
where we want to be shall we look at the
opposite side of the spectrum how many
of you want to see a complete disaster
all right let's have a go at that so
this is a story about the company and
this is a company they were in a pretty
good position they were going to do on
your product and they were in a good
position because they're done something
very similar in the past so the actual
had data the told them that this project
using your five in-house developers will
take your proximately one here yeah I
know what you think a software project
that's predictable ridiculous and indeed
of course someone came up with idea that
yeah one year but you know we have this
really cool trade show in just three
months can't we do it in three months
now how do you take something you know
takes one year and compresses it down to
just three months
it's so easy you just throw four times
as many developers on it and this was a
fast-paced project the initial
architecture was already said and
insured at time than it would take you
to read the mythical man-month
they recruited 25 consultants and she
used to organize them into four
different teams how do you think their
communication diagram looked what I want
to show you now is the real data that
I've been allowed to use from the real
project I've just made it unanimously so
here we go looks pretty cool doesn't it
but let me assure you that's nothing
cool about this this is a complete
disaster this is anarchy this is a
communication breakdown and the reason
it's so bad is because yeah if you look
at those four different teams you see
that the members on the same team they
do have a lot of lines between them
which means they work in the same parts
of the code the problem is that everyone
else get that as well so this is not for
different teams this is virtually one
giant team of 29 developers with
artificial organizational boundaries
between them now I didn't work on a
project myself initially I came in as
part of them post-mortem analysis and
then I get to talk a lot to the
developers and they told me a lot of
interesting things one thing that they
told me was that they had a lot of
defects the second thing that everyone
told me was that the code was hard to
understand and this was a bit surprising
because I had looked at the code
previously and I didn't find it that bad
and but they turned out the reason was
of course that even if you wrote the
piece of code yourself today three days
later it looked completely different
because 500 developers had been working
in the mean time the final thing that
they told me was that they spent a lot
of time merging different feature
branches and if you have a look at that
diagram you see that no they did not
merge problem their problem was that
their architecture just couldn't support
their way of working with it so please
align your architecture on your
organization your code is going to thank
you for it now remember that project I
just told you about that they complained
a lot about their defects and indeed
they had a huge backlog of the facts and
this shouldn't really come as a surprise
given what we actually know about
software bugs it turns out that the
number of programmers behind a piece of
code is one of the best predictors of
the number of quality issues that code
will have so if we know about that
correlation I start to think that yeah
again this is something we can actually
measure this is data that we have in a
version control history so let's see if
we can find a way to identify code at
risk for defects here's one approach
this is a visualization called fractal
figures a fractal figures work like this
you consider each file in your system as
a box and each programmer gets assigned
a color and the more that programmer has
contributed to the code the larger their
area of the box now you see an example
here from a real codebase and if I was
after access parallel development I
would go after the second one there
because that file has contributions from
lots of different programmers and we'll
find code like that you need to inspect
it and understand why and I promise you
what you will find most of the time is
that code changes for a reason the
reason that file attracts that many
developers is probably because it has
many reasons to do so it has too many
responsibilities so further figures are
a good starting point here but we can do
even more if you look at the fractal
figures you will see that the actual
give us a pretty good way to identify
the main knowledge owner behind each
piece of code the programmer has made
most of the contributions are also
probably the ones who know the most
about that piece of code so if we have
that information on each
file again why not aggregate it and
project it unto the static surface of a
code that would allow us to build up a
complete knowledge map of a codebase
here's what it looks like now this is
again real data from a real codebase
this is the development of the
programming language Scala a partner
with a compiler here and it's the same
principle each file is visualized as a
circle in this case and each developer
the knowledge owner is visualized by
color on a corresponding circle now a
knowledge map is something that you can
use to simplify communication so let's
say that we want to join this project
and we want to make a contribution to
the back end in the upper corner now
using a knowledge map we see that the
yellow developer has made master two
contributions so she probably knows a
lot about that code and if she doesn't
know we see that that the pink orange
developer has also contributed a lot to
the back end so let's ask him instead
but instead of focus on individuals we
could get even more interesting data if
we aggregate that information into teams
because that would actually give us a
way to evaluate Conway's law here's what
it looks like now each color represents
a team and you see that firm perspective
our Conway this actually looks quite
good you see that the red team they
pretty much have your own subsystem
where they could focus the reference and
the pink team they also had their own
subsystem where they can work in
isolation but down in a corner we have a
large subsystem with contributions from
all three teams and again when you find
something like that you need to
understand why and the reason might be a
simple that this organization just lacks
a fourth team to take on a shared
responsibility or again code changes for
a reason too many responsibilities and
perhaps that piece of code is been a
rough split into three different parts
and of course once you have that data
you can overlay it with results of a
temporal coupling analysis remember the
project that only about the while ago
the project were a simple change took
one week because it
Fulford organization that's how we found
out now the final part I want to show
you by tying back to the communication
diagrams the social networks when we
talk about social networks we need to
mention the Pareto principle as well the
Pareto principle is better known as the
80/20 rule that and it's something that
holds true across all social networks
Facebook Twitter and I also found that
it holds across code bases that 80% of
the content is created but 20% of the
members and you do have that in code as
well you have some developers that's
been around for a long time in every
system they know the code inside out
perhaps they are you so what would
happen if you would leave if you will
leave your Cameron codebase well you
would obviously get the knowledge gap
how big most of the time we have no idea
but you seem to say that we can actually
found out here's what it looks like have
a look at those red areas in this case
they don't represent any hotspots no
they represent abandoned code that is
code that's written by developer who is
no longer part of the organization and
this is something you could use to
reason about the knowledge distribution
in your codebase and when you do that
you will most probably find patterns
like this we have an entire abandoned
subsystem and again this is the
information you can use a bit more
proactively as part of your risk
management
so if you know that you plan to make
some changes to a feature that's
abandoned like this use that information
to schedule additional time for learning
because it is a hugely increased risk to
modify code that we no longer understand
so I'm almost done now I just want to
sum it up for you today we have seen how
techniques from forensic psychology can
help us identify hotspots in code as
well and hotspots are code that helps us
predict code that's hard to maintain and
code at risk for defects and we have
also seen that once we embrace the past
of a code base its evolution
we get a lot of social information as
well that help us measure things that we
haven't been able to measure before
things like knowledge due to the
distribution and knowledge loss and I
think it's important that these
techniques they don't stand on their own
they need a vital ingredient they need
you because these techniques are there
to help and guide your expertise to it's
likely to be needed the most and this is
a huge topic and if you want to read
more about it I do write a lot about it
on my blood of course also have that
complete book if you want to dive into
the details and the thing I'm working on
right now is to provide these tools as a
service so it's something I hope that I
will be able to release next month if
you're interested sign up for a preview
right now it's free now before I take
questions I like to take this
opportunity and say thanks a lot for
listening to me and may the code be with
you thanks
oh cool
Thanks so I get a bunch of questions
here first question is fascinating but
do you think you're confusing causation
with correlation no I don't think so but
it's always risk and I'm not sure with
that question refers to my study there
on the bugs could be so the thing is
that advice virtually every one of these
analyzes are based on empirical data the
cursor turns out we actually have a
subfield in academia that study software
evolution so you have a lot of academic
studies of really high quality that goes
into most of those findings so if you're
interested in that they have a lot of
references in my book so if you're
interested drop me an email you have my
email address there and I'll be happy to
provide you with some references on the
supporting research material so here's
the next one is really interesting how
do you account for engineers changing
teams over time when analyzing teams yes
it's a tricky question and the way I do
it is that I don't I you can when you do
this analyze this one of the hardest
things is to understand which time
period that you should actually study so
what I tend to do is that I always for
the technical analysis always tend to
run on a complete lifetime of the
codebase because it allows me to spot
long term trends I also tend to run much
much shorter analysis like The Lost
Sprint or the time since the last
release or something like that and this
gives you again a different view and a
different perspective that really will
help you particularly since all the
stories are features we've worked on are
fresh in your mind so you can see how
it's actually impacted your codebase and
when I study the social metrics I make
sure that I don't go further back then
until the last major organizational
change so that's basically how I do it
I think it would be too tricky and too
much risk for bias otherwise actually
yeah and that's the next one how do you
account for people switching teams again
or shorter analysis time span but
sometimes if it's just individuals that
rotating I just leave them in there and
you will see that that's the example I
showed you where have this line across
different teams that's not the seller
that's most of the time that's a good
sign but even if you have heavy
dependencies between your different
teams it's not necessarily bad as long
as you have a plausible explanation for
it I think that's the most important
thing with all of these analyzers is
that what you want to look for are the
things that surprised you as soon as you
find something where don't have a good
explanation to why it looks the way it
does that's usually a warning sign in
your experience how effective have you
found TDD and unit testing to be in
preventing the effects and enforcing
code quality and structure do you
recommend TDD wow I love that question
so yeah a lot of teams I mean part of my
job is I go to a company I make an
analysis of their codebase and I leave
our report with findings and
recommendations and a lot of teams today
they use TDD or at least work a lot with
automating tests and checks and what I
found out is quite interesting I don't
have any data if TDD actually helps you
or not there are other studies that look
into that and then I would say that
their findings are I think there are
still inconclusive there are positive
side effects to TDD definitely but what
I tend to find is that most of the hot
spots tend to be in the test code the
worst offenders are always in the test
code and I think the main reason and I'm
speculating now but I think the main
reason is that we developers we make a
difference we make a mental divide on
one hand we had application code and we
know it's vital and important that we
keep it clean and nice and maintainable
and all that stuff on
underhand we have the test code and most
of the time we were happy forget around
to write any of it at all so and I think
that's a dangerous fallacy because from
a maintenance perspective it there's
really no difference between your test
code and your application code if your
test code lacks in quality it will hold
your back and again that amount of hot
spots in test code is something I've
seen independent of T D or D or not T DT
so those were the questions I got here
and I will go over to the speaker clinic
in case you want to continue their
discussion and I'll be happy to answer
any questions you may have so again
thanks a lot and enjoy the rest of the
conference
Thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>