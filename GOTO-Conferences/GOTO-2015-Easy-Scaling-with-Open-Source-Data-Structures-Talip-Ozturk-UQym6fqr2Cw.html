<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • Easy Scaling with Open Source Data Structures • Talip Ozturk | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • Easy Scaling with Open Source Data Structures • Talip Ozturk - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • Easy Scaling with Open Source Data Structures • Talip Ozturk</b></h2><h5 class="post__date">2015-07-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UQym6fqr2Cw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone my name is to leap i'm
the founder of
hazel cast so today we're going to talk
about clusters we're going to talk about
scalability capture and sweet caching so
all the all the fun stuff and i'll do a
demo I'll take the risk okay okay so
what is hazel cast hazel cast so it will
be we all know hashmap its key value
it's the hashmap is not thread-safe if
you want to share the map across
multiple threads we use concurrent hash
map or any thread safe hashmap right so
what if we want to share the map across
multiple JVMs not only threads but
multiple JVMs that's hazel cos so you
create an hazel cast instance with a
configuration and then you say hazel
cast give me a map so name of the map is
my map customers map so whatever so so
you if the names are the same you're
actually working on the same exact map
it doesn't matter which a vm you're
accessing from so so i'll do a demo it's
it's more than a map it's distributed
queues and locks and all that stuff so
I'll their demo to make it very clear
all right so so when you unzip hazel
cast so you're going to you're going to
see something like this so Lib directory
contains all the hazel cast libraries
hazel cast all contains all the other
jars so take that jar put into your
class pad you're done there's no
dependency so just tech single single
jar okay so and we have management
center for managing the cluster it's a
web UI and documentation and all that
good stuff also I built a console
application to demo hazel cast
so let's look at what what we have here
I'll use ok sublime and okay so let's
see what what we got here it's nothing
other than it says include this jar and
run this console application basically
so make it bigger okay single jar put
into your class bat then you start using
the API that's it okay so close days and
I'm going to run the console application
all right as sage console that Sh so
this will start hazel and start my
application which is a console
application is a command line
application internally will call hazel
cast and his guests will start so it
says there's only one member in the
class cluster and it's me this is me so
it tells you the address and the port
number you started on so I'll do MDOT
put which is like map put right the key
is one and Chicago now if I do end up
get like a map again i get the value of
course now I so I'm going to fire up the
amendment center to see how things are
laid out so it will make everything more
clear SH so seceding to mention turn and
start so it's a rabbit web server
starting so let me make this a little
bigger admin admin login so so that's my
cluster i connect so let's look at the
layout so then i'll make this a little
bigger yep so it says there's only one
member for this default map that we're
looking at right now these are the maps
that that we're working on so the map
contains only one member and there's
only one entry in the
in that member so there's no backup
because there's no other member so let's
let's watch how things change so i'll
start another another note just like
running the same application or
different application on another machine
in the network so if i do console that
ice age so it will use multicast by
default and find out a guy and join the
cluster and form to nut cluster right
now so so now we have so as you can see
here i see the second note as you can
see the man tree actually moved to the
other one so it was on that node but it
moved after the second guy came in so it
rebalances the cluster and moves data
around the cluster okay so and looks
like also this guy is the owner of that
entry and the second node is the backup
okay so so we extended the map so that
we can do more with the data so we can
lock the entry I'm going to lock them to
here and I'm to come I'm just going to
try to update it from here from the
another JVM put and one value Palo Alto
this is where we were from okay it's not
able to because it is locked by the
other JVM of course as soon as I am lock
from the first one it will go through
and an update do the update if I do a
get one I get the new entry so it's a
map concurrent hash map that we know
from Java but we added things like a box
and listeners so I can also listen for
who is updating what so so I'll um I'll
put more data put many command will do
it put thousand entries so and we'll see
that data did
so these two members are sharing the
data so it's almost half and half and
backups are also also there so i'll
start the sex or note right now console
Sh so well let's look let's see how
things change here so for this map so
how data is going to be moving around so
you will see the third node joining I
see that one right here and then still
thousand and one entries so these this
is how they are charted and backups
eventually catch on and become also the
thousand-and-one so so we don't rush to
get all the backups because that would
impact the network crazy if you have 100
million entries so eventually it will be
the same so again it's one single jar
print to your classpath used concurrent
hashmap that you already know and all
JVMs in the network will be sharing that
map so is there anything you want me to
do here like any anything you want me to
try so so locks are only for the rights
while while the entry is locked you can
still read yes yes
you should we try okay all right that's
a so let's lock from where from here
this is the third and dot lock one this
guy is the owner the owner of the lock
not the entry so we'll see the lock
right here I see that it is already
locked so so the lock is on the second
one right now second so we did the log
operation from the third entry is owned
by the second that's why there's a lock
on the second okay so which one do you
want me to kill the second one and so if
the second one goes goes down the lock
owner is still three right what do you
expect to happen should should the lock
stay lock should stay and and and so so
probably the entry will be owned by
somebody else and you still we still see
this if we should still see the lock all
right ok let's boomlet pray and then and
yes so we're gonna we're going to killed
it we're going to kill the second one
right this is the second one right
control see here going down alright so
the first we saw that the guy is dead
now we saw that never law we didn't lose
the data and we saw that luck is there
okay so if I if I ok so still third one
is owning the log and and this guy is
still active so this guy is dead i'm
gonna just removed from this scene so
i'll do so if i try to do end up put on
this entry I should not be able to do it
because the third one is still owns the
log ok so I'm original from Istanbul so
i'll do i'll try to override yes i'm not
able to because the other guy has the
log now let's kill that guy and see if
they dig yeah lock is released alright
okay alright this one is the third one
yes alright so let's see first let's see
what happens the second guy should
should die and there's no lock which is
good and then let's see if the if it
went through let's see if i get istanbul
right i should get istanbul yes so that
means yeah yeah you can still get there
but I'm just saying I'm just double
checking if the put went through right
so because it was waiting on the dying
member right right
okay so are you so you want us you wanna
you want to start a transaction first
and then do puts and then before they
commit you won who won who is going to
die so just tell me like that so it
didn't okay they do note that has the
data that's locked right so so so the
Kohler transaction guy is still alive
right so the data is dying is in your
case right they did the the lock owner
is dying no the entry owner is dying
right right so so in this case
transaction it will stay alive because
the owner of the entry died and the
ownership will be changing will be
changing to somebody else and the from
the from the transaction perspective
nothing happened right because the locks
are going to be also transferred to the
new guy BB saw that so we killed owner
lock was carried over to the other guy
along with the entry and so from them
from the transaction client perspective
nothing happened but if if the
transaction note dies like to denote
that started the transaction dies
obviously all the locks associated with
the transaction will be released and
your transaction didn't complete
basically right but the guy already has
the guy who owns runs the transaction
code died like the you cannot even roll
back like just you just died that means
all we can do is release the locks
associated with the transaction right
actually release all the logs that
belongs to that guy not only the
transaction but all the other ones right
yes
oh so yeah so we do support also the
cues I'll do I'll do play with that a
little bit i'll start this note again so
cues will work the same basically
regularly you're going to say hazel cast
get q name of the queue will be the same
for all jvms and then that that would be
topic yeah so so there are two two
things right like queues and topics
queues are going to like you can only
consume the item once from the queue
just like Java block eq exact same
behavior and there's a safe that this is
not what you want there's also okay so
yeah so so I'll start the third one to
it will be fun if when we have more JVMs
so cute take this guy's trying to take
perhaps a sage consulates age you can so
this guy is already trying to take any
son take something so I'm going to try
to take from here too so this guy is
trying to take two so these two are
trying to take something and this third
JVM will offer something cool for task
so so let's let's do this okay only one
of them will take and the other one will
still be waiting right so take happens
to be that this guy who is the winner so
he took the task so that that's why I
see that so this guy's still block then
still waiting so until somebody else
offers something task two and then this
this guy will so just like pigs I mean
blocking q that you know but across
multiple JVMs
the same transactional behaviors will
apply so the cues are also backed up so
it's so and on the menu center you're
going to see Q's here so this is the key
that we worked on right now there is
nothing we offered something and we
consumed we offer something and we
consume that's why we saw that so those
okay um yes all right okay okay so so
let's double check if the if kyle is
here first she's not here then we can
okay looks like he's not here so I can
say whatever I oh yeah it's recorded so
maybe I should be a little more careful
so multicast is only for discovery after
MultiAir discovery millennium it doesn't
have to be multicast by the way we
support tcp/ip and also for AWS if
you're running the app app on AWS you
don't have to tell us anything we will
figure out the nodes in the cluster
after discovery everything is TCP IP is
unicast basically so so let's say you
have ten node cluster and network
partitioning happen so something went
wrong in the network so so there there
there are simple cases and super super
crazy cases the simple case is this
network is partitioned right in the
middle right so length 5 or like 55 or
28 right the clean separation so the
clean separation is is easier to handle
but still when once you get into network
partitioning is you can probably keep
running somehow but you should know that
things are in memory so that means there
will be some funkiness at the end
eventually you could
you could you can have you can have the
same entry updated on both and so first
let me tell you what will happen from
hazel gas perspective so two separate
clusters will happen right like let's
say four to six will happen will 55 will
happen right and then so they will think
that they're there they're the cluster
and other guys are dead right and
eventually the guy will fix the network
and then hazel cast will detect that oh
I was actually part of a bigger cluster
I should join right so usually smaller
will join to the bigger one if they are
the exact same size like 55 the smaller
hash of the IP will join the bigger hash
so because when they are looking for
each other they're also sent the hashes
so so when so do we call this merge of
the split right so when the merge
happens so there you have to do manage
to come complex right so so because same
entries will end up on both and and
probably they will get updated in the
meantime right so when you merge so we
have to fix that so there are built-in
policies for that so like override
whatever you got or put only there is no
entry for that so much policies already
exist but if none of them fits for you
so we're going to act you can implement
a merge policy we're going to give you
the entries you're going to look at the
dates site time stamp or or the value of
the entry and you're going to make a
selection for us so that's why I said
eventually will be mass there's there is
no perfect solution to that other than
you can say you know what when the split
happens or when you when the number of
nodes goes below seven stop like freeze
the whole thing we should not so you
because you're trying to be more
consistent than available so the current
behavior is a behavior of a look at is
to become available not so when from the
cap perspective when when p happens we
choose to be a but when there's no p b
which we choose to be strongly
consistent when there's no network
network partitioning so which is beyond
the cap this call so why did I say that
because so all your reads and writes are
going to be going only to the owner of
the entry so that means you cannot you
cannot read stale data unless you
configure it so so so default
replication and all that stuff are
asynchronous and all reads have goes to
the owner owner even if you are the the
backup you still cannot read from
yourself unless you configured like read
here read back up
so I should have moved on a little
faster but um yeah let me go back to
slides because there are some of the
answers already there so all right I'll
move on little faster else definitely
answer that question okay so so it's
open source five megabytes single jar no
dependency maven friendly just use maven
is simple so so what is Sahara how's it
how do we partition data so we basically
there's a there's a there's a fixed
number of partitions in the cluster
partition is like a bucket or segment so
it sits by default 207 1 year we can you
can change it two thousand five thousand
that doesn't matter really but once it
said you cannot change well while the
cluster is up okay so it's a static fix
a fixed number and then your keys are
going to fold into one of these
partitions so the way we calculate this
is like hash the key and then mod with
the Parchin count will for for a given
key it will always give us the same
partition ID so we know for a given key
which partition the key is n entries in
and then we're going to go to that go to
the owner of the of the key and all this
membrane the cluster will define who
owns what whoo-hoo which remember owns
which partition so this call this
internal partition table so there's as
you saw there is no like a master node
or like a centralized location of all
that stuff 0 everywhere everybody is
peer and and everyone is almost equal 0
as I said almost equal because the
oldest member the first guy in the
cluster has a little more power because
the oldest member knows more about the
cluster in a year and in
and it will do the assignment of the
partitions so we'll say our Chi give you
these partitions to you and this
question dad and all that stuff but if
the oldest member will do that
eventually all this member will die
that's nature and then another guy will
beat all this member so yes so it's not
like the but when it comes to
replication it's not like my entire data
Ripley will be replicated on to one
other guy it's not like that really if
you look at the picture so these are the
owned the entries but if you can see the
blues or peripherals are on both 22
notes right so the idea here is that
when the node dies we are already
partitioned we really like balanced we
don't have to rebalance things so so
that's why it's not like this guy's
backed up by that guy it's not like that
it's more like this partition is so so
so part the backup for this partition is
this guy so since since you will have
more partitions per member so when the
node dies you're already balanced
there's no need to rebalance things yeah
we still have to take care of the
backups of course because the guy died
and there's missing backup okay so so
hazel cats can run as an embedded server
so it's a jar right you print your web
application war file and each wore a web
server can become a member of the
cluster so that's embedded hazel cast
you can also run as a client server you
can say oh no no I don't want my web
application to contain hazel cast note
inside you can say I'll run hazel cast
notes as a separate cluster and my web
application or dotnet application or C
C++ application will be a Klein to it so
this is a lie client-server architecture
basically so both have its goods on bad
so so it's not so so you're back to your
question a decline comes with two flavor
one is like a
plain click line that oh noes that talks
only one guy in a cluster and everything
will be propagated over from from that
guy so it's like the other one is smart
client smart Klein knows exactly where
the data is smart client will go to the
owner of the data right from here
correct so so you're going to use smart
is the default one but if you have like
10,000 Klein you probably need I mean
you probably will choose the plain one
because it has only one connection right
but if you have like hundreds go with
the smart yeah correct so the client
will say they will do the exact same
thing look at the key find a partition
ID and then look at the partition table
and see our wounds were that partition
and then just go to that guy directly
yeah so it will happen that it even
worse things will happen like to imagine
so so dad knows our pine so it will I
will say okay I lost the connection so
who's the new owner Oh looks like the
first backup was that guy I'll go to
that guy right or the patching table
will be re-published by then and then
you go to the right guy but even worse
as I said the as you add more nodes data
will be moving so we call this migration
of the partitions right so the question
is this so you do a call in a Wendy when
the person is migrating so what will
happen right so same say a same thing so
the client thinks or it doesn't matter
who calls who makes a call the request
comes in and the you guys I owe you I I
think that you I know that you think
that I am the owner but I'm not so I'm
not going to do the operation so you so
operation is rejected basically and and
then two things will happen either
several will get updated that you are
the owner or the client will be updated
by the actual new owner right so either
either one of them will happen and
eventually you'll go to the right owner
so you will never you can you it's not
allowed to read from the alike even if
you have it on you right now but you can
you can say i'm migrating so that means
during the migration the operations are
blocked but only for that partition the
rest of the parts you like if you a
thousand one out of thousand will be
migrating so the rest will be just
flowing just fine so and migration will
will be slower to to to minimize the
impact right okay so eventually api api
will be the same but the way you start
the instance will be different so you're
starting the first server second server
if you like to start to note in the same
jvm you came to to note on the same
server so for example in our you j unit
tests we do start five nodes and then do
i do our assertions on those this is how
you start declined eventually the
interface will be the same API will be
designing for client and server you can
start as a server node and then
eventually say you know what I don't I
don't like this embedded layout topology
I want to go back to client server and
then you just change one line to get the
instance rest will be design basically
so yes
Oh
okay we're not really counting the
entries we're counting the number of
partitions we're saying oh how many
persons do I own and your own and that's
how so it is if you have enough data
meaning like 100,000 it is very unlikely
that your one will have bigger than the
other or more than the other so it will
be a very well balanced so from the data
distribution perspective I think it will
be eventually equal but it is still we
still don't don't mean that like let's
say you're you're writing in like a
stock exchange stuff and an apple is it
is having a good day and all your
requests for the photo for that tick is
going to this exact same jvm all the
time so when JVM is getting hot hotter
that can happen but I mean but other
than that if you look at the amount of
data it's almost going to be the same so
other than like adding new new nodes
removing a node will be vocal calls
rebalance and also eviction will cause
rebalance so if you have tons more you
you're using map as a cache eviction
will kick in clean up some of the data
rebalance will happen too so everything
has to be somehow serializable they
entry doesn't have to be serializable
what but there has to be a way to make
it sterilize right so because eventually
we're going to write it through the wire
installation will be a biggest call so
pay super high attention to salvation so
your network is probably very good and
high throughput is not a problem usually
a latency you'll get from from
civilization so you can you can learn
about the cluster with the API listen
for the membership changes and all that
good stuff
you know distributed map and put a cap
soon kind of stuff all supported I'll be
quick so multi map is easy stuff q you
know that stuff already that's these are
not really so fancy so yeah we looked at
the map that lock stuff right so you
know that stuff so topic is that so very
similar to Q but everybody every node
listening will receive the message so
it's just like JMS topic so that's the
code no nothing else okay I'll so events
you can listen for like who's updating
what on the map and Q's that's fine so
you say simple API transaction you can
you can be support to phase in single
phase commits you say these are my
transaction options I'll you'll get a
transact the transactional queue or map
and then you do your operation and you
commit commit here or rollback affinity
data affinity I'll be fast and I'll
finish and then if you have time we'll
take more questions so as long as the
keys are the same data will stay on the
same node we don't care about the map
names we care about the keys if you if
you want to have the orders and the maps
or origin the customers on the same node
you have to come up with a the key
that's partition aware that can tell us
where this guy should live on right so
we know that it is it's a the partition
key is is pointing to the customer ID
that means don't use the don't use other
stuff use this as the key to find the
owner all right so xq service regular
Java executive service but they're
they're going to run on on the nodes so
you can say hey hazel cast run this task
on this member or run this test on that
particular key meaning go go to the
owner of the key why would you want to
do that because you're
they're right so that that's how you do
it or you can say run on each member and
give me a collection as the collection
of results so we will work on a log on a
scenario we're going to update and
remove an order from a customer so we
have a customer big customer object
there are orders inside it we're going
to remove one of the orders so that's
the business logic so you get the
customers map you lock the entry so that
no one can touch it while you do your
you're doing it and then you're going to
you're going to remove it and then now
the object changed you have to put back
into the cluster and you unlock it the
lock and unlock is not really required
let's say this is required for our
business case like the requirement is
that while you're updating things you
don't want anybody else to touch that's
the requirement let's say normally you
don't have to everything is atomic
already but you're changing stuff let's
say the business requires this so this
looks good and simple but this is this
is problematic can you tell me what the
problem is here what is it it's it's
yeah like it's it's actually more more
yes pessimistic lock is one right so you
have to lock get the locks somehow but
but also see there are four distributor
operations one two three four if you
don't know where the key is right so you
might be doing for distributed calls
just to get this done can you do it
better tell me your way can we do it
better without 40 calls can we do it in
one shot but you're going to do an
update so eventually yeah so you can do
a deletion task order deletion task it's
partition aware so we know where to go
so the vertigo is actual customer ID so
his guys will take this call object and
send it to the owner of the customer ID
and run this call over there
so now the data is there so but still
not perfect because we still cannot get
rid of the locks because it's another
thread I still have to get the lies
somehow get the locks to be safe so can
we do it better yes we can that's that's
called entra processor so enter
processor is also part of the j cash
spec which is huge for java caching
standard so so what you're doing is is
this so we created delete order which is
an android processor and you in the
process method you're saying we're going
to give you the customer object and then
you do all your stop there and then
return it's the code is simpler easier
and this gives you an implicit lock on
the entry so you don't have to have an
explicit lock on gentry so entry
processor our processes are guaranteed
to run on the node that owns the entry
and it gives you the implicit lock you
can be sure that no one else is touching
your entry while you're touching you can
it's guaranteed so that's that's the
best way of updating an entry so
executive service is nice but not quite
and capacitor is the way to go so so
that means it's we care about charting
the data so that we can scale but also
we care about how we update the entry
and entry processor is the best way to
do that of course as you can imagine
entry processor is her entry operation
right so and this is how you call it you
say hey Hisoka has give me the customers
map here's my delete order process enter
processor executes on key this is the
key this is the entry pass sir just go
and run that's it so the crew you can go
to query hazel customers let's say you
have this fields you want to get the
active and age less than
whatever 30 30 this is how you do it so
you can add indexes just like you added
in databases so it so your course can be
faster but then you can say map values
and pass like an SQL where clause his
occurs will go in parallel go to your
notes use the index if there's any and
get they get to get the things that you
really want and then combine the results
back and give it to you persistence it's
his guess is it all in memory but in the
case of distribute caching see these are
your web applications and these these
are your databases you might you might
have multiple databases right so
relational Oracle MySQL or no SQL and
his cats can sit right in between so
that way your touch you're talking to
hazel cast and his guests will be
talking to these guys beyond it seems so
basically we're cashing all your data
layers and of course you're going to say
I know how to talk to Hazel cats but how
is the house how is hazel cast talking
to my databases this is this is how it
is so if you don't know how you access
your database but we can if you write
the code if you implement map store map
loader weekend will call you to talk to
the database so when the time is right
let's say you there map that put as part
of the put will call your store
implementation will give you the key on
value and we'll ask you to write it to
wherever you want to write to it could
be you it is possible that you're
writing to a relational database or no
SQL database we don't care of course we
have plugins for NORTHCOM MongoDB on
others but still you have the freedom to
implement the the business logic here
and go to any store you like so yes
that's also that's also what we are so
so we've when when when you first time
could touch the map we're going to call
this load all keys so you might be
loading all of the keys or some of the
keys that you care the most right looks
like the hottest keys so the end in
parallel let's say you load it like 10
million of the 100 million ok so in this
10 million keys are going to be loaded
in parallel by the nodes in the cluster
so let's say you have 10 notes each one
the load a million of it so in parallel
you'll be loading and warming up the
cash and then you're going to when you
do a put we're going to call this when
when we don't have the entry we're going
to call this to load it back from the
from the data store actual data store so
persistence can be right behind or right
through right behind means right through
means it's part of the put put returns
after the store right behind means it
will return before touching that code
but every 10 X or 10 or X seconds you
can figure that we're going to we're
going to run a batch operation to flush
all the dirt wreckage to the database
this scale is crazy well even if the
database is not reachable it's fine
we're going to we're going to mark them
as dirty until the connection is
established
okay I'll let me finish and then it's
actually almost it we're almost there
you didn't talk about configuration much
because hopefully you will never need
that I hate configuration so Heys guys
will come with the the most meaningful
and defaults but in case you need to so
this is the network this is how we
define who the members are in the
cluster so that we can find other guys
and this is the interview this is the
this is how we define which port to bind
which IP defined because your machine
probably will have more than one IP and
which IP do you want hazel has to use
this is where you define that so that
way you like it's it's very possible in
a cluster environment one note starts at
192 another note starts at ten and they
don't talk to each other and then you
try to like figure out like why why it's
wrong basically say hey when you start
just look forward to 192 address and
bind to that because we know that deck
there they're talking to each other fine
so / map you define number of backup
backups or asynchronous backup you can
have one synchronous one asynchronous
back up to ton TTL for the entries and
all that stuff you define a map store
that we talked about Fermat we don't if
you don't have to define you can say
everything starting with this will use
this configuration so we try to use wild
card as much as we can to simplify that
the configuration yes it's a single java
jar file that can run as an embedded
server or a remote server and and allows
you to do distributed caching or
queueing messaging so it gives you all
these data structures and services
yes thanks so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>