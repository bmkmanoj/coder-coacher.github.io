<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2013 • Solving Real &amp; Big (Data) Problems Using Hadoop • Eva Andreasson | Coder Coacher - Coaching Coders</title><meta content="GOTO 2013 • Solving Real &amp; Big (Data) Problems Using Hadoop • Eva Andreasson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2013 • Solving Real &amp; Big (Data) Problems Using Hadoop • Eva Andreasson</b></h2><h5 class="post__date">2015-10-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HSWfVpRzLEM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and thank you audience for being here
and spending your valuable time with me
or the next 45 minutes ah this is a new
presentation so bear with me please give
me a sign if I need to you know stop for
questions or if I need to speed up board
you know I really want this to be
valuable time for you so feel free to
jump in with with comments along the way
as Steve mentioned i'm here to talk
about a little bit more practical around
how organizations to use Hadoop it's the
most frequent question I get it's like
okay I know kind of what to do this I
know it's a distributed processing
framework and storage solution but what
do customers what do organizations
actually do with it i don't know if i
need to do when when should i deploy it
open what does it actually do for me so
those are the types of questions I'm
going to try to focus on and then of
course I'm going to stick around
afterwards if you want more you know
detailed information or more questions
I'm happy to take that discussion
afterwards how many of you know Duke
oh um so my super quick overview might
actually not be the super quick thing so
I'm going to then walk through what
Hadoop is and take it a little bit more
time here which means I might have to
cut out some slides at the end which is
fine I'll leave them behind um so Hadoop
is as I said a distributed distributed
platform for data processing that's the
easiest way to describe what the hell
you piss and it's aiming those difficult
problems that have become more and more
frequent these days around big data
right there's a larger volume of data
coming in there's many different types
of data that suddenly might not match
your current data models or structure
databases how do you put you know in
Twitter feeds into your SQL life data
infrastructure and make questions across
data sets it's not really a
cost-efficient way to put it in
traditional systems because the flow
data is so rich the variety of data
types is expanding what we can handle
with traditional systems and you know
that the speed of data being generated
is exponential growing so this is the
big data problem space where Hadoop came
around as a very interesting technology
that's grown into the you know most
popular technology to solve those
problems so Hadoop is actually consists
of two components MapReduce and HDFS
Hadoop distributed file system so HDFS
you can think of as the distributed file
system the storage later I'm going to
switch back and forth so you actually
get a notion of all these components
MapReduce is a java based framework that
you can implement your own algorithms of
workloads what you want to do to your
data how you want to process it what you
want to extract what rows in this file
are of interest to you and what you
actually want to do with it do you want
to count the number of a
says do you want to you know aggregate
apply some logic to this data well once
you find it that's the problem that
MapReduce souls so the mappers are
finding your data in your data set that
you have stored in HD fuss in a
distributed fashion and the reducers are
actually doing the logic aggregating
reducing that data set to the answer you
were looking for I'm keeping it very
high levels did so MapReduce is the
distributed parallel processing
framework it's written in Java but you
can actually integrate with C++ you know
other programming languages in many
different ways MapReduce is extendable
through connectors or through API
integrations but you actually have to
code you map reduce workload yourself
which limits the usability to perhaps
only the programmers which is why in the
ecosystem other kind of tools popped up
to make that data accessible in more
traditional ways so if you look at hive
and pig up here those were kind of the
next projects to pop up in the Hadoop
ecosystem hi is basically translating
your SQL queries to MapReduce were
closed so from a standard traditional as
he'll select star from la la where this
in this matches right it actually
creates Java processes sense it off to
the distributed file system with the
data resides in little blocks across
many many machines and process that data
in its place returns the value through
hive to wear your query recite a poem
from the common site and high is
integrated with both odbc and jdbc
connectors so it kind of starts looking
like oh now I'm a BDA or a bi person
coming from an SQL background I can
actually integrate and work with Hadoop
and paid came around as a very simple
you know career language as you like but
not the secret sequence process one it's
more process oriented so if you're a
programmer slightly walking into the SQL
and bi and data analysis space pig has
been you know become the most popular
tool query tool or career language for
programmers moving into data analysis
while hive is more addressing the bi
business intelligence side and the DBA
users moving towards who do pin program
so it is basically serving the same
purpose of simplifying query on your
data but it's slightly different
architected to suit these two different
user groups make sense um further of
course I do king with a promise of more
advanced analytics because you actually
have access to all the raw data in their
data as it is generated is stored
exactly as this in HDFS why because you
don't want to leave anything on the
table you kind of the promise of big
data is if you store the raw data you
can later decide what to query versus in
the traditional world you have to apply
a structure to your data scale it down
maybe lose some information because you
have to fit it into your data model you
actually form your data for your
question in the traditional world while
in Hadoop planned you don't have to
decide what you ask until query time and
that's when you fly your structure on to
your data at that point in time there is
no data model okay with me so far yep
cool
so advanced analytics back to that
advanced analytics is the promise of
this okay what-what can do with this new
kind of data that I can store suddenly
and this opens the door and interest
extremely in the machine learning in the
country so there's a lot of machine
learning research from many many years
ago that now have entered the hindu
bland and formed into you know
implementing those algorithms be it you
know recommendation engines clustering
algorithms categorization algorithms in
a parallel fashion using MapReduce
remember MapReduce was the Java
framework that you can implement your
algorithm in so think about machine
learning algorithms and how the startup
land and research land is now booming of
all these scientists thinking about okay
what if we now with enough data actually
can make sense with machine learning
machine learning has struggled by the
way for a long time like okay it works
for small sets and but once you expand
maybe the models doesn't really follow
what if you can train a model on a
petabyte of data how sure can you be
that it works well more sure than if you
only had a few tablets right so this is
this is like the the booming interest I
see a lot of startups in Silicon Valley
around how to do statistical algorithms
and machine learning algorithms
implemented either through MapReduce
ornate natively through some other work
load on hit it that's happening
so machine learning that's the mahout
project that's part of the ecosystem but
i bet you there's more to come um we
have lately seeing a need for more near
real-time work lives in the traditional
world of Hadoop hi Pig mahout and
MapReduce our old batch oriented meaning
you have to take off a JVM for each
MapReduce process and imagine you have
like thousands of MapReduce processes
running in parallel because you can
that's what enables linear scale of
processing / terabytes of data now that
is not all your use cases that you need
to apply to that data that you've stored
in this platform you actually have some
narrative time use cases as well for
instance business analysis sitting
sitting behind his I don't know click
view user interface or tableau or micro
Scotty you know a zombie I tool if you
ask a query across your tables in a
database the response times is what less
than a second few seconds I don't know
depending on your query but that's
limited data right maybe it's minutes if
it's a really complex bird now if you
kick off the same query on map reducer
through high burp it it's going to take
some time it's like comparing a Ferrari
to a bus right Ferrari you can have a
few people in it it goes really fast I'm
going to do you can have lots and lots
of people in it but it takes some time
but it's more powerful you can transport
more you can you know get more people to
where they need to be I don't know if
that analogy made sense but you're going
to hear more funny analogies for
everybody people didn't make it
just so the near real-time needs created
these new integrations for new projects
even if you look at impala chris's that
was a an initiative actually by by the
company I work for Claudia but that
doesn't matter it's still open source
than anyone can participate so Impala is
in real-time query engine it actually
isn't based on MapReduce that's the
whole key here along with some other
interesting low-level optimizations like
relying on the llvm package and all that
but impala processes SQL queries
directly on the distributed file blocks
that you have across many many servers
in your head up cluster so it doesn't
kick off JVM so it doesn't use Java
which speeds up a lot when you have tons
and tons of processes to kick off right
so Impala is a very interesting native
query engine on your distributed file
system all right solar weight is solar
part of Hadoop ecosystem now yet it is
lately the solar I don't know how many
know about solar no more people right so
solar is a very well-known open-source
search ended so it's built on the scene
it's the kind of the web service built
around the scene which is the most
adopted indexing engine out there so
recently solar integrated with zookeeper
by the way it's out there it's kind of
unknown to some people but it's more I'm
not how to define it in in a simple
English word but it's about process
management or failover if you have
multiple servers running on your cluster
and you want you know say failover if
one process goes down someone needs to
be the brain keeping track of that
process is down let's make someone else
the the master or less
spin up another server to kind of fix
that and have your services going if the
you know if the entity that keeps track
of where all the file blocks for size if
that goes down someone needs to be aware
of that right so it's a process
management it's the one who keeps track
of failover processes and you know also
keep track of configuring configuration
files you know if a new server spins up
what am I supposed to do when where do I
belong that kind of information contact
zookeeper that guy knows it all the
zookeepers kind of the brain that's how
simple i can explain so solar community
moved into integrating with zookeeper
something to enable a more robust and
scalable search solution now hey wait a
minute integrated with zucchi but that
means solar is kind of stepping in a
little bit touching on the head of
community so I I put it in here because
recently we also integrated solar to run
natively on HMS reading and writing
indexes to this distributed file system
hey maybe solving that scale issue of
index storage you know all these things
are moving the same way which is
basically bring the workload to your
data instead of the other way around
because moving data around is costly
when it grows in size make sense
I'm going to cut this short I can't I
have so much to talk about so q is a
really good tool if you want to play
with head open and realize what all the
back ends are about and I recommend you
to in a download and play with you know
maybe you can play with our pre-built vm
and explore these different bonus for
you he must be graphical user interface
that you can kind of make sense of what
it's all about each base is it quickly
said it's a it's a key value store it's
a different way of storing your data if
you have used cases demand quick look up
for a specific ID and it's also
column-oriented you can this is an
excellent storage for click streams for
instance for a specific user ID or IP
address or what have you you save all
the clicks on your website for that ID
and this allows you to track how users
actually walk through your website
that's a lot of data to store by the way
if you have a lot of traffic but it's a
very common use case for HBase because
it's based on HDFS underneath so you can
basically just store that data very cost
efficiently and linearly scalable store
it right but since the use case of
exploring okay what do my users do on my
web page requires that key look up I
want it okay for all these IDs from I
don't know from from Denmark no I want
to see how Danes utilize my travel web
site for instance this is the quick
storage solution for look up information
that you want to process for a specific
set of ID's before a specific ID below
so it's different than HDFS where you
can distort pretty much anything
different month so any questions on
Hadoop I might not answer them now but
we can we can save them for for the
question part but since it's hot in your
your cache memory right now I want to
stop and collect some questions or you
can tuck them in on your app if you're
if you only speak Danish hey that's
silence I know you're stunned with the
vision okay great um so now to the real
topic I was here to talk about how
people use it well i'm going to start
very high up again there are two views
in my mind this is my two cents me
sharing my experience right so there are
two steps or two views of big data
problems in organizations today one is
again the scale problem right I have
this data processing management system
in my environment today but I see or I
predict the great growth and I can't
really afford scaling up the traditional
systems or I have so complex queries
that ask my data have has grown they
take longer and longer time to execute
because the query sir or the model isn't
scalable right so it's a scale problem
for some companies and this is by the
way often where companies start there
are other drivers for Hadoop as well I
would get to them but if there's like a
future growth problem
you're worried about tomorrow's skill
and you can't really figure out how to
fit in or closing in your traditional
systems that's a good point to go and
look at him believe me most large
companies have and the other driver is
more about combining data sets I have
you know my my account info and in
Salesforce oh have some customer surveys
in a database somewhere or in a file
document storage I have log files you
know from these customers I have support
tickets in some other system but you can
only ask questions to that silo or you
have to struggle with some aggregation
application on top and you don't really
you know you have to do all the work
right here's where I find it more
interesting it means this is interesting
too but it's pretty straightforward it's
it's more about I want to do to my data
what I do today more of it is not really
new the technology is new it helps you
pain point but for an engineer like
myself it's like it's not that
innovation machine learning promise
right so this is where my heart burns
and why I work with it it's like okay
there is no box don't know what else can
do people talk about okay here's the box
it's the structured data right analogy
again and think outside the box but in
the inhibit when there is no box people
haven't come up yet when what you can do
it everything hasn't been invented here
you this is the new part at least to me
like can I can I actually get some kind
of 360 view of my customer by combining
their free text survey data their
support tickets their County do they
have actually did have your customers if
they are trained and had some
consultancy services
is that really true or what is a factor
that makes you happy customer because
what they write in free text is never
enough because they don't spend enough
time or they are like having a bad day
or you know what can data tell me that
can help improve my business or I don't
know you tell me what would you be able
to do if you can ask questions across
all your data that's interesting
question so breaking solid some typical
use cases so the most common one that
you know we have many 500 4 to 500
companies running in production today it
is to look at your infrastructure
realizing your data warehouses are great
but you can't scale the load that's
coming and often there's a fixed time
window involved when they really really
you know ok let's go into today who do
plan it's kind of scary because we don't
really know it and it's still although
it's been around for many years five
plus years it's still like ok but it's
open source its kind of evolving we
don't really know there's a there's a
threshold there in perception of should
we really go into the blender but when
you realize that a fixed time window of
having that data process in time for
some customer isolate most customers
realize they have to look at it this is
the big day only only wait of course you
can spend lots and lots of money on you
know enterprise data warehouses but most
companies don't want that anymore when
there is an alternative so the side
related use case here is of course those
complex queries if your data grows that
you need to execute the query over the
query response time grows
and some companies that I've talked to
have actually stopped some of their
queries because the is not a reasonable
response time anymore and this is sound
good to you and stop getting business
insight because of your infrastructure
not being able to support it that's like
you're losing money right there at least
in my opinion you can argue but you all
have the right to my opinion we'll all
right here's a real world example now
I'm moving away from my opinion and
sticking with data which is what I'm
kind of here two percent data is true is
the truth I mean um so one example a
network is storage company they had a
very interesting proactive support
process of collecting all their devices
and components hardware components and
log files in a phone home kind of
mechanism and analyzing these files to
predict when they should send out a
replacement component to the customers
in time for when that component is
actually failing so they could reduce
their customers downtime so this is kind
of a proactive support use case did I
explain that clearly so they had about
600,000 from home logs to process every
week and they had a constraint of forty
percent of those log files who have to
be completed within an 18 hour window
during the weekend to meet their essays
now this was a very good company they
had a lot of happy customers thanks to
this process maybe and their customer
sise grew that's a good thing you want
that to happen however that also came
with a seven-time terabytes a mouth data
size growth of the
and they were already hitting their time
window constraint they couldn't process
forty percent of that incoming data load
within 18 hours that's the fixed window
I talked about this is where the pain
comes how do i scale out the processing
to keep that window and they also had
some of those compass complex query
limitations that I thought when they
move to Hadoop in production they did
find that they could definitely meet
seven terabytes a month growth rate and
they also you know preempted the costs
of having to grow even further with
their traditional infrastructure it's
more cost efficient to just throw more
nodes at a Hadoop cluster and go and buy
some proprietary solution that is stale
and unlimited in his flexibility right
and they could also suddenly do those
complex words they could ask even they
could ask questions they hadn't asked
before questions that touched over 240
billion records the query that wasn't
possible in their previous system
because it wasn't a realistic response
time no one would sit there for you know
months because the response so they went
from response time of the most complex
queries that they still ran from weeks
of response time two hours that's a
measurable improvement and they could
suddenly ask for you know new patterns
for debugging and helping the actual
quality of the components so there
there's not only the point I'm trying to
make here there's not only one win with
Hadoop once you go Hadoop you get to do
other things it's your day
so many ten minutes left this is just
fun use case okay so a little bit
architecture to wrap that use case up
and I have like five more use cases it's
a sad I should get two hours um or talk
faster so here's a very common
deployment today you have your your
enterprise data warehouse or databases
or traditional backends you use scoop
which we didn't talk about earlier but
it's the connector that is able to talk
to structured backends transport your
data into HDFS and keep the structure so
you actually know columns or rows and
all that on your data if you if you want
to you can you scoop for that so you
scoop to get your data into HDFS they do
MapReduce processing to do some
cleansing more aggregation of data it's
it's etl know I want to prepare my data
a certain way for this kind of query or
next week I want to prepare differently
for my query it doesn't matter you can
always change your mind but it's more
flexible in Hadoop them so they use
MapReduce to do some aggregation across
couple of databases or back ends and
then they deployed hi because that was
you know the simplest path forward they
already knew a little bit about
MapReduce and they could do some tests
queries through their traditional bi
tools pretty quickly they got tired of
high response times and deployed Impala
so they got seconds response times or
minutes response times instead of you
know hours and then many companies also
as it's part of the data processing
pipeline you know they have 18 hours
time window to process all these logs
coming in right but then they also want
to do something else with those logs
they want to do you know log processing
in some other form so yes you can do
that in Hadoop but many companies don't
Oh many companies don't do everything at
once you start somewhere they bring your
workload well defined work load onto
YouTube and then they see that in
success it means that they might bring
out those prepare pieces of data to
their traditional systems again using
scoop and then you can serve to your
regular reports you don't have to change
everything at once but what happens
often as people do move to do is those
other things I said it opens their eyes
they can do more things so they move
another workload on to hit it and
realize oh this worked out quite well as
well and then the word spreads to other
teams within their organization they
want to use Hadoop and suddenly it
becomes this Hadoop serving function in
the company and that's where interesting
things happen because then suddenly two
teams within the same organization start
sharing data and you can from there go
to my favorite you know use case of what
new can you actually do what new
insights and you know I'm not going to
be able to cover the other ones the
quick intro took away some time remember
but I do have a log processing use case
you know lots and lots of logs many
types and to process today don't really
have the capability of collecting
everything they could and just to
mention the highlight their one company
I know I took these up ops locks
correlated it with marketing campaign
campaigns and realize for the you know
Deb focused marketing campaigns was
actually the reason that a few days
later their web services went down
because the traffic was suddenly peeking
so for every dev marketing campaign
forward they did an expansion of cluster
for their web services plan so that they
could keep their essays to the new
traffic that they actually wanted to
capture
from those events so correlating data
across organizations that's an excellent
example of what value that works I also
had some financial services firm you
know fraud detection it's a very very
common use case on who being able to
realize the anomalies out of lots of
data is easier than if you only have
work only can afford three months of
data so anomaly detection outlier
detection whatever data you have you
want to see where it doesn't fit the
pattern now we're touching on that
machine learning again event prediction
no pattern extraction categorisation
cluster to the human eye or two based on
too little information being available
in your traditional systems you can't
really detect these and we have many
financial services companies telcos
online retailers deploying fraud
detection this outlier they already have
those pattern algorithms from before but
applied to less data now they have
reimplemented them in MapReduce parallel
way of processing more data and thereby
they can you know obtain this harder to
find anomalies and keep their brand as a
safe trusted brain but the details are
in the slides here they can read them
afterwards in this case they also use
floom floom is that you know streaming
data ingest print work it's a
traditional pub sub architecture it
comes out of the box with a gazillion
sources that consume different streaming
data log4j Twitter API you know what
have you and you can configure flume to
stream those events and you know filter
out any way you want on the data way
into it and from flume you can also
directly write to solar if you want it
indexed on its way in you can write the
data to either HBase or HDFS it's a very
the framework very scalable all to
handle the you know terabytes couple of
terabytes add a large generation and
customers are saying they don't have
that today it might be because they
don't collect them but again I would
want to collect it I'm because that's
where the value is right another and I
talked about HP writing to a face this
one how much time am I out of time oh
really and I get extra time all right I
will stand right outside the door and
take questions um so the 360 do I
actually talked about in the beginning
you have market data account data
finding the customer now switch customer
recipe like customer success recipe for
patient success recipe hmm medical data
like can we draw yeah there are tons of
it by the way can we draw any conclusion
of you know treatment a applied some
time window before treatment be for a
patient with a random set of you know
categories or definitions can we allow
doctors to actually provide a better and
accurate treatment and more personalized
not this is not only used for
personalized marketing you can you can
use personalized for many different
processes and there's a lot happening in
medical industry today how do you
optimize to do less Eric Hadoop if
there's one thing Hadoop is good at that
I've seen production deployments on over
and over again it's collecting data make
new insights to improve a human
interactive process be it how should i
distribute my movies to different
regions
in in the US for instance based on their
preferences a Saturday night you know
you can do analysis of the patient data
or customer data how can i distribute my
my merchandise in a better way to
accommodate the customer Regents that
I've actually had a chance to end lies
with more data available be it not only
twitter feed but you know if you can
combine these two assets you get a
better insight and who you're dealing
and that's the vision so one global
online retailer they had 22 subdivisions
different retailer brands and they had
thousands of different products they
have online store and they have real
stores you want to understand if a
customer that goes online and look
something up actually goes to your real
store and purchase it later you want to
combine that data and for each
subdivision they basically had a lot of
repetitive processes they did the same
processing in mateesah or the entire
data they had both and all over the
place they had the same processing
across parts of the data and they
actually made a Hadoop strategy a
decision to replace a lot of that into
one data repository on Hadoop reduce a
lot of overhead processing workloads
that were done by each department they
could optimize the data warehousing
environment and guess what they could do
better marketing better customer support
and instead of waiting four weeks to
access some of those data sources to
find these business improvements that
were blocking out their teams to do
their job every team could get the data
immediately when they needed it there
was a lot of time
singing and that is a big dollar for
that company right so I wish I had more
time to talk yeah as you see there's a
lot of interesting use cases that I
won't even go deeper into today but here
we're touching on that land of
excitedness that I really keen learning
more about but I can point you to more
use cases if you're interested reading
up so what to consider I'm going to take
one minute for this the key bit benefits
of moving to Hadoop is a for linear
scale without the extreme price tag lots
of flexibility you don't have to decide
up from what to do with your data you
can change your way later because you
don't apply the structure to your data
until you want to ask something it's not
saved with your data now you can combine
and analyze different silo data in a way
that isn't possible or costly or you
have to do a lot of work to do today it
opens the door to expand business the
new questions okay but before you go
there now I promised you you know gold
and shiny castles right what I've seen
with all these companies if you don't
have a valid business driven use case
like you actually have a goal of keeping
that 18 hour window and you're about to
break it you know or the company's not
with you on what you can gain out of
doing this investment in to do ah you're
very likely to fit because Hadoop is an
investment perhaps to a cheaper price
tag but what I'm talking about here is
there's a lot of learning stuff it's not
the prepackaged plug and play it's
getting there but this is a
what did you see like 13 15 different
components that need to be weaved
together and yes Clara does that out of
the box for you but that was my only
product pitch and makes it easier with
management and everything but it's still
a learning curve every customer I talk
to you it's like from the first
realization that this might be will
actually solve their current issues
business-driven there's a six months you
know you have to ms people training
management infrastructure you could you
know purchase that from clutter but you
know it's how do you integrate with the
rest of your environment how do you
actually you know going to support this
in your organization and outside and
last point then I'm going to stop
talking finally you know security PCI
compliance production visibility AJ this
us are coming how do you want to manage
that because yeah there are solutions
but it's still emerging so you have to
know a lot before you go into production
on this and there aren't all use cases
who are fitted on hit it like I often
get the question around what about LTPS
is head up ready for the real like all
the use cases on an interested in
warehouse and no not yet right now I
would say this is what most people do
you take some work load etail be the
most simple most straightforward and
business motivated one for someone who
isn't aware of all the other goodness's
this is the number one use case people
started because it actually turns out
very successful then when you have
everybody's attention you expand to
other these cases and then you can think
of doing actually have to move out
or do it can I serve some other team in
an organization on this can I get
another team that increases your value
of your head your investment once you
expand the footprint it's easy from
there and then you get to grow your team
train people and increase your Hadoop
knowledge within the company I'm going
to leave you with a quiz if they just
kept it I had so many interesting that's
the thing so you learned a little bit
about how we're going to say since you
use Hadoop said I hope was it good Larry
goat please here's where you can learn
more the slides will be at your
convenience through the go to come first
quits how many believe that the
technology is the real big data
challenge good okay how many believe
that the knowledge it's the real data
challenge we knowledge like can I access
information about this technology some
work is it you know it's the experience
of this knowledge bit of this technology
share hmm yeah knowledge about the
technology okay so there are a few hands
I would say yes that's about you know a
little bit the challenge how many think
is the lack of people actually
experienced hey you all got home with
their building started yes so it's very
hard to find data scientists or
experienced people in Hadoop which is
you know one of the key reasons i love
this conference go to because I can be
here I can share my happiness about this
technology and excitement and I can
encourage people to get into it because
they will be need for word trust me know</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>