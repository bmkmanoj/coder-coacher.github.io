<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • Runaway Complexity in Big Data Systems...and a Plan to Stop it • Nathan Marz | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • Runaway Complexity in Big Data Systems...and a Plan to Stop it • Nathan Marz - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • Runaway Complexity in Big Data Systems...and a Plan to Stop it • Nathan Marz</b></h2><h5 class="post__date">2013-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ucHjyb6jv08" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hi everyone my name is Nathan Mars I
will get Twitter I've been involved in
the data and big data space for over 4
years now I've learned a lot of lessons
in that time and today I'll be talking
about what I believe to be a major
problem we have in the industry right
now which is that there's a lot of
complexity and the systems people are
build in and that complexity doesn't
need to exist and that complexity is
taking a real toll in making systems
hard to evolve hard to make robust and
other problems so this talk is
structured in two parts so first I'm
going to talk about some common sources
of complexity that you see in pretty
much every data system and then I'll
talk about what's a different way to
design data systems that doesn't have
these problems so before we get to that
we should actually just define what is
it that we're talking about what is the
data system what are these things people
are trying to build well good place to
start is that a data system is a system
that manages the storage and querying of
data right I don't think anyone will
really argue with that that doesn't
really tell the whole story
because the data system has a lifetime
measured in years right it lasts a long
time like Twitter's been around for six
years and it's been one continuous data
system over six years but of course over
those six years there's been many
different versions of the application
right we've deployed new features we've
taken away features we've collected new
kinds of data we've had schema changes
this is all part of what happens as
you're managing a data system right and
of course as part of those six years
we've had a lot of hardware failures
right we've had a lot of machines go
down things run out of memory discs fail
and your data system needs to just keep
on going through those hardware failures
and likewise there's been a lot of human
mistakes human mistakes are inevitable
in software development and your system
needs to keep on going past those human
mistakes so I'm going to talk about
three very common sources of complexity
which I believe to be nearly universal
and you don't really hear people talk
about this stuff really at all
but I consider this stuff just as
important as a stuff people do talk
about right people always talk about
scalability and the
there've and all these other things and
those things are very important but I
consider these things to be just as
important and the fact that people don't
talk about these things indicates to me
that as an industry we've just accepted
and internalized these complexities
people don't even realize that they're
being affected by these things and it is
affecting almost everyone so the first
one I want to talk about is human fault
tolerance so we haven't figured out how
to develop perfect software yet right in
the lifetime of the data system bugs
will be deployed to production this is a
fact right and people will make
operational mistakes humans aren't
perfect this is just what happens now
raise your hand if you like to build
systems where you know there's gonna be
a fault an own fault and you have no
plan to deal with it oh come on the
thing is this is what everyone does
right you you know a mistake is made and
you shrug your shoulders and you say
hopefully the logs have enough
information then we can figure out what
went wrong and correct it but relying on
luck to me is not sound engineering
practice I think the only way to really
deal with this is to consider humans as
part of your overall system just like
your hard disks your CPUs your memory
and your software and you should design
for human error
just like you designed for any other
fault now there's a lot of mistakes that
humans can make and in fact the scope of
problems that a human error can cause
are much greater than the scope of
hardware failures right like some
examples of human error is these are
ones that just I've made you might
deploy a bug to production that starts
incrementing counters in your database
by two instead of by one and maybe you
don't realize for three weeks
until a customer comes to you and says
hey this isn't looking so good
maybe you'll accidentally delete data
from a database raise your hand if
you've ever deleted data from a database
before accidentally okay see almost
everyone I remember my first year
working as a software developer I had a
colleague who he was collecting this
data set for weeks he was crawling the
web and collecting data and just waiting
to get enough data so that he could
analyze it and just you know do some
interesting stuff with it and then me
like an
just went in and just trashed the whole
thing it was all gone I wasted weeks of
work from this guy and this is my first
year as a software developer and I felt
really bad and you know I was
apologizing profusely and you know again
it's my first year I was like I mean I'm
getting am I gonna get fired for this
like I made a really big mistake and I
remember everyone was really empathetic
I remember one person said to me
congratulations you're now professional
software developer but this is a fact
this is this is what we do and these
mistakes will be made and you have to
have a plan to deal with them and of
course there are many other types of
mistakes that can be made now I would
assert that the worst thing that can
happen as a consequence of a human human
mistake is data loss or data corruption
because this thing is is that as long as
your mistake doesn't lose or corrupt
stuff you had before at least you can
fix what went wrong right now in the in
the interim of the mistake you know it's
up in the air what can happen but at
least you didn't destroy what you
previously had that's really important
and this brings me to the topic of
mutability so the standard operations
and data bases are known as crud create
read update delete and mutability refers
to the U and D and crud now what do you
build a system based on mutable state
the way you model the world is that you
keep state representing the current
state of the world and your system just
updates whatever you believe to be the
current state of the world this is
pretty much standard practice and how
people build systems now mutable systems
inherently lack human fault tolerance
your system is updating bleeding data
all the time which means any mistake you
make can also update or delete data and
that that can be arbitrarily bad it
could update and delete a lot of data
which means it's very easy to corrupt or
lose data and you lack human fault
tolerance I like in mutable systems to
being similar to if you had a guy come
over to your house and pour gasoline all
over your house and then just covers the
whole thing in gasoline and it comes to
you and he says hey don't worry about it
I checked all the wires they're all
insulated there won't be any sparks
nothing's gonna go wrong
but you know something's gonna go wrong
you don't cover your house and gasoline
right and likewise you shouldn't cover
your data in gasoline and you should
make your data fireproof by not making
it mutable now there's a different way
to build systems and this is basing your
systems on immutability now in an
immutable system instead of capturing
the current state of the world you
capture a series of events over history
and each event that you capture happens
at a particular time and it's always
true all right so you never delete or
update data you're always just adding
your data so let's look let's look at an
example of the difference between
mutability and immutability so let's say
you're storing locations on people all
right so let's say you start off with a
database that contains people and
locations so sally lives in Philadelphia
and Bob lives in Chicago
all right now let's say Sally moves to
New York so what you'll do is you'll
update your database to say Sally now
lives in New York right this is pretty
standard now an immutable system will be
different right instead of saying where
someone lives now you just say where
someone lived as at a certain time so
you say Sally lived in Philadelphia as
of that time Bob lived in Chicago as of
that time and when Sally moved to New
York she now lives in this new location
as of this new time right and this makes
sense the fact that Sally moved to New
York does not change the fact that she
used to live in Philadelphia both of
those things can be true at the same
time now basing a system on immutability
greatly restricts the range of errors
that can cause data loss or data
corruption right when you're not
updating and deleting data you can do
things like set permissions so that it's
impossible to update or delete data
which makes it much harder for a random
mistake to cause a severe amount of
damage all right which means immutable
systems are vastly more human fault
tolerant than mutable systems we think
about immutability is that while human
fault tolerance is a major benefit of it
it has a lot of other benefits right
immutability is fundamentally simpler
than mutability right instead of having
to support for operations they have to
work well together
C R u D you only have to support C n R
right that's fundamentally simpler
that's two operations that have to work
together instead of four and that's
about immutability
is that it's actually very easy to
implement your immutable data so when
you look at a mutable system a mutable
system implies that all your data needs
to be indexed so that you can find the
individual data unit you need when you
need to update or delete it now in an
immutable system when you only have to
append data you don't need to index all
of your data and we'll come back to this
later to see how it works but basically
implementing your immutable data store
is as easy as having a directory in a
distributed file system where every file
contains a list of data records and
whenever you want to add data you just
append a new file to that directory it's
very very easy to to build the immutable
part of your system and we'll come back
to this but I have to give a shout out
to rich Hickey rich Hickey is the author
of Clojure he's been a big advocate of
immutability and I think no one has
really articulated better the benefits
of immutability so if you want to find
out more I highly recommend watching his
talks ok so the next source of
complexity I want to talk about is the
conflation of data and queries and one
instance of this problem is the
normalization versus D normalization
problem so if you're working with a
relational database let's say you're
working with a relational database and
you're keeping track of locations on
people so one way you might design your
schemas like this where for every person
you have identifiers representing what
location they live in and then you have
another table which contains more
information on that location like the
city state in population now let's say
that a very common query you're doing is
getting the city and state for a person
well what you might find over time that
it becomes too expensive to do that join
between the two tables so what you do is
standard practice and when using
relational databases as you do normalize
your schema so you remove the join by
duplicating the data into your person
table so instead of just storing the
locations one time in the location table
you copy the city and state into the
person table right so the person table
now contains the name the location ID
and I'm the city and state and this
prevents you from having to do that join
which makes those queries much faster
now obviously you prefer data to be
fully normalized
denormalizing has a lot of problems now
it's up to you as an application
developer to make sure things become
stay consistent but as we know the
lifetime of a data system is very long
and it's pretty unreasonable to assume
that you'll be able to maintain this
very complex property over those years
and that you or someone in the future
won't make a mistake so inevitably what
happens in systems like this it stuff
becomes inconsistent and you have no
idea why and then what you do is you
start adding code to deal with these
cases or you write these one-off scripts
to fix the inconsistencies this is all
extremely complex and difficult to deal
with clearly you want to store data one
time you want data to be fully
normalized but you have to denormalize
for performance there's no way around
this when using something like a
relational database and the reason you
have to denormalize is because the way
you store model and query data is
complected together they're all
fundamentally intertwined now in the
second half of this talk we'll come back
to how you build data systems in which
case these in which these things are
disassociated and it can optimize this
pieces independently this is really
important okay so the last bit of
complexity I want to talk about is
schemas so I don't think schemas have
ever really been done right and it's an
incredibly important thing if you look
at the industry right now schemas kind
of have a bad bad reputation right
people consider schemas to be hard to
change they get in your way they add
development overhead does make your life
difficult as a developer and maybe they
have annoying configuration people just
consider schemas to be a pain in the ass
and so a big trend in the industry
recently has been to use schema list
databases and just store your data as
JSON and this stuff actually demos
really well if you do a five-minute demo
with the schemas database it's you're
looking like wow so flexible so easy I
don't have to deal with declaring fields
and types and all that stuff I can just
get in do my business and get out but I
consider this to be a major overreaction
and the prot the fundamental promise
take people are making is they're
confusing the poor implementation of
schemas with the actual value that
schemas provide there are two very
different things
and I think to really understand this
you have to ask himself well what is a
schema why do we have schemas what's the
purpose of a schema how does this help
us and a schema fundamentally is very
simple it's just a function that takes
in one piece of data and tells you
whether that data is valid or not this
is an incredibly useful thing schemas
give you a lot they give you structural
integrity they let you know that they
let you know that when you read your
data you'll have certain certain
guarantees of what's in it what fields
are in it what types they have what
values they have and the most important
thing this does is it prevents
corruption and prevents those mistakes
and those random bugs from corrupting
your database and these are the absolute
worst kind of issues to deal with
because you know you detect the
corruption long after it may have
happened right when you're reading the
data maybe you read it in a MapReduce
job or you read it just when doing a
database call and since that corruption
happens so long after it may have
occurred maybe weeks or months it's
extremely hard to track down what
happened you don't have any insight into
the circumstances that caused the
corruption right these are the kinds of
things that can cause weeks of time to
figure out and fix first of all you have
to figure out how it went wrong fix one
went wrong and then clean up your data
set to make sure it's clean again now
when you have a proper schema function
you can prevent these mistakes before
they happen right and you get an
exception at the time the mistake is
made which means you have complete
insight into what the problem was right
and this just saves an enormous amount
of time and this is a lesson I learned I
had definitely learn this through
experience so now you have to look at
okay why your scheme is considered
painful and there's a better way we can
manage them well one reason schemas are
considered painful is because it may be
hard to change right if you ever ever
tried to add a column to a massive
relational table you know this can be
take a long time and be really painful
and so you don't really want to mess
with a schema but you have to mess with
the schema because systems evolve over
time you're gonna have to change them
you know a lot of times
are overly restrictive right like you
can only have like maybe you can't do
nested objects or nested fields even
though you want to a lot of times
schemas require these translation layers
like like Oh like object relational
mapper z-- and this again makes it just
hard to deal with a schema it adds a lot
of development overhead right but notice
that none of these things are
fundamentally linked with that idea a
function of data unit right these are
just the stuff that exists or just these
specialized ways to make these functions
and I think the current state of the art
for making schemas is limited and
confuses people as to whether they even
want to use a schema or not so I think
schemas can be done a lot better if I
just describe my ideal schema tool I
would like to just have my data
represented using regular data
structures like like maps and Clojure
right and if schema tool should just be
a regular library it helps you construct
the schema all right so you can say here
the required fields and here the type of
those fields and if I want I can insert
custom validation logic to say things
like ages have to be between 0 and 200
or this this bank transaction has to be
non-negative a schema tool has to have
built-in support for easily evolving the
schema over time because like I said
your data system lasts for a long time
it's gonna change over time one really
fundamental thing you have to do with
data is serialize and deserialize it so
the schema tool should have support for
fast and space-efficient realization and
ideally your schema tools also cross
language because in a large company
people tend to use a lot of languages I
currently use Apache thrift to define my
schemas is everyone here familiar with
Apache thrift no a few people ok
it's basically a message to find its own
language for defining schemas it has
kind of a schema definition language and
then a cross language RPC framework and
thrift is pretty good I think it could
be better the one thing that's really
missing from here is the ability the
ability to insert custom validation
logic by by the best tool they exist
currently but I think it could be better
I think you have you had
that was really good then there's no
question you'd want to use schemas
because it just helps no it doesn't
really get in your way all right so
let's get provocative now I assert that
the relational database will be a
footnote in history it's probably not
gonna happen tomorrow it's probably
gonna happen in five years it's by not
gonna happen in ten years but it will
happen one day all right and the reason
it's gonna happen is not because of
sequel restrictive schemas or even
scalability issues although these are
all issues with the relational database
but it's because of fundamental flaws in
the relational database approach to
managing data right and these things
weren't flaws when they were originally
designed I'd actually say they were
features but things have evolved and
there are better ways to manage data now
and these flaws that relational
databases have our mutability and the
conflation of the storage of data with
how it is queried both of which we have
covered all right and these are the
reasons like the big the new term in the
industry now is new sequel and I think
this term is completely misguided right
you're just rebuilding the things that
have complexity and you're rebuilding
the complexity there's no reason to do
that now the thing is it's a new era we
have new abilities and we should use
these new abilities namely our ability
to cheaply store massive amounts of data
to do data data the right way and not
inherit the complex's complexities of
the past right you look at systems today
and like no sequel and if you do was the
cap theorem and eventual consistency
like it's kind a lot harder to deal with
data then it used to be in the past and
that's kind of a weird thing right like
our systems are way more powerful than
they used to be and we know a lot more
about managing data than we used to and
yet things are getting harder and more
complex and there's no reason for this
to be the case things should actually be
getting easier and simpler I know a lot
of you think are thinking so you're
thinking okay he says sequel is bad so
if no sequel is not sequel therefore no
sequel must be right that must be what
he's gonna recommend but absolutely not
I think no sequel databases are
generally not a step in the right
direction there are some aspects of them
that are namely the idea of using of
your database being a datastore
sure but things that are interesting
about these databases are not the ones
that get all the attention right because
fundamentally there's still based on
mutability and they're not
general-purpose right key value key
value is not a data model
it's an indexing scheme right and it's
not going to support all your
applications or even most of them okay
so how would you build a better data
system that doesn't have these
complexities so let's start from scratch
so first we should answer the question
of what is it that we're trying to do
with data systems what do we actually
use these things for so you might start
off by saying that a data system is
somewhere that you put data and then
retrieve what you stored right this
seems to be pretty standard on what how
people actually think about what
databases do this is not really how
people use data systems right and this
is very easy to see when you look at
some examples so for example let's say
that you're storing data of location
information on people so some of the
questions you might ask that data are
how many people live in a particular
location so here you're not really
retrieving what you stored right you're
you're doing this transformation and
aggregation of the data that you have
right it's different you're not just
getting what you had here you're
transforming it you might say where does
Sally live all right so this query is
more okay you're actually retrieving
something that you stored or you might
ask what are the most populous locations
again this is a an aggregation of your
data another example is you may be
storing pageview information maybe
you're making like a Google Analytics
like product so you might ask how many
page views were there on September 2nd
right again you're not really retrieving
what your stored you're doing this
aggregation you might ask how many
unique visitors have there been over
time right so here you're doing
completely different aggregation over
the same data maybe you're storing trans
transactions for bank accounts so you
might ask how much money does George
have again this is an aggregation you're
looking at his transaction history and
combining them to get one number which
indicates his balance or you might ask
how much money do people spend on
housing so now you're looking at every
one and again you're doing this
transformation entire nation
so I would say that that the way to
really describe what a data system does
is it computes queries which are
functions that take in all of your data
as input your entire data set no matter
how big it is
right now sometimes your function will
retrieve what you stored but oftentimes
will do transformations aggregations
you're not just retrieving what you
store it so defining a data system as
something that implements pure functions
that take in all your data as input is
the most general formulation of a data
system clearly if you could write a
function on all your data you can do
anything right every single data system
is encapsulated by that idea of
functions that take in your whole data
as set as input okay so let's look at an
example to see what I'm talking about so
let's say that you're storing pageviews
and you wouldn't be able to do a query
which computes the total number of
pageviews to a URL over a range of time
so if you were to write this as a
function of all your data it would be a
function that takes in all your data and
put a URL a start time and an end time
and you would iterate through all your
data and every time you see a piece of
data for that URL and with a timestamp
between those two times
you didn't chromatic counter and then
you return the counter right really
simple to implement function of all data
but obviously there's gonna be too slow
right you can't run that function in
real time all data could be petabyte
scale that function is not gonna run in
milliseconds it's probably take at least
hours to run all right but this is a
good place to start because clearly this
is this is completely general so how can
we do things slightly differently so
that we can support real time queries
right so we can do functions of all data
in real time and it's pretty simple what
we do is instead of just doing
on-the-fly computations on all the data
at once you insert a pre computation
step right it shouldn't be surprising
this is what people are already doing
but may be presented in a little bit of
a more formal way so for example for
this query instead of running functions
on all your page views
you might pre compute a view which
contains a mapping from URLs and hours
to the number of pageviews for that URL
for that hour right and that view would
be indexed by the URL an hour so now if
you do a query
know the number of page views for
foo.com slash blog between hours 3 and 6
you retrieve the entries for hours 3 4 5
&amp;amp; 6 you add them together on the fly and
that would be original so if you look at
an architecture like this there's two
pieces to it right there's a function
there's two just it's defined as two
functions one function which goes from
all data to your pre computed view and
another function that resolves your
queries based on your pre computed view
so there's only two problems you have to
solve to build a data system right how
do you implement the view function and
how do you implement the query function
so let's start with the views so a view
is a function that takes all your data
as input and then produces this pre
computed view right this is immediately
screen batch processing right this would
scream Map Reduce so we look at Map
Reduce when you asked what a Map Reduce
is they'll usually say something like oh
it's this thing where you can define a
mapper to reducer and the map will run
on it and the data will get partitioned
based on the key and then you'll run a
reducer on it right that describes how
Map Reduce works doesn't really describe
what it is
well Map Reduce really is it's a
framework for computing arbitrary
functions and arbitrary data because the
MapReduce paradigm is general enough to
implement pretty much any function on
your data this is really interesting
because MapReduce can be used to compute
those views and MapReduce you know maybe
has a little bit of a stigma of being
hard to use but actually there's been a
lot of great work in this area and
there's a lot of really great tools for
expressing those batch functions and you
can build these batch views and just you
know ten lines of code so when you're
doing pre-computation via MapReduce you
have all your data and all your data
will just be stored in a distributed
file system and you just run these
MapReduce workflows that run over all
the data and produce the batch views you
need to resolve your queries cool so
MapReduce is great for for defining and
running those functions over all your
data and producing your batch views
but then the second piece of this
equation is those batch views need to be
indexed so they can be queried now these
databases are really interesting like
all these things batchview databases so
the requirements are they have to be
batch writer Wolfram appreduce
they have to have fast random reads for
queries but most interestingly they
require no random writes and since
random writes cause Pauline 99.9% of
complexity in databases these are
incredibly simple databases and that
simplicity leads to them being very
robust and very easy to scale some
examples of databases like this are
elephant 2b which I wrote and then
Voldemort uno example written by
LinkedIn there's a lot of nice
properties about these batch views right
they're really simple like I described
easy to scale they're very easy to make
highly available right because they
don't have any randomize so you can
heavily optimize these these things
these things can be way more efficient a
way more performant than any in a
database that does random writes okay so
this comes brings us back to some of the
complexity stuff I was talking about in
the beginning so this architecture is
really interesting because those batch
views are defined as a function of all
your data so you always look at all your
data at once so you don't really have
any requirements over how your data is
structured so you can normalize your
data to your heart's content you're
gonna have strong schemas because the
only thing you're gonna do with them is
run functions over all of them so the
left side can be fully normalized now on
the right side the batch views can be
considered denormalized and i put this
in quotes because it's not exactly
denormalization like you have in a
relational database because when you do
denormalization you're still you're
optimizing how you store data so that
you can still retrieve what you stored
in a batch of you you can do
transformations you're not always
retrieving what you stored you might do
aggregations or things like that which
really interesting to see you still have
both ideas here you have both
normalization and the idea of D
normalization in the same system but now
they're independent the disassociated
from each other and they're connected by
these functions of all your data right
and this also solves the complexity
problem of D normalization because your
batch views are just defined as
functions of all your data right they
can't really get out of sync they're
always compute recomputed by running
functions of our
all your data right it's always
dependent on whatever your data is if
there's ever a problem it's very easy to
see what the problem was because you
have all the input and you have all the
output right that's all you need to
understand what went wrong what the
problem was okay so it seems like we're
done right we can use MapReduce to
compute these pre computed views and
they run queries in those views and
we're done right well not quite so think
about batch computation is that it's
fundamentally slow right running
functions of all your data is gonna take
hours which means that your views are
always gonna be out of date there's a
very interesting thing here though which
is that most of your data will be
absorbed into those batch views right
99.9 or 9% it's only the last few hours
of data which are not in your batch
views all right which are not pre
computed so all you have left is to
compensate for those last few hours of
data and then now you have a generic
system that can compute real-time
functions on your whole data set so we
actually look at your batch view because
it's always out of date it has the
property of eventual consistency but its
eventual consistency without the
Associated complexities of eventual
consistency this is a form of eventual
consistency that's very easy to reason
about because you just define a function
of all your data and just gonna be out
of date by a few hours okay so the only
thing is less left is pre computing
views for the last few hours of data
right I call this the real-time views
and this is where the stream processing
paradigm comes in all right so the idea
here is that you build a set of parallel
views to your batch views that represent
the pre computation for the last few
hours of data all right now the stream
processing paradigm is different than
batch processing with batch processing
you run functions of all your data with
stream processing you look at data as it
comes in look at it one at a time and
you update your real-time views as you
go in all right so this is where you use
incremental algorithms and things like
mutability so this portion can be really
fast this is also where no sequel
databases fit into the picture just with
these real-time views that represent
your last few hours data all right and
then when you want to resolve
application
you look at your both your batchview and
your real time view right you look at
what represents most of your data and
then what we represents your last few
hours of data and you merge them
together and now you get a result
all right so we went from this diagram
where you've pre confusing all your data
and you query your pre computed views to
a practical implementation of this which
I call the lambda architecture where you
have a new data stream coming in which
is constantly feeding into your all data
set you pre compute batch views and all
your data as a data stream comes in you
update your real-time views and your
queries go on both of those is a generic
way to build real-time data systems to
build any real-time data system now the
real-time views are the most complex
part of the system because here you're
using random read random write databases
you're doing incremental algorithm these
things are much harder and random write
databases like I said are much much more
complex but a nice thing here is that
because the real-time view only
represents a few hours of data this
portion of the system can be kept
relatively small right you don't need
huge no sequel clusters they can be
small because you only need them to
represent a few hours of data all right
and the other really nice thing about
this architecture is that if anything
ever goes wrong the batch layer is
constantly overwriting the real-time
layer so things will autocorrect over
time right when people build systems now
they basically don't have any batch
layer right pretty much the way people
build systems now is essentially only a
real-time layer based on mutability and
incremental algorithms right you don't
have that that batch layer supporting
you that will simplify your system and
make it easier to reason about and so I
call this property of the lambda
architecture complexity isolation
because most of the complexity of your
system is isolated in this transient
real-time layer that you can discard at
any point okay so any talk about big
data should mention cap is everyone here
familiar with the cap tomb raise your
hand if you're you are okay so I'll just
summarize it real quickly so cap theorem
is a theorem it's commonly expressed as
there are three property the three
properties of consistency there are
three properties of consistency
availability and partition tolerance and
you can only have two there's actually a
terrible way to
describe the cap theorem because you
can't really it doesn't really make
sense to not have partition tolerance so
really cap theorem means that you can
only have guaranteed consistency or
guaranteed availability you can't have
both in the same system now the key word
there is guarantee right as long as
there are no partitions you'll have both
consistency and availability the real
question is which one do you sacrifice
when you have partitions now you saw
that the batch layer is eventually
consistent right it's always a few hours
out of date but in the in a system like
the lambda architecture you can still
make the trade-off of whether you want
full consistency or full availability
and that's decided by a real time layer
if your real time layer chooses
consistency your queries will be
consistent if it chooses availability
your queries will be eventually
consistent now another interesting note
about the cap theorem is that the cap
theorem kind of scares people a lot and
one the main reasons for that is the
idea of building an eventually
consistent system as being kind of a
complex thing to think about right
because when you when you use something
like Cassandra and you're doing eventual
consistency it becomes hard to update
your database right because what happens
in an eventually consistent system is
that you end up with the situation where
you have these divergent values for the
same key and then occasionally you need
to do this thing called read repair
where you as the application developer
need to say okay these things diverged
how do I merge them back together so I'm
consistent again this really complex
really easy to get wrong and if you do
it wrong and something like Cassandra
where that's your primary data store
that's a permanent mistake you've now
corrupted your database now you still
have to deal with the same problem in
this architecture but the nice thing is
that if you mess up which you will at
least you don't corrupt your database
right because the batch layer will
correct for any mistakes we make in the
real time layer again this is what this
goes along with that theme of complexity
isolation and making it easier to build
robust systems that you can reason about
there's another interesting property of
this system you can do something with
this system that you can't really do in
a traditional system they call this
eventual accuracy now sometimes it's you
have some sort of query you're doing
where it's hard to compute the exact
answer for that query in real time all
right an example of this is computing a
unique count
over some set right the only way to do
that in real time is to maintain a set
of just maintaining everything that's
set and have that always available and
then you add and remove things from that
set to update your account maybe that
can be really expensive because the set
can get arbitrarily large but we can do
in a system like this is you can compute
the exact answer in the batch layer
which is easy to do because you're just
running function on your data and you
can compute an approximate answer in the
real time layer so if your unit count
maybe you'll do something like use a
bloom filter to reduce the space
requirements and make it easier and more
efficient to compute that query and I
call this property eventual accuracy
because we've made the trade-off to get
performance in the real time layer but
it's not a permanent trade-off because
your batch layer will auto correct and
things will become fully accurate over
time right and this gets you the best of
both worlds of both performance and
accuracy unless you maximize the value
get out of your data right you can get
as much value as you can in real time
which could be limited and then when
your data's been in your system a long
time it goes with the batch layer and
then you can extract the full value out
of that data and correlate it with other
pieces of data and things like that ok
so I just want to give just an overview
of where all the different tools in the
space fit into this architecture so
Hadoop is the best system out there for
doing the pre computation of the batch
views it's an implementation of
MapReduce it has a distributed file
system where you can store all your data
it's a very good tool for that storm is
good for doing the real-time use we're
doing a stream processing storm is also
good for doing the queries so sometimes
you have sometimes you can only pre
compute to so much and there's still a
lot of on the fly computation you have
left like an example of this is you
might need to do like a search and merge
query which can involve a lot of
computation so with something like storm
you can paralyze that computation to run
on a cluster and you can do things like
parallel search and merge very easily
tools like elephant DB and Voldemort fit
into the batchview portion and I think
there's still a lot of room left for
innovation and in batch views no sequel
databases fit into the real-time views
this is where you use tools like
Cassandra react
in HBase and I think the best tool for
actually managing your data stream the
tool called Kafka which is by LinkedIn
it's a very good system for managing
streams of data and consuming them in a
reliable way
okay so there's a few interesting
properties I'm going to point out about
the land architecture so because your
whole system is based on pure functions
of your data at any point if you need to
you can discard all your batch views and
all your real-time views and recreate
them from scratch in fact they'll
actually recreate themselves
automatically this is really good
because this gives you the ability to
correct mistakes you make be a
recomputation right so there's only like
a few kinds of mistakes you can make
right the first kind of mistake is that
you write bad data into your data set
right you write corrupt data maybe your
schema wasn't restricted enough so what
you can do is you just go in and you
remove the bad data and then you run
your recomputation then your views will
be corrected and things will be back to
normal alright another movement mistake
you can make is that you actually have a
bug in your code that produces your
views and then again to fix this you
just fix the bug and you recompute your
views and everything is back to normal
and the only other mistake you can make
is a mistake in your query function and
in that case all you have to do is
deploy the corrected version and then
everything will be working normally
right and as I mentioned before a system
like this the data storage layer is
optimized independently from the query
resolution layer right so this fixes
that normalization personalization
problem gives you the best of both
worlds of a fully normalized schema as
well as views that are completely
optimized for your queries and because
you can always recompute things from
scratch this also gives you the
flexibility to swap out your bathroom
real x views as needed right so maybe
you have new requirements or there's
some new and better system comes out and
all you need to do is update your
recomputation algorithms to go to a
different target you recompute and now
you're on a different set of set of
views and the other probably most
important thing about an architecture
like this is that data systems are
long-lived
like we've discussed they last three
years which means your needs will change
over
and they'll change in very unexpected
ways and having a system that's based on
computing functions on all your data
means it will support your future needs
right it will be powerful enough and
flexible enough to support whatever you
need to do in the future because nothing
is more general than function of all
data I think there's still a lot of work
to be done in this space I think things
can get a lot better I think one of the
most interesting things that will happen
in the future is an abstraction over
batch processing and real-time
processing right one abstraction that
will do both for you but still has the
flexibility to do different things in
batch in real time when you want to
achieve things like eventual accuracy
all right and I think in the next few
years we'll start to see tools like this
and I think there's also a lot of more
work to do in on those batching
real-time views so different kinds of
indexing schemes I think I think Redis
is a really interesting system we'll
look at for what can be achieved there
and because Redis exports these data
structures and data structure operations
and that's essentially exactly what you
want for your views and there's a lot of
work left to be done and supporting
different kind of data structures for
those views all right if I want to
mention I'm writing a book on this
subject my book goes at a much more
reasonable pace than I did in this talk
so if you want to learn more about this
stuff be sure to check that out okay
that's all that I have can I take any
questions the question was I see a
performance problem because as all data
grows right your batch out of those were
run slower how do you get through them
in a reasonable time there's two answers
to that so first of all everything here
can be implemented with fully scaleable
technologies right Hadoop is a fully
scalable system so you can scale your
Hadoop cluster as you need for your data
the second important point is that so I
defined the batch layers just running
function all your data that's definitely
the place you want to start because you
need that so that you can recompute
things when mistakes are made or when
you need to change things but it's
actually possible to incremental eyes
The Bachelor right so instead of always
running functions on all your data you
just look at whatever new data's come in
since the last iteration and you merge
that into your batch views you can do
that and I think
definitely something you should do if
you need that performance but the key
thing is is that you always have that
pre-computation step that you can fall
back on when you need to an incremental
is incremental as in The Bachelor does
make it much more efficient so question
was there's a moral comment but he was
saying that wouldn't it when it was data
you need precisely in real time this
system is limited yeah but that's what
the real-time views are for all right so
right like the way people built systems
now like I said it's essentially only
the real-time portion based incremental
algorithms and mutable state abases so
that's still in here right so you can
still do that stuff as you need but you
have this patch later backing it which
you know as you solve changes things I
mean for all data oh so that it's just
files and distribute file system it's
you don't have to index it because
you're just running functions on it so
you can so like I distorted industry
fastest time I build some tools to make
that easy you just do as a pending data
it's a mutable data so I doing the
pending data and I define my schema
using thrift and that's it yes you can
normalize it to heart's content the way
I actually model my data is using a
graph schema okay that's not index it's
just represented as a graph so all my
data is represented as nodes properties
and edges and I find this to be just an
incredibly flexible way to evolve the
scheme over time it's very easy to add
new properties and edges and things like
that
yeah it's the same ideas events were
saying I think the so vets or something
yeah yeah absolutely
yeah yeah so yeah it's very similar to
events or thing I think event sourcing
so idea behind event sourcing as you
store your events right but still has
the idea of you throw the events so that
you can reconstruct your current view of
the world or in this case we're just
saying we don't even care about what the
current view of the world is we destroy
our vent like that's my first class data
that I'm never gonna replace but it's
still it's very very similar it's the
same idea and you get all the same
benefits it's all based on immutability</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>