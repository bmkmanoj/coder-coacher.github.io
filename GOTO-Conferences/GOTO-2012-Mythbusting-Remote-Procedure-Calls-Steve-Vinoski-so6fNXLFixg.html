<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • Mythbusting Remote Procedure Calls • Steve Vinoski | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • Mythbusting Remote Procedure Calls • Steve Vinoski - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • Mythbusting Remote Procedure Calls • Steve Vinoski</b></h2><h5 class="post__date">2013-04-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/so6fNXLFixg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">remote procedure call does anyone not
know what that is so first question is
anybody not what that is
everybody good nobody raised that gun
everybody hears me right okay so if you
go to Wikipedia you get this definition
basically says the an RPC allows one
computer to call a subroutine and
procedure on another computer just what
we figured it would be the important
point is that the programmer writes the
same code regardless of whether the
operation is local or remote so what's
happened over time is that the term RPC
has become a genericized mention RPC I
made an RPC to that system over there
and they're not really doing this what's
truly an RPC so I mean as Martin said
I'm kind of old school back when I
learned RPC it was really a remote
procedure call nothing more it wasn't
just sending a message to another
computer and having it do something on
your behalf so the definition I like to
use for RPC is the true definition which
is exactly as shown here
I thought that most people would
understand what an RPC is so rather than
kind of belabor that point what I'd like
to do is go into the history of our VC
because I think there's a lot about RPC
that you probably don't know how many
were programmers in the 70s so think
about early networked systems the
ARPANET predecessor the internet started
in 1969 and back then the protocols
between the hosts were really oriented
toward humans interacting with
applications so you had email in 1971
and telnet FTP 1973 so you have these
protocols that allow human interaction
but they didn't really allow application
to application interaction and there was
interest growing and providing that so
if you go to RFC 707 which comes came
about in I think early 1976 late 1975
James White who wrote believed 28 RFC's
in total rope RFC 707 which was this
high-level framework for network based
resource sharing and it's built on
earlier parties such as 674 but what I
was trying to do was address that
growing interest of application to
application protocols so if you're ever
bored like maybe halfway through this
talk you might want to pull up that RFC
and give it a read
that was a joke we post-lunch love here
so one of the quotes from the RFC 707 is
that you know it's exactly that that
these human engineered protocols built
for allowing human interaction the
system's really aren't good for
resources to draw on one another and
there were some difficulties that were
addressed in the RFC as well there was a
big concern that developers of the time
had no experience in building
distributed applications so how would
they be able to take advantage of any
protocols that were application
application when they didn't know how to
do it so the idea behind RFC 707 is
something called the procedure called
protocol was that if you build something
that could make it look like something
they knew then they'd be able to do it
much better and that's where this
procedure called model came from
developers were already familiar with
making library calls so so let's start
treating these resources as if their
local libraries will provide a library
that makes a call over the network they
can look like that resources local and
the developer will know what to do just
calling a procedure so in other words
the procedure call model would make
calls just look like normal procedure
calls and gives them another quote I'm
not gonna read all the quotes but you
know it's basically saying we can build
protocols by just using procedure calls
isn't that great
after all if you're calling a procedure
you're asking something to do something
on your behalf and there's something
coming back from that request to do
something on your behalf
hey that's just like a procedure so
we'll just do it all the same way and
the RFC goes so far as to say well
compilers generate code to do procedure
calls so we can even have compilers
generating specialized code to make
remote calls so instead of having the
jump off to a procedure it would
actually incorporate some kind of proxy
code that would go and makes a call
frequently and take the parameters bring
them back and send them back into the
program it doesn't explain how to do
that but just mentions the idea so there
are some caveats in RFC 707 as well one
of them is that in many ways as it says
this idea kind of holds together
abstractly it kind of also misleads the
programmer because local procedures are
cheap you know typically just a handful
of instructions to jump to a procedure
or one and local calls are sorry remote
calls obviously are not cheap and
conventional programs can kind of have
one source of control whereas
distributed programs have multiple
sources of control so that's another
warning that the RFC was giving us and
that the applications programmer should
be aware that not all interactions can
be captured by this procedure called
model so you kind of wonder you know
you've got RFC 707 it's pushing this
model but it's got these caveats as well
and you're wondering why does it have
those caveats last year I was talking to
an old friend of mine
it's been doing this tribute systems for
forever and I mentioned that I had found
RFC 707 a while back and was reading in
that's sort of the source of RPC in my
opinion he said follow seven of seven
yeah did you mean 684 I should go read
that so RFC 684 is by my friend Greg
Shantz and it was published in 1975
which if you look at the date is before
707 obviously the numbers before 707 but
as I said 707 is kind of a culmination
of earlier RFC's and it's almost like a
summary so 684 was really commenting on
674 and if you go read this thing you're
gonna be blown away by the amount of
destroyed systems knowledge that's in
this RFC this is 1975 there's you know
we talked about things today that are
related to distributed systems and some
people kind of cure it and say oh gee I
didn't know that and yet here in you
know 1975 I think we had everything we
needed to understand these systems so
the some of the quotes from this RFC you
know procedure called may be appropriate
for certain applications but it doesn't
really model what happens in distributed
multi computer systems the procedure
called it is suitable for inter-process
but it's better to take some kind of
inter process communication framework
and extend that to do intra process
calls than the other way around taking
procedure model and extending
to remote calls that was another idea to
put forth here another quote was that
the procedure called model doesn't take
into account of nature of a distributed
system so you start to see these
parallels in RFC seven of seven between
some of these comments in 684 and those
those caveats in RFC 707 so there's no
doubt that seven of seven really was
affected by this particular RFC as well
that it incorporated some of these
comments just to kind of note the fact
that yes I did read it and you're right
about some of this stuff you know an IPC
model reinforces the idea that you have
this kind of multi multiple sources of
control have two communicating processes
cooperating with each other rather than
with a remote procedure model where you
have one thing in control just sort of
calling over two procedures on another
system and another quote is that you
know there are certain types of
operations like delayed operations or
prioritization but you can't really fit
into a procedure call model so again if
you ever want to I was truly amazed by
going to RFC 684 and reading it knowing
it was from 1975 that we've known all
this stuff for all these years so I
would go and read that RFC you get the
chance
so that's kind of what happened in the
70s all right who who was practicing in
the 80s all right now we're getting
somewhere
so I like to think of the 80s as the
research to practice decade in terms of
distributed systems lots of things were
happening at computing platforms are
going from mainframes to many computers
to workstations to pcs happening very
rapidly I used to work for Apollo
computer they invented the workstation
if you read later Sun Microsystems
literature they say they invented the
workstation they only said that after
Apollo we've gone so just to set the
record straight
there were methodologies going on at the
same time structured programming was
kind of a favorite in the beginning of
the 80s but by the end of the eighties
everybody was into objects and objects
languages there was just an explosion of
languages list before still hanging
around interest in Lisp machines
actually hardware machines I used to
work for Texas Instruments before Apollo
computer and part of the group I was in
was building a lisp chip so it was a
single chip that was gonna power the
symbolics this workstation Pascal one of
the first I started life as an
electrical engineer one of the first
programming jobs I got was Pascal
programming
but with Emacs so I got that right
see he was there small talk interesting
small talk be honest route struck that
done work on C++ Johanna is from our
coops if you didn't know that he had
done excuse me did not work on ck+ into
the late seventies but it really wasn't
until the 80s that it became C with
classes and later C++ Eiffel came around
beta came around Erlang yeah came around
lots of languages and then distributed
systems operating systems even
distributed systems languages and you
know frameworks and applications there
was just a ton of research going on in
that area so we got things like the
Berkeley socket API and we still use it
it's very useful there was a system
called Argos it was developed at MIT by
Barbara iske and what Marcus was is
focused on reliable distributed
computing it used a lot of transactions
over the network and like I said the
goal was to build reliable applications
that was an early research system the
seeder project that xerox produced a
paper by Pharrell and Nelson called
implementing remote procedure calls how
many of you read that you should go get
a free react t-shirt for that
the Fashion Show booth if you ever have
interest in renewal procedure calls not
only should you read the RFC's that I've
mentioned you should go read this paper
because the first time that someone
actually wrote down how to do it and
anybody liked the work I've done in my
career that was required reading anybody
doing that kind of work just really
started with that paper there was a
system called Eden which was an
operating system a whole distributed
operating system it was based on objects
and RPC
and then there was emerald another
object language that provided mobility
and really focused a lot on mobile
remote transparencies it could take an
object that was on one system and move
it across to another system and you
could be making calls to it and you
didn't really care where it was and
finally answer where and so where I was
developed in the UK now this is mid-80s
again it's kind of like you know mid
seventies with RFC 684 kind of amazing
that they wrote that in the mid 80s
and so we're developed a full kind of
full system based on RPC had all kinds
of distributed services in it if you
ever heard of a trader service a trader
is something that you ask for
characteristics of a service or an
object and if the trader has all these
skills it's metadata in this ORS about
services but we think of a naming
service you know you go to a service and
say hey I'm looking for this particular
service by this name and the name
service gives you a reference to that
service a trader you go with
characteristics of what you're looking
for and the trader gives you back
references that fulfill those queries so
that the trader was developed as part of
an somewhere
so a ton of stuff but involved in all of
it and all that distribute systems
research was our PC it's kind of the
backbone of everything all these systems
were what you could call whole stack
systems they provided everything they
had operating system language compiler
everything from they'd basically get the
hardware from the vendor typically the
universities were working with the
hardware suppliers they would get that
hardware and work with the developers at
the company in some cases but basically
build right on the raw hardware Hardware
build these systems from the ground up
and like I said most of them used RPC as
a key abstraction by that time RPC had
just become the way of doing things
there was also a focus on uniformity
where this idea of local remote
transparency was really important
there's a lot of papers about mobility
and transparency and the ability to use
services no matter where they were the
network address them the same way you
would send requests to them the same way
the other interesting part was this
notion of a strongly typed distributed
system so you'd have a type system that
spanned the system and every component
in the system would make use of that
type system and that was a goal of a lot
of these efforts as well and not only
that they wanted to have compile time
type checking the reason for that was
that at this time
the performance of computers on weren't
quite what they are today let's just say
they were quite a bit slower so compiled
code was king if you had things like
that's why were they were building a
list chip because Lisp interpreters were
too slow at the time
so everything was trying to go into
hardware go into compilation and even
across the distributed system checking
types at compile time some of the
protocols that were in use in these
systems you know the research papers if
you go read them they've almost never
talked about the actual on the wire
protocol they didn't define how you put
messages on the wire they didn't define
what the messages look like they didn't
really define how these types of their
type system translated and two bits on
the wire these things were closed they
weren't looking to interoperate remember
they're trying to build a system that
can almost take over the world right
they want a single homogeneous system
with the same type system everywhere
they're not interested in fact that
university across the street is doing
the same thing at the same time and
maybe have people talk to each other and
again this RPC you know it's just this
black box by then everything was going
to be dumped remark you see you didn't
worry about the RPC it was just gonna be
there so what happened as a result of
all this was in industry you started to
see distributing systems appear as well
and what happened as you get vendors
supplying their full stacks right back
then every vendor supplied the hardware
and the software Microsoft came along
and started supplying just software in
the 80s but they weren't really doing
distributed systems at that time so
anybody doing these network systems were
building the network cards doing the
hardware
doing the operations themselves and
again these were homogeneous systems
there was really limited
interoperability so they had to take the
research and somehow get it into
practice and the vendors really needed
to adopt the research that was going on
in the Universities but the problem was
they had their full stacks and they
didn't want to take their stack and kind
of throw it away and take this research
stack and replace it and hoarding that
kind of stuff into their system was hard
because again two incompatible stacks
and of course you needed compilation to
get the performance that was necessary
there was really nobody interested in
building this kind of stuff on top of
virtual machines or interpreters at that
time the other thing the vendors wanted
or standards the users started to have
heterogeneous networks because of that
we framed mini to workstation to PC all
the sudden users would have a mainframe
they'd have a few minis that have a
bunch of workstation confuse and they
needed them to interoperate the only way
to make that happen was to start leaning
towards standards so these users started
asking for portable operating system
interfaces for languages that would run
across all the systems that they had to
work with and for Network protocols and
the actual net
work standard itself to make these
listen to talking to each other in terms
of languages I've already covered that
researchers were building their own
languages they had operating systems
languages all distributed all
homogeneous and monolithic but industry
because of the users wanted to use
standard languages that's another
problem
so the vendors started to adopt some of
the research ideas but they did so by
keeping their own stacks and using
standard languages and some examples of
that again Apollo was an amazing place
in terms of the technology they produced
one of the things they built it's called
the network computing system it had an
interface definition language first time
I've ever seen an interface definition
language that was actually used in
production
I believe the idea I could be wrong
about this but I believe the IDL started
out as a way for Apollo to support
multiple programming languages but have
a common set of headers so they would
generate a seed header and a Pascal
header from this idea then this team
kind of adopted it and said well we
could use this for for our network they
basically use it to build stubs
client server side stubs for the our pcs
then the Sun had their RPC Indian open
and we're computing RPC very similar and
anybody Digital Equipment Corporation
IBM HP all all these NEC and they all
had these kinds of projects going on of
course the the internet was still
growing so ARPANET converted to tcp/ip
in early 1983 and so basically once that
happened you start
have this interoperability that was
needed across these systems because by
the say the mid to the end of the
eighties industry started to adopt
tcp/ip actually one of the reasons why
Apollo eventually disappeared was they
had their own proprietary network it was
a token ring network it's very cool very
fast for them for the day but the users
didn't want a closed Network they wanted
TCP all right I didn't ask any questions
during that part itself
did I ask any questions you know all
right so now we're gonna get into the
90s hopefully there were more of you
practicing by then good that's true I
always asked all right
get into one of my favorite topics
distributed objects spent a long time
doing this kind of stuff so because of
all the vendor interest in making these
distributed systems available to their
users all the research that was done in
the 80s and this kind of heavy focus on
object-oriented programming by then
objects were just the only way to
develop software if you're anybody you
were doing an object-oriented
programming and as a result you know
distribute systems plus object or in
programming equals distributed objects
and all the vendors had their
distributed objects projects going on I
worked by then HP at bought Apollo I
worked on the Corvis stuff starting in
1991 but there were a number of
companies that kind of fed stuff in the
core but based on these projects that
they had so poor WA was this industry
standard taking contributions from a
number of companies I think at the
height of the object management group
which which created korba gunship
management group was founded in 1989
Horvath was first published in 1991 but
at the height of the object management
group in the later 90s I believe there
were over 700 member companies
contributing to korva and services
around it but Corbett was really based
on this research that I described
languages were very important so they
wanted the first core respect specify of
mapping for the C language and then
later came C++ I worked several years on
a C++ mapping local remote transparency
was very important for the reasons I
mentioned
earlier and like I said it was all
distributed objects now here's a
question how many of you bought this
book a few massive things I was a
co-author here with Vichy Henning 1999 I
just got my royalty check on Friday it
still earns money he has 14 years later
really surprises me I had a tweet it was
last year some time because a royalty
check comes in and it was for $24 and 30
cents or something and I said well that
must mean court was finally dead because
no more royalties but the next one was
several hundred dollars so it's still
going it's still going but while uh you
know it spent a lot of time in korva I
also have this hobby of watching kind of
old TV series from the 60s it's when I
was a kid and I loved some of our shows
back then and everybody heard of Mission
Impossible like the original mission
impossible I got something to show you
they were way ahead of their time we
should have a lawyer standing behind I
get the best man in town I'm worried
about going over that way won't work
I heard the quarter has a special way of
getting rid his prisoners
very gassy
I hope he gets rid of his victims
I suppose that's all that's left of
Corvus victims speaking out of court was
way don't you see what he's trying to do
away from clobber you won't do any good
he'll just use you that's all Orbis
sounds like a borderline psychotic
that's real so what
so CORBA there was excuse me
a gentleman by the name of Maggie
well-core was a corrupt police chief in
some central american country but it
just struck me on a southern episode it
zooms in on this door zooms in on this
door that says Manuel CORBA and I
thought well that's just a translation
in their language to corba Manuel
but I just thought that was funny ahead
included no mystery in computing how
many people have read that I encourage
people to go look that up in their
search engine and read that as well
basically uh Jim Waldo was when I first
started working on core but Jim was the
technical leader of the group I was in
at HP he later went to some he developed
things like Ginny and help develop our
line so Jim and Anne Wallrath Jeff Wyatt
used to live down the street from me and
I used to babysit his children but he
was unfortunately killed in a plane
crash and then Sam Kendal wrote this
important paper in 1994 and explained
why distributed objects and local
objects really can be treated the same
way and this was a bit of a backlash
because Jim had been involved in Corbin
and he didn't like the way things were
going up to that point or so it started
in you know around 1990 they published
first spec 91 this is 1990 for Jim have
kind of been pushing for certain changes
to Corvo but not really getting anywhere
so this paper was kind of a reaction to
that they talked about these differences
between you know latency in terms of
local versus room over the access models
if you're accessing a local object
that's quite different than accessing
and distributed object partial failure
which is where you get a net split
server room part of your cluster fails
and how do you deal with that well that
doesn't really happen in the local case
it's either All or Nothing but in
distributed taste it certainly kind of
horrible
and then finally concurrency this goes
back to what was said in the RFC's in
the seventies you are talking about
these single points of control in the
local case versus multiple points of
control in the distributed case and
because you have multiple points of
control
you do have concurrency and you get
quotes like this from from a note on
distributed computing there's two two
things are being emphasized in the work
that was being done at a time
integration with languages and the
problems inherent industry at computing
and the paper says both are necessary
and I think if what you look if you look
at some of the things I've talked about
some of them really focused heavily on
languages and kind of forgot the
distributed computing part there's also
a thing called the fallacies distributed
computing put together by Peter Deutsch
these are the fallacies I believe the
final one was added by James Gosling
later but it's all the stuff that we
kind of know that network is reliable no
of course it is latency latency you just
have to deal with all these problems in
a distributed environment that you just
don't have to deal with dealing with we
could talk about Java in the 90s
obviously that's really came to the
forefront and there's a ton of stuff
going on the Java distributed computing
couldn't really cover it all here but
when you think about it a large part of
kind of we might call the mainstream
Courbet adopt java adopted quarter which
made sense because all the guys that
sung that were involved in court room
are also involved in java they want to
reuse these services so you look at the
java naming service it's really just a
layer over the core by naming service
for example
our minds interoperable with IOT made
use of the inter or protocol in Portland
now what really got me were these
efforts later to take Java and just make
like a homogeneous system out of that so
that Java could talk to Java but then
also kind of have a way to leak out into
korma and they started adding things
from the language consists of basically
from Java adding into corpora which is
supposed to be language independent
objects by value were basically that was
just a way of taking a Java class
passing it over the wire but kind of
treating an abstract way like it's a
korba object but not really a form of
jekt which stayed put it was a korban
object by value it's actually a valuable
object that got sent over the wire and
these things were it was just a
nightmare of if you've ever done how do
you have done standards work yes fun
anybody enjoy it there's actually some
sickos out there that do enjoy it
and they're always there you can see
them you know on this standards Bobby
and then you go I'm tired of that but I
think I'll go work on this wall it's
like opening the door you see everybody
close the door you go and this door and
everyone the same exact room and all the
same people are there reverse me Java
IDL napping so instead of writing IDL to
describe your objects abstractly and
then essentially the protocols you use
to talk to those objects you would write
things in Java and then map that
backwards to IDL and you get this kind
of horrible IDL out but then everyone
not using job I had to go and try to
implement it make sense over so in case
I wasn't clear I didn't like all right
also in the nineties yet the rise of the
web and of course you know korva and
Java really focused on the enterprise
and then so it came later I haven't
mentioned Microsoft at all and it's kind
of like you know you had distributed
comm which was kind of like core of us
he could not close together in the sense
of they're both trying to do the same
things there is this enterprise focus
for this stuff but meanwhile the web
which is taking over the world and when
you think about mysterious objects and
the web the web itself is distributed
objects and you think about it
every resource on the web has a fixed
set of methods so instead of having a
variable collection of methods to invoke
on the object you just have get put post
delete that HTTP methods the coupling is
done through hypermedia rather than
through specialized methods and
specialized data types of
in other disturbed object systems and
it's also language-independent which was
the target of things like Corvo but it
was also designed specifically for
large-scale networks I'll talk a little
bit about that more later
let's leave the 90s behind and we'll
talk about what's happened since then
so Web Services came around in early
2000 and w3c was working on these
standards for web services using so
using web services description language
I was involved in some of that I think
I've repented for my sins by now but
this was another standards nightmare it
was just it took I believe two months to
come up with a definition that people
could sort of agree - of what wasn't web
service just two months straight no
other topics in the standards body at
all just to two months probably another
month to write it up in a way that
people agreed so we didn't get very far
which is probably basically what's going
on was us notion taking the enterprise
view that Corvette and Java had been
pushing and really trying to apply that
to the web and it was almost comical in
hindsight because some of the people not
everybody but some of the people there
just felt that the web people had no
idea what they were doing they had it
wrong they didn't have all these
properties of these distributed object
systems that have been worked on for so
long
they couldn't possibly make this work
the protocols are too sloppy univers no
type checking all these kinds of things
meanwhile of course the wedge is growing
and growing and growing but they were
very adamant that it had to be done in a
different way ultimately what you got
was just corporate with angle brackets
it's what we like to call it because all
the descriptions are written in X amount
of course and you're basically doing
exactly the same thing as Corbin did but
with a different syntax so web services
are thankfully not used on the web it
actually wouldn't work the reason for
that is the scalability problems that
you have but you have to specialize
every single interface of every single
service that you have out there there's
no way for you for that web team to
handle that explosion of method calls
and understanding what the method calls
mean and how you use them one after
another workflow is all that kind of
stuff just can't do it force web
services are still used in the
enterprise I get hate mail occasionally
or I get somebody writing some nasty
thing about me on the internet
occasionally still mad about the
pressure I give talks like this they say
I tried to implement a service and rest
and it took me two weeks and I did it I
went to my IDE and I pushed a button I
got was Delon in like five seconds I
just wrote a java class there's nothing
you can say to these people because they
don't they don't understand its
difference between focus on the language
and the focus on the distributed system
focus on building you know something
that is just going to be used locally or
pseudo locally
case of these sort of small distributed
systems versus something that has to
scale to the size of the web so rest
came along was coined in the year 2000
by real boy fielding and his page these
pieces how many have read the thesis
good or more hands if you look at the
thesis one of the best parts about it is
its analysis of the trade-offs involved
in the distributed system if you were to
go back to a lot of the things that I've
talked about these RPC systems they
never talk about that kind of stuff
they don't usually talk about the
trade-offs involved they might mention
the fact that distribution can have
partial failure and the things we know
but they won't say well these are the
properties of trying to attain and so
I'm going to make the following
trade-offs to get those properties which
is exactly what rest is rest as an
example of doing that and I have to say
for me personally when I read the rest
paper it just opened my eyes it's like
well that's what I've been missing and
I've started to go down that path
when I was at Iona technologies in
around this time 2000 it is the chief
architect but I also did work internally
building services internally that were
used by developers like me and I started
finding that I wasn't using core well to
build those services I was using restful
principles to build with instead which
is why I eventually at the leave there
didn't kicking out RPC today okay here's
a question do we still use our PC come
in come on
it's probably about losing their hand
and the answer is yes we do but are we
smarter about it and I think the answer
is someone there's this awareness that
has come along that's improved how we do
RPC and how we view distributed systems
people understand that there's a
difference between local memory mode
that used to be you know an eye-opener
for a lot of people you have to start
explaining what can go wrong the
differences in Layton sees all those
kinds of things and it McDonald today I
think most people are understand that
the same with partial failure and still
I mean I work at Bosch um now so we have
react which is that's distributed
database and we still run into the
people who say well that's what's done
happen that much so why worry about it
and we actually do have people saying
that not in bash of mind you blocking
calls you know people understand that if
you if you make the call on you block
you're probably doing something wrong
support for multiple communication
patterns I think people understand that
the network is a different medium for
making calls to services than just a
procedure call in the local space
there's issues with coupling there's
also a programming language Renaissance
going over I'm going to talk about all
these more as we go forward but just
overall there's a better understanding
of scale as well so when you think about
the local versus remote like we said all
along here today you can't hide these
differences you can try to plaster over
try all kinds of games but you can't do
it so what we do today is we embrace the
fact that we have multiple systems that
are independent but working together and
we take advantage of that that's how we
get redundancy that's how we get
resiliency reliable systems and at one
computer you rely on the system in terms
of blocking if we go back to that idea
the true RPC the true definition from
Wikipedia for example then you make a
call and you sit there and thread that
made the call just blocks and waits for
the reply to come back
I saw the term async RPC used recently
and I thought what is async RPC well
it's an example of someone using the
term RPC when they really mean message
but an async RPC I don't think you could
actually have one what does that mean
maybe if you spawn a thread you could do
it or a true procedure call happens in
the same thread it's there and it's back
there was no asynchronous listening to
it you know a lot of systems today I
just use messaging so you send a message
later if you need something coming back
from the service to which she sent the
message
when you find the reply consider how
messages fit the overall flow of work
rather than just focusing on these
point-to-point calls like oh this
service needs this music call that line
and get this back I think today people
are a lot more focused on the bigger
picture and they think more about the
message flows through the system and how
to deal with those things in the sense
of what happens if a message gets lost
what happens if the message gets delayed
what are the consequences for the
overall system and people have learned
how to deal with those kinds of issues
if you have a true RPC you you don't
even have the option of a timeout unless
it's hard-coded somewhere in the library
underneath so if you look at some of the
RPC packages that are available today
they've built in timeouts and just by
doing that alone they're not really RPC
anymore because you don't put timeouts
on local calls
remember RPC the true definition is
local remote identical so once you start
to do things to handle the distributed
system it's no longer an RPC you think
about communication patterns you know
this point-to-point request reply that's
true RPC but it ignores things like
multicast broadcast pub/sub all those
kinds of messaging patterns that are
very useful one of the things I like to
do when I think about distributed
systems is actually think about how
humans communicate with each other
you think about that you know I can make
a phone call and call you up and that's
like almost like an RPC because I say
something and hopefully you say
something back it's very synchronous but
if you're not there I can leave you a
message kind of fire forget if you don't
reply I might fire it off again it's a
retry and then you can take these human
community communication patterns and
think about the straight systems
and so you can tell that an RBC couldn't
possibly cover the gamut of what's
required there when you try to build all
this stuff into a programming language
it gets difficult you can't just take
your average standard programming
language and decide to extend it to
handle these kinds of communication
patterns the lesson gear I think that
we've learned is that you embrace these
protocols there are ways of using these
systems or sorry these protocols to
build the systems you need to build it
without being so focused on programming
language and making an RPC kind of a
thing of what you're trying to do in
terms of coupling problems in our PC
obviously actually imposes these app
specific methods that I mentioned
earlier so when you define RPC you're
really calling a specialized method on
the other side of the wire you're also
typically sending some kind of
specialized data structure to that
method and if you contrast that with
rest where the verbs are application
independent and fixed the types are just
come from a like a reusable library of
types those will be content types the
mime types things like application and
Jason for example and then you also have
a content negotiation where you can have
a client that says I want
I'd prefer Jason but if you know that
well then just give me HTML and I'll
make the best of it in most RPC systems
I've ever seen you didn't have that
choice the transfer syntax basically
what you want to call it the way things
are transferred over the wire is fixed
in part of the RPC framework that you're
using so you're using this particular
framework then you get this style of
data you get this type system and you
get this way of putting it on the wire
and reading it off the wire and with
other systems like rest
much flexibility there because every
application is different the other thing
about RPC is that it has this implied
workflow when you have a set of methods
what is it that unites those methods is
just sort of a list of methods how do
you know which one to call which one to
call after that what to do with data
that comes back is the data that comes
back go into this call next door you
know you don't really have an idea you
have to sit down with whether wrote the
stuff figure out how that works or hope
that they documented it in some way and
with rest things like the rest HTML
hypermedia hypermedia of driving the
state transitions so when you make a
call you get back to this representation
this is following now the calls that
come after this are here here here or
here and it pretty much tells you what
to do next
another the Sun before RPC has this
fixed transfer syntax
the other thing about our PC that kind
of really didn't dawn on me until I
started to work more with was the fact
that you had this heavy infrastructure
on both ends of the wire you have to
have the same libraries that are
decoding at the transfer syntax or
encoding and putting it on the wire and
then the protocols that are involved are
often not simple so you have to have the
same libraries on both ends to get
interoperability for sure in the days of
courtroom we did have interoperability
between different implementations it
took quite a long time to get there and
that's quite a heavy infrastructure that
we were putting on either end at the end
of the day and with other approaches you
can have really lightweight clients
really lightweight servers don't take a
lot of effort or infrastructure have
this language Renaissance
I loved programming languages for a long
time starting around 2000 it seemed like
we were kind of stuck you know maybe we
were just kind of stuck with Java and
it's just going to become you know the
language to use for everything and C++
is still around of course but that's
changed since then there's this whole
renewal in the interest in functional
programming how many are doing
functional programming there's new
languages on the JVM I've never been a
Java guy I've done Java you could have
it I never liked it I was a hardcore C++
guy started with in 1988 and I just
didn't have the kinds of problems that
people seem to have with it but when the
Java came around it was kind of one of
those things where you look at it go
yeah C++ is still worse for me I'll just
stick with that as we moved along you
know it got better and so I did some
but then I just never warmed up to it
but the jvm on the other hand I think is
absolutely brilliant the work that's
gone into the JVM and what it's done for
languages in general has been excellent
so you've seen these new languages
coming about on the JVM
and I would incur how many people are
using new languages on the JVM Scala or
closure or even JRuby you know I would
encourage you to do that
how many Java programmers do we have
everybody the Java programmer someone
who uses Java to program I would
encourage you to look at those other
languages and of course you have
JavaScript everywhere but my favorite is
what says I'm not sure here early and I
want to talk a little bit about that
when you think about what's in that note
on distributed computing there was those
two paths that I mentioned the path of
language and the path of distributed
computing problems both had to be
addressed and Erlang is the best example
that I know of personally that actually
does that
how many Erlang programmers to be
okay but it really does look at this
distributed computing problem and give
you ways to deal with it and it also
gives you a reasonably nice language for
dealing with it so it's made good
progress on both of those both of those
paths unfortunately many ignore it
because it's not on the baby annum
it also has Prolog based syntax how many
Prolog programmers yes a few last week I
was at another conference and I asked
how many other language that were about
5 out of 80 people raised their hand and
how many Prolog programmers I expected
zero or one there were actually eight
more prolong than Erlang the shopman but
at that same conference there was talk
about JavaScript by Brendan my pre to
JavaScript so he's showing all these
examples of JavaScript and I'm sitting
there in the audience looking at this
saying how can people complain about
Erlang syntax look at that
so of course
Creston who's here master of ceremonies
here at go-to artists has developed
urging which is an Erlang implementation
that sits on the APM so I might want to
take a look at that if you're interested
in terms of scaling developers have a
better grasp of scaling systems today
we're you know years gone by and they
really didn't so things like shared
nothing architecture is cap theorem SATA
stage of event-driven architectures the
work that Amazon and Google have done
Martin's own mechanical sympathy you
know kind of you can't have the scale
without the performance and Martin's
going to talk about that later and then
just the operating system improvement
and networking and supporting these
kinds of systems of measurement tools
caption you think of caching an RPC is
very hard to cache because you don't
know if it's reading writing read-only
or what and when you're scaling a system
you have to have caching
so in summary RPC in my opinion is
convenient but flawed convenient and
correct we should have listened to Ric
chance and RFC 680 for a long time ago I
don't know what I would have done with
myself had I done that but I guess I'll
never know but fortunately the scale of
the web has pushed us into new paths
even within the enterprise things like
rest are being used for services that
olden days might have been
web services so my lesson is friends
don't let friends commit harpies
any questions judges question is why do
people persist if we know all this stuff
that I've just covered why do people
persist in wanting to use these
frameworks that try to hide distribution
and I think sadly the answer is that
people are lazy you know they want to
push that button in their IDE and just
have stuff pop out and I mentioned some
of that hate mail that it's exactly what
people yell at before you can't do it
that way you know I can go with my IDE
and do this I have my whole system up
and running and you know some of its
just apathy people have to get their
work done they don't really care about
their job there's no ticking a checkbox
so it comes down to people just want to
get something done out the door quickly
maybe maybe not taking responsibility
that they should for the system it's
really not a technical problem any sense
it's more of a human problem that's
right Eric's point is that by the strict
definition of our PC not everybody is
doing our PC people are doing you might
call it our PC like things but like I
said before when they take into account
the timeouts that can happen and they
take into account the fact that the
network can split then they are actually
thinking about the distributed system
and that's really not our PC anything
else did I bust the myth
thanks for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>