<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • From Homogeneous Monolith to Heterogeneous Microservices Architecture • Chad Fowler | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • From Homogeneous Monolith to Heterogeneous Microservices Architecture • Chad Fowler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • From Homogeneous Monolith to Heterogeneous Microservices Architecture • Chad Fowler</b></h2><h5 class="post__date">2015-07-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sAsRtZEGMMQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my job is systems euthanize ER and
euthanize er is a person that goes to
living beings that are sick and kills
them out of pity I've been doing this
for my entire career from system to
system going to various companies
finding these sick ailing systems and
sometimes slowly sometimes quickly but
hopefully humanely putting them out of
their misery and putting them out of the
misery of the people that are having to
maintain them and I hate this job I hate
it
I wrote this book called the passionate
programmer which is about being
passionate about your career but I
really hate my career it's terrible I
spend all of my time destroying the hard
work of other people sometimes of myself
it's really kind of depressing so I hope
to someday stop doing that and my talk
is not really about micro services or
all those buzz words that you saw on the
title slide my talk is about attempting
over my career to learn to build systems
that can survive and perhaps outlive me
so that's what I'm gonna talk about I
hope and I'm gonna do it via a real
world story so I currently work for a
company that builds an application
called Wunderlist
I live in Germany and the company is
called Zacks wunderkind er it's a to-do
list application so when I went out
there and I started working on this
thing my friends all kind of thought
this is sort of silly you're building a
to-do list application isn't that the
hello world of Internet applications so
you've probably all built a to-do list
or at least worked really quickly
through a tutorial where someone showed
you how to build a to-do list and some
sort of technology I have a more
interesting way to talk about what
wonder list is it happens to be a to-do
list but this is a new list sitting on
top of a cross-platform multi-user multi
master real-time replicated mobile
database system so if microservice
wasn't buzz worthy enough for you then
the description of wonder list should be
buzz wordy enough for you but it really
is that to some extent
that's a that's a glorification of
course but it's a pretty hard problem we
have millions of active users very
popular application we're on we have
native clients on pretty much any
platform that you would care to run
something like this on and it's very
popular we have a synchronization system
that is sort of domain agnostic but we
happen to be running project management
and to do kind of stuff on it so this is
what we build now about two and a half
years ago I talked to the team the first
time they brought me out to Berlin and
they were very proudly showing me the
new version of Wunderlist that they were
working on Wunderlist - so they had
built Wunderlist one which was built by
the founders very much the kind of early
prototype sort of thing the surprisingly
people love this it was actually built
in titanium if you know this
Appcelerator titanium so it was one of
those cross-platform things where you
build an HTML and you compile to iOS and
Android and etc etc and to their
surprise it became very very popular and
part of it is because they really did a
great job of designing and they did a
great job of marketing then they built
this grander vision of this thing called
wonder kit which was this huge project
management application that completely
failed almost destroyed the company and
they threw it away
so when I met them they were just about
done with their beautiful rewrite of
Wunderlist - that they were unveiling to
the world and they did a great job of
marketing again so on December 18th 2012
I had already signed the contract to
join the company and they excitedly
posted their blog post and they've lined
up press they had global features and
all the app stores they're amazing at
doing this stuff they anyone who worked
at the company at the time will never
forget December 18th as a date because
it was probably the worst day of any of
their careers it was almost the worst
day of mine and I didn't even work there
yet so this is what December 18th looked
like any of you use New Relic by chance
so for those that don't this is not good
you can probably tell like red is sort
of universally maligned as a color that
you don't want to see on a graph
I actually from the first keynote got a
more simplified view of this picture and
it looked like this so this is basically
exactly what happened the database was
on fire it was a huge mess and what they
had created and this was before I got
there so it's not my fault actually I
told them it looked good so it sort of
is my fault but they had created one big
monolithic application sitting on top of
a monolithic database which is actually
quite a large database given how many
active users were using the product
already and how much data they were
creating in the product and they did it
all in one back-end language this big
monolithic thing and this is a kind of
familiar story now this was on Rails so
we even have a word for this in the
industry now monorail I think the
Twitter people created this word they
had created a monorail they had a
synchronization protocol that they had
created that didn't really work it
worked except for some edge cases but
when you're dealing with this level of
concurrency pretty much it's nonstop
edge cases so they were doing a great
job of TDD but ultimately it took like
15 minutes to run the test suite and
anytime you would change something or
the timezone would change or all sorts
of problems would occur the test suite
would break all over the place so it
looked like this it would take forever
you would end up getting frustrated more
over time there are more more red broken
hearts in the test suite this actually
is what they look like by the way if
you're using Ruby stuff there's a plugin
for r-spec that prints these beautiful
emoji hearts so you stop maintaining
them over a while because you can't run
them it's terrible and the test suite
becomes worse than useless and I
remember on December 18th I was in
Washington DC at the time and I was on
HipChat with them because slack didn't
exist so you know therefore I'm not
uncool for saying HipChat I was on
HipChat with them they launched
everything went down like the investors
told me if the new version still works
within two seconds you did better
because it really did go down as soon
they launched and I remember helping
them and saying like helping them and
saying well have you tried putting some
caching in so that's what they spent the
rest of that day doing that allowed them
to eventually after about a week be able
to stay up but guess what the cache was
invalid so you know I'm sorry to them
for that but one of the first things I
did when I joined was of course remove
all the caching code because usually
that speeds up applications I don't know
if you've experienced that too and then
when I got there it was about two months
after they launched they had a week of
continuous downtime and then they had
two months of barely being up where the
uptime was going slowly up kind of every
week and eventually they got to where as
long as they didn't touch anything it
would be okay and if they touched
something then maybe it would fall apart
and really like I was supposed to be CTO
of this company but I was told by the
CEO on the first morning this is where
you're gonna sit it's the room where the
back-end people are and this is just all
you're gonna do and that was the only
thing that I knew so just go in this
room and see what you can make happen
and I walk into the room and everyone
just looks like oh god what is gonna
happen so they were completely terrified
of being terrified is probably the worst
thing you can experience in a job I
guess except for like being killed or
something but you know but they were
completely terrified so the thing that
they were really afraid of is getting
back into that period that they were in
previously where they were down non-stop
because what they had done is the back
end team had created something that
wouldn't work the client teams had
actually created beautiful clients they
were so proud of them everyone had gone
into this like crazy crunch mode to get
it done and the back-end team had really
failed everyone because no matter how
good the client code is to some extent
it's not gonna work if the backend isn't
working
so they really had failed everyone they
felt ashamed if they felt terrified they
didn't want to go back to that where
they were so they thought like what's
gonna happen this guy's gonna come in
he's supposed to be a ruby expert so
he's going to apply
Rubi magic and it's gonna be great and
the first thing i did is i got the keys
to the system and they had a number of
servers running and i just started
turning them off one at a time and
they're looking at me like this is not
what i expected and then it crashed of
course so on my first day i crashed the
system completely unusable and as a team
we spent a few hours trying to make it
work again and I'm sure they were
thinking well this was a mistake
these Ruby people are crazy I don't know
why we hired this guy but I made it
better because I came in the next day
and I did it again and the beautiful
thing about the next day is we had just
spent the previous day fixing these
problems and so the next day it took a
little bit less time and I did this
every day for about two weeks we crashed
the system over and over and over again
because really it sucked anyway right
but I wanted them to feel was there's a
fear that you have of touching this
thing of changing this thing what you're
afraid of is what might happen so what I
did is I put them into the situation
that happened no more fear right no more
fear any more you're just it's kind of
like you're already being physically
hurt there's nothing to be afraid of now
you know it didn't just end there with
crashing the system that would be a fun
story you know thank you
we're done of course we tried to clean
this thing up well they had created this
monolith and I said it's a to-do app and
you're probably thinking well that must
be really simple of course not
programmers made it so it's not simple
at all but also it's you know nothing is
really as simple as it sounds all
programmers look at a problem and
oversimplify it and think well yeah I
can make that that's easy that's just a
two hour job but they had encountered
complexity and they were doing pretty
complicated things you know they were
having to do sharing across different
like sort of different constellations of
people sharing different lists of stuff
and getting notifications and trying to
do that in an optimized way and in the
process because it's Ruby they were able
to create all of this beautiful magical
coupling so that you couldn't really
tell what was happening it looked simple
but it wasn't logic was just buried in
abstractions and there's just no way to
find out where anything is
and as I said they had created their own
environment of fear uncertainty and
doubt this FUD word actually isn't as
popular as when Microsoft was the bad
guys and that's interesting but most of
you were old enough to remember that but
so I'm talking about the flood that you
create for yourself anyway this is just
the beginning of the story so I want to
back up now a few years I've been
talking about this thing for a while and
this is from this is the opening slide
from a keynote that I gave a couple of
times last year but specifically at
Scala days I think is when I put this up
I mentioned that I'm a software use
euthanize er back in 2006 the beginning
of 2006 I wrote this series of articles
that got on the front page of Digg which
was actually a big deal back then called
the big rewrite and a lot of people have
quoted it since and it's about why doing
big software rewrites is a bad idea
and what I was really doing was just
blowing off steam has already in 2006 I
had spent what I thought was an entire
career doing this system euthanization
euthanization you thought yeah
euthanization that's good enough but I
wanted to find a way to build a system
that would exist I'm tired of this as I
said and so as an ex musician I thought
about this word I was actually in the
car with cory haines by chance getting
lost on the way home from boulder to
Longmont which if you're from Colorado
you know it's not actually possible but
somehow we did it and I was talking
about an upcoming talk that I had to do
and I think Cory came up with this idea
I don't know I don't know how it
happened but we were talking about why
is legacy a bad word in our industry
because in the music industry it's a
good word but in our industry it means
something like this I think there are
two definitions an upshot a consequence
aftermath like really nasty kind of
stuff legacy is not a word that most of
you associate with something good if I
offer you a job and I say like the title
is legacy software engineer you're not
very impressed
however in music and art and literature
and pretty much
anything else you can think of the word
legacy has a positive connotation and it
means that you're leaving behind
something
so like heritage endowment bequeathment
you're leaving something for future
generations but somehow we've twisted it
around in our industry so like Beethoven
left a legacy he's still a bestseller
it's amazing we don't do this in
software though in fact in software as
we saw in this morning's keynote this is
a beautiful visualization of the
Standish Kaos report though I made you
don't see the absolute numbers because
they don't matter but this is showing
you successful challenged and failed
software projects and I think we heard
the definition this morning the green
ones are successful so we all sort of
know what that means although I would
bet that they weren't that successful
the challenged ones are significantly
over time or over budget so to me that's
not really successful that's pretty much
a failure to and then the failed ones
actually never launched so look how bad
we are and this goes through 2009 but it
doesn't matter it looks like this
forever and it probably will look like
this forever I don't know I can probably
do this talk for the rest of my life
that's depressing but ah this is what it
looks like so we really really suck all
of us me and you we really really suck
we're really bad at delivering software
projects and then when we do we deliver
crap maybe it's really crap maybe it's
not I don't know
but business software that's deployed
the average life expectancy is five
years that is scientifically generated
by me when I type these words into the
slide but it feels right to me and it
probably feels right enough to you you
build software and then you hear after
you leave that company that someone
there is some project to rewrite it and
throw it away and you think of course
there was and and you think back to all
the nights that you spent cramming to
get it done by a deadline and how much
it sucked and the sacrifices you made
from your personal life
and it's really depressing so you barely
ever launched a successful project when
you do it's dead after five years and
then people like Joel Spolsky say that
it takes ten years to make good software
so your software is dead and half that
time that's a bummer what are you going
to do how do you create legacy software
how do you create a legacy and software
this is what I've been thinking about
for quite a while now very very deeply
so I talked to my friend Mike feathers
Mike feathers wrote a book called
working effectively with legacy code and
until recently I think it was the only
software book that I could find that had
the word legacy in the title because
most people are smart enough to know you
shouldn't write a book about software
and put the word legacy in the title but
he did and so I talked to him about it
and I asked him like how do you create
legacy code and he said well you know
tight coupling and not understanding
business requirements and using fringe
technologies it's like dude you're
depressing me I'm talking about like how
do you create a legacy they said oh okay
maybe you should read this thing by dick
Gabriel it's a dick Gabriel if you don't
know him he wrote a bunch of really
influential stuff including like common
lisp object system and a bunch of papers
that are worth reading like worse is
better but he wrote this thing called
design beyond human abilities and it's
actually a presentation that he gave
that he converted into a paper and it's
about creating software beyond what
humans could do so he's talking about
like multi trillion line software
projects where probably all of us
combined have created less than three
billion lines of code already but like
how would you create a multi trillion
line software project as a way of
thinking differently about design and
one of the things he says here is that
biological systems are very much larger
than anything coherent the people have
built and he says a lot of really
interesting things about biological
systems like he talks about cellular
regeneration cells go bad
and the cells die and they get replaced
and we've heard all this all our lives
right like I am not the same physical
being that I was ten years ago or when I
was born
I'm much larger than I was when I was
born for example like at least 50%
larger when I was born that's that's
also scientific and actually if like if
I think about me you should see what
I've eaten today I shouldn't be alive I
don't maintain this system biological
systems also have a thing called
homeostasis it's a thing as a process
whatever that I'm not a science II guy
but homeostasis to oversimplify it is a
way that a biological system can
self-regulate so one part of the system
leans toward a certain type of behavior
that would be bad for the system overall
but then another part of the system
regulates that and the cellular
regeneration stuff is sort of part of
that same idea so what in a software
system is a cell because it seems like
in in the system that's me it's okay if
my cells are replaced and somehow I can
still keep giving this talk over and
over again I'm still me I am the system
but what in software is a cell and what
in software is a system and when do you
build a system versus a cell how do you
know you're building the right one this
is what I started thinking a lot about
and if I could get it right could I
build a system that can outlive me
without of course just prematurely
ending my life this is a dangerous
career goal I understand so anyway back
to back to the Wunderlist thing
Here I am I'm I'm spending long nights
and weekends trying to keep this ailing
system alive because my desire is to not
kill any more systems I mean already for
six years I've desired not to kill
another system
but then the day comes we have a board
meeting and we say to the board yes we
are going to have to kill the system
we're gonna have to replace it but this
time it's gonna be right and remember
this is the third time we're doing
Wunderlist and the fourth time we're
doing a whole new system and one of the
board board members who is one of our
investors where vc-backed business so
he's a board member who really you know
holds a lot of power you could hear it
was a call and you could hear him like I
don't know spitting or something I don't
know it sounded like he could hardly
breathe and then when he started
speaking he said these words please tell
me I'm never going to hear you say that
you have to do this again but you could
just feel like the hatred and not be
hatred for us but the hatred for a
situation and and I said I promise that
we will never rewrite the system again
and if you hear me saying that we're
gonna do it you can fire me from this
job you can publicly ridicule me and I
will leave the software industry and
that's it thank you that's the end of my
career but yeah the system didn't
survive we had to kill it so how
starting this process how can I create a
new system that can survive so that I'm
not publicly ridiculed and I have to
leave the industry so I'm going to talk
about some of the things that we did
starting with kind of like tenets if you
want to call it that that's really fancy
sounding but really it's just like
random ideas that I had collected over
time that I thought sounded smart that
we tried and and then I'll talk about
more concretely what we ended up with
but we created something kind of strange
I think so here's the first one now this
list we're not really using Microsoft
Visual Basic exclusively no we're not
really using Microsoft Visual Basic but
the first rule that I decided to come up
with was that the system should
heterogeneous by default not like it's
okay to be heterogeneous that like
actively encouraging we're gonna write
this thing in a bunch of different
programming languages and why don't you
do this well one reason is I didn't want
this weird coupling that happens when
you have a language like Ruby which by
the way I love I'm not bashing on I've
used it for many years but you have a
language like Ruby where you can just
reach into the guts of anything and do
anything you want like you can't find
out you can't there are no boundaries in
a situation like that and that's really
why I love Ruby but I didn't want to
build the system that way so I wanted to
create an environment we're tight
coupling at least that type of tight
coupling would actually be so hard to do
that you'd really have to go out of your
way to do it so like we have some JVM
stuff we have high school we have node
we have rust now we have go pretty much
anything you can think of that you might
want to try if you're some nerdy
programmer we probably have it running
in our system and I think that's okay
the other reason that I wanted to do
this heterogeneous by default thing is
really just I have a career full of
standing at the watercooler of a coffee
maker or whatever listening to and
participating in complaints about why I
can't use this technology at work
because my boss says I can't or we're
tied to some old crappy version of Java
or Tomcat or struts back then or
whatever it was and I want to do
something new and if I find a job where
they let me do the new cool thing I'm
gonna go do that and so I wanted to
create an environment where my people
own the technology choices and can do
literally anything they want and that's
what we have the rule is you if you're
on my team you don't have to ask me for
any programming language anything you
want to try you can do it as long as it
follows some rules one of the rules is
the code for a new thing but primarily
this is the rule needs to be no larger
than this so you have to look at me for
this one it's this big the code must be
this big or smaller I always hold my
fingers up and I never
really measure their code with my
fingers but it directionally kind of
sets the idea I want code that's so
small that anyone can come along and
read the Haskell code even if they're
terrified of Haskell which it seems
everyone on my team is except for me and
the other guy who were at the Haskell
service but anyone can understand it
because it's this small and they're not
stupid and Haskell is not hard to read
Haskell is easy everyone should do it a
bunch of idiots to do Haskell anyone
could read it and even if they can't get
it to compile then they can rewrite it
if there's a problem no big deal
throw it away throw the code away that's
the other thing this is the idea from
cellular regeneration when we first
started building the system I said we
know we're gonna have to rewrite it we
know we're gonna have to upgrade
frameworks it's gonna be terrifying when
we do even if we stick with Ruby it's
gonna be hard because Ruby's gonna go to
version 2 and we're gonna be pressured
to go to version 2 at some point
libraries will start working blah blah
blah so let's just get into a habit of
throwing code away and if it's this big
it doesn't matter just throw this little
piece away so it's really easy to
upgrade and it's really easy to think
about new things and change whatever
else so this has been an active goal
whenever someone talks about writing
something that already exists and works
ok and a new language
I say excellent let's try it worst case
they're gonna spend an afternoon have
fun decided didn't work runs and tests
done work whatever throw it away
best case it's better than the thing
that we had before and we throw the old
thing away
perfect so the code should always be
changing inside the system and it starts
to already force this idea that the
system is a thing and the cells are a
thing and we also move from the
monolithic database idea to a bunch of
tiny databases this is actually a
diagram of our database topology it's
very exactly created and Keynote where
you hold down command and you just start
caught moving stuff around but we really
do have about that many databases right
now we used to have one just for a
simple to do
we have this many databases and you
might think this creates complexity but
it actually makes things a lot easier
because we took what was a really big
hard problem for us and we turned it
into a bunch of tiny problems that are
easy to solve and my hope is that
eventually we'll have millions of
databases because I think every user
could have a database or every task in
the system I don't know there's probably
something we could do and you know
hopefully it will never have to go
through the Jepson tests but it works
pretty well so far so what we did is we
have a bunch of small services too and
the tiny services sit on top of the tiny
databases just like dr. Parsons said
yesterday is common in a micro service
architecture the service is owned the
databases and this is a little snapshot
from our configuration management system
that we made in bash where I'm just
counting the number of service
configurations we have and there were
150 something of them for our simple
little to do app at one point in time
there are a few more now and then we
also kept the the actual requests or the
invitations of things tiny and yes we
are using crystal meth to do this
HTTP but we have some ways that make
this okay one thing that makes this
better than what we had previously and
tolerable as a synchronous thing man
it's really terrifying to be the last
keynote and listen to the the ranty
keynote in the morning before you and
then they're sitting there looking at it
but it works I promise the requests
themselves if they're tiny yeah no I am
NOT even gonna explain it you scare me
too much but no we used to so we used to
have a system where we made these large
requests and we created this batch API
that was the worst idea ever but the
plan was like let's reduce the
connection overhead from the mobile
clients because that's expensive and
will allow them to do a whole bunch of
requests via single HTTP requests and
get the whole batch response back so is
this huge blocking thing that was
synchronous that was terrible and what
we've done in effect we actually have an
ace
Kronus layer on top of this so you can
stop judging me now wait till later but
well we've done is tried to create tiny
tiny little requests and yes the clients
actually make them but in effect you can
do things asynchronously and you don't
have this big blocking thing going on so
again take a hard big problem and turn
it into a bunch of tiny problems and
then solve them one at a time and then
to avoid the piles of abstraction thing
we we have not favored reusable
libraries at all we've done it a couple
of times and we're actually refactoring
them out into services so does this hurt
performance on the small sure but it
helps performance in the large and it
helps our ability to be productive in
the large so we're creating more and
more reusable services and when we find
ourselves building a library that we
share among a bunch of different
services or apps we think is there an
opportunity and does it make sense to
make this a service instead of a library
that we mix in everywhere ok so well
some more deployment oriented stuff but
still in this kind of cellular
regeneration
vein all of the nodes that we deploy are
disposable
which means you will never actually
worried about the uptime in fact the
only worry I have of uptime of a node is
it's been up too long and I can't trust
it and a node is a server in this case
so we happen to be on AWS which seems to
have the most advanced set of features
available for doing this kind of thing
but it would work on other cloud
providers we for any new deployment of
our software we generate new images we
don't do docker yet but we generate new
images and we we clone them deploy them
out and then destroy all the old ones so
we never upgrade software on existing
nodes so you don't have that thing where
like I remember in the 90s I was a
system administrator and I would do
uptime on the Sun box and it would be up
for like two years
and I was amazed and proud and horrified
because we've changed so many things
there was just no way we could ever
reaper
it so over time people built stuff like
chef so that you could let a system live
for a long time but have a way
supposedly to reproduce its state but
that doesn't really work so in our world
we we throw away the servers and with
that we can throw away chef the
complexity of chef and this is one of
the best things we did for ourselves and
it's not chefs fault it's the fault
where chef solved a problem that we
don't have to solve anymore at least in
my limited perspective of the world I
don't ever want to have to solve this
problem again so we replace chef with
originally a makefile and because we're
lame we called it weight which is wonder
make because everything has to start
with wonder at our company it's no
longer a makefile because that was a
really stupid idea but it's it's bash
scripts which sounds even less and less
intelligent but I promise it works in
fact the author of ansible posted
something yesterday saying this
immutable deployment thing this is the
way to go and by the way you should just
make bash scripts for this that was
gratifying so we created wake it's just
a convention driven set of bash grips
that allow us to provision these new
instances and then roll them out and
what what have I run here you can't see
it because it's too big but I'm running
wake info for our files service and you
can see various things we're running for
the file attachment feature of wonder
list at that time and then here I run
awake describe ELB for icon we can see
server's running to support our iCal
integration and then I wake scale iCal
to three servers and it changes the
number of servers from nine to three I
think it is there and so we can do all
this stuff just usually there's one
command if you even need a command to
you know we have too much load right now
on a certain service so we can scale it
up from 20 to 30 and handle load that
kind of thing and then we we built a
simple little web hook thing so github
for every every one of our hundred fifty
plus repositories when you merge
something to master
goes ahead and hits this thing that we
call awake of course which starts
building the images for us and then we
can scale with a slider and do replaces
and all this stuff so we built with very
minimal investment and bunch of bash
scripts and a crappy little Ruby Sinatra
app we built our own immutable
infrastructure system so getting into
some of the details of what we created
we have this multi-tiered architecture
and probably this isn't going to be very
obvious especially where it says English
and German but I'll explain what this
means basically every Wunderlist client
connects to a WebSocket service which is
the asynchronous thing I was telling you
about and then we have a proxy because
we have all these you know hundreds of
services behind a thing you need to have
at least one place where the clients can
talk to so we have a proxy that we built
which is really like a modified nginx
and then we have two layers of rest
services and this I will talk a little
bit about because I think it's an
interesting and useful part of this
story and then we write everything
through the same right now reusable
software library that's a right layer
and then for everything we push every
change that happens in the system into a
message queue and we actually push it
into two message queues just in case
that's our poor man's hacker way to deal
with rabbit not working for example but
it actually has been working and then
Wunderlist I mentioned is a real-time
synchronization service and if you
haven't tried it it's free so you can
try it I think you will be impressed if
the internet works like you you have
your iPhone you have your Mac your
Windows client whatever you you share
something with someone else that
immediately pops up like we want it to
feel like a remote control so although
it's all rest driven and there's all
this stuff going on with asynchronous
stuff in the background it happens
pretty much instantly and that happens
because we have a standardized message
queue for processing every change that
happens in the system so we don't have
to build that over and over and over
again there's just one convention and
this is what the network diagram looks
like
so I mentioned we have this real time
layer that's
thing we built in Scala with actors so
every every client that connects
actually gets a WebSocket which has a
running actor as an actor model and akka
which represents that clients connection
which can then subscribe to the message
queue and then we did something sort of
interesting here and this is the
synchronous versus asynchronous thing
where hopefully I redeemed myself a
little bit although from the proxy all
the way down through all the rest
services to the databases obviously it's
just synchronous HTTP the way that the
clients make their requests is they wrap
what would normally be an HTTP request
in a very small Jason envelope and then
they just spew them into the socket
server as fast as they can and each one
has a request ID which is generated
which they can then pair up with a
response in the way back and then the
socket server because it's akka scala is
running in massive parallel on the
parallel in the background hitting all
of these synchronous services
asynchronously and then streaming the
results back to the clients over the
WebSocket so though the chain and the
things you have to think about are
completely synchronous from where it
says smart proxy all the way down which
i think is great because for me it's
very hard to think about things
asynchronously we localize where we have
to think about the asynchronous part
just that one bit that's really just
doing joins essentially and then so I
mentioned we have an English layer and a
German layer and this is important
enough to mention just because it tells
you a bit about the benefits of an
architecture like this the English layer
as we call it is the stuff right under
the smart proxy and smart is in quotes I
won't talk about why that is so the
reason it's called the English layer is
all the services there are named for the
English name of the thing that you would
expect it to be managing so in our
system we have lists we have tasks users
memberships those sorts of things
that's the name of those services none
of those have any connection to a
database they handle authorization some
business logic etc and then they also
handle talking to the German layer you
know the German layer it's the German
name
so off grabbin for tasks listing for
lists einstellung in for settings and
now I'm just showing off that I've
learned some German those things are
actually sitting on top of the data
sources directly they are abstracting
away the data source which allows us to
have my sequel Postgres dynamodb Redis
etc etc as data data sources underneath
these things without any anyone else
caring and they have superuser access
essentially so they're like little
they're basically like connecting to the
database itself but having a rest
interface instead of a data direct data
database interface whatever that would
be and of course the German one is the
superuser one because Germans in our
company can do anything and the rest of
us are stuck with the English thing and
then as I said any write to the system
WRI te right goes through a common write
layer which make sure and that's a
software layer and so all the writes
actually still happen in Ruby which is
an unfortunate consequence of code
sharing versus service sharing they go
through a write layer which ensures that
they get written to the database archive
to another place the mutations get
sprayed out to this common system that
routes them all to the right places and
that's what drives our real-time
integrations all sorts of other things
so we'll we'll hear a little bit more
about that in a minute so I wrote rails
recipes as I know mentioned and I did
rails for a long time and I'm not going
to apologize for it but one of the
things that that I think rails did for
our industry it certainly did invent it
but popularized at that time the notion
of convention over configuration and
that's one of the like a huge step
forward in the mid-2000s for every
everyone doing web development at least
so this is a thing that has been
ingrained in me on the day that we
decided that we are going to start this
rewrite the one of the other developers
and I went to my house and type
something like this so we just decided
all the services are going to be the
same we put the service names in an
array
we literally typed at the bash prompt
this command that had a shared template
that just generated all the things
exactly the same so it was stupid wrote
copy and paste basically but what it
allowed us to do is though we have a
hundred fifty ish services we don't have
the complexity of 150 ish services we
have really just one way we talk to
things and they have different names so
it's quite easy to deal with and one of
the other one of the only places that we
have left where we do code sharing is we
built a library in Ruby at first because
we prototype to everything in Ruby
called API client which is a very
generic name which is just a stupid way
of wrapping conventionally how you talk
to rest services which gives us a bunch
of benefits of like performance
monitoring and logging and stuff that
you can bake in and every time we add a
new language which I said we I think we
have like 15 back-end different
languages now each one of them now has
an implementation assisting if it's
necessary like we can build it once for
the JVM so we don't have to which means
that when you want to talk to a service
you don't need to think about how you
find it and what you call things and how
the URLs look it's all done for you and
that's part of the magic that makes this
thing work and it's just stupid simple
stuff simple of course we had to build
some sort of service resolution thing
and speaking of stupid we initially
started with a yeah mole file just a
text file that we would check into a
repository and just copy it to every
system as we deployed that was sort of a
pain in the ass so we changed to Jason
eventually we eventually auto-generated
the Jason now we're in console which
actually just Auto generates Jason on
the server for us but you'll need
something like this if you go to one of
these micro service architecture
thingies and then here's one of the
benefits so at the beginning of this I
was talking about showing you the red
lines and the database on fire this is
really where we were going is thinking
back to the days of the monolith one of
the biggest problems with the monolith
is when there's something wrong with it
it's really hard to figure out what's
wrong unless you're like a real
expert and it's some language and
runtime where you have tools to get into
the details of it and see what's
happening whereas Ruby PHP etc are not
good examples of that in our system it's
very easy it's very easy to see where
hot spots are where trouble is because
everything is so separate as long as we
measure everything so going back many
years if you haven't read this this is
Etsy's post on measuring everything they
measure like how much coffee is in the
coffee makers at the Etsy office and it
shows up on graphs so that they can get
alerts if it's too low
when I was at LivingSocial we were
measuring like business metrics and we
have a system for measuring aberrations
so you could throw any kind of business
metric into this system and we could
tell if it was weird so even if there
wasn't an obvious problem with the
system you would get alerts when things
were weird and we've taken that idea to
heart here and as just one example this
is a screenshot from liberado and this
is just what la parada looks like so I
know you shouldn't have black slides
with colors on them but I sort of tried
to enhance it one example because we're
going through AWS and everything's
automatically fed into liberado and we
have a hundred fifty ish services when
there is a performance problem there is
literally one dashboard that we didn't
even have to make that shows up in
liberado
where you could mouse over the services
and you see every single load balancer
and what the performance characteristics
of the requests going through that load
balancer are and you just mouse over and
if you're having any kind of issue you
mouse over it and you can see like oh
it's tasks but Alf Govan isn't messed up
and off GABA means tasks in German so
what does that tell us there's something
wrong at the tasks level that is not off
cabin service and therefore probably not
the tasks database that off goblin is
wrapping but rather it's something up
there maybe we're just over capacity
there and we could go to a wake and
slide it up and usually that actually
solves the problem and we've of course
automated that stuff at this point also
now that we have all these things we
have all these separate services
normally I would have a slide
that says tests are testing as a design
smell and I thought about putting it in
today but I want to dwell on it too much
but we favor monitoring over testing so
monitoring is very very much important
and an ingrained part of our our process
one of the rules for any service we
create is when you create the service
you have to actually put all the metrics
in generate the dashboard and put it in
the readme before you can launch it so
when you go to the code and github for
the service you can link directly to the
dashboard that has all the important
stuff that the service is supposed to
wrap that allows us to do canary in a
coalmine style deployments which means
we can roll out just a few of a
different version of a service and watch
all of our metrics and see if there's a
problem so this is a case where
obviously performance was negatively
impacted by a deployment and in fact I
think this was I'm cheating here this is
a screenshot of one of my first days in
the office when I was crashing the
system this is what it looked like so it
was ok and then I did something and it
was terrible four hours after that
doesn't it make you want to hire me the
way I talked about this so how we
migrated we were in this big massive
thing and of course we were gonna kill
the big massive thing but we had the the
luxury to take a little bit of time to
do it so the first thing we did is we
started removing all the joins from the
database because we had a massive
database and of course we thought that
we could do these big queries and those
would be more efficient than doing
separate queries of course that was
wrong we removed all the joins we
separated the databases out so that we
would have smaller problems to deal with
but we did it all in the old system and
I think this is an important part of any
rewrite that you ever want to do so if
you have to do a rewrite you need to
spend a lot of time with the old system
making it better even though the whole
point is to get away from it like
there's no clean way to get away from it
you have to spend time in the old system
so we separated the databases we started
prototyping new features with ideas of
the new approach that we had of this
whole thing of these mutations flowing
through a message queue and going
real time out to the clients and we
built really ugly stupid not very good
versions of all that and then we replace
the database connections on our old
system with calls to our new api's so
this allowed us to actually run some of
the new api's when performance was a
huge concern for us in production for
months as we built the whole system so
we had a really strong level of
confidence before we deployed which was
then additionally bolstered by the
creation of this project that we call
buuza gorkon which is sort of a play on
the cucumber testing framework but it
means evil cucumbers and you can read
about this of course you're not going to
memorize this but the slides will be up
but if you google for saving our bacon
with evil cucumbers it's probably the
only hit that's gonna be obviously about
software and you can see what we built
this is again an actor based thing in
Scala and akka where we simulate crazy
users and what we did is we took our
Android sync library which is in Java
wrapped it in Scala built a thing that
we could we could deploy with our same
immutable infrastructure service so we
could deploy effectively millions of
crazy users that were actually talking
to each other and sharing lists with
each other and sharing tasks and you can
see like it would say roughly half the
time create a list and then pretty often
create a task they would share with each
other and then there would be points
where they would stop and check to make
sure that they were in sync with
themselves and that list that were
shared were in sync with each other in
doing this we found bugs in Amazon's
underlying and just infrastructure in
rabbitmq we found all sorts of things
and really right up until the launch of
wunderlist 3 I was horrified that this
was the end of my career because we kept
crashing things so so badly but it turns
out it was just because we are doing
really crazy stuff so finally we do the
launch and again our people are super at
marketing and the launch was boring
there was actually there were two
moments of panic on the on the launch
day
one was it felt like nothing was
happening and we thought well does no
one care about us anymore
but we looked at the metrics and yes
they did that brought us to a new level
the next was I saw a big spike on the
graphs and I went looked and it was our
one Haskell service it spiked from two
to five milliseconds or something like
that of response time so that turned out
to be okay in our world embarassingly it
hovers at like 10 now but Haskell
Haskell Haskell Haskell ok so I told
them I would never rewrite the system
again and of course it wasn't exactly
true it just depends on how you define
system so this was August of last year
that we launched wunderlist 3 a lot of
the system was in Ruby at that point and
what we've done since then is replaced I
think the number is now 80% of the back
end with new code in different languages
so we now have where Ruby used to be we
have Scala closure rust go more haskell
elixir etc node in the backend and it
wasn't just for fun it was partially for
fun and that's ok it wasn't just for fun
we also have gone from like 30 servers
for a specific service to 3 servers for
the same service and those 3 only
because we want to run in 3 availability
zones but really you could go to one or
you could if you wanted to you could
pile them on to each other so it's
amazing the money that we've saved in
the process and the ability to scale by
rewriting and by regenerating and
throwing away these cells the system has
not had downtime that had anything to do
with these replacements these upgrades
these rewrites so I feel pretty proud
about that so where do we go in the
future well we didn't really make this
thing happen this homeostasis
self-regulating system we're still
working on that we're you know like the
the system is a network of things that
depend on each other in different ways
and it has output signals you care about
some of which might be performance some
of them might be errors who knows there
are different things and you figure out
how to let the system evolve itself
internally and like this this service is
hurting
service so kill it that kind of thing so
we're working on this we also had built
a global asynchronous validation
middleware so that every single JSON
request in and out of the system or
Jason packet in and out of the system
would be valid it validated with JSON
schema I think this is actually a really
important thing because some of the bugs
that we have found that are probably
inherent in this sort of micro-services
crazy heterogeneous architecture are
because of just validating data so we're
working on this a big one is you can
imagine to run it to just a to-do list
with a hundred fifty plus services and
multiple availability zones it's pretty
expensive this was ok though like every
investor every every person involved in
this project said do not worry about how
much this cost just make sure it's up
make sure it works so we did this and
the goal was make it work then make it
fast then make it cheap and then
probably somewhere in there make it
pretty but you know certainly those
three things so we're on make it cheap
right now and we found sort of a silver
bullet for that so are any of you using
AWS some are this is hans hassel berg
he's on my team he wrote this post
recently that you could also look up
hans has figured out that we can run
every single thing in our infrastructure
on AWS spot instances in a spot instance
is what its amazon has this marketplace
that's kind of like the stock market
where you can bid on prices for servers
of a certain type and when they're
available for that price they will be
booted however many you want it and when
they're no longer available you'll get a
message that's a warning and then
they'll just shut them down so it
probably seems to some of you like a
crazy idea to run your production
infrastructure on this but this is what
we're gonna do and we've actually got
like half of our servers now are on
these spot instances and we're goal in
the next month or so is that everything
is spot instances which means it could
be killed at any moment it could be
killed actually one minute from any
moment or two minutes from any
which I find to be exhilarating because
I'm the guy that crashes the system for
fun
but no because if we want to build a
system where it's all about the system
surviving but the cells not like
necessarily not surviving then running
it on an infrastructure that's killing
it for you that's really exciting and
the savings are literally like 85 and 90
percent per instance it's insane
I think like I think Pinterest is doing
some stuff like this when we do it and
some others get wind of it they're gonna
have to change the model because it's
not gonna work for them anymore I'm
pretty sure but get it while it's hot
doctor we're gonna do just because it's
slow to make Amazon instances that's not
very exciting I saw this post from Kraft
come from a craft calm presentation a
couple of weeks ago Francesco sat cerini
who's an Erlang person of course as we
get more and more sophisticated micro
service implementations each one grows
their own crappy version of early and I
thought yeah we built a crappy version
of Erlang that's awesome but it made me
think like do you need micro services to
do this like yeah what if I take all
these same ideas and apply them to the
clients cuz you may have noticed I've
only been talking about our back-end how
do I do that in the clients how do I do
it in an iOS app I don't know exactly
yeah that's my next focus and then of
course the next step is we were a factor
all this into a monolith I'm not really
joking about that I think there's
something to be said for that because
micro services are more expensive this
SOA thing is more expensive and a
variety of ways and I think this micro
service architecture thing is just an
example of service architecture done
well at least I think ours has done well
and people call our as a micro service
architecture so it's service
architecture done well and you know back
to the idea that like Erlang apps that
are in one process so to speak could
also benefit from
the same things that microservices can
then maybe service architecture done
well is just architecture done well so I
don't know maybe next year I'll be
talking about refactoring to monoliths
and you know the big micro service
disaster of 2015 but we'll see anyway I
think we have no time for questions am i
right I'm right because we have to go
relax but it has been a pleasure talking
to you
Thank You go-to for organizing it's been
a fabulous conference and I am I know
what humbled means and I am actually
humbled to be here speaking to you so
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>