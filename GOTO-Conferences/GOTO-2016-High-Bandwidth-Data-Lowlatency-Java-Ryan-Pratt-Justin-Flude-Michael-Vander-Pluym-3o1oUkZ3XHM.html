<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • High Bandwidth Data &amp; Low-latency Java • Ryan Pratt, Justin Flude &amp; Michael Vander Pluym | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • High Bandwidth Data &amp; Low-latency Java • Ryan Pratt, Justin Flude &amp; Michael Vander Pluym - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • High Bandwidth Data &amp; Low-latency Java • Ryan Pratt, Justin Flude &amp; Michael Vander Pluym</b></h2><h5 class="post__date">2016-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3o1oUkZ3XHM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let me start by saying thank you do a
little introduction my name is Ryan
Pratt I've got with me Justin flute and
Mike van der bloom we're all from
company called peak6 investments I'm the
CTO of that firm and they're both senior
developers within the capital management
business which is part of the
proprietary trading business that's part
of that business we are here today to
give a presentation in particular around
a system that we built over the last
several years that deals with highly our
high bandwidth data and low latency
systems some of those lately latency
systems we've written in in Java some
we've written in C we're going to go
through the various areas where we
picked and why we picked those things
and as you can see we we have sort of a
parenthetical renaming of our talk which
is drinking from the firehose without
losing a drop which is maybe a better
explanation of what the problem is that
we're going to walk through let me set
the stage to begin with when we first
started dealing with the or finding out
about the problem that we ended up
building all these systems around it was
around late 2013 and we realized we had
spent the last several years building up
a new framework for how our overall
trading system worked we made a lot of
use of Java and along the way one of the
things we built was a new feed
dissemination system dissemination being
so this market data that we are
listening to and using as part of our
business and we have a bunch of systems
that need to get at that data and do
various things with it and we wrote some
stuff that would parse it and hand it
out to all the systems that needed to
use it we had not correctly anticipated
some of the changes that were going to
happen with the markets though and in
particular one of the things we had
decided from a design perspective was to
disseminate the the data using TCP
which had its benefits we needed
reliable delivery of this stuff and we
wanted to try to keep things as simple
as possible and we had in previous
iterations of building things used
multicast and tried to use various
things for reliable multicast and it had
run into a bunch of problems so we got
very clever and we made something that
conflated and compressed and did all
this fancy stuff and we thought that we
would be able to manage the amount of
data on our network by while still being
able to use TCP well 2013 came along and
things really started changing their
they already been a little bit on a on a
move in the previous years but they
started sort of ramping up in 2013 and
the big thing that was happening were
the options markets were fragmenting and
it really wasn't just the options market
but for peak6 that's or primarily we
focus and by fragmenting what I mean is
there was a lot of new exchanges coming
online when you go back to 2008 there
were seven ish exchanges and all the one
came out on that year and by today there
are 14 going on 16 by the end of this
year options exchanges and what matters
around that is that every time you have
a new exchange come online the only
thing you're sure of is that the amount
of data you're going to handle is going
to go up you know roughly you could say
double I mean it's a little more
complicated than that and and it doesn't
mean that there's necessarily more
opportunity from a trading volume
perspective because really all your
knowing is there is another place where
there are orders in quotes for the for
the instruments for which you might be
trying to do trading activity on but
while there can be more quotes for those
things it's not necessarily going to
cause any more actual trading activity
and so every time you bring a new
exchange online the one guarantee you
have is that there's more complexity in
the systems you have to build and
there's going to be a lot more data well
for 4 from 2008 to now to have it go
from 7 exchanges to close on 16
exchanges the amount of data we had to
handle when crazy to give you a view on
options markets options markets are just
quickly going over it at us equity
options our derivative products where
the the asset they're a derivative of is
winning so you would be familiar with
this as stocks public public stocks that
trade on the stock exchanges but for
everyone equity that might be trading
and quoting on one of the stock
exchanges there can be up to thought you
know several thousand individual
instruments that are derivative products
against that equity and so while there's
a lot of data within the acuity markets
the amount of data that's just that's
pumped out from a coding perspective
from the options markets is orders of
magnitude larger to the up to the point
i mean it's within the gigabits per
second of data that these all the
exchanges pump out and if you're
building systems that need to do with it
you have to somehow be able to keep up
with it which is where we say well i say
it's like drinking from the firehose to
give you a sense in 20 2013 the
beginning of the year we were underway
with building a bunch of stuff for our
for our business and when we had things
more or less working pretty well and by
the end of the year while we had
successfully produced a bunch of a bunch
of new software that was both
architectural infrastructure as well as
new actual strategies for the business
we also found ourselves a little bit in
a crisis mode where things that had been
working we're no longer working I've got
I've got a couple graphics that you know
as they say pictures are worth a
thousand words that kind of talk to it
this is maybe how we felt at the
beginning of 2013 and and frankly we
were maybe not quite so sure that we
were getting that full but we were
handling it we had a big amount of data
coming out of the options markets but
our systems were able to take it and
were able to more or less manage it and
my systems I mean both our software and
our hardware we are you know turn a long
feeling really good about things
building news and do strategies and
mid-year all of a sudden it started to
go like this where it was a lot more
than any of our systems both hardware
and software really able to fully
managed especially within the spikes and
we started seeing our 20 gig capacity
Network start dropping packets you know
as especially at the open and at other
peaks within the market day where the we
were trying to push more than 20 kings
of data through our routers and they
were just dropping packets left and
right and by the end of the year we
ended up in a situation that
this is short little video is probably
best representative Oh Oh Joel Miller
you just found a marble in the oatmeal
you're a lucky lucky lucky little boy
who you know why you get to drink from
the fire
Oh
so yeah it was it was a little rough in
the stuff that we were writing that had
that was basically then it's fairly
representative of how things were
starting to go it was the core of our
business it was our ability to succeed
and manage both capturing Opera finding
and capturing opportunity on the markets
but also manages the risk we have it
play required that we really be able to
keep up with this fire this fire hose of
data otherwise you just don't know
what's going on you don't know what risk
you're at so we had to come up with a
way to solve the problem let me take a
step back quickly talk about what peak6
is so that's the trading floor this is
our office we have a bunch of software
that we've written and a bunch of
various systems we put in place that are
our way of creating this really the
backing infrastructure for all of the
traders on the floor here to be able to
implement their strategies and manage
the risk around them across the US
equity options markets peak6 is a what
would be called a proprietary trading
firm and just to like what does that
mean I know a lot of people have heard
this and don't always know what that
means it really just means peak6 is a
private it's a company that takes
the capital that is from within the
company so it's no outside customers or
anything like that take that capital use
our own strat propriety Terry strategies
and proprietary software to try to trade
on the market and hopefully make that
capital grow and not shrink doing that
year after year is a relatively
difficult thing there's a lot of
competition in the market and the
technical challenges especially when you
try to do it at scale are very are very
broad and very difficult and in
particular peak6 our strategy is about
breath it's about being across the
market being able to trade and be in
most of the things that are out there as
far as liquid equity options which means
we actually care about not a niche of
that fire hose of data we care about all
of it to further kind of make our
problem hard in the years leading up the
few years leading up to twenty thirteen
when we started running into the
sort of overcapacity issue with our
networks we had riorca tected all of our
systems so that most of our stuff was a
distributed service-oriented and a lot
of it written in Java because Java was a
frankly an easy great ecosystem to do a
lot of this in that said as we started
falling over and getting behind on data
it was exacerbating what had been a
small problem at the beginning there
around areas where we needed low latency
to be in place and in particular just
some stability to be in place and as
things got much much worse than what we
had planned on we started hitting big
gc's we built a bunch of stuff in a
garbage collected language that has a GC
and and we had built stuff in a way
where we thought we were managing the
rate of garbage creation that was going
to work and then we found out we were
way off on what we were going to need to
be able to handle and the and and the
problem with that is is that we had
spent years building this and now we had
a had to figure out how to both fix our
infrastructure from a feed management
perspective but not have to go redo
everything it was important that Java
still work for us and then to kind of
throw in the final a final part of the
problem was at that same moment we had a
bunch of new opportunities showing up in
the marketplace that for which the
solution was basically creating a bunch
of more downstream client systems and by
downstream I mean you've got your fire
hose of data coming in from the market
and you've got some internal management
a way of disseminating which at the time
was TCP and we needed to more or less
you could say double the number of
systems that we're going to be listening
to that streams of data so if we were
only already knocking our routers over
with listening to the data on you know
however many systems we had going to
twice that was going to definitely not
work so like I say we knew we had a
problem we had to fix it and we really
knew just three things about it we
didn't exactly know how we were going to
do everything but we did know there are
three things that were at play one tcp
had to go we had to somehow figure out
how do we go back to using multicast in
a reliable way without hitting all the
problems we had before because TCP was
not going to scale for us we were
killing things killing our systems
killing our network with this and so
that's for sure all we knew number two
we knew that Java had to be a part of
our platform we had to figure out how do
we make it so that we can stay within
this ecosystem scale it out manage the
various different types of needs across
all the systems we had both where there
was true very low latency and if you're
in Peters talked before less you low
latency means a bunch of things but in
our scenario there were some systems
that we needed to be sub milian a very
small number consistently sub milian
there was a few more that we need
consistently to be in single digit
millisecond management on and again
consistent so every time no matter what
all day long we can count on end and
this thing from there to there was going
to be one you know single digit Millie's
and then we had a ton of stuff where it
was higher than that variable you know
100 milliseconds who cares you know like
we're not totally who cares i mean if
you get into half a second or more we
definitely care but a lot less stringent
but we had all this stuff in this
breadth of things going down to even sub
milli and we had to make it work
otherwise we were looking at a
multi-year project and not something
that we would be able to actually make
the business thrive let alone tackle the
new opportunities that were coming our
way the third thing we knew is we had no
idea we had learned our lesson we didn't
know what was going to happen scale was
going to be just part of the game it
could be 2 x 10 x hundred x who knows
and that meant from the data coming in
upstream and it meant potentially in the
number of systems we put in play
downstream and so we had to make sure
that the architecture we went with would
actually work no matter where that led
so we ended up building this thing that
internally we called feed store caster
and i'm going to initially just jump to
the end that's the picture there's no
way we go through all of that all of
this in this talk there's a bunch of
components to it we're going to for the
rest of this talk just pull out the few
pieces of this that i think are the most
interesting certainly the thing there is
where we learned the most from and where
there was potentially some of the most
fun innovate
things that we got we got to play around
with high levels of what's going on here
has got a bunch of data coming in on you
see there's that little line in the
middle you can consider everything on
this side of it to be the server side
the side of the things where you're
managing the raw feeds from exchanges
and everything on the other side is
effectively systems that are downstream
so you have the stuff you know multiple
servers partitioning the world of what's
coming in from the exchanges something
called feed stir that we did write in
C++ that is parsing that raw feed and
then basically writing it down into a
shared memory file then we had another
thing that's sitting on that same server
as the thing parsing the raw feeds that
is written C and it is taking the data
out of the shared memory file and just
broadcasting it on the network on the
other side of the line there you've got
all your client systems there's another
little guy rent and see that's listening
to that broadcasted data set basically
doing the inverse of what the center is
doing taking it throw it into a shared
memory then you have however many apps
you might be running on one of those
pieces of hardware that's on the client
side that's accessing this data the
internalized market data out of that
shared memory store that's pretty much
it that's all there is to the overall
sort of high-level view of what we what
we did now the real trick was how do you
make it so that that transport mechanism
from the server side where it's taking
out a shared memory brought it
broadcasting it out and then getting
down on the client side how do you make
it so that it's reliable because again
we're given up on TCP and so we're and
so we're the the network protocol itself
is not going to guarantee us that we see
things in order let alone see them at
all but we need it we have to some we
have to make sure that the client side
of things actually does see everything
and so we had to come up with a way to
solve the problem I'm going to have just
enjoying the conversation here because
Justin pretty much developed most of
this stuff there was he worked with a
small group of other people all over the
time we did this in particular another
guy named David where they initially
came up with the solution of how do you
create this reliable multicast thing and
I've got a you'll notice there's a bunch
of names
various things on the screen and and
what those are is a bunch of various
third-party open-source pay for some are
just protocol implementations of getting
data from one side of the other of a
network where you are attempting to get
reliability in place and multi with
multi cast as the underlying delivery
mechanism so Justin walk us through if
why didn't we use one of those
third-party thank you Ron so we did try
these these ones out and they kind of
their general purpose libraries that
solve the problem of if you want a
multicast data across the network you
have given up the reliability mechanisms
that TCP gives you for free so the way
these systems work is that they
essentially keep a copy of what has been
sent over the network and when somebody
listening for that data indicates
through what's called a negative
acknowledgement a knack that they didn't
receive that data the data is
retransmitted to over the network to
give the give the client another chance
to to read it so so we tried these these
systems out because they were kind of a
little bit like a drop-in replacement
for just our networking code they didn't
require us to change an awful lot of the
actual kind of application to use them
and unfortunate the results were not
really very satisfactory and one of one
of the main issue that we had is that
once the kind of bandwidth of the
incoming exchange feeds increased and
the network started to drop packets the
network we quickly become overloaded by
the negative acknowledgement by the naxx
themselves so in other words the
bandwidth would spike and this would
cause some packets to be dropped
whereupon the clients would request that
please send me those packets that we
dropped they would then get sent on the
network but meanwhile the the incoming
exchange feet still needs to be
delivered over the same network so you
tend to just magnify exponentially
really the amount of bandwidth you you
require just to sustain the same kind of
data rates effectively I mean what you
may have heard this if you use anything
like this is just a knack storm you know
that tons of data coming in things start
dropping all your clients start saying
hey I need
this I didn't get this and they're
sending all these snacks back up streams
resending more data as it's sending the
other data coming in the door and it
just becomes his vicious cycle right so
that didn't work for us that didn't work
for us and we didn't feel really that
although the the general-purpose
libraries solved the reliability problem
in the general case we didn't really
feel that they were taking advantage of
what was special about the data we were
sending over the network which was that
its market data and with market data
you're really just interested in the
most recent message or the most recent
values for for something you're not
really interested if you've dropped some
packets in reconstructing what what was
the case a few seconds ago or even a few
milliseconds ago you really just want to
know what was the very latest piece of
data you've received for Apple stock or
the price of a Google option right so so
the one one thing that is different is
is the these solutions that you can use
our very general purpose there are I'm
going to try to give you a reliable
delivery of packets I don't care what is
in the packet I'm just going to try to
say overall mechanism if you drop back
and ask for it and i'll give you that
packet if i still have it which by the
way the the upstream doesn't can't hold
infinite number of back backlog of
packets so that some point some client
who's slow enough might say hey i missed
this stuff and the answer is well too
bad we don't have it no reliability for
you and so so one thing that I'm going
to flip to the next slide on which was
really this aha moment I would say that
that Justin and and the team working on
it came to was we don't have to solve
world hunger we don't have to solve the
general purpose solution we can solve
something that's just for the problem at
hand right so I mean on the screen you
see a kind of a typical example of
what's what's going on when there's a
lot of incoming market data and we've
got three Apple quotes kind of on the
wire one one behind the other probably
just you know a few dozen micro seconds
apart and the one that's in blue is the
most recent one and that's really the
only one that we
we care about the we can see on the
right of the screen there's our kind of
key value store that we're retaining the
the the quotes the latest quotes in in
the in the client application and the
leading to Apple quotes in my really we
it would be would have been better if
they'd never been sent there they're
really superseded entirely by the blue
apple quote because by the time the
client application looks and says what's
the current quote for for Apple it
really wants the latest one it wants
that the blue one and the two that
preceded it are redundant so what did
this change like how did you make use of
this and what you did so what that meant
was for us that we would we kind of
understood this is a peculiar aspect
this is a peculiar characteristic of the
data we're sending is there some way
that we can take advantage of this
peculiarity and come up with a better
protocol for sending the data and more
importantly deciding when not to send
the data and the kind of that the
strange kind of thing is about market
data is that when the market is moving
very very quickly and it's producing a
lot of kind of quotes for a lot of
different securities that's really the
time you care about retransmissions the
least because if the market is sending
your whole stream of quotes more and
more quickly the chances are that by the
time you realize that you missed the
quote just a few milliseconds go on my
few microseconds ago there's going to be
another quote that would have superseded
it coming along anyway so you really
don't care about the retransmission at
that point it's really when things are
moving very very slowly that you might
care about the retransmission because
you then you've you've missed an event
which is going to be long-lasting in its
significance and so that's that's the
time you really care about the replay
but not when things are really really
maxing out so how did it would how do
you take advantage of that how does it
actually work where that makes it so
that you don't have to retransmit
everything so the way the system works
is that the the publisher of of the
quotes in this case associates with each
change in a quote that gets sent out a
version number and when the client
application realizes that it's missed a
packet it requests essentially through
teeth
p a reliable channel it requests to be
recent the packets that were missed
which contain version 27 version 28 off
said the avi Apple quotes and then the
publisher can look at the version number
and meanwhile the publisher is still
sending out more recent apple quotes it
can look at the request from a client as
saying i miss something and actually
looking at the version numbers say well
I you've requested version 27 of the
Apple quote but I've just multicast out
version 29 so I'm not going to send you
27 i'm just going to quietly ignore your
request because I know you've just going
to receive version 29 if you don't
receive version 29 then you'll
re-request 29 and we'll start again
considering what we need to do but
essentially the the publisher realizes
that it can avoid honoring retransmit
requests because it knows it's already
multicast the data that's superseded
that right up request so so I mean the
depuy the peculiarity of the data is
really that it has a key to it and if
you conversion the data that you have on
a key your back channel is basically
able to know I already said I've said
more recent one you know i'm not i'm not
going to send this to you that's right
and and the and the beauty of the
protocol is that the more you drop
packets because the network is multi
casting more and more data the less
retransmissions you actually need to
recover from that drop and so there's a
kind of built-in dampening effect in in
the protocol when you unlike with the
general-purpose libraries the more you
dropped the harder it was to recover
with this the more you drop the easier
it is to recover right and again it's
because of the nature of the data we
knew the data and how it behaved in more
to em we were able to sort of at least
in theory and this is you know as we
built it it was all like theoretically
this should be true that we were able to
take advantage of the fact that when the
market goes crazy that's when you get a
lot of data and it goes crazy for a
little while and that's when you're
likely to get most things multiple
quotes you know many sometimes thousands
hundreds of thousands and within a
millisecond of the same thing over and
over again and so we were able to say
using this protocol knowing that when it
gets bad we're gonna we're going to back
off and we're going to
make it so that we are less likely to
actually handle any more traffic on the
network than what was already coming in
from the upstream potentially even less
whereas in the more general purpose
solutions you get these nak storms and
it actually ends up being more and more
and more and you just kind of can't keep
up so that was really for us this aha
moment and I don't I mean for years
we've been we've worked with this data
and built systems along this and and
they said after we did it we built it
we've both in testing as well as in
reality it's been out there for years
now this is played out to be quite true
when when the market goes very goes
bonkers on us this thing just hums along
and it comes along not like suddenly
magically have not any bandwidth and use
are on your network but it's not much
higher than what would have what the
actual upstream that is coming from the
exchanges is causing so if you're
getting 5 gigabits per second sustained
or 10 gigabits per second sustained
coming in from the upstream that's more
or less what you're going to see within
your internal network and even if some
things are dropping stuff the likelihood
and then they're coming back and say hey
I dropped this almost invariably it
doesn't matter because they already have
another one that they didn't drop that
that means that they don't no longer
need to to get it and and for us I mean
again it seems obvious now but it took
us years to get to this generally simple
solution that is now working really well
in scales pretty well but so so we get
this protocol in place and it's great
excellent we're able to actually get a
bunch of data from the upstream stuff
that's handling all the exchange data
and push it out to all these machines
and it's on multicast so we can scale
those downstream and it's not going to
cause generally going to cause further
issues with copying data and overloading
our routers and whatnot but we still
have this big system written where we
had a bunch of stuff in Java we had
frankly a lot of stuff in Python we
started using more and more go which by
the way we used as part of building out
components of the system as well and and
we need to solve those things to be able
to interact with this data that we were
now storing in shared memory oh and also
we wanted our stuff to be able to
continue to optimize the hardware we had
in place so and by that that would mean
that that we wouldn't have to end up
having if you've got five apps running
on a piece of hardware because the apps
themselves are not doing a ton they want
each need their own version of getting
this data sent to them and so that was a
problem that we had to solve and we had
to do in a way that would son you know
somehow make it relatively easy for all
these apps to integrate with this so you
know Justin I'll let you kind of talk to
what do we do and how do we solve it so
so having kind of solved the redundant
copying problem on the network we didn't
really want to reproduce a bunch of
redundant copies on the client machines
where we've got multiple applications in
different languages as you say trying to
access this data so the obvious solution
is to vote how can we avoid redundant
copying which would be the case for
instance if these guys were opening a
socket or a connection to some service
on the machine and requesting data so we
avoid that problem by using shared
memory so we store one copy of the
incoming data in a file which we memory
map and that makes the data available to
every process that requires it without
having to reproduce multiple copies of
it the other advantage of that is that
providing a data in memory and address
it's more or less easy in any language
that we use to access that data it there
are some languages where it's trivially
easy to do that there's some languages
where you perhaps need to wrap that
access in a library because they don't
expose raw memory to in their
programming model so does that mean that
then each of these languages you have to
write your own custom version of going
and looking at that memory so we thought
about how we can provide a uniform one
single way of making it available and
there's there's kind of an obvious
universal solution to that which is to
provide a C library that accesses the
shared memory and the C library is the
beauty of a C library is that pretty
much every language on the planet allows
access to the facilities provided by a c
library in one form or another you can
import a see library into Python pretty
easily obviously very easily into C++
very easily into go and and quite easily
into into Java it may be a little bit
more work in this language or that but
it's all very doable it's it's a
well-understood programming model and
the C library also has the advantage
that it can hide the messy details of
opening a memory map file and setting up
that kind of light mapping and make it a
kind of a much more comfortable
interface to accessing this data for
four people within these different
languages right so there you go we found
a use for see which by the if you
haven't caught on it is it is Justin's
preferred language and it's found its
fun a good home in at peak six right now
so great so we have a/c library we can
open up this stuff we can interact with
it through that library you don't have
to redo everything in all these
languages but let's talk about Java so
obviously i mean if we've got people who
are writing Java out there certainly
Hooten if you haven't done much or any
off heap stuff the first question maybe
is oh wait a minute how am I going to
access this shared memory stuff from
Java like that doesn't it's not
something that's naturally offered up to
the API it's it kind of falls out of
that that realm of write once run
everywhere and so that was the first
thing which is what do you do with
memory my files and since Mike did all
of this sort of single-handedly well
then walk through all the problems and
what he had to do sure yeah thnkx right
so yeah I didn't just take a minute like
just like I kind of talked about memory
mapped files a little bit like I mean
really you know memory map tiles you can
think it was just like a block of
virtual memory that you know like you
can map in your process and then
interact with directly you know I can
it's going to be backed by something
else like a like a file or just shared
memory in this instance so you know like
it's it's going to get used often in
places where either you need shared
memory you need to share a memory
between two different processes or you
know like in like places where you just
have a lot of heiio and you want
performing and so like really yeah we
needed to be able to interact more like
with this shared memory we were able to
use justins see library in order to kind
of handle that for us you know like in
like in a way they were we didn't have
to reimplement it there's definite ways
of like doing that you know like in Java
like mapping and the memory but it's not
it's not something that's really
standard you know like across platforms
and stuff like that so it you know like
we were able to take advantage of that
library in order to open up the file and
kind of interrogated investigated but
but but yeah we didn't exactly want to
use that that library to have to read
every single record or every single
field out of the out of the shared
memory and so that in the main reason is
that like so we're using this library
right you know like the C library but
you know like we're really trying to use
J&amp;amp;I or in our case like really JMA to
interact with that that file you know
like to be able to call functions that
know where the records are and
electronic stuff but you know like
really there's there's some some
performance and like overhead that
you're going to get you know like with
going through that library you know like
with using Jay and I are you know like
even worse with Daniel a through J you
know like and so there's really there's
not not really a need you like to do
that you know once you kind of know
where it is like where the memory is
that you really want to start
interacting with then you know if you
don't you don't have to go through the
library in order to do this kind of raw
stuff to be able to read this off heap
memory you know like they were the
records are actually being stored right
so what do you what do you do how do you
access it yeah so the answer is really
using unsafe so if you hadn't heard of
unsafe rodin see it like unsafe is you
know like the this library that you know
like really class that this part you
know like that this part of Java where
it's not it's called them safe for a
reason it's not not really meant be able
to be used or wasn't originally meant to
be used by you know like developers like
us that are just making applications
it's really not to be used internally
you know and so it's unsafe and that you
know like it its API itself can change
and actually is changing you like in
Java 9 and so we're like you're going
have to you know like redo a bunch of
your co if you code too much to do to
the API itself and then secondly like
you it's unsafe because I'm Java like
you know we are like you create objects
you throw them around like you don't you
don't worry about you know like things
like segfaults right but you can you can
usually get one if like if you're going
through and safe and using it directly
so like it'sit's like it you got to be
careful about it but but yeah you know
like using unsafe is a way that you know
like without even using like anything
external Java we're going to be able to
interact with these these shared memory
files in a way that's that's high
performance all right so why knock us to
an example here it should show us
exactly what it is okay so you know like
say that we got this like little class
quote you know like and it's only got a
few firms in it you know like typically
and you know like systems like you you
just kind of create one of these objects
you populated you know like and then
toss it around but you know like the
place that you populate it from would
typically be from some other on keep
memory sort some on keep store so you
know like here in like we just have
these few fields like you know basically
you know like you can you just how to
use didn't ask you know like fields here
for these records like like you know
like your these really just represented
like basic properties you know like of
of market data and we just we just want
to be able to take these objects read
the data from op store and from off heat
and then use them on jeep right so like
here's an example of us using unsafe in
order to to do it so we we kind of come
in here you know like we know what
record ID that we want to read write and
so we're going to do is just compute the
address you know like that we're going
to start reading from right so we we see
that records at aren't there like that's
that's really something that we're only
going to know by you know like by
reading the location out of the accusing
the library right like we have to know
where the record array or being like
records themselves start so we have to
get that out of the stories in library
we know what their record ID is that
we use but then we have to you know come
and compute the address just using the
record size as well that way that also
came from the library so we get an
address this record they're going to
start reading you know like we create
one of these quotes you know we we
create the you know like the divider
rated store that the bytes representing
the apple symbol right we copy that from
a off heap onto the heat and then we
recreate the string object to term you
know like hold those bites and we read
the bid we read the ask and weaker the
exchange right so you know like these
are these are every time that we're
calling unsafe here then it's reading
something that's out of that shared
memory store and then putting it into an
object that's totally I thought up I
mean a big part of this is managing
memory off the heap so that you're not
creating a bunch of GC but your solution
basically is generating a bunch of
garbage right yes right so uh we ya like
like that's a really point so you know
like you know this is just as bad right
like where we're still creating that
cord object we're still creating that by
DeRay right like we don't really want to
do that because you know like we build
up enough objects and we're just going
to get DC if it's going to just
introduce more causes right so like we
want to stay away from that so here you
know like what we can do is just pass in
that quote object we can we can get it
you know like from the guy thats that's
calling it maybe he's reusing one you
know like maybe like a pool of them for
some yeah yeah it could be in a pool you
know like we're even it could just be
one that's that's really a sense down to
the thread that's calling you know like
her if we know that we're in a
single-threaded complex we can take
advantage of that lots of ways where you
can like just you know like kind of keep
one existing object around that you're
going to reuse over and over okay so
we're going to just pass that object in
and then just set the field on that
instead of creating anyone right so
dilson news there though yeah sorry so
we don't like we still got that byte
array we still got that string so you
know like how are we going to solve that
so you know like here we can pass in
that backboard object and we can change
instead of having just a string field
you know inside that object we're going
to have to make it a byte array in this
instance like you know like that's how I
was to just read it into that
pre-existing array okay so then we're
still just we're not creating that extra
garbage here anymore
we're able to take advantage of that you
know like the thing this might look bad
in a sense because like you're thinking
like oh yeah everybody that's calling
this then is going to have to interpret
you know like this is like bytes or
still going to have to create a string
to really know that that's Apple but in
reality like if you kind of think about
it this is really the key of the record
right the guy thats that's calling this
and asking for this record he's asking
for this record for a reason I like he
knows that this is apple and he can
probably if you know like he since he
knows that it's Apple he probably
already has a string around that says
Apple in it that he can then attack on
to the object after we can read it out
of memory so you know like this this
actually you know like its strings are
notoriously hard right now like when
you're trying to conserve your
allocations but am i doing doing stuff
like this will help where you know like
it's okay dude and kind of use it for
keys and in contexts where you know what
the data is because you can then go and
reuse those strings right all right I
mean while somewhat complicated
obviously seem relatively simple so I
mean why not done quickly and we should
have interested a little replace
everything and if I recall right this
took a while yeah this to take a while
yeah so I mean really you know like
there's all these little problems that
it kind of started pop up when you you
might just mean change the system for
one but then really start to use more of
this different approach so like you know
like with these files we have to know we
know like what each record represents
you know like we have to be able to
watch for new records that are added
after you start right you know like we
have to be able to interact with that C
library and we have to know that you
know like okay if the subscriber in our
box you know shuts down and then
downloads a new one that he's gonna he's
going to create a new shared memory
stored there's there's just all these
different problems that just started
popping up that you know like are all
solvable problems but nonetheless they
do even take some worked at this out
okay so that actually seems like it's
relatively complicated to try to achieve
this like how how much of our I mean how
often do you have to go through all this
nonsense just so that you can use Java
and not have it gcing like crazy with
this amount of data yeah I mean not not
as much as you would think like so I
mean really this is the ten percent of
our code that you know like we really
need to kind of
tomatoes randomly in to kind of make
very very very fast right but I mean
like the the rest of our code like you
know like the ninety percent of it is
you know just like it's not suffered
into you know you like avoid allocations
this this intensely and all that good
stuff so you know like you really gotta
you know think about the ninth n roll
and kind of you'll be able to when
needed you know like apply this stuff
okay but no so as you can see there's a
bunch that there really was a bunch of
issues that came up with one some we
anticipated some that literally just
came up along the way there's no way we
can go through all of them but many of
them are pretty interesting week there's
two we're going to pull out that are at
least interesting to look at so first
one would be this one that's like how to
lock without locking which first what
the world does that mean and why do why
was this a problem yeah I mean you know
like so really you know like this isn't
exactly locking right like this is more
just you're like we want to read an
actual like legit value from the store
right so you know like we we not really
we're not really able to lock in the
traditional sense because we don't want
to slow down or kind of create this
contention between different processes
here remember where we're sharing this
memory you know like we're not just
talking about locking inside the jvm or
anything like that we're we're talking
about you know a potentially you know
like by doing any real locking solution
blocking in the process right so there's
this thing that that Justin wrote that's
writing how much data right so so
there's this hundreds of thousands of
records being updated every second and
there's in in each file and there's that
there's a lot of these different files
and there's one unix process which is
writing to the file and then there's a
bunch of different processes application
programs on the machine looking at that
same memory and they need to kind of be
able to read that memory in in a correct
way not read garbage out of it because
they read partial update and yet they
can't really lock the record because
they may be reading something that's
running quite slowly or they may be
doing some long update or something like
that it's gonna block right so like
summer python thing could be written
that you that need to look at this data
that for whatever reason it was written
away where it locks the the writer out
goes and hits a database comes back
time later and then says oh yeah that's
that quote I wanted and and all of a
sudden everyone's falling over so can't
do that so we need to see data correctly
but not lock so this is really about
avoiding reading partial data right
partially written data yeah there's
something right like so I mean you like
we're going out to this example really
quick like so say that we've got this
this Apple quote you know like first we
write out you know like oh it's 106 and
five cents and you know like the ask is
106 at ten cents right then you know
like a new quote comes along and we're
now we're the bid has changed 106 15 and
the ask has changed 106 20 so you know
like when the reader it without really
doing anything if we don't try to
prevent it then you know like we can get
to a situation where this happens where
we read that the bid is 106 and fifteen
cents and they ask is 106 and ten cents
when regale in reality like that never
really happened you know I bed that's
not really a true record so you know
like what we're going to do is this you
know like we're really going to try to
lock just using the version of the data
okay so the so the writer is is you know
every time that he's ready on record
he's effectively incrementing this
version that's attached to its record
but he doesn't just increment okay so he
really does three things he he locks it
meaning that he negates the version of
the record okay he writes a negative
number there you know then he writes the
full record value and then he unlocks it
yeah mux it by negating that number and
then adding one okay so effectively we
incremented it but we had to go through
these three steps to get there the
reader on the other hand he's going to
wait until he sees a positive version
you know like in that record he's going
to read the version number then he's
going to read the full record values and
then read that version again right and
he's going to repeat if the versions are
different so is an example of this like
you know like see that we see that same
first quote ok we have 106 in five cents
for the bid and 106 and ten cents for
the ask right so the writer he's going
to you know like it's restarting out his
version 101 he's going to write negative
1 01 and he's going to write the new
quote out okay so the new record value
and then write the version 102 ok the
reader on the other hand like he's going
to read the the initial version 101
there he's going to read some mixed
record like a record with the new bid
but the old ask all right then she's
going to see oh okay there's this
negative version negative one on one
that means that I
to kind of keep going because negative 1
0 1 and 10 in 101 the version of the
first red are different right so I have
to I have to repeat so maybe he reads in
the version again it's still negative
now he breathes it and it's 102 all
right and so now he can read that full
record value and you know he can read
the version again and hate they're the
same that means that we got a consistent
read and we can use that value so
another thing we had to deal with is
right there's a c library that we're
dealing with and it does a bunch of
stuff on us and we don't control what's
on the other side of this as well and it
in both the interaction with that c
library live as well as just in you know
general development became problematic
right like I mean talk about some pain
points we had here yeah so I mean really
this isn't a highly concurrent system
right so like you know when things go
wrong we're like during startup or
whatever like you know there's
definitely times were that block of
memory those or one of those blocks of
memory that we're we're reading from you
know like we all of a sudden you know
like need to switch to using a new block
of memory for that same that same data
so that means it's so just to give like
the why let's say the other side crashed
or something changed it has to be
restarted or moved what's going to end
up happening is it's gonna you've got
this thing this ecosystem the other side
that's going to be saying oh I'm done
with that file i'm i'm closing it
reopening and putting a new file in
place and you might have this other app
sitting there trying to access it at the
same time because they're just
disconnected yeah so I mean we really we
need to be able to kind of switch these
two things out and everybody that's
reading from this stuff within the
process means that kind of all at once
be able to start looking at the new
block of memory and try not to seg fault
in the process okay so you know like and
then the other thing is just like we're
using Java that's right ones burn
anywhere and everything right like it's
it's supposed to be easy to just you
know like compile and then start running
more everywhere like C is not like that
you know and so now we're kind of we're
trying to make sure that we're building
with the right version of the library
the one that matches what we want to do
stuff with and so we had to kind of
write some some custom build stuff just
to be able to take it to build this see
stuff and you're like as we're building
our java stuff and ship it along with it
it's just silly practicalities but like
Justin sitting there furiously writing
this API being like oh we need this so
we have to
you're fiercely trying to use it yeah
and that the the tooling for just the
buggy and stuff was just as bad like
just trying to you know like call in to
the Seaver library and getting safe
faults there and then figuring out like
where did it go wrong like that I mean
you know like that there are funky ways
of doing it but you know like it's
difficult it's not it's not right
because you're looking at the core from
your jvm alright yeah so definitely pain
points and like I said there's a whole
laundry list of things but if you work
through them and especially if you
contain the area of your program that
you're dealing with these problems you
can achieve very high performance
systems and controllable amounts even at
high data rates and and and and uh a
great amount of activity going on within
your vm you can still control the amount
of garbage getting created and so when
all said and done we did do this and we
managed to keep it relatively controlled
we went from crisis to okay maybe smooth
sailings going a little bit far but it
worked it worked we did not have to our
three issues were more or less solved
our network was no longer being
completely destroyed by them both the
markets as well as our sort of abuse of
them internally by using TCP to get all
this big data around we had all this
stuff that we'd build in Java that we
got to keep we had to change some of the
core elements of how it interacted with
market data some other stuff along the
way that we learned was a result of even
doing this work that we said hey that'd
be smart to do to you know use these
same techniques techniques in other
places as well and we got a better
system as a result and we didn't really
cover this exactly in detail but this
thing scales extremely well if the
upstream side keeps growing there's ten
more exchanges next year if they all
triple the amount of data they're
pushing we can scale this out really
easily we're not going to have to write
a line of code to be able to have it
scale and again because of the way that
we've managed and segmented the data on
the downstream that's going to be also
true from that perspective so are the
three things we win in knowing we need
to solve by doing this we we really got
to a good solution and and we ended up
having a
that more or less is able to do all
these things and here are some of the
things we learned as well like 10
gigabits per second while it doesn't
happen all day every day some days it
happens and it can happen for an
uncomfortable long amount of time where
you're getting that much data from all
the markets and you have to just keep up
with it and in fact when that happens
it's the worst moment for your business
if your stuff doesn't keep up or is not
it is starting to drop data and it's the
highest opportunity for you if you build
stuff that actually can keep up with it
there usually are not 30 million
messages per second but it does happen
and and that's what the tenth 10 gigs of
data can break down into and you have to
be able to not just say here's the
bandwidth I've got it but also parse it
and do something with it and from a low
latency perspective you know you we had
systems that are built in Java where we
have to have either sub sub milli a
consistent sub Milly management or
single-digit million and it worked we we
also did a bunch of all those things
along the way we used go for parts of
this we didn't go into it in this but
we're really finding that it's a lovely
little language and there's a bunch of
things about their philosophy on keeping
things simple that works and we
everything we've tried to do with it has
has ended up being pretty fast pretty
maintainable and definitely a more on
the fun side of things to work with as
we said earlier see is still there C is
still fun C++ is still around we did not
fully get rid of it I you know there's
definitely a little bit of a love/hate
amongst people in at peak6 around where
we have C++ and use but it has a home
and more or less the way we built this
it achieved goals across optimizing
hardware and working across all these
languages and that is pretty much it
yeah thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>