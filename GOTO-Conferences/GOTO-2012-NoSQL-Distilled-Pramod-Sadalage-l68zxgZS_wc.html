<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • NoSQL Distilled • Pramod Sadalage | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • NoSQL Distilled • Pramod Sadalage - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • NoSQL Distilled • Pramod Sadalage</b></h2><h5 class="post__date">2012-06-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/l68zxgZS_wc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right thanks for coming my name is
promoter I work for talk works doing a
lot of data
at work not necessarily like DBA type
work but how do you store data what do
you do with it how do you use it and for
what purpose and what are the most
efficient ways of storing taking them
out and refactoring them over a long
span because applications come and go so
over a span of the company itself like
don't think application the company
itself the data is valuable so the
application can come and go but the data
has to survive so that you can use it
properly for further analysis and all
that kind of stuff so over the period I
thought works I have written a couple of
books one of them being written a
refactoring databases so in a mature
database how do you refactor it and that
kind of stuff one other being how do you
do databases and continuous integration
my latest book along with Martin Fowler
is no SQL distilled because we think the
no SQL movement is come to stage where a
lot of people are thinking about it and
thinking about how to use no SQL so this
book gives you an introduction in the
sense that how do you have to change
your mind when you think about no SQL
like you can't apply the same modeling
things you did with our DBMS to annoy
skew L technology and hope that it'll
perform better right just because it's
nice to go so the way I like to do this
is if you have questions right there
then and ask so that we can have a
conversation instead of me just dumping
a bunch of content on it right so we can
tailor it to the way the questions are
going instead of just following slides
so whenever you have a question just ask
and if you don't understand any terms
I'm talking about just ask that to like
I don't understand what you're saying
our right okay so the first question is
why no SQL right so we know schema
changes are hard right it's not easy
especially if you have a 24/7 kind of
environment large data sets you want to
rename a table rename a column add a
column remove a index it basically locks
up your database and then you have to
basically bring your app down or people
do this master/slave kind of thing
a lot of juggling around just to make
that schema change stuff work right
there's impedance mismatch so we develop
in objects but we store in relational
tables that creates all kinds of
interesting things the ORM layer the way
you are thinking about it or searching
patterns change and all that kind of
crazy stuff happens and because of which
the code gets really complicated and
changing code for changing requirements
then becomes hard
that's where schema change effects
there's impedance mismatch right then
the next is like we are used to thinking
about integration databases like there
are a bunch of applications for running
on top of our database and then the
database becomes your bottling because
if you change something for your
applications database it affects
everybody else right so this is this is
how we are stuck on this side here
because the integration database is
where you move information around
instead of going through the web service
layer or a restful layer or something
like that and then the application can
decide what to do with the database
right so if you are on the right side
kind of a situation then your
application can decide what to do with
the database refactor change redesign
whatever it needs to do without
affecting the external customers of the
data itself right so in the integration
database model if you change something
for your application most likely you're
going to affect others and that fear of
affecting others stops you from doing
things to the database or doing better
things to the database right then
there's also this running on cluster so
again on running any kind of a DBMS and
a cluster is a thing like it's not that
easy it's expensive
like Oracle 8 application cluster will
cost you somewhere upwards of seven
figure speakers right so running on
clusters is really hard in an earlier
DBMS not hard as in technically hurt but
financially all right and nowadays with
the whole web thing coming up and
there's people wanting to track
everything or all kinds of data there's
this whole notion of unstructured data
and the structure of the data itself
keeps changing on you so do you go and
refactor the schema to fit to the new
structure how fast can you do that
that's again in question right and also
there's this rate of growth like that's
the idea mass capacity planning whatever
BBS type people do they believe in this
or they plan for this linear growth like
40 portion growth every year let's plan
for that have a SAN in place and all
that kind stuff nowadays rates of growth
are totally skewed because sometimes
your product me go hit viral and in two
days you'll have like terabytes of stuff
coming at you and in a month or so
something new comes and people don't
visit your site anymore right so that
happens and the rate of growth is up and
down so you need to think about
technologies that can scale to that
model of going up and down or going up
very fast and that caster right so the
rate of growth is also important so
having said that let's go into this
thinking about how do you model for this
no scale moment so right now don't think
about key values documents anything like
that
just think about your object how you are
modeling that object and how the data in
that object is gonna like this is
generally how your object model would
look like and let's take that object
model and this is how your data would
look like in the database right so think
of the first impedance mismatch slide
you saw and this is what is happening
like the model is being reflected as
rows in the database right it gets
complex pretty fast and has like Eric
was talking about this like in the cab
we just came in just talking about this
unbounded like relations are all over
the place and it doesn't get bounded you
can keep traversing as far as you want
and it doesn't get bounded pretty
properly right so that model creates all
kinds of core complexity as well as lots
of ORN edek right there is actually a
paper which says like over m is the
vietnam of computer science somewhere so
maybe it's that's a good read to do but
that's where this whole notion of
aggregate model comes in like Eric Evans
talks about domain driven design think
about the domain first and then you can
think about how to store that data in
different ways right
so in this here we are saying the
customer is aggregate as well as the
order is an aggregate so having taken
that
you can then come up with an JSON
representation of the aggregate itself
right so we have customers with the idea
of one and the name of Martin and a
billing address of something and in the
orders aggregate we have the order
itself like all the orders the customer
ever placed with all the dependent
things embedded inside it right so once
you start thinking about in aggregate
then you can also start thinking about
how this aggregates are to be related
because sometimes you may look at a
customer and want to get all their
orders or sometimes you may want to look
at an order and get the customers
information right this is where you can
also need to think about options of how
much do I get data by reference and how
much do I get by embedding right so you
know SQL a lot of people talk about
embedding data as well as a referencing
data right and these references are not
enforced by any no SQL product like if
for example here I have a customer ID of
1 and in the customers I have idea of 1
if you put ID 2 now nobody cares not
like relational database where somebody
like if you have a foreign key hopefully
you define then the database will barf
active say in foreign key doesn't meet
the requirement there's nothing like
that you know SQL so you have to think
of that too like a references can
sometimes if your data is not set apart
there's a bug somewhere and you have
wrong references nobody's gonna come and
tell you that this reference is invalid
right so you have to see how you can do
that but once you do this then life
becomes a lot easier because now you are
thinking in aggregates and that
aggregate itself can be moved around
without having to worry about where data
for the dependence comes from because
that aggregate itself is a
self-contained thing right so think of
like having to set up like test data in
your database right in our DBMS you
would say oh I need order that means I
need a one row in order table four rows
in order items five rows in product one
row in custom or one row and you have to
that chain here there's no chain there's
a Jason and that's it right with that is
self content so that simplifies a lot of
thing right so you can have different
aggregate models still like in the
previous edit model we had the customer
as well as the order right you can also
go down one more path of customer being
the highest level aggregate that embeds
other aggregates inside it right this is
where no SQL also will help you because
you could do rapid prototypes of this
you don't have to come up with the model
that matches you perfectly in the first
iteration you could do multiple turns on
this and then as you grow you can think
about like okay that aggregate model
suited me better than this one or maybe
this is where this is lacking I can come
up with a different aggregate model and
things like that right so you can choose
that and then now the JSON totally
changes is once you do JSON get
everything comes in it like the customer
all its orders and everything is in one
packet coming back from the database
right so you don't have to do multiple
gets on the database right so here's
like a easier image of the same thing
let's say for example you had you had a
customer as a key right and the value is
an object and inside that value object
there are multiple objects like the
customer object the billing address the
orders is a collection of orders and the
order itself has the shipping address
the order payment all of that embedded
inside the key of the customer ID so
once you get the key and its value from
the database everything is inside you
don't have to go back to the database
again at all for anything else right and
the application can then break it down
and put it into your object and then do
whatever you need to do with it you can
also have references within the
aggregate so for example here we have a
customer ID and the value is object
about the customers it has a list of
references to the order IDs right
and then there's also a object for each
order within that list right so whenever
I place a new order as an existing
customer I create a new key for that
order ID and persist the rest of it at
the same time add to the list of orders
in the references for the customers
right so that's how I do a double right
one to the orders and the other to the
customer just append the lists of order
IDs so obviously this has does some
downsides because as you know most no
SQL databases don't support acid
transactions so your order may get
written and your customer right may fail
then you have reference problems or your
order may fail and your customer may get
written so you have to do this on the
application side things like if the
write fails retry or something like that
or make the other guy fail something
like that you have to think about this
at the architectural level you can't
just hope that will work right so this
is a important trade-off you're making
while making the choice of splitting the
aggregate right so you can use the same
kind of references with JSON documents
here's a JSON representation of the same
thing right so what does aggregate
orientation gives you right one thing is
the ID BMS themselves are not aggregate
of where like they like Adam and missus
have this concept of materialized views
but what hellas view is still a row
right you can't have a materialized view
with one customer ID and three addresses
you will need like three sets of columns
four addresses for that to show up
properly you can always take xml and
stuff it into a column in the database
for a key and a value in the ID - but
then you're making the idea misbehave
like a key value store right so that's
the difference between aggregate
thinking and non aggregate thing or
relational thing right once you start
thinking about aggregate the whole
notion of acid compliance kind of
becomes a lesser priority right because
now it's a single read and a single
write you need acid in the database like
stones mostly because you're writing to
four or five different tables like think
about persisting an order you're adding
to the order table you're writing to the
order item table maybe you're writing to
the stock inventory table because you
did a subtraction so those many rights
that's where asset becomes very
important because if any one of them
fails you want to fail the rest of it
right here you are writing a single
aggregate if the aggregate fails
everything fails as it is right because
it's a single aggregate okay so that's
where like lot of people have these
questions of oh no SQL no acid
I have questions how can I use this so
switch your thinking in terms of
aggregates and that will make thinking
about how SQL is really easier right
once you go to the aggregate model
running on clusters becomes easier
because now it's just an aggregate it's
a self-contained unit you can store
wherever you want right
once are over tens hours thousand
servers doesn't really matter because
it's just one aggregate yet it gets
stored
wherever in relational databases like
clusters make it very hard because then
you have to think about related rows
where they go and that kind of stuff
here are the aggregate itself is a
single unit that can be pushed wherever
you want it will be pushed right so now
coming to the data bases the key value
stores document databases and column
family stores are all aggregate based
according to us right so a key value
store just as a key and a value the
value is the aggregate in the document
databases also you have a key and a
value that goes with it
the value is understood by the database
sure you can query on parts of the value
but it's still a key and a value the
value is visible to the database right
and the column family comes in the same
model off there's a row and the row has
columns but the row state is an
aggregate at the aggregate level right
so when you start thinking about no SQL
databases think whether you are in the
aggregate camp or the non aggregate camp
and then you will have to also think
about like graph databases which are a
totally different beast altogether
so we'll talk about each three of this
as well let's talk about graph databases
right so in graph a devices the modeling
totally changes right each node is on
its own and the node has relationships
with other nodes right so think of like
a simple ecommerce app where your
customer actually purchased a product
right and he was billed to a given
address he was part of order and that
kind of stuff and these relationships
have names and those names can have
values - like a key value pair attached
to that relationship right so you can do
very rich domain modeling on these nodes
as well as the relationships right and
that's where traversal becomes very
important so if you are really
interested in connected data like how
the data is connected to each other and
what does the connectedness give me in
terms of the intelligence about the data
and the actions people are doing that's
where you start thinking about graph
databases a easier example to think
about this would be like let's say you
wanted to look at knowledge sharing
within a given organization like let's
say your company you are thinking about
analyzing how people share knowledge
right how people interact with each
other and that kind of stuff you could
easily go through your email archives
and see who wrote to whom
on what topic right and in what
locations they were I do we can do all
kinds of modeling and then decide like
what is the most highest bandwidth
between two sets of people all right
that can be done using graph databases
what that gives you is like what are the
blockers what are the enablers for
better communication right so that's
where you would use graph kind of
modeling right so the key points is
inter aggregate relations hard to
maintain that's what I was saying like
if you use references and one of the
operations fails then you don't know
which one succeeded which one the
references may be out of sync so you
will have to do that on the application
side in the sense if something fails
rewrite it or fail the whole transaction
our failed whole operation on the
applications right schema-less everybody
likes schema-less in no SQL but the
schema is still implicit in your code
that's the biggest trip I have seen
people do is like they'll think of
schema-less I don't need to worry about
schema couple of iterations later they
can't get to the data because the schema
has changed now some of the rows are
some of the documents that are returned
using the old schema cannot be parsed
anymore and that kind of stuff so if
you're thinking hey schema free I don't
have to worry about it stop that
thinking right now right you have to
worry about it because it is schema free
only in the database the schema is in
the application side right so we'll talk
about a couple of techniques on how to
do this schema migration multiple object
versioning and that kind of stuff in the
airbase right and like I said graph
databases work especially with complex
connected data that you want to analyze
so I having said that let's go into the
details of each of these types of
databases right so key value store is a
simple key and a value right let's make
this work it's a simple key and a value
the database doesn't really care what's
in the value it could be a JPEG or it
could be a JSON or XML it doesn't really
care it doesn't even know what's inside
you put a key along with the value in
the database you get a key and a value
back and you would your responsibility
to decode or decipher whatever is the
content right so if you are like a car
kind of a company with or at least
thinking about doing something with cars
like be a key which is a vehicle
identification number is a natural key
for that value right and then inside
that you could store all kinds of stuff
right it's a natural extension or key
value some of these products are
reactive an example Redis is an example
Voldemort is an example memcache a lot
of people know memcache is an example
he vanished all right when doing key
value stores you need to really think
about key separation or like key
namespaces if you are doing like for
example you want to have multiple
objects one for the vehicle
identification number when it was bought
what was the things that were installed
on the car if that's one of your key and
value if the other key and value is
water oil changes what are times added
oil change and maintenance on the car
then your key can collide so you need to
design your key properly it could be VIN
underscore new or win underscore in
factory installed options and the second
would be win underscore maintenance
something like that you need to think
about the key or the namespace design
when you are talking about doing the key
some of the products like react have
this concept known as buckets so you can
create separate buckets with the same
key right so one bucket would be called
as factory installed options and that
would have the same bin in it but the
value would be the same thing right
would go in a different bucket so that's
a key space separation technique that
they have which is called buckets right
but you have to think about like if you
are storing multiple objects you have to
think about how you would do those key
separations so we like to compare back
to our known knowledge right so like if
it's react that would be your like a
bucket that's where I saying pocket
right so that's what the terms on the
react side would be and in the Oracle
side that's what the terms would be
right so there is one key one value a
value is opaque to the databases right
it doesn't really matter what you store
and it's just like hash so if you are
used to using hashes in Java it's a key
and a value and whatever the stuff in
there doesn't really matter all right
and because it's aggregate based model
then you can distribution becomes easier
yeah so let's move on to Mongo
databases that's what document database
is mostly people think about Mongo right
but document databases there are many of
them and
like if you think the model is the same
here like I saying key value and
document databases the only difference
or the big difference is that the value
is understood by the database right and
they get to this fact by forcing the
value to be either JSON or XML like
MarkLogic is an example that has XML as
the actual content right so again going
back to the same example we have our ID
as a vehicle identification number and
the document is basically a bunch of car
facts right like it's a JSON here as one
such things like document databases here
are MongoDB Raven right and then you
have couchdb terrorists or Orion DB
right so in like for example if you take
MongoDB in MongoDB you can have this
underscore ID that is the actual key of
the document and if you don't provide
one the database will actually create
one if you provide one the database will
overwrite that value and give your own
key to the underscore ID value right so
here is an example like for example raw
ID in Oracle would be underscore ID but
in Oracle you cannot change the row ID
in Mongo you can change the row
underscore ID value by overriding it
with whatever you have
of course lot of period like times
people ask why would I even override the
underscore ID because the database is
providing it to me sometimes like in
high performance situations the index
that MongoDB works off of it's best to
have the whole index in memory for it to
have proper scans if your index is not
all paged in memory then MongoDB
sometimes performs does not perform as
well as you want it to so you can take
over the underscore ID space in memory
by giving it your own key so you are
saving this space in memory by using the
underscore ID for your own key right so
for example the underscore ID I think is
a 16 byte value or something like that
the default one provided if you had your
own ID which was the customers ID or
whatever if you
replace that swap that you'd basically
save the index space that fits into RAM
for that underscore ID value because you
would have high index on the customer ID
on the document anything to access those
right so think of that when you do index
design on memory right but again going
back here the instance is a MongoDB is
FEMA is a database table is a collection
row is a document and the row IDs is
correct
right again there's one key one value
the value is visible to the database so
you can do intelligent queries on in the
JSON document find me all the documents
where the name is like all of that can
be done right like a standard a DBMS
query the value can be queried you can
put JSON or XML documents in there all
right any questions so far
I'm doing pretty good okay so let's move
on to column family stores right so
column family again has ID and a column
families and these column families have
like a row that has columns inside it
and each column has a key value so it's
kind of a hash inside a hash kind of a
model right so usually a column family
data model is very hard to grasp in the
first go but think of it a row as
columns and columns have key values
inside that kind of makes it a little
easier to understand right Cassandra
hypo table and HBase Apache HBase are
examples of these products one thing to
think about this is when you write to a
column the column gets worshipped it
doesn't get deleted right so let's say
the previous value was named and the
value was John and the new one you write
is the name the value is Robert the John
value is still around it doesn't get
deleted that's why I like data size
grows exponentially so you have to go
back and do a repair on the database
like kind of like offline that compacts
this data right so you see the same
stuff happening even in CouchDB like
data grows exponentially because it
doesn't delete anything it just keeps
versions of stuff and if you don't care
about old versions you just compact or
they're amazed and it just goes away so
if you see exponential rise in disk
usage in one of these products make sure
you understand the repair or the
competition there are algorithms used by
these things in a column family database
like comparing back to Oracle like
instance would be a cluster a database
is a key space a table is a column
family and a row either row which is
kind of all confusing but that's how it
is the old big difference being in a
given row or in a given table all
columns in the dayy are DBMS or of the
same type right if you say int it's all
over every row in there has to be a int
but that's not true with a column family
database or cassandra at least right so
you can have the same column mean
different things that's what I was
saying each column has version data and
you can expire them you can even put an
expiry date on them like let's say when
you write John got changed to Robert and
you can put expiry on John saying after
three days this just goes away and
that's where you can show multiple
versions if people wanted to like come
back and then compare data and pick
which one is correct like manual
intervention or things like that column
family store a real exact example would
be like you wanted to do real time event
rights for example right so stuff is
coming from the web stuff is coming from
your internal system stuff is coming
from your third-party vendors who are
using your widgets or whatever all these
events are coming at you at a higher
rate then you want to just put like a
rest service in the front and then write
it out because column family source
support higher right volumes because
they don't do any deletes they just keep
writing with new versions right and then
the data size goes and in the back end
you can then compact them out right and
you can also write different types of
what I like to call them as like you are
aggregating data online
in the sense like an event comes in and
you put the event as is but at the in
the same transaction you then aggregate
it saying okay count the number of
customers incremented by one count the
number of products sold increment by one
all that kind of counters can be done
real time in the sense the moment the
event comes and then you can use that to
display somewhere saying oh in the last
hour
54 customers bought this item three
hundred and two times that kind of stuff
is a very good example of using column
family right right so onto graph
databases neo4j is an example then we
have infinity graph Ori and DB and hyper
graph G be a graph database can be used
for anything actually right so it
doesn't necessarily mean it has to be a
graph type of a problem because each
node there can have its own key value
store like each node can have multiple
keys multiple values so you can buy
stuff a bunch of data in there not just
the Nord even the edge or the
relationship can have its own key values
right so let's say for example I am
related to him and I could have like
when did I know him what was the last
time I met him what was the last time I
had dinner with him all that cough
information can actually be put on the
edge not only note right because that's
the property of the edge not of the node
right because my property with him can
be totally different because that's the
property of the relationship right so
then you could do intelligent query like
give me all the people who I had dinner
with in the last year or so or two in
the last year that are related to me
right that's known as a graph traversal
can be done here like for example you
can do a query here give me all the
nodes that no Martin right and like no
SQL list in something like this right so
you are not only querying the nodes but
as well as the relationships does it
make sense right so graph those price
it's basically a multi relational graph
relationships are first-class citizens
like in in our DBMS we think relation
it's a relational database so it
relations may be important it only
enforces relationships it doesn't let
you traverse the relationships you have
to write complex sequel to traverse the
relationships it doesn't let you do what
graph databases that you do because you
can traverse the relationships the way
they achieve this is by writing the node
properties as well as the edge
properties in separate storage that's
why it's more efficient for ten nodes
the query time and for a million nodes
the query time is the same in neo4j
because they write different information
it's optimized on write not on return in
our DBMS you optimize the right but not
optimize the read because you are
traversal if it changes you have to
write music right so there are also a
lot of traversal algorithms available
that you can apply to the graph itself
right and northen edges can have data
which is basically key value pairs right
so let's see some good u6 cases for
these key value stores right so key
values they are good for session storage
like browser of session whatever you
want to store across session caches
profiles like I want I have a user ID
for example a blogging platform or like
a ecommerce web app or something like
that
I have a user ID I want to get his
profile like name and anything else
email shipping address billing address
whatever is it all can be stuffed under
the user ID as a key and everything else
is a value right you get one everything
comes with it
similarly would be preferences so like
screen color font size or whatever else
you want to have preferences can all go
under the same or the shopping cart is a
good example like a scalable shopping
cart like for example if you use react
you could put the customer ID underscore
order ID as your key and the value would
be whatever people are buying at that
time and have that thing saved so you
can get it back when the customer comes
back because of losing session cache or
something
right in a document database you could
use it for event logging prototype
development is a big way to like get
your iterations out without having to
think about like schema or deployment
complexity or all all that kind of stuff
you can have rapid prototypes out and
once the prototype is mature then you
can think okay how do I do this like
what kind of different technologies do I
need for aggregates right ecommerce web
app is a good example for column family
like large right volume like I saying
you could use it for content management
you can use it for e-commerce
applications right and the graph
databases they can be used for connected
data routing like how do i route my
delivery truck from point A to point Z
with three stops in between what's the
most efficient way then you can have the
stops as nodes and a distance between
them as your edges and then you can just
ask the graph database to give you the
shortest route and it will give you the
shortest route without having to think
about it right location services you can
even run recommendation engines for
example customers who buy this product
usually buy this other product team it's
very easy in the graph database sense to
say that all right so those are some
general use cases of the four categories
of databases that we are talking about
Thanks
it's obviously coming back to the schema
less so we did say databases or schema
less but the schema is implicit in the
sense that your code is saying okay put
this JSON in there and when I read back
I need to be able to parse the JSON make
some sense out of the JSON converted it
to my Java objects or what are other
objects you've got and then work with it
if the structure of the JSON changes
then obviously it's not going to map to
your object so that's where the friction
is instead of in the database it's now
at your mapping layer right so obviously
when you say schema free I can change
the schema whatever way it doesn't mean
you don't have data migration all right
so imagine you had a customer object
that had a name and address as its two
fields now you change the name
you say customer name for example so the
new ones that store store it as customer
name but the old ones that are there are
still there as name right so when you
type a partial back you need to parse
both of them right so if there is
customer name use that if there is no
customer name use name right so you have
to have this pattern in your JSON parser
that said okay these two map to the same
thing right so that's where data
migration comes in is let's change every
document I have to be customer name
instead of name right
that's expensive proposition and goes
back to same relational model off like
if I change a column name it affects
every row right that's where people are
moving away from our DBMS because they
don't want to deal with it right we be
changing the column name and affecting
everything so you can do techniques like
lazy migration right so in the sense
when you read you read both the columns
but when you write it back you write
with the new one right so that you're
basically upgrading one document at a
time or one value at a time this given
your average it reaches both of them and
does the right mapping when it writes
back it writes back a new way of doing
things right that's one way of doing it
the other way of doing it is writing the
same logic offline and it basically goes
one document at a time and doesn't
affect anything else and the new code
can then be deployed once the whole
thing is done or can be deployed in
parallel in whatever other ways you like
it all depends on your data size if you
have like printer bytes of data then it
will take a long time so you have to
think about how both can go parallel if
you don't have that much data then you
can maybe run it offline and then deploy
the new cup all right but your new code
needs to understand the data structures
so that they can pass them properly all
right
don't just go with the feeling that oh
schema-less
I can change doesn't matter you will
lose data you will lose data in a sense
you will not be able to read the data
that's there because the structure has
changed because schema is implicit in
code and when you change this implicit
scheme and the code you are
data also had to conform to the new
scheme right so many projects where I
have worked on the way we have done this
is written probes in the application
code itself that would just go pick
random for documents and see if it
conforms and if it doesn't come from we
would come come back and say ok the
database doesn't match what I'm looking
for then we would go back and then write
like offline in one document at a time
another thing they have to go back and
change it all of those that's one way to
just report saying ok the application
and the database doesn't kind of match
right this is a very good thing about no
SQL is all data doesn't have to be
migrated at the same time because
different documents can have different
structure so I can do one at a time and
doesn't really affect availability or
any kind of other scenarios right so
that brings this whole notion of
polyglot persistence because we think
the application needs different kind of
storage the storage for order
persistence is totally different from
recommendation engine storage is totally
different from session storage is
totally different from any other kinds
of reporting and up reporting stuff you
need or care about right so why do we
use the same technology same storage
engine for all of these needs right so
why not use what is suited for that
particular purpose to do the tasks
instead of just churning everything into
the same thing right it can be across
the same application and can be across
the whole enterprise for example I would
have a recommendation engine running for
the old enterprise in the graph database
and slap it slap a service on top of it
and it could be a service for the whole
enterprise that says enter this is how
you do a conditioning or I could do the
same thing that a session storage your
secession storage service you do a gate
and a put on the service and we will
manage everything in the back and none
of the other apps you will write in your
whole enterprise don't need to worry
about session storage anymore right you
could do that kind of service too right
it doesn't have to be you don't have to
think about only in the context of
application you can extrapolate it up at
the enterprise level to write so that's
where the key concept of
getting data access through services
comes in is I just have a service that
does what our it's supposed to do and in
the beginning stages just as a proof of
concept we could even write to a desk or
something else and slowly then migrate
to a better storage like using a key
value or something like that or whatever
makes sense to match the needs instead
of writing all to the same database
right so yes like a simple way of
showing it right those are services
sitting on top of databases right so
session storage will go to a key value
store order persistence may go to a
document you're reading inventory and
price and all that kind of stuff from a
relational database which is legacy
that's where all your back-end systems
are right to and you could write the
customer recommendation stuff coming
from a graph database right you could
also do there multiple models of this
this is not to say one and only model
right you could also do like your
shopping cart data can still go here
session data can still go here because
that's what you are used to but
searching is becoming a pain for me
because I have to write complex sequel
to just to find data I could pump the
data through like a solar and do that
kind of stuff
because data is going from here being
fed here and I am just posting source
requests on top of right that's one more
mode so you can have all kinds of model
here think of that like I can just have
this and with this also have one more
for this graph stuff where data is being
fed from the our DBMS to a graph
database and I am just doing queries
against the graph database for
recommendations right that can be done
you don't have to go crazy like this in
the first code you can do slower
iterations on that right so here Martin
actually blogged about this polyglot
process this is totally speculative use
everything under the Sun kind of model
here but we are doing like user sessions
in Redis financial data that's already
coming from back-end systems in our DBMS
shopping cart is going to react quickly
recommendations is going to new for J
part catalog is coming from Mongo again
ad hoc reporting is going to a DBMS
analytics is going to Cassandra user
activity logs are going to Cassandra
right so this is totally like use of
and make sure everything is processed at
using the same thing right obviously the
final question you would have is how do
I choose right there are four broad
categories 20-plus products what do I
choose right you can choose for
programmer productivity right so what
will make my friction of saving this and
getting back the easiest like you can
choose for that if taking the product to
market is your single most biggest
concern like let me prove if this works
then you can choose for that you can
choose for data access performance right
like for example if relationships and
recommendations are important then you
would choose for performance right today
I have 10 customers tomorrow or a year
from now I have 5 million customers will
my recommendation engine scale all right
you can choose with those questions in
mind you can choose with hey I am
working already with this like why you
won't think about new stuff that's a
choice you can make right you can also
test the expectations right so I did
this project for the Democratic National
Committee which is the Democratic Party
is headquarters kind of stuff in 2010
ish and we actually ran tests like all
kinds of tests because most of this is
open-source you don't actually pay a
dime to install and run this all right
so we install this stuff data whatever
we needed to do and we were supposed to
provide a 1 million hits per hour
proof that it can handle that amount of
volume so we actually coded ran there
and performance tests on it and then
tweaked and then chose that ok this
works right so you could do tests your
expectations right so like we generally
choose based on reading some stuff
somewhere and making sure that that
matches with our context or our problem
set and if it matches we choose
sometimes it doesn't match because
somebody's talking about like web app
and you are building something else then
actually code it like do a short spike
on it and performance test it and see
how it works right because it doesn't
cost anything to download and install
Cassandra or Mongo or neo4j or anything
for that man right doesn't cost you
anything other than your time which is
actually very good because you are
investing your time to find out if the
product is good for you like I said they
are all open source you can try all of
them and see what fits where and then
test your expectations and then you can
write performance tests against them
like in the dnc project I mentioned
actually every check-in actually went
through like check-in then we did like
this code analysis then we did unit
tests then we do an integration test
after that came the performance
benchmark tests which basically took the
code deployed against production test
environment and ran the whole thing for
two hours and we would say ok this code
passes that and then it was deployment
ready at that time so you could do that
kind of scenario - all right that's all
I had I know I was a at an hour but it's
done in 45 minutes which is always good
so that's the book that's the other
books are saying I write here so if you
want to go read what I write or if you
want to you can go so yeah sure so there
are different things like in qnu you
cannot actually search by value at all
right so you your application needs to
take that value put it into a different
search index like solar or Endeca or
something like that itself right the
database is not going to do that
nowadays react is also supporting index
search kind of product that sits on top
of the rail base that indexes for you in
the document store you can actually
query the value right but let's say your
query was a return for the JSON document
containing the customers name to be
called the attribute to be called as
name
and then you change it right you change
it to client for example then you are
query that you had returned we need to
now know the new stuff right that's
where the difference comes in is the
database can index the name column for
you but you change the column itself to
be called client now so the index
doesn't even hit the client column at
all because the index is hitting the
name column so now you need to create
one more index for the client column or
the client attribute in the JSON column
right and then you would she be able to
hit like you search for name as well as
client and then you would find one of
them yes yes yes and that's where like
JSON documents are self-contained schema
definition documents right so every JSON
document has its own schema defined
because you can traverse any JSON
document by using Jaxon JSON library or
anything like that you can do a diamond
dynamic traversal of it but if you
change the document the index definition
doesn't change along with it because if
there's other disconnected things right
so you change the document you stored a
new document the index was defined on
the old document using the old name the
index doesn't care the old name still
there even if any of the document in
your whole database doesn't have that
column the index still doesn't care
because you defined the index on the old
column that's what it's doing right so
that's where the disconnect comes from
thinking is kima is in the application
and it affects other things you do along
the same thing of applies to relational
databases - all right you change a
column name you don't change it
installed proc you don't change it in
the trigger stuff is going to affect you
right similarly it will affect here also
that's what I'm trying to say is don't
just think schema less means I don't
need to worry about scheme you have to
catch it you have
if you have applications that from
walking away from our DBMS to something
else
pop up to you dude Greenfield approach
we don't know what the data would be
sure and that's actually a very good
place to start with like you are domain
modeling right so when your domain model
think about the aggregates not about the
storage right and when you do that you
can extrapolate saying okay given today
if my product goes viral
I will get this amount of customers or
whatever you can make up the customers
and extrapolate and test for it right
let's say today I have 10 customers like
first day of launch I get 10 customers
and I will extrapolate out in a year I'm
gonna get 2 million customers each one
of them is a burn order for products
every other day right so you can put
that kind of calculations and then
calculate what kind of transaction rates
do I need to run right so like the
Democratic Party I was saying we didn't
have any real data we made up data in
the sense like what are the ratios what
are the rates that are going to come
what is the kind of randomness we are
looking for so all of that has to be
calculated right yes yes and you don't
even upfront model to the like nth
degree
you just need ok this is the first
iteration I'm looking for model it for
the first iteration and then keep
iterating over it because if first go
you mean you may come up with the
aggregate model of like X and in third
go you may come with totally different
aggregate model that's as you iterate
you will find different options yeah
okay and it will come to you
vikrum databases populace about
relational databases factoring and
databases I don't think we are at that
stage yet but there is certainly a lot
of thinking that you should be done
about like how you change data right so
a lot of times people will just like ok
let's change it in the application and
then after like three or four days
somebody complains about my order
doesn't show up or my this thing
customer information is wrong then they
start digging back so the first part you
need to think about is how do I change
the structures and make it visible
across all the dependent systems so a
lot of maturity needs to come there it's
very innocent right now so I don't think
there are patterns emerging yet to write
something about so far yes
like putting keeping them in production
is that what you are asking yeah yeah so
most of these are like not version 10 or
not version 11 Oracle kind of products
right so you go with that mindset there
are a lot of like command-line tools
right because there are now not much
commercial vendors in this space here
there is not any UI tools but most of
them are command-line tools most of them
are open source so you can actually look
at the code and hack it yourself too
right
there's also a lot of tools available
that are plugins to this existing
products like Maggio's as a plugin for
Mongo for example right so you want
nudges alert there's a plug-in that you
can plug in manga also has this MMS
activity monitoring thing going on that
can report on the cloud how your
database is doing for example or neo4j
also has like lots of other tools in the
commercial edition they have this high
availability cluster available and that
kind of stuff different products have
different layers or levels of maturity
and if you go with something that has a
company behind it like for example neo4j
has new 4j technologies neo technologies
behind it so you can buy a commercial
support and that kind of stuff route
them if that matters to some
organizations because they want someone
to call and do something with them kind
of stuff so that is possible too but in
production most of the time like I have
run I run Mongo in production for almost
three years now it one client and like
recent climb and it's been like three
weeks since we have put memcached
as well as Monte bean production you can
like mostly do all perpetuation of these
these are very easily scriptable that's
one switch from the relational like
relational rows all UI based like
installing Oracle on a command-line is
hard like try to do headless Oracle
installation manager right so you can do
like Mongo you can just download runs
there penetrance right so think of that
too like you can do a lot of puppet kind
of scripture on this
he just said I should first design my
data aggregators or whatever in my
design model and then decide which is my
quake storage but if you know also that
if I use a graph database I'm going to
kind of match my design so that it fits
in this P and connections connections
all right something I'd like like back
and forth but I kinda like I have a
problem it matches somewhat more the
graph problem sure and I mean there are
two big categories your aggregate and
graph right so when you're modeling you
will like within the first day know if
it's aggregate or graph that's kind of
easy decision I think right and then
within the aggregate then you okay what
do I use p-value document or column
family is a more detailed discussion and
discovery that needs to happen in the
graph when you start modeling then the
graph modeling is a lot different than
like relational modeling because keys on
the nodes keys and values on the nodes
and keys and values on the edges the
names of the edges are are like a
discussion out of themselves like how do
I do like do i do all as incoming
relations do i do as outgoing
relationships how do i query that makes
the change like even in graph once you
come up with like a first version of
your domain model and three or four
versions later your domain model make
totally change on what you learnt about
the domain as well as what the graph
databases support right because lot of
times we go with okay let's do all the
leaf edges pointing to the branches and
higher as you go and after some time you
may realize how I actually want to go
top down into the bottom right so you'll
have to switch every edge that's one
interesting thing about graph databases
is if you change the name of the edge in
your code all the edges will not change
automatically you will have to go to
every node and create the edge again
that comes with schema migration so we
have written this about in the book is
like two nodes have a
named as father off for example and you
want to change the name of the edge to
you like dad off or biological father
off or something like that you change
that in the code the database doesn't
know about it
it doesn't really care so if you query
for father off and query for by this
father off you'll not get the same note
you'll have to go traverse the database
and make these same connections between
those two a to nodes with a new edge and
then delete the old edge yes yes yes
right so you have to think about what is
the context what is the context you are
solving it under what is your problem
right I can just sit here and say a used
key value or uske its it will be kind of
stupid right for my part I mean there
are many use cases right so one is like
your organizes capability to use noise
fill itself like no SQL comes with lot
of baggage on its own like it's not like
it's dependency of a lot of blocks
around that says how we fail with no SQL
there a lot of blocks around if you go
read that right that's where like
thinking about your problem first right
what you're trying to solve is more
important then I have a project I want
to use no SQL the straight connection is
wrong right so you have to think about
how what it fits the problem how you are
going to solve it and what are the
properties of that particular product
that actually entice you and what are
the negative aspects that may affect you
right so you have to think of those two
and then make the decision right like
there's existing like for example you
have this product running in your shop
or application running in your shop for
like 20 years it's working with
no don't go I change it to annoy school
tomorrow I mean that's a straight answer
right there like so I try something
something new Greenfield is coming that
is different in terms of the problem
it's trying to solve then see if noise
will apply it's there like don't just go
and change a legacy app that's already
running to use noise okay maybe my time
wise are you doing okay yes it will get
complex like similar to like in the
refactoring databases book we talked
about this transition phase right old to
new and you have some transition phase
in the middle like I'm gonna drop this
column in two months when all my code
moves right so if they're two months
becomes three years for you you will
have a complex scheme same thing will
happen with your no SQL store to like if
you don't clean up fast enough then you
will have lots of these layers in the
middle that say okay name client party
name and all are all the same then your
code at the transmission layer core and
the JSON itself will become complex in
the sense it's not cleaner similar to
like any code that if you don't clean up
regularly it becomes dirty same thing
applies
no you'll have to do it yourself yes yes
there's nothing like because the
database doesn't really care what you
saw right for it every document can be
different that's that's by design right
yes yes there's no such concept here yes
that you have to do it that's why I'm
saying you have to do something about it
yourself the database doesn't give you
any like automated way of code
yes so you will have to write your own
query like give me all the documents
that have this old name in it and then
go through it right there for them and
then change them or do whatever you need
to do I have seen three of them in the
same enterprise system yes you have to
think about like how will you set up
this in the sounds like developers need
a local version of this stuff while
they're developing we do all systems
using continuous integration so the CI
system needs to know about it the QA
system needs to have a different
instance right so the first time we do
this we will do like a puppet version
for the develop our instance itself
right and the second would be for the CI
instance from QA onwards it changes a
little so that needs a different kind of
operate system because QA needs a
cluster because we don't want clusters
introduced in production only like it
needs to be introduced a little earlier
right so you need to think about how do
I do this
deployment that's where the whole DevOps
comes in like put in more puppet
versions to deploy different kinds of
systems and you don't want a person
manually sitting and doing this on
different machines right so for example
you can add nodes transparently to these
clusters like in react you can just add
a node and say make it part of the
cluster
you don't want a person sitting there
and typing this every time you want to
add a pound right so that's where you
need to think about pipette or some kind
of script I don't really care if it's
proper at or a shell script but some
kind of script that does it for you
right even on a developer machine right
we had this DNC project we had a ruby
script that basically went to the
internet downloaded the latest stuff
unzip them packaged them and said run
here's your local developer instance
ready to go right so you could do that
also you don't need to actually like
rely on a portion clicking through our
portion manually like a document
document that says download sure so
there's the ops team that doesn't know
what did the DBS they don't know about
how to take backups of this stuff right
how do I take a backup how do I restore
what are the other things that go with
it
they don't know what they need to learn
right so that's all the other things
that comes with it the other option that
you may have is like how do I deal with
data that is needed by this external
system and the external system usually
is poking directly into your database
and reading off of the tables right so
that's where that integration database
pattern comes in and now you are moving
more towards a service based pattern so
in the meantime what we have done in
this current client is the application
on the other side can just invoke a rest
call and the rest call goes back in the
database dumps a CSV file out and that
it can read the CSV file backup right
that's the integration pattern we used
is basically like a service invokes
returns or gives it to a CSV file right
so you have to depend like how are the
dependent systems gonna interact with
this
not just dependent systems it's also
like your power users like they are they
are so used to like pointing their excel
at this database or access at their
database and trying to do all kinds of
crazy queries they can't anymore because
it's Mongo and they can't understand
JSON or something like that how do you
give them the data that they need right
looks like we are at a logical end here
okay
thanks guys thanks for coming and
resting your perfectly good evening with
me all right thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>