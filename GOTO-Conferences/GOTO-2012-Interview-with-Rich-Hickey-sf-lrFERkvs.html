<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • Interview with Rich Hickey | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • Interview with Rich Hickey - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • Interview with Rich Hickey</b></h2><h5 class="post__date">2012-08-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sf-lrFERkvs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Kyle Foucault and now here
goes to Copenhagen 2012 with Ritchie and
creative closure and a tonic and first
of all I want to thank you rich for
doing this intentionally and then maybe
just to get started and I'd like you to
maybe say a few things about yourself
and introduce yourself or people who
don't already know you okay well I was a
software developer had been for almost
25 years now and worked on a variety of
different kinds of systems and built
closure in 2007 or finished it in
delivered in 2007 and that's taken off
and yeah subsequent to that I've been
working on the atomic that's what I do
yeah I'm personally close to you sir and
I be close to you system and my team
isn't so take off in your keynote that
you escaped yeah we should call the
value values and so move into closure
and those things so so just for people
that weren't at the talk could you just
tell me what's the difference between
place oriented programming and value
oriented programming sure I think place
oriented programming came up around the
fact that you know when we first started
defining programming languages and
databases computers were really small
and we had to deal with memory and
storage explicitly and we had very
limited resources and you had to had to
use use them as places very specifically
and then we developed abstractions that
we wouldn't be touching memory addresses
and disk sectors anymore but those
abstractions weren't really about
information they were about keeping us
from touching memory addresses and this
sectors which is which are places but
that the challenges we face right now
are writing information systems that
really do need to maintain information
and facts and really all of the facts
and so the biggest fundamental
difference is you know with a place new
information replaces the old and with
values new information requires new
storage you know space yeah and you
don't change things you
and in Utopia you know several points
and one is that this makes sense in the
local process it makes sense between
processes community in different systems
and it makes sense of storage yes so if
we just focus on the in process thing
and obviously if your work has been you
know inspired by closure and then moving
that gets our closure so how does close
your support about your programming
closures pretty much about value
oriented programming you know I think a
couple of things very critically one is
all the core data structures are
immutable so right away we we get that
part in addition all of the collections
are also immutable and efficiently so
because these persistent data structures
and so that that gives you sort of a
foundation for your value oriented
programming and I think that's the yeah
so it's the star so one of the questions
that may pop up you said when you hear
about loan is this how can it be fishing
right how can you have all these miracle
things and you're not you know changing
and you're making you versions so you
think about cooking and stuff like that
so right so how does it work inside
culture well in any functional language
that has to assistant data structures it
basically the answer and the future of
programming is trees as a tree data
structure the sort of the underpinnings
efficient immutable data structures so
what happens is that a new version of a
data structure really shares a lot of
structure with the old because they're
trees you only have to modify a few
nodes and then the rest of the notes can
be shared and that means that new
versions are efficient
the other key thing we're finding about
trees is that that that structure is
also what we're gonna need to support
parallelism moving forward so a tree
oriented programming it's kind of the
you know the future
yeah and perilous isn't this is also
where I want to move to because you're
not maybe the world's most frequent
blogger but I know with him the last
month you made new to new blog posts yes
and those were
which call reduces hmm and how does that
relate to closest data structures and
this notion of values so reducers are
sort of orthogonal I mean I guess the
one the other points about closure being
evaluated to programming language is
that it keeps the functions out of the
values so func you apply functions to
values and they're sort of orthogonal to
each other and so reducers is actually
mostly about the function side of things
you know how do you how do you map a
function across the collection how do
you filter collection filter values out
of a collection and so the reducers
framework is a way of doing those things
by sort of specifying the recipes for
doing them and not actually physically
accomplishing them until you really need
to realize result so you can say other
collection I'd like to have this
collection where every number was
incremented or only the even numbers
were kept and that actually doesn't do
any work when you say that sort of gives
you the recipe for a collection that has
every number incremented and even number
is removed and then you wait for what
what's called the reducing operation to
do that work and that means that all
those tasks can be combined so if in the
end you want to take that collection of
incremented even numbers and add them
all together that would be reducing
operations that add them all together
and then only when you were adding them
all together with the filtering and an
increment didn't happen so that's an
efficiency advantage over other
techniques for doing that that have
other properties but when you don't need
those properties like laziness mmm would
be a property you trade off to get to
take this other approach sure but the
critical thing about the way the library
does it is that by defining these
recipes in a way that's independent of
both the representation of the data
structure and the order of operations it
means that they can be parallelized and
so the reducers framework both gives you
a new way to define things like mapping
and filtering and that way is subject to
parallelization
there's a way to sort of say fold
instead of reduce you could say fold and
that process will happen in parallel by
a fork I want to get into that yes
before we get into that I'd like to so
it seems to me that there is this has to
be some kind of relationship with
relationship between reducers and a
notion of laziness because Mason this
also seems to be about not realizing you
know interview results mm-hmm and not
producing anything before you actually
need it right so is there some way to
compare those two approaches this this
so yes both both delay the work until
some later times you laziness will
basically do the operations one at a
time and it's oriented towards a
sequential path right through the
collection so when you define map or
filter as a lazy function it is kind of
a recipe but it's a recipe that
incorporates the fact that you intend to
consume the results one at a time from
the beginning and reducers abstract
actually do less you know they presume
less and they and they promise less they
don't presume that the the processing
will happen sequentially and they
specifically say they have nothing about
them that presumes that and they also
don't require that the collection meet
the processing through some sort of
sequential interface they actually let
the collection do the navigation so by
being independent of the order of
operations and collection navigation
they really specify less and by
specifying less they're subject to
parallelization on the other hand a
parallelized reduce is essentially an
eager operation when it actually does
happen it happens on everything yeah so
a lazy computation can be partially
consumed but a reduction is always okay
so now it's seen as complements yeah and
so now to me it seems that closure will
have some two sets of similarly named
operation
and that one that's math what you don't
know with that then you know reducers
map okay so this one going to be primary
in some senses are going to hug
admission the usage of this reduces
wrangling it's an optimization or it's
the primary math enclosure so you know
they are complementary I think there are
definitely distinct use cases certainly
if you if you intended to consume the
entire result then it's clear that
producers are an optimization over
laziness because you're not actually
leveraging laziness then right you're
gonna consume the entire result and and
if it's a memory let's say it's the
other criteria that would might drag it
to laziness if it didn't if that was if
all that is the same then reduces are an
optimization of course reducing and
parallelization is always kind of an
optimization but I think it's sort of
fundamental that it be available to
programs because otherwise they won't
get fast or anybody new computers yeah
so it was very important in the design
of reducers that the resulting programs
have the same shape right that's where
all you know that's one of the big value
proposition is in addition to access to
parallelism is the fact that from a
logical standpoint what you've asked to
happen is mat this function over
filtering this collection what you know
this criterion and that is the same
logical operation in either case and so
that and it has the same shape sauce is
really important because then you can
make these other decisions orthogonal
right I intend to consume the results
incrementally and possibly not all of
them make brings you towards laziness or
I know my sources or results will fit in
memory so I need to use laziness or
those aren't true and therefore I can
choose the other but my program will
structurally change so it's not like a
bifurcation of the world like you have
to make this critical decision early on
yeah it's not like that at all so I
think that both will be used I think
they truly are complementary and
switching between the movie trivia yeah
it's not like you're gonna replace core
map with our our map in any sense I mean
no no no I mean people might Ally some
attorneys
because they're just they want that all
the time but I am so if that means from
a practical point of view and that would
be just a replacement namespace and you
can exactly correct it maybe just faster
yes did you have any sense of how fast
if you just keep it to a sequential
enough introducing parallelism do you
have in this location yes this is
actually less allocation memory and it
depends on how big your work is per step
right so allocation has some overhead if
you had a big job to do for step than
the allocation as a percentage of the
whole work as small and you actually
don't care yeah but if you had a very
small job for stuff like Inc like plus
or Inc then you've dominated by the
allocation cost and then each step of
that of a multi operation like filter of
map of something adds an allocation to
the chain first step so you can see two
three four X depending on how how deep
that was nested between just sequential
between of reducers and laziness okay
that sounds big yeah and I guess also
because people are doing performance
okay so you mentioned fork/join yes can
you say a bit about how fortun works for
people who don't know for sure
the idea behind fork/join is that you
want to do some operation it's one it's
one piece of work that you want to do so
we're talking about parallelism here or
knocking concurrency right so it's
parallelism you have one job to do and
you want to let respect that you have
more than one processor by dividing it
up and fork/join is literally a divide
and conquer kind of strategy there's a
library for the JVM called fork/join
it's a framework they call it and it
implements a strategy of setting up a
pool of workers that will do the jobs
and
your program needs to just feed it work
by taking their work to do
dividing it in half right submitting
each half as a job and then recursively
those halves will divide themselves in
half and so they get small enough that
you say there's no point in you dividing
this I'll just do this sequentially and
so that forms this big pile of work for
the queue and then the queue uses this
work ceiling approach to making sure
that as as threads become idle late you
know they've done what they're supposed
to do they hop on and steal work that
was allocated to other threads so that
collectively they can get the entire job
done and they all run on independent
threads and it's a beautiful way to do
parallelism
especially when potentially your work at
the leaf level is not uniform so if you
if you work as uniform you can almost do
this yourself I don't you mean my work
uniform it takes the same amount of
effort to do each sub sound right okay
so if I if I knew that I know I had four
cores I could just divide up into 4 jobs
and just do it but if I if some of those
sub jobs might take longer than others
then one guy would finish and he just
have nothing to do and work stealing mix
means that even if you have non-uniform
work or not your tree doesn't have the
same depth or just some jobs are bigger
and some you know so the nice thing
about fork/join is you're isolated from
needing to worry about that mmm
sort of it composes well you know and so
the tree being inhabits having a
particular shapes on it I guess that
that's the relation to the structure of
right so either you have a tree or you
gonna be forced to you build abrogated
yeah in order to feed fork/join you
always have them yes close your data
structures or all trees so this means
that so can you repeat you said you
mentioned it earlier but can you repeat
how fork/join enters via reducers right
right so the reducers library is about
how do you define the fundamental
collection operations like map and
filter in terms of these transformations
of reducing functions and reducing
function you would use to produce you
know the traditional closure reduces
sequential process you saw were the
first two things and add them together
then take the result and add it to the
next thing and that's you know reduce or
full of left there so once you've said
the way I define map is by modifying
plus you know in each step and the way
to find filter is modifying that in each
step then you have this modified
function that you could reduce with
sequentially or you could do what the
reduces library calls fold which is
essentially logically like reducing the
collection but it doesn't make that
left-to-right single pass promise it
says we're gonna divide the collection
up and do reductions in parallel on the
sub segments and so though the result of
fold is the same as a result of it
reduced but but fold makes the plumbing
of MapReduce
I'm sorry to fork/join available and
sort of accessible to you because it
recognizes the fact that what you're
going to do is you divide your task up
and then eventually get to the point at
the bottom where you're not going to
divide up anymore and what it says is
that bottom job is actually a
traditional reducing job just it's now
just reducing a sub section yeah and
then the results of all those reductions
need to be combined so unlike reduce
which only takes a single function which
is used to do pairwise operations
fold takes both the reducing function
which is used to do pairwise operation
at the bottom and a combining function
which is used to take those results and
join them together at the higher parts
of the tree to give you a single result
right and so sometimes they're combining
function the reducing function of the
same like with plus that would be in a
case and you can leave the combining
function out yeah other times you might
be building collections and then wanting
to combine them in a particular way and
in which case you'd have a unique a
different combining function then you
have a reducing function and so there's
a functional library called full of cat
that does that bit it does a reduction
that makes a collection at each node and
then it just takes those questions and
quickly makes a binary tree out of them
without any copying yeah so when would
you use
to function well fool cat is the fastest
way to get a finished result that's also
a collection right and of course the
resulting collection doesn't have many
collection properties for instance it's
not actually a vector or a map but it is
itself reducible and foldable and see
Keable and so that could be just enough
to carry it to the next part of your
computation but the thing about actual
do when you do the fold after the work
gets done just like when you call reduce
the working so yeah so if you want if
you need the work to get done for
instance maybe the result of your thing
takes a large collection that makes it
smaller you know you want to call fold
cap so that that happens and then you
can free up the large coin
yeah so okay so what's the state is now
introduces library and concussion it's
in it's in the office right now so
people can try it so alpha 1 5 alpha
that's right and fork/join itself was
incorporated in java 7 and the guys who
built it Doug Lee and the concurrency
group made a library that you can use
with Java 6 so it reaches back as far as
Java 6 see the Java 6 or you used to
have a 7 either way you can use it and
that's only required for the fold right
so that's the library Russell library in
the design approach so yeah and we have
a lot of the collection functions done
and you know we have people submitting
some of the others and so it's great and
then so that's closure 1 5 do you have
any secrets you want to okay I'm looking
forward to using this and just want to
thank you for interview sure thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>