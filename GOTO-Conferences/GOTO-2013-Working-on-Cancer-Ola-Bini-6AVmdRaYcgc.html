<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2013 • Working on Cancer • Ola Bini | Coder Coacher - Coaching Coders</title><meta content="GOTO 2013 • Working on Cancer • Ola Bini - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2013 • Working on Cancer • Ola Bini</b></h2><h5 class="post__date">2015-10-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6AVmdRaYcgc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hello and welcome everyone thank
you for coming
my name is Ola Binney and I work as a
developer for thought works if you've
been in the architecture track you have
probably already seen two of my
colleagues go before me both Neal Ford
and Sam Neumann and they've been talking
about a lot about Architecture from the
perspective of what you should do and
what you shouldn't do and things like
that this talk is a little bit different
this talk is about a project that I led
I spent the whole of 2012 working on a
quite interesting project and I'm
basically going to tell you about it a
little bit about what makes it
interesting but also a little bit of the
architecture that we used in order to
even make it possible so before I begin
I've been told to remind all of you
about the app and that you should vote
in the app so now that I've done that we
can start with this so I'm a developer
I've done a lot of different types of
development through the years I'm a
programming language geek among other
things but my work at thought versus
allowed me to do a lot of different
things in the beginning of 2012 I was
tasked with being the tech lead for a
project for a company called an AI
systems and we came in and we didn't
really have any idea about the domain
and they didn't really have any idea
about the how they were gonna solve the
problem they knew what space they wanted
to do the solution and what kind of
constraints they had in order to get to
a solution but they didn't really know
what the solution looked like so I'm
gonna the first the first five minutes
or so I'm gonna give you a very quick
introduction to the pertinent details of
the domain and what's actually relevant
for solving the problem from our
perspective and then we're gonna go into
how we actually ended up solving the
problem I might use the words e two or
Fred both of them are the words for the
solution we ended up with it doesn't
really matter what they mean because
they're both backronym x' if you have
any questions as we go along you have
the app and if there's time at the end
which I hope it will be I'll be able to
answer questions done if you have
something that is very unclear or
anything like that just just raise your
hand and ask as we go along some of this
stuff is a little bit complicated and I
know because I I didn't know anything
about molecular biology before I did
this project so I had to pick it up as
we as we went along is there anyone here
who knows molecular biology ok fantastic
so when I simplify no one is gonna
notice ok so the problem if I if I if I
should simplify the problem statement in
one word its to solve cancer a little
bit more context to solve cancer based
on the current technical advancements
made in sequencing and to to kind of
expand a little bit more on that to
solve doesn't really mean to solve but
to give better to create treatment
options than are available right now so
before we get into cancer itself it's
it's useful to be sure about some terms
so DNA is something we all have heard of
I think I'm not even gonna bore you with
the what it actually stands for but it's
a string of basis and these bases are
usually organized in a long long long
string that is usually seen as a double
helix you've all seen the pictures so
what is a base or a nuclear base a
nuclear base is one of four different
molecules these molecules are usually
called a C G and T and in the helix form
at most of the time these will
complement each other so on one side you
will have a C then on the other side you
will have a G or on one side you will
have a T or you end then you will have
an A on the other side so you have two
strings that are wound around each other
and no matter which side you read from
you will get the same information
although complementary to each other
RNA is what you take DNA and out of it
comes RNA so RNA is free floating
particles that float around inside of
yourself it's very similar to DNA but
it's used for communicating and
a lot of other things inside of a cell
you will see sometimes a you instead of
a tee when talking about RNA but it's
really the same kind of molecules we're
talking about
and then finally RNA gets translated
into proteins and a protein can also be
named polypeptides and approaching is
composed of a string of amino acids and
there are 20 amino acids that are the
common ones that we use proteins are the
things that actually do stuff inside of
your body there they can be enzymes or
they can be all kinds of other things
and the proteins are also the pieces of
the body that actually it's in short of
the process of turning DNA into RNA and
turning RNA into proteins and so on so
it's a recursive process in order to
translate DNA to proteins you need to
understand that so since we only have
four letters in the DNA alphabet we need
a way of encoding them so that we get 20
different amino acids and this is done
by using codons one codon is a set of
three different bases and that gets
translated into an amino acid so we have
a 64 64 different possibilities and that
actually means that our code in the DNA
code is highly redundant so any single
amino acid has several different
coatings for it
and then finally what is a gene we
talked about the word gene is used very
unspecific we most of the time a gene is
really just some piece of DNA get that
gets translated into a protein but the
real proper definition is a little bit
wider than that it says that a gene is
an inherited unit under selection and
all of these words are really really
important so inherited means that you
get it from your parents a unit is the
smallest possible thing that if you
don't take it apart if you take it apart
it will not have any meaning anymore and
under selection means that the benefit
to the organism is going to be reduced
if you take it away in one way or
another
variants if you hear me say variant you
can you can mentally substitute mutation
they are not actually the same but for
our purposes it doesn't really matter if
you read the literature you will find no
genetic papers or no molecular biology
papers that that use the word mutation
is that it's called variants now there
are two types of variants germline
variants and somatic variants germline
variants are the ones that you get from
your parents they are the mutations that
make you different from each other
somatic variants are the ones that you
have when you have a disease that causes
genomic differences so if you have a
tumor you're gonna have somatic variants
that are different in that part of the
body compared to the rest of the body a
typical human has about 3 billion base
pairs in our DNA so that that's the full
recipe for a human 3 billion base pairs
what makes you different from any other
human in this audience or anywhere else
is roughly on the order of 5 million
germline mutations so that's what makes
us different
so that's DNA and genomics so I
mentioned in the beginning that the
reason why this project was timely was
because of the rapid increases in
sequencing now what is sequencing well
it's taking physical pieces of DNA
actual clumps of DNA and turning that
into bits turning that into strings that
we can use in a computer figuring out
what the code is for a specific person
at a specific place and this is an
interesting process that basically you
have to do a lot of different things you
have to prepare the physical sample then
you chop it up into really really small
segments once you've done that you take
each one of the segments and you do a
physical process on each piece of it in
order to figure out what base is there
and then you have to put it together
again because these pieces come in in a
jumbled order so you have no idea where
they came from and there are two pieces
that is called assembly and alignment
and this is why up until very recently
you need a supercomputers in order to do
this part of sequencing so it was
greenly expensive even now the the big
labs doing this stuff it takes several
days for them to do the alignment
because alignment is taking each one of
those two 200 base pairs remember we
have three billion base pairs in our
whole body and you shop that up randomly
into two hundred base pair long things
alignment is really taking each one of
them and doing a fuzzy string search
against the reference that we have
figuring out where it should be but out
of it we get a long sequence that
hopefully matches more or less what the
what the DNA for a specific individual
is optional here is to do something
called variant calling where you compare
something against something else
it's basically add if you compare two
genomes and out you get what the
differences are what the variants are
now the first sequencing of a human
being was done through the Hyuga project
and the cost for that was in the range
of several million dollars and it took
several years to do it it was extremely
expensive extremely labor-intensive
since then the price and the time
necessary to do a sequencing for a
person has dropped rapidly and we are
now very very close to the point where
you can get your genome sequenced for a
thousand dollars and the thousand dollar
genome is what people have been talking
about as this magic limit once we get to
that place then we can do everything we
want to do with genome sequencing of
course there is nothing really specific
about the number ten a number of
thousand dollars but that's what people
have been talking about and when we
started this project the price was down
to maybe ten thousand and we could see
the writing on the wall we knew that the
cost was gonna go down further and
further and very soon you're gonna be in
a situation where actually every time
you go to a doctor my vision is that you
take you you open the door you take the
handle and the sequencing starts when
you it just snaps up your DNA from your
hand and when you sit down
the doctor will have the results on his
screen we're not exactly there yet but
we're very close to the point where for
any serious for any serious problem you
have you will get a sequence done just
as a part of the treatment or a part of
the diagnosis
however at this point genomic
information has not been really part of
the well it's been a lot of research but
it's not a part of the clinical world so
what we wanted to do was to take these
advances and apply them in a clinical
world and actually give better benefits
back to people so that brings us to
cancer cancer is not a disease it's
really a class of diseases it's ten
thousand different diseases because
cancer is really a broad classification
of something that can go wrong to cause
your cells to multiply without control
and there are many ways that this can go
wrong in fact there are tens of
thousands of ways that the genomic
machinery can go wrong to make this
happen
so we we also classify cancer based on
where it happens if you get breast
cancer it's very different from getting
skin cancer for example well in reality
did genomic mutations are actually very
similar to each other in many cases so
they don't have to be different so these
modifications of DNA that can happen
there are lots of them for example you
can pro em you can have a mutation that
stops cells from killing themselves
which they will do if something goes
wrong but if you have a mutation that
stops that then you get a cancer or you
can have something that enhances the
growth cycle to make to make the cells
multiply much much faster than they
should otherwise or you can have
something that removes one of the error
correction mechanisms I don't know if it
made any news here and in Denmark when
the name is escaping me there was a
famous actress a few months ago who
decided to have her breasts removed and
oh thank you yes Angelina Jolie and the
reason why she did this was because she
discovered that she had a very mutation
in a gene a gene called bracket one
bracket one is an error correction
mechanism the only reason why you have
that protein in your body is to stop
your body from developing breast cancer
and there is a lot of different pieces
Machinery inside of the body that just
exists to stop these things from
happening but if a mutation happens in
one of them then you have a much higher
risk of getting one of these cancers and
then you most likely will so these
genetic modifications are really what
makes cancer happen however once a
cancer mutation has happened a lot of
other mutations usually crop up really
quickly because a lot of the pieces of
DNA that is in charge of guarding
against that it basically disappears
when the cancer starts multiplying so
that means that you get something called
so the driver mutations are really the
ones that cause the cancer while the
passenger mutations are just along for
the ride they don't really matter so
that means that it's very very noisy
environment to figure out why a cancer
happened from a genomic standpoint now a
typical cancer when we when we notice it
a typical cancer will have about 10,000
different mutations 10,000 doesn't sound
like a lot but actually it's very
complicated to figure out which ones of
them are relevant and which ones are not
so that brings me to the final part of
the the reason why we did this project
and that's the treatment problem and the
treatment problem is really that if you
get cancer today and you go to a doctor
the standard of care is based on the
organ you get that cancer so if you get
ovarian cancer and you go to a doctor
ovarian cancer happens to be very deadly
by the way it's it's one of those
cancers that if you if you are alive
within a year or if you're alive a year
from the point you get it you're in the
lucky 5% or something along those lines
so it's almost a death sentence to get a
variant cancer in terms of time now if
you go to a doctor with and they find
that you have a varying cancer there are
three different chemo therapies that you
can be prescribed each one of those work
a little bit different from each other
and which one you will get there is
really no way for the doctor to make a
good choice there so what typically
happens is that the doctor flips a coin
or
he uses something that's worked before
or he used something that his Hospital
prescribes or he uses something that the
insurance company thinks should be used
but the important point is he has no
real basis for making the choice between
one of these three chemos so if it
doesn't work try the next one until you
find something that works
now of course chemotherapy is extremely
painful they're expensive $100,000 per
round and three months of time and
extreme suffering for the patient and
pain and damage so in this situation
it's very important to make the right
choice first if you get a varying cancer
for example three months of time it
actually can be the difference between
life and death
and so in making the right choice first
it's really really important now the
information is out there there's a lot
of research papers that have done a lot
of research on which Chema's work on
which genetic markers but connecting
that up with what mutations the specific
individual actually has it's not been
done so far but research papers and
clinical trial data has this information
so so this is really the problem
statement this is the information after
we learn enough molecular biology to
understand what all this stuff meant
this is a synthesis of the information
we had when we maybe two months three
months in started to actually solving
the problem instead of exploring the
solution space I'm just gonna mention a
few things quickly about us because I
say us all the time we were working for
this startup and I systems they provided
us with a few first-class molecular
biologists that could teach us what we
were doing the team was for people from
thoughtworks
it was not a standard team in any way we
were all developers we ended up all
being domain experts we ended up all
being QAS we all ended up being business
analysts we all ended up doing a lot of
XD and we all ended up doing devops so
this is a team that we were four people
that
basically did everything that had to be
done on this project from from beginning
to end and a big part of the first few
weeks was just understanding the domain
enough to get to a point where we could
decide what to do and that brings me to
the process side I would say we did
things in a very agile way but it was
not capitulate a j'l this was heavily
changed process to work and there are
constraints and worked with the kind of
problem statement we were doing we were
not really solving a problem we were
well rather we were not really
implementing a solution we were actually
trying to press all the problem and
figure that out
so the first the first thing we did was
to put up our infrastructure so on day
two we had a full continuous delivery
infrastructures so we could deploy over
and over again until all the new things
that we came up with and then we started
building a back-end that kind of
implemented a lot of the the
understanding of molecular biology we
got and at the same time we started
building front tents that we tested out
on some of our some of our clients or
client people so fundamentally the first
three weeks I think we built maybe 16 or
20 different user interfaces to explore
different aspects of the of the data we
were dealing with and then on the
backend at the same time we were coming
to grips on what the problem actually
was and what could provide value for our
end users and we continued like that for
a few months until we got to the point
where we understood it well enough and
then we switch gears and we started
working on on kind of the final solution
that we that's a very unfortunate term
sorry we started working on the solution
we came up with that we thought would
actually solve the problem in a good way
and and continue doing rapid releases
through the whole thing we through the
whole project we we did multiple
releases a day every single day
including to production once we had a
production to to release - so that
brings me up to the the part where I'm
talking about how we actually sold is
maybe I should stop here and ask are
there any questions at this point
fantastic I love when there are no
questions because that means that I have
been crystal clear in my explanations
right okay so our solution at a very
high level what we ended up doing was
was quite simple well simple once you
know about it we took a lot of different
databases out there because we also
wanted to use as much existing data and
and there are lots of really good
databases of what genes there are or
what drugs are available and things like
that however they're all good but
they're all incomplete or has a lot of
errors in them so we ended up having to
use several databases for each specific
type of data we needed because we had to
you get to a point where they
complemented each other and we got
better data quality than each one of
them could provide on their own the the
problem is that this field is still a
lot in the it's still a research field
and if you're a researcher working on a
gene database for example you can write
a paper and you can put together a
database and you don't really need to
clean up the data that much you can just
say ok this database has 20% error it as
long as you documented in the paper it's
fine now I'm being a little bit
facetious but that's what it feels like
when looking at some of this data of
course that kind of error rate doesn't
really work if you're doing clinical
work so we had to clean it up we ended
up with I think between 15 or 20
different databases that we unified and
normalized because as it turns out in
the field of molecular biology even
between different databases the
different databases didn't really use
the same terms to mean the same thing in
many cases and even when the domain
terms were the same sometimes you had
cases where I typically example that I
remember so a gene can call me different
variants it's something called
alternative splicing and sometimes
different researchers disagree on
whether something is different genes or
different splicings of the same gene and
it's really a
distinction but in one case a fairly
common gene called ugt1a1 we found in
one place had nine different splice
forms but in the other database it was
represented as nine different genes
instead and we ended up finding this
kind of stuff over and over again all
over the place so we had to unify
normalize and do a lot of processing on
all the data so we had three different
types of data we dealt with we dealt
with all the data the same way so we had
patient data which is all the biological
data all the patient history data all
kinds of things like that
we had reference data which is all the
data bases that contained gene
information or information about drugs
and things like that and then finally we
had this thing called experience data
and I told you the information is out
there well that information in the form
of research papers the clinical trial
data and things like that is not exactly
in a useful format to to actually ingest
and automatically into a system so we
had that formatted and kind of
synthesized into something that we could
use and that's the experience data and
what experience data really is is a
connection between a specific treatment
a specific mutation and the expected
outcome from that so for beer as v600e
which is a very very common mutation the
drug X is really good or the drug Y
provides toxic toxic interactions and
together with that we also got the
information about if it was from a
clinical trial how many people were in
that clinical trial and what kind of
clinical trial was there and so on so we
could because all of these experience
data we had multiple lines for each
combination and sometimes there was
conflicting information some sources
said this was really good and some
sources said this was really bad so we
had to figure out a way of actually
rolling all of that information up into
one as well so we take all this data we
put everything into a graph database and
then we had some biology I think almost
all of the code is under the small
heading of model biology but
that's really what we were doing and
incidentally this is why the approach we
were doing is almost the opposite of the
the the Watson approach for doing for
understanding biology and things like
that were a very statistical very I
don't know a very non specific version
of understanding is applied to a
specific field and in our case we
couldn't have done it without actually
building in a lot of understanding of
biology so we spent a lot of time doing
that and finally so all this stuff like
the patient information we had the this
like we got mutations from our patients
that information was very raw it was
basically as close to the biological
level as possible but that didn't really
make it very easy for us to compare and
use it with the other databases so we we
used a recursive process where we just
enhanced information we had with what we
could deduce at that point and then we
did that over and over again until we
got to a point where a lot of the data
was really high level and we could
compare it and finally we could connect
it up with the treatments when this is
really a nice big fat graph query where
we took the treatments and find the
relationships between treatments and
biomarkers and once we got to that point
we got a list of all the treatments and
once we had a list of treatments we
could we could do we have a very simple
algorithm to to rank the different
treatments and out of it and I'm not
actually going to show you the user
interface because the user interfaces is
really boring it's just a list of the
treatments and the order in them in
which they are applicable so this is
really where we go to and we used a lot
of different technology I think I know
introduced this during this morning as
using a lot of languages and and yes we
did use a lot of languages because they
were the right things to use but the
primary text stack that we used that
most of the code was written in closure
and in and the date the graph database
was neo4j and then we used JRuby well we
actually used Ruby but we deployed it on
JRuby
they turned out that for the things we
were doing JRuby was between 10 and 20
times faster than regular Ruby so we
felt like that was a good choice our
front end was written in CoffeeScript it
actually started out being written in
JavaScript but halfway through the
project we got really bored with writing
all the ending curly brackets so we used
CoffeeScript as a slightly more succinct
version of JavaScript we deployed using
Sinatra and composure as a front end
things and everything was some jetty all
over the place so I mentioned that we
used a graph database I want to talk a
little bit about that because it was
actually one of those choices there was
not I haven't seen I've spent a lot of
time before this product just looking at
the different how people model biology
how they deal with biology and up until
I don't think I've seen almost any
anyone applying graph databases to the
task of modeling biology at this level
and it ended up being really really
useful for us to do it and a lot of the
intelligence of how our solution works
actually has to do with how we structure
data in the graph database now we ended
up having maybe 15 different node types
and a lot of different types of
relationships between the different
nodes in order to get to a point where
we had that data but the thing that made
a graph database really really
convenient for us was also that we
continuously evolved the database we
continuously evolved our understanding
of how we should format the graphs and
neo4j made it really easy for us to
evolve this understanding we could just
add the new edges and then later on when
we really knew that we didn't need the
old edges we could remove them if we we
had several we have several cases where
we have duplicate information where we
structure the data in two different ways
in order to make it much easier to do
other kinds of queries depending on
depending on what we want to do so data
modeling ended up being much more
convenient in this kind of format and
the queries were really fast the query
language oh yeah
so neo4j comes with cypher you can also
use Graham
those are the two main choices now
cipher is a nice language for the kind
of queries we wanted except it's string
based so you put in strings and out you
get query and we were doing all of our
work in closure and we really wanted to
live in more flexibility so I got really
sick of composing strings for doing
queries so instead I wrote a small query
language on top of the back end of
ciphered that that we use to to really
good effect because then we could embed
things like closure functions inside of
the query query so the query could
actually execute closure on the fly
while it was paging through the
materials and things like that it looked
like it was executing it only on the
actual neo4j server but it ended up
actually not being that way because we
could compile it to a very convenient
in-between form cipher is implemented in
Scala so we had closure code that
generated Scala code that was kind of
interesting I haven't done that before
but yeah and I mean this is a small
investment it was a few days of time to
get to this point and it gave us
significant improvement in capability
through the whole project we had a good
infrastructure we used Amazon AWS for
our infrastructure as I mentioned before
we started out one of the first things
we did on the project like day 1 day 2
was to put up the infrastructure so that
we could get to a point where we could
deploy really quickly immediately so the
whole infrastructure was self-serve
everyone could do anything by by just
executing a few scripts we used puppet
for describing our environments we used
bo2 and fabric to do the provisioning of
the different of the different
environment we had some custom
provisioning code written in Python to
deal with things like switching out
elastic IPS and and working with the
whip data volumes and things like that
and then yeah ok so yeah and the whole
project was basically we had lots of
different repositories all of them had a
single script called go that was kind of
the the main entry point because after
that we used whatever tool was the the
right tool for the
language so if it was a Reba project we
use Drake if it was a closure product we
used lining in and so on and so on but
we used to go script to make it possible
for our continuous delivery server to
actually know where to stalk no matter
which one we had and the go script also
contains self installation using setup
tools in virtual enman and a lot of
other tools to make it possible for
anyone to download the code and have no
way no installations whatsoever and it
would take anyone's devil machine up
from scratch using these tools we also
were planning on make it possible to run
vm locally with this but we actually
never used it it was so much easier to
just you see ec2 for all the things we
were doing wondering am I gonna talk
about that later sorry I I don't know
what I have on slides next slide so we
used go as our continuous integration
server a continuous deployment server we
started with Jenkins
it was absolutely hellish to get
continuous delivery working on Jenkins
at this point I just I would recommend
if you want to do continuous delivery
just don't bother with Jenkins it's not
worth it and it's never gonna be as good
as you have taking a tool that is built
for it from from the ground up so yeah
so we switch to go and life was happy we
had a go master that was also fully
described in puppet and and could be
built from any dev machine and then we
had self-serve aged agents that could be
completely picked up from the fly
whenever we needed a new agent it was
like a five second thing so we had oh
the other thing about go so we had a lot
of data processing to do and we had a
lot of different things that we needed
to do in the process of building
products so we had separate pipelines
for the ingesting and and massaging the
data into the right form and then we ran
some tests on it and then we packaged it
all up and when we another pipeline
could use it for whatever data they
wanted that also meant that we could put
together custom data sets for specific
testing tests and so on
yeah so and we had separate pipelines
for all the various components of course
and then a final pipeline that that runs
the provisioning and installs all the
components and then we also used go for
doing self tests of our infrastructure
to make sure that everything was
actually
running and so we were actually running
go pipelines against our production
production service to make sure that
they were still up and and of course we
had a components on the on the deployed
machines as well too that knew how to
check itself for consistency and so on
so on every deploy to machine we
provision a new ec2 instance we copy all
the keys attached of data volume clone
from snapshots we install puppet we copy
old puppet manifests we apply puppet and
then we take all the RPMs that we
created from because we put all of our
things including the database we
actually put in separate rpms and then
we install all those rpms and then we
store the server's then we run self
tests against the server and then we
associate the elastic IP with the new
box and then we terminate the old
instance now notice this is what we do
an old deploys so we actually have
completely immutable servers we never
upgrade or use anything else we just
throw them away because it was Messier
to try to deal with the upgrade process
than to just install it all from scratch
anyway and the deployments were like
less than a minute long anyway so it
wasn't really a big problem we used a
lot of different monitoring tools but
primarily PI which we use that's the in
graphite really liked it especially
stats the is so easy to build build in
support that it's just like something
you should do in five minutes and then
you're done and then you have that
forever I'm on it and and started
logging and started shaking using go so
the data ingestion I talked about that
was an interesting process because we
had a lot of data millions and millions
of lines of data all of them in
different formats all of them with
different problems with them so we ended
up writing a lot of Ruby code for that
and we choose Ruby because for as the
data like just munging perspective it
was easier to get Ruby code up and
running and Ruby code that could read a
lot of different formats now we still
had a problem how do we get the Ruby
code or the the output of the data
ingestion into the database our our
first solution used the Ruby neo for DES
libraries to create the database
directly that ended up being annoying
because of verse
q libraries and the neo4j libraries for
Ruby are not actually very fast so the
second thing we did was to create a data
description language basically a DSL
that the Ruby code emitted that was a
unified description of all the data we
had in the system now that data
scription layer or the data description
language was then used to create the
database from it so in real terms our
data didn't live in our neo4j database
it actually lived in our data
description language and we that was one
of the reasons why we could change the
neo4j configuration to graph
configuration really quickly because we
could just change the code that put it
into the database and then rerun the
whole thing now what we actually ended
up doing was to create a data
description language that also happened
to be closure
so the actual DSL was closure code that
knew how to put itself into neo4j that
ended up being a little bit slow once we
started getting millions and millions of
lines of code because compilation limits
and stuff like that so we used the same
format but we don't have a very basic
interpreter to do the data ingestion we
had a one page app that was
communicating completely over JSON with
the backends so the only thing we served
as HTML to the front-end was just the
initial HTML piece that loads to
JavaScript and then everything was for
the front-end was done in JavaScript so
we got to see we basically defined all
the services as we were going along but
everything was accessible to JSON
interfaces as well and this made it much
easier to run tests on it because we
could we could test the front-end
separately by testing the JavaScript and
we could test the the rest of the system
by by hitting the JSON from JSON
interfaces that was quite nice mmm yeah
so we ended up with a lot of different
languages in our architecture we had our
own internal dia cells we had Ruby and
enclosure we had a lot of Java and a
little bit of Scala most of it used the
fact that we were running on the JVM to
interact but we had at least four or
five different services that were
running a separate services separate
Jetty's that we were standing up
communicating
only through through restful api and
that happens it happens several times
during the product we were not happy
with a piece of something and we ended
up completely rewriting it completely
different way maybe using different
libraries or different languages and so
on and we stood it up again and the rest
of the stuff all continue working
because we were using the same
interfaces I I think it depends on what
project you're on I think in something
like this this is an extremely
complicated problem we ended up having a
lot of closure code a very dense closure
code in order to describe the biology
and the overall project we were remember
we were four developers working on this
for a year so we didn't really have a
big team and we ended up coming fairly
close to a solution and a solution that
is actually running in production right
now for a varying cancer and I think one
of the reasons why we could do that was
because we used to write languages for
the right job and we didn't really shy
away from adopting a new language when
it was the right time to do it and oh
and and another part of the the whole
language thing the other three people on
the team had never done any Ruby had
never done any closure at all so I was
the only one with experience in all the
languages we were using and through
really promiscuous pairing we ended up
having a team where everyone were really
really comfortable in these environments
really quickly I don't think I mean when
you have the discussion about different
languages and things like that you
always get back to the question of
whether your team will be able to take
it and I think that actually after this
experience I'm more prone than ever to
say that actually this is something that
most teams should be able to deal with
as long as you do the other things right
as long as you as long as you focus on
code quality as long as you focus a lot
of pairing as long as you have good
mentorship in line and as long as you
have someone there that can teach some
of the aspects of a language so I think
this is a key to our success in this
regard we used a few internal dsls I
mentioned the data description DSL we
also had a small DSL for communicating
with our
demain experts about some biology
aspects but yeah now this was not
something we used we we used it a little
bit but we most of it was actually done
like internal dia cells because we could
do them in enclosure Makris that ended
up being very very succinct and very
easy to manage at the same time so some
conclusions from this experience is that
actually small teams win over large
teams in many regards especially when
the problem is really complicated and
when you have so complicated things that
you have to communicate that actually
the communication is gonna be more of a
barrier than learning this is why we
ended up seeing ourselves as domain
experts and business analysts because
honestly if we had had separate bas that
were communicating to the devs what they
needed to understand in order to do it
the communication of of all that data
all that information would most likely
have taken longer and been less
efficient than just us learning it
ourselves directly and communicating
directly with the with the client I
think if we had been eight instead of
four we would not have been as fast
using the right language I've already
talked a little bit about this now
molecular biology is a fantastic field
it's very very interesting it's very
intricate it's a field under strong
development during the June 2012 we all
had to read lots of research papers
because there was continuously coming
out new results actually changing what
we were doing or adding more information
and new understanding to what we were
doing now research research is an
interesting field especially when it
comes to something complicated like
molecular biology they desperately need
programming programming is at the core
of a lot of these things but there are
not that many good programs that are
actually working on it instead the the
people who write code are usually
researchers who are really good at
molecular biology and they just want to
get something working so they get
something working and then they continue
on to the next thing they want to do
from a research standpoint and I'm not
judging that in any way I think that's
the right way to do it but that also
means that there is a huge opportunity
for good
programmers to get in and work on
helping building really good systems
because that that's where the the the
weak side is in a lot of these fields
and especially molecular biology so yeah
I think you should take a look and maybe
think about going into it continues to
livery I don't think I could ever start
a product without having C D at the core
of the product it's it's just
inconceivable how you can how you can
build software without continuous
delivery at at its base at this point
and when it comes to cancer this is not
the final solution the final solution is
going to arrive in maybe 10 15 years
when when we have a complete
understanding of how the genome works
right now we understand a very small
part of it but once we get to that point
we might be able to have a better
solution to cancer up until that point I
think something like this not
necessarily this system but this kind of
approach is going to be the right way of
tackling it and giving the better
treatment options to people and and what
I didn't mention is that a part of this
the experience data I was talking about
for bootstrap purposes we were using
research and and clinical trial
information but a big part of our plans
was to make it self improving so once it
started giving recommendations and
treatments you could take back outcomes
and start improving in that way as well
and sooner or later you will get to a
point where actually the system has
learnt enough to be able to improve just
by its previous recommendations and so
ya know so it's an interesting field it
there is a lot of stuff that's changing
and at this point I'm pretty hopeful
that we will get to a point where cancer
stops being a real problem mmm oh and
and genomics I talked about in the
beginning the hold
RNA protein DNA stuff all of those
things are only they only cover about
1.5 percent of your genome all the rest
of it is doing other things that we are
not very certain about so when I say
that it's going to take some time to get
to the point where we understand the
whole of the hill whole of genomics
that's because there is a
lot of open space to find things in so I
think I have time for a few questions do
you have questions ok great
how difficult would it be to add a new
cancer type well the system the backend
system is completely independent of
cancer types the reason why it's ovarian
cancer is because because we had an
organization that was willing to help us
take the I see I take the legal risk of
putting this into into production
because the the problem is with
something like this is that
traditionally you have to go through in
the u.s. you have to go through the FDA
and they're quite stinging about all
kinds of things especially they're
stinging about stuff that uses
algorithms so we haven't had an
organization that helped us take the
legal risk and act as intermediaries for
the system so that that was the only
thing that made this start with ovarian
cancer ok okay so why did I choose
closure instead of another language and
how did it help me well yeah so closure
I I had a very short list a thing of
languages when I started this project I
I knew roughly what I wanted and needed
closure I'm a lisp ER so I naturally
reached to lisp for my most complicated
problem but I think that the thing that
made closure the right choice was
because there are a few things the
closure gives me that no other language
really could have helped me outside of
the JVM actually one of the constraints
was using the DVM in order to get access
to all the libraries and performance in
GC and all those things but Lisp helped
with macros the this system uses macros
really heavily and I think it's one of
the places where getting the code size
down to a manageable level was
impossible without using macros the
other thing that closure does really
well is the data structures the
the operations for managing data
structures and doing all kinds of
generic operations on any kind of data
structure it's much more powerful than
basically any other language in
mainstream news right now so those two
things were the main things that I
really really cared about the fact that
it ended up being a functional language
didn't really I'd it did it doesn't
butter me bother me but I don't think
functional languages are the end-all
solution to the to the software clock
crisis either the fact that it had some
very useful threading constructs and
concurrency constructs didn't really
help that much either a lot of what we
did ended up I mean we did basic
parallelization but we didn't really use
any of the fancy structures for it
we didn't really need it and what was
funny about this is that a lot of these
things look like they're really
complicated but if you if you put the
logic in the really hard work at the
right place
the end result ends up being fast so we
did a lot of calculations at runtime and
we usually got back answers within half
a seconds of time and that was doing a
lot of really interesting work to get to
that point the last question okay
in hindsight what would be the one thing
I would change well I wish I had those
three months of figuring out the problem
back because of course that helped
finish the solution but if I were
starting right now I would know how to
architect the problem from the beginning
I mean that that's really but that's not
something I could have changed if I go
back to beginning of 2012 I couldn't
have changed that because I needed that
time to understand the problem and
understanding what the solution should
look like I don't think that there is
any big thing that I would change there
are lots of small decisions that I mean
you can always tweak things and there's
always like the second time you build
the system if you will always make very
different choices but the broad strokes
of the architecture I'm very happy with
actually okay thanks for your attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>