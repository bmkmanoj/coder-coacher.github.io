<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • Disband the Deployment Army • Michael T. Nygard | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • Disband the Deployment Army • Michael T. Nygard - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • Disband the Deployment Army • Michael T. Nygard</b></h2><h5 class="post__date">2013-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Luskg9ES9qI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Michael Nygaard I'm a CTO of
relevance and author of the book release
it and I'll be talking to you this
afternoon about the deployment army and
why we should get rid of them what I
mean by a deployment army is something
like this this is a picture that a
friend of mine from a former client sent
me now what you see here is a conference
room with about 40 people in the
conference room some of them are behind
the camera somewhere in front of the
camera in addition to the people in the
conference room
there is a Polycom phone open to a
conference bridge with about an equal
number of people on the phone as there
are in the room this deployment began at
4:00 in the afternoon
actually it began a little bit before
that when they sent some of their people
home to take naps so they could take the
late shift because this deployment will
continue until 4:00 in the morning some
of the notable steps during this
deployment involve smoke testing by
business users at 1:00 in the morning
where they have scripts of instructions
that they're supposed to click around
the site and make sure that everything's
working okay and of course you can
imagine that at 1:00 in the morning not
everyone is at their full alertness and
so some of the people executing some of
the scripts report random errors that
aren't actually errors but our known
conditions from years ago some people
click incorrectly and report errors that
are their own errors not the application
errors it's a very noisy process it
involves a lot of people in fact they
have so many people that they have steps
in the plan for who to send out to get
food at various points in the night and
they've added steps in the plan as time
has gone by to make sure that someone
empties the trash cans from all the
containers from the food that they bring
in to feed all of these people this is
the deployment army it's a remarkably
persistent structure it is also an
anti-pattern it develops because of this
belief this self-fulfilling prophecy
the self-fulfilling prophecy says
we have risk created by these large
infrequent releases a certain amount of
unknown code or inventory that's being
deployed and this risk produces
threatening situations threatening to
the business and threatening to
individuals who get blamed if things
don't go well during the deployment
they're ingrained response to this
ambiguous or uncertain environment is to
apply people to it and so they have this
belief that the deployment army is the
best way to react to unknown situations
now it is a self-fulfilling prophecy
because humans are good at reacting to
unknown or novel situations much better
than automation or scripts in fact but
because this deployment army is there
it's a very costly process you can
imagine if we've got approximately 80
people spending almost a 24-hour period
doing these deployments when I walk in
and say you ought to be doing
deployments once a week instead of once
every quarter they kind of look at me
like I'm insane because given their
context it would be insane to say you
should do this every week but that high
cost of deployment actually reinforces
this whole cycle the high cost of
deployment means you do less frequent
releases and the less frequent the
release is the larger it's going to be
and therefore the riskier and more
threatening it's going to be creating
more need for more humans so you can see
how this spiral works eventually it
produces paralysis if you go around this
loop often enough you reach a point
where you can't add any more people you
develop a playbook that's you know an
Excel spreadsheet printed out that thick
eventually you just hit the point where
it's impossible to add anymore to this
process and you feel like you can't take
anything away from this process and then
you're stuck I'm going to talk about
what we can do about it I'm not all you
know pessimism and gloom and doom we
there are some solutions but first I
want to look at why this situation comes
about in the first place why it persists
and why people perceive a conflict now
I'm a believer in the Theory of
Constraints and in the Theory of
Constraints there's this tool called the
evaporating cloud it's a great name
your problems simply evaporate the idea
is that you try to identify the root
cause of a conflict based on conflicting
assumptions rather than real conflicting
imperatives and so the conflict we're
going to look at here is achieving
financial success in your company along
one branch of this cloud we have an idea
that we want the benefits of being agile
and we want continuous integration all
the way through to production that's
what we're here to talk about that's
what continuous delivery is all about
that's the cheerleading that we've done
and we've heard about for quite some
time and that branch looks something
like this
financial success is produced by high
velocity and short cycles which means we
need to do more deployments but there's
another branch of this cloud which says
financial success is achieved by
avoiding losses from downtime bugs
problems with legal compliance brand
brand reputation loss and so on and
along that branch of the financial
success path
we see we need to reduce risk which
means we should be doing fewer
deployments well when we put these two
branches together we see the conflict it
looks like we have two things that we
both need or that we need both of them
to produce financial success I contend
that this is not actually an inherent
conflict we don't need both more
deployments and fewer deployments we
need to look at these assumptions about
low risk and high risk and so we're
going to do that when we talk about risk
some people use a very cloak wheel
definition of risk risk means there's a
chance that something bad is going to
happen risk means I feel nervous or
uncomfortable risk can even just be a
scare word that you throw into a
conversation the way that some
politicians
like to throw it into conversations
label something a risky scheme when you
just don't want it to happen but have no
other argument against it we're going to
use a more nuanced definition of risk
we're gonna say risk is the expected
losses from undesirable events and I
mean expected losses in the statistical
sense if I do this trial enough times I
can average out the times that it goes
badly and I lose something and the times
that it does not go badly and I don't
lose anything so we reach this notion of
expected losses and we can look at that
as our risk exposure risk exposure is
often expressed in terms of the annual
loss expectancy or al annual loss
expectancy is used in actuarial markets
like life insurance and health insurance
it's used in security management for for
IT security as well and to arrive at the
loss expectancy we basically run a small
equation that says we have a number of
events at which a loss could occur we
have a probability of any one of those
events being erroneous thus producing
the loss and we have a cost each time
one of those events does occur so the
number of events times the probability
times the cost that gives us our
expected losses this is a useful
definition of risk because if we have
expected losses in some kind of economic
terms then we can decide first of all
how do we minimize it and more crucially
is the loss actually greater than the
gains that we could achieve by doing
something else humans are very bad at
economic decision-making on an intuitive
basis and we've had study after study
that shows this so we will tend to be
loss averse if you ask someone whether
they're going to bet a 50/50 chance of
winning a million dollars or hold on to
a hundred thousand dollars they'll hold
on to what they've got rather than
taking the chance even though
mathematically they should take the
chance
so if you simply present the loss people
will try to avoid the loss every time
but if you present the loss and you
present the gain available then you have
a chance of getting a rational decision
after plenty of meetings and plenty of
arguments and proof from other places
and so on let's do an example of this
kind of an exposure calculation let's
suppose I'm running a retail site and I
have some kind of a bug in my checkout
process I have a lot of checkouts per
year something like you know 10 to the
6th 10 to the 8th I can't actually read
anymore 10 to the 8th so we have a lot
of checkouts happening per year but it's
not a very common bug it's something
that occurs just once in a while and so
the the odds of it occurring are small
we have a large number of events and the
cost of each of those events is
basically whatever's in the cart and we
can say there's an average value of a
shopping cart call it perhaps 25 euro
and I can multiply all these together
and say how much I should be willing to
pay to fix this bug how much am I going
to lose on this particular bug per year
it works out to about a hundred and
thirty-one euro per year so how much
should I be willing to pay to fix this
bug well if the system is going to live
for one year I should be willing to pay
only up to a hundred and thirty one euro
to fix the bug
otherwise I'm paying more to fix the bug
than I'm losing from the bug if my
system has a design life of five years I
should be willing to pay about 650 euro
to fix the bug because I'll save that
much over the five years of the system
and if I have a 10 year life and and so
on you can work out the expected losses
from this bug over time and then balance
that against what it costs to offset it
so this is an example of dealing with a
small and relatively frequently
occurring event so these checkouts
happen all the time the cost of each one
is relatively small
there are similar methods for dealing
with the really large horrible scenarios
so there are people who who spend their
time thinking about wonderful things
like what would it cost if an asteroid
hit the earth I like to read these on
the beach during the summer time one
book I can recommend is called
quantifying and controlling catastrophic
risks the method is not dissimilar the
hard part is getting data because we
don't have a lot of asteroid impacts per
year to measure and measure the cost of
them and we expect that when they do
happen it's going to be very expensive
the author of that when John Garrick
also did a wonderful analysis on the
actuarial impact of nuclear war so maybe
you'll want that for your summer be
treating when we talk about risk and
loss we can start grouping the risk into
a couple of categories one of them is
compliance risk so this says there's
some regulation or some law or some
industry body and if I'm non-compliant
with it they can levy fines against me
the other kind is the technical risk
more like the bug and check out where
I'm producing the losses myself
compliance risk is sort of tough to deal
with
it tends to be industry specific and if
you're in one of these industries with a
lot of regulation you're probably
dealing with approval bodies third-party
testing agencies and the like and
there's a few things you can do you can
deliver into a certification environment
and you can deliver continuously into a
certification environment and then get
certified once a year or whatever
frequency that the certifying agency
allows you can make sure that you pass
on your first time through the
certification but that's probably about
all I can say on the compliance risk
it's simply a friction in your process
that you have to deal with I can say a
lot more about the technical risk
because that's much more under our
control and so that's where I'm going to
spend the rest of the the session
focusing
when we're managing our technical risk
particularly related to delivery we're
gonna think about our Ellie equation and
look at how each of these variables is
affected by doing continuous delivery so
the number of events is going to
increase that's sort of a given that's
what we will that's actually what we
want to achieve we want the number of
events to go up so we can work on
decreasing the probability of an error
and decreasing the cost when an error
does occur and that's where we need to
focus so I have a little comparison
chart that we'll come back to and
revisit a few times we want to make sure
that the decrease in the probability of
error and the cost of an error is large
enough to offset the increase in the
number of events that we're going to be
doing and so let's look at where the
probabilities of errors come from and
see what we can do with it certainly
defects in code are one of them but
that's not the only place you can get
errors because the code can all be fine
and your unit tests can pass but then
when you put everything together and you
push it out into the environment
maybe things don't work out there so
that would be an error in assembly you
left your development configs in the
package that you put into production and
so you're trying to connect to a
development database that's not
accessible from the production
environment that would be an assembly or
packaging error you can also just have
an error in executing the change and
when I was doing IT operations this was
the most common kind of error that
occurred we had something like 50
percent of all of our outages
immediately following a change to
production now
our changes were like the deployment
army so it was all manually executed it
was an administrator copying commands
out of a playbook and pasting them into
a terminal that's actually a good
practice the copy and paste you know
everyone style you pick up some smart
quotes from Microsoft Word but most of
the time copy and paste is relatively
safe because you can't typo something
you can't accidentally put in the wrong
hostname just out of finger memory habit
the defects in code are one of those
things that accumulates over time the
longer you go between deployments the
longer you build up inventory the more
opportunities there are for defects the
more untested code you've got and we can
look at this with something like a
continuous flow diagram a continuous
flow diagram comes from the the lean
school of process control and basically
it looks at things entering a process
and things exiting a process but instead
of counting sort of the number of things
in process and having the the wavy line
like that you simply add up one line
every time something enters and you add
up another line every time something
leaves it looks something like this what
we can see here is the red line
representing code that's been checked in
it's been you know unit tested and
feature complete and it's done to your
team's definition of done it increases
monotonically we never subtract from it
it's continuous flow so we keep going up
what we have on the black line is
deployments so nothing nothing nothing
and then all at once there's a big
deployment with a whack of code that
goes out and then nothing nothing
nothing again for a while and then
another whack of code goes out and the
nice thing about a continuous flow
diagram is that you can just look at the
vertical distance between the two lines
and say that is my accumulated inventory
that's how much work in progress I've
built up that hasn't exited yet and I
believe that accumulated inventory in
code more or less linearly equals
accumulated risk in code so this is a
diagram of a batch deployment or a
punctuated deployment if I take exactly
the same flow of features so the red
line is going to remain the same
but I just moved to a daily deployment
following one day after the code is
checked in the CFD looks something like
this you if you're in the back you might
not even be able to see the amount of
accumulated risk between these two lines
I've done nothing else but change my
deploy
frequency so it more closely matches the
frequency of checking things in but I've
dramatically reduced the amount of
accumulated risk that's built into my
pipeline at any point in time this is
the type of thing that we get with
companies like Etsy and Flickr and
SoundCloud and so on who are doing
continuous delivery and getting their
code out there and validated in
production as rapidly as possible
because production truly is the only
place you can validate all of your code
one way to achieve this is with the
build pipeline idea some of you have
probably seen this before if you've read
jezza's book you've definitely seen this
before the notion of a built pipeline is
you're essentially extending your
continuous integration flow which is
this and you're making that reach out
all the way to your QA environment and
eventually to your production
environment deploying as you go and
applying tests for things like
non-functional requirements in an
automated fashion as you go through and
you've tagged your tree and you log and
you measure at every step along the way
so when it fails you can tell where it
failed and why and go back and either
fix the pipeline in about half the cases
or fix your code in about half the cases
now this is a fairly lengthy build
pipeline and it looks a lot like what
you would do manually it's a good place
to start by taking what your manual
deployments would be and just
translating them into an automated build
pipeline as you get better at this you
may add some stages or you may remove
some stages you may change the way that
you're assembling we can look at
reducing our building assembly errors by
applying some some practices we
certainly want fast tests so we get
feedback right away a very good practice
for reducing build errors is the
building servers clean from a definition
in version control so using something
like chef puppet or cfengine so that you
can commit all of your server
definitions to version control and keep
those in sync with the needs of your
application
definitely building the application from
version control instead of building from
you know polluted dev box good practice
I hope everyone's doing that already if
not that's an easy first step to begin I
still occasionally see environments
where people are promoting sources and
building on each target environment this
introduces a vector for change in that
the target environment can have a
different dependency set than your
production environment or than the
earlier environments where things have
presumably worked and so you don't
really want different versions of
libraries creeping in different
snapshots of jar files and that that
type of thing creeping in so building
binaries once and then promoting the
binaries is a good practice and
certainly deploying the same way
everywhere helps I've seen a lot of
situations where developers don't do a
deployment on their own machine they're
just executing directly out of the
directory that they're they're building
in and then they use something like
Capistrano to deploy to a staging
machine and then they use a virtual
machine snapshot and deployment to reach
production and so we're using three
different mechanisms in three different
environments that more or less maximizes
the opportunity for assembly errors to
creep in so we don't want to do that we
want to build the same way and deploy
the same way everywhere I know I already
said fast tests fast tests really are
pretty important here so I believe fast
tests are an essential practice some
shops continue to do feature work while
the build is broken I don't advocate
that I like to have a clean head of
master and if it's if the head of master
is not building correctly you should not
be committing new feature work you
should first get back to a green build
and then continue feature work you can
commit feature work on branches but
first priority on master is fixing
master as you get more advanced with
your builds you can look at enforcing
some of your architecture guidelines or
standards in the build process itself so
one thing that we're taking an increase
look at is dependency management and the
things that we're pulling in it's been
fairly common for us to have a developer
pull in a jar file so he could use one
function from one class but then that
jar file brings in all of its friends on
the bus so we end up with a stack of 40
jars sometimes including entire other
languages just because we needed one
thing for example I'm doing work in
closure and I discovered JRuby in its
entirety existed in one of our projects
because a transitive dependency included
JRuby for its own interactive shell some
organizations are very careful about
what licenses they'll allow and so this
is another place where you can enforce
license restrictions if you have a
separate development and operations
group it is really important to get
operations involved in creating the
deployment scripts so that same company
that I showed you the picture of at the
beginning spent something like a million
and a half dollars creating automated
deployment scripts the development group
handed over the automated deployment
scripts the Operations Group said well
that's great but we don't like that tool
and so we're going to take those scripts
and use them as specifications and then
we're going to write our own scripts
in the tool that we like so this is an
enormous waste of money and the odds of
those two things the the two sets of
scripts doing exactly the same thing are
pretty small this could have been
avoided had they gotten more buy-in from
ops at the beginning of the effort and
worked directly in the tool that Ops was
eventually going to support and of
course if you haven't gotten the the
message about deploying from head of
trunk deploying from head of trunk is a
really good approach I favor feature
branches as short-lived branches but
then reintegrate against trunk some of
you may have seen this get flow article
that talks about having a development
branch and merging from master into
development and deploying off of
development I I found that that
creates too many opportunities for error
people will merge things into
development because they think it's just
development but if you have a culture
that says master is deployable everyone
will get that in order to do these there
are a few things you're going to have to
setup in your environment I definitely
think you need a package repository you
need someplace that you can put your
binaries that they can be safe they can
be checksum they can have version stamps
they won't get tampered with you you
need logging for sure you need that for
good operations anyway and you need
metrics to know if a change that you
make is harmful and to know as rapidly
as possible if it's harmful and I'll be
revisiting the subject of metrics in a
little while
so that's packaging and assembly errors
the next kind of error that increases
our probability of errors is deployment
itself and so let's look a little bit at
ways to make deployment safe having 40
people working off of a playbook where
there's a step that says you know we're
going to call the DBA and tell the DBA
to shut this off and then we're going to
call this other person and tell them to
do that thing that that's sort of the
opposite of making deployments safe that
kind of PlayBook evolves because someone
really wants to be like NASA and Mission
Control actually the best kind of
deployment is one where you don't need a
room full of people reporting to each
other all the time
the deployment actually should be kind
of invisible like nobody should even be
able to tell when it happens except by
looking at a graph so there's a few
patterns that you've heard mentioned
today and that you've perhaps seen
elsewhere one of them is this notion of
canary releasing so in this pattern you
take a pool of your capacity and you
make it the test pool or the dirty pool
and you deploy your new software to
there and see how it does that's called
a canary from the the old story of
bringing a canary it coal miner is
bringing a canary down into the coal
mine because if there was a buildup of
carbon monoxide gas the
canary would die first before the miners
so the idea here is if something's wrong
the canary pool shows it first and the
users who are hitting the canary pool
experienced the problem first so this is
reducing the number of people exposed to
a problem but it doesn't actually
eliminate the problem you do have to
assume that a certain number of errors
are still going to get through here so
we're talking about reducing the
probability of error but we can't make
it zero another approach is something
that jezz has written about called
Bluegreen deployments this is where you
have two pools of capacity yeah you
split them up into arbitrarily named
blue and green pools you deploy your new
code into one of the environments you
validate it you can run some smoke tests
you want some monitoring and measurement
on both and then at some point you
switch over to using that my favorite
tool for using that is this really cool
high-tech name and dependency resolution
service called DNS at this point Green
becomes your new staging environment and
it's the target of your next deployment
if you're in a cloud this is obviously
very easy to do if you're not in any
kind of a virtualized environment it's
going to be a little bit tougher because
you do have to have some spare capacity
laying around another style of making
deployments safe is using a virtual
machine-based deployment so in this case
you have a server not a dev machine a
dev workstation but a server that's your
dev environment and at some point you
say yes this is good I bless it and you
create a virtual machine based on that
image and then you use that virtual
machine to deploy into your higher
environments the key advantage here is
there's very little opportunity for
errors to creep in during the deployment
because you're not changing the
essential machine
as you move through environments the
next approach you can use for making
deployments very safe and predictable is
configuration management based
deployments so in this scenario I've
split up my capacity into the control
plane which manages the infrastructure
and the service plane which actually
does the work not a lot of work is going
to get done yet in this picture but at
some point I have a base image in the
previous slide I was showing you a fully
configured application in that virtual
machine in this slide I've got just the
operating system plus a configuration
management client it knows how to talk
to that config server and that's about
it so that basic machine gets
instantiated in the service plane and
the first thing it does is it talks to
the config server and asks Who am I this
is how chef and puppet work cfengine
also works this way so the machine asks
Who am I what's my role what are my
configurations it downloads those
configurations pulls packages from your
package repository or from the config
server and starts installing things on
itself if all of that succeeds it begins
reporting its metrics to your your
metrics server and at some point it also
needs to report its versions of
everything back to the config server and
it's really important to close the loop
here because kind of a common failure
mode is one of your machines doesn't get
updated and so it's actually running old
versions of stuff this is a really weird
problem to detect in production and so
it's good to build a page into each of
your apps that reports the version that
it's running and then collect all of
those on your config server so you can
look at a simple report and immediately
identify anyone that's running an old
version of the software and then you
blow that machine away and start up a
new one in its place
now in order to do any of these things
that I've just talked about you do have
to put in place some kind of a zero
downtime deployment mechanism because at
some point in time you're going to have
the new version of your system and the
old version of your system alive
together so it's important to do this in
a structured way so that you can handle
things like database migrations and you
can split up your database migrations
into a sequence of safe steps so that
you're doing additive operations first
you get all of your code rolled out and
then you do the subtractive or
restrictive operations so first you add
tables and you add columns with one
small change then you push out your code
with another small change and then once
you're satisfied that everything's
working
you push out another change that leaves
the code alone but drops tables that you
don't need any more drops columns that
you don't need anymore
adds any not null constraints that you
might want to apply at that point and so
you do these in phases as you do this it
is very important that you think about
how the different applications and
services are going to know each other's
protocols as well so maybe you just
build all of your applications to always
speak every version of every protocol
they've ever spoken you could do that
might be kind of a costly approach you
can build protocols that are forward
compatible so they ignore any unexpected
elements this is a good approach and you
can do it by explicitly negotiating what
protocol versions you're going to accept
to think about something like HTTP
content negotiation where you have
custom media types that represent
versioned
formats of your representations and then
you can say I accept version one and one
one and one two and then the server says
I'm happy to give you one too but I'm
not going to give you one one or one I
don't do that anymore
another very key issue with zero
downtime deployments is a decoupled
architecture I'm going to talk about
this more a little bit later so I
contend if we do these things you know
pick a safe deployment mechanism pick a
safe assembly mechanism create a built
pipeline that gives you visibility you
can actually reduce the probability of
error to something quite low but what we
can what can we do about the cost of an
event by an event I mean you know some
kind of a downtime or crash or bad user
experience this is where it gets kind of
interesting when you talk to people who
don't have a kind of quantitative risk
approach because they start to talk
about all kinds of scenarios and you see
this weird sort of Glee in their eyes as
they're talking about you know getting
on the front page of the newspaper and
getting called in front of your
legislative body to to testify about why
they leaked everyone's credit card
numbers or health information or this
kind of thing and and they have this
sort of you know apocalyptic joy as
they're talking about these things I had
a boss in a past job who referred to
this as the wouldn't it be horrible if
scenarios with the Whydah he's people
love to talk about whether he's in the
same way they love to talk about movie
plot threats of course it's not a
rational approach if you're engaging in
wiggies then the cost of any event is
unbounded right because you can't
imagine a limit to how bad things could
get and of course if the cost of an
event is unbounded then the things you
should do to prevent it is essentially
unbounded and you're going to end up
with 80 people in the conference room
and on the phone doing deployments for
24 hours the real cost of an event is it
is bounded it is limited it adds up from
several factors you have direct losses
so revenue and cost incurred during the
event you have some indirect
losses loss of brand reputation is an
indirect loss the cost of people's labor
while they're responding to the event
instead of building the next great thing
that's a loss but we can add all these
up we can measure them and we can work
to minimize them so let's look at how we
do that the first end best way to
minimize the cost of an event is just to
minimize the time to detect it so M T TD
M T TD is the mean time to detect the
event and it's shockingly high in many
cases so I've seen systems where people
are running synthetic transactions so
you have some service that you've signed
up with that runs load against your site
and they run a transaction once every 5
minutes but that transaction they're
running goes through a load balancer and
it gets directed to one of n machines
that you've got so the first time they
suppose you've put out one bad server
out of a pool the first time that
synthetic transaction happens they might
not hit the bad server and actually once
they eventually do hit the bad server
they usually retry a couple of times
before they say yes it's a real problem
not just a transient network issue or
failure to resolve DNS or something like
that and so with synthetic monitoring
your mean time to detect is usually on
the order of 15 minutes or more that's
kind of long in systems that I deal with
if you're using something like real user
monitoring where you're watching traffic
coming in from outside and you're
watching the effects of that traffic you
can usually see the event in a minute or
less so real user monitoring gives you
better mean time to detect
we can talk about different testing
methods and see what the meantime to
detect a problem with the different
testing methods is so I do unit testing
I think it's a big advance in our
profession but I have to say I don't get
a lot of confidence in the validity of
my system just based on unit tests it
tells me a certain amount but it doesn't
tell me it's actually going to work in
production so I have relatively low
confidence in its validity but it's very
fast to detect things so unit tests have
some utility integration tests take
quite a bit longer to run right I can
usually run unit tests in a matter of
seconds my integration tests take a few
minutes to half an hour or something
like that they do give me more
confidence in the outcome sometimes we
do this user acceptance testing where we
actually put the system in front of some
of the sponsors or stakeholders and
they're supposed to be the final step in
approval to make sure it actually works
that takes a very long time and I don't
think it actually adds much confidence
what I normally see in user acceptance
testing is a very cursory sort of you
know
click click glance at a few pages yeah
it's okay I don't see you know real
in-depth exhaustive testing a method
that does give pretty good results is
this real user monitoring and so I
definitely advocate measuring everything
your users are doing charting it and
find the charts that matter for your
environment in your business Mike
Brittain is going to be doing a talk a
little bit later he comes from a company
called Etsy that has blogged about their
cult of charts and so I've borrowed with
attribution some of their sample charts
so this is a chart showing technical
metrics and just at a glance you can see
that something very odd happened
immediately after a code deploy and the
number of PHP warnings went way up
another code deploy went out and the
warnings went way back down
this is a great example of detecting a
problem and correcting it pretty rapidly
so I would I would wager that the time
to detect was on the order of five to
ten minutes and the time to correct was
five to ten minutes this chart will show
you problems before anyone in a call
center will get a call and actually way
before synthetic transaction monitoring
will show you a problem here's another
chart that they've got this one shows
some metrics that are perhaps more of
interest on the business side as well
and again you can very clearly see
something odd going on in the middle of
the night there and so this kind of
visibility shows you errors faster than
anything else it's very easy to set up
charting and chart everything you take
your server with your stack of stuff you
add some open-source software like stats
D you do a little bit of internal
monitoring you send it all to graphite
and then you can get out your dashboards
and charts and information radiators on
the wall and everyone starts looking at
those things
there is one caution which is the first
time you make something visible it will
probably look horrible because if you
haven't looked at it before
weird things will have crept in and this
probably means that your your higher-ups
are going to look at it and go that
looks really strange please explain that
to me and so you'll get asked to explain
a lot of things that goes away quickly
don't let that scare you away from doing
this you can blend in external
monitoring with this and feed it into
the same system and everything that I've
represented here is approximately a
day's worth of work so it's really easy
to get going with this stuff the next
step in minimizing the cost of an event
is reducing the time to correct it or
the mean time to recovery
if you're looking at mean time to
recovery it comes from a few places
first you have to figure out what's
going on sometimes that's the fastest
part you look at it and you instantly
know what's happening you know no orders
are being placed sometimes it takes a
while longer I've had time to determine
the problem measured in weeks in some
cases but fortunately that was a very
intermittent problem it wasn't a huge
crisis issue certainly fixing the
problem and deploying a fix are parts of
the meantime to recover as well so being
able to make the change quickly and get
the change deployed quickly help you
reduce the cost of the event I think an
area that we don't look at enough is how
to reduce the scope of an event now you
remember when I talked about the canary
pool I said that the users that you have
hitting the canary pool are the ones
experiencing the the problem well that's
a small fraction of your users so you
can regard that as kind of the scope of
that event if something bad happens
there so we can ask how many users are
going to see the error split testing
like the canary pool or just a be
testing features is one way to reduce
the scope of the error we can also
reduce the scope of the error with a
dark launch where the only people being
exposed to the problem are sort of our
own people and friends and family and
people within our our circle of trust
I guess feature flags are a way to
reduce the scope of the error as well so
you can turn it on on a limited number
of machines and if something's going
wrong you can turn it off on the same
machines very quickly I always like to
have some kind of a remote control
mechanism built into my systems I don't
want to have a bunch of admins
scrambling to SSH into 20 or 30
different boxes and try to make the same
exact change to 20 or 30 different boxes
because they'll get it wrong in at least
one of those and it just takes time you
don't they the first box will get fixed
fairly
quickly by the time they get to the 30th
box 5 or 10 or 15 minutes have gone by I
like something instead where I can drop
a message in a queue and hum all the
apps in my infrastructure reading from
the queue and doing the thing to
themselves that the message tells them
to do whether it's turning a flag on and
off whether it's accepting load or
shutting off and shedding load another
aspect of the scope is how far does the
error go so if I have an error is it
confined to a single feature or is it
affecting all of my features
simultaneously this is where we come
back to the notion of the decoupled
architecture we can use bulkheads to
separate different pools of servers so
if I've got some kind of a distributed
bug that's causing machines to lock each
other up I can have different groups
that are incapable of locking each other
up and I can use circuit breakers to
prevent errors from jumping gaps in the
system across vertical tiers and
propagating outward into from from
service provider to service caller so a
couple of examples of what this
decoupled architecture would look like
this comes from an actual client project
essentially they had one big app there
were multiple teams working on the
independent components each of the teams
would check their code in and each of
the team's code was being built into a
module but then the modules were all
being glommed together in a single big
jar file actually I think it was an ear
file but effectively it's a jar full of
jars and then that same file was being
deployed to every app server in parallel
and they would all get restarted well
this is the opposite of a decoupled
architecture we can make it a little
better by doing the canary releasing and
saying I've got a little bit of my
capacity that's now my canary pool and
I'm still kind of assembling things the
same way but in this case at the very
least I can
reduce the scope of who all is affected
by this and so this is better it's still
not great but this is an easy step to
make and so if you're still in that kind
of monolithic you know doomed scenario
take this step and then move on to the
next step which looks like this
we've now taken the modules apart into
groups of related facilities and they're
each being deployed to their own pool of
servers at this point I can have any
kind of hairy threading bug in my
account management I can have this thing
calling through to some back-end system
that exhausts its thread pool and never
answers again and the worst that happens
is that I lose those account management
features but I don't also simultaneously
lose all of the threads that respond to
all the other features now one other
advantage of doing it this way by the
way as I depict over here on the right
is now that the communication is not
straight from the user to each of these
facilities I can also start saying
individual facilities have their own
canary pool and I can validate those and
then turn them on for a fraction so I
can make multiple versions of these
services coexist either for a limited
time or for an extended time if I want
to do protocol versioning in an easy way
now one of the interesting things is
when we talk about decoupled
architectures one particular group of
developers gets very high marks on the
decoupled architecture that we tend not
to give very high marks anywhere else
and that's the PHP developers because
their code is so UI focused and it
doesn't tend to get packaged up into
these artifacts within artifacts within
art artifacts with this stack of
dependencies it's relatively easy to
just push out a few files with a few
lines of changes there's not an issue of
restarting whole app servers with you
know 25 megabytes jar files so I think
we have to cut the PHP developers some
slack or give
some credit on the decoupled
architecture area so if we've done these
things I contend that we have reduced
the cost of an event and we've made it
quite low and I believe when we reduce
the probability of the error and we
reduce the cost of the event we have a
squared effect going on so we're
increasing and events linearly but we're
reducing the probability and the cost by
a squared effect and so that should win
which means if we do these things then
the cop the risk of doing continuous
delivery is lower than the risk of doing
batch delivery so this is the motivation
behind a lot of the practices that we've
been talking about earlier today that
we'll talk about through the rest of
today and that we've talked about in
previous iterations of this track there
are some unsolved problems that I'll
mention very briefly these are areas
where we're looking for contributions
we're looking for people to to figure
out the innovation that will solve these
problems one of the big ones is library
dependencies so if you're using some
kind of a build tool that pulls in
libraries from random places around the
net like maven central you have a risk
exposure to anything happening with
maven central I work in closure and it's
common to use this service called
closures for closure jars closures has
no authentication that you are actually
the owner of a project you can claim any
project you like and start uploading
code this to me feels pretty risky so we
don't want to just depend on all the
random libraries out there we probably
want to run our own repositories with
the libraries that we trust but now of
course we've got to manage that
repository of libraries and keep it up
to date
so I regard this as an unsolved problem
service and protocol dependencies we've
got some answers none of the answers are
perfect so still more more to do there
and if anyone's in an organization where
you're dealing with ITIL processes
there's nothing in the definition of
iTel that's incompatible with continuous
delivery but the implementation of ITIL
in many organizations is incompatible
with continuous delivery so this is also
an unsolved problem if you are in such
an organization I'd love to talk to you
at some point later on today so with
that I'll leave you with a few questions
to think about
ask yourself how would you do things
differently if you could rebuild your
environments this way if you're not
doing this already I've given you many
different small steps you can take and
some large goals to aim at think about
the smallest change that you could make
that would take you one step of the way
there and then think a second time
because usually when we try and think of
the smallest change we actually think of
something way bigger than the smallest
possible change and if there are manual
things that you're doing just find one
thing that you can automate and move in
that direction I'm sure there's at least
one thing that you could automate that
you just haven't done so far and
organizational barriers are going to be
there we heard from Josh about the
people changes and the culture changes
so think about ways that you can begin
to initiate some of those changes and it
usually doesn't start by telling someone
else that they need to change
that's an anti-pattern so finally I just
want to say that we should not support
or allow this type of thing to happen
anymore if you have a deployment army in
your company someplace send these people
home it's a waste of time it's a waste
of resources it's a waste of humanity to
do something that computers could do we
can do way better than this and so let's
let's let these people get some sleep
because it looks like it's about 1:00 in
the morning there thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>