<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Chaos &amp; Intuition Engineering at Netflix • Casey Rosenthal | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Chaos &amp; Intuition Engineering at Netflix • Casey Rosenthal - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Chaos &amp; Intuition Engineering at Netflix • Casey Rosenthal</b></h2><h5 class="post__date">2016-08-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Q4nniyAarbs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is surprisingly for me my first
time is Chicago whoever put my name up
on the marquee at Chicago Theatre thank
you that was really awesome my name is
Casey I'm an engineering manager for the
chaos team and the traffic team at
Netflix and so I'm gonna tell you a
little bit about what those teams do a
little bit about Cass engineering and
intuition engineering and how they help
keep the service at Netflix always on
those of you who aren't familiar Netflix
is kind of like blockbuster but online
another another way to think about it
that's okay I'll just I'll just hit the
keys yeah another thing yeah like Westar
who another way to think of it is it's
kind of like Facebook but instead of
people for friends your friends are
movies and shows that you've watched
with advantage that on Netflix they're
not going to disagree with your
political opinions and like Facebook
Netflix is pretty popular were worldwide
now except for in China and thankfully
for us people like us so when our
service is down a great sadness falls on
the internet and I don't want to dwell
on that again too much again you know
this is the always on track so we're
gonna talk about how we keep the system
always on so at Netflix again we're
large you know like Facebook where you
know a lot of people use us for very
large we stream some somewhere around
the order of four billion hours of
videos per month which is a lot of data
turns out that's about 40% of the
Internet at peak is Netflix traffic and
of course most of that is served out of
a CDN now cache expiry distributed
caches that's a solved problem in
computer science right so that's boring
so we're gonna completely ignore the CDN
I'm sure you don't have any question
about that and instead focus on the
other component to the service the
control plane so the control plane you
know when you're when you're using
Netflix everything from signing up to
logging in to choosing a movie
negotiating DRM all the way up until the
movie actually starts streaming to your
device is all handled by the control
plane turns out that's fairly large as
well and we deployed that entirely in
the cloud in Amazon specifically AWS so
the control plane is deployed in three
regions in AWS if you're not familiar
with AWS terminology you can kind of
think of a region as a data center
although it's it's not actually that
under the hood and and that's that's the
world that my team lives in the code
working with the control plane now when
I look at an architecture or when I look
at an engineering team one of the ways
that I measure the architecture or the
team's sophistication is how well they
optimize for these three things so in my
experience a young company or a new
product will often try to optimize for
one of these things at the expense of
the other two so we have performance
fault tolerance and availability and a
team with more experience again in my
experience tends to be able to balance
these three things better a Netflix adds
a fourth thing to to this triangle which
is feature velocity so at Netflix we put
a high priority on feature velocity we
believe that one of our critical
competitive advantages is the rate at
which we introduce new features and we
have a very sophisticated a be testing
system that gives us data to back this
up so to get feature velocity one of the
architectural choices that we made a
while back is to move to a micro service
architecture now a lot of you probably
have experience with micro service
textures and your micro-service
architectures there's a lot of different
varieties so I'm gonna describe what I
mean by Microsoft's architecture is that
we know where so that you know where I'm
coming from
so here's a very simplified example with
just seven services represented and a
service might be as simple as handling
just login another service might handle
just subscriber data another service
might handle movie data that sort of
thing
and the way Netflix are structured we
have many small engineering teams that
are responsible for one or more micro
services so the subscriber team the
subscriber of micro service is fairly
large that's a critical component of the
service that we offer so we have just a
small team of four to five engineers who
just support that micro service so when
somebody uses Netflix either you know
from a smart TV Xbox website smartphone
what-have-you a request comes in and
we'll just give a hypothetical example
here quest comes into service Steve we
can imagine service D as a proxy or a
PLA or something like that and whatever
that request is that service isn't gonna
have all of the information that it
needs to return the response so service
D is going to reach out to service C and
serves that F serve C might be the
subscriber data service that might be
movie data and those will turn in turn
will reach out to a and F we reach out
to BMG and they will reach out to B and
G will reach out to be as well and also
to E and you can see that this quickly
gets pretty complicated if any of you
saw Katie's presentation yesterday in
the distributed systems track she put up
the death star view of services and it
gets pretty complicated Netflix has
somewhere around four hundred services
micro services in our system not all of
them are in the critical path not all of
them are in the streaming path
but again this this example of just
seven illustrates the point that the
communication between those
microservices can get pretty tricky
pretty quickly and then once they have
in for the information that they need
from downstream services they bundle it
up they wrap up their request and send
it upstream and eventually that response
is returned to the device that accessed
the service so I want to talk about a
specific property of distributed systems
like like this one like this micro
service system and to do that I'm going
to need some beer I'm gonna tell you
story take a little diversion with me
into systems thinking now if you're
taught if you take a systems thinking
class in university or an MBA program
you'll probably at some point play the
beer game and the beer game is unlike
any beer game that I played in college
so bear with me I'm gonna describe the
rules to a simplified version of the
beer game and then I'm gonna carry I'm
going to take us through an example
round of the beer game so in the beer
game there's three players or in the
simplified version of those three
players they're customers who like to
drink beer retailers who sell beer and
manufacturers who make the beer and if
we were actually gonna play this what we
would do is we would separate the class
the room into three groups each group is
represents one of these team one of
these players and they're not allowed to
communicate with each other except by
passage passing slips of paper back and
forth messages with intent to acquire or
exchange of beer or cash so they're
really not allowed to communicate with
each other and I'm gonna throw some
numbers up on the screen now you don't
have to follow the numbers along just
follow my narrative of what happens so
in the first week we'll talk all this
week zero of the beer game
demand for but we're engineers so we'll
call this foo beer and I'm gonna turn
Wi-Fi off so that doesn't happen again
so we'll call this a foo beer so demand
at week zero is for one case of foo beer
and so customer goes in buys one the
retailer has two in stock so after that
purchase they have one remaining they
order one from the manufacturers eight
in stock and the manufacturers bottling
one case of food beer so we've got one
at either end of this system so we're in
equilibrium and then as the game really
starts in the first week the antagonist
in our story will call him Nathan Nathan
wants to get drunk so he goes to this
store and he's he's a desire for an
additional three cases of food beer but
he can only buy 2 because the retailer
only has two so I indicate that with a
negative 2 in the remaining columnist
like a dissatisfied backorder and the
retailer wants to you know get ahead of
this so they order 4 from the
manufacturer and the manufacturer sees
this increase and they start producing
bottling five cases and Nathan gets
really drunk and comes up with this
brilliant algorithm ok he's pulling from
quantum colocation and nanotech and he's
running it through a Lorentz
transformation and relativistic physics
and he he figures out a way to beat cap
theorem and he publishes a blog and it
makes it to hacker news and because he
was inspired by foo beer he named it the
foo beer algorithm FBA of course and so
there's a rush on foo beer so demand
spikes the 64 cases but people can only
purchase a because a retailer only had
eight so there's a loss there of 56
sales retailer sees a spike and wants to
stay ahead of its order 70 from the
manufacturer manufacturer has the option
to build a new distillery and bottle 250
cases at a time so they go ahead and do
that but the protagonist of our story
will call him Steve is a beer aficionado
so he gets one of these last bottles
that's floating around takes one sip
spits it out says that case terrible
sits down and writes a blog post proving
that Nathan was full of beer and that
cap theorem is in fact safe and the
algorithm was crap so the next week
we're back to just Nathan who wants to
buy more cases of food beer to drown his
sorrows in at this point the
manufacturer delivers there are 250
cases and finally we get back to
steady-state where Adam and is about one
case per week but we end up with this
really disturbing result so now the
point of the game for the teams was to
maximize value and value for the
customer was customer satisfaction for
the retailer was sales for manufacturers
also sales and had we played a longer
round when you play this usually the
rounds are a lot longer usually there's
more steps in the distribution chain and
usually you end up with this exact
result which is not these particular
numbers but which is really frustrating
in that the retailer ends up with 76
cases that they're not going to be able
to sell now they missed out on selling
58 cases that was lost profit the
manufacturer has 180 cases they're never
gonna get rid of an extra distillery WTF
are they gonna do with that it's a it's
generally a frustrating experience for
people who play and I don't want to
leave you frustrated so here's a piglet
eating ice cream so why did it why do
they teach this in
systems-thinking classes well
specifically I don't think it's because
the professor's have a superiority
complex that never happens I think it's
to demonstrate a specific systemic
behavior called the bullwhip pattern
where a small oscillation and input in
the system produces a much bigger
oscillation further down in the system
I think there's a more important point
here which is that the customers the
retailer and the manufacturer none of
them did anything wrong they all made
correct decisions given the information
that they had in the scope of what they
responsible for and yet the systemic
behavior was highly undesirable and
there's a direct analogy to distributed
systems like micro service architectures
here for for any given services for
these services that you know say that
the subscriber service that could be
formally verified perfectly to spec
absolutely correct
and yet even if all of those micro
services are absolutely correct the
system as a whole could still exhibit
behavior that's highly undesirable I'll
give you another hypothetical example so
here's another story not that this has
ever happened to me but say say you're
in Washington DC and you're on the
Amtrak going back to Philadelphia
because you live there at the time and
you think oh you know would be great you
know it's night it's night time on the
Amtrak it's late it'd be great to catch
up on house of cards
and you're so engrossed in house of
cards that that you drool on the
keyboard right but this embarrassing but
it could happen and you know it's dark
and slate and so you start fumbling with
a laptop and I have no idea what you hit
but your laptop starts just refreshing
Netflix which is weird but ok on the
Netflix side you're on a train so the
requests were there was a network
partition and then they were buffered
and then they all hit the server at once
and because this the requests are very
similar you're the same user always
requests go to the same server for the
subscriber service it's trying to look
up your information over and over again
and because it gets all these requests
at once it doesn't have time to look
them up off to look the data up off disk
so it has a very sensible fallback it
serves responses out of cache
they're not quite as up-to-date as
what's what's on disk but it's an it's
still unacceptable fallback right we
don't want to we don't want the service
to shut down when a server gets
overloaded we just you know so the way
this manifests is if if I went to the
home page I might see instead of movies
that were personalized for me I might
see just like the top movies for
everybody or something like that so
that's reasonable so this this one
server in the cluster is serving from
cache and because it's serving from
cache its CPU load goes down and today
we have another service that's
responsible for auto scaling because we
don't want to pay for servers running
when we don't need them and it notices
that the average CPU load goes down for
the subscriber service so says oh okay
well we don't need as many servers there
then and it kills one of the servers
that has higher CPU load which further
lowers this the average CPU load and now
the servers in the subscriber service
are handling more requests with less
machines to do the work so more of them
start flipping into cache and the CPU
load goes down further and so the
autoscaler shrinks the cluster further
and now many users more than the one who
just Roland's keyboard because he's a
dope start looking at Netflix and
noticing hey this isn't what I usually
see and a very reasonable thing to do if
you're a human and a program isn't
working quite the way you want it to do
is you start unplugging and re-plugging
in or clicking refresh really quick and
all of a sudden we have a human created
retry storm hitting Netflix with all of
these requests
none of most of which we'll never
complete because by the time they even
could get all of the context to return
back to the user the users are already
initiated a new request anyway so it's
essentially wasted work I have no
evidence that what I actually just
described has has happened at Netflix
but it's
things have happened and it's certainly
the kind of systemic behavior that can
happen in a micro service architecture
or in a distributed system that we want
to address so so far I've been we've
been drowning and I'm just describing
the water right I've just Illustrated
the problem I haven't given you a
solution to anything yet so let's - let
me tell you another story this one about
chaos a happy story and Netflix we have
two primary chaos tools chaos monkey and
chaos calm and I'll briefly describe
each you might have heard of Cass monkey
this is a program that will for a given
service randomly choose a server and
turn it off during business hours the
reason that we we built this is because
we were tired of being paged at 3:00
a.m. in the morning the witching hour
when servers usually disappear so
instead we brought that pain forward
during business hours so that when we
got paged it didn't matter as much now
at our scale at anybody's reasonable
scale it's guaranteed that servers are
just gonna disappear because you know
hard disk goes bad so many trips over a
cable some you know power goes out
somebody shoots at fiber-optic
connectivity lines in the Midwest true
story all sorts of you know backhoes all
sorts of things happen so servers are
going to disappear and we just brought
that that pain forward and the result
was great we hadn't explicitly told our
engineers hey you have to write your
services this way instead we introduced
this condition that drove high alignment
among the engineers to automatically fix
right there services such that they can
withstand that kind of failure it was
very very successful and it's been years
since any of our services as far as we
know have had any ill consequences due
to
to a server just disappearing chaos
monkey runs regularly today it will turn
off dozens of machines in honors in our
system so we thought okay that's great
that's chaos monkey Kass Kong now we
also have the situation running in the
cloud where an entire region will go
down so we thought okay let's take that
pattern and apply it from instead of
just a server on the opposite end of the
spectrum let's apply it to an entire
region let's turn off an entire region
now for whatever reason I'm little
unclear about the details amazon won't
allow us to turn off an entire region
something about having other customers
the idea so we simulate it as best we
can and we'll choose a region that's
sick and we'll move the traffic team
we'll move that traffic over to the
other two regions we'll slosh traffic
around the globe move it over to other
regions
so that we can fix the problem there and
then we'll move it back that's chaos
Khan and again this is very this has
been very successful it aligned all of
our engineers without us explicitly
going out and telling them how to how to
run their services it aligned them to
build their micro services in such a way
that they can run into any region and
they're always running in every region
and if one of those regions goes down
they can quickly scale up into other
regions and our subscribers can continue
to get the experience that they want so
we thought okay this is great chaos
monkey and chaos Kong there's something
here there's something here that's very
valuable so we sat down and tried to
figure out how do we formalize this into
something that we can evangelize inside
the company to help us get these that
this better resiliency which leads to
better availability on a more regular
basis and also to other industries
because we like we like having peers and
so we came up with chaos engineering and
you can you can read about the the
formal definition there but the the
one-liner is Cass engineering is the
discipline of experimenting on
distributed systems in order to build
confidence
and the systems capability to withstand
turbulent conditions in production so we
think of this as a new discipline within
software engineering and we know that
bad stuff is going to happen in
production because it just does we live
in a physical world and so we want to we
want to empirically we've got the word
experimenting in there we want to
empirically bring that forward and help
build confidence in the system by doing
it in a controlled way so I'll give you
an example so for if we want to test the
resiliency of service see say that's the
subscriber service I've been talking
about we might take the cluster that
that's running on and spin up or in the
cloud so it's easy to spin up new
clusters will spin up to new clusters
that all get traffic from the same
source one's a control group and one's
an experimental group and we'll feed
them live traffic and we build a
hypothesis and the hypothesis is that
our system is resilient to an outage in
this particular service or some sort of
failure condition in this particular
service and then we apply that condition
to the experimental group it might be we
don't let it reach downstream
dependencies or we introduce a bunch of
latency or something along those lines
something that would that could actually
happen and the degree to which the
steady-state behavior at the edge the
user's behavior at the edge is the same
for the requests that went through the
control group as they are through the
experimental group is the degree to
which we can add confidence to our
hypothesis that were resilient to that
kind of effect
this has worked really well for us
internally I'm happy to say
industry-wide we've also gotten a really
warm reception last year we had a cast
Community Day this is from the poster
from last year's where we got fellow
practitioners from Microsoft Yahoo
Google Facebook you know the the other
companies that run things at scale and
we'll do this again at least once a year
so we're developing a community of
practitioners a community of peers if
you do chaos engineering hit me up we'll
do this at least once at least once a
year and we get to share all of these
great stories about things that we've
seen in production and ways in which
we've gone ahead and simulated them and
introduced them into our production
systems again to build confidence in our
systems by addressing these systemic
effects so while we were building that
community we also developed a gold
standard above just an experimentation
for additional principles that will help
elevate us to the best level of chaos
engineering so one of those principles
is build a hypothesis about steady state
behavior as engineers we have a
predilection to want to figure out how
things work and in this case chaos
engineering is more concerned with
whether a system works not how it works
so it helps us focus on what's really
important the business metrics are KPI
or whatever you want to call it by
focusing on the steady-state behavior of
the system and building your hypothesis
around that and not trying to create a
hypothesis about the internal workings
working on verification of the system
not validation of its internal model
second principle is very real-world
effects this is kind of a no-brainer but
we all have a lot of experience of
things that happen in production we want
to recreate those things
experiment in production this is a
little bit more controversial chaos
engineering is not testing if you're
testing you can write unit tests
integration tests those sorts of things
and you can run those with synthetic
traffic and you can find lots of bugs
and you should do that that's another
great way to build confidence in your
work but this is an experiment and
there's no way certainly at scale to
accurately duplicate your production
environment the experiment itself is
going to have a systemic effect that
could change your results so we firmly
believe that the only way to accurately
build confidence in the system that you
have now is to experiment on the system
in production now on top of that for us
it's particularly infeasible for us to
stand up a second Netflix as a testing
environment particularly when for any of
our hundreds of micro-services any of
those teams can update any of those
micro services at any point during the
day and a fourth principle automate your
experiments to run continuously any of
you who are in operations is probably
already obvious to you doing something
manually at first to figure out how to
automate it perfectly fine but if we
really want the the confidence in a
systemic effect to carry over then you
need for you need you need to use it as
sort of a regression test this needs to
be running on the system 24/7
particularly since the system itself
could change at any time because anybody
could update any micro services so those
are our four advanced principles of
chaos engineering
so that's one story that's that's my
calf's engineering story about how we
how we specifically look at use this
practice to look at resiliency for those
systems we had another issue with
systems which is that because the
engineering teams are experts on their
own micro services and because the
system is so vast there's no way for
anybody to understand how all of those
systems fit together we don't have a
chief architect at Netflix we a human
couldn't even fit all the pieces in in
his his or her head even if we did have
a chief architect so when my team is
moving traffic from one region to
another sloshing around the globe we
need to know are the regions that we're
moving traffic to you doing well and
because we're not experts in any of the
particular micro services we're
certainly not experts in the holistic
picture so we needed a way to understand
the service as a whole holistically
essentially if you know if we have
experts in in these red micro services
that green layer of requests going
between the micro services nobody owns
that and because the micro services are
coming and going so fast there's no way
for a human to own that and chaos
engineering gets as part of the way
towards having confidence regardless of
not understanding that green spaghetti
web but we still want a better human way
to interact with this so we came up with
what we call intuition engineering and
we have an application called visceral
that's our flagship example of intuition
engineering so what I'm gonna what I'm
going to describe doesn't have a formal
definition yet maybe somebody here can
help us come up with a formal definition
of intuition engineering but the point
was we've got this very complex
distributed system and we want to have a
sense of its State now we have great
metric tools that Netflix so I could put
up
of charts and graphs of things changing
in real time but it's overwhelming human
brain just can't parse information from
that ineffective way so we wanted to
some sort of interface that we can get
that gut-level feel of whether or not
the system as a whole is doing well or
something's a little bit off I have an
absurd metaphor but it seems to work one
of the first things that that we looked
at is could could we build a new sense
for people around the state of the
system so imagine that we built a suit
and the suit is covered in electrodes
and the placement of the electrodes
corresponds to the 400 or so micro
services that Netflix and when you're on
call you get the privilege of wearing
the suit we affectionately call it the
Payne suit so you can imagine you know
you're at 3 a.m. in the morning to wake
up how my shoulder hurts oh subscribers
down again I you know we hypothesize
that after a very short period of time
wearing the pain suit you would get this
extra this extra sensory feeling for the
state of the system even when you won't
be able to explicitly say what the
problem is so if the system is different
you would just kind of feel it and you
might not even be able to put in words
what that feeling is but we've got great
tools to do root cause analysis and to
go delve into you know the network and
figure out if things are wrong so just
wearing the suit would be enough to give
you a feeling of hmm something's not
right
so we haven't built the pain suit yet
but we built this visual analogue that
we call visceral so I'm going to show
you the the first screen of Israel if I
can pop out of presentation mode here I
can't see the screen so hopefully that's
displaying so what you see ignore the
numbers the numbers are irrelevant the
circle in the middle represents the
internet and the three circles in the
corner represent the regions that were
deployed in the dots moving between the
internet and the regions correspond to
volume of requests the speed at which
the dots are moving correspond to
latency the color corresponds to error
code status of blue is 200 reds are 500
there's oranges in there you might see
every once in a while
and as long as I've been talking while
you're looking at this your brain has
already kind of sort or pretty much
figured out what normal looks like for
this time of day given this traffic in
fact it's it's so effective that we use
this when we slash traffic around we
have this this up so that we can just
add a glance look at it and go yep
it looks good and if the pattern of dots
moving is different we're taking
advantage of the brain's ability to
process vast amounts of visual data in
parallel very quickly we're taking an
advantage of that to just instantly form
an intuition that hmm something's not
right over there the pattern of dots you
know you you might not even be able to
say they're moving slower or the colors
off or something the pattern of dots
right there isn't quite right so let's
you know stop what we're doing and dig
into that before we go ahead and move
millions of customers over to that
region so this is the front screen of
visceral we're gonna open-source this in
about a month or two
if you click on any of those regions it
will take you into this view which is a
view of the microservices within that
region so this is traffic coming from
the internet in the upper left-hand
corner and then propagating through the
the very top layer of our my cursor
micro-services architecture and I hit
the labels because those aren't
important but again from looking at this
for a very few very short period of time
you just kind of get what normal looks
like and now if I run into a problem in
production I can bring this up and even
kind of get an idea of something with
those services up in the upper right
hand corner something up there is not
right and then we have the tools to go
like dig in and look at logs and do
root-cause analysis and all that to
figure out what's going on in that
region or if it's nothing so this helped
us take formerly intractable problem
with this very complicated system and
bring it forward in a way that we we can
develop an intuition of what in this
case the system should look like and
that's been very powerful for us I'll
give you one more example of this here's
that same first screen during a traffic
you can see this is you know again at
for this time of day this is what normal
would look like but in the upper
left-hand corner uswest
the number of errors increase is
starting to increase and in this case we
sped up the simulation so I don't know
why there's are there but at some point
the errors get high enough that we
decide you know what we got to move that
traffic over to us east we got to move
that traffic over to the other coast so
that bar along the top what you see
happening is our proxy layer on West is
starting to proxy those requests over to
the region in the east at some point as
the region the East scales up to handle
that additional traffic at some point
it's ready to take all of it so we go
ahead and flip DNS
and then you know takes five minutes for
dns to finish and then eventually all of
the traffic that was going to us East is
now going to us or what's going to us
West is now going to us East and you can
see that represented in the in the the
increased volume in the traffic going to
us East and we fixed whatever was wrong
in us West and then we reversed the
process we flipped the NS back while
we're still processing over to us East
so that we don't crush the region that
we just fixed and then we slowly taper
back the proxy and then we end up back
in steady state so again feature
velocity was one of the things we try to
optimize for and having this capability
allows our our engineers working on
their micro services to really push the
bounds of the features that they're
implementing and if gods forbid
something bad happens we deploy
regionally so in that region we can just
evacuate the traffic from that region to
the other regions fix the problem and
then get back to the root to to the even
spread that we want and the service
stays up most of the time almost always
customers of subscribers don't notice
when we do this and that's what we want
so that's visceral again intuition
engineering we we think of this as a new
thing and we're still kind of fearing it
out but we have some new interfaces that
were working on for for visceral and
some other ideas for intuition
engineering applications that again
would give us an interface to a very
complex system that gives us the
capability of developing an intuition
about the state of that system okay and
again this is my simplified depiction of
our microservice architecture in case
there are any questions and this is the
awesome team I get to work with so I
just want to give them credit for it
that's my talk I hope we have time for
questions
Thanks
very nice cool talk we do actually have
at least four questions here I want to
remind everybody to vote and then of
course you can add some other questions
as well but the first one here is how
difficult is it to change the contract
for one of the services when you have
many consumers yeah so that depends on
how quick footed the consumers are right
so I think there's it's actually
provably the case that there's always a
Safeway at least in theory to update a
contract while maintaining the old one
so at least we have that as a baseline
right whether or not we do that in
practice is really up to the importance
of that service so don't do we always go
that route no sometimes we change stuff
and sometimes that means that other
things will break because we change the
contract between the micro services but
we have other programs that tell us how
important micro services are for
critical services like streaming video
so we have other tools that I didn't
describe here but I would love to talk
about outside that help us determine
what's what's what what are our Tier one
services what are the services such that
if any single one of them goes down
nobody can stream a video and it turns
out that's that's a small very small
subset like tennis or so and so for
those they're obviously going to be a
lot more cautious about changing their
contract and anybody who who they depend
on is gonna be more gonna keep that more
front of mind okay thank you
this next question which is one that I'm
glad somebody asked us when I was
thinking of as well as an experiment
production ever gone so wrong that the
downtime was substantially enough
monetarily that management was unhappy
without with the situation at hand like
when you're playing monkey kong or
whatever yeah monkey con so so there's I
think there's two questions there one
and I think that the more fun one to
think about is have we ever gotten in
trouble so the the VP right so that's a
different but whether we cost money is a
different question and whether we
whether we did a disservice to
subscribers is a different question so
whether we ever got in trouble so in in
my meetings with one of the most senior
most people that I report to so a VP in
the division that I'm in he has told me
that if we don't occasionally have
surface a weakness in our system then
we're not pushing fast enough and I
think that's right so we're very
fortunate in that we had that at Netflix
we happen to work in a place that
understands the value of chaos
engineering to the ultimate service that
we're providing to subscribers and so
they give us a lot of rope to to really
push chaos engineering so I cannot cost
no we haven't affected cost subscriber
impact we don't like to negatively
impact subscribers of course there have
been times when experiments have
negatively impacted subscribers and in
those cases we have the functionality to
turn those experiments off right away
and so we do that and by the time they
hit refresh they don't realize that
anything happened usually okay thank you
this is a fun one have you ever tried
releasing actual monkeys in the data
center to see how services that respond
to physical chaos
so again Amazon won't let us do these
things for sight they won't give us the
keys to the data center but if I ever do
get the keys I'll keep that in mind huh
okay how about this one at Netflix do
you release code to production as soon
as you identify chaos in production and
then it has a follow up if yes how do
you preserve the user experience while
new code is pushed and systems to
rebooted it's such a large scale so
everything so so we preserve the the
second part is easier to answer first
so we preserve the user experience by if
we identify a problem in a region we we
deploy regionally in stages so if we
identify a problem usually we're
identifying it before that problem has
propagated to other regions so we could
just turn that region off move traffic
to the other two regions while we fix it
now as far as how we handle a fix
ameliorate a problem what do you do it
as soon as you identify chaos in
production that's the question
yeah so no because usually a caste
experiment is determining a potential
vulnerability in the system so what we
do is we generate context for the
service owners hey you probably didn't
think about this but if your downstream
dependencies go down or if you switch
over into serving things out of cash
then it could have this other bad effect
on the user experience so we're getting
ahead of the problem actually
manifesting in the system which is
really important and then we give them
the time to fix it in an appropriate way
and we don't we don't watch that they're
they're experts on their system so you
know it's it's up to them what their
deployment pace looks like makes sense
okay how are your SLA is affected by
switching AZ's so we don't switch AZ's
we switch regions
it's so we by virtue of our business we
have this great advantage most websites
have have an SLA they shoot for
something like less than you know 600
millisecond response time say the great
thing about working for Netflix is by
the time somebody gets the popcorn ready
and sits down on the couch and wants to
watch their show if that show takes at
worst-case scenario 10 seconds to start
streaming they're not getting off their
couch so our our the impact on our SLA
of moving traffic to a different region
is not noticeable to subscribers we have
time for two that's good does Netflix
have synchronous or asynchronous
micro-services architecture and then any
observations on a particular approach up
until recently the core pieces of
Netflix is micro micro service
architecture were synchronous and that
wasn't a problem we're now moving
several core components over to
asynchronous communication IPC and
probably for reasons that most other
companies wouldn't look at moving to
async usually it's to alleviate our
burden from internal service requests
not to handle increased traffic
externally so I can tell you who else a
question I can tell you more about that
decision offline but it's not it's not
the usual reason that you would go to an
async system we did put a lot of
resources into looking at our X Java and
Eric's nettie and reactive architectures
and those are very cool and advantageous
in some conditions but we don't see an
impending need to move
in that direction because that would
probably fall in the camp of over
architecting for performance at the
expense of feature velocity okay and
then we'll do one more is in probably
break time and then people could still
obviously year-round for the rest of the
day yes so do you currently use
intuition engineering to predict
failures or is it more of a tool to
address faster reaction times it's
faster reaction times so it doesn't it
well almost always
there have been cases where it helped us
avert a disaster for example and I had
one engineer who had his finger over the
button enter literally and was about to
move traffic from u.s. West to u.s. East
and he turned around and he saw visceral
up on the screen and yelled an
exploitative and this was a very like
calm person so everyone just froze like
and he said oh my god okay we we I was
you know I was just about to move
traffic over to this region but I made
that Co change earlier today and I
forgot to put a gate that would prevent
it from then redirecting to the third
region so had he hit enter it would have
brought the whole service down because
it would have sent the whole world to
one region and that wasn't scaled up to
handle it so it kind of prevented a
disaster but that disaster would have
been self-inflicted
okay let's thank Casey once again and
don't forget to thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>