<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Taking Storage for a Ride • René Schmidt | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Taking Storage for a Ride • René Schmidt - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Taking Storage for a Ride • René Schmidt</b></h2><h5 class="post__date">2016-11-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kq4gp90QUcs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is raised bed talk about storage
sorry that was a single about how big
this is a work that we have been doing
over the last almost three years to an
obvious was in almost so in case you're
not from Denmark we have an uber office
in Davos Anubha is as sort of yes you
know a global company where we do rides
across the world and the engineering
organization is ours actually also
distributed across the world so we have
offices and in Denmark we have offices
in Bulgaria and advanced of places in
the u.s. to and for myself so I worked
there for about three years primarily on
storage before that I have worked on a
bunch of other companies we and we're
mostly at VMware actually did what was
called virtual Center but was the
management platform for building out ESX
servers across a big data center so that
is actually somewhat similar to what we
ended up doing here uber of managing our
huge fleet of storage systems and if any
of you remember what your website is
it's a pretty Mason alrighty let me just
sort of set the stage for for this
storage system so once a little bit
about how goober works I assume you sort
of nobody with right you install an app
on your phone you request the right and
so forth so on the top of our stack of
course we have the cell phone app and
that basically talks to a layer we call
the marketplace so this is where we're
matching riders with drivers the
marketplace system keeps track of all
active trips that are going on at any
given time so we track solid GPS
coordinates it matches finds the closest
car keep track of the route and then so
forth and then when the driver clicks n
trip then ibiza generates a JSON plop
with all the GPS tracks you know who
would've Ida was would arrive
was was a search and all kind of
interesting information there's like
twenty forty fields or something like
that and that gays basically get put
into the back end of the system and the
back end of the system has to do you
know send an email will recede as charge
the credit card has to eventually put in
some money into the driver's bank
accounts and so forth it does a lot of
fraud and then those kind of things all
those data all those trip histories all
those receipts of course has to be
stored in some kind of storage system
and this is what we call operational
storage this is the swords that
basically drives the whole thing has all
the city definitions or the user
accounts all the trips that has been
done and so forth and that's what I'm
going to talk about today we also have a
warehouse where everything gets into it
sort of a big Hadoop HDFS kind of thingy
where you can do all kind of queries
that's not the part that I'm going to
talk about consol about operational pad
and the key thing about operational part
is that this is actually part of the
online business right so when you are in
the app and requesting like I want to
see my past web histories I want to see
my receipts I want to look up my account
I want to change my phone number and
those kind of things that's what is in
the operational storage now as most
startups you start out small and simple
so not very long ago actually two years
ago everything was in a single Postgres
database pretty much and of course we
had sort of scaled that up and bought a
big machine and had a lot of Asus tease
those cut things but it was sort of
clear this was early 2014 it was total
clear that this is not going to work
forever actually it's probably not going
to work by Halloween if we don't do
anything about this problem of scaling
out our storage by by january two
thousand Halloween 2014 the new book
will basically stop working so it's
pretty clear now that we actually
succeeded in solving this problem we're
still here of course but there was one
of the channels we had like what do we
do
and just sort of you know when you can
solo looking inside of where we were
it's pretty easy to see that you know
this was definitely something we need to
do about January to 2014 of is basically
where that era was and at that time we
actually didn't really do have many
trips today we do almost 50 times more
trips to meet at that site and that's
only two and a half years ago so things
are going really really fast and since
we have changed to the storage system
going to talk about we actually been
able to scale to this sort of massive
growth we just did our two billions trip
in june this year was sort of a major
milestone right now we do just so
foreign sort of reference about 5
million trips a week which basically
means that every second there's 60
people getting out of an uber across the
world so it's a it's fairly big if you
follow if you've compared to Twitter of
course it's very few but this is
actually real people moving around in
the real world now when you have to
address this problem right so we have
this post quest database you have
millions of lines of code that that
basically access all these tables we had
hundreds plus tables to keep track of
this whole system so how do you sort of
go about attacking that you cannot just
say well in six months we're going to
rewrite the whole system and fix will be
good so you have to sort of figure out
where to attack it so we did some
analysis of the data we had in the post
quest database and not too surprising it
turns out the trip sort of the big thing
right this is basically what we do we do
a roll of trips so seventy percent of
the data was more or less trips in
Postgres database so we decided to
address this up front and say well this
is the first problem are going to attack
by halle and if we can get trips out of
the post quest datastore we'll be in
good shape um it turns out that it was
not just scale and latency and those
kind of things of a problem it was
actually also that making changes to a
big relational database turned out to be
sort of complicated sometimes when
people needed a new index
let's say you need a new index and a
trip table the whole Postgres database
will stall and then everything stalls in
the whole system so we also wanted to
have a system that developers very very
quickly could make changes to the data
it didn't have and they could do it
online without taking the system down
you know this is sort of you know the
system itself is sort of a ship that has
left the harbor right you cannot stop it
it has to keep going you can never turn
it off so everything we do to the system
has to be done online so very simply
that is sort of the world we want to get
to at that point and the world we are
now where we still use postgres and my
sequel and our database technologies for
various things but but sort of the main
part is the schema system the
marketplace is actually run mostly are
on Cassandra and those kind of things so
there is sort of a bunch of different
technologies in play the system we have
then ended up building now two and a
half years later I have come up with is
is a fairly substantial system I would
say it's not just storing trips it's
storing a lot of different databases for
a lot of our services it's installed
across multiple data centers across the
world there's a huge amount of cell
service and self-healing infrastructure
and I'm actually going to spend a little
bit of time to talking about sort of the
operational aspects of running a huge
system that runs across thousands and
thousands of hosts so it's again sort of
thinking about installing a single
database is easy right you know once you
want to have 300 databases across
thousands of hosts it sort of becomes
very interesting so a quick summary sort
of of the status where we are today
about eighty percent of the data
particularly user user accounts and
trips stored in this system we have gone
from this single trips datastore that we
actually targeted back in 2014 to more
than 300 data stores and this is Barry
think of these as separate tables
a separate databases instances if you
will uber is a very micro
service-oriented architecture so
basically services can get their own
storage and add back in the days when we
set up the first 48 machine clusters to
run trips you know we thought that was
huge we have since revised sort of our
idea of what is huge but it's something
that that scales and n grows and again
if you looked at the graph we basically
doubled every six months right so things
just sort of quickly kids they can take
a bigger okay so let's dive into a
little bit about how the scheme Ellis
has been designed and is working and as
all systems when you start up building
in your system you sort of look at know
how does it has to perform you know what
kind of features should have what should
I available to be and so forth the
interesting thing as you know if you
look at all these requirements it's
always a trade-off but you cannot just
say I want the perfect API I wanted to
scales a little without any bounds it
has to be super efficient it has to cost
nothing and so forth so it always sort
of a trade-off and particularly with
sore system this is definitely a
trade-off you cannot just get full
consistency and full availability and
those kind of things this is what for
example the cap theorem tells us so
there has to be some kind of trade-off
and you really have to figure out sort
of what storage model does really work
for your problem and then the final part
is sort of the time to market at the
time when we started doing this we knew
we had six months so whatever we decided
to do it had to be done within six
months so you're not you know you're not
going to reinvent the world or spend
three much sort of prototyping some idea
you actually want to have something
rather simple because you are going to
run this in production it has to work so
let me just quickly go through sort of
the basic requirements we had when we
decided design the system the first part
was in these indexes indices was super
important as all the code that was
running group of us written to work
against the Postgres database and they
are used to having indices and there was
lots of queries like you know the one
that I should have the right where
you're sort of looking up all the trips
for particular user in some kind of time
range the code here was written in most
of the code was written in Python using
secret alchemy directly into the into
the Postgres database so in sometimes we
were actually in good shape because
people didn't write sequel statements
itself it was actually modeled using
sequel query reduces classes and so
forth so we can actually fairly easy if
we had the right abstraction sort of
change these classes inside the existing
code and then I will certainly call into
the scheme alert system the next
requirement was on bats operation so if
you if you sort of think about how uber
works right is that people take a trip a
trip comes into the system you have to
build it you have to pay out you have to
do some fraud detection you have to do
various kind of analytics and then you
just sort of sits there so it's very
much sort of computing in the front of
data or you can think think of it sort
of as a stream of data that is moving in
that you sort of have to be processing
so we also wanted the trip status also
sort of support that model that all
these trips are sort of trickling in we
do a lot of computation on them we
ideally do it I'd importantly so you
don't get charged you know twice for the
same trip or worse that we forget to pay
out the driver so we had to have sort of
the robust system for processing all
these trips and then they sort of lay
dormant and this is actually very true
we have done some analysis where we sort
of look at the access rate two trips no
after has been taking and you know you
can imagine it's sort of a curve that
sort of goes like this there's no cause
a little bit people look up the trip
histories but you know as opposed to
facebook where people go and look every
time you know looking at pastor of
history's it's not sort of a favorite
pastime yet so that was the second
requirement and then the last one as I
mentioned before we are big into
microservices the uber system actually
consists of more than def
austin microservices most of them are
fairly simple this is the idea with
micro services they call into other
micro services to perform some kind of
function but a bunch of them of course
have state and then basically get their
own local database or we call it data
store their own local data store this
can be a my secret later base but it can
also be a data store backed by
schema-less whether it's basically just
scalable to better you want to store
trips trip scale data similar stuff so
it's important to build a system where
we sort of quickly can support this
model of creating new data stores
tearing down data stores when a micro
service dies a solid decommissioned and
replaced by other ones and generally
this sort of micro service architecture
is making it very agile to make stuff
right because we can easily add a new
feature we can easily take out a new
feature take out a feature that is not
needed anymore has been replaced by a
better implementation as its single
single micro services and as these micro
service has their own storage it's sort
of clear exactly know what stores
belongs to this we can migrate it into a
new data store if need be and then of
course the obvious one you know the data
a sort of has to be stored a disk
somewhere and this is sort of the crux
of the problem of scale because these
disks are not really very big you can
maybe get a two terabyte maybe a 16g
white if you're lucky and then actually
this disk exit looks like this nowadays
just in case it's just so kind of nice
with the old ones but you know we were
honest or days of course a lot of SSDs
and how many SS needs to really need
well it doesn't take a lot of
calculation to sort of figuring out that
let's say you have an SSD card with two
terabytes you stick four of those into a
server and you have a server with a
chair bite if you want to stop a
petabyte which is sort of the scale we
are looking at here you know one to tell
by petabytes you need at least one and
twenty eight machines and you cannot
just saw it on one machine right because
machines dies Rex ties data signals goes
off so what we normally do is actually
replicating data across two data centers
and we end up having four copies so that
means for a petabyte you need five
hundreds of machines so that that's
already very serious amount of machine
the infrastructure you need that and
that's actually only a petabyte so this
is just sort of the tip of the iceberg
of the scale that that we eventually
we'll get to him you can ask why hl5 why
don't you just use a bigger machine of
course you can do that but the number
still gets paid and then I'll talk about
a little bit later by making that number
bigger too has a different set of
problems actually the API that we did
and again you sort of have ok bodies
trips trips is really just a stream of
data coming in all these trips actually
tracking real life events that are
happening out in the real world and the
nice thing about events is that they are
sort of by definition immutable you know
they happened this is what happened and
that is actually how we want to model
our data inside over where we track
trips is that it's sort of a stream of
changes that we that we track so so the
way we designed this API and also
inspired by for example a big table from
from google and all the places where
it's just sort of a big column sort of a
table style API where you for a given
row you can store a number of columns
and each columns you can just all jason
plops so this is the schema less aspect
you just saw random JSON blob you can
store whatever it is the developers can
change that Jason plop anytime the ones
you stole something else you don't have
to make any table modifications to this
and then you can think of a trip coming
in as like his base information about a
trip you know this is to write a driver
here's the route it's another column
with the GPS points then the Sun process
that kicks in is like are based on this
I can compute the fair then just almost
affair and we basically saw all this
information so each row is sort of the
entire sort of computational history of
one particular trip and this is all
immutable so we have the whole track we
can go back and say well at this given
time you know we computed this value for
this field
and it is very important because this is
actually transactions are sort of
business transactions that we are
tracking if you are if you are familiar
with the lamb tagged architecture that
they very much advocate this model where
you neyo will write data you really want
to keep all values in your database just
in case right you could have code that
accidentally overwrite your data and
then you are sort of in a bad place so
this is actually following this model
this also means that this is an append
only data store you meant once you write
a cell you cannot overwrite it again
that value is there you can write a new
value in the same curve in the same role
and the same column but wait a different
time stamp if you want to and there is
sort of how we track changes over time
it turns out that from a scalability
perspective this API also makes it very
very easy to scale which is also nice so
how do we actually scale it the physical
layout of the databases that we take
each of these rows it is using uuid the
UID I just have a very short one here to
get it to fit on the side but in
practice is 128-bit uuid and we just do
module 4k on that that value and divided
pre started into a number of shots and
these shots are then co-located on a set
of data base classes and my sequin
clusters so we didn't have to reinvent
sort of a big storage technology we
basically take rock solid foundation my
sequel my sequel has support for bases
replicating into slaves right across
multiple even across data centers and so
forth so we are reusing the async
replication in my sequel we are reusing
all the curvy capabilities where indices
and so far from my sequel and just have
this in some sets fairly thin shouting
layer on top of my sequel to to shout it
out player like this and then we can
actually run up to you know 4,000
clusters here if we want to as we shot
but we start out with typically
relatively few clusters let's say 8 16
and so
and I'll talk a bit a little bit about
how we we actually can expand this
dynamically as we go but the trick here
again is that it's actually very very
simple and very very simple translate
into robustness and this is really what
we're after the next feature which is
sort of unique and actually hard to find
in a lot of these scale out solutions is
the efficient indices and as I mentioned
earlier this is the key thing that we
had to support to support the migration
from the old code base 22 to this one
and the idea where indices is that for
cubase of specifying index we see that
here on the right place called the
client index anybody says every time you
write the column base then take the
field client ID and shot on that and
then stall an entry based on that pine
ID and this is what you see here on the
left if I do a put cell now I'm putting
a base column at you know the uuid here
is 100 and if you let us assume we just
have four shots as shown here if you do
100 modular for its cells shot.0 so it
will be stored on under several shot
right now there's an index defined for
the base column here and then on the
client ID we take the client ID and also
shot on that and that means that we end
up having an index entry on chart too
and that's based on the client ID and
that has a reference then to the real
cell at a real row where is 100 and if
you can see how this works that let's
say this is the I'm client ID 10 and I
take another trip well my trip just gets
random your IDs right so they get
distributed across all the shots but my
client ID ends up being the same and
since now the index is shattered on
client ID all the index entries ends up
on the same chart so if I now have to do
a query to find all my trips I know I
can just do that on char two and get all
of these your IDs I don't have to do a
distributor query across all the
clusters
to find that so it's very very efficient
indices and in this particular case we
also included the fair this is basically
what we call it the normalized field
that I can very quickly get access to
the fair tube so i don't have to sort of
go back and then pick it out or under
original cells the last feature is the
ability to sort of process the trips as
they come in and this is also sort of
very cool in the sense that as it's in
append only data stop that that means
you it is really also just a log and you
can think of all their pins if you order
them by the time they're put in it is
just a long sequence of lock and we
actually have an API that allows you to
sort of look at well what happened in
exactly this range you know how did the
database look in this time range we can
actually look that up and we can use
that if we just have some mccallum
triggers that sort of looks in the front
of the database all the times what
happened you know what God put in now
and then you basically just get a
callback set you know somebody called in
put in a base column for this role
somebody put in you know route column
for this role and so forth so we can
sort of tail the front of the database
all the time and this is what we use to
drive all the back end processing of all
the floats and there is one simple API
call to basically do this we call it
partition read you can sort of think it
sort of an analogy is would be tail on a
file if you aren't unix you can sort of
tail everything that gets into the
database and then the trigger is
basically just sort of look at ah
there's a base column coming in our
there's a route already exists for this
row ah i can compute fair and then it
computes fair and put fares into the
database then there's an honor taylor
that's as ah there's base and there's a
fair i can do client billing and we just
sort of process at all so by moving
through all these shops in this way we
actually treat the database both as sort
of the primary operational storage but
also as sort of the queue of things that
needs to get processed and then we don't
end up in this sort of we have to store
information twice
can get out of sync do we have to do two
phase commit sin two different systems
and so forth all that stuff is basically
not needed which is also translated into
a simple a more robust system so that
sort of schema-less system sort of the
core database in a nutshell and as you
can see it is a big table style API so
if you use to something like that well I
guess you're only if you work for goon
but that's that sort of how it is it's
sort of a very standard API it turns out
that this API is applicable a lot of
places so we have written it was
originally designed to do the trips we
use it for a ton of stuff now it turns
out it is it just fits for long stay a
lot of places there's a and especially
we are in the basis where we are sort of
tracking real life events this is sort
of what you want to do you dont want to
overwrite you don't want to you want to
keep a log of everything that has
happened is horizontal scalable by
shouting which you have to do in some
way and all and make a bowl scale I oh
and the capacity by adding more clusters
so cool talk a little bit about
availability and once you get into
operation white people will click on the
app and once the right and it sort of
has to work all the time so how does
this system fair in the in the case of
availability and this is sort of where
where the rubber hits the road in some
sense right but that anybody can write a
system or you can buy some kind of thing
but you know no matter what kind of
product you buy with a something you
know for for the garden for the kitchen
of all your data said that the
difference is you know how does it hold
up you know once you start stressing it
when everything starts to fail right
does it suddenly just fall off a cliff
for performance wise or does it suddenly
start losing data and you cannot get it
started for four minutes and so forth so
this is clearly very important to us
that it's something that we can we have
high confidence that is working all its
and when is not working we can fix it
very very quickly that that is just
super important you know there's no time
to call anyone when when that happens so
so let alone gets a little bit about
what happens actually and the
fundamental thing everything is in my
secret databases so we basically have to
make sure these my sequel databases up
so this is where we spend a lot of time
it's all of worrying about keeping them
up well we don't really want to keep
them up but we want to make sure that
when something bad happens to them we
can quickly recover because they are
going to die once in a while disks dice
hoes dies Rex ties or they get
misconfigured or people accidentally
turn them off well you know what so if a
slave dies it's not a big deal right we
will have to still have the master we
still have a second slave everything is
fine so life goes on slaves we really
only have to to recover in case the
master co-star so what happens if your
master dies well we have to sort of
right to the sequel database so if there
is a trip coming in that has to write
for example to that shot how do you you
know what do we do about a Reich and
really only have two options well either
we can fail as a sorry you know I cannot
write right now you have to wait a
little bit and that is actually an okay
answer for some cases right because the
bunch of the stuff that we do is batch
processing like sending out receipts and
trips so this is actually something that
we can safely delay without all of doing
you know too bad of a user experience in
some sense it doesn't mean that you
cannot get right but what we the problem
of course is that the caller once this
stuff to be persisted and replicate it
so we never lose it so we actually have
this what we call buffered rights or
what if you are used to Cassandra
terminology called hindered hand off
well we'll just stored some wells right
we have more than one plus we have 16 or
we have maybe 128 clusters so we can
just store it in one of those clusters
so we just want to put it aside and said
okay it's fine once I care of it it's
tall it's replicated will never lose it
but you cannot read it right now you'll
be able to read later that that sort of
the guarantee we get and they
you guy is very simple there's just the
put cells and you say if you want this
semantics if you really want really high
availability are really high durability
you just say it's okay to buff it it's
fine i just need to get in order to get
this thought I don't need to read it
right away it's fine and we do that when
trips actually come down from the
marketplace into the system and the
reason why this works is because all
this process exactly happened by
triggers right so people don't need to
read this right away the triggers
doesn't need to know that this actually
got put aside in sort of this hindered
hand off table once it gets put back
into the right place when the Masters
alive two triggers runs and everything
works flawlessly so this is sort of the
nice thing about an ASIC programming
model now growth what you know as I said
we are growing every basil doubling
every six months so so what happens when
these clusters gets full and this is
also in some sense simple I'll show a
little bit picture about how simple it
really was but in principle we just
create a new class down the side we've
set up my secret replication so all
these six databases will have the same
amount of data and then now we have to
duplicate set ups and then we can
basically just tear them apart so now
they're separate with the same data and
then we'll just configure the
distribution layers i will have the
shadows on one side well you're half is
on the other side and delete them and
then we're basically done what we call
aspect so we can basically double the
size of the whole system by toppling all
the classes at any point and that is
actually a fairly fast operation so the
key thing here is really to handle all
these my sequences so let me dive into a
little bit about how we do this and how
fast is it actually possible to to
created you my sequel database and this
is actually also interesting you have
these networks right on house and you
have disks on host
and if you want to get data into a host
you have to get it through the network
and if you have a 10 gigabyte 10 gigabit
nic on your host the amount of data you
can get into a host in an hour is about
3.6 terabyte that that's actually sort
of the maximum in practice you can
probably get much less in because you're
using your network forth tough to and if
a host dies it trip really has to be
restored in less than 30 minutes ideally
much faster right so how do you deal
with this in the example ahead before I
was a terabyte in disk space in each
host why do we have hosted at a much
bigger like with hundreds of bytes and
so forth how can you actually restore
addresses are those hundreds here by is
gonna take forever to get through a
network to get up the answer of course
is it always is but you're right a
storage system is that you have to
petition stuff so in the trick is you
don't have to think about the host you
have to think about the data and used to
think about the host just as a conduit
for a bunch of data blocks and what you
want to restore is not the host it's the
data blocks and you can restore those
data blocks in parallel if let's say
there were 16 data blocks which is all
512 gigabytes then you can actually
restore them in 30 minutes in parallel
across a lot of hosts so we actually
have not only to partition the data
across all of us you also have to
petition them within the hosts so when a
host dies we can just restore them
chunks in parallel in a lot of systems
and then we can actually doesn't really
matter how much disk host has it just
means that when something dies we just
have to have more parallelism in the
case we've set up so let me move into
the operational scale so how do you
start thinking about this right because
now an operation is not just about a
host right is only like if you're hosed
eyes I have to find 16 new hosts though
I have to start things in parallel 16
things the that doesn't have them sort
of like by itself and so this place go
and talk about this sort of red block
and in eyes are a thing of the the
storage system is that
you want to keep the storage system
itself as minimal as possible and then
you want to have a side system I've
called it an outer shell something like
that that knows how to manage this it is
sort of a micro service architecture for
for storage services itself right that
you don't want it a sort of one
monolithic block right because the
operational system so we have a tendency
to have you know it's something you can
update much quicker and with a lot less
risk than sort of the co storage engine
so having this separated out makes a lot
of sense in practice and basically once
we operate this let's say we have you
know a few thousand host we have
multiple data centers we have a lot of
data stores we really want to be able to
grow this you know grow and grow and
grow without having more people for one
reason is that you know of course it's
much more cost effective to have fewer
people operating bigger systems the
bigger problem really is that you cannot
hire more people fast enough so in some
sense you just have to fix it you know
it is like people just don't exists and
the traditional way of dealing with this
and this is actually an actual
screenshots of our first run book for
doing a split so this sort of you know
splitting things is that it's very easy
you just replicate and do stuff in
practice you actually doing this on real
life right those trips going on in this
system and you have to do this split and
if you do something bad there something
bad will happen in real life right so
there was a long run book with a lot of
checks and balances and so forth and the
problem it is one box is that people
will not read them the same way or they
will forget something or the phone will
ring or they'll have to get coffee if
you read it at four o'clock at night
your rate ah he really didn't mean that
I'll skip that part you know so it is
just not scalable doing this kind of
stuff so basically everything gets much
harder with scale in some sense and the
typical things we hear all the time is
like well hose fails or bike fails and
all those kind of things the biggest
biggest biggest biggest problem is
really distressed finger
you have a big system at 4000 host let's
say that and it just drifts if you don't
keep track of it because some of the
hose will have one version of the way
some versions some hosts will have one
over sort of my sequel then you upgraded
some of you operational agents and some
of them then somebody has h into some of
the host and applied some commands in
consistently with some other place then
there's some half done operation some
wells and if you don't do something it
just ends up being complete chaos and
then thirdly a sort of lastly or
separately in some senses that once you
did but once we did the trip status tour
we were really deeply involved with the
developers doing the trip status oh
right so we knew the access pattern we
could optimize the indices and so forth
but once you get into scale when you
have hundreds and hundreds of developers
using this data stores you really don't
know how about data they are storing in
there you don't know how to use the
embassies and so forth so the tooling
around troubleshooting performance
creating new indices and so forth just
has to be much much better right because
they have to be able to do it itself at
scale otherwise you cannot on board all
the developers in your organization to
use this stuff so this is also a site
that we spend a lot of energy are but
mostly focus on sort of the operational
part on how you actually work with this
sort of drift fin and there are some
famous lights this is actually a slide
I've stolen from from I think actually
guide Microsoft that back in two
thousand twelve you presented this pets
versus cattle idea this is a picture of
my daughter's rabbit and it has a very
good life it's getting cared for it's
called biscuit and the thing about these
things like when you have your pets it's
like you care for them all the time even
when they are not sick you constantly go
and check them and it turns out that if
you are developer and have you little
service and you you know you go into
work every it's like I'm just going to
run a dtrace on it just to see if it's
fine maybe it maybe there's something I
can improve that does how developers fix
to write what you have
one you have to sort of get into at some
point into the cattle mode where things
like I have a lot right and what matters
is that I have a lot it doesn't matter
that I know that all of them by name and
if some of them dies I'll get another
one because what matters is that I have
them all great and they're won most of
them to be healthy of course so so what
does this mean okay how can you tell
whether you're sort of in the pets mode
or in the cattle world and the death of
very distinct signs that you can sort of
see the organization in if you ain't the
pets world then the some developer in
your organization you go and ask he
would know everything about the system
and if he's not there nobody would know
that sort of defect all the changes is
something about locking into machines
running some you know big run books if
you're lucky all of us he just knows
what commands to run on all the things
in the most telling one and then this is
definitely true is that somebody knows
how to do these commands and he is
immensely cool right because he is all
of the magician that knows all the unix
commands that will fix stuff and this is
how all organizations work so i'm pretty
sure you have all been or are there two
to a certain extent and this is where it
has to start when it's small it's easy
to do this kind of thing but as you
scale you sort of have to get out of
this that you really want to move to a
world where you describe what you want
you actually don't really care whether
storage databases on one host or the
other all you care about is that this
cost I exists and it has three slaves
and put it wherever you want and this is
what we call gold state and this means
that this whole notion of having these
you know experts that knows all the
commands is sort of like a non-event you
actually don't want any experts you want
all your developers start writing code
that makes the best is better and this
is to just do it itself right it's like
this is what we do so some examples of
gold States gold state is
really about sort of describing what you
want in a way that the system itself can
remediate it so a non-story analogy is
that well if you have a service you say
tell the system I need to run their
service I need to have five instances
then the system knows that you need five
instances you know the alternative is
that you go in and say deploy an
instance on machine X deploying instance
on machine y deploying instances of
machines you know tea or something like
that the department is if you specified
in that way to a system then if one of
these machine goes down then the system
has no idea what to do is like well
machine she died I don't know do you
want three inches to about five inches
your static it has no idea so you want
to move to a model wave describe what it
does so in case something fails you can
automatically remediate if you take a
look at these ones from from the bottom
then you can see I just described to
resist my need instance X then the
system says I need to create an instance
okay it needs some clusters that that's
sort of the second one so I run the
clusters well a cluster needs to master
database or create a master database
then another rule kicks in as well all
masters should have slaves so we write
slaves and so forth if a slave dies a
rule kick said as ah the the slave
missing for this one I'll create another
one if a host dies you know a bunch of
slaves will be missing somewhere the
system can just go in and create it this
might sound a little complicated but
this is actually a fairly simple pattern
that we use all the time which is this
on how to basically deal with these goal
state system and the idea is that the
operator never works directly or
anything he just goes in and changed the
model describing what he wants this is
sort of the goal state on the top here
with the green arrow and we constantly
go in and money so what is the exact
state after system at any given point we
put that into the actual state then we
basically run a diff in some sense you
know we can report on the drift on the
diff it is actually a report on the
drift you know how different is the real
system from from the from the desired
state
and then we can basically have rules
that says well based on these drifts
I'll kick off some workflows to go and
remediate it and this is a continuous
process that runs and runs and runs
right so we constantly keep track of any
kind of drift whether it's simple stuff
like you know the OS is different
versions or or maybe it's just saying
that well this always has one too long
you know we want to reboot all our
machines every 14 days you know so we'll
just go and do that and we can ensure
that the entire fleet is done and as the
developers this is just something to run
soldier time so okay in substance is
very very simple pattern but it's super
powerful so a very simple example of
where we use goal state we actually use
this in several levels we use it at the
top level where we're talking about
databases and clusters but we push it
all the way down into the host so when
we have to start run much multiple my
sequin servers on each host because of
this sort of chunking we have to do for
restore and we actually have an agent on
each host and that agent and as agents
are they are supposed to be active
entities right so that agent based wants
this goal state loop and it goes in up
to the management software which we call
ops less and pulls what is my desire
state and then it goes and does local
basically PS Azure togepi s stuff threat
to figure out what is actually running
out at host and ensures that all the
processes on this host matches the gold
state it's the only thing it does and
then it also reports on actual state all
the time so this means that when we when
we make a change to the goal state it
sort of automatically get pulled into
all the machines and if if you're making
a change and a machine is down it's okay
no it's down for a little while what the
network is down maybe it's petitioned or
whatever at some point it comes up and
these machines will come back up and
pull the state and basically get them
back into where where I was supposed to
be this is super scalable to write
because we don't have one process that
has to sort of manage thousands of hosts
across all databases this is a big
distributed system running across the
whole thing so
very simple example let's assume we have
sort of two hosts here with a master and
slave it in my sequel and this is basis
set up in this table saying that you
know foodie be one should be a master
foodie be two should be a slave let's
say I wanna do a slave from ocean you
know you can imagine maybe I've done
that with my sequel is executing all
kind of weird comments every web you
have to wait for replication lag to go
down and then so forth what we do is
basically just changing disco stick and
then we set back or get coffee somewhere
and that once you change this gold state
right we just have to wait for the off
less agents to basically pull the state
this is happening ASIC so they could
pull it in in any it could get to one of
the hosts first let's just say it gets
to the right one first this one says
that well I can you did no sir agent
knows that while i'm a slave in order to
disposition to a master i have to have
all the latest data i can only ensure
that i have the latest data if the one
I'm replicating for is read-only so his
face is sitting there waiting but but
the acts of loop actually happens all
the way out on the host right so there's
very little latency in this loop and
then once the data propagates into to
the other host where's our I'm supposed
to get idle I'm not supposed to be a
master it basically means that it goes
into read-only mode then the other agent
figures that out say uh now the ones i'm
saving from now has stopped being a
master it's read-only my replication lag
has gone to zero i become a master and
then you suddenly do this very very
simple to very simple state machines
running on two different machines with
very little coordination and of course
this is all driven through a nice UI
where we have tons of workflows wherever
we can just do this oh this is sort of
phase one because the ideal thing is of
course why do anybody has to click about
and this is exit them but we're moving
into right now is that these life
emotion of course has to have really
really quickly so we actually moving
away from do using my secret replication
and my sequel host is run in ref cluster
so we basically have a process in front
of that that the selects who
is the master and replicates the rap all
the rights using consensus protocol to
the different databases and that means
that if some if if database becomes
unavailable maybe even temporary it
automatically does the master election
in less than hundred milliseconds which
is actually fast enough that for most
lines they would notice the most of
course are increasing in the lac but we
do reach Weiss and then it's completely
transparent so this is also a way of
sort of pushing this sort of gold stayed
down there saying that well now the ref
consensus Parker's is make sure your
gold status just to ensure and one of
them is writable and you handle this so
this is actually the end and I think I'm
also about our time right very very
quick to of some of the highlights of
this ski blessed or systems the takeaway
here is you know one thing is the API at
all thing is the operational Pat and
when you and you're doing sort of an
online business the operational part is
super important you really you know you
develop on it all the time but you also
keep it running all the time so you
spend a lot of your energy there and
making stuff sort of easy to operate at
scale is it's not a trivial effort we
have spent serious manner engineering
resources on this but this is really
what makes its cape it is super
important stuff to do so finally we had
to have a block in case you find found
this interesting a lot of this stuff
that I've talked about the schema lyst
migration software that there's tons of
blog posts and that on our block I have
even written one of them and enough part
of the team up here has written a bunch
of them so you won't see them tell us
all a bunch of all different stuff that
we do and I think that's it and we might
have time for single question maybe
you talk a little bit about where you're
going but maybe more you can talk about
right since where's going so so clearly
we are growing and growing and growing
and storage is is very tricky right so
in some sense this system is all of the
first generation of what we've got and I
think where we going in the future is
there's a bunch of different directions
one of the thing and most of it is
really about ensuring that we have the
right api's that makes it easy to
develop services and one of the areas
where I see where we can improve where
we are today is how do you deal the data
there's partition across multiple data
centers we have various solutions today
but I think we can come up with better
ones there is definitely areas where you
want better transactional support or
some kind of isolation and multiple of
updates so this is also we what we're
looking into but we're actually spending
quite a lot of time thinking about sort
of what are the perfect api's because
while the big table API is sort of is
very flexible it sometimes you want some
high-level abstractions and then there's
is definitely the cross data center
reliability is a big big big focus for
us anything else
but so the question is about indices how
do we ensure that i actually updated
consistency and we actually don't so
they are secondary indices and they I
eventually consistent so we don't
guarantee that they updated at the same
time as the primary cell is written so
typical that it will be so we definitely
try to do that but if that database is
down that shot is down where the index
is hitting that that will just sort of
be put aside also in sort of hindered
hand off and applied later so they the
secondary indices I eventual consistent
this is one of the things that we're
actually looking for in the future to
think about having making them
consistent</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>