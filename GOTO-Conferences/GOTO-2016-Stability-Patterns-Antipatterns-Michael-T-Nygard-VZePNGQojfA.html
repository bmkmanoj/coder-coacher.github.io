<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Stability Patterns &amp; Antipatterns • Michael T. Nygard | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Stability Patterns &amp; Antipatterns • Michael T. Nygard - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Stability Patterns &amp; Antipatterns • Michael T. Nygard</b></h2><h5 class="post__date">2016-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VZePNGQojfA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so most of my career I've been a
developer and an architect but in the
early 2000s through a somewhat bizarre
and slightly desperate set of
circumstances I made a trip into
operations and I lived in operations for
a while and in fact I lived in a part of
operations where I wasn't just
responsible for the availability of
systems I was responsible for the
availability of systems that I didn't
write I had to keep other people's code
running and get the wake-up call when
their code failed in the middle of the
night my initial reaction was oh I've
got this I've been running stuff in
production for a long time no problem
and then after a few weeks reality set
in and I started to realize that almost
all of the production failures we were
dealing with had nothing to do with
hardware and nothing to do with the
network they were divided roughly into
two categories one was operator error
and usually sort of an induced error
because things were misleading or
ambiguous and the other was software
defects operator error that's a separate
topic for a different kind of conference
a lot of interesting stuff around there
but the software defects are something
that we ought to be able to do something
about so I was one of the few people in
operations who had a background in
development and could look at the
software defects dig down to the
individual lines of source code and send
my bug reports with patches and fixes
back to the development teams and I
found that these problems came in
categories or types I started to see
recurring situations commonly recreated
problems this is actually a hopeful
discovery because if problems come in
types then maybe we can find categorical
solutions for those types of problems so
I took my my experiences of you know
living in another field and sort of
being in a foreign country for a while
and I tried to bring them back into the
development world with this book release
it
which is all about you know what do we
do to make our software run in
production how does it how do you get it
to survive more than just Hugh a how do
you survive contact with the real world
and as I got into this I got in touch
with some of the long and rich
discipline of reliability engineering in
other areas mechanical and electrical
and civil engineering and so on so I
want to start by providing just a little
bit of terminology a couple of
definitions the first is availability so
what does it mean for something to be
available it's the probability that the
system is operating at time T so when I
try to do something is the system going
to successfully complete that thing now
notice this doesn't talk about servers
being up and running it doesn't talk
about hardware being alive or networks
functioning it actually talks about
being able to complete the mission that
I'm giving it so if I'm trying to do a
business process I can complete the
business process if I'm doing a
transaction I can do a transaction and
so on now availability is not something
you can directly control it's the result
it's an output variable it's a dependent
variable based on your ability to create
stability in your system and stability
is the architectural characteristic that
allows you to maintain availability in
the face of outrageous fortune the
slings and arrows of faults and errors
now just a little more terminology when
I say fault what I'm talking about is an
incorrect internal state in your
application so a fault arises in your
application through one of two ways one
is that you've got a latent defect and
that defect creates an incorrect
internal state the other place that a
fault can enter your system is through
the perimeter through interfaces into
your system bad data that you're not
correctly checking for you might call
that a defect of omission malformed
network packets are a great place to see
faults being injected now a fault is not
necessarily that bad faults do happen
there are two broad
schools of thought on how you deal with
faults one is fault tolerance means more
or less exactly what it says you can
tolerate faults and recover you can
repair the incorrect internal state and
get back to a nominal operating state
the other approach is fault intolerance
fault intolerance basically says as soon
as I detect an incorrect internal state
exit terminate the fault intolerant
school says there's no state as clean as
your initial startup so the best thing
to do is to exit and let somebody else
start you back up this is the school of
thought that we see in airline or actor
based systems fault tolerance tends to
be the state you see or they approach
you see in languages with exceptions
like Java c-sharp Scala Ruby etc now the
reason that we do any of these
approaches is because we want to prevent
a fault from creating an error the error
is when the fault becomes observably
incorrect so fault can be latent in your
application for a long time but if
nobody ever sees it it doesn't really
matter
it's a haizen bug the error is when you
actually produce incorrect output or you
start taking behaviors that you would
normally not take so maybe it's not an
output but if your system suddenly
begins
hammering another remote system with
excessive requests that's an error even
though it's not really an output being
produced some errors lead to failures so
failure is a loss of availability you
can no longer accomplish the mission
that your system is meant to accomplish
and failure is what we really most want
to avoid so within this framework we're
going to look at some of those recurring
patterns that I observed that create
failures on a frequent basis now I refer
to these as anti patterns so if a
pattern is a solution to a problem in a
context an anti-pattern is simply a
solution to a problem in a context that
produces a worse context as a result
it's something that people rediscover or
recreate a lot intentionally or
otherwise but we don't like it it
produces outcomes that we'd rather not
have so the first and most important I'm
only doing a subset of the anti-patterns
because the the whole thing takes about
three hours and I definitely don't have
that much time so I'm going to give you
the most important ones by far the
single most important one is integration
points an integration point is any place
where your application makes an out of
process call so you're talking over a
socket a pipe
maybe you're forking a new process
remote procedure calls all of these
things are out to destroy your system
you should look at every integration
point as an evil to be avoided if
possible and contained if not possible
and I'm going to show you an example of
how even your friendly neighborhood
database is out to get you now most of
you have probably encountered things
like database deadlocks
right you've got two threads one's
holding a lock needs another lock the
other one holds that lock needs that
lock textbook problem in fact it's been
addressed in the textbooks for I don't
know 40 years that's actually one of the
easy situations the database server can
detect that problem and just abort one
of the transactions one thread gets an
error the other one gets to complete no
problem but I had a situation where
there was a database on the other side
of a firewall from my application server
and during the wee hours of the night
the database connections were sort of
idle for a long period of time and as
they were idle the firewall timed out
connections from its table of allowed
allowed traffic so firewall exists to
break the internet it's a computer with
finite resources it's got a table of
rules for when it allows packets through
on a packet that's trying to open a
connection it consults rules about the
source IP the destination IP the port
number all of that once a connection is
established it just allows packets
through but if it expires a connection
from that table then a new packet from
the application
server just gets dropped on the floor
the firewall doesn't send back an ICMP
reset which would tell the server that
the connection is no longer available it
just drops the packet on the floor so
TCP says aha your network is unreliable
I'm going to retransmit and it
retransmits and that gets dropped on the
floor and this keeps going until the
tcp/ip abort interval elapses the
default is ten minutes on that tcp/ip
abort interval so it's going to keep
retransmitting for 10 minutes 10 minutes
on a web application in a data center is
eternity 10 minutes might as well be 5
days so what would happen is the
connection would appear broken after it
had been idle for more than an hour now
this problem appeared at 5:00 in the
morning because during the night we had
few enough users that only one page was
being served from each application
server at a time the connection pool
that was being used to serve these pages
was last in first out so one database
connection would get exercised all
through the night the other 39 would sit
there aging and getting dropped from the
firewall 5:00 in the morning turned out
to be the first time we needed to serve
two pages simultaneously on one of these
app servers well the first one would get
served from the connection that had been
kept warm and the second one would well
it would block retransmitting and
retransmitting and retransmitting until
10 minutes went by at which point a
sequel exception would be thrown and the
application server said aha this
connection is no longer any good I need
to close the connection and discard it
well of course closing the connection
sends a packet to the database server so
that one gets dropped on the floor and
we go through the whole ten minute
retransmit timeout again so basically at
5:00 in the morning all of our
application servers hung every day until
we diagnose this problem it took a few
weeks now the interesting thing about
this is
the number of parameters that had to be
exactly right in order to observe this
particular problem so we're dealing with
a database connection every application
has a database you talk to databases all
the time
in order for this problem to manifest we
had to have a firewall in the way the
firewall had to have a short timeout on
its live connections table we had to
have a large enough application server
pool that we didn't need to use multiple
connections through the middle of the
night the connection pool had to be last
in first out not first in first out
otherwise they would have gotten
refreshed enough I can probably find
other parameters if the tcp/ip abort
interval had been set to a low number
then lots of people in the early morning
would get errors but the server's
wouldn't hang that might actually have
been worse because we wouldn't have
detected that problem for even longer
but my point here is really to tell you
that it's impossible to engineer away
all of the failures that could possibly
happen instead you must expect failures
to occur around every integration point
you've got and deal with them you need
to put things in place to prevent a
failure at an integration point like
this from propagating to the entire
system and taking your whole system down
now that a lot of developers in the
audience we're all problem solvers by
nature if I don't tell you how to solve
this problem you're gonna sit here
thinking about it and googling for it
instead of listening to the rest of my
talk so I'm gonna tell you how to solve
this one it's to enable a feature that
the database server has called dead
connection detection basically that's
just a way of checking if the client is
still alive and cleaning up server side
resources if the client is dead
periodically the database server sends
out a little ping packet up to the
client and says are you still alive are
you still alive you're not answering you
must be a Windows box you're probably
crashed I'm gonna clean up all my
connection stuff well in our case we
didn't actually care about things
crashing and disappearing we didn't need
the dead connection detection
to detect dead connections but the ping
packet was just enough to tell the
firewall that the connection was still
alive so that's how we solved that one
and the good news is you'll never have
this problem because you've heard me
describe this and you'll be on the alert
for it every time you see a firewall
between your application and your
database there's an alternative solution
where you can run a trivial query on a
much more frequent basis than than the
timeout in the firewall that one
requires a little more effort in the
code so I prefer the configuration
solution could we have prevented this
well we could have prevented it from
taking down the rest of the system so
one of the things that was going on here
is we would use a thread from a
connection pool up front or from a
thread pool up front that was receiving
the incoming requests that thread would
then be the same one that went and did
the database query and there was no
mechanism for another timeout to be
applied everything was synchronous and
on the same thread had we introduced
some asynchrony there maybe we could
have decoupled the front-end thread pool
from the interior thread pool that was
actually issuing the database queries
that's the type of technique you do to
make sure you can remain available even
in the face of partial failures okay
we're gonna leave the database behind
now and just talk about other issues
with integration points one thing that I
see a lot is that we test for and we
design for failures that are defined
within the specifications of the
protocols we're using so you know when
we're dealing with TCP connection
refused is a perfectly legitimate kind
of response to a TCP connection request
accepting the connection into a listen
queue but then never completing it and
leaving it in a half-open state that's
not really an acceptable part of the
protocol but it does happen when we're
talking HTTP getting back a response
code 500 perfectly reasonable we know
how to deal with that if you think
you're talking HTTP and instead the
server answers
with smtp you're gonna have a much
bigger issue right you're not actually
talking the protocol you thought you
were talking so none of your parsing
will work I love doing things like
testing services by sending an infinite
stream of open element tags when they
think they're talking XML blows out
nearly every XML parser on earth they
just can't they can't handle it and they
eventually consume all memory and throw
you know out of memory errors or just
abort a process these out of spec
failures are the kinds of things I call
wicked errors they do happen in
production they will never happen in QA
unless you force them to and sometimes
even trying to force them in QA seems a
little bit insane you know I'm designing
an integration point that's supposed to
talk JSON over HTTP why would I send
back streaming binary mp3 files well
just to see if the server can handle it
because somewhere some someday in
production someone's gonna send you an
unending stream of binary data maybe
they do it accidentally maybe they're
doing it deliberately to try and kill
your system another issue with
integration points is that debugging the
problems with them often requires
peeling back one or more layers of
abstraction so think about the database
problem that I was talking about in
order to observe that we had to get all
the way down to doing packet traces on
the wire to see a high number of TCP
retransmits well in the application code
we were dealing with a Java sequel a
sequel connection object that object is
an abstraction over some bytes in memory
right the object tells us how to
interpret the bytes in memory one of the
sections of bytes is supposed to be a
pointer to a socket we go and look at
the socket it's basically another chunk
of memory one of the items is a file
handle which is an index into a table in
the kernel that table just has a bunch
of numbers that say you know this
destination address port and sequence
number represents a connection so we're
drilling through many layers of
abstraction to finally get to what's
happening at a level where we can
diagnose it
the bigger your system is the faster
these problems will propagate so we tend
to think that scaling something up buys
us more time or gives us more ability to
survive but in fact it usually just
means that there's more traffic and you
amplify these errors even faster there
are some patterns that we can use to
help us with these integration points
I'm going to talk about a few of these a
little later on and test harnesses are
not something I'm going to be talking
about today but it's a way that you can
test for these conditions before you
reach production basically doing evil
things to your own system before someone
else does
once you have a failure in one node
you're actually more likely to get
failures in other nodes so it's tempting
to look at a horizontally scaled a farm
of servers or machines or containers or
whatever and statistically treat them as
identical independent devices they are
not they're highly correlated one way
that they're correlated is through your
software so the software you put on
these devices is always a common mode
among them in this case suppose we have
something that goes down due to a load
related condition maybe it's a memory
leak maybe it's a race condition where
the more load you've got the more likely
you are to observe the race condition
but in whatever case we have one of the
nodes S 4 here going down because of
load well the load then gets
redistributed on the other nodes not
only are they equally well if they were
uncorrelated we would expect them to be
equally likely to fail as the original
one but in fact we're distributing more
load on them so their failure given the
failure of the original one is more
likely you can watch these sort of
popping like a row of flashbulbs on a
camera and you'll see each one failing
faster than the one before
this is something that's really common
when you do have memory leaks because
things tend to get restarted all around
the same time when you deploy a release
and so they're all leaking memory at
roughly the same rate just one of them
tips over first and then the traffic on
the others causes them to go faster and
faster so if you've got a large array of
servers it is not like the lighted sign
in front of a casino where one lightbulb
out is no big deal and you wait until
10% of them are down to go fix it when
one goes down you need to find out why
and fix it as quickly as possible this
often happens with resource leaks and
connection pools and separating your
capacity into different bulkheads as is
one of the ways that you defeat this
once you have a chain-reaction
in one layer you need to be concerned
about what happens with other layers
that call into that one so this is
something that I think of as the micro
service system failure mode because we
often have this deep stack of tiers of
services calling other services that
call other services when one of those
goes down your system can either amplify
the failure and allow the the damage to
jump the gap or it can dampen the
failure sadly most of the ways that we
write calls between systems are
synchronous requests response calls with
no timeout and that amplifies the
failure so we really need to keep a
mentality of containing damage and
preventing it from jumping the gaps one
of the places you see this a lot is
around things like connection pools or
thread pools for making out calls once
those get exhausted you'll start
consuming the threads in your layer and
then you stop responding to your caller
timeouts and circuit breakers are
effective against this the next anti
pattern I want to share with you is
something that my friend Paul Lord
described as good marketing can kill
your system at any
time in fact the better your marketing
is the more likely you are to suffer
from this this is an attack of self
denial because you're doing it to
yourself very common pattern retailers
have have demonstrated this for us many
times I remember when the Xbox 360 was
brand new Amazon sent out a a poll to
all the Amazon Prime customers asking
which of these three things would you
most like a discount on during
Thanksgiving would you like a discount
on an Xbox 360
would you like an Xbox on a garden stool
that you can put your gardening
implements in or would you like a
discount on a bicycle well of course the
voting was like 98 percent for the Xbox
and so they offered a steep discount on
the Xbox 360 on a particular day at a
particular time with the timezone
specified so they made sure everyone is
gonna show up at exactly the same minute
of the same day and Amazon took
themselves down for 20 minutes and when
you think about the kind of capacity
they have you know that you're
vulnerable to this there are things you
can do to to defend against this make
sure marketing offers don't send out
deep links I had one client where the
marketer was just browsing on his own
desktop picked out a link that included
a specific machine that they were bound
to it bypassed the CDN because inside
the company walls they didn't want to go
all the way out through the CDN and
incur those costs and it had a session
ID in it that was the link that got put
into the email that got distributed to
everyone there are things you can do to
make serving the traffic lighter static
landing pages are a great approach you
can even put a static landing page out
at your CDN and only allow the second
click to reach your actual servers and
there are things like throttling and
lightweight pages that you can do the
catch is this only works if you know
that the traffic is coming and in many
companies marketing
gotten the the no from IT so often that
they don't even talk anymore so one
thing you can do is try and re-establish
lines of communication by talking to
your marketing department and saying hey
can I see the marketing calendar first
of all it'll be fun to watch them faint
because someone from IT is actually
interested in marketing when they get
back off the floor they'll be happy to
share the marketing calendar because
these things don't happen
instantaneously they're planned well in
advance and if you don't support your
marketers they will work around you and
they'll go through third-party services
and do things that might jeopardize your
systems even worse than the original
offer that they were contemplating so
moving on another big issue is scaling
effects so we tend to build things that
are like 1 to 1 to 1 you know I've got
my service and I've got a thing I'm
calling and I've got a database and
they're all sized about the same there's
one instance of everything running on my
box and then you get to production and
you're like well my service has five
instances and it's being called by
something with 4,000 instances that
ratio is a little bit different the
traffic patterns are a little bit
different and it can cause a big issue
for your services for example one of the
communication patterns that never
belongs in production is point-to-point
messaging so this was a particular
platform where they had a caching layer
as part of their database abstraction
tool and if you wrote through to the
database your node would communicate to
all the other nodes to knock that item
out of cache well when you're in dev
it's just a local call it didn't even go
out through a network in QA it was sort
of unnoticeable because there was one
local call and one T speed connection in
production it's kind of going up as an N
squared function right so in production
we had about a hundred nodes each one
would open up 99 connections send one
packet to each and then close 99
connections almost all of our network
bandwidth was just these cache
invalidation
notices why was it point-to-point no
reason at all that should have been
broadcast so watch out for this type of
communication pattern another place
where we see scaling effects really
kicking in is with shared resources so
anything like a cluster manager a lock
manager Network components like
firewalls API gateways ESB appliances
these kinds of things they're all a
place where you have a fan in from some
horizontally scaled thing to some
vertically scaled thing you can always
win at horizontally scaling over the
amount of vertical scaling you can do
now this is not the type of thing that
you can test out of existence very
easily because we don't build QA
environments up to the same size as we
do production it's not economically
viable so this is a place where you have
to apply some engineering you know desk
check the ratios among the things that
you're dealing with all the different
layers of your system watch out for the
things that are invisible in your
typical diagram so normally when you
draw an application architecture diagram
you don't depict network components
they're part of the substrate but the
network components are a funnel that
traffic goes through even after you've
done that desk checking you may run into
this kind of a problem where you have
different sizes at different layers of
your system and you have that
deliberately because most of the time
most of the traffic from something like
your online store doesn't go through to
order management and so it's kind of
okay that the online store has six times
as many threads available to make calls
as the middle tier has to respond to
them but if traffic patterns change
you're not really stable you're only
sort of stable by convention and under
normal patterns so at this particular
retailer there was a big offer during
Thanksgiving weekend offering free home
delivery and installation of
big-screen TVs just in time for the peak
of football season and that caused a big
change in people's traffic patterns
suddenly a lot more people were looking
up delivery dates and availability for
home installation and so a much larger
fraction of those online store threads
were making calls to order management
and it was making more calls to
scheduling than normal
well that caused everything to to pile
up so the scheduling system stopped
responding the order management system
got slow couldn't actually process any
orders because all of its threads were
busy trying to look up delivery dates
and we had to sit there on the online
store turning on and off features all
weekend long to try and you know eke out
a little more revenue and then turn it
off when things were getting wobbly so
it is a type of scaling effect it's very
common in a large enterprise because you
you tend to build each of these things
in isolation in different groups we can
do some desk checking on this and see if
it's a potential problem when it is a
potential problem your best bet is to
stress both sides of the interface so
unhook the front end collar and put in a
load generator that calls the back end
with much more load than its intended to
take and see what happens does it simply
crumble or does it give fast errors does
it get slow maybe it's good and it sheds
load and tells the front end I can't do
this right now ask again later do the
same thing with the front end and see
what happens when the calls it's making
gets slowed or stopped responding your
front end should be safe against that
kind of thing but you need to test for
it and observe what really happens now
I'm emphasizing slow responses because
this is another anti-pattern I would
much rather have a fast no or a
connection refused then someone say yeah
I can try to get that done and then drag
it out and drag it out and drag it out
oh I'm supposed to be talking about
systems not co-workers with systems I'd
rather have a fast
you're fast no than a slow response or a
slow failure so you look at something
like a TCP connection refused and that's
like a one microsecond operation you
look at something like allowing a
connection into a listen queue and then
just not getting around to it until it
times out and that's like a one second
to one minute operation so let's not tie
up the resources on the front end
because the longer the responses are in
a service provider the larger the caller
needs to scale to handle the same amount
of traffic so it's actually like a gain
knob on our scaling of the calling side
and very often slow response ends in a
failure or a timeout and the work was
wasted anyway this can creep in in a lot
of places excess load is the most
obvious one and the most common one but
we can also find things like the network
being too busy so I've been in companies
when they were dealing with a worm
rampaging around their network and
suddenly you know their networks at like
80% saturation all the time and back-end
calls get really slow components like
firewalls can get overloaded you know
the first time I had a network admin
tell me that the firewalls CPU was at a
hundred percent it was a revelation I'm
like oh yeah they have CPUs they can be
overloaded to some protocols you really
need to be where NFS has retries
built-in DNS has retries built in in
fact some older versions of NFS had
retries built in in the kernel that were
uninterruptible your own remote
protocols may be an issue as well if you
do things like an N plus one query
pattern you you ask for the extent of
all the results and then you ask for
each result one at a time slow responses
are one of the ways that damage jumps
the gap and causes those cascading
failures they do provoke people into
pounding reload at the end-user level
and they provoke calling systems into
retrying their calls so we want to watch
out for that the final anti-pattern i
want to talk about is unbounded result
sets so when you make a query you don't
often get too
by how many results do I want if you're
going through something like an ORM it's
definitely not the default behavior and
if you're going through an ORM and
actually traversing a relationship from
a parent record to a child record type
it's very difficult to say no I I really
only want the first million of those
things don't send me all 12 million
sequel queries you can add limits to the
clauses but you know you have to think
about it and the syntax is a little bit
different for each flavor of sequel I
often see this with service
architectures so you'll again have this
n plus 1 pattern where you ask for
the extent and then you ask for each
item so you want to make sure that you
test with real data volumes and real
relationships and real distributions so
you know if you are building a social
network make sure you're testing with a
power law for the number of connections
that each person has not a Gaussian all
right because you'll find those those
black hole people who have 50,000
connections instead of the median of one
connection okay so that was kind of the
bad news section I want to talk about
the good news you know one of the
patterns that you can use is time outs
so a lot of the anti patterns I
described are just failures to apply
time outs and and the willingness to
block infinitely things like connection
pools almost always have one version of
check out that blocks forever and one
version that has a timeout I wish people
would just get rid of the version that
blocks forever and only supply the
version that takes a timeout when you
apply timeouts you have to decide what
you're going to do if the timeout occurs
so do i retry the work do i queue it up
to call it later
how am I going to handle that do I just
report error to the user another area
that I'm really leery about is binary
client jars or client dll's that sort of
thing
because they're written by programmers
just like us they don't have enough
time they don't get to do enough testing
maybe they don't know about these kinds
of problems the main difference is you
can't see it and so when there is
something problematic inside that code
it's much harder to get to
so timeouts are pretty useful against a
lot of those anti-patterns circuit
breakers are kind of the the pattern
that's gotten the most popular and
caught the most buzz I guess the idea is
we shouldn't be doing retry loops like
this where you you attempt an operation
it fails and you basically immediately
attempt it again it takes what 1
nanosecond to get from the bottom of
that while loop back up to the top what
kind of problem disappears after 1
nanosecond what makes this more likely
to succeed on the third attempt or the
fifth attempt instead of the first and
if the third attempt is more likely
shouldn't I set my retries to something
like a thousand well I don't want to I
don't want to do the retries at all the
problems are very likely to still be
present 1 nanosecond later the only kind
of problem that disappears that fast is
something like a dropped packet which
your lower level protocol takes care of
anyway retries make users wait they tie
up calling resources on the system they
very often end in failure anyway so
you're just delaying the response and
provoking a slow response so the idea
with a circuit breaker is we wrap it
around any kind of a dangerous call and
you've already heard me talk about what
dangerous calls mean we count failures
in making that call and when there are
too many failures we we basically cut
off the interface and say we're gonna
return an immediate error or an
immediate fallback rather than making
the call all the way through after some
period of time has elapsed and and
usually this is a human-scale period of
time I'm willing to try again to make
the call if the call works then great
I'm back to normal operation and I can
reintegrate the feature into full
operation if it doesn't
work then something's still wrong and I
go back to the state where I don't keep
trying one of the nice things about
using the circuit breaker is it doesn't
require every thread and every request
to discover anew that the thing it's
trying to talk to is broken you know if
you got a timeout one millisecond ago
why would you issue another call when
you think you're going to give another
timeout there are loads of open-source
libraries that implement circuit
breakers I definitely recommend picking
up one of those rather than doing your
own work because you are sharing
information across threads that's tricky
business I do not recommend sharing
circuit breaker State across all of your
calling services and that's basically
just a trade off between saying yes more
of my nodes have to independently
discover the failure versus the risk of
new failure modes from sharing the state
across the server's you definitely need
to talk about what to do when the
circuit breaker is is popped
what's your fallback strategy can you
use cache data do you need to cue the
work for later retry is there a
secondary service you can try these are
all legitimate approaches it's both a
technical and a business process
discussion to have exposing the state of
circuit breakers across your system
makes a great way to look at sort of
overall health if I have 20 nodes and
they're all showing red on the same
circuit breaker then I'm pretty sure the
problem is with the provider if I have
20 nodes and two of them are showing red
then it's likely that the problem is on
the calling side or there's an
intermittent problem in the network in
between them so next pattern I like to
apply is something I call bulkheads and
the metaphor here is from ships
never mind the Titanic they were just
implemented wrong there the design was
fine it was bad implementation the idea
is that we compartmentalized the system
so that some parts of the system can go
down and other parts survive
when we create multiple thread pools
inside an application we're creating two
compartments or multiple compartments
with bulkheads between them process
binding to CPUs
again compartments with bulkheads one
approach that I particularly like
especially in the sort of SAS arena is
different server pools for different
priority clients so if you're on a
freemium model you know you should
definitely think about allocating some
pools to your premium clients and having
all your free clients kind of mingle to
get mingle together in steerage class
bulkheads really helped with this kind
of common mode dependency foo and bar
here can each damage the other in a
denial of service by over-consuming Baz
if you can separate them into different
pools with dedicated clients then
neither one can harm the other now one
thing about doing the bulkheads is that
you really need to pick a useful level
of granularity to apply this at it's
it's not helpful to do it at every level
so you don't need to do thread pools and
process binding and server binding and
containers and so on okay I am going to
jump ahead just a little bit the next
really important pattern is fail fast
it's often possible to find out whether
an operation is going to succeed before
you get all the way into it
so there are things like acquiring
critical resources and checking
parameters and checking on your internal
state these are all ways of detecting
whether you're going to be able to
complete a request before you've spent a
lot of time doing it it's useful to do
this sort of thing so that you dampen
failures so if you can apply fail fast
and do it by looking at the state of
your circuit breakers then you can say
you know damage to this other part of
the system would normally put stress on
the entire enter
but I'm actually gonna act like a shock
absorber and not transmit that damage
upward checking resources and
integration points via circuit breakers
is one of the great ways to do it
validating input is another great way to
do it we can often apply more validation
than you would think it sometimes looks
a little bit like breaking encapsulation
but I could also make a case that it's
it's it can be done by providing a
richer API on your domain objects the
final stability pattern I want to talk
about is using decoupling middleware and
basically this is all about going async
if we look at this spectrum of coupling
that I think David Gardner originated
there are a lot of positions along this
spectrum that we can explore when we're
doing in process method calls we don't
worry too much about differential
availability if the library is there the
caller is there because you're running
in the same process we keep reinventing
remote procedure calls under different
names about once a decade we get a new
acronym for remote procedure calls but
there's a lot of value to be had in
moving further to the right on this
spectrum when we do messaging for
example we have the ability for the
provider to be offline while the caller
is still functioning or vice-versa we
have the ability to absorb a spike in
load and spread it out over time because
we've got messages in a queue that we
can digest at the throughput of the back
end and so those unbalanced capacities
and scaling effects are much easier to
deal with
now unfortunately changing middleware
often requires a rewrite it's not a
simple kind of drop in thing switching
from you know wrests HTTP JSON calls to
Avro messages on a caf-co bus yeah
that's a that's a big change and
sometimes those decisions are made at a
level where we don't really get to
counterman them but I'll say the more of
these different architectural styles you
can
having your toolbox and the later you
can make the decisions the better the
chances you're gonna have of creating a
system with a lot of stability so we
have to accept that problems are going
to occur faults are going to be
initiated we're gonna have bugs we're
gonna have latent issues the big issue
or the big question is do we amplify
those throughout our enterprise or do we
find ways to dampen or nullify them and
I view this a lot as acting like cracks
in metal so when metal is fatigue a
crack gets started someplace and then
maybe the structure of the metal is such
that the crack gets stopped at a
crystalline boundary and nothing bad
happens if that doesn't occur the crack
accelerates at supersonic speed through
the metal and you have a catastrophic
failure of the metal something like a
plane fuselage ripping open we don't
want our plane fuselage is ripping open
we want something more like this where
we've got cracked Stoppers we've got
ways of dampening the failure and
preventing it from propagating and
accelerating the system is still damaged
we can't prevent that completely I hope
I've convinced you of that with the the
5 a.m. problem with the database but at
least we can preserve some features for
some users and we can prevent those
catastrophic failures and with that I
thank you for your attention and I
believe we have time for some questions
thank you Michael
I'm very interesting we do actually have
question here I'll bring up first and
then we have time for some more possibly
what are your thoughts on tools such as
chaos monkey I'm a big fan of chaos
monkey and the whole simian army I want
to see them go on a world tour I think
those those tools are fantastic for
exactly the the reason that I said it's
hard to test these problems out in QA
you kind of need real production
environments real scale real topologies
and I love the idea of forcing yourself
to be anti fragile by creating problems
when there aren't enough I liken it to
the the theory that autoimmune disorders
are on the rise because our immune
systems aren't challenged enough and so
you know they're immunologists are
recommending that we go out and eat dirt
to get more challenges to our immune
system the chaos monkey is kind of like
eating dirt okay we actually did have
another question we have another
question here how can ma during
monitoring system design help or hurt
cascading failures that's a fantastic
question
unfortunately there is a there's a big
issue with a lot of monitoring systems
in that they only report when things
happen they don't report when things
don't happen and so this is one of those
places where the absence of a signal is
the information you want so I would look
for monitoring systems that know how to
collect heartbeats and as soon as you
stop seeing heartbeats that's a big
issue okay do we have it yet we have
time for possibly one other question if
somebody didn't put one in and what like
to bring one up anybody else go back
here okay so what we'll do one more here
you go let's get the mic so that we want
to make sure that everybody hears it
when's the second edition coming out no
comment
oh that was a quick answer anybody else
want to throw one end before we break
for lunch or everybody's ready for lunch
okay listen I'm ready</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>