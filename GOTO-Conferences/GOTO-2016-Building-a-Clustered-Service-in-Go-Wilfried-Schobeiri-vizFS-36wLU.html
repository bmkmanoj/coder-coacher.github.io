<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Building a Clustered Service in Go • Wilfried Schobeiri | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Building a Clustered Service in Go • Wilfried Schobeiri - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Building a Clustered Service in Go • Wilfried Schobeiri</b></h2><h5 class="post__date">2016-10-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vizFS-36wLU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm will
I'm with me the MS and I'm gonna talk
about clustering and go so just as a
heads up I'm gonna do a little bit of
like live demo stuff which is probably
going to catastrophic Allah fail and if
that does we can all just laugh at me
I'm totally cool with that so a little
bit about myself actually real quick who
here uses go in some way I know you do
all right handful anyone written any go
for fun that didn't raise their hand
okay so some light amount of experience
did anyone actually come to go to last
year in the crowd well I know you guys
did that's not fair okay did anybody
actually come see my talk last year on
go this is not the same talk so don't
worry I'm not like shilling the same
same stuff anyway a little bit of a go
enthusiast I actually sadly don't write
a lot of code anymore myself which you
know is just what it is I guess
sometimes in life and career whatever I
lead up the infrastructure and platform
teams at media ass and I'd be remiss to
say not to say that we're hiring because
we're hiring so if you're interested in
those sorts of things come talk to me
later okay so really quickly why go this
is not a go pitch I'm not going to
actually pitch you on using go in fact I
think I'm gonna say some things that
might make you think that go is not good
which is totally not true go is awesome
but like the really quick basic things
which I'm glad that none of you came to
my talk last year because this is
literally the same slide almost it's
easy to build services like it's just
really quick to build micro services
since that's the buzzword it's got a
great standard library if you want to
build something like truly from scratch
you have pretty much everything that you
need to do that out of language and if
you don't want to build something from
complete scratch there's a ton of and a
very quickly growing community that has
a ton of libraries and a ton of
utilities which some of a couple of
which we'll talk about soon but you know
topic du jour if you like docker that
things and go and there's like a lot of
other awesome examples out there the
built in tooling is also amazing in fact
I think this is one of the places where
Co actually really excels over other
languages and tools like Go fum
so the formatting built-in formatting
stuff that exists within go that makes
it possible for you never to have an
argument about whether or not the brace
goes on the next line or whether or not
there's a space between the function
name and the open parens like I'm really
happy that I never have to have the
argument ever again
go tests which is you know a package
that basically has baked in tests and
make it really easy to build your own
whether it's you know unit tests or
functional tests or whatever go vet
which basically says hey this code looks
suspicious you could make it you should
make it better that's not really a good
description you should go look it up if
you're interesting to go and then like
the race race condition checker which
has been quite handy at times for me
something you'll see a couple times in
the demos that I'm gonna hopefully make
work in a second are the compile times
they're like really fast it's really
like more of a scripting language when
it comes to just being able to do you
know control see up arrow enter and test
real quick so that's super spoiling and
it just feels productive I can't
quantify that in any meaningful way but
I like it because I but the code that I
wrote to put this thing together I don't
think it took me more than like six
hours anyway
cool so why talk about clustering go
well clustering is not really batteries
included
in go and when I say batteries include I
mean like there are a lot of other
languages that have things that are
really closely tied to language or
closely affiliated with language that
are like the preferred mechanism for
clustering so things like gosh akka
cluster or ylang OTP like quas are I
guess like stuff like that like go
doesn't really have a de facto that I'm
aware of and there are a lot of newer
libraries but none of them are really
that mature from what I've seen so like
I think beehive and go circuit are a
couple that come to mind and so more
often than not services that are out
there kind of roll it roll it themselves
and I think a decent example of rolling
it yourself and then having to roll it
again a couple times as you understand
you're laying problem space better has
been like in Flex DB which has had to
actually go through and you know rebuild
a bunch of they're clustering a few
times over for good reasons so what I'm
going to walk you guys through is like
one way of building a clustered stateful
service there are many ways to do that
this is not the perfect way this is just
a way I only have so much time to
actually like demonstrate a couple
different tools so you know you're gonna
get to see those tools in action and
yeah cool so let me just set up a fake
scenario for the thing that we're
building um let's say let's say you're a
business that has like multiple data
centers each of those data centers is
like distributed in different parts of
the world so hypothetically like Chicago
Hong Kong Amsterdam I picked those three
in particular because there's like
anywhere between 180 260 milliseconds of
latency between each of them which means
these events happen when they happen
concurrently it's non-trivial to tell
the other data center that they happen
and certainly you can't do so in a way
that says
wait you can't do anything else until I
tell you and so what we want to do is we
want to build a service that counts
those events this is actually not too
dissimilar from what we've got at medium
F we have several thousand servers we
have millions of events per second we
have nine data centers we have you know
tons of concurrency tons of different
types of events and it's really
important for us to be able to handle
those events with a specific bounded
latency budget and if we miss that
budget we effectively are missing out on
you know potential money so the
constraints that we have here is that
Counting should be fast right we can't
actually afford to cross the ocean every
time we can't afford to wait on that
transfer back and forth every time an
event happens certainly and certainly
couldn't do that synchronously and count
should be correct so we should try to
make it possible not to lose events
right as they as they are effectively
incremented upon so for those guys who
are familiar with the cat theorems is
starting to look a little bit of like an
AP system right basically I need I need
my operations to occur and be available
often in the state of a potential
partition cool so let's get started
just as our bedrock I'm going to do a
really basic counter service so we'll
start with one node and a atomic integer
nothing fancy right so those two little
curl lines and I did write the code will
demonstrate in a second but that's
basically what you're going to expect
right it's if before I do the increment
if I do it get on it I get zero after I
get one yay cool so this is this is the
code it's pretty much exactly what I
just said there's a counter with an
integer if I run an increment on it it
increments the counter if I count then
it tells me what the counters value is
so I'm going to do this in my browser
and hope that it doesn't break cool I'm
incrementing it by one and incremented
counter to one wow this is working so
well okay it's incrementing me three
four or five yeah okay
close that awesome all right well that
was really nice
that worked it's always good good to get
a running start with the demos that
might totally crash and burn
Oh Microsoft Area reporting like Excel
crashing and burning so okay so that's
cool we have one note in one data center
not particularly useful have definitely
not met the needs of the hypothetical
situation that I brought up which is
lots of data centers lots of events
happening distributed so next step is to
co distribute said said service so we're
gonna take that same exact thing that we
just built we're gonna put it in several
data centers and then we're going to
route those increment requests to the
closest node and so let's let's let's do
that real quick let's see all right I'm
sure some of you guys in the back can't
actually see what's going on
go run by the way every time I do this
I'm compiling so that was pretty quick
fast rate alright go run go port equals
4,000 to go run go port equals 4,000
three alright so I've got three three
nodes and now I'm gonna increment them
awesome it still works this is the best
day ever cool so I just did seven
increment requests on that first node oh
you can't see it can you
damn it hold on let's fix this magic Oh
what is this
that's Excel guys I love Excel okay so
here's my second node Oh No see I got
two like twelve here but now when I hit
refresh on this one it's only two hmm
all right only got two five well they're
counting they are counting that's pretty
cool by the way if if I were like
actually a truthful person I would I
would put each of these in parts
different parts of the world but because
I'm on conference Wi-Fi we're just gonna
do on my laptop and pretend so all right
so I just did a bunch of increment
requests let's make these go away a
bunch of the increment requests they're
different on every node and its really
obvious because we're not replicating
state all I did was take that counter
and like deploy it three times so well
you're boring us I know so basically we
in order to actually get this whole
thing to work we need to counter to talk
to each other which means we need the
nodes to actually know about each other
which means that we need to solve for
cluster membership so I think I
mentioned at the very beginning that
this is a stateful service right this is
when I say stateful I mean like if you
were implementing a database you would
carry the state locally one option would
be obviously to like take something like
a Cassandra
and let it do all the work for you
that's cheating you wouldn't be here if
all I said was using Cassandra with go
and then like five minutes later it's
done hooray bye so what we're gonna do
is we're gonna use member list and
member list is a go library that's built
by Hoshi Corp which or I guess managed
oversee yet built sure by hashing Corp
which is an amazing company it does
really good work
they built vagrant in console and
hatless and terraform and all sorts of
really great DevOps magic sprinkly dust
tools basically what it does is it
manages costs cluster membership it's
based on swim which is a gossip style
membership protocol I'll speak more a
little bit more about that in a second
it has baked in member failure detection
which is really important right because
if you're managing a member a list of
members you want to make sure that you
have an accurate list of who you should
be talking to and this library is
actually in use by like console docker
Lib network anymore actually if you want
some fun like go to github and do the
whole code search thing type in hash you
Corp slash member list and there's like
I don't know forty pages of results of
people actually implementing it so swim
is scalable weekly consistent infection
style process group membership protocol
that is way more than four letters it
basically has two goals which is to
maintain a local membership list of non
faulty processes local like local to the
node and then detect and eventually
notify others of process failures and so
mechanically
the way that works is it's gossip based
and basically that means it's typically
these interactions happen node to node
like one node to another node and so
when a node joins a cluster it does a
full state sync with an existing member
and it begins gossiping its existence to
all of the other members in the cluster
gossip about that internal member list
state actually happens on a regular
interval and against a randomly a number
of randomly selected members so
basically what this means is on it like
every three seconds every five seconds
or so I'm going to gossip to like
another three people hey I'm here here's
my state
there's also this thing called probes
and probes or basically hey are you here
and and there's an expectation there is
for you to say yep I'm here I'm live and
so if a node doesn't act a probe message
it's marked as suspicious and because of
gossip whenever you mark a note as
suspicious you actually tell the rest of
the cluster well sorry you gossip to the
rest of the cluster
hey this notes suspicious and then if
it's mr. snowed suspicious no doesn't
dispute suspicions after a certain time
out it's marked as dead which is also
gossip doubt so you end up getting this
kind of cool eventually consistent like
weekly consistent set of members that's
managed across the entire node so the
entire node eventually knows that a
particular set of members exist and
every so often a full state sync
actually happens between like two
different random members and depending
on how big your cluster gets like this
is really okay and fine in my you know
hypothetical cluster of three that's
running on my laptop but if your cluster
is tens of thousands of servers you
can't really afford to do this all the
time unless you have infinite bandwidth
which is not a thing the speed of light
is like way too slow for that and so the
trade-offs between the bandwidth and
converges time here are configurable
right you can you can say I want more
bandwidth to be used by this process and
converges time across the cluster will
be less or vice versa
cool so your nodes becoming cluster
we're using member list is actually
relatively straightforward this is the
code snippet behind it you basically set
it config and then create the member
list instance and then join another
member list node if you know there are
others that's pretty much it at that
point then the cluster does does things
on itself on its own so let me show you
the whole does things on its own part
we're done with that last example member
list all right cool
all right so what I'm doing right now is
I'm spinning up this is the first node
this is the seed node so I've got a note
here who is basically the first node in
a particular cluster I'm going to spin
up a second node here in the middle and
I'm gonna actually make this little bit
bigger and try to explain it what's
happening so here we have I don't know
if you can see this cool so what
happened was
remember the member joined and now we
have some syncing happening so that full
state sync that I talked about a second
ago is actually called a push-pull sync
so let me show you really quickly this
is what the cluster looks like right now
I have two nodes in it I have this this
node which was well the second node
because maps aren't ordered in go or
lists are in order and go I guess Lysa's
I should say and then here we've got the
first first node okay cool so I'm gonna
spin up a second one or sorry the third
one
remember list all right and we've got
three that was pretty quick that's
because of the gosset mechanism so let
me let me actually go back and take this
one out I just killed it
so if you watch over here it's gonna say
suspect so suspect has failed and no axe
received so this has failed no accident
received it's going to continue to do
that until this point where it says
marking that node has failed because the
timeout on it being suspect has actually
been reached so if we go back here and
hit refresh now there's back to two
nodes relatively straightforward again I
can configure the settings that this
entire handshake process will happen or
gossip process I should say will happen
on a regular basis and I can either make
it converge slower or faster in this
case because we're doing a live
demonstration I've actually cranked up
the time so that I can show you things
as they're happening
and yeah cool
awesome so I have this counter they're
still disconnected what are we still
doing here why haven't you solved the
problem we'll okay fine
so I still need these counters to
actually you know interact with each
other in some way so the way that I'm
going to solve for this problem and
again we mentioned at the very beginning
this is an AP like system I'm gonna use
CRT T's mainly because I think CRD T's
are super awesome and super cool and
like the best invention ever and I will
say really quickly I'm going to attempt
to explain it I actually have some
drawings that are gonna attempt to
explain it I would really recommend if
you're into distributed systems or
actually even if you're not read or
watch Scott Michael John's content on
this especially about last he talks a
lot about sturdy T's because they are
actually pretty cool so what's ERD T
stands for is it's a conflict-free
replicated data type these are data
types that are typically like things
like counters sets maps Flags registers
the operations within those types have
to be associative commutative and item
potent and because like those are
principles if I believe it's called
implement sorry implemented through
semi-lattice algebra you basically get
this thing where you can item potently
like you can basically merge all the
time right so the side effect of that is
there is no order or there is it's it's
technically partially ordered right but
the the data types here do not actually
have or support someone's of order and
because of that in almost every maybe
every failure scenario all you have to
do to get these counters to merge again
is just to merge again like that's
literally it if you have two nodes that
can't communicate and then finally they
can just have them merge again if a node
dies and comes back and is out of sync
just have them merge again like there's
literally every scenario that you have
if you have like several divergent
States just have them merge and then it
will
come to come to consistent converging
state or rather converging state so CRT
T's because of that again they're by
nature eventually consistent there is no
single source of truth this works really
well for our made-up scenario so yeah
the most basic of these the one that I'm
actually gonna implement is called a G
counter it is very cleverly named that
way because it is a grow only counter
and it's a counter that has literally
just two operations increment and merge
right so it only goes up it never comes
down now if you actually want one that
can come down too that's called a PN
counter it's implemented by actually
taking two of these counters one goes up
one goes down sum them together
problem solved so the way that G
counters work is each node manages its
own count the nodes then communicate
their own counter state with other nodes
and then when they merge they take the
max count for each nodes value so the
value itself of the G counters of sum of
all the node counts so let me just give
you an example out here we have three
three actors Alice Bob and Claire Alice
does an increment of one Bob doesn't
increment of 1 and then Claire at some
point doesn't increment of 6 so Bob
sends its state to Claire and so Claire
here at this point has knowledge of Bob
State before this increment of 3 and
knowledge of Claire's own state because
Claire you know did set action and so at
this point Claire knows that this
happened and this happened and counts
both of those things right so in her
storage she knows both Bob and Claire
and can sum them together to 7 next step
we have another sink here that happens
so we have Bob actually sharing state
over to Alice and so now Alice knows
that Bob is at 4 right because 1 plus 3
is 4 awesome
and Alice herself has done this
increment one and you've got this that
is not 4 that is 5 yes fantastic these
counters are definitely divergent okay
cool so now we have like this this other
jump here and I hope that this map is
right it seems to be that's pretty cool
let's just pretend that didn't happen
okay so Alice is now gonna send her
state which she understands the system
to be what she what she knows and what
Bob knows or rather what bob has told
her up until this point so she's gonna
send all of that down declare and since
Claire has understanding of Bob and
obviously is the canonical owner of
Claire's own data you have Claire now
understanding Alice's won Bob's for
right because at this point four is
bigger than one right so we should take
that value because Bob is the canonical
owner of said value and then Claire so I
did the math again in my head 1 plus 4
plus 6 does equal 11 so that number
there is correct that's basically
basically G Conner here's a really you
know two since since drawings are
actually not sufficient and we need code
this is basically the implementation of
it so the increment here says you know I
have my ID n which is identifier which
represents me and then for every
increment I'm only going to increment
the number that I own to get the count
I'm going to sum all of the numbers that
I know and to do the merge I'm going to
take the largest of any given value that
isn't me cool so let me show you this in
practice
all right cool so just didn't increment
on the first node that number is
increasing and oh that's too big now we
have this is the this is actually a raw
view of the data structure underneath so
as I increment this guy this is my node
name this this I here I'm really unique
and so I like URI IDs for node names
because they have a ton of uniqueness to
them and then these seven is the value
that I have for my counter if I do the
same operation here on the second node
you will be not surprised at all to
notice that they aren't sinking but now
I have a second ident with that
UID of that second node and that is here
the value that matches right so if I go
over here this is a totally different
set of data same thing if I run it on
the third node this make sense is
starting to make make sense come
together awesome well we still have a
problem which is we haven't merged state
yet so we should probably merge the
state and that's what we're gonna do now
so member list has this thing which I
mentioned before which is that kind of
complete state exchange with another
random member that's called the
push/pull sync in the code it actually
also provides developers a way to
piggyback off of it
and because it this happens over TCP you
have basically anything that you can
send over the wire you can choose how to
serialize it over the wire and how you
want to deserialize on the other side so
there's a delegate has basically two
functions that are important relevant to
this point it's local state and merge
remote state local state which basically
kind of dumps the state of someone who's
being asked to provide State and merge
remote state is does the opposite it
takes that remote stay
and merges it in and that again that
interval of that full state transfer is
configurable and so we're gonna use it
to eventually emerge the state in the
background so this is what the code
looks like it's exactly what I just said
I'm in a marshal that the state of that
CR DT to JSON and then on the other side
I'm going to instantiate a CR DT for
that external cnut JSON that was just
sent over the wire and then I'm gonna
merge it into my CR DT so let's do that
real quick and this is the demo you've
all been waiting for
cool all right
all right so as you can see I mentioned
there's a state transfer that happens
here so this is sharing a road state for
push/pull sync and merging it there's
not really any state at this point if
you just go in and look at the contents
there's it's empty right there's nothing
to talk about
however if I go in here and increment on
node 2 you see this one that one is
eventually going to show up here
eventually there it is landed right and
it's already here because that's where
we did it it's also already here because
this thing happened right so I'm gonna
just kind of keep doing this a little
bit I just did it on the third node we
should get in some 5 to 10 seconds I
think I said yeah there it is see that
event showing up just kind of keep doing
this a little bit so it's here not quite
here yet
oh that happened quick I must have
caught it in between oh there it is do
you guys catch that should do that again
no everyone's tired it's like the second
last talking last day cool this is like
pretty I mean it's pretty cool to me
it's pretty cool that like you know this
is state transfer happens a merge
happens the CR DD you know again if the
state transfer fails all you have to do
is do it again like just do it again
and then it's gonna happen so let me
actually play with this a little bit I'm
actually gonna kill the second node and
it's see it's dead you actually can't
get to it if I go over here to the first
node well it's still there
right the data from the second node is
actually still there and I promise you
oh sorry not that node let's do just to
prove to you that the cluster is gone
the second notes gone the second notes
gone we only have two nodes here but the
data from the second node is still there
so if I spin it back up it will rejoin
the cluster I'm going to increment on it
and
eventually eventually there it is you
see a fourth right so because I assign
that new unique identity you nee qid
I now have a fourth item here so the
nifty part here is that the amount of
state that you can potentially lose is
basically a function of how recently did
the last straight state transfer captian
happen right and so in cases where you
you desire a lot of correctness you can
tune that the amount of correctness
there based on how often you'd like to
do some sort of the state transfer
obviously there are actually a lot of
interesting some things that you can do
on this point to ensure that state
transfer happens locally as opposed to
remotely if you have multiple different
data centers and say transfer can happen
between you know nodes in the same data
center which is cheaper than going over
the network
those sorts of things are possible but
I'm not going to go through them here so
awesome we have achieved success
it's very eventual right it took us
about 10 seconds for that first sync to
happen but we did sync member listed
solve that problem for us there are
obviously ways that you can sync faster
one way to sink faster which actually
would be very stupid for our use case
that we're talking about right now is to
broadcast on every increment request I
say very stupid because we just said
that we want increments to be fast and
we want bandwidth to be the amount of
bandwidth used for this entire operation
to be low so if I broadcast on every
single increment request I'm blowing
through that entire requirement but let
me just go ahead and do that so that you
can see how this works so I had this
broadcast state function here which you
know appends a broadcast event into the
queue and then on the other side anyone
who receives the broadcast event gets
this notify message function invoked and
then they can do the same thing that we
did previously which is instantiate the
CR DT merge it in and then get the merge
value so quick demo I have this line of
code it's a go routine which broadcast
the state
let's hope this works again it's
compiling super quick promise this is
not a go talk all right cool
so I'm gonna hit increment several times
and you should see here several of these
receive broadcast remote states actually
if I spam this a ton of times you'll see
lots of those you see lots of those on a
few of the different nodes so again this
is just a somewhat clever way of
ensuring that the the cluster converges
sooner rather than later but again it's
not necessarily suggested mechanism cool
so what I would do next if I was
actually building this thing as a
service is I would build tests my source
code while up on the internet and
available for for all of you guys to
critique directly and make fun of me on
Twitter for doesn't have any tests so
because I said that it's okay so you
can't shame me although I will take
complete blame for being a bad developer
after tests for that thing I would
probably implement some former system
sometime when doing one counter that
just goes up forever it's probably not
that useful so you know I would also
implement more than one node per
datacenter again this is just my laptop
so it's a very small and cheap data
center but we would if we actually cared
about it being an AP system and being
available we would put lots of these
things in every local data center and
allow them to sync
I would also run and implement Jepson
testing for this or some other form of
correctness validation right so modeling
it with TLA plus or similar would
probably also implement a real RPC layer
right so member list elegant is pretty
useful but if I wanted finer performance
control or authentication
position control so not just any node
can join my cluster I would want to
implement my own RPC layer there and
then as just kind of part of the last
set of extremely important to do's I
would run it as a unit kernel within
docker running inside a VM in AWS and
then I would sprinkle DevOps magic dust
on it and then I would have achieved
peak microservice which is what we're
all here to do so yeah there's one
takeaway it's actually not that hard to
build clustered services and go again
member list is your friend CR DTS are
also your friend all this code is up on
github you can play around with it and
again that pic as you will ask questions
whatever and that's it for me so if you
guys have any questions
say it one more time okay
the question was how do you handle fault
tolerance in a clustered or discrete
architecture there are many many ways to
do that and there are I guess several
different implement definitions of fault
tolerance that I would you know be able
to take here but in this case I'm
piggybacking completely off of the fault
tolerance or rather failure handling
that member list has and because it is a
very weakly consistent system those are
the constraints that I have if I wanted
to try to enforce you know stronger
handles I could I could do other things
like maybe try to implement some sort of
a dynamo model where I actually have
more control over the amount of ways
that a data is a piece of data is
actually sent around the answers
question any any other questions about
NGO or member list or seer DTE or so
state I guess the question was how do
you reduce replication for the amount of
state that you're actually as the amount
of data so are you are you specifically
talking about the C R DT implementation
or the member list imitation like the
data that we care about replicating is
the counter but member list that
possible by sharing the cluster state so
which one are you asking about member
lucerne is the actual member list we we
happen to be piggybacking off of the
member list protocols to facilitate
transport of the state in the CR DT so
there are a lot of other ways that you
could do that right I think actually
when I originally wrote this talk I was
going to do it first and member list and
then build my own RPC layer that did
something that was more akin to a dynamo
where I could say you know do a quorum
right where it funnels out to quorum
waits and then does some sort of like a
two or three phase commit to ensure that
all the rights are made but I didn't
have time for that the the problem I
think with what you're saying here is
that
my assumption is that every node has the
state of every other node in the CR DT
in eventualities and that is correct
because we're using member oulous to do
that work so another way to do that
would actually be to build like some
sort of a ring or partitioning or
sharding how whatever where do you want
to use to structure the movement of the
data and the limitation of the data to
only relevant sets of nodes so one of
our systems at media mass actually you
know makes huge amount of this it
borrows heavily from the V node concept
that cassandra has where basically you
have pools of data that are classed
within like a mod ring and that that
data is effectively shard and pinned to
a specific node and the node sorry the
cluster sizes don't have to be the same
across all the data centers they're
effectively like a piece of data is
replicated to a particular node in a
cluster on the other data center none of
that's implemented here sorry cool
thanks guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>