<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Embrace the Past: How SW Evolution Lets You Understand Large Codebases • Adam Tornhill | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Embrace the Past: How SW Evolution Lets You Understand Large Codebases • Adam Tornhill - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Embrace the Past: How SW Evolution Lets You Understand Large Codebases • Adam Tornhill</b></h2><h5 class="post__date">2016-09-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0cY0vkFw7cY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's embrace the past and in this
session you will see how our past
history helps us measure aspects of
software development that we haven't
been able to measure before and we will
also see how that information helps us
prioritize the improvements we need to
do to our code you will see how it helps
us get insights into our designs based
on her way of actually worked with the
code and you will also see that we are
now able to reason about social aspects
of software development things like
organizational problems or team
profitable knack that shows up in our
code I was once given the advice that
when you give a presentation always
start with a motivating example because
if I do that you will know why you
listen you will know why you should care
you will know what this is important so
I went home and fuck that if a
motivating example is a good thing more
examples must be even better so today
you will get not only one but three
different examples and the first example
is a personal story this is about the
personal failure but the interesting
thing is I failed by actually improving
something so what happened here this was
a number of years ago I worked on a
fairly large system and my task at that
time was to prioritize the improvements
we could do to that system in order to
become more productive so the way I
approached it at that time was the type
used a bunch of different tools that
were capable of measuring a code
complexity so I took my tools of freedom
at my code base and out came a
prioritized list of the most problematic
modules and the interesting thing was
that those tools de pointed to one
component in our system the data access
layer so I went over to the people who I
knew had worked on that part and I asked
them how is that code actually and they
pretty much confirmed the findings my
tool did so they said that yeah it's a
mess to work with it's quite nasty code
and it's no fun at all so forth great we
have our candidate so what we did was we
took some of
best people and they spent two months
Rep riding that whole thing and after
two months we had a shiny new data
access layer it looked beautiful and all
tests passed now what you think happened
to our productivity what if in Captain
to our quality measuring the number of
bugs absolutely nothing there was no
difference and this was quite depressing
because it means we not only waste the
two mums improving something that didn't
matter it also means we missed an
opportunity to improve something that
could have made a difference another
related story this is about another time
were worked with a different client and
this client they had a fairly old code
base had a history that went 15 years
back in time and again they used the
tool that was capable of measuring code
complexity the interesting thing with
this tool is that it was also able to
quantify technical depth so we actually
get a number so they use that tool under
15 year old code base and the tool
reported that on this code base you have
4,000 years of technical depth Wow 4000
years just to put that into perspective
4,000 years ago that was when Moses
parted the Red Sea and I mean 4,000
years of technical depth it may well be
accurate but it isn't that helpful
because where do you start my final
example here before we try to look at
how we can solve this challenge yes it's
taken from Fred Brooks classic book the
mythical man month so how many of you
have read the myth myth game a month so
a third of your proximate lates one of
my favorite books and the mythical land
month is perhaps best known for what we
have to what we now call Brooks Law and
Brooks law says that adding more
manpower to late project makes it even
later and this is quite interesting
because this is something I've
experienced myself several times in the
most dramatic
setting what happened there was that I
joined a project and that project
originally was scheduled to take one
year to complete they had historic data
that showed them that this will take you
approximately one year now of course
they decided to well let's do this in
just three months so the question is how
do you take something you know takes one
year and compress it down to just three
months it's quite easy if you're a
manager so what they did was they hired
four times as many consultants and threw
them at our code base and yeah once
again that project proved a mythical man
month to be true because that project
didn't take three months to complete it
didn't took one year either it took more
than two years and one of the reasons
was because the developer said that code
was really really hard to understand and
I had inspected it killed myself and I
fucked that it's actually not that bad
the reason it was hard to understand
that it's what this was surprising to me
was because what actually happened was
that even if you wrote the piece of code
yourself today three days later it
looked completely different because five
other developers had been working on it
in the meantime so they didn't have a
technical problem they had an
organizational social problem of excess
communication coordination overhead and
when I put together these examples I
really asked myself why do we as an
industry keep repeating the same
mistakes why is it so hard to prioritize
the improvements you want to do why do
we keep violating Brooks Law all the
time and I think one of the main reasons
is because when you look at human
decision-making we tend to make
decisions around the things that are
easily accessible to us the things we
can see and in the case of a codebase we
tend to base our decisions upon this
upon the code but I will claim that
that's incomplete information and the
most important pieces of information are
absent and those two pieces of
information or time and social
information
and time is important because without a
time perspective we cannot see any
trends in that we don't know how that
code evolved and I think that it's
important because how do you know if a
design is good I think a design is good
if it supports the kind of changes we
want to do to the code without a time
perspective we cannot really evaluate
that we can only speculate and socially
information is important too because if
we don't have social information we
don't know if that piece of code is a
team productivity ball neck where
multiple people and different teams need
to coordinate their efforts all the time
so things we need to fix this and we can
fix that by embracing the past so how do
we get information on time and social
information the good news are you all
already have all the data you need we're
just not used to think about it that way
I'm talking about our version control
history I've came to view our version
control history as a behavioral log of a
we as developers have interacted with
our code base and you see an example
here from get and you see that if we
look at that log we see that we actually
have a timeline there because the
commits are ordered in time I am
interesting enough we also get social
information because we know precisely
which developer that it will change at
what point in time so let's embrace the
past and see how it helps us and my
first observation here today and this is
something I learned of the many many
years is that when we talk about code
all code is equal but some code is more
equal than others and to explain what i
mean about that we need to take a
different perspective on code we need to
take an evolution i review here's what
that looks like please have a look at
the following graphs they all show the
same thing on the x-axis you have each
file in your system and they are sorted
according to the change frequencies that
is how many commits have you done that
touched that particular file and that's
what you see on the y-axis that's the
number of commits
now the interesting thing here is that
you see that data is from free radically
different systems develop at different
organizations using completely different
technologies they have different sizes
and of course they have different life
time spans as well but still you see
they all exhibit exactly the same
pattern they show a power-law
distribution and this is something I've
seen in every single code base that I've
analyzed and I haven't analyzed few
hundred code bases but now so what this
means to you is that in your typical
code base most of your development
activity tends to be in a fairly small
part of the code most of your code is in
the long tail and that means it's rarely
if ever touched and this is important
because it gives us a tool to prioritize
so let's have a look at that so this is
one of the tools I used to prioritize
this is something called a hotspot
analysis and this analysis basically
takes the data from your previous slides
and shows where it's located in your
code so it identifies the areas of high
change frequency and I'm going to walk
you through this visualization the first
thing I want you to focus on you see
those large blue circles they're the
ones that are blinking right now each
one of those represents a folder in your
code base so this means this is a
hierarchical sensation that follows the
folder structure of your code it's also
interactive so you can zoom in to the
level of detail you're interested in and
if you do that you will see that each
file in your code base is represented as
a circle and you will see they have
different size that's because the size
dimension here represents the complexity
of the underlying code so this is
something you measure from the code and
you can basically use any complexity
metrics that you want because what they
all have in common is that they are
equally bad so I tend to use the
simplest possible possible thing again I
tend to use just a number of lines of
code but whatever you use let me tell
you about this that complex it there
compact clip complexity alone is never
ever a problem complexity is only a
problem when you need to deal with it so
that means we need to find out if we
really have to work in that code or not
and again this is something we can mine
from a version control system so we
calculate the amount of change to each
module based on a version control
history and the interesting thing here
is the overlap between those two
dimensions because now we're able to
identify complicated code that we also
had to work with often and that's what i
call hotspots in code so let's return to
the visualization I showed you earlier
this is a business data from a real
system this is the open source
application tomcat written in java so
you can identify a number of different
hot spots here right and we know that
those hot spots they're complicated code
that went to work with often so how do
you use that information well typically
the hot spots makeup great targets for
refactorings you can also choose to just
do an extra code inspection of the hot
spot to see if there's a real problem or
perhaps you just want to direct extra
testing efforts to the hot spot areas
and we're going to look at the number of
other ways that hot spots help us but
before I go there I want to make another
observation I want to claim that the
code is actually order destructive art
so how many of you have heard about the
concept of all the destructive art oh no
one so actually get to witness that at
tate modern in London a few weeks ago
this concept is something pioneered by
the artist Gustav Metzger and what you
see there that says the liquid crystal
light for reactions and they're quite
fascinating because when you look at
them they keep changing all the time so
the art looks different tomorrow than it
did today and the interesting thing in
it is in how it changes because what
happens here is that it necessary
destruction of an existing form to
create a new form and
I will claim that that's exactly how
code evolves and I think hot spots one
of the reasons i love them is the
because they make it clear that code is
never done successful code evolves and
that's a good thing the reason I sites
order destructive is the course changes
and new features tend to get
increasingly more difficult to implement
over time but it doesn't have to be that
way so let's have a deeper look at hot
spots and see how to actually help us so
what you see here are examples on hot
spot analysis of three different code
bases written in different technologies
and of different sizes and for example
you can see the mo new code base here in
the middle it's really hard to see the
details and that's not my intent eater I
just want to show you that even here on
the highest level of view you can spot
the number of hotspots now remember that
those are fairly large those code bases
right somo know is almost seven million
lines of code so i know you see doctor
on their your right hand side it's also
half a million lines of code and you see
a cluster of hot spots there right how
can we use those hot spots what do the
actual amine well let's look at some
numbers and again this is something I
find in virtually every code base that
analyze that there's a dress proportion
the hot spots tend to back up a really
small part of code yes most of your
development efforts is in that where a
code and that means that you can make a
huge impact by just improving a small
part of your code all code isn't equal
no you don't need to consider all those
4,000 years of technical depth most of
those years doesn't matter at all there
in the long tail so what we do with hot
spots is basically we don't defy the
parts of the code that you have to work
with most of the time and then we look
for an overlap with complexity and that
helps us take those million lines of
code and narrow them down to the parts
that really matter for your productivity
and quality and the reason I see quality
is because hot spots
tend to have a very strong correlation
to defects all right so remember that I
told you that code is all the
destructive art we know that the code
will change and when you know that some
new requirements they will invalidate
the design decisions that we have done
earlier we also know that we need to
react when that happens unfortunately
this is where many organizations fail
and I want to explain why and what we
can do about it but talking about
something completely different something
called the normalization of deviance so
the normalization of deviance it's a
theory coined by Diane Morgan and they
invoke am coin that theory when she
investigated a space shuttle Challenger
accident so I guess most of you remember
the space shuttle Challenger was
basically happened was that the space
shuttle was launched this was back in
1986 and 71 seconds of the launch the
whole thing dissing integrated in a
large fireball and what's so interesting
with the Diane von Gunther investigation
here is that she's not an engineer she's
not a rocket science scientist she's a
sociologist because she means that the
technical failure of Challenger was on
the symptom the real problem was
organizational and cultural so let's
look at that and see how that relates to
software first of all the technical
reason at this Challenger blew up is
something you can see here you see this
area here of gray smoke that's not a
good thing this is the solid rocket
booster of the space shuttle and that's
a huge booster so it's made up of
several different segments and it turned
out when it started to test that thing
in the early 70s they note that the
joints between those different segments
didn't behave as expected so what they
did at that time was they discussed
their problem and they found out that no
this is not desirable but it's an
acceptable risk so they moved on
some years later they start to fly the
space shuttles and then notice that the
ceiling in that joint they used on oring
so basically a piece of rubber band with
in that joint seal they note that that
ring got hit by hot gases and that was
really unfortunate because o ring was
never decided never designed to be hit
by hot gases so again that is just the
problem and passed it off as an
acceptable risk later flights showed
that that I overing actually eroded when
it was hit by gases so it actually put
them at risk for a leak again they said
its desire it's not desirable but let's
pass it off as an acceptable risk and
that's the normalization of deviance
each time you accept a risk you get a
new point of reference the deviation
becomes your new normal and the reason
I'm so fascinated by this has nothing to
do with spatial de la saw at all that's
because the normalization of deviance is
about people and we have the
normalization of deviance in software
development too so let me give you an
example let's say that you start to work
on your code base and you inherit a file
which has five thousand lines of code
how many of you would be happy with that
not so many looks like I'm the only one
here so five thousand lines of code yet
it's likely to be a mess but if you work
with that long enough that will become
your new normal and besides what
difference does a few extra lines of
code do so after a while you have six
thousand lines of code and then you have
seven thousand lines of code so what I
would suggest that we need is some way
of catching the normalization of
deviance in our code base here so
starting point this is something i call
complexity trends and complexity trends
work like this that you identify the hot
spots in your code base then you take
each of those hot spots and you go for a
reversion control history and you pull
out each historic version of that code
and you measure the complexity of that
one and you move on free library control
history
which allows you to plot the trend in
time like this now this is real data
from a real project this is from the
open source project mo know where we
examine a single hot spot here and we
see that that timeline shows the
complexity growth over 14 years and the
interesting thing here is that over
several years that complexity just kept
increasing slightly then we see that
back in 2015 someone made a refactoring
so you see this dip in the complexity
curve right and after that it went up
words quite rapidly so when you talk
about the normalization of deviance
that's one of the reasons that
whistleblowers in organizations are so
important and i find that complexity
trends are great whistleblowers when it
comes to normalization of deviance in
our code base all right one more thing
related to the time perspective and
those of you who have tended the nerves
talks yesterday will recognize this the
cross this is an ID i actually get from
ma'am participating in one of these
sessions the North talks about the
software half-life and dance ID is that
just as we have decayed for physical
quantities we have the same thing in
code and just so I get it right let me
quote then on this this is what what
then actually wrote then says that
reducing this self life means that any
code you look at is likely to be either
way we roll or very recent and he goes
on to say that we discover that
reasoning about code becomes harder when
there's a lot of code in the gray area
between the two extremes so one of the
reasons i like this concept is because
it ties in so nicely with our human
memory work a long-term memory right
because old code that's been stable for
a long time allows us to keep stable
mental models in our head of how that
code works it's much easier to reason
about it we can reason about it as a
whole which is basically a shank
something that our working memory just
loves and code that's very recent it's
so recent that we remember how it works
so I like this
and I figured when I heard about this
that yeah this is something again that
we can actually measure in our code
because when we embrace our past we can
actually calculate things like code H so
let's have a look at a few examples this
is what the code h map of our large code
base looks like so what you see here is
approximately 600,000 lines of scala and
java code unfortunately need to keep the
name of the code base anonymous but you
see there sir it's quite interesting
here because it's exactly the same
visualization that showed you the hot
spots its chose the hierarchical folder
structure of the code and you see that
each file again is a circle but this
time the color of that circle indicates
its age so the more dark blue the more
recent the code and you also see that
there's kind of some kind of breaking
symmetry here because the right hand
side of the code seems quite young and
the left hand side of the code is quite
old so what's the most common reason
that we managed to stabilize code the
most common reason is because the code
is dead dead as disco so if I were real
to you that the right hand side here is
called application code and the left
hand code left hand side is called test
code you will see that those large light
blue circles here that represents old
code that's actually obsolete test data
test data that's no longer used which is
the reason that I haven't been changed
in several years but we can do even more
interesting things with code base with
code age we can use it to get insights
into our designs so let me show you an
example from another code base let's
look at the development of the
programming language closure so this is
what closure looks like that's the H map
of closure so what I did now was the
devil was interested in all code that's
been stable for more than two years who
haven't made a single change let's see
how that looks here we go Oh
so we see there's actually a whole
package that's been stable for more than
two years where we haven't made a single
change now a closure as you may know is
a dynamic language on the JVM which
means it compiles to bytecode and this
package ASM is responsible for helping
out with that so if we look at that code
we see that we have a stuff like yeah
close reader a method writer and so on
so to me this seems to model a different
domain actually so for that perhaps that
package would be better off extracted
into a library so before I make a
recommendation like that I always look
at the code so let's do that together
this is what the heading of each of
those files look like it turns out it is
already a library it's just been copy
pasted into closure basically the reason
is probably because it wasn't a good
distribution available of the library
but the I think this kind of reinforces
my point that code the changes at
different rates tend to model different
aspects of the domain all right one more
example for you this is from the
development of the Python language and
python is interesting because its
history goes 25 years back in time which
means we find a lot of old code inside
Python and what I do in those cases is
that I look for packages where the
different files change at different
rates and you see one such example here
this is a package called cjk codex and
you see that some files are much darker
blue so they have been changed recently
and the other ones are quite old let's
look at what that those files actually
are turns out the stable ones have been
stable for more than 10 years not a
single change those are mappings doors
are basically encoding pages for
languages like Japanese and Chinese so
that's the reason we managed to
stabilize them if you look at the more
recent code we see things like a
multibyte codec that has a much higher
change frequency and that we have
touched over the past month
now I would argue that a multibyte codec
is a much more general thing it's much
more general than specific language
encodings so what else would suggest in
this case it's that you actually split
that package according to H because a
multibyte codec is a more general
concept that's potentially useful in
other places as well so to sum this up
and again this is an area that are still
exploring us they learn a lot here but
my finding so far is that code age is a
much underused driver of design and I've
found that there are some
recommendations that I try to explore
IAM deeper and so far I've found that
what if we organize our code by age what
if we turn stable packages into
libraries and what if we refactor code
that we failed to stabilize I'm pretty
sure that we follow those
recommendations wind up with a set of
advantages first of all it will help a
long term memory because we will be able
to build those stable mental models we
were able to turn a library into a chunk
for a memory that really fits her head
and since we end up with less code in
the code base itself since we're able to
stabilize a lot of it also get an easier
onboarding because we have less
cognitive load as new developers and I
also think that there Cody edge is
something that can help us to decide
where we need to write extra tests and
this is interesting because most
organizations where I've worked that
focuses a lot of test automation what
tends to happen is that after a while
the build takes like forever so what
then happens is that eventually some
tests get shut down and using code H
this is also not a desirable situation
but using code age you can actually use
real data to decide which parts of the
test suite that are safe to shut down
and which aren't all right let's move on
to the final section of this
presentation and now this is something
really important that I want to tell you
and this is everything to do with the
social side of software development
because i have found out that
organizational problems are often
mistaken as technical issues and again I
think the main reason for this is
because social information is invisible
in the code we cannot see it and still
it's vitally important so we need to fix
this so let's take a social view of a
code base here's one approach so this is
something i call fractal figures and you
see an example here from the closure
code base now how frak the figures works
is that you basically consider each file
as a box and each programmer gets
assigned a color and then more that
programmer has contributed to the code
the larger the area of the box so with
factor figures we get an excellent
communication tool let's add a color
legend and see how it works so let's say
we want to contribute to closure and we
want to make a contribution to the
module over here that module is called
evaluation and we see that all of that
code is written by the light blue
programmer oh that's due at l away so if
we have a questions you at can probably
guide us through this and you also see
that close you in general seems to be
ridden by the dark blue developer and
that's a guy named rich shaky so if we
have a question about closure in general
well ricci key probably knows a thing or
two about it but we can do even better
than this across instead of focusing
individuals let's use this to
investigate teamwork and this is
important but first let me ask you in
your day job how many of you develop
software in teams most of you like
almost everyone great also do that and
it's interesting because social
psychologists have been studying
teamwork for many decades and one thing
that they have found out and it is
consistently repeated in experiments is
that teams constantly underperform
so since all of us works in teams let's
invest the guy to understand why so we
can do something about this one of the
reasons teams underperform is something
called process loss a process loss
that's a concept on social psychology
and it's borrowed from mechanics so if
you think about the mechanical
construction a mechanic machine just
like it cannot operate at one hundred
percent efficiency all the time due to
things like friction and heat loss
neither can teams and you see the model
of process lots here you see that to
your left you have a number of
individuals and each one of them has a
potential productivity if you sum that
up you get the whole team's potential
for activity however that's never ever
what you get out of it you get much less
real productivity some of that potential
is just lost and that's kind of loss
depends on the tasks that you do when we
talk about software development the
typical reasons for process loss is
communication and coordination overhead
but also things like motivational losses
so perhaps you're assigned to I take
care of that the legacy service written
in vb6 and you're not too excited about
it you have a motivational loss with
influences the whole team so i think
that the trick with process loss is that
you can never eliminate it it's
inevitable it will always be there the
trick is to minimize it and the first
step towards that improvement is to
measure I understand how severe your
process loss is today so what it
suggests there is that we take that
knowledge mining that we did to generate
the fractals and group the individuals
into teams so in the following
visualizations you will see that each
color now represents a team and once we
do that we can start to measure a lot of
interesting stuff we can start to
measure how well aligned our
organization is with Conway's law so
Conway's law how many of you have heard
about it almost everyone right it has
received so much
over the past years that are always
almost didn't want to mention it at all
but it is an important observation and
it's important guiding principle and the
interesting thing with come with law
coleslaw basically says that the way
we're organized that will be reflected
in a kind of software structure that we
design and it's usually used to sell the
idea of micro services because with
microservices each team can own their
own service and you minimize the
coordination overhead between teams now
let me tell you that you don't need to
go full microservice in order to benefit
from Conway's law here's an example from
an architecture that built on a
principle or pattern called package by
feature so you see that each one of
those folders here they represent the
code for a complete feature and you see
that this organization is really really
well aligned with Conway's law because
each color represents a team right so
you see there's very little overlap
between people on different teams now
Conway's law is a great observation but
it's also oversimplification because
it's always a trade-off if you isolate
teams too much you run the risk of
running into process loss in terms of
increased conflicts between different
teams so you always need to balance that
but no matter what model issues are
strongly recommend that you measure and
see that the reality in your code
actually supports the way you work with
it otherwise you may end up something
like this so this is a different code
base a real word case study that again I
have to keep anonymous and it's
something I call the perils of feature
teams feature teams are quite popular
these days and again they can work
really really well if your architecture
supports that way of working if not you
run the risk of ending up here so what
you see here is basically 12 different
teams and if you look at that map it's
really really hard to find any patterns
sure does one or maybe two modules where
it looks like one team has done most of
the work but the most cases what you see
here
it's basically our collective chaos it's
communication breakdown because you
consistently need to coordinate the work
and actions between 12 different teams
in the same parts of the code and let me
tell you it's not going to end well and
organizations do realize this and one
way the typical address it is by
something that is known as the
gatekeeper pattern so the ID with the
gatekeeper pattern is basically that all
code that you're right has to be
reviewed and approved by designated
person and quite often that person is
someone called an architect and I think
that this kind of solves the symptoms
but not the real problem in addition you
get the number of potential drawbacks
and first of all this pattern it reminds
me of something it reminds me of
something called and dolls law so and US
law has nothing to do with teamwork it's
about the theoretical speed up in
parallel computing so what and US law
says is that the speed up of a in a
parallel computing word is limited by
the serial part of your program and I
think they are a gatekeeper architect
it's exactly like that it's like a
global lock up on your organization and
in addition it isn't really a
cross-functional team if you need to
depend upon someone else to approve your
code so I'm going to talk a little bit
about this at the end but first let's
look at the alternative that I've seen
work much better in real world this
alternative is to use the whole team as
a gatekeeper and the reason I like that
is because first of all it doesn't rely
on on individual everyone can make
changes everywhere but the team decides
what code that gets approved and not and
second while it do assigns a clear
ownership to a team it also helps
address an other issue with the
relational gatekeeper pattern and this
is a social issue this is something
psychology is called diffusion of
responsibility and the future of
responsibility relates to accidents and
emergencies in a real word so if someone
get forbids has an emergency it turns
out that the large
or any group of bystanders or witnesses
then less likely that an individual will
offer their help because they feel less
personal responsibility to do so and
just assume that someone else will
probably help and we have the same in
software and that means that in software
we must feel that our individual
contributions make a difference and I
think it's necessary for good code to
have a sense of personal responsibility
from everyone involved so there's really
no way around that now that we have
looked into the social aspects let's
hide them together with our hot spots so
this is what a typical I fine when I
analyze really old code bases code base
it's been around for 15 20 years I tend
to find that most of the code is quite
stable in terms of development activity
and then you have a number of really
strong central hotspots in the most
important parts of your code when I find
something like that to always look back
in time and I see that those hot spots
have been around for years which means
we have probably spent a lot of time and
effort on code that it's less than
optimal in quality so why hasn't anyone
improved that code do you know the
reason why match existing code isn't
improved the reason is because the
fractal figures look like this so that
means no you cannot just redesign the
code because at any given time you have
further different developers from
different teams that depend upon the
code looking it the way it does right
now on different branches and things and
this is quite in for unfortunate it
leaves us in a context that I call
immutable design now I'm a functional
programmer but trust me on this one in
this context there's nothing good at all
with the immutability and I find it
quite ironic that the reason we cannot
improve the code is because we're so
many people were killed in apparel and
the reason we have to be that many
people were killed in in parallel is
because we cannot improve the code and
the first step towards real improvement
is to measure and understand that this
is an organizational social problem not
a technical one
so please align your architecture and
your organization your code is going to
thank you for it so I'm almost done now
before I take time for questions I just
want to sum it up for you so we have
seen that once we embrace our past we
can get a lot of valuable information
that helped us get a new perspective on
your code base you've seen how hot spots
helps us prioritize the code that it's
an extra need of refactorings or any
other improvements we have seen how we
can use complexity twins as
whistleblowers to avoid the
normalization of deviance and you've
seen how we can reason and measure
software half-life using codec maps
finally we have seen how we can even get
insights into how well our architecture
fits the way we actually work with it by
studying the distribution of teams
inside our code and this is a huge topic
and this presentation has really just
scratched the surface so if you want to
read more I blog about this regularly at
Adam Thornhill com you have a number of
links here and of course it would want
to dive deeper into this you have the
whole book about it your code is crime
scene finally everything I'm working
right now and this is still work in
progress is that I want to provide these
tools that I used to do those
visualizations as a service so if you're
interested in that you can sign up at
code scene I oh it's free and if you're
interested in the tools that I use today
you can have a look at the ampere calm
and you will see what's available and
I'm happy to tell you more about it
later so before it take some questions I
just want to take this opportunity and
say thanks a lot for listening to me and
may the code be with you thanks
all right yeah I think most of the
questions also the questions I had was
work and I download these awesome tools
yeah okay now we know it yeah so that's
impaired calm and the reason I had to
write my own tools and pirates mi
startup so when I started to do this
stuff five years ago there were no tools
available to do the kind of analysis I
wanted to do so I brought my own tools
not necessarily because i wanted but out
of necessity so please have a look at
them and i also have on the code scene i
hope it will be available sometime this
summer and as i said where there is
going to be a free option on that one
besides working a download these tools
would you recommend marking actually I
think it's making making incremental
code changes for this bug fixes to find
buck hot spots oh yeah I'm not sure I
understand like a question completely
but it's about making small iterative
changes yeah I probably know when I read
it would you recommend making yeah this
changes yeah so there I think there are
actually two questions there we do my
interpretation of it and the question
are free to approach me off the words I
will be hanging around for a rest all
day but it's basically done a lot of
case studies on this and we're always
find is that the hot spot they always
make up this small portal code like two
to six percent maybe but they tend to be
responsible for lots of the bugs in the
code so if you look at your bugs they
also tend to cluster backs like each
other right so you will see that the hot
spots there with just a hot spots it
typically possible to identify the most
boggy areas in your code like 22 as high
as seventy percent of your defects will
be in the hot spots any more questions
no okay then
thank you very much thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>