<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2013 • From Big Data to Big Information • Dean Wampler | Coder Coacher - Coaching Coders</title><meta content="GOTO 2013 • From Big Data to Big Information • Dean Wampler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2013 • From Big Data to Big Information • Dean Wampler</b></h2><h5 class="post__date">2014-04-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2E-6DP-QCPw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Dean wampler and this is a
talk about going beyond the big data
hype and actually getting something
useful out of technologies like Hadoop
and no sequel and so forth it's based on
my experience as a consultant doing
Hadoop consulting mostly in some no
sequel the last few years and I've been
doing enterprise Java since the
Pleistocene era and what I've seen is a
lot of people are just really anxious to
jump on on this bandwagon they don't
want to be left out they don't want to
miss all the cool stuff they want to
mind gold from Twitter sentiment
analysis and stuff like that but a lot
of times they really don't know what
they're doing they don't really know
that there's caught up in a hype cycle
and what they really need to be doing is
figuring out what value they want to
extract from their information and
that's the goal of this talk and what
I'm gonna do is start by defining what
Big Data means to me so we put you put a
definition on this buzzword and then
talk about the current ecosystem sort of
focusing on Hadoop what's good about it
and what isn't and talk about
particularly use cases that you may need
to solve and what are the best tools and
approaches for those use cases and then
speculate a little bit about the future
so let me begin by defining Big Data you
may not be able to read this from the
back but there's two definitions here
Big Data is anything which is crash
Excel or small data's when fitting Ram
big data is when it's crashed because
it's not fit in RAM that's one
definition or two actually maybe a more
realistic definition and I'll let you do
the math on the right to make sure it's
correct
big data is data that's really so big
that our traditional tools and approach
is kind of break down either they get
too expensive or they simply cannot
scale to the sizes that we need to scale
to so we've invented alternative
approaches underlying all this stuff are
three trends one of which I think is
obvious and the other two maybe not so
obvious the obvious one is the data
sizes are growing we're capturing more
log data we're capturing more data about
users we're not throwing as much data
away as we used to and so in general we
just have more to man
this is not anything that none of you
have not heard before what may be new is
the idea that maybe we don't really want
to be so hard about schema anymore that
maybe we need to relax our notion of
schemas be more flexible and adaptable
in part because first of all the schema
that applies to our data today is going
to be evolving to something else pretty
quickly that's pretty common and the
other thing that happens is that people
want to bring together wildly different
kinds of data and extract information
extract value out of the the
intersection of those datasets so being
able to be very flexible about data
schemas turned out to be really
important and the last big trend is
perhaps the most interesting in terms of
technical innovation and that's the idea
that we're de-emphasizing the idea that
our programs are masters of the universe
that they understand everything about
our domain and instead we're writing
generic algorithms that are driven by
data instead and this is a picture of
the second-generation self-driving car
that Google built called Stanley which
is a great example of a data driven car
it's using some fundamental
probabilistic algorithms to figure out
where it is to detect pedestrians and it
turns out they've driven these things
several hundred thousand miles around
the United States and the only accident
these cars have ever had is when a grad
student was actually driving the car so
but they works very very well another
interesting example for me of this whole
debate about formal models versus just
probabilistic figuring out structure is
an argument that broke out between the
famous linguist Noam Chomsky and Peter
Norvig who's the director of research at
Google and one of the pioneers in
probabilistic methods for machine
learning and of course Noam Chomsky is
famous for building your very
sophisticated models of how language
works you know at a fundamental level
and then we layer on top of that English
and Dutch and Danish and you know
whatever where as what has actually
turned out to be most effective at
building like machine translation tools
sentiment analysis has been building a
probabilistic models that are relatively
stupid about what's actually happening
in terms of meaning but
probabilistically we infer meaning just
by throwing lots of data at the problem
and so this sort of public debate
erupted between these two guys I don't
think they actually met face to face per
se but you have competing blogpost sort
of thing in interviews where they
debated the merits of formal grammars
versus probabilistic reasoning about the
way things really work so there's the
three trends and that sort of drives
architectural decisions you know it used
to be we all built applications like
this where some event like a query or a
user login or whatever triggered some
information that are a query run against
a database we brought that in through an
object relational mapping tool we put
that in a perfect world model of Java
objects or whatever in in middleware we
massaged it and then we sent it off to a
browser or whatever but now we're moving
more towards this model which is you
know forget all the object stuff in the
middle or at least this idea that we
should have a complete representation of
the world in objects in the middle and
instead let's use the fact that our
databases are returning basically
collections of data technically the term
as a result said but it's really like a
giant a list array whatever hashmap why
don't we just embrace those collections
maintain minimal knowledge of what it
actually what the larger context is of
the particular scenario were
implementing and and stay lean and mean
as a result because we just know only
what we need to know to transform that
data in some way in the middle and then
send it off to the browser which can
also be functional if we're using
javascript in the right way and so forth
so it's sort of forced us to focus less
on domain-specific objects and more on
fundamental data structures you know
like the usual things that we all know
and love and work with those and stay
agnostic which goes back to this idea of
like the probabilistic data structures
versus formal grammar models where our
our code no longer knows a whole lot
about the domain that's working in it is
just driven by data and actually the no
sequel movement sort of fits into this
in the same way where we're
de-emphasizing the schema de-emphasizing
domain knowledge that's embedded in in
the logic of our schemas and instead
working with more fundamental data that
is relative you know the database system
is relatively agnostic about the data
what we're just working with the minimal
understanding we need and then you know
moving with as much scalability in as
much speed as we can in order to scale
but using logic only where we need to to
get the answers we need so an unintended
side effect of the object approach was
that it was very difficult in practical
terms to decouple things into small
pieces so almost all of the mature
Enterprise Java applications I've ever
seen were basically like this this big
wad of code in the middle through which
everything had to pass because that big
wad had all knowledge of the world but
when we decouple things that focus on
minimal abstractions and minimal
understanding and use a more functional
approach then it turns that it's a lot
easier to be decomposed things into
smaller focused processes and guess what
now we can you know make copies of them
so we can run them concurrently we can
run in parallel in other ways as well
and hence we get better scalability and
it's a lot easier to maintain stuff like
this that's well decoupled we can
upgrade process to if we need to and and
keep going so that gives a smaller
focused middleware and all of the
structure sort of fits back into the
three driving forces where it scales
better when we can scale horizontally
with smaller pieces rather than big wads
of code it we're de-emphasizing schema
in the sense that our objects are no
longer your models of the world but
instead are minimally of focused
manipulations of fundamental data
fundamental collections and so forth and
this better supports the idea that we're
gonna let the day to drive the emergent
behavior rather than try to build a
model of the world that we have to force
fit or data into and the tools that we
have available that's a
or this today one of them is actually
MapReduce and the distributed file
system that goes with it called HDFS for
Hadoop destroyed file system now
MapReduce is actually pioneered at
Google although of course the concept of
map and reduce is basically old
mathematics and Google also had a
distributed file system and then the the
Hadoop project was a cleanroom clone of
this but it does fit the model nicely
where we're going to have this generic
infrastructure that knows only how to
map things and how to reduce things and
we program it to do the actual
transformation so we need it scales
really well horizontally the distributed
file system in principle can can be
unbounded in size although right now
there's some practical limitations in
size and it turns out that companies
like Facebook have 600 petabytes now
resident in Hadoop clusters now one know
one cluster could hold that much data
but you can easily scale up to several
petabytes in one single Hadoop cluster
over several thousand machines and
there's other approaches besides to dupe
there's other kinds of middleware that
we might write some custom some
off-the-shelf and then there's often
people using those sequel databases as
the durable tier underneath and we'll
talk about scenarios where you might use
one versus the other and one interesting
example I think it's worth mentioning is
that suddenly we can actually use
JavaScript from top to bottom because of
middleware capabilities like nodejs and
JSON based document databases like Mongo
and of course we've had Jason in the
browser forever well forever in dog
years I guess anyway we can now have
Java scripts you all the way from top to
bottom if we really want to which is
actually pretty pretty compelling to be
able to use one tool for the whole thing
although certainly there some people
hate node so may I'm not necessarily
saying node is the right thing to use
but nevertheless I think you get the
point well let me take just a moment to
describe what MapReduce is in case you
don't know what it is you've probably
all heard the term by now but if you
don't actually know what it means it's
worth just a moment to talk about it
because it actually has some great
strengths but also great weaknesses
basically a MapReduce job is literally a
matte phase and a reduced phase and then
some magic that happens in the middle in
the map phase you actually read in the
files off your distributed file system
and it turns out what Hadoop likes to do
is store files in very large blocks
typically a multiple of 128 megabytes
whereas the underlined file systems that
we're all used to like whatever is
underneath the Hadoop distributed file
system the file system on my computer
you know typically a block size is like
five kilobytes or something but in
Hadoop they like to use hundreds of
kilobyte block sizes and the reason is
it's a very practical reason going back
to physics or least hard drives you know
we have these hard drives that are
spinning rust iron oxide basically and
they this little head that's traveling
over the top and that head movement is
by far the slowest part of the process
it's you know like 10 milliseconds or
something to move ahead across the hard
drive what we want to do is Park the
heart that head and then just scan all
the data that's spinning underneath it
and and that's why you want to have big
contiguous blocks of memory and not
stuff broken up like you wouldn't have
on a normal file system so Hadoop from
the very beginning is very much oriented
towards I have a lot of data I want to
scan all of it at once or maybe a big
chunk of it and so I want it to be able
to read it contiguously off off the hard
drive I think the rule of thumb is that
you can drain like a terabyte hard drive
in 10 minutes it takes that long to read
a terabyte off a hard drive these days
as fast as they are it's a huge
bottleneck so anyway the way Hadoop
works and this may be different for like
Google MapReduce but you'll spawn a JVM
tasks for each block on the filesystem
and that initial task will do some sort
of mapping over that data and to be
specific let me just describe what you
would do with something called word
count word count is like the hello world
of a Hadoop space it's the first program
everybody writes because it's a simple
algorithm and so you can focus on the
nasty API that we're gonna look at in a
few minutes but basically in word count
I want to read you know a corpus of
documents let's say Wikipedia and I want
to find all the words
that corpus and then I want to count the
occurrence of each word so you know I'm
gonna see the word Shakespeare and the
word Denmark and the word Hadoop and so
forth and so I want to find that across
all of these documents and I've got a
bunch of map tests and parallel reading
chunks of Wikipedia and what they're
gonna spit out in the middle towards
this thing that's called sort shuffle
here is some key value pairs and those
in this case this algorithm the key
value pairs will be each word it finds
em in accounts of those words you know
and the data that it processed and as
these key value pairs are coming out
they'll be sorted in the same process
not globally but in the same process and
then they're shuffled which means that I
want to make sure that all the
occurrences of Denmark show up and let's
say the second reduced task which is
another jvm process running on my
cluster somewhere maybe all the
Shakespeare key value pairs will show up
on the first one and so forth and you
can see where this is going the reducer
isn't going to get all of the
Shakespeare counts and all that and
another one's going to get all the
Denmark counts and that's going to do a
final total count and output the global
count for each of those words that is
really all there is to a MapReduce job
it's these three steps the one in the
middle that Hadoop does for you the sort
shuffle and then the - of the map and
the reduced task that you write yourself
now it turns out if you want to do
something like a sequel join or a group
buy that's where things get really hairy
because even though this is very generic
and flexible and in some respects it's
also very hard to map a lot of
high-level concepts to these low-level
api and most of the time in more
complicated scenarios like a group by
followed by a sort on that kind of
sequel statement you might write you'll
actually have to sequence a bunch of
these jobs and that leads to one of the
important limitations of Hadoop after
each of these MapReduce tasks it's going
to write everything back to disk and if
I've got a bunch of job sequence
together that I'm going to read all that
stuff back into memory so there's a bit
of inefficiency here but it's a sort of
inefficiency that's been tolerated
because if I need to process a hundred
terabytes of data then I'd rather throw
a thousand machines at it or whatever
and and even though it's each process is
inefficient at least in the
aggregate I get the results as fast as I
want so that's sort of the gist of how
MapReduce works this is an example of
something a MapReduce job without a
reducer I'll let you figure out if that
makes any sense well it turns out right
now Hadoop is really the best generic
tool we have for this kind of
general-purpose arbitrary massive data
set manipulation you know it's free
although free as in not quite as severe
because you do have to do a lot of work
to make it's a to knit and all that
stuff but it's it's the sort of thing a
lot of people can adopt fairly and
expensively right now if you look at the
cost for terabyte Hadoop is dramatically
cheaper than almost any other option we
have except some of those sequel
databases but as I described it really
wants to do everything in batch mode it
really wants to take that data that
you've been collecting and do some big
analysis all at once it doesn't it all
want to work with stuff incrementally
it's you can't write incrementally to
the file system conveniently and the way
that it scales up and the overhead of
trying to be massively concurrent
doesn't let you do incremental event
processing very well and we'll come back
to that it's a problem we need to solve
so it's not so great for stream streams
of events it's also no good for
transactions because once again it
doesn't want to work with individual
records for some definition of record it
just wants to work with all the data at
once so transactions is another problem
you might need to solve for big data and
that API that I'd walked you through
sort of conceptually is somewhat
coarse-grained it you would like
something that's smaller and more
composable so that you can combine
things together to get the kind of
behaviors you want
whereas trying to force fit everything
into you know one map in one reduce
phase or a sequence of those it's
actually difficult it's it's one of
those things that becomes a special
skill that certain developers have and
the rest of us are doing everything we
can to avoid having to acquire that
skill because we don't want to spend the
time on it so one of the first
alternatives I want to mention today is
this tool called spark which started as
a research project at Berkeley
University
California and it's very similar in the
sense that it's designed to be highly
distributed to let you do the sort of
general-purpose computation that Hadoop
is good at but it has a lot of
advantages over Hadoop at least now it
hasn't been battle tested at hundreds of
petabytes of data but at least by design
it has a lot of advantages for example
it caches data in memory between steps
and so sometimes it's actually easy to
get jobs that run 32 even a hundred
times faster in SPARC versus MapReduce
but it is something that needs more
battle testing to be as robust as Hadoop
is in terms of being proven and and
known reliable now another problem with
Hadoop that's really specific to Hadoop
is that the low level Java API is very
verbose and difficult to use so for
example here's word count in that API
and I know that nobody can read this and
that's sort of by design the actual
interesting logic that I describe
verbally a minute ago is just in a
relatively few lines of code and
actually I'm not even showing you the
whole program I haven't shown you the
main routine which is another 20 or 30
lines of setup so this is this is a lot
of work excuse me besides the fact that
it's kind of verbose there's a lot of
setup to do and then if I want to do a
non-trivial algorithm I really have to
develop an expertise of how to map that
high-level concept to this low-level API
in fact I think it looks a lot like EJ
B's yeah when I first started reading
this code I thought me and we're well
 the 90s is a little early it's
really like the early 2000s but anyway
it really feels like we're back to the
EJB days and you know EJ B so everybody
likes to complain about him now because
they were kind of a failed experiment
but they did get people thinking about
enterprise Java they solve some problems
just not especially well the problem
with EJ B's were they it was a
inefficient protocol it was very
invasive at the API level in your
application and it took some rethinking
by people like the Spring Framework and
rod Johnson to figure out the right way
to do enterprise Java and I think the
same thing is going to happen with a
dupe so in fact if you use a particular
Scala based tool for
this job then you get something that
looks a whole lot smaller in effect
without going into a lot of details
about the code it's pretty easy to
understand what's going on here once you
understand a few little basics of the
scalding API the scalding is a project
the Twitter wrote it is actually a
Hadoop API it's it's actually two levels
above the low-level API I described but
you get back to something that's much
more what we really want which is to
express our algorithms using the
terminology that is appropriate for the
level that we're working so whatever
people are learning Hadoop when I'm
mentoring people on Hadoop I tell them
you know adopt scalding or if you're a
closure guy adopt Casca log things like
that but avoid the low-level api and
just for convenience park gives you
something very similar this is the same
api and spark and it gives the same kind
of concise very descriptive and
intuitive kind of explanation once you
understand things like flat mapping and
mapping and reduction then it's really
easy to use these api's which is the way
it should be
so that's one problem we need to solve
in Big Data is expressive ap is that let
us express our intent as concisely as
possible and these are examples that are
taking us in that direction
ok but let's look at a few specific
usage scenarios problems that we need to
solve where we think we have a big data
problem and what that means for us at
first I want to contrast when you would
use to do versus the database either
sequel or no sequel for that matter
well Hadoop is great when you really do
have a very flexible need for
computation you know machine learning
algorithms don't really fit a relational
model query very well
but they're easy relatively speaking to
implement in a tool like a dupe Hadoop
is great if you really do have massive
data sets that you need to do scan scans
of to do sort of global analytics and
it's ok or great if you do all this in
batch mode in other words it's not an
online system it has to be fast because
you've got a user sitting at a browser
it's something where you run it
overnight to analyze your petabytes of
log data or whatever whereas on the
other hand if you have a problem that
does fit a particular data model whether
it's relay
lower document-oriented or key value
store then databases are fantastic for
this and in particular if you need
transactional behavior of some kind if
you need record level access as opposed
to full table scans and even I have a
vendor even here because one thing that
people use these databases for is to
solve the event problem where I treat
each interaction with a database as an
event so it's great for these sorts of
things and in fact most large
organizations need both kinds of feature
sets and so they often combine the two
and one of the interesting combinations
is using this database called HBase
which actually sits on top of the Hadoop
distributed file system when you need
that kind of record level access and
fast updates and deletes and so forth
but you want to do global analytics over
that data each night or whatever then
HBase plus a dupe is a good combination
well the other way that we're seeing
Hadoop in particular used is as a low
cost data warehouse option so I worked
with a client once who could only keep
six months of the data that they were
accumulating and they accumulated a lot
of data the only way they were gonna get
beyond that six month window was to
actually drop another million dollars on
a unnamed data warehouse appliance and
they were they did not want to spend a
million dollars when they could use
something that was 1/10 the cost and
what we did for this customer actually
is migrate them to Hadoop and now they
have like several years of data in a
cluster that's I think a hundred and
sixty nodes or something like that
well the advantages though a traditional
data warehouse your Terra data's and so
forth is that they are mature
technologies they give you a very nice
environment they tend to be fast they do
scale beyond traditional analytical
databases or online databases rather and
they give you a rich sequel and query
semantics which is great and so if your
data set isn't petabytes size then maybe
you just pay the money for one of these
things because they they're just mature
and they work well but they are
extremely expensive they can be up to a
hundred times more expensive at a
terabyte cost level compared to Hadoop
and they do have this limit that when
you
- there's really big data sets Isis they
don't scale so well well one thing you
might consider doing is what if we used
a no sequel database as a replacement
for a data warehouse and this will work
in some scenarios but the fact is that
when you think about the environment
you're dropping these things into you
really need to support sequel a lot of
the enterprises I go into you'll have
like a few it's sort of like an inverted
pyramid you'll have like a few
administrators at the bottom you know a
larger group of developers and then
you'll have this massive team of data
analysts that know how to write sequel
queries or know how to work with SAS or
R or one of those tools and so unless
you have something that gives them a
sequel abstraction you're probably dead
in the water now there are some no
sequel databases quote-unquote now that
are offering sequel query capabilities
like Cassandra for example but for the
most part people are gonna need a sequel
answer for this problem and it turns out
even though Hadoop is designed to be
just not working Ellison all right let's
try this
even though Hadoop is designed to be
this very generic data platform it's
actually quite good for doing sequel
kind of stuff as well see in a minute
and once again we're talking about data
warehouse problems not online
transactions where I've you know I've
got this data set and I need to do
offline analytics on it so I don't need
the I mean put another way the batch
mode nature of Hadoop is okay I can
tolerate that because I'm doing offline
analytics so even though we have a lot
of benefits of traditional data
warehouses which we just discussed
Hadoop also has an interesting trade-off
to offer now it is less mature and it's
less sophisticated as a sequel
environment which is kind of shocking to
people unless you warn them about this
but the sequel story is getting better
as we'll discuss in a second so it
actually is a viable option for data
warehousing and the cost and the
scalability cannot be beaten so and
that's why one of the most common
scenarios for using Hadoop now is as a
data warehouse replacement so let's talk
for a minute then about sequel on Hadoop
and what that means it's not really a
contradiction in terms
well it turns out you Facebook is one of
the FIR
companies to really scale Hadoop in a
big way Yahoo maybe being the other big
one and Facebook had the problem I just
described where they had a lot of data
analysts they were collecting all this
data about their users maybe more than
we realized or whatever and they wanted
to these data analysts one of the EU's
sequel to query it so Facebook actually
wrote a hive query tool on top of Hadoop
called sorry a sequel query tool called
hive and it's it doesn't give you
transactions there's no crud operations
create read update and delete it really
is just scan this table which in this
case is just files on HDFS and use query
abstractions that are then turned into
MapReduce jobs so in a sense it's a DSL
a domain-specific language for Hadoop
but it's been extremely successful as a
result in fact it's so important one of
the books I wrote was on hive even
though I'm not really a sequel person I
realized just from talking to customers
that it was essential to have good
documentation on this tool so I helped
write a book on it and there's some new
options emerging there's a port of hive
to spark called shark there's a new
version of hive sort of hive it's called
Impala where they're replacing the slow
MapReduce back-end which I'm with a much
faster query engine and then there's
even now a much more ANSI compliant
sequel version of a tool called lingual
that's based on an API called cascading
so it's sort of like a thousand flowers
are blooming in the sequel space on
Hadoop and it's int it's in fact you can
do things like even write word count in
hive this so this is actually the entire
program for doing word count on hive
it's just about as concise as that
scalding example and it demonstrates
that when you have a sequel or a rather
a query problem it's really hard to beat
the concision of sequel and that's why
people really want to be able to use it
well just quickly I want to spend a lot
of time on these but hive was this the
pioneer of this movement it was invented
at Facebook to solve their problems but
it does run MapReduce so no queries
going to come back you know
instantaneously over a small said it's
going to take seconds two minutes to get
anything back shark actually gives you
much better perform
moments because spark is much faster
than MapReduce in most cases but it's
not as proven in terms of you know
deployments and and battle hardening and
then there's this improved query engine
underneath the hive sequel language
that's going to make hive somewhat
obsolete
but it's going to take a while for this
Impala tool to catch up with all the
hives features so one of the things
that's happening though is that people
are starting to recognize that MapReduce
while it's gotten us a long ways and
it's been a powerful enabler it really
does have a lot of disadvantages a lot
of limitations so wherever we can
replace it with better compute engines
we should do it and this is a good
example of a query specific compute
engine that's being used in it by Impala
instead of MapReduce and I think we'll
see this as a trend where MapReduce will
slowly be you segment into a smaller and
smaller section of the compute
environment if you will and will have
alternative compute engines running and
then finally this alternative if you
really want more ANSI compliant sequel
this tool called lingual gives you that
more than hive DOS well another scenario
that people need to solve is search so
if you think about what you're doing
when you search Google they've actually
done a index over the interwebs so that
when you do a query for Denmark or
Shakespeare or whatever you get the most
relevant pages back and that's the
general problem of enabling search and
it could be search of I want to index
the wiki and our corporate environment
I want to index customer records that
have like transcripts of phone calls
things like that this is an example of a
specific solution being added to solve a
problem rather than going with a general
purpose framework like MapReduce and the
two best known examples is leucine with
solar or elasticsearch elasticsearch I
think has a talk this afternoon but
these are great examples of a much more
domain-specific solution that's tailored
to a particular problem and it solves it
in a much better way than you could
solve it just using generic hadoop and
so forth I mentioned that Hadoop is a
batch mode system but there's a lot of
problems that are moral
like a real-time event processing so how
do we solve those if we can't do it with
a dupe because it's not so great at
solving these problems then what are we
going to use well there are several
alternatives and one of them that I'll
mention is called storm this is very
much it's it's sort of conceptually like
a dupe in the sense that is designed to
provide distributed processing of events
but it is very much focused on event
processing as opposed to batch mode
processing it tries to do this with
distributed reliable services so that
you can trust this thing in a high
volume high scale environment
conceptually a spark deployment looks
like this where they treat their data
sources as spouts and you implement
spouts for file systems for message
queues whatever and then the processing
is done by bolts where the messages are
channeled to these things they make
decisions about the data they transform
it and so forth and then you know send
it off to wherever it may land could be
landing in HDFS it could be landing and
some other of message queue or whatever
so to compare these two both of them are
cheap both of them are highly scalable
Hadoop has much better commercial
support in battle testing and real
deployments than the storm does at this
point but that's going to change storm
is just newer and less mature and
doesn't have a it's actually commercial
support is starting to emerge now in
fact the data stacks I think is one
company that's offering it now
but again Hadoop is great for these
batch mode problems storm is great for
event stream processing well if you have
individual events coming in you know
that's sort of what an online database
transactional database will do for you
so maybe you could throw a database and
and let that come to your rescue now I
have to talk about this picture for a
second so this is literally outside in
my condo window in Chicago I live on the
34th floor in this helicopters below eye
level so why is a helicopter flying
between the buildings well it turns out
they use these heavy lift helicopters to
lift like new air conditioning and
heating units off the street and put
them on top of buildings when they're
upgrading or replacing old units so some
every now and then on a Sunday morning
you'll hear this Wack Wack Wack Wack and
you wake up and look out the window
there's a hell
Caprica buzzing by so anyway so what
about a database well it turns out that
you know obviously sequel databases are
great at transactional create read
update and delete operations so a lot of
times that's what people use but in this
sort of big data world it's become
popular to combine and no sequel
database like Cassandra for this kind of
processing to store stuff very quickly
or if you really are a Hadoop centric
shop where you want to be able to run
MapReduce jobs a lot of people just use
HBase because HBase uses HDFS the file
system is this durable storage so you
can be doing transactional stuff with
HBase and then run your MapReduce jobs
that are analyzing the same data so
that's a pretty common combination
scenario okay so one other use case that
is kind of driving the excitement about
Big Data is machine learning so how many
times have you heard someone say boy I
wish I could mines Twitter for sentiment
to find out you know what people want to
buy or what they think of my company
well that's exactly what we're talking
about is doing things like you know
recommendation engines like the famous
Netflix way that it recommends movies to
you based on what you've seen already
Amazon does this for products spam
filters are probably the most valuable
machine learning tool that all of us
rely on every day imagine if you had to
deal with spam as much as we used to and
so forth down the line a whole bunch of
machine learning things that you can do
well you can implement these algorithms
in MapReduce but again it's not such a
great model for it one reason actually
in this particular case is a lot of
these machine learning algorithms are
very iterative especially when you're
training the system for example there's
a month listen approach called
stochastic gradient descent or where
you're you have an equation that you
can't solve analytically and you just
have to guess the solution and iterate
towards the actual solution that's not
so great if you have to do this like in
a MapReduce job and a MapReduce job and
so forth people have figured out
workarounds for these problems but
MapReduce is not the the best tool and
especially programming these algorithms
is difficult well it turns out there's
some interesting alternatives that have
emerged and one of them is a tool called
pattern
where you build predictive models in a
tool like SAS or are one of these tools
that a lot of machine learning experts
like to use you export them in a markup
language and then you can just run them
without modification on one of these
platforms in this case cascading is the
platform and underneath that is
MapReduce so there's a lot of ways
people are solving this problem of
implementing machine learning algorithms
that in a non-trivial platform like a
dupe and it turns out spark which we
mentioned already is actually originally
designed for MapReduce so it's actually
an easier tool to use at the API level
if you're writing MapReduce or sorry
machine learning algorithms because that
was one of the original design goals of
SPARC and just finally for completeness
how this often works in practices you
might use the doob to train your models
because you're training it over a very
large data set and then store the model
data you know the coefficients of the
equations or whatever in some faster
storage medium like no sequels so that
it in real time when you're making
recommendations you're not asking Hadoop
for the answer you're asking a database
for the pre computed answers the last
thing I want to talk about is a common
data scenario which is like analyzing
social network data so Google invented
MapReduce in part to implement
algorithms like their famous PageRank
where they built basically if you think
about the web it's just a giant graph
right of links from one page to the next
and other great examples of course are
your followers and those people
following you on Twitter your friends on
Facebook etc well it turns out these are
giant graphs you want to use graph
algorithms to analyze them but once
again MapReduce is not a terribly great
fit because in the naive implementation
you'd started a node you'd run a
MapReduce job to do the processing to it
to its neighbors whatever messages and
then State updates required then the
next messages would propagate or the
traversals whatever you're doing and
note at a time or at rather an edge at a
time and if you were doing this one
MapReduce job at a time to take forever
so people have gained the system and
amazing ways to actually get this to
work and MapReduce even though it's a
horrible compute model for graphs so
what people are trying to do and I say
trying because this is actually
cutting-edge kind of research is how do
we actually implement graph algorithms
in a distributed way so that the the
graphs themselves and the traversal
algorithms are first-class citizens
well Google worked on a system called
prego prego that's the name of the river
that runs through I think of königsberg
Germany and it the invention of graph
theory was done by Leonard Euler a
mathematician who noticed there were
seven bridges crossing this river and
could you actually cross them without
going in repeating a crossing and did
without going into a lot of detail it
turns out you couldn't but it was the
first example of doing an analysis of
graphs and that's why they named this
thing this but essentially they were
they were trying to treat graphs as a
first-class citizen and then do
traverses over the graph and there's
some other as usual that's the history
of the last ten years has been Google
does something writes a paper a few
years later and then people try to
imitate it and there's several imitators
giraffe and Hama and then there's a very
interesting alternative database called
titan' which is probably the most
sophisticated of the three for doing
distributed graphs but the problem is
nobody really knows how to do this well
yet so even Google is still using
MapReduce for a lot of graph algorithms
and a lot of the lore that's built up
about how to program MapReduce is
focused on how do you do inter iterative
algorithms like graft reversals at the
MapReduce level so this is a not
entirely solved problem even though we'd
love to be able to do this we do have
some mature graph databases like neo4j
which is really a single machine
database so it's not the sort of thing
you'd throw at a massively scaled up
graph but it is the more mature graph
system if you want to use it so anyway
that's an unsolved problem is how do we
do distributed graphs we're sort of
hacking our way through it right now
okay so we just recap where we are
so Hadoop is fine for batch mode
analytics if you have a you know I need
to scan my tables going quote for some
definition of table and do some
analytics then it's great for that but
it's got a lot of performance problems
the API is kind of nasty it's really the
enterprise javabeans over time so it's I
see a somewhat transitional
but it certainly solved some important
problems for us and their sequel
obviously scales well it fits a lot of
the problems that we have it there are
times when we need sequel but a lot of
times we don't and we'll get better
scalability I mean we all know this
story from the last decade or so we're
also seeing a lot of purpose-built tools
like elastic search and solar that are
solving a focused problem but and doing
it very effectively rather than relying
on a generic solution like a dupe and
then there's this emerging vision of how
to combine batch mode processing with
event stream processing and really the
definitive book on this right now is the
inventor of storm is Nathan Mars and
he's right he's writing a very good book
on this sort of vision of how to combine
real-time event processing with batch
mode and a lot of us came out of the
time you was at Twitter and their
experiences so for example he would
recommend using Hadoop for the batch
mode stuff real-time updates with storm
so that you can you have current data at
your disposal you don't have to wait for
that nightly run over the hadoo process
and then whatever persistence tier you
need and so forth it obviously can't
summarize what he's doing in just a few
minutes I do want to emphasize though
something that people are I think not as
aware of as they should be and that is
sequel is roaring back even though we
went through the you know the sort of
the romance with no sequel and it's
obviously got its place and we like to
do because it's general-purpose people
really need sequel it's what they know
it's what they want to use so if you
sort of ignore sequel with whatever
solution you're building you're probably
going to fail long term if unless you
address this problem and in my personal
view is that Hadoop would not be nearly
as popular something that everybody
wants to do if it was not for the fact
that we have a crude but effective
sequel tool called hive and even a lot
of the so-called no sequel databases
have query engines Cassandra's is sequel
dialect
Mongo's is entirely different but it
nonetheless gives you similar kind of
relational semantics over documents in
this case and now there's new generation
databases that are
sequal again and they're applying the
lessons of no sequel and they've been
called new sequel every one everything
has to have some sort of buzzword right
but they're trying to bring the
performance of no sequel back to sequel
a really famous example it's worth
reading the papers about this is what
Google is done with span or an f1 which
are basically globally synchronized and
when I say globally I'm not kidding they
actually use atomic clocks and GPS units
and all kinds of crazy stuff to
synchronize the time clocks on these
databases nuodb volt DB or a couple of
others and the last very last thing I
want to talk about is this sort of way
in the distance trend that I think is
very interesting and could actually be
transformative over time and that is
that a lot of the stuff that's
underlying machine learning that's
underlying this debate between Noam
Chomsky and Peter Norvig is using
probabilistic data structures to
represent data and make predictions to
infer what it's going to do and so forth
that stuff's hard to program and it
takes a fair degree of expertise to even
understand what's going on so there's
actually a movement starting to build
languages that embed probabilistic
reasoning within them so it's a lot
easier for mortals people like the rest
of us to write these kinds of programs
that use Bayesian networks and Markov
chains and other buzzwords so that's
something to keep in mind and that's it
so I believe we have questions probably
on the little app and I'll take those
now yeah so can Hadoop be used on data
that's hard to decompose like a
aggregating interpolating over time
series data you know the general answer
is if you're willing to work hard enough
you can pretty much do anything the time
series problem is an interesting one I
didn't really talked about it but that's
actually a good example of
special-purpose approaches there's some
time series databases that people are
building on top of Hadoop mostly through
actually HBase and Cassandra and so in
general and I didn't say this either but
hey one of the things that Hadoop is
really great at is you can just pretty
much throw any kind of data you want
into the file system and then figure out
a way to analyze it whether it's
relational whether it's a key value or
rather comma delimited data or whatever
and so I think in the case of
time series data people are solving that
problem either with a special-purpose
database or figuring out an ad hoc way
to represent the data and I'll mention
one database you should take a look at
that I think is going to also be
disruptive and that's des Tomic which is
the one that Chris rather rich Hickey is
working on the guy who created closure
the thing about the atomic is that it
remembers everything that ever happened
it treats the databases of value so when
you like write an update to a record
you're actually creating a new virtual
copy of the database of course they
don't really copy the whole thing they
share data but for time series data I
could see that as being really
revolutionary for solving that problem
and that was the only one all right well
I'll be around the rest of the day so
feel free to come asking questions
thanks a lot</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>