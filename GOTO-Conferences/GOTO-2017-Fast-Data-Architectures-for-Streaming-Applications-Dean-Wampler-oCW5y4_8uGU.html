<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Fast Data Architectures for Streaming Applications • Dean Wampler | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Fast Data Architectures for Streaming Applications • Dean Wampler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Fast Data Architectures for Streaming Applications • Dean Wampler</b></h2><h5 class="post__date">2017-08-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oCW5y4_8uGU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay why don't we start thanks for
coming
my name is Dean Wampler on the track
host and the first speaker and I have to
say I'm really excited about this track
you know with the exception of the first
speaker I think we put together a good
lineup a couple of them are here in the
audience Glen and Jesse that's the only
ones I notice but really I think this
will be my talk will be like an
introduction to the kind of a streaming
landscape as it's kind of emerging in
sort of the big data world I mean really
we've always been writing streaming apps
even restful apps they're kind of if you
think about it they're kind of you know
data keeps coming and never stops and
it's sort of the same at its scale of
course and then subsequent talks today
we'll dive into some of the particulars
of beam and Kafka and spark and and so
forth so I should be pretty exciting I
hope I hope you'll enjoy it all the
pictures in this thing I took and
they're basically pictures of water
because we're talking about streaming so
and you can get this talk and it
obviously be on the conference website
but I keep all my talks and my vanity
website polyglot programming calm okay
so as always please rate the sessions
when you're done because I'm the speaker
I can't really look at the app that
easily did four questions so please just
ask me questions at the end if you don't
mind and once again please rate the
session most of what I'm going to talk
about is go into more depth in this
little Riley report it's like a 30 40
page report so it's not too painful to
digest the only catch is you have to
give us your email address at like bent
hopefully you won't mind too much but it
goes into more detail but what I'm going
to describe today so let's talk with
some start with some context about
streaming this is the area in your
Sacramento actually some of the cool
wind farms there and this happens to be
in the Sierras anyway
the Hadoop classic architecture is kind
of a good starting point so how many of
you have worked with the doop a lot of
you probably okay I'm not gonna spend a
lot of time on this but you know if you
think about its architecture it's
essentially three big pieces
it's a distributed file system you know
that's HDFS it's something that to do
computation over the files you know was
a MapReduce now it's more likely spark
than anything else and and then there's
a simple structure called the yarn that
knows how to manage all these resources
and allocate you know tasks across the
cluster for your job and all that kind
of stuff and then there's obviously
support tools that's kind of what's on
the bottom left corner to get data in
and out and so forth and etc
yeah but the general characteristics of
Hadoop you know at least in classical
terms there it's obviously been evolving
a lot but it was you know originally
just a bachelor oriented system let's
just get this data in here and then do
massive analysis over it I'm kind of
glossing over the role of HBase which is
more of a traditional database cheap
massive storage you know he's kind of an
artifact of the unprecedented sized data
sets we acquired in the internet era
back in the Pleistocene but it did
support things like multi-user jobs
diversity of workflows and so forth and
in a common use case for it turned out
to be data warehouse replacement
especially if you used hive as a sequel
query engine that was really nice for
that in terms of lowering cost and but
maybe giving up some maturity and
sophistication that the kind of data
warehouses are good at but now we're
kind of transitioning to more of a
streaming world and it really boils down
to time equals money you know if I have
valuable information in my day too the
sooner I extract that value the better
off I am
it also has other benefits like if I'm
running massive batch jobs I need a lot
of infrastructure maybe a few times a
day but if I kind of you know bleed it
out over the day you know amortize it
the data out then it you know I need
less infrastructure so that's kind of a
side benefit as well so this diagram is
from that little booklet and that the
numbers correspond to points made in the
booklet but I'll walk through the
details a little bit there's a lot on
the slide but hopefully it'll make sense
as we go through it this is what I see
is kind of the canonical emerging
streaming data architecture today
obviously it's going to start with data
coming in so I kind of drew three
different bubbles of data coming in to
sort of represent classic restful
requests like
someone is authorizing a credit card
against your website or that kind of
thing it could also be the service
consoles for your big data system and
those are typically going to be handled
by what let's just call them micro
service toolkits which is sort of that
you know the state of the art today you
know whatever tool you use for that
purpose today and then the bottom two
blocks are meant to represent you know
large volumes of data coming in you know
a you know maybe in very large streams
but in some sense anonymous Records and
I'm basically used sockets to represent
anything coming from the outside that
could be IOT telemetry that could be the
Twitter firehose anything like that that
you're ingesting maybe partner data and
then I use logs to kind of really kind
of represent anything internal like the
exhaust logs that come off your server
so you might want to do analysis it
could also be like clickstream logs for
your website that you might want to do
you know usability studies on anything
like that but anyway the way it's
tending to play out these days is it all
ends up in Kafka at least most of the
streaming data maybe not so much the
rest data let's talk about that one
first so again typical micro service
toolkit kind of stuff we could be
anything including our bar P up there
ARP is the reactive platform that's
light bends of spin on react on micro
services but it could be go no js' you
know whatever your favorite thing is and
I'll mention and I'm gonna come back in
a minute to why I have microservices on
a slide about Big Data well I want to
come back to that point because I think
there's something interesting going on
here zoo keepers one of these things
that everybody has especially if you're
running Kafka it's required by Kafka but
Kafka it's really kind of the backplane
of this whole system and when it's going
to talk a lot about this later today but
it's really a great tool for ingesting
data in a reliable durable way not
really intended for long term storage
although some people are kind of pushing
the envelope about what that means
normally data lasts for about seven days
by default in a Kafka cluster and that
means anybody that wants to consume a
down stream even like to replace
something because something failed or
maybe decided to change how the analysis
was done you can just go back and reread
the data out of Kafka organized by topic
like a message queue one crucial
difference though is that under like
most message queues where as soon as
somebody reads it they delete the
message Kafka doesn't do it that way
they took the harder route but actually
more effective of any consumer on a
given topic will see the whole thing
they won't have to coordinate with other
consumers who were each consuming
messages on the topic instead everybody
gets to read the whole thing so if I
have like a dashboard they get to see it
all if I'm doing offline analytics they
get to see it all training machine
learning etc etc and then there's some
mechanisms for basically timing out the
data in which case it's garbage
collected as it were another important
benefit of Kafka for architectural
purposes is that it gets rid of
deliberately ugly architectures like
this so I I drew every possible
combination at least it won't say it's a
bipartite graph so anything on the left
and anything on the right is connected
and this is obviously messy to manage
and messy to draw so if you're the
architect responsible for all this you
know you like the architect and the
matrix it looks like me except I'm not
wearing a suit and this is not such a
great thing the other problem with this
is let's say service two crashes then
all of a sudden I have all these broken
connections and you know what's going to
happen to the data that's coming in from
this services on the left is it just
lost is you know service disrupted how
does that work well if you put Kafka in
the middle then you're basically using
the the that famous computer science
dictum that you can solve any problem
with another level of indirection so
here you go and I mean does several
things for us it cleans up the
architecture it gives us kind of a
common protocol if you will I mean if
you push this to the extreme then the
only thing you know I have to know how
to do is talk to Kafka whatever service
you're doing not necessarily what you
want to do all the time but you could do
it this way and then of course because
you have this thing in the middle it's
fairly durable if service one goes down
let's say then you're not going to
necessarily lose data as long as you can
recover within seven days or whatever
not always assured but hopefully seven
days is long enough so Kafka is pretty
important in this architecture but then
you've got data in Kafka now what are we
going to do with it well obviously we
need to process it so basically the
analog of MapReduce
and the Hadoop world is streaming
engines if you there's there was a great
blog post about a year ago and I really
should put a link in it in this talk
some guy looked at all the Apache
projects and found there like eleven of
them that claim to be streaming engines
so you know the paradox of choice here
you walk into you know Best Buy and you
see a hundred refrigerators and you walk
out because I'd want to buy the wrong
refrigerator I've got all these choices
so how do you decide what to pick well
what I'm going to do is argue that
there's maybe four engines that you
really ought to focus on because I think
they're the ones that both you know meet
all the criteria you could imagine like
the full spectrum of what you might want
to do and streaming and they're like
viable projects they're gonna live more
than you know the lifetime of some
company that's struggling to survive
that sort of thing but there's certainly
others that we could put up on this
slide
of course I'm these are all gonna be
open source one so I'll come back and
talk about these in a few minutes but
just to finish this diagram you know
these are basically all open source
components so that you can do whatever
persistence you want you can you know
write to file systems object stores like
s3 you know elasticsearch and that sort
of thing as well as traditional
databases and then the other thing
that's nice about this architecture is
it does free us up to consider different
deployment scenarios so we happen to be
big fans of maysa at like bend but some
people just run on cloud platforms and
leverage whatever they provide for
resource management etc kubernetes is
now emerging as important in this space
and people are doing it on Hadoop yarn
as well but you have a lot of
flexibility with the tools I put up here
basically everything I drew up here of
course I didn't give a lot of details
about the persistence tier but you're
not necessarily tied to hit dupe with
any of these tools today ok let's go
back and talk about the streaming
engines a little bit because there's a
lot to consider here when making this
choice this is a friend of mine on a
backpacking trip trying not to fall in
the the creek as he retrieves water so
what are some of the things you might
want to consider I said that I put for
streaming engines on this slide so
earlier so why for what what kind of
trade-offs do I need to consider when
I'm picking one let's say for a
particular task I have to
well one is what is your latency
requirement if you're using spark
streaming you can't get under like 200
milliseconds latency that's not so great
if you're trying to authorize credit
cards let's say on the fly I talked to
somebody to bank a few months ago who
said that you know when you click buy on
a ecommerce site you know there's
something like you know the two usual
200 millisecond response is like the
rule of thumb for when people want to
start to get annoyed if they don't get a
response on the webpage so 200
milliseconds that sounds like that would
fit within the SPARC streaming time
frame but no because there's a lot of
stuff going on you know in that cycle
from you clicking all the way down and
back and this guy said they get about 10
milliseconds to make a decision if their
credit card off system has a 10
millisecond budget to decide yeah or nay
so obviously that's not going to work if
you're trying to do that with spark
streaming but the other engines that
we'll talk about could handle that just
fine so just to walk through some of
these scenarios if you're really down at
the bleeding edge and some of you are in
finance and may have worked in high
frequency trading none of the tools I'm
talking about really address this need I
mean you're talking about custom
hardware you know kernel bypass network
cards and stuff like this not the sort
of thing that is we're really talking
about people use the word real-time a
lot in this space and they you know are
butchering the definition of the term
real-time is more like this you know
picoseconds or microseconds but most
people in the Big Data world are really
talking about milliseconds when they're
saying real time so we're not even going
to talk anymore about that if you you
know maybe at less than 100 microseconds
you could do this with some of the very
high-performance a JVM based message
handlers like the L max disrupt or if
any of you abuse that akka actors can
can give you this kind of turnaround but
you really have to know what you're
doing to avoid like garbage collection
pauses and so forth mostly that we tend
to work more and maybe the 10
millisecond range or something like that
where we have a wide range of choices
and pretty much all the tools I'm going
to talk about with the exception of
spark streaming could handle the this
scenario this sort of latency
requirement and then if you when you're
getting into like hundreds of
milliseconds now you might be thinking
about like windowing things like I'm
going to do a sequel query over the data
and do some window functions group buys
that kind of thing and then when we get
to much longer time frames then you get
into the realm of like I want to train a
machine learning model incrementally as
data arrives but it doesn't I don't have
to have the model exactly up-to-date to
the millisecond maybe every five minutes
I'll swap in a new machine learning
model into my stream processor to do
recommendations or spam filtering or
whatever and you know actually when if
you get to that point where you're
talking minutes I think you really ought
to consider just running batch jobs with
cron or some equivalent because one of
the cases I want to argue here is that
stream processing is as bad as
micro-service processing where you've
got something you're going to deploy and
it could run for months and if you wait
long enough anything is going to happen
you're gonna lose hard drives from that
time Network partitions you'll you know
maybe an s3 availability zone will go
down it's happened right so you're gonna
run into any kind of contingency plus
you know the usual ups and downs and
scalability requirements so a lot of
times it's actually easier just to run a
lot of batch job so that you know they
only have to be good for the few minutes
they're gonna run or maybe hours they go
away then when the next one starts it
can be scaled appropriately or whatever
but if you're gonna run something that's
a stream engine that's gonna run for
months then that kind of raises the bar
on your engineering requirements another
criterion would be what's the kind of
volume that I want to process yeah if
you're doing you know lots and lots of
data like you're ingesting the Twitter
firehose that's going to be put some
pretty serious requirements on
horsepower compared to just handling
maybe rest requests for a moderately
busy website once again just to put some
numbers here if it's less than say
10,000 or so events per second you know
we have a lot of tools that can handle
this pretty well for classic rest
requests it's not not really you
considered a high scale problem when
you're getting into the range of
hundreds of thousands well you can still
do like non-blocking rest or something
like this but you're gonna have to be
very careful that you don't have
bottlenecks in the system that limit
scalability with Amdahl's law and those
kind of things but then when you're
really getting up into like the millions
of events
records whatever per second then you're
probably going to want to do something
use something that can actually just you
know partition your data over a cluster
and do things in parallel like spark or
flank or so forth and just to wrap up a
few other things to think about what
kind of integration do I need with other
tools if I want to connect directly to a
database for my streaming engine kafka
streams for example it is really
designed to work within like with kafka
topics reading and writing not so much
connecting to other things there's other
tools for that purpose for example and
that's really why this is here because
most of the tools otherwise will usually
provide some sort of API to talk to
whatever it is you need to talk to and
then the last one is really basically
maybe the most important question what
are you actually trying to do with the
data if you're trying to write sequel
queries and here I just sort of mocked
up a little spark example using the
spark API version of a sequel query then
that's that obviously that's going to
limit your tools spark is the most
mature sequel option whereas for example
akka streams doesn't provide a sequel
capability at all but then on the other
hand if you're doing like ETL stuff then
you know I think this is a wonderful
example of for kafka streams where I
just like I have raw log data we're not
supposed to use strings for log messages
anymore according to the experts but
let's say let's suppose you're
old-school and you're still logging
strings but you actually want to parse
them into some record format for
downstream consumption that kind of
stuff is one example that's fantastic
for a tool like Kafka streams and then
another like maybe the opposite extreme
in terms of computation is what if I do
want to train machine learning models
dynamically rather than just do a big
batch training periodically okay so I'm
kind of belaboring a bunch of points
here but the last I think the last
important one is in terms of the kind of
processing I'd like to think of events
you know like server on fire or
something like that to have an identity
where I may need to process it
individually whereas something like the
Twitter firehose where it's all tweets
yeah sure you could drill in and say
that who's the person doing this tweet
or whatever but mostly we tend to
process that kind of stuff in bulk and
think of it as like anonymous
records in a sense and then actually
some of these engines are better at
individual event processing especially
at smaller volumes than they are at bulk
and vice versa okay so that's a long
list of reasons for picking one tool or
the other let's actually look at the the
four tools that I I had on this chart
okay so I'm this is sort of the right
side of that diagram it turns out that
Google has been doing stream processing
for a long time you know who knew but it
turns out that they tend to everything
like you know last twenty years
everything that we know now is something
Google invented and then either
described in a paper or actually a
recently open sourced and a good example
of the recent phenomenon is they've had
several iterations of stream processing
tools and they thought about all the
kind of scenarios you need to worry
about in stream processing and the open
sourced part of Google dataflow as
Apache beam the part that they didn't
over open source was the actual runner
except for like a test runner that just
runs in a single VM and that's actually
an interesting place where other tools
like spark and flink are stepping up to
function as runners for data flows
defined with Apache beam what's great
about beam is that they as I say they've
really thought through all of the
sophisticated semantics that you might
run into and stream processing
especially if you want to go beyond just
like yeah let's just kind of compute
averages over like the last hour of
sales per store but I don't really
actually need like an accounting quality
number I just want the average but what
if you actually wanted to do everything
in a stream processing scenario then
you'd have to handle a scenario like
this where let's suppose that you know
every whatever that time is I said
minutes here let's let's suppose every
minute I want to do some sort of
aggregation over some datum that's
occurring in my system some metric well
you know time-of-flight is obviously not
zero for even stuff going with him my
cluster in a normally operating cluster
there's always going to be some delay
which I'm trying to show with the arrows
that sort of trend to the right but then
you're going to have real problems when
you have like a network partition where
data might show up ten minutes late or
maybe a day late or something so the
question is if I'm if I'm actually let's
say feeding a dashboard but also feeding
my
counting system when do I decide I can
actually it's safe to do the calculation
when do I know I have all the data or if
I know that I may not have all the data
when is it okay to do maybe a
provisional calculation with some
facility for posting a correction later
on if data arrives late or when do I
decide that data can be ignored if it
arrives too late
these are the kind of questions that you
really get into if you think about what
it would take to make a streaming engine
like you know I'd like to say accounting
quality something that actually is you
know 100 100 percent accurate in some
sense of the word as opposed to
approximate and Google has thought about
these sort of things and beam has is
really kind of the state of the art in
terms of defining these kinds of
semantics that you might want to do
there's a I think in the speaker notes
for the slides which I always post with
the slides I think I have some links to
some really good talks by people from
the beam team if you're interested in
more details about all of this and I
think Jess you might get into it a
little bit later as well
but anyway that's sort of the state of
the art for a stream data processing and
one of the reasons that I really like
flank which meant you have actually
heard of link by the way it's maybe the
Lisa I know quite a few that's great one
of the reasons I like flank is a it's it
it really strives to be an effective
runner for beam semantics and by that I
mean that all of these runners usually
do a subset of the available semantics
you know based on what they're capable
of and that's obviously a moving target
but flink is ahead of everybody else I
would say other than Google's own
dataflow which is if you're in Google
Cloud actually you can use their data
flow but if link is also designed in
contrast to SPARC to be a low latency
processing engine but still at high
volume you know with you know partition
data pipelines and so forth and that
they do have early but evolving support
for things like sequel over streams and
machine learning and so forth but not as
mature as SPARC so akka streams is a
streaming API on top of the akka actor
model occas is one of the products that
light Ben supports it but again it's
open source I really like akka actors as
a concurrency primitives that removed
the need for me to worry about thread
safety and those kind of
basically an actor is like really it's
like an object where it is guaranteed to
be thread-safe by the implementation and
you send it messages for work to do and
then it processes them one at a time so
it's a really nice primitive but it is a
pretty low level primitive so if you're
really thinking about a stream
processing scenario then it's nice to
have a streaming API and that's what
akka streams is designed to do so
because of the fact it's built in on
actors it's very very low latency per
event it's really great for complex
event processing and I should mention
blink has a growing complex event
processing component as well this one is
one of those engines where if you don't
have massive datasets it's actually very
efficient on a per event basis as
opposed to something like SPARC where
even though SPARC is generally thought
of as efficient if you throw like you
know thousands of bytes at it then
you're just going to have a lot of
overhead that's going to swamp out the
the actual computation work so you don't
get much amortization over the data but
if you need something that's good at its
small data up to missing medium sized
data then akka streams is great for that
Kafka streams is a tool that's really
oriented towards the Kafka ecosystem so
if you're really based on Kafka as your
core you know your data pipeline in the
middle your back playing and Kafka
streams is a really good engine for
writing a lot of stuff I mentioned that
ETL scenario already another nice
abstraction they have in Kafka streams
is something called K tables or it
basically it's a table abstraction again
think about this scenario where I have
like a dashboard and I just want to see
like aggregations like average per
minute or what's the last high-water
mark for a given key sort of sort of a
dual case of instead of seeing every
event I just want to see some roll-up of
that like a typical database hence the
name K table it's really nice for that
Kafka streams does not try to be like
the solution to every streaming problem
but it's really good for a lot of
scenarios where I just I need to read
some stuff out of a Kafka topic
manipulate it in some way and write it
back to Kafka for downstream consumption
like that log ETL scenario I mentioned
earlier
and then there's spark which probably is
the one that you all have heard about
the most spark originally started as a
batch mode system just like MapReduce
but because it's relatively efficient
they came up with this clever hack you
know what if we just capture Windows of
data you know like fixed time intervals
and then just run little batch jobs over
that data and that actually works pretty
well for the most part but it does limit
the latency I said 200 milliseconds
earlier that's like the absolute lowest
limit it's more realistic to be like
half a second latency that would be like
the this window size but with spark you
get lots of very rich sequel semantics
and capabilities a pretty nice machine
learning library and the and there is a
group I think led by some people at
Cloudera to actually implement those
beam semantics on top of spark streaming
this is one of the areas where spark is
really evolving quickly because they
know that this latency is actually a
drawback for them because it opens the
door for tools like flink so we expect
that over time they'll replace the core
of spark so that it can actually be a
true streaming engine and not a mini
batch engine but I would typically turn
to spark you know in a complex
architecture like this you spark
streaming for things like training my
machine learning models for example even
though I may not be able to actually
score data with those models if I have
lower latency requirements so in fact
one of the things we're thinking about
it like Bend is well if I do that if I
score with safe flank but I want to
train with spark how do I share those
models and there are a lot of ways to do
it but right now you kind of have to
roll your own so we're thinking about
how to make that easier and as I said
because it was started out as a batch
engine you can actually you know write
code maybe the core of your code that
does some sort of processing and then
run it in a batch mode or run it in a
semi batch or mini batch mode so if you
know the lambda architecture this
actually makes it easier to implement
the lambda architecture okay
I want to talk a little bit about this
idea of microservices and fast data and
the kind of synergy that's emerging as I
see it this is a cool sliced rock in a
pond up in the Sierras they're actually
a big lake so I thought they look like
micro-services I meant many lifts
instead of macro lifts or something okay
so the question is this how is something
like this look like something like this
and this is just sort of a made-up
diagram of a typical micro service
deployment where you know my ecommerce
site has separate micro services you
know one for order processing one for
account management etc each of them has
its own storage that's usually a pattern
that's supposed to be good and they
communicate through messaging to get
services from each other as needed so
just you know strawman micro service
well I think there's actually some
interesting synergies here the first is
that a typical data application like
that ETL job that you're gonna write in
Kafka streams or maybe your daily
roll-ups that you're doing in and spark
sequel they tend to be something that
has a single responsibility they're
small and for some definition of small
you know spark may be massive these jobs
might be running with you know 10
gigabyte heaps or something but the code
you ride tends to look really small like
a very simple microservice for some
definition of micro much like micro
services themselves are supposed to be
single responsibility and and delegate
to other micro services for other things
you know as I mentioned at the beginning
both of them are gonna have to be able
to process data that never stops
arriving whether it's requests for
service or its actual you know the
Twitter firehose or something like that
so they have to remain responsive you
know they have to be available all the
time they have to be scalable and
resilient all these things there's this
idea of reactive programming that is
kind of designed to encapsulate these
sort of core requirements and micro
services so I'm gonna argue also that if
you think about a successful company
building micro services eventually data
becomes their dominant problem and when
Twitter started
it was basically a three-tier web app
you know for a bunch of guys within
Frigga what the name of the company was
that Twitter actually started as but of
course his people signed up for Twitter
then suddenly they had the the so-called
Justin Bieber problem where every time
he tweeted something they had to you
know broadcast to millions of followers
what he said and that just raised the
bar on making their architecture more
data centric so I actually think that
we're seeing this happen that if you
think about the past where we had like
monolithic services and big data where
there really wasn't a whole lot of
overlap between them they the engineers
let's say in each of these worlds tended
to think about different problems that
now they're starting to look more alike
that microservice people are kind of
overwhelmed by data if they're
successful and fast data people have to
be really good at writing highly
available durable resilient etc services
so I think there's a lot of overlap here
so just a few shameless plugs from from
light Ben I'm actually leading the team
that's building a platform and I did it
in color because you know it's a
commercial product so yeah I actually
drew a piece but anyway we're trying to
think about these problems and how would
we help people build these architectures
that blend microservice and and in fast
data streaming tools in a coherent whole
we've talked about these pieces circled
in red obviously a commercial product
like this you would expect to have
production tooling so we have monitoring
tooling and management tooling as well
and we're actually focused on Enterprise
DCOs which is the commercial version
Amazo says our deployment platform and
we also think there's a really
interesting opportunity for machine
learning in this situation because as I
mentioned these services have to run
forever and so how are they going to
remain reliable well I think the way to
do that as much as possible make them
the cluster self managing and
self-healing so we're spending a lot of
time you know building services around
that idea to make it you know truly
let's face it once you get used to
installing these clusters and writing
these apps then maybe you don't need a
commercial distribution
much but we hope this will actually keep
it valuable forever well I've actually
finished a little early so here's the
link again for that that book and I
thank you very much any questions yes
the question was to repeat for the video
how do you deal with merging streams
especially when they're like a different
velocities different rates well if you
watch Ghost Busters you should never
cross your streams so just professional
tip there yeah that's that's an
interesting one because most of these
tools do provide some notion of a join
and that is a really interesting
question when you don't have all the
data what does it actually mean to do a
join between say two tables in that
sense again these are some of the
semantics that people like the beam team
have thought about and there's just a
whole bunch of solutions like for
example if some of the data is small a
lot of these engines are now building
the capabilities to keep moving state in
memory and with maybe backing store of
some kind so that you could let's say
you have it's some sort of lookup table
and it's slightly evolving I don't know
let's say it's like a skew catalog in
your store and you want to actually do
joins against the real-time traffic in
the store against the current catalog or
something like that that kind of stuff
you would I think is really doing some
amazing stuff in this area right now to
make it easy to get this kind of your
out-of-band data into the system or even
write it out of the system as you go but
a lot of times you can actually think of
these as a stream itself that you're
just going to you know like to go back
the example the skew a catalog is going
to be a stream and I've got my you know
traffic as a stream and I'm just going
to do joins and as they come in
so then you'd have to figure out well
okay that seems that makes sense if it's
maybe Delta's to the catalog that I want
to update but what if I just want the
whole thing what does that actually mean
in this case so I think it brings up
some examples to really point out that
it probably depends a lot on the
scenario
but these are all problems that these
streaming engines are trying to address
in one way or the other and in a
particular case you'd end up you know
figuring out what makes sense and a lot
of times what would make sense depends
on the amount of data you're talking
about for either one or both streams
yeah so the question was what is the
runner do in the beam case if we've
already got orchestrators and I assume
you mean like meso sand yarn and so
forth it's really well if you think
about it it's it's it's the way you
define the data flow and how you
actually run it how you how you go from
like a good analog would be how you
write an sequel query in the abstract
and turn it into a physical
representation that's actually run
against real data files real tasks
distribute across a cluster if you've
ever looked at like the described
command of a sequel query you know it'll
show you this breakdown from like a
logical query down to a physical query
and basically what you're what you're
putting in there with the runner is sort
of that physical manifestation of this
more logical data flow so that's that's
sort thuggin all to how those tests are
going to be scheduled in at runtime and
and managed basically anybody else yes
Pachi storm that's a really good
question I should probably add a slide
about this
I think pragmatically speaking storm
seems to be vanishing from the landscape
independent of its qualities as a as an
engine you know it's it's it's sort of
the usual dilemma where it's not always
the best thing that wins or the thing
that's the newest tends to be you get
the attention what I would do if I were
really devoted to storm is I would take
a look at a Twitter Heron which was
their complete rewrite that's API
compatible I don't know a lot about it
I've talked to the engineers at Twitter
about it a little bit and and they've
they've addressed some of the
limitations of storms such as the
problem of trying to evolve the the the
graph of nodes so I forget the topology
and I think they've enabled that ability
so that it can now be more dynamic at
runtime if you think about the way storm
typically works you kind of lay out the
topology of the nodes and then you run
data through them
whereas what tools like flink and spark
are doing instead is sort of dynamically
building that topology based on a kind
of abstraction which is the program you
write as opposed to having dedicated
nodes that are more statically oriented
that's my understanding of the drawbacks
of storm the way I tend to think of it
is it's it's like the first generation
stream processing engine and a really
good one but we've kind of learned a lot
in the last you know it's only been like
seven years which is basically 49 dog
years I guess so it feels like a long
time but and I think some of the newer
entrants are trying to you know leverage
what we've learned and be more adaptable
but yeah I certainly wouldn't remove
storm just because you know it's not the
sexy thing right now
anybody else yes
knife I yeah so this is a really
interesting Apache project the question
is about knife I it's an interesting
Apache project where you basically
graphically draw out your stream flow
and then it generates the tasks for you
the only thing I dislike about it is
that they do use their own proprietary
engines it but it's still open source of
propriety
in the sense that it's not like spark or
something I think that's a really great
idea especially for novice users that
want to define streams the only thing
I'm really wonder about it though
actually is where as sequel queries are
the sort of thing that everybody wants
to be able to write no matter what their
level of expertise I'm not entirely sure
that people are going to define stream
processing flows arbitrarily that seems
more like an engineering job to me I'd
like to be proven wrong at the very
least what I would really love to see
knife I evolved into is actually both
the definition tool for streams but also
like a visualizer of runtime behavior of
the stream if you could have the same
thing in one view then I would use it
for sure
because then I think it would be really
useful to you know both see what I wrote
and then see it in runtime yeah
next year yeah that's again seven dog
years so it's a little hard to know what
would it look like I think actually that
you'll see beam a lot more I think it's
kind of one of those it's interesting
about beam is I was talking with Jessie
about this yesterday they're going to
adopt a Scala API that Spotify wrote
right yeah and yeah I'm a scholar
developer as you may know working for a
light band may be no surprise so I'm
really a big at about Scala it's a
strong word but I actually don't like
the beam API that much at least the job
of one compared to the Scala api's so I
would I would love to see a Scala API
and but but the reason I mentioned this
is because what seems to be emerging is
that beam might be the meta API that
sits on top of all of these others so in
a year we might be talking about not
using the SPARC API or kafka streams or
whatever we might be all writing to the
beam API and then materializing
underneath with the runner that makes
sense for whatever the like the data
format looks like or some per units
whatever that kind of stuff so I think
beam is going to be the thing to watch
in that regard that's a really good
question of other things I think you'll
definitely see more machine learning in
a streaming context and more sequel in a
streaming context because once again
everybody loves to write sequel and they
don't want to give that up just because
their stream processing now instead of
batch processing so that's always a good
way out of a question like that it's
just more of the same if there's a
question here
what about turning legacy data sets and
sources into streams
well I think the if you ask the beam
guys or any and also the Flint guys this
question what they'll say is that really
a batch job is just a finite stream and
that's how they actually implement batch
in flank for example is just treat the
stream is finite so that's one way to do
it you could certainly connect a stream
job to a legacy database and have it
just process say the whole table and
then it's finished or you could I even I
don't know if I've seen anybody do this
but you like here's the interesting use
for spark streaming maybe
suppose that you have periodic updates
to a database and you want spark to
watch that and then immediately grab
those deltas and then process them for
some downstream use so so I think you're
gonna see a lot more of that sort of
thing it kind of pushes a little hard on
what databases are used to dealing with
in terms of connections and the kind of
processing but the other cool thing
about these open source projects is you
know if your legacy batch stuff is just
like HDFS files you know it's no
different as far as spark or flink are
concerned you're just going to read them
in in and process them and maybe join
them against a live data as well yeah so
it to clarify you clarify this question
for the video that you know what about
I'm going to convert like a batch
oriented applications to a more stream
processing and a couple comments about
that I've been talking with people you
know a lot of times your wart you're in
an industry where that is really hard
just because you might only be getting
data once a day like medical is really
bad about this you know they'll still
FTP a file to you once a day so there's
no way you're gonna do stream processing
anyway you know it's kind of defeats the
point
so if but if you can make that
transformation then the next question I
would ask is really what is your latency
requirement you know is it a nice to
have to have like subset and processing
or is really 10 seconds or a minute or
whatever fast enough because if it is
then you have a lot more options like
maybe just running those batch jobs more
frequently on smaller data data sets
that are arriving you know new or
whatever and then you avoid all of this
effort I mentioned about keeping a
streaming job happy for months on end
but if you really do get to the
especially at larger volumes where you
really want to process stuff on the fly
maybe because you'd like to have
up-to-date and like search engines are a
great example of this yeah breaking news
you want to see it on your search engine
right away then what I would probably do
is I would avoid the lambda architecture
actually because I think it's too hard
to implement both a stream processing in
a batch mode if you can avoid it but a
lot of these batch jobs if you the logic
can actually be repurposed as streaming
but you might have to use a new tool set
to do it like and this is something
SPARC is actually very handy for if
you're using SPARC already so that was
kind of a rambling answer but hopefully
it gave you some thoughts maybe one more
question
go ahead yeah sure Alyssa so the
question was about I made this comment
about the challenge of sharing models
between the training engine like maybe
spark mini-batch and you're scoring
engine which could be flaying Karaka or
something like that so here's there's a
bunch of ways to do this one is if
you're using a lot of these big machine
learning systems like deep learning for
J or tensorflow
you can run them as servers so if you
can tolerate the overhead of making a
restful call to a server to do the
scoring so we're on the scoring side now
you've got tensorflow also training the
models in some process and now I've got
data I want to score against the the
model you can use tensorflow as a server
so at least everything from the machine
learning perspective is in one place
it's in tensor flow if it's a fairly
simple model like logistic regression or
something like that where you could
actually just take the parameters that
were trained and then just
reimplementation over in your scoring
engine then you could move the
parameters over there's several ways
that are kind of emerging to do this and
I'll just reel them off one is you could
write those to a database and you have
some process that periodically refreshes
from the database with flink they have
this site input and output feature I
think that's the name of it for a member
that lets you you read data you know you
have the stream processing thing going
on and then periodically you read data
from the
and that's how you could bring in your
parameters and you could do that with a
database you could do that writing to a
Kafka topic and then bring that in this
bit might be another example of the
other gentleman's question about maybe
I'm actually I'm sort of doing a pseudo
join where I have here's my model
parameters coming in here's my data and
I somehow joining them that's a bit
awkward to do actually but and then the
last one I'll mention is for really big
models like neural network models
especially in a distributed context
there are these parameter servers that
are emerging as a way of as fast as
they're like a customized database
really for serving the parameters of a
model to engine so I so for example
maybe in tensorflow I train a neural
network I write to this parameter server
and then I pull them into a different
model over in flink let's say and the
last one that's kind of related is there
are these interchange formats like PMML
and PFA and although they have their
critics and their limitations in that
sense you're really just exchanging the
metadata about the model or the
parameters and then once again if you
have a way of reproducing the model in
both environments either through a
library or a separate implementation
then you could also do it that way so
that was off the top of my head just a
bunch of ways that it can be done okay
thanks again
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>