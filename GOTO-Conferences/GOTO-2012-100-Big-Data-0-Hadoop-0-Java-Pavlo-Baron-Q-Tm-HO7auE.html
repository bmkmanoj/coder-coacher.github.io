<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • 100% Big Data. 0% Hadoop. 0% Java • Pavlo Baron | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • 100% Big Data. 0% Hadoop. 0% Java • Pavlo Baron - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • 100% Big Data. 0% Hadoop. 0% Java • Pavlo Baron</b></h2><h5 class="post__date">2013-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Q-Tm-HO7auE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the name of this talk is one hundred
percent big data you need to pronunciate
it like in a cool way so it's big data
and it's zero percent Hadoop and 0% Java
whatever it means basically after the
Sun keynote given today I can just pack
my bags and go home because those are
impressive numbers and this is useful
stuff and but well I had this
presentation you have this time so let's
do the best out of it actually I like
smaller rooms because of being close to
the audience and being like a good
target so you can throw beer bottles at
me I'm a little bit too sarcastic I
think you would call it like that so
okay briefly about myself um well i'm
working for code centric i'm doing
technology consulting and bleeding edge
areas also whatever it means i mean
everything is kind of bleeding edge
technology which is not typically jsf
spring and hibernate so um well I'm
trying to find my way through different
technologies and I also write books
about different things so I have three
of them already done and I also have
this one running right now big data for
80 decision-makers it's absolutely
non-technical compared to the previous
one which is on the market for now one
year it's a line OTP okay let me put
this disclaimer I told you I'm sarcastic
but basically I don't rent I just
expressed my opinion and those who know
me who have attended my talks will also
know that I like to tell stories so it's
a story and we can do questions
afterwards it's an experience
presentation so does anybody know what
this is oh sorry this book
okay let's go so the story begins like
that I'm in a presentation of different
companies and products sitting there
listening to is anybody from the company
called intensity here in this room
nobody okay does anybody know what they
did so basically the presentation was
like you know are we have big data and
big data and big data and remember don't
rent so for my experience every talk
given at the conference is kind of
marketing either you market a product to
a point or you market the company or you
market yourself that's my experience so
but in this case I'm just sitting there
listening to this end it looks like
Houdini magic so what is done there does
anybody know what who is Houdini awesome
so it's like you know they analyze real
time I will come to this point later arm
social networks and looking for for
example for different companies who are
partners with them or customers for
rents on products for example somebody
just writes this I'm sorry am I
allowed to say this on video also so
this samsung s3 doesn't work and
it's real-time analytics so the vendor
will be will get informed about this
incident in a way so it looks like magic
and they're in the presentation was a
map of the world tendent all the time
some tweets are popping up with rents
and stuff so what I did just in the
middle of the presentation i sent a
tweet renting pretty much about a
product what I could derive from the
presentation but my I have never seen my
tweet to pop up so well
I mean okay so I think you're telling me
it's smoke and mirrors right oh I'm in
this industry for over 20 years so I
learned that there is nothing like smoke
and mirrors here well smoke basically
when you just burn your hard disks wow
so it's like it looks like natural
language processing does anybody do like
natural language processing in this
audience does anybody know what it is
okay so it looks like this to me and it
looks like a lot of math and also well
sort of machine learning the machine
does something and it informs people
about brands does anybody also do
machine learning for living that's hot
stuff so I think hey come on i can think
of this what is happening there I mean
as I see my own twitter is not popping
up so something must be behind the
scenes so it's sort of big data that's
basically the explanation what I did to
think of this so the first thing that
I'm thinking of getting back to the
hotel is our we're going to get this big
data and friends you need a platform
where people say what what they think
before they think what they say what is
this platform of course it's Twitter and
including me I also say what I think
before I think what I say most of the
time so I realize of course I need to
drink my big data pretty warm straight
from the firehose that's how you would
name this thing so I asked Twitter
through searches and stuff how can I
drink from you or from your fire hose
basically the firehouse is not available
for normal people so you can get the
data from Nippon from DataSift for a
small licence fee more or less maybe pre
analyzed or something so it's nothing
that I can easily do in a hotel room and
there has been something like garden
hose but as far as I can understand it
doesn't exist anymore even for research
project so the poor man's alternative
would be to you know to consume this
public stream did anybody do something
like that previously consuming public
stream from Twitter okay so you get
something like you know up to two
thousand tweets per minute maximum just
let it come
so huh I mean well it's not oppressive
right it's not big data isn't it where
is this big data but just remember I'm
sitting there listening to the
presentation not even seeing my own
tweet and but still I think I can kind
of think of this so start sketching some
bubbles here well basically how i would
do this i have some tweets here i
consumed this thing and IQ this stuff
and then I did some analytics and then I
would report this in my case it just
decided to write a file because I didn't
want to send around tweets anybody so
now I need some adequate basic tag to
tinker this
so well i'm doing java for far over 10
years now and there is a lot of stuff
you can use in the Java world to do
something like that more or less quickly
so it's a big selection here of stuff
are there any java fanatics around here
in this audience real hardcore fanatics
remember I'm sarcastic don't take it
personally if I rent or if I'm being
being honest it's all in good fun so
when I think of Java strange things come
to my mind um I've seen a lot of
solutions based on well basically
written in Java and using the whole
ecosystem they're pretty overloaded and
for my tinkering I'm just thinking come
on a little bit too much maybe and just
another disclaimer I like the JVM I
actually love the JVM because uh it's a
complex and it's proof tag and you also
can do mechanical sympathy does anybody
know what mechanical sympathies okay
that's a pretty cool term I think L
makes guys have brought it into the IT
industry but when you do it on the JVM
your coats that starts looking like sea
and you would push away to see cycles as
far as possible so the question is then
why not just you see but anyway around
Java there is a biggest ecosystem and
thus a huge community and there are real
bright guys working on the JVM itself
they're real cool experts
but still I like it I love it but
strange things come to my mind and I
think of it does anybody know what it is
can someone tell me this what is that
excuse me yeah that's actually may be
true but I would say that it's just big
that on your down on your hard disk
because in order to compile something
which is half way sophisticated you will
download half the internet on your hard
disk that's real big data so actually
the big data on the JVM is also a pretty
rich thing so you have Hadoop you have
pig you have storm aspirant all these CP
engines have no hope to do machine
learning you also have tons of lips and
frameworks and libraries experiences
from other people and it's also a big
part of a hype around the Hadoop
ecosystem but still strange things come
to my mind do you want me to elaborate
on this one not really let's give it
there is also this addicted to a
different platform for the reasons I
just wanted to learn it I just wanted to
meet those awesome guys working in this
community so I like erling and I also do
erling i also have written a whole book
about Erling just to learn it so it took
like two and a half years doing so and
there is a different perspective on this
I think it's not well readable so just
let me read it for you I'm it's one of
my tweets and like saying something I
think before I think even you know so
it's I slowly realized that I will never
have something they call success because
I simply refuse writing the mainstream
wave so just want to experiment
so what i did i just decided to combine
erling based software that i delved into
into previously with python hacking
because i wanted to do more python
hacking and button is actually or python
is actually very popular among
scientists if you know this and it's
also scripting and I really value
scripting I mean it has of course
drawbacks when you really need your
stuff get to be compiled but anyway I
like scripting because it's pretty
flexible so let's catch some concrete
bubbles for that I just I mean it's
pretty simple you just read from the
sample stream using tweets small library
for Twitter XS through Python then I
would just build on rabbit I would tell
I would cue this stuff and i would use
an l TK and i'll write the results to
the file does anybody know about NLT k
here well NLT k this is the framework
it's written in python and and also on
fortran well basically the basic
libraries are written in fortran
partially so it's absolutely awesome to
do things on text analyzing text in
whatever way the NLP would need it but
the question from a big data expert
could also be i mean you could really do
multiphase MapReduce on that right well
the problem here is that you read from
the stream so first of all this sample
stream you will never know where it ends
and the problem with MapReduce that you
first for example with a new beautiful
first need to fill the data kind of into
the HDFS if you don't do it well the
other way it would be just to consume it
from the web or somewhere but then it
comes to the split you need to split
this data you need to
that many were workers or mappers
whatever that would fit into this
concept to this model so he just
typically you can't nap reduce stream
data you don't do it I didn't attend the
talk bye bye nothing motts yesterday but
I think he will have explained in detail
why it's not possible and still what
will be done there has nothing to do
with real time real time enters terms of
computer science is something different
it's I mean time is time so it's time
bound and you have hard real-time have
soft real-time the difference is just
what happens in the case of the error of
the problem so you can call it near real
time or as fast as possible or Eve or
even something that you that can be done
while you are ordering a pizza but
anyway everything in this chain is
pretty boring I think half of you guys
at least have done something around
rabbit in q4 queuing well except NLT k
except NLP so what I thought first was
well i would use analytica just analyze
the text and then i can think of this
kind of intent i can find ransomed my
problem was that it just came different
in this case because of this does
anybody know who justin bieber's does
anybody like him is in this audience
okay i I don't judge but the problem is
that it was a fluid of believers and
disbelievers are people who love
official in love Justin Bieber so in
this case I have like round round about
sixty percent of the sample stream was
just garbage wasn't useful I mean
something comes in it has nothing to do
with anything it's just a promotional
tweet or something for Justin Bieber so
those of you who do machine learning and
stuff will know that for statistical
work you need a lot of data so the more
data you have the better you can trade
your classifiers if you classify the bed
is the result or the end of the day so
then I say okay I need to filter
something I can filter out beliebers
it's okay but what's next what are the
next criteria to do so let's walk
through possible criteria um reasonable
user names does anybody in this audience
know with these guys okay this is a big
geek and the problem is that we probably
all know who he is but how can machine
know about him I mean it's just d h h
here almost impossible so just forget
real names just forget user names just
forget with the names it's well it's
almost anything let's walk through the
next possibility for example a profile
bio James is an awesome geek but in his
profile in the bio he says sexy as
so would you filter out this knowing
that this guy could possibly also rent
the answer is no so you wouldn't filter
it so even if it's where words in there
the other cool guys sui the history of
this slide is pretty simple I've met him
in San Francisco in March and he left
before I left and I think for three or
four months even after he came back to
Israel
his tweets have been geo-located with
San Francisco so it's absolutely
unreliable information as well this one
is not that well-known um well basically
you have this field for your own
location that you can provide to Twitter
my location says senior rubber duck well
it's kind of professional title ah does
anybody do rubber duck in here for a
living are you guys popular I do it in
both direction I love it it's a cool
thing so I just wanted to pimp my
twitter profile page so how about
correct language how do you what would
you do with the language this is just a
small example from google translator we
just type in would night and it's get
getting detected as german i mean google
translator is a pretty awesome thing
that's real cool it took me some time to
think of this together here but well I
mean I know Google I I don't have this
data I don't have this information so i
need to think of something from my own
you well basically it's not English it's
not German and well for me it looked
like eighty percent of all tweets that
come through the sample stream ah when I
scratch away sixty percent of garbage
okay so i can do only my best and that
means i would not filter for location i
would not filter for profile by oh I
would use an l TK to classify between
English and Spanish
just let it come so the problem there
without NLT k is everything everything
everything is based on coopera so corpus
is just a text it might be tacked it
might be not tagged however you use it
but basically you need a sort of basic
information where you can that you can
sort of compare with what comes in and
find out if it's negative or positive or
something what they do they're actually
in the available corpora is they use
movie reviews and some chat protocols
the data is not that big and i'm pretty
sure that it's been censored before so
for real rants with swear words it
wouldn't work that well so what I had to
tinker also two separate reasonable
english language front from a useless
language for me was to well i just said
spanish after a couple of hours of
tinkering spanish seems to be a magnet
for anything else anything but english
so just push it away everything that
comes in in spanish and the rest is
english and i can deal with the english
thing the probability called clay
calculation there is also not that
simple anyway I would risk catch my
bubbles then and the bubbles would look
like this I would consume the stream I
would cue this that I have to filter
that I would push it in another queue
then i do my analyzes and then i will
pre port just getting a little bit
bigger the concrete bubbles are looking
like that it's still pretty simple is
just 2 q's in the rabbit exchange so I'm
careful now and basically what can I
filter out losing some data which is not
good but I need usable data so it means
that I can also i can almost filter
nothing out
so this also means that what is there
from the data point of view is almost
nothing so I need to make this data even
bigger it needs to smell like big data
because it's Houdini magic and big data
and stuff so you would just asked
whether how would you stalk somebody you
know users and you get you can get more
users through you know when your
reference using or mention a user you
can have this user as well so it would
kind of explode and how can you stop
this the problem there is that you have
the current limitation is 150
unauthenticated api calls per hour when
you want to do stuff like me you need to
have an authentication in an app for
that so it's 350 calls per hour just
compare it with the numbers I told you
about then you have limits / IP address
so how would you scale this scale would
mean you can use more IP addresses I
will just throw the name cloud into the
audience right now whatever you could
scale through more users you just create
users and apps and you will scale like
this but chances are that you will hurt
yourself with Terms of Service and it's
just an experiment so I just wanted to
tinker this edina magic here so anyway
there is more bubbles than originally
consumed q filter q store somewhere do
MapReduce analyze the data and report
this the concrete bubbles are those that
i have used even still thuy p rabbit and
l TK rabbit react to store data disk 0
to do MapReduce NLT k file
but wait react disco mapreduce I mean
didn't I tell you that I wouldn't do it
without that I would do it without her
do well let me explain the tag a little
so what I didn't explain before a pic
rabbit because RabbitMQ is fast and
reliable and flexible it's written in
early so it's Ireland like and reliable
first of all second thing about react
we're in the no sequel trachsel needs to
be related to this it's also written in
in Erlang it can store your data
distributed redundant and reliable and
it's based on a link so our link
distribution redundancy and reliability
you just buy it with it did anybody ever
hear of the disco project no well disco
project has been created as far as I
know pi by nokia and open-source it it's
something that they use internally for
my produce things and it's pretty cool
because it's not only written in early
well its core for distribution for
firing up workers and then to this all
stuff is written in early but what you
do there you write Python Python mappers
and reducers or jobs and they get this
distributed so you would natively be
able to access an l TK and that's it
well actually this is a whole chain for
that I just wanted to use the NLT k with
the most experienced of all and LP guys
all over the world so what i do this
well what I do with this chain here is I
still use this in the rear data store
then I run disco mappers data locally on
the react nodes and i would ask twitter
for users and the followers you know in
order to grow the database
it's very slow down recursively it's
becoming a huge graph it slow because of
the API limitations but still just
remember I'm sitting there and listening
to the Houdini magic so why would a
queue this because I want to drink from
the sample stream I just assumed that I
don't know how much data will get those
are not impressive numbers but still I
just wanted to store date data and react
in the distributed way without any
possibility for a data store to slow
down the whole chain I can absolutely
not impress react with these numbers but
it can it can become more and more and
more maybe i will buy a yearly license
with data saved I'm just kidding so what
about this Python stuff the Python stuff
is what pattern is slow at some points
yes the whole chain will would balance
it out so again I want to do qualified
NLP using LDK and well there are there
is a couple of possibilities how to
solve it for example with pi PI to run
impressive benchmarks how you can run an
l TK programs on l TK based programs
scripts with pi PI even faster of course
I can do this whole thing on the JVM but
just remember those strange things
coming to my mind I mean just to be
honest we all are maybe all guys in this
audience are currently doing jvm based
work for living but you know just to
tinker something that is absolutely
outside of this mainstream and this
whole hype because I will explain you by
the end of the presentation that it's
not about this
tooling and stuff so well I'm finally at
this point of this rant analyzes now I
have the whole tool chain i have my
tweets i can grow my database with the
users and stuff i would stalk for them i
would read their last on the tweets or
something so how would i do this well
the very naive way to do this would just
look for swear words but how will you
deal with something like that it says oh
my god it's cold as I love it how
would you classify this is it a negative
or positive emotion
almost impossible well in this case i
would say love would maybe kind
of balance out each other and oh my god
well it's also it also can be positive
as negative so anyway this one doesn't
help much you cannot even filter it out
because it's not about Justin Bieber so
the right the right way to do this and
that's what worked in this case is
centered on sentiment analyzes for
example from nave bias classic based
classification does anybody know about
this okay cool so um I would just
suggest that even if you hate the talk
and if you don't agree with not using
Hadoop or something just have a look at
statistics and things like that um well
basic actually this classification
problem of the text is a home of ml TK
that's why I picked it and created a
whole tool chain around it so it can
tell a from being on the text classify
this but you need better corpora for
that because as I told you they use this
movie reviews and I'm pretty sure there
are they have been censored before so
you won't find those heavy words in that
the question is where would you get this
corpora does anybody have a suggestion
if somebody no knows what what is a
corpus it's just a list of words for
example or text words or parts of text
and stuff what can I get something that
is real real heavy swearing stuff
twitter well I'm trying to analyze
Twitter you need to sit there and
classify everything you need something
which is maybe well more predictable so
that's easy
does everybody know Linda's traveled
well he's not fearing that much is an
awesome guy he's one of those who have
actually managed to change the IT world
but anyway when you have a post starting
with like you saw our own you can expect
that this is sort of negative maybe so
you can derive some information a little
bit more here mrs. hecho does anybody
know about him I hear somebody laughing
or is it in the different room now he is
also a great gig but I mean it says
programmers need to learn statistic ah
or I will kill them all and it goes on
and goes on in the post so it's pretty
impressive how much well rent
classifiable information you can pick
out of this post so in at the end of the
day I have my file with rent and the
numbers of this round about 50,000 users
per week maybe five really qualified
friends I don't have colorful charts I
don't have this map and tweets popping
up even my own tweet not popping out
there because i'm pretty sure that the
sample stream of twitter is being well
explicitly filled with belieber tweets
but nevermind so what I learned from it
and what I can tell you what to what's
also possible to learn so I learned some
useful things they're experimenting I'm
now more electric against kool-aid then
I've been before they have written a
book called pragmatic IT architecture
which is pure sarcasm
well big in the big data doesn't
necessarily mean that it's big whatever
big means I mean certain guys know what
big is but in a different way there are
different ways to deal with big data so
what I did here I had to tinker some
technology I had to teach disco to not
to fire up workers on existing nodes or
available notes i had to tinker it to
use pre started nodes and two call
erling functions natively because i want
data locality i want to pull data out of
the v notes of this one react node well
with possible risks of duplicates maybe
but still data locality is a very
important principle when you do this
when you do MapReduce on something
wearing just need to fill pile a with
the data from pile be this will slow
this whole thing down I also started
implementing something called so it's a
german word for germans in there right
ok so you can you know what I mean ok
it's like pig but in German and did
anybody use pic here nobody now I can't
believe it really when you fire it up it
says grunt so mine says oil just for fun
so I will I will work on it than the
test maybe potential just to have a
problem python python implementation of
of pig latin processing engine here so i
can use even more abstract way to do
MapReduce to write MapReduce jobs on top
of this disk on the whole tool chain
behind the scenes
so I mean for those numbers which are
not very impressive maybe it's possible
to run the whole thing during the week
during several weeks on a single w520 so
the northern machine itself a cloud i
call it half a cloud video greetings to
mr. Darrow kindness he loves it when I
say this like it's a mobile workstation
so I have a laptop 7 kilogram of weight
what do we learn about big data in this
talk well actually in the area of the
social data analytics first problem that
you have it's absolutely chaotic so you
need to find ways to store this data to
process this data in a way that you
haven't expected to come you cannot rely
on anything me the location or something
like that in the case of this stream
it's mostly garbage you can expect the
same problem also when you access
facebook or something it's a lot of
tinkering I think everybody who does
some statistical Brooke would say that
you need to tinker the result kind of
until it makes sense so you need to
sorry so you need to tinker your
algorithms you need to filter that's one
next problem so you have to combine two
approaches this MapReduce stuff wouldn't
work on as a filter because I mean
before it hits the data store you would
need to filter data so we need two
different approaches here it's a lot of
math statistics machine learning
analytics and all that it's basically
it's not that complicated with NLT k for
example but still you need to understand
which algorithm is better for what and
natural language processing in this case
tool selection freedom is very important
that's what I'm trying to say it's not
about the tool what about what you pick
don't expect that when you pick a dupe
out of the shelf
it will break automatically for you
that's the point in this kool-aid
acceptance or not acceptance in its
endless playground for geeks with
aspiration well I think that I have some
inspiration here I just want to get
better every day to learn new things so
I need to get to the ground of how to
process this data however it comes
wherever it comes from in the best way
which is possible so just let me say
more abstract it's about what you're
trying to find in the data that's big
data you need to know what you want to
find that it's about finding the best
mathematical way to find it because it's
about math analytics it's about it's
about filtering out what you don't want
to see and it's pretty much garbage in
there wherever it comes from it's about
knowing limits and hot spots and trying
to create a set up around this and it's
also about picking the right tool chain
for this job so only one two it's a
combination of different tools even if
you go for the JVM you would maybe kind
of combine for different purposes as per
or storm and Hadoop for different things
from my experience companies failed
right now potential customers in this
big data adoption not because they've
heard about Hadoop and don't really have
anybody to do this because it's
technology technology is learnable it's
because they don't know what to do with
the data so if you go out and say them
hey it's mathematics we can analyze this
so then my job is done so let's let me
just wrap it up pick that is one hundred
percent data it's not new it's just like
chaos but it's still data you can
analyze this you can process it
with all possible uncertainties that you
have it's zero to one hundred percent
Hadoop so it's actually different than
the original title of the talk it's also
zero to one percent java that's very
important to know that even if this Java
world is really rich there are different
solutions for different purposes that
you can use and it's always a
combination of different things it's
also not only sequel no sequel whatever
however whatever buzz word you would use
that no sequel or not it's also not
black or white you just need the right
tool for this special job in my case I
just picked react because a I wanted to
be create be I had experience with react
see I know how it distributes and how I
can access it then so that was the best
tool the best tool for this case and
it's absolutely science it's absolutely
analytics and it's a lot of
experimenting before you get results
that are reasonable so that's it thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>