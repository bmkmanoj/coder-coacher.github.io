<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • Data Science at Scala with Spark • Dean Wampler | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • Data Science at Scala with Spark • Dean Wampler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • Data Science at Scala with Spark • Dean Wampler</b></h2><h5 class="post__date">2015-07-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dsiTp4sCHaE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I've been doing big data consulting
for a while before i joined typesafe to
work on tools like spark and I pretty
real I realized pretty quickly that's
the tools were terrible so I started
trolling the community back in you know
2012 or so when I started back in those
days typically if you wanted to do big
data science at scale what you would
typically do is like you create your
models in Python and so forth and then
throw them over the wall to some Java
developers to write MapReduce code which
kind of sucked and I didn't talk about
three years ago at the Northeast Scala
symposium where I argued that big data
needs to be functional and what I meant
by that is that he you know i love what
Martin and todd said this morning about
functional programming it can be a bit
religious and dogmatic at times or at
least certain people in the community
but it does give us the right
abstractions for working with data and
thinking of it as immutable values that
we transform and so forth and it turns
out that SQL is actually a functional
language of sorts and that's always
served us really well so you know first
of all you might say well why use the
JVM well you know I think we all know
that it's it's a well-established
platform it's it does scale extremely
well it's just a lot of tools we've had
for a while different programming
languages now like Scala and closure in
addition to Java those are math
libraries on the right for in Scala
algebra inspire that I won't really talk
about and then we've got you know mature
development environments and these days
we've got a very mature ecosystem of
tools mostly around the Hadoop ecosystem
but not necessarily all of them that let
us you know approach different kinds of
data problems at scale you know there's
just a laundry list of some of them here
we're going to talk about spark a little
bit a predecessor to spark that was very
influential was scalding at Twitter
wrote which sat on top of MapReduce and
that birds actually there's their
summing bird which is a tool that
abstract / scalding and storm so mostly
what I'm going to talk about is in the
context of hit do but it's not specific
to Hadoop areli but it sort of sets the
stage
for what we have to do at scale I think
actually how many of you have actually
used to do before okay most of you so
I'm not going to spend a lot of time
talking about hit do because it's mostly
familiar but just to give you a sense is
kind of you know how you would approach
data science at scale and very
schematically Hadoop cluster looks
something like this where you have
masternodes that that own the the jobs
that you're going to run on the on the
system so for example this resource
manager is the current a dupe way of
managing a job that you submit it
figures out when it has resources to run
your job and it breaks it into tasks
that are run in the slave nodes and it
delegates to these node managers you
know the actual execution and monitoring
of your tasks locally on each machine
and then underneath Hadoop has always
been this distributed file system called
HDFS for Hadoop distributed file system
it has its own services the name note is
the master for the whole file system and
those all the metadata about files and
then it has data notes that manage all
the blocks that are on hard drives
around the cluster and you know
traditionally you would write MapReduce
jobs that you submit to the cluster
through to the resource manager today
and they would then be scheduled and run
and that's great if you can write
MapReduce code but it's not really such
an ideal scenario for a lot of people so
what does MapReduce how does it work
briefly very conceptually one of the
examples I'm going to go through but not
actually in MapReduce I'll talk about it
in the context of spark is a fairly
famous algorithm called the inverted
index and it's it's sort of what you do
is a minimal implementation for
something like a search appliance or
Google or whatever which is you know I
have all these pages on the internet or
some document repository I want to
create an index they call it reverse
index or inverted because I'm going to
find all the words in this document and
build an index of word to all of the
locations that word can be found and I
even want to know the counts because
obviously I'm probably more interested
in documents that talk in great depth
about a topic not ones that mention it
in passing so we would like to calculate
this
if you actually want to see sort of the
particulars of how this works in
MapReduce I have some bonus slides at
the end of the talk that I didn't want
to go through now but the idea is that
I'm just going to pass it through some
miracle some cluster and then it's going
to pop out the other end and what has to
happen as a precursor step is we're
going to have web crawlers that go
around the internet and find all these
documents and write an initial data set
that has as it like a two column table
if you will the location where it was
found or some ID representing that
location and then the contents and then
once again the inverted index that comes
out the other end will be indexed by
words and then a list of places where
the word is found in it you know
frequency and just to zoom in a little
bit this is what you know kind of the
data would look like if we were crawling
wikipedia it might look something like
this but I won't I don't want to belabor
the point too much and the output would
yeah I forgot about this of bill so
anyway getting onto the issue the
problem with MapReduce that we ran into
and the reason I was you know trolling
it a couple years ago was it's actually
really hard to map problems down to a
low level very coarse-grained you have
this map step that does initial
processing and a reduced step that takes
that temporary data and you know
collapses it down to your final result
that again I skipped over the details
for time you can find them at the end of
the talk if you want and this this is
one of the reasons why most data
scientist never bothered really learning
MapReduce because it was a waste of
their time to master the arcana of such
a low level system instead you know they
would let Java experts Java engineers
worry about that it really actually
became a sub specialty is knowing how to
write MapReduce code how to actually use
that API so in fact if you actually go
through the Java code it looks something
like this I know you can't read this at
six point font but it's it's like
something like I think even a way what
was it like 80 lines of code but it was
very detailed arcane code the kind of
stuff he would spend hours writing unit
tests thoroughly that was another thing
was hard to do in MapReduce not
something that you really want to do for
what is conceptual
a very simple algorithm so we want to
see a better way to do that in one ways
is well can we layer some higher level
abstractions on top of this to at least
make it easier for people to work in
conceptually in like data flows or
sequel and the first one that became
extremely popular was a sequel query
tool that would generate MapReduce jobs
for you where you're basically using
sequel as a domain-specific language for
the problem and you could write things
like this create some table load data
into it and then right you know select
queries on the data it wasn't a
transactional database what's still
around so it's not like in past tense it
was really purely for putting the
queries back in SQL and it wasn't
designed to like do record level stuff
it was designed for table scans I love
this tool so much even though I don't
consider myself a sequel person that i
co-wrote a book on it because I wanted
to make sure people knew how to use it
because it was so useful for sort of the
8020 problem of the kind of things
people did for at least basic analytics
but the problem with it was that you had
to write any extensions that you might
want you so called user-defined
functions and Java which again brought
you back to working with the engineers
and figuring how to deploy this and all
this kind of stuff you couldn't just
like embed a Java function or class
inside your hive query and then just
have it work you had to go through a lot
of rigmarole to extend hive there was
another tool that got popular called Pig
this is written at Yahoo and the name
was sort of an accident and just kind of
stuck around and the dialect is called
Pig Latin okay this is a dataflow tool
it's it's designed a little better for
the sort of scenario where I'm going to
just walk data through a series of steps
of transformation may be forking the
data and you know they have two
pipelines that are doing different
things this is actually basically the
same thing i just showed a simple
situation where i load some data and I
project out just one column so to speak
of that data pig is kind of a strange
language in terms of its not familiar if
you know sequel it doesn't kind of look
like Java and it's not turing-complete
so once again you are kind of stuck with
a situation of you know if I want to
extend it I have to write code in
another language in a link attend
although in this case you could use
Python and Ruby even and JavaScript if
anything that could be run is in a JVM
that you could actually use you weren't
just limited to Java which was nice but
my favorite in this early stage again
around the 2012 time frame was a this
scalding library that yes written by by
Twitter it's actually a scholar language
and I'll just show you a quick code
example of this that actually was
sitting on top of another nice API
called cascading which gave you more
data flow abstractions even though it
still had kind of the verbosity of pre
Java 8 joven by that I mean it really
could have used lambda sore anonymous
functions which we now have in Java but
this was you know again several years
ago and that would abstract a lot of the
hassles a lot of the low-level details
of writing MapReduce code so that you
could write very high level language
kind of stuff and this is what it looks
like in scalding I won't go through all
the details really I want you to get a
sense of how concise this code is that
I've suddenly taken a software
engineering problem and turned it into a
script a lot of people when you say the
word script they think you know that's
kind of throw away you know real
developers don't use scripting languages
you know you kind of hear that mindset
instead they write hundreds and hundreds
of lines java code or whatever but the
beauty part about this is you know i can
very quickly read this and once I know
the API it's got a few little
idiosyncrasies and know the kind of
concepts that go back to functional
programming of like mapping and
filtering and flat mapping and what all
those mean and i'll explain flat mapping
in a little bit if you don't know what
it means but basically i'm going to load
some tab-separated data i'm going to
split it into words this would be like
the contents of my Wikipedia pages and
I'm going to spit out some new tuples
and you can write tuples literally with
parentheses in Scala it'll have a word
and the ID so I've already done a flip
right there came in with an ID and then
text now it's word an ID and then the
last steps where I do group by is where
I bring all the common words together
and do final counts to get the inverted
index and the count on each document
actually i forgot i'm actually not do
the counts here when you see the spark
example it's actually a little more
complicated than this or let's yeah
complicated is a good word only because
it's actually adding the ability to
count the words occurrences per document
which again is what you really want so
this is slightly simplified but I think
it gives you the sense that knowing this
API or something like it and there's a
closure API that's like this to you can
just bang out this stuff really fast you
can experiment you can play with it
there's a local mode for running things
on your laptop and cascading this is the
word we want to get to as far as the
kind of non-sequel like data processing
that we like to do but it didn't solve
all our problems and one of them was
that MapReduce was only ever designed
for doing batch mode processing and when
you hear that word in the big data or at
least the Hadoop context it always means
I've already got the data parked
somewhere you're some massive file
system and I want to just ingest it all
or some big subset of it and do you know
a big batch of processing you know not
live not real time and so forth as
opposed to streaming where I have data
coming in and I want to analyze it as
quickly as possible and actually my
favorite example of why streaming is
important and growing important is
actually this one we just talked about
inverted index in part because that's
something you can certainly do in batch
mode and that's obviously the way that
the you know the search engines used to
do it but there is definitely a
competitive advantage if somebody makes
an edit on their web page and a web
crawler hits it you know say within 20
minutes or something you'd really like
that change to show up immediately in
your search index and not you know three
hours from now or tonight when you run
your batch job so streaming even makes
sense for things that we don't think of
so much as streams like this data set
but it also makes sense for things like
a monitoring the Twitter fire hose I'm
monitoring my log files for signs of
anomalies in the system you know that
kind of stuff so we need streaming and
the performance needs to be a lot better
you know once again it was fine at the
beginning when things were crude to
throw more hardware at the problem we
all want to say power we all want to
optimize the resources so we need better
performance and that's where spark comes
in so about two years ago TWEN
13 Cloudera the big Hadoop vendor said
you know we've kind of reached a point
where MapReduce really has gone as far
as we can take it this spark tool has
been incubating for a while first at
Berkeley and then it's an open source
Apache project we think that's kind of
the future for what the compute engine
underneath everything ought to be in
Hadoop so they embraced it and sort of
put it on a rocketship it's now maybe
the most active open source project in
the world I believe it definitely is on
github and apache but you know maybe not
for Linux or compared to Linux or
something so what does it give us well
this will see in a minute it does give
us very concise ap is that expose the
kind of abstractions we need for
composing data flows whether they're
sort of traditional sequel and there's
even a sequel dialect you can use for
this which we'll see or I want to do
something that's maybe non-trivial like
an ETL process where I'm going to do
various staging and validation and
transformation and splitting streams and
so forth the sort of things that you
wouldn't really think of doing with a
sequel query so much and it actually
supports for AP is right now the r r1 is
actually coming in a future release but
it's basically ready but python skull
and java this actually made it possible
in fact a recent change in the API
finally made it possible for the Python
code to be just as fast as the skull and
Java code so it's completely no longer
necessary for the data science guys to
throw their code over the wall just some
engineers to port it to java or scala
now of course there's always still the
usual issues about deploying something
to production you know testing it before
it gets there the usual software
engineering challenges that we all know
we have to you know address so I don't
want to minimize those kind of aspects
but it's not necessary anymore to simply
port things over to is what is perceived
as the real systems language like it
used to be and it actually has a sequel
dialect so we can actually write sequel
queries even interactively when that's
the right way to work with the data so
we do have this interactive shell it's
basically the scala shell and it gives
us it doesn't give us java but we can
write in these three languages and we
can also write sequel queries in fact
the example are shown a little bit
beb sequel queries in a larger Scala
program but there's even a variant of
the shell that just brings up a sequel
prompts you just write sequel queries
like you would with hive or any of the
database tools that you've ever used
which is really really good for I've
even worked the product managers people
that are not technical at all that know
how to write sequel queries because
they've just learned how to do this as
part of their jobs and enabling those
folks that just log into your Hadoop
cluster and write sequel queries is
pretty powerful and do it at reasonably
good speed not the slow speed that
MapReduce forced on you with hive and
because of these composable primitives
that I mentioned it's really easy to
implement a wide class of algorithms and
you get the recently good performance so
you can do even iterative algorithms
like graph traversals like training a
model machine learning and that sort of
thing it used to be that these were very
hard to do with MapReduce and you'd have
to either work around the system or
tolerate poor performance but now we can
do that a lot better some other really
interesting things they do MapReduce
never actually knew what the long
sequence of jobs were that you wrote
we're trying to accomplish that it had
no visibility into the pipeline it just
knew about the map step and reduced step
that might be say you know in any of
these sort of coarse-grained steps here
spark actually builds up a lazy directed
acyclic graph of the computation and he
can do things like you know squash some
of them together in the same jvm process
so you can conceptually right things
separately but he'd get the optimization
of having things rolled together this
also benefits the data recovery so in
fact if you lose like a partition of
data spark can go back to wherever the
last checkpoint or data source was and
reconstruct that partition because it
understands the lineage of all the data
because it has this directed acyclic
graph in memory and I actually mentioned
this already there's a new data frame
API that's inspired by Python and our
data frame concepts that AB strays a--'s
the abstraction level even a little
farther than the core of spark but also
enables query optimizations and the
ability to use
any of these four languages I mentioned
including actually five including sequel
and still get the same performance which
i think is a huge step forward and
because of its ability to be fairly
efficient spark can actually support a
streaming model which is basically a
mini batch model so spark actually
evolved also as a batch mode system like
mapreduce you know he started it was
about 2008-2009 but because of its
relative efficiency they realized that
they could actually support sort of the
8020 solution to what most people mean
when they really mean streaming which is
I just need to process the data fairly
quickly it doesn't have to be perfect or
you know instantaneous like milliseconds
like my inverted index example where it
yeah I don't have to respond to a user
immediately you know when the index is
updated but sure it'd be great if I
could do it within a minute or whatever
so they invented this mini batch process
called I actually have a slight of the
next slide will show it a little bit to
other important benefits though one is
that because it's a mini batch you
actually can just repurpose the same
code you've written for your batch jobs
inside a stream processing system and if
you ever use the lambda architecture for
example this really solves a huge
problem that is typical the lambda
architecture which is in other tools you
often find yourself re implementing the
same logic twice once in your streaming
tool like storm let's say and watch in
your badge to like mapreduce so now you
don't have to do that even you can just
write one your core code base and then
you'll repurpose it for those two
different scenarios and in addition to
reusing batch code you get the ability
to do window functions so it
conceptually it looks something like
this each of these blue boxes is a
resilient distributed data set that's
the distributed collection that's behind
the scenes always holding your data it's
resilient in the sense that if a
partition is lost like I said it can go
back and reconstruct it it's distributed
and that the data will be partitioned
across your cluster so that you get the
benefits of parallelism and in the case
of a streaming process they use
something called a discretized stream or
D stream where each time window and
is something you specify you know down
to about a second is the minimum
resolution it's going to put that batch
of data into an RDD and let you process
on it or even do things over windows of
them so if you're like computing moving
averages or you know what are the most
frequent tweets or whatever you can do
it this way okay now i am going to show
some Scala code even though this is data
science talk and I'm going to use this
my you know source for the permission to
do this Vitaly Gordon this actually is a
link to a talk he gave at scala days but
it was convenient that he talked before
he elfin advocates using scala and
especially in this sort of pre spark 13
mode where you didn't have the ability
to run Python at the same performance as
Java code so i'll just use scallop and
mostly because that's the only thing I
know how to do anymore so anyway let's
actually look at the inverted index and
you know I assume that you don't know
Scala how many of you've actually used
Scala just out of cure all right more
than I thought the purpose isn't really
did teach you skull of course but to
give you a sense of what it's like to
create a data flow of processing data in
a language that or toolkit that gives
you the right functional abstractions
namely immutable data operators I string
together to get from A to B with you
making all the little micro steps that I
need to make the Python API is almost
identical it's amazing how similar
Python code looks to this in fact I'll
show an example of that in a second the
Java code isn't quite as concise but now
because of Java 8 you can do things like
embed anonymous functions which is very
important for code concision so anyway
you start always with importing stuff
whether it's java or scala the same idea
and then scholar let you declare
singleton objects and that's what you
would put your main routines in so
that's what i'm doing here to calculate
the inverted index obviously that's my
entry point when this thing runs you
always start a spark program by creating
a spark context that's your entry point
it the first argument actually is the
cluster I'm talking to in this case I'm
just talking to my laptop so it's just
local mode and there's that there's very
various ways you can construct these
things but now let's actually start
processing this data so the first thing
I'm going to do is assume that my kraal
data is just in text files so I'm going
to load that and again actually this
could be a directory of you know
terabytes of data but I but it will
figure out how many tasks to spawn for
me and the first thing I want to do and
it's we're a little cut off at the tub
but I think you can still see it I'm
going to map over each line and so this
is an anonymous function i'm passing to
the map method and you can use curly
braces instead of parenthesis for
function arguments because it gives you
this a nice block structure and also
actually while I think of it notice
their dots at the beginning of these red
things the red things are always methods
because that does the real work this is
actually one giant expression where I'm
just building up a pipeline you could
certainly have intermediate variables
for each of these stages but when I'm
just doing it all at once you know I
don't like to declare variables where I
have to come up with a name anyway so
I'm going to split the line on tabs and
then return a tuple containing the first
element in the second element I'm
actually splitting into two elements
that's what the two is four because an
obviously text could have tabs in it I
don't want to treat those as fields so
this is now I have a couple of my
document ID and the contents of my
document you one record per line flat
map is basically like map but instead of
being one to one its 02 many outputs to
hear what I'm doing is taking each line
and splitting the text into words and
then outputting another tupple which
will be the word which I've now put in a
key position by default it uses the
first field is the key and then the path
because I need to keep track of where
this word was found and it's i'm using
flat map because obviously i'll get many
of these per line and flat map will just
automatically compress that nested
collection of words per document into
one giant a collection of word path
pairs that's of course what we want this
is my favorite little bit of code this
is kind of a classic spark idiom for
counting I'm taking those tuples and I'm
going to you still use the word and path
tupple as a key so it will be nested
inside a larger tupple that has a seed
count of one for each tuple and then the
next step we reduced by key is
is an optimization and spark where it'll
just it's basically grouped by over all
of those word path keys and then it just
sums up the counts by whatever function
I pass it so I'm just going to grab the
two counts n1 and n2 sum them up and
I'll end up with data like in that gray
box where now I'll have unique word path
pairs and then some count that's one or
more for each word all this thing does
is it rearranges the print you know what
I love about this line this this would
be like nasty job it'd be like you know
five or six or ten lines of Java code
all I want to do is get to back to using
the word is the key so I just shifted
the parenthesis of these tuples in the
nested tuples so that now I have word is
the key and then path and count as the
value and that's why I just love about
this the reason the word case is here
this is actually a pattern match idiom
and Scala it lets me reach in and tear
the tupple apart just without having to
you know like call get element get
element kind of stuff so I'm pattern
matching on that nested word paths
couple the original key and count and
then creating a new temple with the word
is the key and then path and an this the
count as the value of this of the temple
now we do a traditional group x over the
key and then this last bit all it's
really doing is formatting the output
and it on the fly sorting by count
descending so that's what we're it's his
sort by and then I have another pattern
match all I'm really doing is saying all
right when you're doing this sorting
treat the count is negative so it will
actually be descending and do a
secondary sword on the path actually not
necessary it but it's useful for unit
testing and then i'll just make a string
out of those and then save all of this
to a text file and then stop when we're
done everything except that stop at the
bottom and the initial input through
text files i could take that code put it
in a library and do it in batch or
streaming on the fly if I wanted to so I
get that reuse so anyway you know
whether you understood all those details
I hope you got the sense of how the AP
and AP I that exposes the right
abstractions in this case you know
design
for data really lets his crank this out
quickly I wrote this in 30 minutes the
first time I wrote this thing now it's
evolved a lot since but it was because I
understood those abstractions and I
wasn't thinking about really really low
level gritty details that MapReduce
would force on me and I was able to
crank this out quickly and this is a 12
point font I think so you can actually
almost read the whole thing maybe those
of you in the front can okay well once
again amazing Combinator's that's sort
of a functional programming term i'm one
of those uppity guys on the top hat that
anyway operators is a better word really
but that's why i use the red even though
it sort of fades out on this projector
because those are doing the real work
those are the heavy lifters and I just
tell them you know okay when you reduce
by keyway reducing by when you're
mapping all right how do i want you to
map these tuples and so forth so let's
look at the slightly broader aspect of
the spark ecosystem as a sub ecosystem
of Hadoop or you can actually run spark
outside of Hadoop on may so sore even as
a standalone cluster shameless plug
typesafe is offering commercial support
for those latter two options so talk to
me afterwards if you want to give me
your money alright we can write sequel
queries so let's look at that first of
all very important for a lot of people
in amateur Hadoop ecosystem is they've
already got a lot of stuff in hive
they've been using hive tables forever
spark and interoperate with hive so you
can get faster queries or you can so my
favorite example of this actually is you
spark for that etl cleanup you know
munching process and then just write
hive tables and then your data analysts
you know as soon as that process is
finished they can go right there queries
to see you know what's the latest tweets
or whatever it has an internal query
optimizer called catalyst and so
basically the last two lines you have
two options you can either write real
sequel or you can use this new data
frame API which is sort of like a you
know a like a builder pattern for
expressing the queries in a more type
safe way as opposed to embedded strings
which actually is the preferred way
today so let's actually see an example
of how you might use this with high
and if you wanted to use sparks own
sequel dialect the code would be almost
the same but it's not quite as mature or
as feature complete as the high version
so this is still really popular so
there's some stuff that got cut off and
just an import statement it doesn't
really matter create a spark context as
before wrap that in a hive context or
use the sequel context if I don't if I'm
not talking to hive and I can basically
write the same kind of queries we saw
earlier I can create a table I just I
have this sequel method and I have to
pass strings so the downside is it's not
type checked until runtime but that's
okay if this is actually an interactive
show where I'll just tell I can tell
right away if I messed it up and redo it
I can load data this is classic high for
just loading data that I have you know
stored in my like local file system into
HDFS and then I can start writing
queries this last show method is just a
data frame method that's convenient for
seeing like the first 10 records and to
see what you've got so whenever when I
did this what do they do like is it off
the screen actually when I do these
queries they return something called a
data frame which has this higher level
abstraction for sequel awareness schema
awareness of the data and then I can
just I can actually use this in the
regular spark API if I want to take the
like the results of this query and do
some munching on it and create a new
hive table or whatever it's just all
like all integrated right there and if
you don't like the nested query strings
well there's this alternative that we'll
see in just a second so it turns out
actually when you do this either in
Python or source kala there's most of
your code looks it's going to look
really similar all you'll do in this
particular example I showed you which is
even a trivial change as you replace to
statement or one statement the import is
replaced with a from PI sparks equal
import hype context and then you just
delete the Val keywords from the spark
of the code I just showed you actually
were there any I guess there were at the
beginning for the spark context and then
the code is actually Python code so
that's really nice you know in a way
that's great too because it means that
if you know python and somebody gives
you a scala example to look OH
where you can probably figure out what
it's doing pretty easily so let's look
at the alternative which is the data
frame API and I'll basically do kind of
the same code this will be more type
safe because it will be doing checking
at compile time and that is an advantage
the disadvantage for anybody who already
know sequel is they have to kind of
learn this dsl as opposed to just
writing with the language they already
know but it starts off the same you know
we create this spark context and this
time I'm going to use the sequel context
and not deal with hive at all I'm going
to load some data and by default it uses
part k which is sort of the hotness in
terms of data formats in Hadoop file
system it's a column-oriented storage it
actually compresses columns does some
other nice things that are pretty handy
this is a something Twitter helped
developed actually but you can load
Jason and it can parse JSON on the fly
for you and infer the schema you can
certainly use some other file formats
that are popular including text even
though Martin and Todd convinced us
never to use text again for the rest of
our natural lives so maybe I want to
actually the WC this is supposed to be
like a word count data or just think of
it as inverted index and maybe I want to
see the counts descending I want to see
the most frequently occurring words I
can just do data frame order by and then
this funky little syntax with a dollar
sign that's a that's one of several ways
you can express I want you to find the
count column in this data and sorted
descending so it's it's a bit idiomatic
scala that they had to use for this
particular API but you get used to it
really fast and then I can see the first
ten very easily just to make sure it
looks sensible and then cash is an
optimization spark exposes to you to say
I'm going to keep going over the state
over and over so cash it in memory if
you can so that you don't just recompute
the whole graph every time otherwise it
would actually go back to the and reload
the data every time which is going to be
inefficient I could do things like you
show me all the words that are greater
that are longer than 20 characters again
using that idiomatic syntax this time
I'm passing a predicate is an argument
to filter and then I could save the
results back to park a if I want
actually I put in the extension just to
show that it's parquet but it doesn't
matter it's going to write by default
parque and read part k i just use that
as a naming convention here so very
simple actually i guess it's not quite
like the previous example but if you
prefer working in an API that's type
checked at compile time and and also
gives you really good performance it's
it's an excellent tool so what about
machine learning and graph processing
let's finish up with those and how am i
doing for time pretty well ok so there's
a ml live which is part of spark it's a
rapidly growing library of standard
machine learning algorithms and it's
built on this core infrastructure that
gives us really good performance for the
most part by the way I just thought of
something if you want to post questions
post them on the app and also remember
to review the app I meant to say that
when I started let's look at a kind of
an interesting example so i mentioned
streaming is really hot well a lot of
things that we do a machine-learning
we've been doing in batch mode but
there's some things we'd really like to
be doing in real time and i use the word
real time with air quotes of course and
one of them is finding clusters and data
as it's coming in the other one they
support right now is logistic and linear
regression so you can like train models
on the fly and do predictions on the fly
the example i'm going to show you is
actually from the spark distribution
itself so you can just download spark
and you'll find this code in there
there's a bunch of messy imports but
mostly what i wanted to show you is i'm
importing an example and I've aletta the
org dot Apache on the front here that's
not legal scholar but I just do so it
would fit there's some built-in linear
algebra stuff for like representing
vectors of features you know like a
classic example would be like you know
Sam rating like housing prices I might
have a vector for the price number of
rooms those kind of features and what
else here then labeled point if I've
data that's already pre labeled as being
part of a certain cluster or grouping
then I'm going to use that to test how
accurate this is so then we so I've done
these imports now i'm going to import a
streaming context so we'll make
both machine learning and streaming this
also wraps I think I've gotta build here
let's get through that so there we go so
I once again create a spark contacts
create a streaming context and hear the
second argument is important it tells me
how many times are you know what's the
length in some time window that I want
to capture data and process it as a mini
batch for various technical reasons that
we won't get into this kind of minimum
is really about a second you don't want
to go in you can't really go below that
and have reliable performance you can go
up to really long times if you want like
if you have very slow streaming and data
you know you can have several minutes if
you want the only risk there of course
is that data is going to be sitting in
memory and could you know get lost there
are some ways that they're adding
resilience so you don't get data loss on
the event that the job happens to crash
why you've got data that's been sitting
there ready for the next iteration we're
going to load two data sets one of which
will be the training data that's already
labeled no it's not labeled we want it
we want to use those for predicting but
then we're gonna have some tests table
that test data that is labeled it will
also load and what we're going to do
when we actually start processing is as
the data comes in on the training stream
we're going to your train or look for
clusters you know with k-means and then
we're going to take that data and see
how well we did at predicting the
location of these training sets so the
training data already knows where it
should be we're going to see how close
we got to it so txt file stream means
we're going to just listen on a socket
for data to come in yes that's correct
there's also like directories you could
watch directories and so forth we'll
treat each record will be like a single
line of text and then this is a classic
idiom of just mapping over that data and
passing it to some parse method that
knows how to tokenize that line of text
and turn it into an object like a vector
in this case or a labeled point it's
also a vector but else it has a label
attached to it and I love this because
it's so concise to actually use
streaming k-means we just instantiate
one and we set some property
now all these things in yellow would
normally be like command-line options so
that you can vary them but just for
simplicity I treated them as constants
so we're going to find like say five
clusters in the data all the time the
decay factor says should I remember
previous data and mix that in or should
I forget it and basically it's a
weighting in this case one actually
means forget all previous data treat
each new batch as brand new data and
find its clusters zero would mean use
all data in the past which is kind of
dangerous because obviously you're going
to be accumulating data and then it we
need to initialize centers and we're
going to assume that the end features is
actually the size of our vectors so we
let's say we know that we've got 20
features in our vectors that we're going
to do predictions with this is a
function in Scala for just transforming
a labeled point into at uppal that has
the label as the first element and the
rest of the features in a vector is the
second element I need that for actually
well one of the next functions here so
this is the way you could declare an
anonymous function or actually a named
function in Scala would look similar in
like Python and Java now as well now
everything I've done up to this point in
fact even at this point nothing's
actually happening I'm building up a
pipeline and it's not until the next
line on this when I say start that will
actually run so as i said i'm going to
have these two streams coming in i'm
going to train on the training data on
each batch iteration and then predict
from that data using what came out of
the test data with my f function that
does it can extract the fields that i
need and Prentiss just really it's a
debug statement that they have in the
API it'll show you like the first 20
results of this returned rdd a resilient
distribute data set so normally you
wouldn't see that in production you'd be
writing this out to a file or a database
or something and then finally we start
it and then wait for it to just
terminate which could be forever
hopefully okay the last one I want to
talk about and then we'll wrap up is
graph processing so this is the idea and
there's a API called graphics well
there's a whole bunch of these in the
real world
Vitaly talked about some actually at
LinkedIn your social network your
Twitter followers epidemics are an
interesting case the interwebs they form
a graph I did not misspell teh actually
but it used to be that distributed graph
processing was actually sort of a
research problem and we didn't know how
to do it well so most people just sort
of hand coded specialized versions of
whatever algorithm they were trying to
compute using MapReduce or whatever but
now we actually have the ability to
build more general purpose graph engines
that can be distributed in this case
they're built on top of spark quickly
I'll go through the code a lot of it is
just set up but I wanted to I wanted to
give you a complete example so that you
know the first thing is a bunch of
imports again and I will this will not
even say any more about that I want to
do 20 partitions again of something I
would actually make a config option
there's several ways I could partition
the data actually in this case we're
going to actually duplicate some of the
vertices whenever an edge is going to
cross a partition I think I got this
right actually we want vertices to be
clustered together more than edges so
we'll actually be duplicating vertices
instead of just splitting edges this is
this particular model and these two next
two values too fast these next two ones
are just like try to save everything in
memories or processing and then the
tolerance we're actually going to do
page rank is I didn't really say that
yet we're going to page ranks when we
iterate on computing the weights of our
nodes in our you know social graph or
whatever we'll wait until we get a
tolerance down to point zero zero one
and that'll be the end of the iteration
so you'll just run to completion with
this tolerance and then somewhere we're
going to get the data that'll be our
input okay so this is the real meat of
it once again spark context create that
use a static method edge list file where
we give it the spark context our input
and then those other values we just set
that tells it now how many partitions
and so forth and then cash that in
memory is the last step here that's
actually the first step here will be to
give us unpartitioned graph data once
again i'm setting up a pipeline I
haven't done anything yet
this fold left will be used to take that
unpartitioned data and partition it by
the partitioner that I told it to use
somewhere actually where did I tell
anyway it's somewhere in here maybe I
left it out and then the last thing
we'll do is actually print out how many
nodes and edges we have in this graph
and then finally we can just run page
rank with one line of code cache the
vertices because we're going to keep
going over those this last step in just
computes the total rank it sums up all
of the ranks of the individual nodes
just a little bit of Scala isms here for
that and then finally we can just save
this as tab delimited data back to txt
files and then stop so once again
actually most of this is setup and
teardown that there's just like maybe 10
lines of code they do all the real work
and the rest of this is like
configuration and input and output I
think that's my last example it is i
want to mention a couple of other things
that you should watch if you're a data
scientist one of them are going to get a
talk on i think it's the next one
actually are no is going to talk about
h2o which is a really amazing in-memory
compute engine where a spark can handle
not only in memory but also data that
has to be flushed to disk how many of
you looked at the julia language by
chance so Julia as sort of an aspires to
replace our it's it's a relatively new
but it's rapidly evolving it's a really
elegant language you might want to check
out so we have maybe five minutes for
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>