<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • Designing for Performance • Martin Thompson | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • Designing for Performance • Martin Thompson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • Designing for Performance • Martin Thompson</b></h2><h5 class="post__date">2015-07-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fDGWWpHlzvw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so welcome off to what's gonna be
the last track so I get the graveyard
slot hopefully you're a little bit more
awake than I am because I'm desperately
jet-lagged and full of coffee so that's
hopefully I can keep it together so what
I want to talk about today is about
performance and the most important thing
my performance is you comment from a
scientific perspective and this guy sums
up so much for me for how we should be
thinking about things he famously was
involved with many different things but
particularly the Space Shuttle and the
whole thing of defining the problem
being the o-ring and explaining that he
was part of what was known as the Rogers
Commission and these were two
interesting facts about Fineman involved
in this stated by Rogers so Rogers
headed the Commission and I started off
by saying Fineman is becoming a real pin
it cap sticking his nose and everything
he kept asking questions it kept
challenging everything by the end of it
Rogers had a different view firm he
described him as someone who's got the
greatest scientific integrity of anyone
I ever met and a lot of what that's
about is how he looks at problems is it
must be an experiment that can be proven
he's got a lot of nice things about it
so let's wind back a little bit about
fine man and he got a Nobel Prize he got
it for his work on QED and whenever he
proposed it originally there's like
three proposals are on the same time all
the slightly different means no one
realized there would the same thing for
a while most people didn't even was down
with her about and Freeman Dyson worked
it out that he actually helped them
explain what it was but I loved his
description offered like the quantum
electrodynamics it's so and so
interesting because it explains
everything that you kind of want to know
in this piece but the bit I really like
is it's the most accurate theory ever
tested by experiments on earth and this
kind of sums up on how we should think
about performance we want to have
experiments to prove it so let's take a
point from the man himself is it does
not matter how intelligent you are if
you're the gas that gas cannot be backed
up by experimental evidence then it's
just the gas and it's kind of nice as
some people are starting
to do this like been give a great talk
yesterday where he explained how they
went through and worked out what they
should be doing with the vent loops but
they did a beast aplomb experimental
evidence and this is really important if
you only get performance right you've
got to think this way so what I want to
do is talk about how we design for
performance and I'm gonna cover four
topics so on the cover what is
performance the sort of set some ground
for it I'm going to talk about how we
sort of do things in the clean room
representative modern which is very key
to this talk about implementing models
and how we actually do performance
testing itself so kind of what is
performance it's a very nebulous concept
we talk about what a system to be fast
we want it to be performant but that
doesn't really mean anything what we
really start caring about is a number of
things one is throughput and it's like
how many units of something over a given
time period can we get through well also
hear the term bandwidth word bandwidth
tends to be maximum throughput for
something that you're doing we also will
talk about latency and we'll talk about
response time these are two different
things
and again in our industry you keep
mixing these up we get it all quite
confused but it's like how quickly does
something responds or how long are we
weirding if we want to think about it
can I also just want to make a quick
point on scalability which is something
else
so by scalability what we mean is if we
add more resource we should get a
proportional increase in throughput that
is a scalable system if you just adding
more resource and you do not get a
proportional increase in throughput you
do not have a very scalable system
you've got a problem somewhere and ansar
a great example of this to have great
scalability does not mean you have great
performance in either throughput or the
latency cents per unit and the great
example of this is ants are actually
very dumb so most of the ants have the
job of going out finding a leaf taking a
piece of the leaf and bringing it back
now whenever ants go pick up leaves they
grab with one or three little hands a
bit of the leaf and with the other one
they cut 50% of the time they grabbed
the wrong side and the leaf falls on the
ground so they're not really striving
for efficiency but what's really neat
about Gardens where
evolution has done something very clever
that is not the dominant cost the
dominant cost is taking our thing back
to the mother lode where it's all going
to back so where we need to bring it all
back to that trip back is the more
important part and we do this a lot in
technology is we focus on these little
things and miss the big important thing
and that's the gap back so you don't
need to be intelligent we just need to
do it right
now it comes to performance I think this
sums up nearly the entire picture for me
this is my life a lot I end up flying
and I end up going to border control and
with this you can explain everything
that's actually relevant about
performance so we're here but latency
we're here but response time different
things like ah so what is the difference
between these we just interchange them
but there is actually a distinct
difference if you look at queuing theory
so if I join one of these queues I'm
going to be doing a number of things and
each of those things you can imagine
them in different times so how long I'm
waiting in the queue is my latency that
is how long I am latent to do something
when I get to the front of the queue and
I go to the desk and now being serviced
that is the service time of a system
those are distinctly different
now what's response time its latency
plus service time and if you don't start
modeling these things right I'm starting
doing some real math on it we have to
break these down and understand what
we're dealing with and how we deal with
it and so then what is parallelism well
I just have more Q's to go to here it's
out see the perfect model for this you
also talk about things like
work-stealing well what if one of these
queues is too long and another one's too
short you'll easily have someone come
along and tell you to move from this
queue to another queue there are all
these sorts of things we do this in
reality but quite often it doesn't
really manifest itself properly in
software you're just really really
simple like before I had this little
chat who thought response time and
latency was the same thing so care to
admit a lot of people probably do in
fact actually most people use latency to
describe response time latency look at
the word look up in the dictionary is
about being late
you're weeding response time it has to
include the service time as well and
once we understand these you can
actually design systems better and you
can optimize and profile and do all of
the right sorts of things so in
particular let's look at queueing theory
for a second so whenever we start using
the system the more we use it we notice
that our response time starts to slow
down well why does it slow down because
actually the service time is the same
but you start joining queues and the
thing is if you go to something and it's
being used more often so the utilization
is going up the probability of someone
being in the queue is greater so if
you're only using something not like 10%
of his utilization the chances are
there's no one in the queue so you get a
good response time because you're not
livened so the secret to having very
responsive systems is keep the
utilization down don't have them running
in maximum utilization and we can see
here on this graph that once you go
avoid sort of 60 to 70 percent
utilization things start getting pretty
bad this also works in reality for
projects and people and everything like
that if you run your teams at over 60
percent utilization if you need them to
respond quickly to need guess what
they're not going to be able to it's
math you cannot run away from it this is
just probability theory it's a hundred
year old math and it's well described
and so if you start breaking these
things down we can get that so if you're
designing say a financial trading
systems you want to have it so
over-provisioned that whenever some work
comes along you want to have utilization
so low that you're pretty much just
gonna get hit with service time if you
end up having your system working quite
busily at that point in time and you get
a burst of traffic coming in utilization
is quite high you're the queue
I knew response time is gonna go so
you're gonna be aware of these and we'll
look at how we measure them and deal
with them later on so that's kind of one
of the pro tips is always ensure you
have sufficient capacity and by having
sufficient capacity you will keep your
utilization low and you can respond
quickly so if you want to service is
gonna respond quickly utilization needs
to be kept low
so can we go parallel speed-up let's we
looked at that's a single serviced in a
single thing we go back to those rows so
we can have multiple rows and multiple
queues going in well it's great whenever
those rows are all completely
independent but in reality quite often
they're not there's usually some
component of an algorithm that
sequential unassured between sort of
multiple threads going and so let's say
if we look at our process and we can fit
up into NB if I can break up air I can
get a speed-up so I'm running on set for
core machine I can speed it up by doing
air and parallel but let's say I can't
break up B I'm not limited in what I can
do it's it's the other way around I can
split up b but i can't split up at i can
only get a certain speed up this is um
dolls law it's been around for a long
while it's not actually i'm dog didn't
coin the law he talked about um dolls
argument and he's trying to convince us
all not to go and buy mid-range machines
he will need to buy big mian firms that
are very fast processors because he
didn't want to do in parallel computing
he wanted to peddle his mainframes they
need a good point that actually parlour
programming is difficult so here's your
typical to this speed up curves like if
50 percent can be made parallel you can
only give it to XP dub you will not get
more than 2x speed-up like even let's go
up to 95 percent of your algorithm can
go in parallel you will not get more
than a 20x speed up it's math it's the
law you will not get past this and so
you've got to make sure that you do not
have contention in your algorithms but
it actually gets worse that's the
perfect kiss world that builds a
contention it doesn't deal with
coherence so when we have that
contention we have to make the results
of that contention coherent to the
different rods and we get into slightly
bigger formula at this stage Universal
scalability law I recommend you look
into it but we can now model what's
going on in a system based upon
contention and coherence I start working
out what is the response time and what
is this speedups we can get we can start
plugging this in and if I plug in Co
here
RINs factor of 250 microseconds and
contingent factor of 5% these are the
curves I get as I throw CPUs a problem
not if you sort of look at this graph if
you look over here at the sort of 248
CPUs you can see it over here it all
kind of stares and step and that's the
world that we've been in for a while for
the majority of people but we're now
moving into the sixteen thirty-two
whatever core is going up and these are
sort of problems you also see this if
you sort of running something on Amazon
AWS and you run with lots and lots of
machines where actually the coherence
costs gets a lot higher you can't split
up certain problems and where's it just
won't scale and after a while the cost
of the coherence starts to dominate to
the point we start to slow down again so
going parallel is not the answer if
you've got any coherence problems at all
in fact you need a problem that's
embarrassing parallel and by
embarrassing in parallel means there
isn't a contention point then you can go
and scale up quite well and it's hard to
do this so that's just something to be
aware often watch out for it's why I'm a
big fan of not ever having multiple
riders to the same thing if you got
multiple riders you've got contention
you've got coherence as a problem if you
have a single rider you avoid most of
that and you can actually perform and
scale much better and it's a lot easier
to understand and reason apart so
there's good reasons for doing that so I
also believe in the things need to be
cleaned and representative so we have
here the clean code often it's clean
movement and stuff I like to look at the
meanings of words quite often so they
had a bit of a bash on latency but let's
see look at clean I love this
description from the Oxford English
Dictionary and it's like morally
uncontaminated most code when you look
at it is far from uncontaminated it's
actually quite disgusting and so let the
have clean code we've got to get back to
the purity of what it should be doing
it's also important that it's
representative and what representative
really means is that it's it's a true
portrayal or something it's the place
where we capture our understanding
I think documentation is great it's a
great way of thinking about
Don you should do it but code is
actually the place where you should
capture your best understanding and
don't ever feel bad about it like what
do you put in code today is your current
understanding it's imperfect you learn
more things tomorrow you should update
your code to reflect what you've learned
if you don't you end up with the whole
creep between your understanding and the
place in the code so if you just
constantly keeping this up so it should
be the place where you capture your
current model and move forward and that
model is not just the model of the
business it's the model of how the
system works because then it's a true
reflection and a true portrayal and when
you've got that direct mapping you find
it you get really great performance if
you have an impedance mismatch between
what your model is and what's actually
going on you end up with a performance
problem and that's what I can't call the
mechanical sympathy you don't need to
have a perfect understanding you've got
to know what is enough who's
representative to what's going on
underneath and that kind of takes us
into abstraction so we're abstracted
what's going on underneath and we use
this term a lot I keep hearing people
say abstraction particularly like they
will create an interface or a type for
something and that type is generally
used to represent a number of things
that are like something we use it kind
of generically we tend to abstract too
quickly well here's my rules of
abstraction rule number one do not
abstract rule number two don't do it new
number three's start considering it when
you've seen at least three things that
are the same not sort of kind of maybe
are the same thing then used to start
doing that abstractions have a cost they
have al Arif interaction they have lots
of things that imposes upon your code
and on your thinking so they've got a
pair for themselves and I'm saying don't
do them totally I sort of start off by
joking if I don't do them I mean don't
jump into them and actually go to the
other extreme is dry is very dangerous I
keep seeing this all the time well
people will start drying out code really
quickly don't repeat yourself they'll
look at a problem think yeah I think
this is kind of a seam on the create
abstraction before they even create any
of the
verse into the code just build two three
whatever it is completely independent
things that you need to do then look at
them afterwards and see if there's any
commonality I keep seeing it on that
although we're run the other way around
a big flaw because you've committed
yourself you created your little baby or
in some cases your monster and you want
to keep it alive and keep it go in there
and then this stays in the code even
though you know better and I but you
because you were invested in it you
don't get rid of it it's so much better
to go the other way around even if you
have to completely copy and paste
rewrite something make it work exactly
how it needs to be then factor out
what's common that's the best way to use
abstraction on this and then it doesn't
cost anywhere isn't as much especially
to yourself and understanding and
dealing with your code because one of
the things is once you start abstracting
let's say we're in the object-oriented
world we create a type and then we deal
with our type polymorphically once
you've got one version of that that's
kind of great that's mega morphic all
wrong times can do a lot to optimize
that if you get two versions you get
biomorphic and we can still do some
things there as well we can put branches
in and just go one of two wires
typically once you get three or more you
go mag amorphic and we start going into
things like junk tables will go into
linear scans of interfaces find out
which method I'm going to actually
execute people even realize this you put
an interface on a class you've got
multiple implementations C even got
multiple interfaces on a class when you
go to call that method at runtime quite
often you can end up with linear
searches through interfaces because
you're getting carried away with these
layers of abstraction if you need them
grit but don't put them in just for the
hell of it
they're got to be worth it and it's good
sometimes if you've just got one or two
types be careful when you start getting
many types of stuff because processors
are not good with branches the grid with
predictable branches not good with
branches you cannot predict and quite
often you start doing that it's not
representative and that's usually a big
smell
i Todd mentioned it earlier in the
keynote today we got to sort of trust
our senses trust our taste and you know
when stuff starting to smell we've got
to start listening to that and stop
doing it so
kind of right a classic for this is kind
of big frameworks and hey you say well
is it a framework is it a library
I've got a really simple distinction
where once something's a framework when
something is a library if it in facts
your code and forces you to work in a
certain way it's a framework you
shouldn't be doing that
if it's the library you can use it you
use it in a nice little place and it
doesn't in fact direct your code that's
good that's composable that's what we
want we want stuff to be we reuse it
because it's great it helps us out and
it's there's isolated if it forces you
to work in a certain way start to worry
you can't later optimize that you can't
let our change a code and work with the
court well so what should and these are
the things that get you there you end up
with this sort of mentality especially
we get carried away with frameworks and
it's like the people here ever go
backpacking season backpackers so they
know how to travel light people who sort
of think oh yeah I have all this stuff
this stuff is gonna be great for me
whenever I go our code bases get like
that too quickly I too often see people
turn up on a project and before they
even work out with the problem they're
solving they're installing spraying
Hibernia this that the other that's like
what is all this crap puns like why are
you doing this oh we need this before we
start coding really start coding then
see what you need and everything you
introduced you introduced it because it
saves you time and it pays for itself if
it doesn't do not use it travel light no
because your code base is a liability
the more you have of it the less you can
react the harder just optimized harder
is to get it all right so keep the stuff
light you'll be going a lot faster so
deliberative abstract but only when
you're sheriff the benefits
and one of the things that really stands
out for me is like the fact that we have
a law of leaky abstractions tilts
politics law it's because we've got it
wrong we're abstracting the wrong things
and I talked about the non-trivial in
fact abstractions should not be big
abstractions should be small if we look
to something like mathematics
abstractions used all the time and she's
brilliantly but it's small its
composable it's not big in imposing
these are that's usually a good sign of
the smell and I said Dykstra pointed
this out really well is if we use it
we're using it to be more precise we
want to deal with something as a
concrete thing that we understand so I
said how can we abstract a memory system
so it's very complex modern memory
systems but actually if you understand
the abstractions of it it actually works
really well so it's getting this stuff
right so memory systems are by three
bets it's that simple our Hardware
friends are working with three bats and
the first bat is a temporal that if I
use something I'm likely to use it again
soon so things are closing time in how
do you so they're taking that bad on the
time they also take another battle in
space so things that are close together
tend to be used together and this is
where things like cache lines OSP ages
or TL B's all of this sort of stuff so
what you wanna do is keep your stuff
together that you use together that's
the way you can play it it is bad and
the third bet and this is it only three
in total there's a striding or potter no
basement bat so if you go through code
in a predictable way you go through our
data in a predictable way it can be
prefect for you and it hides that
latency response time you have to
dealing with something so this is what
matters with the abstractions you got to
understand what it is what is doing
nothing of detail
I see people get obsessed with what size
is the cache well shouldn't really care
about it you got occurred that there is
a cache and you're using it in the right
store when those are some things that
matter so sort of start thinking and not
sort of where - like what is the
hardware given me what are the sort of
design principles the people who used
who built that hardware we're working to
and make sure you're sort of playing
well to that and then you're doing the
right sort of things
so let's move on from this night if
we're gonna build models we need to
build them in the right sort of where to
get our performance and quite often I
see some fundamental issues so coupling
and cohesion is kind of classic there's
lots of other things of separation
concerns but these two in particular are
a massive impact on performance and it's
quite a soft and subtle thing so let's
take a really simple example that's
actually riddled with implications so
developing a cue a really simple
concurrent Q and not Q is going to be a
rare back so we've got a buffer in it
which is an object or rare so I can put
references into my Q and what I could do
is I could use buffer lint as the thing
to represent the capacity of the queue
from a modeling perspective
don't make me cringe do use buffered
lengths so I will typically put capacity
into the class itself to say what I'm
doing what actually ends up happening
with this subtle little change rather
than just hacking errors one is I'm
being more expressive and being more
representative about what I want but if
I want to work out the length of the
queue I've got there with the best
pointer I can get to it by dead
reckoning whenever I use this so that's
the the bet on spatial it's probably in
the same cache line it's gonna be really
fast if I go to get buffer length
I've got to follow a reference off to
another object and read the lens field
that's a data dependent load and your
processor counts Bakula to get there so
it starts costing you
I also gets a bit more subtle than that
so if I'm using the cue cues are
typically I was full or empty that don't
hint exist in any other states and so if
you get a burst of traffic in guess what
happens it just fills right up and if
you wrap her in a circular cue the head
and he'll come right next to each other
they end up in the same cache line quite
often slowing each other down through a
thing called false sharing all right
let's say for example I decided to make
capacity whatever the size of buffer dot
length is minus 32
I've reduced the capacity of my queue
all of a sudden I'll endure burst
scenarios if I'm gonna be filling the
queue I'm not gonna be impacting the
consumer taking out of the queue slowing
them down because the producer is not
causing false sharing a fax and cache
coherence traffic all because I'm just
able to control it with a simple little
variable and I'm modeling what's going
on so just pulling these things out
making them important I think you've a
bit further than that is stop thinking
of your classes it's just bags of
properties they have to be a thing and a
half of a purpose a very clear very
precise purpose and make it expressive
and once you start doing that you get
someone's better behavior I mentioned
this morning in the keynote hi I could
often going in to see different clients
and I'm going to see the clients and I
get to know the codebase by just
starting to refactor it and I look for
all of these smells like feature and the
bad coupling things this should be
cohesive and brought together and not
together you start pulling these things
all together and you get much cleaner
code base that's much easier to
understand and you tend to get a big
bump in performance because things end
up being where they should be they end
up in the same cache lines you don't go
chasing pointers across to other things
you pled to the bats of how Hardware
works how the memory subsystem works and
you get the benefits from it so you've
got to respect this locality of
reference and as our memory gets bigger
and bigger it is not getting any better
on response time so you better get the
patterns right to get the right sort of
behaviors because the golf nigh between
having a full cache miss and processing
instructions is getting on the order for
put 500 instructions a cache miss is
about the equivalent of 500 instructions
with a fall and cache miss on a server
know so getting this stuff right really
makes a difference what it actually
screams out to me is it doesn't matter
about how many instructions you're
processing you shouldn't even be
thinking abut I thought almost today
you've got to be thinking in cache
misses for any given algorithm you can
almost start counting cache misses two
kinds how long it's going to take to do
any set of operations
so we've come to catch mrs. quite often
we also end up thinking about
relationships between our objects these
innocuous little lines when people draw
things so classically in financial there
were orders and order books on people
we'll just draw a little line all the
gold is in not little line so we're
gonna think about it what's it doing so
forgive the little bit of you'll nail
here for a second it's just gonna pick
it because it's common well in fact this
relationship is actually two
relationships typically there's bids and
offers and it's a one-to-many in this
case but is more than not it's also
usually qualified on price under sorted
and typically Pfeiffer going through the
thought experiment of really reasoning
about a relationship tells you so much
and you understand how things work but
really importantly you get to not choose
what is the data structure you should be
using to represent that line that is the
really important thing in modeling and
the thing that will stay with you your
entire career no matter what happens in
changing hardware and software is the
data structures matter you want a set of
characteristics for that relationship
which data structure do you pick to give
you those characteristics and not starts
to really matter I've interviewed people
and asked really simple questions about
what is the difference between a hash
map and a tree map and have things
throwing back and though you don't need
to know implementation detail these days
it's like I'm not asking you how to
implement a hash map or treating mappers
what's the difference what does it give
you what are the characteristics offered
we should know these sorts of things you
should be picking up the right tool for
the right job to give you those
characteristics so really make friends
with your data structures one of the
best investments you'll make in your
career and it will stay with you for
your entire life do you learn about
bloom filters you learn about Maps you
learn about trees all that sort of stuff
I guarantee you if you're 20 years old
none you're still programming the sixty
years old that stuff will still be
useful it won't go away it's just useful
stuff that keeps keeps us all in jobs
interestingly as well as how do we look
at this and how do we understand it I
had some really good dare talks with
people who understand the psychology
around this and like jump into the code
is great but it only gives us one way
thinking about a problem you should
document how you're gonna do something
it uses different pathways in your brain
and explores the problem in an
interesting way discuss it with other
people discuss are you gonna design the
tests discuss how you're gonna code it
maybe even do crazy stuff like write the
code in an imperative fashion write the
tests in a functional fashion doing
those different ways of looking at a
problem maybe even their formal
specification you will get something
much more succinct much more precise and
get it there but at least document it
and talk to people about it and your per
is not sufficient for this it's a really
important part of doing stuff we should
you need to be collaborating and working
together and it gets simple much more
refined solutions and it kind of passes
this what I call the outloud task then
you just make a lot less mistakes this
we also need algorithms and we'll all
have seen if the order of all girls if
you care about performance and stuff
what's really interesting is you've got
to know what an is and this is back to
the relationships and the cardinality
and the things that you're dealing with
but there's also constants in all these
algorithms so we quite often look at
what ends the only thing that's
important well quite often the constants
are very important especially for sort
of small and moderate sizes of an the
constants can be very dominant I quite
often see a case when people go
this must be a hashmap and this must be
a tree or whatever and yes well how big
is an wine everybody gets bigger than
about 20 why using the map or the tree
just thicker than a rack I mean yes will
you ever have to iterate over it yeah
all the time so you picked that tree or
a hash map or something and you've got
something that's in the order for about
20 items and you're iterating them all
the time why isn't it in a rack think
about these things it really starts the
modern and open said oh yeah well what a
man gets bigger well fine do you know
where their ends gonna get bigger or not
that's the question you've got to be
asking and find a nice you find out that
an is massive yes you should be doing
something else a really interesting
thing around this I like to do is not
just know the order of the algorithm
actually work it out through testing and
experimentation because quite often
people think the
the algorithm they're gonna get is not
what they actually get so you've got to
do that climbing thing you've got to
write the experiment you've got a finite
would it really gives you and so like
two importantly focus on and focus on
see and start measuring really finding
this stuff I'd not just end but the
cardinality of the relationships like if
you ever worked with really good data
because people who know about large
datasets they're obsessed by the
cardinality on lines yet we so often
just draw boxes and don't think about it
star what is it what is star and becomes
really really important how many orders
in a market how many customers how many
products how many movies whatever it is
it's like what are people doing for
these things how many times do they
watch it how many times to buy it so the
thing is but algorithms are the key to
the service time so we know that
response time is service time plus the
latency time you're waiting in the queue
so getting the service time dying is
really important what's really really
important about service time is service
time is related to utilization so if you
have the service time you have half the
utilization of something and so imagine
you're running at eighty percent
utilization and you half the service
time you know down to forty percent
utilization guess what happens now -
your response time it's a much better
it's not typically just half as good
it's much better than not if you're open
90 or 95 percent utilization a new half
you can probably get a 20x improvement
in response time so just think knowing
these things mapping them through
botching so important I think if there's
one thing I do in any design is
unlocking for every opportunity I can to
batch because this is where all your
performance wins typically come because
they what it does is it allows you to
amortize expensive costs that go in the
desk that taken a trip across to another
machine they're taking the big cache
miss wherever it happens to be you want
to batch off as many things as possible
when you take those big costs we do this
in reality like we don't get everybody
going from air to be
in an individual car free time if
they're all going to Sam lab we put them
on a bus we put them on a plane that's
all the stuff where you've put many
bones and seats and get them there it
should be exactly the same thinking in
your code and particularly for this
we've got to be thinking about being
sink being synchronous is going to hurt
us we need to be thinking a synchronous
for everything because soon as you think
a synchronous bottom becomes so easy
everywhere and this is the one example
that you're saying expensive resource I
want to use it multiple producers want
to put some data into this expensive
resource if they all go to do it at the
same time first of all I'm probably
gonna have to have mutual exclusion on
this resource so I'm gonna have to do
something and then you're gonna have to
cue to take your turn to enter that
critical section is protected by your
mutual exclusion so if you're gonna use
something and I even it's just three and
there's one unit of time one thing to
take one unit of time one's going to
take two and one's gonna take three
units of time so in average you're gonna
take two units of time and as your
batches get bigger so four bursts of
traffic arrival you start ending up with
very large periods of time really simply
if you put some structure in between
that you can gather up the work and then
something the other side of it is
dragging all that work down and put it
into the store as a single on contended
operation you'll get much better
utilization much greater throughput and
also much better latency I keep hearing
over and over again that is a trade-off
between latency and throughput
really it is and we've been
believing this in our industry for far
too long
anybody wants to come and chat to me but
this I can work you through the math I
can work you through examples it is so
simple if you botch correctly and Bochy
is a real-time thing not an offline
slowpoke process you can put it into
designs and I love just doing this over
and over with my showing them how simple
it is to get this stuff right or not so
how simple it is the reason about the
code and you did the right things and
it's just simple patterns like this
allows you to do it if you do this that
blue line is what you end up having
rather than red line
the KeyArena fact gets greatly improved
because you don't end up over utilizing
the resource because you amortize the
cost which takes your utilization dime
ultimately you do sides through it
everything saturates
if you throw enough load on it but you
get this really nice constant property
to the point of saturation and even then
at the point of saturation it still goes
at the maximum of throughput it can and
does the best at canopy and you tend to
get a linear progression then rather
than this just steep J curve where it
all goes horribly wrong cuz that's
another thing we want is when our
systems hit maximum throughput we don't
want the wheels to fall off we want them
to continue doing the best they can
about kiss and not fall apart and
designing for this especially the
botching gift.you there so remember this
batch processing is not just for offline
branches branches branches horrible in
our code and a real major problem it can
be so simple and we didn't like whenever
we run branches in our code or CPUs are
gassing there they're predicting where
you can go on those branches most of the
time they get it right and they're
progressing ahead they're speculating
they don't actually process instructions
and just stop because they're constantly
cache missing and they want to keep
making progress the view branch of
quickly unpredictable and also there's
only so many branches you can have
before starts burying a cost you start
to slow down from a performance
perspective you also slow down a lot
from understanding and reason about your
code I quite often see stuff like this
where people will okay I'm dealing with
novels which you really should not be
dealing with in your code like we're
we're past this now it's 2015 we should
not be passing things around as novels
as some sort of value unless it's very
few reasons for how to do that like
empty sacks in for everything but then
don't start doing shortcuts like if is
empty just be alive sooner it just
doesn't make any sense it's it's evil
it's also not the common case so you
start hurting the common case for what
you think is an optimization just get
rid of it this sort of code get rid of
it
it makes your method smaller make some
cleaner make some easier
stand on a lot less boogie and they're
faster so it's a kind of common pattern
that keeps coming up and so I don't have
the principle of supply I surprised
company any other time it's a Middle
East surprise so don't be dealing with
the Knowles always be doing no objects
empty sets all that stuff so you start
doing the right thing all the time he
was at Tony Hoare says like the null
pointer was his billion dollar mistake
just shouldn't be doing this sort of
stuff loops are a fascinating one I've
seen loads and loads of statistics that
said that our programs probably spend
80% of their time in loops so how we
write loops really matters this quote
and I like how you ride so again if
anybody's ever written anything so
you're writing blogs or books or
anything you start realizing that you
can go back and you can reread what
you've written and you can use to
shorten and make it more concise make it
more precise and make it easier to read
it's exactly the same with your code
like that if I had more time I'd written
a shorter letter usually I've had more
time I could have written a shorter more
concise
more elegant loop so go back to your
code quite often and do this because it
really starts the matter so I said we
have an L zero cash in our processors
they can only handle about 1,500 micro
ops
if you don't fit inside that you're
constantly evicting things from the
cache and pulling stuff in really slows
you down
even more so once we have dealt with
that we go through to a new branch
buffers 28 micro ops at that stage only
it branches allowed for a loop and no no
returns and no exceptions coming out of
a loop those will can't fit into those
you've got to go out to the other sides
of things like many people have multiple
return points from functions especially
in the middle of a loop that's bad it's
bad to reason about it's also bad from a
performance perspective like either more
branches in a loop that's like my smell
a bit as well why are you doing that
single responsibility principle keep
your loop simple make them really easy
to deal with
you should craft your loops like good
pros in fact this is what I like to do
is constantly go back and reread your
code because you keep seeing things you
see bugs you see where's to improve
you see whether making stone batter we
should make them normal where we work is
you'll end up with code base
that's much cleaner much faster much
easier to work with and a lot less buggy
and so we've got these nice simple
things that need to be composable so we
got to think in the composition and for
this size really matters cliff made this
greer code a while ago and we were
dealing with them around hide the right
methods to make them particularly fast
and he pointed out that how most
compilers work is that inlining is the
optimization everything else is enabled
by inlining so we take a method if you
inline it into another one can in unroll
the loop we can register cash we can do
right combining we can do all sorts of
really nice funky things but if we can't
inline it its opec we can't combine the
things together and so by start making
things small making them composable it
works out really well if we make all of
our code nice and small and composable
really simple methods don't worry about
the function calls RDBMS
will just inline them and the function
calls gone and it's also much more
composable I had a great tweet three six
nine months ago where I've done a
training course for some people and I
was banging this into the make small
methods make them composable and there's
they went through in the done a lot of
this and made a lot easier to test and
then this guy was spelling the practice
do just by doing that some bits of our
code are 40 or 60 percent faster and
there's a whole lot easier to reason
about because we let the compiler really
get his hands on at now and make it nice
and clean the compiler can't do much
with 3 and 400 line methods that I can't
compose them together because they're
OPEX that don't in line I've got to
think about these things so keep things
the single responsibility what one
statement one thing one method one thing
one class one thing one module think
everything has one purpose not multiple
purposes then the compose much better so
we think we can build neat things from
small items
theda kind of pushed on here we get
obsessed with object orientation we're
quite often we don't need objects this
betwee a bad thing of the java world but
really we want to have grid big tables
of data and
if your table of data is customer object
with an array of references to customers
you're gonna pointer jeus all of the
time say I want to go through and I want
to read all the dates of birth I want to
go through and I want to scan for who's
with a certain surname
you don't want to be doing that with
objects think about this differently so
imagine if each of those fields were
just in a rare of a given type now if I
want to search through I can just scan
down not a rare I don't have all the
headers for the objects those are nicely
co-located in memory if I want to view
the whole object well I can just go
through and take the index inside those
arrests how do I go walk down through
them we'll just walk down with the index
that could be done with a fly wit this
all suits much better for the locality
as you walk forward you get the prefetch
who's helping you it's going back to
those beds you gotta think about data
it's not thinking of a code think about
data and this is one of the ways you'll
get much much more performance and a lot
can be learned here from set theory and
functional programming so if you've done
a lot of object orientation one of the
ways you make yourself a much better
programmer is go learn set theory go
learn functional programming the good
learn logic program learn other ways of
thinking there's a lot you can take away
from it and there's some great stuff
there so go finish off here with some
performance testing what do we what
should we be doing with performance
testing first of all define your
performance goals you need to have
something you're shooting for you need
to know when you're done when's good
enough quite often people say I want to
be faster it's not helpful I want to
have X throughput per second with the
response time of Y or have a certain
latency curve and these are the things
are important so you establish those
goals from there you can establish
design principles when we worked on air
on one of the things we done is we
defined our design principles this lets
you make decisions so for example we
have no garbage in steady state running
you could not aaalac it so we write all
of our code we write over a pass we run
a profiler that tells us we've got any
allocation if we have we get rid of it
we work on it because the allocation
will cause GC and GC will cause latency
spikes on your respond
times gone so being a worth these things
just how to do with it how do we measure
response time the important thing is we
have to do it with histograms if you use
means if you use standard deviation
you're deluding yourself and you're
completely wrong you probably should be
in Gail's talk which is done a little
bit but let's look at some things so
here's a typical histogram of measuring
the response time of a system mode is
kind of interesting the most common case
it actually tells me something useful
median yeah less useful mean completely
bloody useless it doesn't tell me
anything about the typical response time
in the system and it doesn't tell me
anything about all this bad stuff that's
out here
do not use means to describe your
systems you need to characterize your
system in a histogram like this and this
is how you'll see your systems
performing another way you can do is use
quantile distributions or percentile
distributions and see how your systems
behaving at the 90 the 99 three nines
four nines five nines and this is a
typical round top you'll see from a
system in higher the behave this is
courtesy of Gil from one of his
customers who were measuring their
system I'm thinking this is what we're
getting but that's not natural when you
see steep lines and jaggedy bits like I
thought is not not so that's because of
the thing called coordinated animation I
advise you to go ahead and Google for
coordinated mission and learn what that
means so and how do we record this use a
tool like HDR histogram it's out there
it's open source and it does a great job
and very cheap to use if you've got a
benchmark we're gonna macro benchmark
it's riddled with problems and pitfalls
you can fall into don't write your own
micro benchmarks unless you know a lot
of JVM vendors very well and even then
you're kind of crazy doing that go pick
up something like Java micro benchmark
in harness really grid tool
it's even got a nice profiling
information that comes with it as well
so write these tests and write them or
not also track CPU performance
characters so what I mentioned about
these big loops and the fact that you're
evicting from the l0 cache how do you
know that will the CPU can tell you
because it has all statistics is the
statistics on cache misses branch misses
bandwidth all these sorts of things and
you can find it from the CP
performance characters ms are the things
to go look for there and performance
testing needs to be part of continuous
integration it's something you should be
doing all the time and running all the
time and feeling if you don't meet your
targets it's also really good to tram
things as well and trend things and
graphs and get you doing the right sort
of things when we were at L max we had a
wall of screens for our continuous
integration and a lot of thought was our
performance figures where we were
graphing response time graphing
throughput between bills and like
whether we're improving our getting
worse and it's really useful to see that
and so make sure that this runs well but
also consider all your acceptance tests
suitable for performance tests quite
often they can be and make your life a
lot simpler but this is one really
really important point is we're
typically blind in our live systems we
need to be building telemetry in his
first class we need to be building it in
from the ground up into what we're doing
let me just make that point again we
need to be building this into our live
systems and doing it from the ground up
it is not an afterthought if you want
your systems to perform well and not be
surprised in production you've got to
have telemetry in it so how do we do
that well we put counters in our code so
things like queue lengths number of
concurrent users number of exceptions
transactions all of that sort of stuff
write them to things like memory map
files so you can read them without
impacting the system at all without
using any locks and some great
techniques for doing that there's even
we've put some of this stuff out as open
source it's really useful but also
capturing histograms capturing
histograms of response times of service
times of queue lengths like if you don't
know what is your service time how do
you know that unit is that your problem
right is it your services are taking too
long or you having a distribution of
times so when you go back to all that
sort of like queuing theory like is your
service deterministic is it Markovian is
it Erlang and behavior you'll start
using these models and predicting it you
need to know the models that are
possible and you need to fit the right
figures into it and you need to get the
things from reality and we can do that
so very quickly in closing we've got to
think of clean as being uncontaminated
get the stuff out of your code that
isn't adding direct value because your
codes liability
one tip if you measure one thing on a
project and hi people
work that matters more than anything
else lines of code deleted that will
drive much better behaviors than almost
anything else get the code base time
make your code representative it should
be a true portrayal of the business
problem it should also be a true
portrayal of how to use the system if
you've got classes called weird things
that are not related to anything you're
doing it wrong
I look at it believe it you shouldn't
even be documented you shouldn't have
comments in your code other than
references to papers or algorithms or
something useful that you need to did
you're doing your code wrong you should
have nice simple clean methods that's
found on their own that makes sense
and does it pass the outlawed test so
quite often we'll write things in code
that just makes no sense that if you
said it out loud to someone else you'd
laugh would be embarrassing so think of
it say these things out loud done North
stalks it but he's got a rubber duck on
his desk but he says things I'd like to
even if you're not talking to another
person but most importantly measure and
don't guess if you want to have
performance you gotta measure and don't
guess and I'll leave you with a kind of
interesting quote from bill Lear they
invented the Lear jet he said if it
looks good it will fly good and I find
that is so true of code you can look at
code bases and you'll almost know if
they're gonna perform or if they're
gonna be buggy and if there's a kind of
love attention to detail that you can
see whether it's clean and whenever it's
not and this is I think it's the same
thing with aircraft because exactly the
same sort of thinking goes in that the
perfect aircraft is not the one that you
can add more things to it's the one that
there's nothing left you can take away
it should be the same with your codebase
when you get it right and I'm not I'll
finish off I think we might have time
for a second or approximately
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>