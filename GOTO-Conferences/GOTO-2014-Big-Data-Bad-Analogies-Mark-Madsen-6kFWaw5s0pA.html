<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • Big Data, Bad Analogies • Mark Madsen | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • Big Data, Bad Analogies • Mark Madsen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • Big Data, Bad Analogies • Mark Madsen</b></h2><h5 class="post__date">2014-10-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6kFWaw5s0pA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody I'm mark Mattson I have
been doing data stuff or
about 30 years now and I started as a
programmer moved into operations I've
done pretty much every job in IT I have
been you know CIO and a CTO I've been a
developer I've been a DBA I've been a
janitor in a data center which is in
fact how I learned about mainframe
programming janitors have a lot of free
time it's a good job if you're an IT so
I'm going to talk just about some
analogies now one of the things about
analogies in the Big Data market is that
they're all wrong and the thing is if
you use a bad analogy to explain
something to somebody else then you
frame the problem incorrectly and then
you think about the problem incorrectly
and so part of what I want to attack in
this talk is just some ways of thinking
about problems a little bit differently
as an example the horseless carriage the
horseless carriages how people talked
about cars for many years and the
problem is that when you think about it
as a horseless carriage you think about
it as a carriage and if you look at one
of these early horseless carriages this
is not the earliest but it's the best
picture I have it's a carriage with
carriage steering and carriage brakes
and everything else in an engine
strapped to it the one before this one
the carriage was pulled by a motor on
four wheels harnessed to the carriage so
people were thinking about it exactly as
a carriage problem not thinking here's
the technology what is the capability
that I want and how can I use the
technology to give me that capability
which was driving a car likewise in the
in the Big Data world we have terrible
analogies data lake is a great example I
hate the marketing around data lake
lakes occur in nature you don't make
Lakes they just exist and a lake is full
of what water it's full of
undifferentiated stuff so a data lake is
just a place with a bunch of bits in it
but you can't tell one set of bits from
another set of bits but it's important
to be able to categorize and sort and
differentiate data and so when you start
thinking about the data lake you start
thinking about just dumping the data in
don't worry about it or as Devlin would
call it a data reservoir right you at
least have the engineering to build a
dam after which all of the the data
piles up behind it and floods all of the
infrastructure drowning everything
that's inside and when I first started
doing data oriented systems like big
data systems back in the 90s which would
have been machine learning for fraud
detection and things like that or just
basic data warehousing business
intelligence of course back then big
meant you know 50 gigabytes but it was
the same problem the whole idea there
was early on just just back up all of
your databases into this one big
database and you have a big pile of data
and you ended up with a data junkyard
and now you're just going to end up with
a polluted data leak we know this
approach this mental model of how to
think about the problem doesn't work a
new one that's different from the past
is that today we have more data for many
more sources than we did previously and
so people have talked about data exhaust
I think splunk marketing uses data
exhaust is an analogy which is great if
you're thinking about choking on the
fumes of the applications that you're
running and you're thinking about data
is a byproduct it's you know data is
probably the most important part of your
data processing job your programs
operate on data and to think about it as
exhaust it's sort of like this cloud of
stuff there's no structure to it there's
no pattern to it and once it's emitted
you don't think about it anymore the
problem is that data has a life of its
own that goes on for a long time and one
of the final analogies is is data is the
new oil right looking at the economic
aspects of it they say well it's a
resource and you can process it into
many different forms and make money off
of it doing this or when your business
better doing that and that's actually
not a bad analogy it works it also works
in the sense that there are data spills
privacy leaks people privateer with data
they take it hostage and blackmail you
with it so it fits for both models but
the real thing about data is that it's a
choice it's your choice when you write
an applicant
when you write an application you have
data structures you structure the data
in your code and then you're using
persistence layers of one sort or
another to store and use that data and
what you choose to record and what you
choose to process is what's important
what you choose to measure in an
application what you choose to log or
not log is important I just sat in on
the Pinterest case study and one of the
things at the end of his talk was pay
attention to what you log log as much as
possible about your operations if you
expect to be in a high-scale environment
that is the choice that you're making
and that's the same whether you're
building ERP systems or building web
scale sites so when you look at these
analogies they frame how you think about
things and you know one of the questions
is how you know we got to some of the
places we got and I'm sort of a computer
historian as well as doing geeky you
know data stuff and I love some of these
old quotes you know there are there are
lots of things out there we didn't know
existed before and so one of the things
I like to look at is is looking at past
ways of organizing data and last year I
talked about clay tablets and you know
sumerian information hypercubes and
things like that because they actually
invented the multi-dimensional data
model 6,000 years before they patent in
1962 so I'm just going to go back this
time to some paper era stuff around you
know the Elizabethan era because that's
what books really took off so we're
talking 1550 to 1600 and we have
commercial printing presses you know
printing is now gone wild everybody's
got paper and books and the thing that
was unique is that instead of personally
transcribed copies printing presses
produce exact copies so now we can have
an exact copy of the same thing many
times spread over many places we solved
problems just like data problems today
the data problem of how you serialize
objects for example you know do I use
avro do i use thrift do i use some
custom xml it's just like font
standardization the
the principle there is that you're
writing out data that somebody else has
to read now when we had Latin texts
there were three different variations so
there were more than that but three very
common variations of Latin text one of
the texts was for the academics one of
the texts was for the religious scholars
one of the texts was for the the non
religious scholar the the you know well
we won't get into it now but the the
philosopher and the font that you wrote
could not be read by the other classes
unless they were trained to read that
it's very much like writing data into
Hadoop and then having to have an input
output for matter that you have to bind
into a program essentially customizing
api's for everything or writing out
formats that other people can't read the
single aspect of font standardization
plays out in a database world in the
form of SQL is a least common
denominator for describing data sets now
during this period we went from eight
million books to 200 million books and
the population of Europe was much
smaller at that time that it is today
obviously and so one of the things we
ended up with was how do you categorize
data where do you put this data data is
recorded into books at that time so
that's the container the logical
container of structure and in the Alize
Elizabeth an era you basically had two
choices sacred profane sacred date of
the religious stuff profane everything
else and so when you have these books
you you just shoved them in the library
there in two things there's maybe 50 of
them you chained them to the shelves
it's not really big deal but when you
start to get so many books being
produced you have new problems so along
comes bacon not that bacon Francis Bacon
and Francis Bacon of course you know
he's looking at the way the Vatican did
it he's looking at the classical era and
jiri classifieds knowledge and he says
well actually there's three
categorizations you know instead of
sacred and profane we're going to have
history Posey and philosophy history in
his world was everything that happened
Posey was everything that you could
imagine could possibly happen that is
fiction and philosophy is everything
that theoretically or actually can
curr and so he had this division and he
started taxonomy around data and
knowledge so you get into the 17 and
1800's now we're talking about the the
Georgian era natural philosophy the
scientific revolutions the Enlightenment
and when this happened everybody was
making even more books the number of
books doubles again during this period
and the variety of information in the
books the ability to reproduce these
things creates another explosion we get
into the Victorian era we're now 300
years on from Francis Bacon and the
information problem of the day is
industrial printing because industrial
printing produces books it's such an
incredible rate that you no longer have
the ability to properly store and manage
them but it's not just books now we have
newspapers we have the equivalent of
modern-day magazines we have leaflet
ears running wild in the streets and
this explosion and explosion of
specialized knowledge creates the need
for more and more taxonomy which is ways
of categorizing and storing information
and levels of indirection you now have
to think not just about a library you
walk in and find the book you're looking
for because there's 50 books there's now
5,000 books in a library how do you
organize the shelves to find those books
and how do you organize the knowledge
inside of those books and these old
models had no levels of indirection so
people started paying attention to
library science which brings us into the
modern era essentially with melvil dewey
and i'll point out a few things about
him which will be relevant later do we
came up with a dually classification
system which was for a period the most
common library classification system on
the planet it's taxonomy basically it's
a top-down tree structured orientation
where you navigate down a tree like a
bee tree only not a bee tree and you get
down to a shelf of books and that's
where you land and he built this to help
you get books onto the shelves and for
librarians to help you find the books
that you needed he was an efficiency
expert so you'll notice his name
normally Melville would be spelled VI el
le but he stripped off the Ellen Ian
changed his name
legally because it was more efficient to
strip those two letters off he was all
about operational efficiency the other
person that he was competing with during
this era where's Charles cutter cutter
had the cutter classification system
which a lot of university libraries
still used today the cutter system of
course managing knowledge not just books
and he designed a system that was not
top down it was bottom up like a faceted
classification model so as you add
attributes you expand the system it's
like the difference between the scheming
or relational database and the schema in
a sparse table store something like
HBase where it can be flexible and it
can adapt over time except that in that
era it was on paper which made it a lot
harder now what this gives you as a
bottom-up orientation it gives you a
model of flexible information
architecture and lets you identify a
single book see the thing about the
Dewey system is that it's a block
oriented model people always ask me why
I look at library science it's because
it's like a database a shelf is like a
block the books on the shelves are like
records and the Dewey Decimal System
says you can find this book here and
then you have to scan that bookshelf to
find that book which is exactly like how
Hadoop works right the basic Hadoop
model is there's a 64 or 256 megabyte
chunk now scan that to find the records
that you're interested in now if you use
H files or something of course you have
bloom filters and other things on the
end of that but that's more like the
cutter model the cutter model says
here's an identifier it is a unique
identifier for the single thing and it's
immutable if you come up with a new book
a new version of the same book a book
with the same title it's going to get a
different identifiers right that is
essentially saying that there is
immutability in the data structures
which gets you a totally different range
of behaviors and ease of finding
information but who won do we won do we
won because of pragmatism right it
wasn't that his model was better in fact
his model was worse for most things
but for the problems of the day and the
technologies that he was using it worked
really well and the technologies our
paper card catalogs and paper systems
where every time you have to do
something you just scroll it on to the
end whereas the cutter system and having
to do the equivalent of like a tree
balance and so it was good enough to get
the job done without overworking people
on the paper front the other side of it
is far more interesting he solved the
ingest problem the real problem do we
solved was that libraries received books
faster than librarians could put them
onto bookshelves and so if you can't get
a book on to a shelf fast enough then
the books pile up and eventually they
start throwing out books which is
exactly what happened in many big
libraries they could not afford the
number of people to put it on it's like
the equivalent of trying to run a
website with a database behind it like
Oracle that can't scale past the size of
that one box because you can't afford it
and so he solved the problem of ingest
by making it very efficient to very
quickly get shelves on the book or books
on the shelves because it doesn't matter
exactly where that thing goes it just
goes into this bucket the other thing to
pay attention to libraries for is how
the librarians themselves shelve books
interestingly you'll find that they
invented things like the quicksort the
shell sort the insert sort but
librarians showing books use adaptive
sort algorithms computer scientists try
to find the best sort algorithm and so
the quick sorts the best sort unless the
cardinality of the data or the histogram
distribution of values in the skew and
the data is a certain way in which case
the insert sort is better or the shell
sort is better and so understanding how
these things behave is is very
interesting and these guys figured all
this stuff out hundred years ago with
this stuff teaches us just looking at
sort of books and libraries and all
these other things is that information
does require organizing principles and
at first when you just have 50 or 60 of
something you can dump them in this is
very much like using splunk for data
analysis I have one big deep clickstream
data set dump it in it's great throw in
my customer data my Prada
data my sales data my costs and suddenly
you have to build big messy systems that
can't navigate anything because you have
essentially one big wide search index
and you can't join data and joining data
is one of those things that turns out to
be really important and that means
categorizing data into different chunks
having a global schema that just tells
you where to find the different data
sets second thing is differences in
scale require different principles it
was easy enough to shelve books when we
had 200 million of them worldwide it was
very hard to shelve books when 200
million were produced every few months
and so it's just like today's technology
revolution where the ingest rates of
data that we have coming out of the
systems we have today have outstripped
anything that has been produced it's a
it's multiple orders of magnitude shift
of data rates third thing is that in the
adoption cycle of information there's a
point where people stop paying attention
to the creation of the thing and start
paying attention to the consumption of
it so it happened with libraries it's
what happened with a lot of this other
technology first you pay attention to
how can I capture the transactions
shelve the books and do other stuff and
then later you think about how can I get
the stuff back out how can I use it if
you can't solve the capture problem
there's not much reason to pay attention
to solving the query problem and that's
how a lot of database technologies have
evolved so when you think of analogies
one of the ways to think about data and
and databases is that you know data is
to database as books are to a library
right the data in a data structure in a
program is a structure that has a schema
now when I say scheme I don't mean a
relational schema I mean a schema in a
sense of organizing structure so a
structure in a Java program or an object
or a see struct these are all structures
a book has a very rigid structure and
the data fits into that thing the
library is the collection and there's a
big difference between dealing with
each's individual items and collections
that has been lost on the industry for a
long time and many programmers don't
realize why set oriented models are
important
they're coming back in a big way and the
other thing is that there's a point
where the printed became more important
than the thing that produced at the
printer lots of history focuses on
printing presses very little history
focuses on how people organize and
manage the books that came out of them
the same is true of the data markets and
so it's a good analogy in thinking about
how one manages it because there's two
sets of things here the production of
stuff at scale and the consumption of
this stuff at scale and so one of the
things you need to think about is that
when you write an application it might
be Pinterest you know huge scale
application it might be a little thing
for your department when you build a web
app when you build a mobile app anything
it's going to produce data that is
useful for that application the other
thing that comes out of it though is
that other people will find that data
useful there is a life cycle to data
that extends well beyond your program
and so when an order for example comes
in customer service needs access to that
but they do things to it as well if you
build a nice Mongo datastore to take
orders you may discover that the model
of access path and the nesting
relationships you constructed in that
nested hierarchical model do not map to
what customer service needs so what do
they do they force you to redo your
physical model or they siphon the data
out and historically what we've done for
scale reasons and other reasons is to
just keep moving data from system to
system and pass data around in the
organization until it looks like this
I've done a lot of IT work and I'm
always looking at system context
diagrams that show data flows of data
sets that look like this big rats nest
here now the thing i love the Douglas
Adams quote in an infinite universe the
one thing sentient life cannot afford to
have is a sense of proportion because
when you start looking at this you feel
like it's hopeless but it isn't hopeless
it's just that sometimes you have to
think beyond how your program produces
data to how it gets used downstream as
developers and I was a developer for a
long time we think about what's inside
the box and then if somebody else uses
our data that's their problem not
the thing is that code lives longer or
that data lives longer than your code
data always lives longer than code just
like books outlasted the printers that
produce them and when you look at other
kinds of supply chains I just showed you
one company system diagram and it wasn't
even a big company this is the multi
industry data flow for cheese production
in one country and so it's a very
abstract one but all this stuff up here
is data flows going back and forth
between all of the companies down here
and so there are levels of composition
of data that go beyond the system the
process and the organization to this
sort of very large perspective and
that's the context within which this one
little application over here runs but
the data that it produces might go all
over the place and so you have to think
about how data is produced what its
lineage is what its structure is so that
other people can consume it and other
people other systems other processes
that gets me to a point about design so
I'm going to take a quick side track
here because really what I'm trying to
get out with a lot of the things that I
I talk about is better ways of designing
systems and one of the things that we
have a lot of his technical debt now
technical debt is basically just what
happens when you make a decision with a
trade-off right I'm going to do this now
because I have to ship in three weeks
and I'll fix it in the next release so
you've taken on a debt there
intentionally taken on that debt other
times you didn't know you missed a
requirement you made a mistake there was
a bug you didn't understand the
Pinterest talk was great for talking
about we tried this and that worked for
six weeks and we had to throw it out and
get a whole new different platform in
and then we tried that and that you know
it reminds me of the monty python
episode with the the swamp you know i
built a castle and the swamp people said
I was crazy I did it anyway sank into
the swamps I built a second castle that
one also sank you to the swamp so I
built a third castle that one burned
down fell over in and sank into the
swamp but the fourth one finally stood
we can avoid some of those fourth one
finally stood if we understand
building in a swamp and what the nature
of building in a swamp is and working
with data is often like that now that
said a lot of times we like to blame our
tools Oracle doesn't scale or relational
databases don't scale or sharding sucks
it is the poor carpenter who blames his
tools except that sometimes the tools
really do suck so think about technical
debt in terms of the kinds of things you
accrue in your software portfolio from a
short-term in a long-term perspective
and what you'll find is the short-term
stuff is going to be mostly about your
application code up here so you know you
can you can do a lot up there
infrastructure architecture and design
or a different nature of problem and one
of the things that I would argue is that
a lot of iterative design which comes
out of agile development and DevOps
approaches is great for turning features
fast and truly terrible for building
core infrastructure you don't build a
city by not planning it and just letting
a shantytown grow up you build a city by
creating zoning and building
infrastructure and roads and water and
electric and sewer plants that support
the things above it without constraining
them too much and the balance is not
having over heavy infrastructure which
we have had I would argue for a long
time and not enough freedom on the front
end but if you go the other direction
it's just as big of a mess and so the
kinds of short-term debts you take on in
the long-term debts you take on will
manifest later and that's what leads to
big failures sometimes now when you
think about the decisions that led to
these things up here short term
intentional I meant to do that we're not
going to put this feature in or we're
going to do it this way and then we'll
fix it in the next release long term
it's more like design choices we're
going to design it this way it will
constrain what we can do and we may have
to fix it two years from now versus the
unintentional things the the flaws and
how you address the methodologically
there's a lot of stuff at this
conference about in particular agile and
DevOps things up
here to solve these types of problems
versus you know redesign and and over in
here in the unintentional box experience
in education is typically the only way
we have of getting out of that what I'd
like to see more of is moving out of
this box and shoving important
infrastructure decisions which in my
book tend to be hardware topologies
network topologies models of use and
persistence layers into an intentional
box where we understand the trade-offs
that we are making the question is how
do we get into this mess in the first
place so I'm going to throw some history
at you this is sort of the database
industry history and what we've got here
is you know starting back in the 1960s
multi-value data stores you know in a
multi value data store you could have
variable structures of records just like
we have in Mongo or Cassandra or HBase
so these models have existed for a long
time we had hierarchical databases which
let you build out a hierarchy which is
very much like for example a document
model database where you have a customer
and then you wrap the orders inside the
customer or the classic example of
Mongo's you know blogs and comments
model and you make a trade off there and
you make a trade off here and it's
exactly the same trade off so if you
redesign literature from the 1970s for
ims you too can be a good Mongo designer
because the principles that operate
behind these pre relational databases
are all the same my career started with
record management systems which is
basically just spitting structs onto a
file system in records and you had for
primitives for navigation apart from
opening and closing you had the first
the last previous and next there was no
index there was no b-tree there was no
key to look up we had to layer those
things on and the the the databases that
came about in this area like database or
IMS gave us some things like
recoverability and acid transactions and
moved us out of this record oriented
model where every navigation you had to
do in your code but I see Java people
still do this today they write a query
against a relationship relational
database pull it into memory and then do
the join in memory which is an iteration
building iterators and that's not really
a good idea
it's not a good idea for a key reason so
I don't know how many of you have seen
conan the barbarian right a classic old
film what is best in life well according
to Larry Ellison what is best in life is
to crush the vendors to to see them
driven before you and hear the
lamentation of the salespeople and
that's driven the the computer industry
for a long time but i would say wrong
it's loose coupling reusability and
scalability and that's what databases
pre relational databases gave us they
gave us loose coupling instead of me
writing records into a file i have this
schema descriptor which just says this
is what the data looks like now i could
have written something else into there
but that's what i was supposed to put in
there and so somebody else can come
along who wants to use my code because
our key problem in those days was
reusability when i wrote a file up and
so now i've got this idea of loose
coupling where the schema is separated
from the physical storage which means
now some other system can handle
optimized navigation of data on disk or
cashed in memory my program simply has
to call the api to get it it gave us
schemas which gave us reusability of
code multiple applications writing
against the same data set it also gave
us reusability in the sense of data
portability because back then everything
had to be written through libraries
everything had to be done all these
different ways and so there was no
portability of data from one place to
another because you had to rewrite all
of the data in a different program
different language binding third thing
with scalability once I hand it over a
whole set of tasks around I owe to a
database it could start to manage
concurrency far better than individual
programs reading and writing data and so
we got some great things back then which
are very similar to things we have today
and what we've been pushing today is
relational databases suck because
they're too rigid therefore we need
schema flexibility well we had schema
flexibility relational database
implementations today traded it
and they're starting to build it back in
again but what the trade-offs are is
really these right here global
validation versus local validation right
the context of your program defines the
rules for what proper data is that goes
in and out versus everybody must conform
to this rule schema isn't it assertion
about data that the data looks this way
and if you have typed schemas than that
they fit into those types and so global
schema rules like a relational database
has or like ims had a very strict these
types in these columns if you try to put
characters into a numeric column it's
going to kick it out if you do that in a
document database doesn't do that if you
do that in HBase doesn't do that unless
you enforce these rules so you have the
ability now to decide when that's
important and I think that's a really
good thing you have rules on read verses
right and that's really a useful thing
too because for example in systems I've
built that take telemetry data like like
game telemetry data coming in at a
constant rate over the network the games
and the firmware and the devices and a
hundred other things change every week
which means the record structured varies
week by week it's the same data maybe
it's been reordered maybe something new
has been added if I have to stop
operations alter a table reload that
table recreate that table change code in
order to write that data into that table
I lose data and that's exactly what
happened to me we switch to things that
just did log ingest invariable record
structures I don't have that problem
anymore but I've shifted my workload
I've made it easy to ingest data very
very fast and more or less know what
data is there for example if you use
HBase you know what's in there you've
got variable structures your problem is
if you want to read out specific sets of
data you pay the price on the outbound
side because your data typing your
efficient storage and everything else
are not there because they weren't done
it in jest so you do all that work on
the other side but we have enough cpu
and we have enough io bandwidth in a
parallel system that tip
quickly we can eat those costs on the
outbound side which means that now you
have a choice match the data to the
table relational models or match the
table to the data predicate schemas and
so you do have problems though that
sometimes you don't recognize and one of
those problems is in flexibility what
you think of as flexibility like MongoDB
a friend of mine is a statistician at
amazon he deals with lots of data they
built a new system for a new feature in
Amazon the developers came to him and
said hey we're going to try out Mongo
for this do you mind and he said no can
you give me this data and he said yes i
can give you that i can build this in
Mongo it'll be great so they went off
they built the system they got it back
he got this stuff they did some
statistical analysis and then he he was
looking at some of these things and he
said this is great I need this
information over here well Mongo like
ims and things before it has a model
that requires you to physically couple
your data structure with what's written
on disk there's no loose schema model
here that lets you say like a relational
database throw a new table out here you
have no joins in most of these no sequel
databases which means if you need
flexible out on your data you have to
rewrite the schema so they went away for
three weeks they recreated dumped and
reloaded the database he did some more
analysis a couple weeks later he needed
some different data and again three
weeks to make a schema change and this
time they did it smart instead of dumb
you know they didn't dump and reload
they did a lazy rewrite pattern against
it so you know you read the old data in
and you write it out in the new one and
over a three-week period it churned
enough data that he could do his
analysis but after that they said we're
not going to use this anymore because
every time we eat that and the reason is
back to that simple principle from the
early days decoupling if you can
decouple the logical model of your data
from the physical model of your data
whether it is relational or not you win
something
and that gets us into some of these
things like the cap theorem which is the
big driver for a lot of these no sequel
databases which was here's how to try
and scale by eliminating things like
transaction coordination which is a key
bottleneck in data systems anytime two
things have to synchronize you serialize
and when you serialize a point you limit
paralyzation which means you limit
scalability and so most systems that you
look at are down here on say the a PCP
axis and what you're what this is is
consistency partition tolerance and
availability right consistency if you
write data and I read data it's
consistent I'm not reading halfway in
flight committed transactions if you're
looking at availability it's that the
clients can actually get access to the
data and if you lose part of your system
for example lose a disk or lose a node
the system will hang if it's a if it's a
CA system which is modern relational
databases whereas if you are eventually
consistent like Cassandra Mongo whatever
you choose something on these accesses
and what what you give up its
consistency and that consistency
trade-off is the thing that most of
these databases made because most web
apps don't need acid transactions they
don't need to worry about bank balances
I might lose a registration for a
newsletter who cares the upload from
your mobile going to Pinterest or
Instagram if it fails and you're looking
you can re-upload it's not a big deal
whereas if that were a 10-q filing for
the stock exchange and the filing didn't
go through you're in trouble so there's
a big difference between when you do and
don't need this now what are you trading
off because the trade-offs are the key
thing because you're going to inherit
technical debt if you choose the wrong
thing and so this is what a relational
database does for you right storage
management data access data navigation
query optimization transactions
consistency and of course a standardized
layer that's universe
on top when we started out I was doing
record management systems by the time I
moved into the like few years into my
career we had databases that did data
navigation handle joins for us dealt
with the physical data access left me to
deal with the higher level when a
relational came along we got all this
stuff and so a database a relational
database does this if you choose not to
go that route this becomes your job and
that's important to know a lot of times
people just assume oh I've got good
enough transactions because if my
database is consistent within 10
milliseconds there's no problem right
wrong Google transaction malleability
and look at the Bitcoin repository
failures and what you discover is that
they built things with non acid
semantics for a financial system that
had hundreds of millions of dollars in
it and that led you into problems where
I could do several different attacks
this particular you know problem leads
to basically the equivalent of
nondurable reads or non isolation of
transactions which is a simple way of
saying it's wrong and so you can pull
out more data or pull out more money
that is in there's other attacks that
you can use against non acid systems
like the difference between cash latency
and physical io if you send your
requests fast enough you can withdraw
essentially from your bank account twice
as much money as you have in your bank
account that's why we have acid
transactions and this is arguments been
going on for a few years now and Google
last year wrote a paper for the f1
database which I've referenced here and
I just have a few quotes from it which
essentially say this is you and your
code and so over time the edge cases to
deal with eventual consistency when you
need consistency outweigh the costs of
performance that you take over here from
not having it and so we're going back
the f1 database is a fully transaction
compliant globally distributed
acid database and a number of new
databases on the market are also going
this way because it turns out that the
relational model is useful enough and
some of these things are useful enough
that if you can relax them in certain
contexts but still maintain the rest of
the model you're better off than if you
threw it out and started from scratch
which is where the industry is today and
that kind of brings me sort of back to
you know 1985 when we looked at the
performance of these systems relational
was standardized you know sequel became
a standard in 1986 and that was really
the opening gates for things and at that
time a hierarchical database gave you
far better performance three times the
performance at a whopping what it was at
thirty transactions per second or you
know hundred transactions per second 30
I ops per disk right think about that
today we're doing you know two million I
ops but 25 years later one single sequel
server database could do that an
interesting thing is why it could do it
right part of why it could do it was the
things that relational models gave you
and one of those was joins I no longer
had to navigate through data fixed path
navigation that's transaction processing
every transaction is like every other
transaction you know the i/o path you
know the code path you know everything
that will happen it's so predictable
that you can just build that queries or
a different story because queries don't
have to be fixed they can vary because
it's just this set of data to that sort
of date and the interesting thing of
course is you throw out queries and the
first thing people realize is there's
more than one kind of join and then you
think about things like you know I can't
get my sequel to scale therefore I'm
going no sequel or I can't get
relational to scale so we're going to do
something totally different when the
problem is probably something else and
when you think about the gift in 1985-86
actually the Heritage's and system are
which came earlier is that the core is
declarative language a declarative
language as opposed to a procedural or
imperative language lets the Machine
figure it out for you and the same way a
compiler can optimize the loops in your
code
a database can optimize the path of i/o
through the engine to get the results
that you want they're not perfect
anymore than compilers are perfect or
JVMs are perfect but they get you
something that you didn't have before
the second thing is cost-based optimizer
if you know that this costs as much an
i/o here is this much a network transit
costs this much in a memory access is
this much you can choose the best path
and this is important and it's the only
way you can do this is if you separate
logical schema from physical schema and
you don't lock the storage model and
couple it tightly to the
representational model of the data and
what this optimizer does is it says
here's all of these things that I can do
the way that it gets that is that the
declarative language which happens to be
sequel in this case the operators are
commutative associative and distributive
which means the optimizer can change the
order of them why is this important well
imagine that you have 10 million
customers 100 million sessions and a
couple billion klicks you can join those
things based on the least populated
table or the most populated table if you
choose the wrong one you burn a lot of
extra time and you waste a lot of
resources which translates into wasted
time in your code which translates to
latency in your application and the
optimizer tries to fix this now how hard
can this be oops well this is a simple
thing selecting three table joint
customers to orders two products I just
want to see the customers and orders
that say contain a particular product
there are nine logical plans to this
nine ways of ordering the tables ABC CA
B and doing the joins now the physical
layer joins hash join merge-join ness to
join now most of us programmers would
say oh just slap it in a hash and do
that not efficient under certain levels
of cardinality nested lookup would be
better or a merge would be better if
they're already sorted so
the possibilities the combinatorial
explosion for this simple query is 324
possible physical joint plans now I want
you to think about this when you are
writing a query against a cassandra
database because there is no join there
so if you have to do a join how does it
happen you write it query here you write
a query here you bring the tables
together and then you do it in your own
code did you think about every single
possible one of these joins that you
could do to order your operators
correctly to get the optimal result I
really seriously doubt it if you wrote a
pig script in Hadoop did you do this
very unlikely because that's a lot of
work and it varies with the cardinality
with the distribution of values and the
skew in your data and so if you know
those things you'll choose one plan over
another a query plan is dynamic a
program is static and written by you and
so you know what's the solution well the
obvious Hadoop solution throw nodes at
the problem just add hardware but of
course you know i love this quote about
the software industry and basically how
software simply eats hardware for lunch
because that is what happens and since
your databases web-scale don't worry
about it well when you write code to do
this you have one path and typically
you'll write it with something like this
iterators inside and you'll iterate over
this declarative languages make this a
heck of a lot easier and this will not
run in parallel short story 1994 we have
just sold a 1.6 million dollar symmetric
multiprocessing server with a whopping
32 cores in it which you know the 1990s
was just amazing developer came to me
with a bunch of code he had written
procedural imperative a lot of iterators
open up cursors did things like this he
wrote it really tightly it was actually
very nicely written code and he told me
my laptop is faster than the 32 core
server and he was convinced of this fact
I said how can you be you know sure of
this and he shows me the numbers and he
shows me the graphs and I looked at his
code and I said you do realize that all
of this fits in in the processor cash
and it's single thread and he said yes
but I round-robin the data and I spray
it out to each of the 32 processors and
so my reader reads this and spreads it
out and so I'm going you know and I can
get better scaled and then your system
so why would I do this and I rewrote his
code which was probably about eight or
nine pages into a series of three pages
of sequel statements that just happened
one after another and of course I lit up
all 32 cores on that system because the
stat had not looked at when he ranted on
his laptop was how many threads he had
running on the database server why
didn't he have all the threads lit up
because he wrote serialized code the
problem is that no amount of hardware
solves serialized code and it's very
hard to write parallel code and so yes
things like mapreduce give you shuffle
operators and other things but you can
still write very sloppy code it's very
hard to do a more realistic example you
know query like this which is what most
say bi query tools generate out the back
end 22 million possibilities that's a
lot of choices and you have to come up
with the optimal answer for that in a
few milliseconds declarative cost-based
optimizer is just in time compilers
query compilers things like that solve
these problems I'm not saying use sequel
to solve data processing problems use
declarative languages data log is
perfectly acceptable alternatives to
sequel and it's a logic language I don't
really like logic languages I prefer
functional languages but that's what it
is key point about just add hardware and
if you're interested in this side of
things I've stripped all the database
performance stuff out of this talk this
is am dolls law right this blue line
that represents adding processors and
getting better performance linear scale
if I had four processors its four
hundred percent faster if I at eight
processors its eight hundred percent
faster with ten percent of your code
being cereal ten percent having to do
some sort of rendezvous or create a lock
and block something ten percent having
to do a boundary synchronization you
know basically a MapReduce so you can do
the next
produce ten percent overhead leads to
this curve what does this mean it means
that right here you stop adding
processors or you stop adding nodes why
well my fourth processor gives me the
equivalent of three processors my fifth
node gets me one-half of a node
real-life example of this was in 1993 I
was Iran TPC benchmarks and I was
running a benchmark on a4 processor HP
server costs two million dollars each
proc board costs about a quarter of a
million dollars in the HP server they
had fifteen percent overhead at fifteen
percent overhead the simple calculation
of Amdahl's law which is not the best
one by the way that fourth processor got
me netzero it got me about ten percent
of a processor $250,000 buys me ten
percent improvement and everything that
you're doing is going to have some level
of parallel and efficiency so it's
important that you look at your data you
look at your algorithms in your code and
you try and figure out where useful
parallelism ends because throwing
another hundred nodes of Hadoop cluster
or Cassandra cluster or teradata at
something won't solve the problem if the
algorithm or the query can't take use of
it efficiently and so if you want to
know more about this the high frequency
trading talk that's this afternoon that
Martin Thompson Tom Thompson Thomas does
is is really good because he gets into a
lot of the theoretical constructs but
he's talking about it from the
perspective of messaging and queuing
systems and as it turns out it's exactly
the same in principle so I want to give
you a real life example in case you
don't believe me ebay six petabyte data
warehouse running in a standard teradata
20 bit of i compressed obviously Hadoop
cluster and then this thing called
singularity which is a a teradata
appliance model database it's got the
really big slow disks in it that has 40
petabytes in it now I've had the
opportunity to actually run a couple of
queries against this thing in a trillion
row table with about a 40 to 50 billion
row return set filter on this takes 30
seconds
this is the important thing though that
Amdahl's lies about resource efficiency
and parallel efficiency sloppy code and
just add nodes is not efficient we
learned this with symmetric
multiprocessors you know multi-core
chips back in the 90s even in the 80s if
your code or your system that underlies
your code is not efficient that is it
just it wastes resources saying doing
type conversions every time you read
data as opposed to doing the type
conversion once before you write the
data slows things down so this first
graph right here how much do things cost
so normalized costs Hadoop damn cheap
right so you that that's a per-unit cost
of one right the teradata appliance and
the big expensive teradata nodes
ridiculously costly like 30 X what
Hadoop costs look at this one then you
know how many units are consumed and
what we're talking about is the kind of
things we do a lot of sorts aggregates
counts groupings you know select these
things group them in this way aggregate
these numbers and give me the results
big ugly bulk moving parallel problems
and the data warehouse is
extraordinarily efficient this thing not
quite as good and of course that thing
horribly inefficient why because of
things like read 64 megabytes in to pull
four things out of it the inefficiencies
at scale bite you in the butt and so
when you look at what does the units
consumed crossed with the cost look like
normalized well that big expensive
multi-million dollar teradata that has
six petabytes in it costs the same as
your Hadoop cluster and yet equivalent
this thing right here which is modified
for bulk data operations and bulk data
parallel that's about a third the cost
of your Hadoop cluster for the same set
of operations this blows a hole in the
argument that just add hardware is the
right model to go in the Big Data world
but for which operations right for
parallel count sort aggregate operations
if I
doing calculations and bolt calculations
that didn't stream through that but
iterated this would be the better option
probably and so you have to begin to
understand what your workloads are like
because if you don't you'll make the
wrong mistake because we're in a
multi-engine world now and so yes this
thing is great for these problems but if
I have a different set of problems I've
got a different set of solutions so
workloads are key and there are three
kinds of workloads I started off talking
about application workloads and writing
data and reading data and acid
transactions and consistency that's one
set of systems um brief aside I had with
the guy talking about Pinterest is that
he said oh I needed acid transactions
like your Pinterest people upload
pictures why would you need this and he
said we get dmca takedown notices if you
forget to file a dmca takedown notice
that somebody has sent you you've just
broken the law and the lawyers come
after you so even simple web sites that
seem not to need things like acid
transactions in certain cases do so what
do you do well you could have a system
that records that stuff in acid
transactions while your web serving
loads that can lose stuff and it doesn't
matter or can be inconsistent you can
write to do two different services or
you can go with what they did which
charted my sequel but that's an
operational system analytic systems
which is what I spend most of my time
working on the query systems big
analysis systems and scientific
computational systems big data
processing computationally intensive
things they're all very different the
natures are different so how are they
different these are the key things I'm
not going to go over all of this you can
download the slides if you want to look
at it in detail but basically
distributed commits problems with
consistency making sure that I can't
take more money out of the system than
you put into the system scan based
selectively things and a key aspect of
these systems and what they're good at
is whether the selectivity and retrieval
are low or high selectivity is how much
data do you actually filter out down
here retrieval how much data do you pull
back whether it's into your processor to
a client over the network
analytic computations read one hundred
percent of the data process one hundred
percent of the data and typically put
either a small set out or an entire set
out these things usually summarize they
don't return lists they return
aggregates that's why bi databases
actually have optimized so well that's
why that singularity system can do that
where Hadoop can't this sort of stuff
now you're into a world where if you
need ask that you need consistency you
go one way if you don't you have a whole
new range of options that you didn't
have before and so you need to think
about those workloads and think about
you know performance / size versus
performance over complexity if you're
interested in that unfortunately you
just missed a fantastic talk by cliff
from hex data but you can watch the
video because it's being recorded and
that's all about you know very important
things here for example in databases you
would think disks are the bottleneck io
channels are the bottleneck they're slow
and I'm getting data into memory in
memory solves all problems which is what
s AP would tell you with Hannah doesn't
work that way if your algorithm is not
smart about how it moves data through
the processor pipeline you spend upwards
in a relational world of sixty percent
of your time on level one level two
cache stalls you wouldn't think about
this but if that's where you're wasting
all of your time more resources won't
help you more memory might help you but
unlikely because the problem is that
your memory is not feeding your cash
fast enough you should be able to do
that why can't it do that because the
way you move the data into memory and
process it is not optimized for that
pushing that into engine solves that
problem that's one of the things hex
data does dynamic does vertica does and
so you know when you think about these
problems we've kind of come as an
industry full circle from the the early
era of we have no sequel which is where
i started to know you really need this
stuff to the 2000s when everybody said
no no sequels bad memcached memcache
will solve all problems he'll just use
key value stores for everything to you
know Mongo and Cassandra and everybody's
saying we don't need no stinkin sequel
and then they all added sequel like
languages back in but they called them
HQ
or c ql or whatever and now we're back
with you know the Google paper and other
things and post relational systems like
meta not extended relational they have
kept that model and the declarative
added new operators and made them better
and relax some of the constraints so
where we are is there is no such thing
as a free lunch you're always making
trade-off decisions and you will inherit
technical debt and you have to think
about the data infrastructure because
it's so hard to change once you've built
an application on top of it no your
trade-offs up front and understand some
of the where's and why's of these things
and avoid the unintended consequences
you installed Mongo and then you didn't
realize the tight coupling of data
structure and schema to physical storage
led to a whole new set of problems this
is my favorite example I use it a lot
for unintended consequences which is
that back in the 1950s atomic power was
all the rage and atomic power was going
to solve every problem society had just
like no sequel today and so radium being
such an easily available thing we could
paint it on watch dials and radium is
radioactive so we'll put it in chewing
gum to give you extra energy women's
makeup had a radium in it for that extra
healthy glow and my favorite example of
all the only one that works the the
radium laced prophylactic so condoms
with radium that actually would prevent
you know not just the current pregnancy
but every possible future pregnancy
those are the unintended consequences
that you can you can inherit when you
make the wrong technical implementation
decision now i'm sad to say there's no
one answer you know clutter and
Hortonworks would love it if you just
bought Hadoop and threw everything else
out but that's not going to work oracle
would love it if you just bought an XO
whatever and paid them eight billion
dollars but that's not going to work
we're back to Best of Breed
architectures for a whole generation my
generation we thought the relational
database was the answer but it turns out
that there are multiple workloads with
different answers
and that's the world that we're getting
back to and so you know don't make these
these mistakes that some of us made that
we had to unlearn because I spent the
past seven years unlearning a lot of
stuff which is that you really have to
say what is the problem for which this
technology is the answer number one and
number two more important one what's my
goal and how does that translate into
the capabilities that I need and how
does that map onto those technologies
and finally there are no best practices
right now in this world there are some
good practices but mostly best practices
come from survival bias and survival
bias is the people who made it through
the minefield and it's great if you can
follow them through the minefield and
take three steps to the right and two
steps forward and one step left but
every company is different every
engineering team is different you're not
starting from the same place that that
guy started from which means that if you
follow his exact pattern of steps you're
likely to step on a mine and that's what
we're you know we're learning in the
emerging tech world now and so really
what you should focus on is a lot of the
worst failures you know what was the
real story of why Mount gox failed we
may never know but they blame it on
transaction malleability we know what
that is and we know we knew what it was
in 1972 so there are solutions out there
look for those failures and study the
pattern of those things and you'll learn
a lot and so that's just the summary of
the course and with that I'm finished
and you guys are all free for lunch
thank you very much for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>