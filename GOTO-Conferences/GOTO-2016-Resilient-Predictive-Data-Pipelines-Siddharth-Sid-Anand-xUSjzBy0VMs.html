<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Resilient Predictive Data Pipelines • Siddharth &quot;Sid&quot; Anand | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Resilient Predictive Data Pipelines • Siddharth &quot;Sid&quot; Anand - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Resilient Predictive Data Pipelines • Siddharth &quot;Sid&quot; Anand</b></h2><h5 class="post__date">2016-10-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xUSjzBy0VMs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">um first of all thanks for coming I know
most of you are stuck here because your
flights are tomorrow and the people who
could leave took the train home but I'm
just joking thank you very much for
coming to this talk resilient predictive
data pipelines just to get a show of
hands how many of you actually work on
data pipelines ok so like looks like ten
percent not bad so a little bit up about
me I work at a small software startup in
samma-tee-oh called agari and before
let's say about a year ago I worked at
LinkedIn Netflix ebay and etsy and that
those companies i work in roles mostly
dealing with scaling and scaling of data
i'm also a co-chair for q con san
francisco and in my spare time i am a
committer on a new newly incubating
project called air flow apache air flow
and i also report to a little three
nature and his mom in that picture below
so you might be asking yourself why is a
data pipeline talk in this always
availability are always available track
so mostly you know always on work has
like traditionally focused on the
availability of serving systems and
serving systems are like typically
synchronous or semi synchronous by semi
synchronous i mean there's some sort of
asynchronous aspect to it but it's
usually time-bound they're often
transactional and their most often
latency sensitive and why is that these
tracks not represented more of the you
know the data pipeline community and
it's mostly because you know outages are
big news right in 2007 it was a little
bit of a badge of honor I around the
time in the bay area for your site to go
down because it meant that you had a you
know the thinking was that you had a lot
of traffic and your site went down and
you know I think given one of the
pitches that AWS made at the time was
you're a startup you can't afford all
this infrastructure just for being on
techCrunch's front page for you know two
days so use ec2 to scale up
and the reality was in 2007-2008 cuz
nine a lot of sites that are stable
today we're not so stable back then and
they actually thought about their down
page like they redesigned it many times
to have the coolest my site is down page
and you know the problem with that is
after a while your site being down
actually becomes your brand and for the
first few years of Twitter people really
recognize the fail whale and how many of
you have used etsy raise your hand if
you have or heard of it okay so um does
anyone recognize who this is raise your
hand if you'd recognize now so he's one
of the founders his name is Haim and he
was the like DBA / sysadmin for at C&amp;amp;S
he was going on quite a lot around 2007
and then it became like a cultural
movement in the community of makers to
craft stuff about it being down and it
became like one of the popular
categories on etsy now as I mentioned
you know this track is usually about
synchronous systems or in a semi
synchronous systems but I would say that
it's more important to most of the
modern web sites that we start focusing
on data flow systems because data flow
systems are arguably more important to
most online businesses today and and you
know they have these features that
they're asynchronous and they're not
really latency sensitive but they are
throughput sensitive and and why do i
make the statement right so if you go to
your favorite sites right for example
netflix on the top left or even spotify
there are data pipelines that are
building personalization models to to
predict what you'll like to watch or
listen to and keep you engaged with the
site and in the case of things like
Pandora or I should not really kind of a
most like Netflix that have a
subscription model that leads to churn
reduction right because they don't want
people leaving leaving the site for
social networking sites like LinkedIn or
Facebook you'll often find those people
you may know widget right and it's super
important to you to these social
networking sites to continue
you to grow the edge of their social
network and for example in the case of
of LinkedIn right linkedin makes sixty
percent of its money from recruiters
finding you and something like seventy
percent of the profiles had nothing
beyond the name right so by adding more
of your friends like Lincoln hopes to
shame you a little bit into filling out
your profile so that recruiters will
find you and and then you know they can
sort of get more you know profit from
from that from that side so it's it's
very important that you know the network
grows for these sort of companies on the
upper right you'll recognize amazon and
the people who bought ex bought y and
people bought x and y together and again
you know lots of clicks and purchases
happen through that route for amazon and
lower down below ebay so seven years ago
i actually built help build the ranking
algorithm that's still the default on
ebay called best match and what that
essentially does is it will rank results
by what's being purchased like right now
or in the last ten minutes in order to
prioritize like make make it a higher
probability that what you click on will
actually get bought and linked to look I
mean II VA will get a larger chunk of
that profit so all of these businesses
you know depend on these pipelines and
in an e base case it's like a live
pipeline or whatever you're buying
watching bidding on that sort of stuff
and it's like changing the ranking order
of the results on the fly so I hope I've
made the case why you know it's
important to have an always-on data
pipeline and in a modern systems today
it's slowly architectures that that
provide both provide a serving
architecture that's always on and a data
pipeline architecture that's always on
and this is sort of a design you could
say for these systems on the left you've
got the typical serving like stack or
serving architecture you'll have fun and
load balancers followed by web servers
followed by a micro services layer then
you'll have some sort of data layer
which is essentially databases
seek search Eric search engines caching
machines graphs all of that sort of
stuff and the one mistake I feel it's
sort of like a stumbling point that
every company's what it makes is you
know they start hiring couple of data
scientists and maybe you've seen this in
your experience and then you know the
data scientists start running these
expensive scan queries on the oil TB
database and it soon becomes like a
problem availability so you know as like
a step forward like LinkedIn maybe seven
eight years ago had had decoupled
they're like oltp database from all
their analytics and and product like
like data product stuff and essentially
there was a data integration layer at
that point at that time it was something
called data bus now it's Kafka
essentially by separating these two
architectures you can you can keep both
of them available and that's sort of the
goal of of this of this slide so what
are some of the challenges you know with
keeping a data pipeline up I mean you're
probably very familiar with keeping a
high availability side up right avoid
like single points of failure that sort
of stuff but what do you do for data
pipelines so one of the problems is
called a blast radius problem so let's
say you have a someone in your jaw in at
work who writes some code for a data
pipeline job let's call it job one and
job one reads data a and writes data be
and say your developer introduces a bug
there's another job called job to that
reads that data be and writes out data
sea and I think you get the point
there's another job that reads that
output and input and writes another
output and finally some output makes it
to a serving system because at the end
it like LinkedIn widgets have to surface
some of that data and that's probably
where you're going to catch the bug and
if it's like a recommender system you're
probably not going to catch it for a
long time I remember walking along the
halls and Netflix a few years ago and
ran into one of the VPS he said we found
a bug I'm like great it's like it's been
around for two years I said yeah well
that's recommender systems there are
probably many bugs you don't even know
about and essentially you know
the reason this is called a like a blast
radius right is that a bug that was
created in an upstream job corrupted
data downstream and it affected a huge
swath of services and what I show it was
just one path but in reality there are
many jobs that depend on the output of
the first job and so forth there many
jobs that depend on the output of that
that line of jobs so it's actually a
whole bunch of jobs in a pyramid that
are essentially affected by corrupt data
at at the nucleus so why is this an
acute pain point in in companies like
LinkedIn Facebook and such well first of
all you've detected the bug right so you
the next step is to go about identifying
the cause so first you look in your like
serving database and say okay here's
this data is bad what wrote to this data
oh this job let me go and look at this
job let me look at its code let me look
at its input and out its various inputs
and then you get into this situation
which is like awesome if you're my
three-year-old boy but it's not so much
fun for me right lifting every rock and
looking for the bug and eventually if
you still have a job and your sanity
you'll come to the point where you
identify with the bug is right yay we
found the bug so now what do we need to
do we just need to deploy a fix but wait
that's only going to fix data going
forward and this bug has been around for
weeks so it's probably corrupted a whole
bunch of historical data and I need to
find out who to contact so let me
contact all the teams that own all of
the jobs that care about this output and
then they need to do the same for
everyone that uses their output and
basically this is what you get into
right you're hurting tasks you're trying
to figure out what's affected how do you
let everyone know all of that sort of
stuff and they all run on different
cadences so it's bit of a pain so
essentially after all of this is done
you've rerun all of the jobs all over
data is clean and then the next bug
comes right so very quickly in this
field you learn that there's a there's a
high cost in people time and morale
people get burnout so what can you do
about it right someone will come and say
I have a great idea let's let's do more
testing right testing is going to solve
this problem
testing doesn't always solve this
problem actually the in in reality it
more often than not doesn't solve this
problem because in data pipelines bugs
are either in the logic or if you
haven't changed the code for like a year
it could still break because something
in the input changed and the data is
typically semi-structured or
unstructured and for you to mimic all of
the ways it could possibly go bad to
create a test corpus and maintain that
test corpus it's a huge investment so a
better option is more like detect that
there's a problem and roll back or fix
forward like an automation solution so
what you really want is at any stage of
your pipeline to be able to detect it
there's a problem that helps you
essentially identify what's wrong and
then fix it forward or roll it back in
an automated fashion another problem
with data pipelines has to do with
timeliness so imagine that I have these
three jobs that I talked about before
job one job to job three so the output
of job one is going to be read by job to
the output of job tools may be read by
job three and job three is going to push
something to a serving system and in my
definition here a job is equal to a
workflow and these are directed acyclic
graph of tasks now let's say I'm going
to run this on a daily schedule so I run
it on Monday I run it on Tuesday I run
it on Wednesday and everything that's
fine Thursday let's go to and hey it's
Friday it's time to head out to the bars
right but something is not looking great
because my first job is taking double
the time and then it's the weekend
you're with your family and all of a
sudden job one is taking 3x the amount
of time and you're getting paged and
what was supposed to be a daily job
doesn't complete in the day your auto
vessel a and you have to go in on Sunday
and fix it because if you can't finish a
day's job in a day you're never going to
catch up right so now you have a problem
so the question becomes why do jobs
especially the big data space gets lower
right why do they get slower so I'm
going to give you in an actual example
so what you're seeing here is time like
performance of
as overtime so the x-axis is about two
months of data at hourly points the
y-axis is the task duration the time
durations expressed here as a fraction
of an hour so we're running an hourly
job in this case so essentially what
you're seeing is a large variance of one
of our jobs this one in maroon it's
actually a spark task and the spark task
it's actually smart job or task in our
dag it's it's taking up to a more than
half an hour to complete so why does
that happen so it turns out in the first
week of January one of our data science
folks started computing some new
features and that's perfectly acceptable
because that's what he's supposed to do
compute some new features for some sort
of prediction model but what's not okay
is over the next four weeks it starts
getting progressively slower and it
turns out that his model building or
future building code was looking at an
increasingly growing window of data
instead of like a sliding window of data
and as a result it was doing longer and
longer scans and it was just taking
longer to complete so he fixed it and
then he made a couple other bug fixes it
took a few weeks but you know we're all
in good territory and we got back down
to 9 minutes and we all took a deep
breath and like they push new code five
days later and we're back to it where we
were before and essentially there's a
regression once again and if you've
worked in the web space you would have
noticed over the last few years right
this tension between ops and and
developers right where devs are like
writing code and then also have to
maintain it getting burnt out and then
there's like this like kind of tension
between them well that sort of thing is
playing out in the data space you have
data scientists you know these guys from
like academia like check out this new
algorithm I'm going to run it and then
the data engineers are like why are you
doing this to me I'm like so tired out
by now so it's it's actually coming to a
point where people are realizing this
and data science and data engineers are
forming single teams of call like data
science and engineering where they can
work together because the effect of a
new algorithm will be a regression and
there's
a virtuous cycle that we should all
accept essentially that you will compute
new features stuff will get slower your
data size will grow stuff will get
slower then stop look back and try to
like trim it optimize and then continue
and that's essentially the virtuous
cycle in in the world of big data today
and also latency matters a bit right
because we're a throughput based system
but if you can't finish yourself in a
day you're in trouble so given these you
know to kind of problems I've talked
about one has to do with correctness
having a large blast radius if you have
an a bug that affects correctness it's
going to affect a lot of jobs downstream
and also timeliness right um what sort
of desirable qualities or design
criteria can we come up with so we have
for correctness timeliness operability
and cost and first cause something I
threw in because I'm gonna start up and
we have finite amount of money so
correctness is all about like data
integrity don't lose data don't corrupt
it don't duplicate it if you're doing
predictive stuff the output distribution
shouldn't change too much time unless we
talked about operability is about
putting in tooling that does fine grain
monitoring and alerting we also need
something like quick cover ability and
for cost we want to build something
that's pay-as-you-go we don't want a
bunch of standing infrastructure that
we're paying for when we're not using it
so I'll make one point here how many
people have seen this before this sort
of raise your hand if you've seen this
specific graph or this image yeah um so
I actually saw this at a talk and a guy
from etsy gave many many years ago it's
about MPT r &amp;amp; MTBF so m TT RS mean time
to recovery MTBF is as you've heard in
this hurt today it's mean time between
failures right and you know Rolls Royces
are built slightly differently from
Jeeps so jeeps are built to be abused
right there owners abuse them out of
love of course but it's still abuse and
they break down a lot right but they're
also super easy to fix and they're
designed that way they're designed to be
used out
side of their design boundaries a
rolls-royce works great on the country
roads and in front of hotel but if you
take it up a hill it's going to spend
like two months in a shop so the idea is
how should you build your website or how
should you build a data pipeline should
you build it to never feel like have in
like a lot of tests to catch every
possible problem that can happen or do
you just roll with it write a bug
happens it's in production you hit a
button you roll back immediately you
have all the tooling in place to recover
quickly and the point I'll make is data
pipelines in the past have been treated
in the NPPF way and you know coming like
I personally came from the high scale
web world I actually believe mttr is the
way to go and if you go online there's a
youtube video of a set of Marines taking
a jeep apart and putting it back
together and four minutes so it's quite
impressive so how can we use some
commonly available tools right to meet
our design goals what are some things
available to us and and available to my
company is basically amazon so we we
started looking at some of the
technologies so just to get a sense of
the room how many people here use Amazon
Web Services that's excellent how many
of you keep your hand up if you use SQS
oh cool ok very good so it looks about
like half of you actually use amazon so
SQS for the others is a low latency
highly scalable hi available message
queue it's a queue but it's not exactly
50 and it's pull based so actually so I
think I'm on this side up right so
here's an example to teach you sort of
how it works so imagine that you have
producers and they're producing messages
message one through five on this queue
and you have some consumers that are
waiting to consume these messages so the
first thing that happens is a consumer
will read a message from SQS and as soon
as it does so SQS will make that message
invisible and they'll start a timer then
the consumer in question will write the
message or commit the contents of that
message to a database and finally it
will
the message which will result in
deleting the message and canceling the
timer and this is how it works in normal
operation now imagine the same scenario
where a consumer reads this message and
it takes a bit of time to write the
message to the database during that time
period the timer goes off and the
message now becomes visible and
available for some other consumer to
read it and so the consumer the bottom
picks it up commits it an accent so this
is these are like the semantics of SQS
based on this model you can actually
implement transactions quite well
because as long as you're right to the
database or idempotent and one
interesting feature of SQS is this thing
called a dead letter cues that that was
mentioned earlier in a dead-letter queue
essentially is so imagine I have this
message one and there's a problem with
it and my code can't parse it so rather
than have message one clogging up the
queue you can configure SQS to bounce
the message into a dead-letter queue
after n tries and the benefit of this is
that you can maintain high throughput in
your queue and you also never lose a
message and your code itself doesn't
need to handle the transactional writing
of this message to another place which
could have its own bugs SQS has handles
it for you and similarly you can have an
alert enabled on that cue so that
whenever the dead-letter queue has
anything in it you're paged and then you
know there's some sort of bug you fix
the bug and then you drain the queue and
you never lost a message another thing
that we use is SNS and again a show of
hands who uses SNS here so about 78
people may be kind of great so SNS and
how many of you were the CAF guitar
anyhow Kafka talked yesterday okay so um
this is similar to Kafka except it's
push-based it's it's also a topic but
it's push-based rather than pull based
so the benefits of this are you can when
you publish to SNS it will reliably send
it to all consumers unlike s
yes ask us will only send it to one
consumer but the downside is it will
make some number of attempts to send it
and if it can't the message is lost so
is there a way we can leverage this
beautiful like transactional multi push
feature with what SQS had which is the
ability to reliably save and write data
and there is am i off by one slide I
think I actually I'm off by one slide
alright so this is actually the pattern
that that we have the the idea is
basically that you have a topic and the
topic in this case is an SNS topic and
you have two messages in it message one
and message to and you also you can
amazon allows you to register two
different s q SQ s or multiple SQ s qs
as subscribers to a SNS topic so in this
case we have two SQ s qs subscribe to
the SNS topic and whenever we publish
messages do the SNS topic its reliably
multi pushed to the SQ s qs then we can
have services downstream consumers
downstream of each SQ SQ to reliably
write the data I mean you read the data
and committed to a back-end to drive
this home let's say that we wanted all
of our data to be written to an elastic
search cluster oh and also to our like
Cassandra DB we can't really do that in
a consumer in a transactional way but
one way we can do it is through this
model we have producers producing
messages to us and s topics they get
pushed to each of these two cues we have
separate consumer groups the purple
group and the green group the purple
groups responsible for elasticsearch
updates and the the green group is
responsible for like a standard dbi
print and this model like works really
well for us so now let's put these
different sort of technologies together
to build an actual fault-tolerant
pipeline but before I go there I should
give you some context about what agari
does so first of all we don't steal your
passwords we try to prevent others from
doing so and we've been operating in the
consumer space for about five years so
like eighty percent of consumer in boxes
at like gmail hotmail yahoo offer g65
are protected by some of the technology
that AG re-built you won't even see a
fish email in most cases because it'll
be filtered out by the inbox provider we
are now focusing on like solving this
problem for the enterprise space and
what that means is we we put these like
collector like appliances in each of our
enterprise customers data centers and
every time they receive an e an email
they send us the headers for that email
what we do is we use a trust model to
analyze the email to determine if it's
fish or not and now this is not a spam
problem it's like we're looking for fish
fish it's something more malicious and
then we score the email and and then we
you know our current product shows it in
a web UI our future product will
actually be in line and eliminate such
email from getting through so let's put
this together in an actual framework
this is our current batch pipeline later
or not we talking about a real time
pipeline but this is our current batch
pipeline so what we currently have is
these collectors in the in the in the in
the wild they're sending data to s3
every hour or so we create an EMR spark
cluster which is an elastic MapReduce
par cluster we spin it up we suck in all
of that data for the hour we will score
it and also generate some statistics
write it to a different s3 path that s
three paths will trigger off a series of
SNS and SQS alerts or events and we'll
have what's called an auto scalar which
starts off with the size of zero spin up
and start rapidly ingesting that data
into our database so that the web UI can
see it so there are some benefits to
this model let's first talk about how we
solve the the cost and timeliness
problem so
what I just mentioned happens during
runs but between runs we don't pay a
dime I mean what we what we essentially
have running between our hourly runs is
just an s3 bucket where our customers
can continue to send data and we also
have a database at Postgres database
that's sitting around it has data from
the previous runs and a web UI that hits
it and we also have a one box that hosts
air flow which I'll talk about a bit
later so let's say it's the end of the
hour and it's time for the next run air
flow will spin up EMR and it'll execute
that whole thing where everything gets
spun up just for that in jest so this is
sort of how we tackle the cost problem
what about timeliness so I've kind of
mentioned auto scaling a little bit so
again I'm going to ask the crowd how
many people use auto scaling here okay
three four okay cool so yeah we use auto
scaling an auto scaling is essentially
Amazon solution to cluster management
given some rules it can scale a cluster
in or out a variable size to deal with
variable load so it's typically good for
feeds i getta feeds it can also be used
to just keep a cluster of fixed size up
all the time so that's another use for
it and as I mentioned we use it for for
this case so one of the challenges with
using auto scaling is picking the right
metric for scaling in and scale and
scaling in and out so our first pass at
this was to use cpu we thought let's use
cpu on the importer box to tell us
whether the cluster is overwhelmed and
needs to be scaled out or if it's
underwhelmed and underused and should be
scaled in so that was our first pass at
this so I'll explain sort of what's
happening here so we have an orange
graph and that orange graph is
throughput of messages landing in SQ s
right these are like published messages
per second and because there's nothing
in the auto scale group the green curve
which is consumption of those messages
starts off at zero and slowly starts
ramping up
blue curve which is like right at the
bottom it sort of looks like a little
line that's CPU but really think of it
as a proxy for the size of the scaling
group that's essentially what it is its
total cpu so it's a proxy for the size
of the the auto scaling group so what's
happening is the auto scaling group is
growing right it's expanding and as it
expands the consumption rate starts
increasing and eventually it crosses the
production rate of messages because
after a spark is done essentially
there's at some point there's nothing
left to produce and then it catches up
and drops down right the green curve
drops down so and the curve at the
bottom it shows sort of like a little
Mesa that that's like the effectiveness
of our auto scaling so what we're doing
is we're using forty percent CPU average
forty percent CPU as the signal for
scaling out so we tell aah trees if the
CPU on average in the cluster goes above
forty percent add more machine to the
cluster and when it drops below
something like ten percent or five
percent then start scaling in because we
consider I think five percent noise now
this should work right but it doesn't
and the reason is sort of shown here so
what happens is when you get just a few
messages left on the SQ SQ that are now
in flight and being processed ninety
percent of the machines are doing
nothing and their CPU drops to noise
levels and then the average drops below
the five percent trigger and then all of
a sudden it aw sends the ASG scale in
events it says start scaling in but it's
not smart about which one percent of the
machines are currently doing stuff is
just killing machines like this and the
machines that are actually working on
the last five messages never complete
essentially there's a you know because
they're being scaled in randomly without
any knowledge so there are a couple
machines that keep trying to get the
next message from SQS like okay I'm
going to process it and the autoscaler
tells it die right so we had to find
sort of a better we means to do scaling
and the more obvious thing to do was
actually to use the queue depth so if
you remember from my earlier sort of
design the messages can be visible or
they can be invisible so we what we do
is we use two separate scaling triggers
we use a scale-out trigger based on the
number of visible messages in a queue
and we use a scale in trigger based on
the number of invisible messages in the
queue so what does this mean in practice
so as messages are added to the queue
the number of visible messages grows and
we just keep growing the cluster like a
balloon until there's nothing left in
the visible Q message queue and then
we're at fixed fixed size and we just
start consuming all the messages and at
that point the orange curve which is the
number of visible messages are all at
this point they've all been consumed and
then we're waiting to write them to the
DB once that goes to zero and there's
that means there's no message left to be
consumed and we act there's no message
left to be act it's safe to scale in we
scale the whole cluster in and this
works really well for us so with this
model we can actually figure out both
our timeliness and cost we can actually
solve those two design goals I will
mention one thing with EMR spark we can
actually predict before the run how many
spar tree saw resources we need for that
next run based on some metrics so we can
actually scale it out to so that it
doesn't run into bottlenecks but if you
want to know more about that you can
come and talk to me how about
operability and correctness so for
operability we were essentially looking
for some sort of automation that can
manage all of our workflows we wanted
something that was easy to manage easy
to author like we want to author
workflows we wanted to get visual
insight that could tell us about the
state of our workflows and the
performance and the performance and we
also wanted something that integrated
with all our existing monitoring and
alerting so we found Apache were airflow
actually back then it was called Airbnb
airflow and just very recently it became
an Apache project so we started
leveraging that and essentially what you
get is this is my this is not meant for
you to read but this is a sense
a workflow written in Python and
essentially you write your workflows in
Python before this at like LinkedIn we
use azkaban which was essentially a zip
file of a bunch of cause if of a bunch
of con files config files and uzi which
was used heavily at Yahoo was like a
gigantic XML file and those are horrible
things to deal with and a few years ago
a Spotify came up something called Luigi
which is also based on Python but the UI
was not very good and is also not very
stable so when I saw this a pop up about
a year ago I was like very excited and
then when your code is parsed it'll
actually show you the graph the dag the
workflow for it and the the workflow on
the left is actually used to manage the
the whole pipeline on the right and you
can manage multiple DAGs very easily we
have something like ten most of them are
model building and you can see for any
given run what the slow parts of it are
right you don't need to dig into it it
just it shows you and in this case it
tells us I run a given run to 20 minutes
and the slowest part was the spark job
that took like 10 minutes and you can
also see as we saw before a historical
trending for all of the tasks in a given
dag given workflow on the alerting side
we use like we use a combination of
pagerduty and victor ops both solve the
same problem if there's an issue on the
side it'll page the first on call and
try a couple times and then start
escalating to follow on on calls it
integrates well with air flow so we get
a certain types of alarms like if the
workflow is slow it misses SLE or it's
encountering data loss and when it does
that it'll page us and more
interestingly we integrated with slack
what you see here is the you know
airflow telling a slack that you've
missed your time SLA you missed your
time SLA and then further down on it
says hey this run finished but it lost
data and there's a link in it you click
on it and the link will take you right
to airflow and show you hey there's a
data loss that's happening right now
and it works really well so with this
combination we're sort of able to hit
all our targets that's sort of our bash
pipeline the next sort of thing that
we're working on is our stream
processing pipeline so this must look
kind of overwhelming I mean it's
actually funny to have like the title
that says you know architecture and then
show something like I threw up on this
slide and depending on how all your kids
are you may recognize like Richard
Scarry's like Busytown and basically
looks like a busy town design but there
is some you know no there's something to
this madness and actually it's a pattern
and the pattern is that we essentially
have Auto scale compute followed by
Kinesis which is sort of like Kafka so
anything that does compute will read
from a kanisa stream do some computer on
it and will be auto scale so that we you
know its scale for compute and then
they'll publish its data downstream to
Kinesis and then something on the other
side of kinases will read it and do the
same so essentially everything is built
of these repeatable pattern of compute
with Kinesis in between and the
computers auto scaled and we call this
elastic stream processing essentially
because Kinesis and and the compute can
be elastically elastically scaled and
i'll talk about that a little bit a
little later on and then there's this
other thing on top called a schema
registry so how many of you have heard
of a schema registry before especially
if you've used Kafka you would know
about this awesome all right so I'm
going to talk a little about a little
bit about a bro how many of you know
either Admiral or protobuf or thrift
awesome okay so a lot of you so we use
Avril um so just like you have data in a
database right and that data is
protected by a schema and some sort of
schema enforcement what about data in
files or data on streams that should
also be protected by some sort of schema
and Avro provides so do the others it
provides typed types cardinality
optionality and also does nesting which
is even better than most databases to
you
and it also supports is very cool thing
called a schema evolution so you're in
our case we have ten customers that all
have different versions of our collector
they all upgraded at different times and
they're all sending slightly different
schemas to us like the data is coming in
binary but with slightly different
schemas and then we're pushing code on
our readers like at will and you know to
one of the best practices is this
decoupling model right so we have q's to
decouple but then they would be coupled
by schema and we want to be couple that
even further so data coming in is we use
this concept called schema resolution
which will take a reader and writer
schema and figure out what's common
between them to be able to read any
input that we can so this is kind of an
example of a bro ok did you guys not see
that all right so this is an example of
a row so it's a self-describing schema
it's your anization format and as i
mentioned it has types and nesting now
typically when you're working in sort of
do you store the schema that the data
was written written with with the file
and and that's because the data files
are so large the schema overhead is like
tiny but in the world of streaming the
overhead is like ninety-nine percent of
the data you're sending so it's not a
good idea to send the writer schema
appended on the top of the actual data
packet that's being sent so instead the
thing that LinkedIn came up with many
years ago was to strip out the schema
send the payload without a schema but
just an identifier and then on the
consumer side to look up the schema use
that to decode the binary and the only
problem with that is it's very tightly
coupled to the kafka project so what we
did is twofold we decouple it from the
kafka project and we also made it a like
something that works perfectly well in
the cloud and essentially we're going to
open source it in a few weeks it's it's
basically a Gradle script that you can
use to publish a zip file which has two
lambda functions in it and the lambda
functions either take a register
FEMA call or a get schema by ID call and
whenever you call the register schema
call it will validate your schema to
make sure it's backward and forward
compatible both as a reader and writer
schema before it accepts it and you also
pass it on the fly in the URL a JDBC
connection URL URI to the database
that's backing it and on every call you
can change it if you wish but in you
know so for example you can pass it a
JDBC URL to a dynamo on that call and
what it will do is it'll it'll validate
it if it works it'll registered at that
URL and then return success to you and
it'll return an ID to you and then that
ID you pass down the stream and the
consumer will read it and look it up and
be able to decode on the fly so we'll be
open sourcing this in a few weeks and of
course for those of you have not used
lambda it's basically the next evolution
of survey lists it's it's essentially
the next evolution of auto scaling
you're given a hosted execution
environment all we need to do is upload
zip in you know python node Java now I
think Ruby and then when you upload it
all you do is you pick sort of a profile
like medium memory I'm sorry medium CPU
high CPU super eyes review and 256 gig
ram or one terabyte RAM or whatever I
didn't exactly know how large gets and
then soon as you do that you get a new
version and the beauty of this thing is
if you have a bug and you want to roll
back all you do is in the UI you say
dollar latest equals v1 and you've just
rolled back your code about your bad
code so it's actually a very useful
innovation and I think I'm going a
little bit over time but i'll just
mention this we're also creating a tool
for elastic stream processing currently
if you use Kafka or Kinesis your
scalability is / shard if you have three
shards you have an exact provision limit
what happens if your producer starts
producing more data so what we do is we
will detect that there's such a problem
and automatically add new shards and
auto scale the compute to handle it
and then detect when it's below
provisioning and do the opposite and
this also uses dynamo behind the scenes
so it'll also auto like automatically
scale dynamo up to the next provision
limit and scale it down both of these
will be open source if you guys want to
follow it feel free to follow these two
Twitter IDs and I guess this really
probably no time for questions but if
you have any oh they have five minutes
for questions there is one question in
from the app and it is what was the VP's
response when you told him there were
many books in the recommendation system
know so um I didn't tell him I was not
from the others from the infrastructure
team at Netflix I name was actually a
mer the recommendations team who told me
and you know I I think there was some
frustration but having worked I've
worked in recommendations for a long
time and the beauty is you never get
bugs and you know if you want a good
work-life balance I would work in
recommender systems I one more question
remember to vote I can say that while I
walk down here can you go back to the
slide that shows the Twitter feeds to
follow I think it's the second to last
second to last oh the Twitter yeah sure
here sorry
right any other comments or the slides
you want to see you all any questions
okay great so I am had a brief moment of
Terror when you showed the using the the
Q offset and lag 2dr auto scaling groups
and wondering what means you have to
prevent say a totally stalled cue from
just creating servers on infinitum are
you watching it are you setting hard
caps do you have stall detection or some
combination so your if I answer your
question when we move to the Q based
auto scaling you're saying how do we
keep so with all the scaling you provide
a min and max size so we take steps so
every five minutes we check a rule in
the rule is is the visible message depth
still greater than zero yes step by four
so we take steps of for up to 40 or
something and we're capped at 40 and
then we scaled in after we keep them all
around until the last message drops off
the the in flight q and then we're safe
to bring it in and we love SQS and SNS
but we couldn't really use it for stream
processing because it'll never stop
right messages never stop and there's
never a scale in you kind of would never
scale in but it's definitely got some
benefits over yeah kinesis good question
right no more questions I think we
should call it a conference and thank
you one more time said thank you and
here come up</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>