<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • Responding in a Timely Manner • Martin Thompson | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • Responding in a Timely Manner • Martin Thompson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • Responding in a Timely Manner • Martin Thompson</b></h2><h5 class="post__date">2014-11-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/q_DCipkMsy0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so welcome to this afternoon and
I'm going to talk about designing
systems that are responsive in this
track and I spent a lot of time in
finance Spears Big Data spears different
things where we need to respond in a
timely manner which is an interesting
part of performance so performance is
typically something that people say but
it means one of two things that place
throughput or latency so the latency or
response time is what this is all going
to be about and when we design our
applications if we don't respond in a
timely manner things don't happen so
well so take this per person here they
haven't reacted quick enough and that's
going to cause a lot of pain a lot of
our applications end up the same way and
a lot of people talk about how things
need to be real time these days I want
to clarify a little bit about what real
time means and I like to think is real
time in one of three wires it's either
what's known as hard real-time so this
is typically what you'll find when
you're programming in to the things are
very very time constrained you must be
so deterministic that if you're not
deterministic it is a catastrophic
failure this example here is a gun that
you'll find on a British warship called
the goalkeeper and its job is a third
very last instant when a ship is being
attacked it has to defend it whenever
all your other defenses have been got
past there's a missile or some plane
coming in and it's really close moving
really fast what stops it it's not a
person driving this it's a piece of
software being guided by radar operates
this sort of gun now if this offer on
this does not respond people die that's
the extreme of real time but many fears
aren't at that extreme but we're at
other ends of the spectrum and one of
the things is like I call soft real-time
so financial markets are typically like
this and in this if you don't respond in
a timely manner you end up losing money
maybe not a lot of money sometimes a lot
of money Knight Capital been a good
example a lot of money but so we have to
be fairly responsive in this and if we
don't we've failed
but more typically this is the sort of
art that most of us end up writing and
we have users who need to use our
systems that must be responsive I call
this the squeegee end of it but it's
really important that we get this right
we get too lazy about this we end up
with frustrated users I'm sure all of us
are a word that if we're frustrated
using an application we don't want to
use it so much more we don't want to
explore it so getting this right starts
to become critical nicely as we get
involved in this subject I've often find
that typically users of these sorts of
systems we develop them and users on
meaning the developers who are building
these sorts of things are in this sort
of situation the building stuff and
they're very much unaware of what's
going on we're hopefully working aware
and we don't know what we're about to
just dig into whenever we go live so I
want to try to avoid us getting into
this situation by giving you some
knowledge for what to be prepared for so
we're going to cover a number of
subjects one of them is how do we test
and measure I'm going to go into a
little bit of theory about what goes on
in these sorts of systems a little bit
of practice some stuff I'm going to
share from a client of mine that allowed
me to share some of this some pitfalls
that most people fall into I'm going to
finish off by talking about some
algorithms and techniques you can take
away because I don't want be leaving
this being all depressed this is all
hard the stuff we can take away the
stuff we can get better at so first of
all let's talk about how do we test and
measure this becomes critical in some of
our systems so let's start off and
imagine I'm Darth Vader I've taken
delivery of a new Death Star how do I
know how it's going to perform well I've
got to test this at this stage and so
simply I'm going to load it up going to
test it by some types of agents that are
going to run against this could be nice
simple means but our world has changed
radically we have automated clients
night it's not just users pushing
buttons sometimes some of our clients
can throw a lot of loaded systems in
finance world this happens a lot so
testing with small numbers or even
individual very large clients that can
throw a lot of loaded our system starts
becoming really important it's also
important to test with a large known
of smaller users that are not doing so
often but just it's another dimension so
we find things like well I'm
concentrated on one point or actually
just having large numbers Breck's or
algorithms in otherwise we're going to
do that in two different ways but really
importantly is how we measure these
systems we should not be measuring from
the low generation side because if we're
looking up here and if these guys are
generating the load plus you're also
using those to measure you get the wrong
results like what happens at their
picking GC pauses what if they get
swapped out the run out of throw a
quantum in the operating system they
start skewing the results so we need a
third party observer to observe this and
it's usually bath by capturing network
traffic now this could be at the extreme
love you buy some really expensive
hardware that hardware will capture
everything and can timestamp all the
pockets and you can later on analyze it
you don't have to go to that extreme
this could be as simple as using
something like Wireshark a TCP dump to
capture the packets you then analyze
them later and you can see the true
performance of your system from a
latency perspective because we're
capturing on the wire we're not just
relying on the low generation units to
do that so so to start burning not in
mind we also gotta set this up in
continuous integration if we're going to
be performance testing we need to be
doing it all the time because if we're
doing this all of the time we then find
out when things go wrong as soon as they
go wrong not later not six months later
or a year later whenever we try to work
out how we fix that it gets really
complicated we learn much more by short
feedback cycles just like we do with the
rest of our job we have to do this with
performance and also let's record
everything if we just sample a system it
hides all manner of ales and here's a
really good example of what I'm going to
capture everything on the system and
store it in a histogram and so what is
the histogram at this stage well for any
given time period i'm going to record
the number of observations i've seen for
that given latency so this example here
is a chart that is got a log scale along
the x-axis and so it's going greater out
in time but the majority of our
observations are
here so typically this system is
responding in less than one millisecond
which is grit but it's also responding
in a much slower fashion further on so
capturing and getting this revisional
image is so powerful so histograms are a
great way to do this so they record the
number of observations at any given
point in time now let's look into hi I
start a station will look at this and we
often use terms like what's the average
response time what's the standard
deviation well this is how nonsense this
can become so let's look at the mode the
mode is the most common occurrence that
we find our mode is there on the graph
that's interesting that's typical it's
easy so one of the few measures that is
kind of useful median so that's much as
we line up all of our observations and
we pick the middle point it's not
actually telling me that interesting
things here this is a really silly one
the mean the average the one we always
talk about that is telling me really
nothing about what's going on it doesn't
tell me the most typical kids and it
doesn't tell me about what's going on
out here it's actually in the middle of
a wasteland telling me nothing useful at
all so don't use means don't use
averages when you start describing your
system and don't talk of it standard
deviation because look at that that is
not a normal distribution it's
multimodal it's very skewed it's got
high kurtosis it's from so many levels
it is not something you want to standard
deviation to describe so capture all
this data and put it in histograms now
let's take a simple example from a
system so let's say I have a system I
want to run it out of thousand
transactions a second and the main
response time a dot is 50 microseconds
this would be a trading system you could
easily just translate this into any
other type of system and skill the
number so that could be five
milliseconds it could be 50 milliseconds
it's just the unit of time but typically
in finance we're looking at around there
so we're looking at 50 micros in this
case the whole looks really good well
what if we're generating quite a lot of
garbage and we're seeing a young
generational garbage collection
happening once every millisecond and
there are every second that not takes 20
5 millisecond pause if you think if
you've been putting them in at one a
millisecond we've got an interesting the
fact goes on here because you put
something in and it's going to weird 25
milliseconds but run that period you're
still in jacked another load so
someone's taking 25 next one's taking 24
next one's taken 23 this continues right
down until you're back at the 50
microseconds level again so 12.5 on
average is what's happening during not
pause you end up with an average mean
latency a three hundred microseconds see
it's kind of crazy you just mix in a GC
pause and all of a sudden your average
makes no sense now Martin yours offering
this to a customer and you could a
service level agreement on it and you're
expecting 50 microseconds and you can't
explain these twenty fives or whether
you're aware or we're not you try to
explain it to your boss you ain't a feel
a bit like this because it's not making
any sense you've been using averages and
it's crazy so forget these averages
start talking about percentiles guilt
anna has left me this graph and it's
kind of really interesting so rather
than looking at our distribution let's
add things up as quantiles are present
i'll so let's have a percentile
distribution and so as we go along the
x-axis we're going up in nine ninety
percent ninety-nine ninety-nine point
nine and we can see what's happening to
our distribution over time there's an
interesting of fact in here so go back
to my example about the GC if you were
not capturing those 24 additional
requests because your system is
generating load is your system that's
measuring this maybe even blocked by
what's going on as it suffers from a
thing called coordinated omission and
without means is the 24 samples that you
should have had we're not we're not GC
pauses going on when you're running at
one a millisecond they're lost that
skews all your results so things look
better than what they are and the real
giveaway in these sorts of things is you
don't get nice smooth curves you get
these jagged jumps and you start
catching things like this you can see it
by looking at the graphs but you've got
to be measuring it to do it so I
recommend you look at a thing called
haytch dee or histogram
and start capturing your data and put
this in this so don't deceive yourself
we've got a manager we've got to know
what's going on we've gotta be able to
read this and if we start doing this we
start behaving like proper scientists
and we won't deceive ourselves so let's
get into a little bit of theory now to
explain a little bit of what's going on
on why systems become unresponsive who
all seen the G a curve typically
response time of systems and I'm looking
something like this where as we increase
load and we use her system more and more
so we increased utilization response
time starts to go up this curve this is
very typical to what's going on there's
a lot of interesting characteristics but
most systems follow this Jack curve
where response gets unresponsive over
time well let's look into this a little
bit why does that happen and how can I
work this out well what we're talking
about if you start looking this up is a
thing called kendall notation it's a
good way of describing this so you
characterize the system for how things q
about it using this notation and what
the notation here is showing you is
three parts it's the arrival rate to the
system which is the M the service time
itself for the behavior of that and then
how many instances of your service so md
one means it's a Markovian arrival rate
the service itself behaves in a
deterministic way and we have one
instance of that service if you describe
your systems in these sorts of factions
you then pick the equation you need to
work out what it's going to do under low
that you increase utilization a rifle
Ritz can be different that can be
completely uniform that could be
Markovian that could be our along that
could be very many other different
options in this but it's a very
interesting thing to look into a lot of
what I'm going to show you in the talk
is the kind of will inspire hopefully
some wiki walks in a bit of googling to
find out with some of these things I
don't spend a lot of time on exactly
what these things are but for an MD one
I get this formula so for this formula I
can work out what my main response time
is going to be plug it into this formula
for the service time of the utilization
and I can then plot that graph the
really interesting thing is what is
utilization well utilization is a
function of service
time again so the how fast our service
runs becomes really really critical to
how responsive it is that may seem kind
of obvious but it's much worse than you
actually think so if we go back to this
graph let's say I'm running here at the
ninety percent utilization point so
normally when my systems not loaded it's
responding there but one unit of time
when i'm running a ninety percent
utilization is running in more than 10
units of time whatever that unit happens
to be now if I take my service and I
profile it and I work out how to make it
faster and I take it down from whatever
it was to being fifty percent of what it
was before your system doesn't have an
average spawns time of fifty percent
better because that response time goes
in the utilization what actually happens
is you come right down this curve to
about here and your system now is ten
times more responsive so understanding
the mathematics behind this is so
powerful and it gives you great results
in your system so relatively small
changes can have a massive impact and
this applies to anything you do anything
that's a service we are a service to the
projects we work on if we're used a
greater than seventy percent utilization
we become unresponsive so there's any
project managers in the room please pay
heed don't try to use people one hundred
percent utilization or even more because
they will not be responsive we have to
have slack in our systems this to work
so we've got to make sure we've got
sufficient capacity this is a key part
and how we work now the original
queueing theory is a hundred years old
but but 50 years ago John little made an
interesting observation on this in that
for any system that since Teddy's did
and that's where the number of arrivals
is equal to the number that are leaving
the system and not nice steady state as
its flowing through these properties
hold true regardless of the service time
regardless of the arrival rates and so
we can easily work art one of these if
we know the other two this is a very
useful property of the system so looking
into the work of
little and littles law gives us a good
where forward and the way I like to use
this is if I bound my cues I can limit
the service time so if I want to have
our response time that I'm going to
always honor if I bind my cue I can then
make sure I don't exceed that response
time my system can stare responsive when
people go well what if you're getting
more load then you've got at that point
will you reject input it is really that
simple you do not want to keep taking
input and as you keep taking input your
system keeps growing and growing and
growing in its cues and eventually run
out of memory and you crush which one's
better this is say sorry we're busy
please come back again later and give a
really good service to everybody's
already arrived or let in more load let
the queues or build up have the system
crash people who are in don't get a good
service nobody can get a good service
afterwards because your systems dined
that's a much more professional approach
it feels we should be thinking these way
so you can unbind it queues in your
system you have a real problem so what's
one of the other things we can use to
speed up well we can go parallel and the
common where people look at going
parallel is um dolls law what's really
interesting about um dolls law is people
think I'm does laws are all been
encouraging parallelism if you read our
dolls original paper you're very quickly
realized he was not encouraging parlor
ilysm he was trying to discourage it he
was showing how difficult that actually
yes he wanted to pedal mainframes he
wanted to sell you one of his big man
forums and not buy one of these new
mid-range systems that had many
processors in them because he was
pointing out that it's actually very
hard to do parallel programming and so
what does argument is about is if I've
got any task I can split it up in some
ways now which bits of it can I split up
to run and parlor so in this case if I
can split up a I can get a nice speed up
if I applied over for course but if B is
the only thing I can split I don't get
sim speed up so it becomes critical how
much if your algorithm you can actually
make run in parallel to make this work
better and so we start
graphing this and we look at what speed
as possible look at you start going up
these cases so even if I just have five
percent of my algorithm that's the
sequential component I can never get
greater than a 20x paid up in fact I'll
never even get to 20 expedia because I
can also topically approach it but I
never get there this makes a really
interesting challenge like if your
algorithm is 50% parlor you're never
looking at two acts speed up no matter
how many cores we throw out this so it
becomes really critical to work out what
is the sequential component of your all
of them and many people don't even know
but do you know what I'm doll had the
happy clappy view of the world it's
actually much worse than that Neal
Gunther discovered from real worlds
measurement that actually you could not
even achieve Amdahl's law from speed up
perspective what he discovered is that
contention point is really important but
um doll haven't talked about the
coherent side of things so you have to
plug in the contention and the coherence
values into your equations to work out
what is your possible speed ups now if I
apply the contention and coherence what
i end up with is with this graph so if i
assume it takes two and 50 microseconds
to make any state i have as part of my
contention coherent among those threads
or processes after a while you start to
find out that the coherence call starts
the dominate and if we look at this
graph it's kind of looks kind of nice at
the two and four near processors but
look what starts to happen beyond 16
processors 32 processors we start to
diverge and then it actually starts to
get worse now we're not in a world where
by we're getting more and more cores and
the light has been shown on algorithms
to the point that these things are
starting to appear and we have to be
aware of that so going in parallel is
not necessarily an easy thing we have to
be aware of what we can do we can't do
them what are our limitations and how
this works so let's come back again to
that so we looked at we can queue on
things we can make things going Paulo
but it's all about the service time
itself because if the service time
itself is not deterministic it gets
really really tricky and this is purely
about algorithms at this stage we have
to go back to basic algorithm theory we
cannot wait a year and get a faster cpu
we can't even with 10 years to get a
significantly faster cpu anymore we will
get more cpus we will get a marginal
speed up but we cannot just do that we
have to go back and learn about
algorithms like we did in the past and
start taking these seriously again so
you've got to know the order of your
algorithms big o notation back to
university stuff again and start
applying this and if you've got
algorithms that are an squared or n
cubed or whatever interesting on input
you're going to get catastrophic
behavior as we start to scale up we need
to stop doing that and that can be as
simple as I go to the database I read
out some data and then within a loop as
I met a really over that I go back to
the database again you just get an N
squared algorithm people need to start
thinking like this and realize what's
going on otherwise we're not responsive
so that's the theory over with for a bit
let's look at a bit of practice and see
what can actually happened so one of my
clients says kindly let me share some
data where I came in for a consulting
engagement over a week and the goal was
to make their trading system more
predictable from a latency perspective
but also lower the latency if we can the
main goal was actually to increase
predictability and latency and how it
happened there's was really interesting
this is these were really really good
folk and they were very good at what
they've done and most of the obvious
mistakes the people neck they had
already corrected you're working here at
the level of microseconds tens of
microseconds for response i know not
everyone works at that but this is where
these guys were what i find really
interesting in coming back into this is
you when you start dealing with really
really good people it's not that you
have to be better than them you can come
in and help people by coaching and i
find that our industry really suffers
from the fact that we don't apply
coaching so often i used to come from a
sports background and i had coaches who
I was faster than them I could do all
sorts of interesting because I was
younger and able to but they could still
couch we can do that in our industry as
well and we just don't do it enough so I
really enjoyed this engagement because I
was able to get these guys to
better just by coaching and following
them through stuff so what did we
discover well we measured their system
on the first day and this is a scatter
plot of the response time of their
system for a whole given set of inputs
and this is where you start seeing again
averages mean absolutely nothing there's
lots of really interesting things going
on here there's a warm-up period over
here there's all of these interesting
spikes and there's a lot of activity
around so it is kind of fascinating to
what's going on so you do this you start
profiling your profile in a number of
different ways you start realizing where
your costs are and one of the first
things we discovered in this is there
was a contention problem with multiple
producers into the system causing thread
contention as I work through the
codebase i discovered they're using the
disrupter they were using an older
version of the disruptor that had a
particular problem with multiple
producers I know that version 3 onwards
had that corrected and improved so we
upgraded to that that's what happened so
software upgrade boom we've actually got
a bit better and we've got a bit faster
so that's one change but again they
notice like we measure we apply sands we
create an experiment to test something
you know then we go ahead and make a
change and we measure it again and so
we've got faster but we still got some
interesting artifacts once we taken out
that other thing there's some very
interesting artifacts like what is going
on here I find out sort of stuff
fascinating the big curious dig into it
you learn so much well if i zoom into
that this is what it looks like in some
detail so it's clearly this is a normal
sort of kiss some things are taking
longer then all of a sudden I get a gap
jump up another gap in a jump time so
what can be causing that you start
asking the questions again let's go back
to apply science I have the theory how
do i test something to see if that
Theory's correct so I design an
experiment for that I then measure and I
check the results so one of the things I
had a theory about was that this was a
data structure resize issue and
potentially a garbage
some problems now the garbage collection
problem is easy because you output the
GC logs and you correlate the GC logs
with the time of these events and guess
what there's a GC event here on a GC
event here so okay that told me
something that was the cause of a number
of things but it didn't tell me what
went on up here I've seen a lot of these
two graphs this is nothing as humans a
grid pattern matching to visualizing
this stuff is so important plak your
data put it in grass put it in
histograms scatter plots so the present
i'll burn up so all the stuff is really
useful because you start spotting
patterns and you start realizing with
this is so this screams out to me data
structure resizing so how do i test that
I wrote an aspect i use aspect-oriented
programming to go and intercept any time
soon my structures had a resize method
and if that research method was called i
logged it and i logged where it was
happening in the code and guess what
these were hashmap resizes an
interesting the hash mark presides was
actually the cause of the GC because as
you resize a heist map it creates it
copies all of his internal structures
and Alec hits them in a bigger space so
it's resizing upwards and that was
interesting because then whenever it
finished copy and all of this it lets it
all go and i end up with another GC
events later on and so that's just doing
some analysis digging in friday night
you understand so much about your system
it's really quite good fun so we did
that most what a graph look like
afterwards so we don't have those
step-ups anymore we don't have some of
those weird facts but yet there's still
lots of interesting things going on why
have we got these big like hills and
spikes in the car in the curve so we try
to work out what some of that was what's
the cause use ab performance counters to
find out what the cpus are doing and
find out there's lots of bursts of cash
missing but there were almost metronomic
we dig into it again oh it looks
suspiciously like the time whenever you
get loose time slices quantum's to give
up on the cpu so we track the counter so
that it was definitely all the cache
misses according to that how can we stop
that from happening will
if you're constantly just moving around
threads whenever things happened you
will get those cache misses because your
cash isn't warm so we wrote a script
that after the application has been
running for a little while so we let it
you've run for a while and then we pin
threads to course they're not allowed to
move around that's what happened say we
get into that oops we get into the run a
little bit then we apply the script now
it's much more stable all those
migrations all this stuff up here has
gone away it's gone down again this
would seem like oh there's interesting
Kasia facts and what's going on and this
is just fun analysis what's all this
other stuff well that didn't take too
long to dig into that was GC young
generation GC so if we cleaned up the GC
this almost becomes a straight line and
then you've got a nice soft real-time
system so it's just like over a week
doing a lot of analysis finding out
what's going on and we can greatly
improve the system at the end of this
it's a lot faster than it was but it's a
lot more predictable in its response
time so that's part two let's talk about
pitfalls what can go wrong along the
West you're digging along you don't want
to end up into that big pile of the
brown stuff start at the bottom our
modern CPUs are doing lots of really
interesting things but the most
important thing that you're driving
towards is energy usage they want to
reduce energy usage as much as possible
so as they're running they will come
down into lower parsley it's they will
speed up speed dine all sorts of
interesting things and these have a big
impact on the responsiveness of your
system you can end up turning some of
these dates on turning some of them off
but he could be a word that this is
becoming more and more for features so
if you're using lots of CPUs very
intensely you can guarantee that the
frequency over all of your socket is
going to go down if most of your CPU is
acquired and one or two are running fast
it will rebalance and actually allow you
much higher clock frequencies and what's
going on you can control some of this
stuff at our lower level so things like
terrible those pasty at sea States these
are all for
interesting things the control what's
going on we've also got things like
hyper threading we're within the same
color we can have multiple threads going
through the same core and in some cases
that's faster in some cases it's slower
you have to measure the work out what's
the right thing for your application but
even more interesting is things like SMI
so system management it interrupts so
you stopped it you can't do anything
about that an operating system level
where your CPU will stop occasionally
take its temperature check for false do
all sorts of interesting things to see
what state it's in is he looking at some
of the latest CPUs this is getting even
more scary because they're doing things
like maybe checking things and sending
data to the NSA looking for amt and
you'll get scared this sort of stuff is
happening but so lots of stuff is
happening at this sort of level as we go
up a level to the bigger box on the
whole will see that our CPUs and our
systems have become very complex there
in fact networks know I well we've got
multiple cores inside our machines with
layers of caches shared caches but their
own separate sockets with memory
connected directly to them so I've a
core up here and it's talking to memory
down here it's a different cost than
talking to memory over here because you
have to cross a network on your
motherboard between sockets and not adds
additional costs we're not in the world
of non-uniform memory access so knowing
how you access memory becomes really
important but one thing to take away
from all of this is like close to CPU
things are very fast the further away
they are they start getting
significantly slower but it's three bats
are being taken one is the temporal bat
so if you've used something recently
it's likely being cash for using it
again soon the second that's the special
bet so things that are close together
will be used together so your CPUs are
doing lots of things for that and the
third bat is a pattern vs back bet so if
you use things in a predictable pattern
hardware will prefetch your memory for
you if you don't do one of those three
things you're going to be getting a lot
of cache misses and very unresponsive
memory how extreme this is is actually
faster to stream data so quaint
they often SSD then it is to randomly
access mere memory that's called random
access memory it's a complete misnomer
it does not give you random
characteristics it behaves quite like
tip the more you work with different
memory subsystems you start to realize
that it does not matter what type of
memory subsystem Vietminh memory be its
storage be it whatever i'm not going to
be spinning desks it could be SSDs could
be anything if you deal with them in a
sequential fashion they're very fast you
deal with them in a random fashion or
you try to have an arbitrary access they
get a lot slower everything is tip and
you got to start thinking that lat so we
come up from the hardware into the
operating system so many teams
programmers do not talk two systems
people on vice versa and things are so
misconfigured we need to work better
together because there's a number of
things at the operating system level
need to be configured and set up or have
a massive impact on performance the
virtual memory systems one thing how we
set of buffers for networking how we set
up buffers for dealing with storage if
these are misconfigured it's like three
four acts difference in performance
sometimes 10x difference in performance
and how this stuff set up and most
people don't even do any work in this
area so we could be spending weeks
optimizing our code but all we needed is
our kernel parameter change and we could
have had a three or four acts speed up
we will not get that unless we start
working together so we have to start
doing that with our teams and going
forward we get up into our VMs and
there's lots of things going on one of
the most horrible things in our VMs are
these things called CF points now many
people probably will be aware of what a
save point is but their operations were
by within our govt MS and this happens
across other virtual machines as well is
that under certain conditions the world
must be stopped to perform that action
so that actually maybe garbage
collection it may be sort of inflating a
lock from vast lock into a full lock and
all sorts of interesting things whenever
these things happen all threads have to
be brought the save point and then that
action goes forward that starts to
really impede progress inside our VMs
and going back to our dolls law
starts becoming the sequential component
and our algorithms I see systems whereby
you throw lots of course and you think
your code is even completely parallel
and having no contention but because it
contains on things within the vm like
having a lot of having a lot of
allocation or having a lot of locks it
just chokes to the point that you don't
get utilization up on the box we have to
be aware of this and how we move forward
virtualization is another good one
whenever we go to the virtualized world
when we make a system call we call into
a ghast operating system it then has to
call into the host operating system and
depending on how that's set up this can
be a massive cost and what's going on
this is one of the ones where i have
seen at an ax difference between reading
data off a network that's well
configured in a virtual environment
through compared to working in a native
environment and that can be as simple as
seen sort of few thousand messages the
second come in to jump into seventy
thousand messages second getting this
stuff configured in getting it right
really matters people say you through
virtualization that the problem that
only adds a few percentage points that
is not true in many cases when you start
going out of your virtual machine to go
to desk or go to network or do things
like get a lock it makes a massive
difference because you're calling out
through multiple layers and there's a
large tax for that many says have to
write code and we're sharing between
threads and so whenever we've done work
on one thread we need to hand it off to
another thread how we typically do that
is with a lock and then within the lock
we use a condition variable so in Java
that could be wit notify it could be
signal a weird with a condition variable
if you're using Java util concurrent 5
these are the things happen across all
languages what you're basically doing is
getting the operating system involved in
notifying from one thread to another
this is a very expensive operation
typically for what's going on and once
you do this quite often your thread will
be suspended and it will not run for
quite a large time in the future this is
a nightmare and trading applications if
you're in games or audio is really
noticeable but it's starting back up
oblem just even normal applications
because again it comes down to that
sequential component it chokes
algorithms it ties threads together and
you don't get the through but you won't
scale up with multiple cores you
basically end up plain roulette with
your latency and your throughput when
you start doing these sorts of things we
got a unintel the things so Joe suppose
he has this law of abstraction that's
really interesting the leaky abstraction
problem and this has become a serious
problem as we keep layering these things
that we call them abstractions they are
not abstractions their layers of crap in
many cases if we go back to what Dykstra
said about abstraction an abstraction
has to be introduced to make something
more precise note the word precise
whenever you put a layer of abstraction
on and make something more vague that is
not a good abstraction it's back to my
point again it is a layer of crap and
yet we keep doing this and we keep
talking about we're out in abstractions
all we're doing is making things vague
unclear and giving ourselves bad
performance and bugs in many cases I i
I've got this thing where I hit
frameworks and I think libraries is the
way we should go frameworks in pose ways
of working that we've got to stop doing
so dear Thomas has this expression that
frameworks are the ways of injecting
your dependencies into other people's
code shouldn't be like that what we
should be doing is writing nice
libraries that care for themselves so
when you include the library it makes
your life better and you include it
because you want to but you shouldn't be
forced to do that and our abstractions
is one way of looking at that because
many of these things are non-trivial and
they have to deal with the complexity in
the right way I like to think of this as
mechanical sympathy we have to
understand what's going on below but
just in sufficient detail so we can get
the bastards but we don't have to become
experts in it but we've got to have a
reasonable idea so we're using it in the
right way I don't hide behind the excuse
so we have an abstraction layer it hides
that it should be making it more precise
and easier to use if it's not doing that
it's doing it wrongly interestingly if
we want to be responsive
what happens when our system start to
fail well as our systems get bigger and
more cloth complex and distributed
things are feeling all the time in fact
in any significantly large system
there's always something in the state of
being broken but you got to continue on
and i love this sort of thing and we've
seen this scene is wonderful it's like
it you keep going on it's only a flesh
wound lost the leg lost an arm keep
going our software needs to do that as
well and hopefully not pause and give
poor response time as it does it so we
get into our algorithms at this stage
what can we do what can we go forward
with our first thing you should do is if
we're going to do science we're going to
measure them do things right we have to
do clean experiments if you didn't
chemistry you would not use all three
elite nsel's that are dirty hey I want
to check a chemical reaction and I'm put
two agents together I'm not going to do
it in a dirty test tube because it's not
going to give me the right results we've
got to start doing this in our software
so we need to work out how to isolate
our tasks how to take all the noise out
of what's going on so we're measuring
the right sort of things I'm not going
to go into the detail one of the slides
are available you can go off and you can
google for some of these sorts of things
but one thing I would really recommend
is looking at j a hiccup on this so just
google for a J hiccup if you get the
chance it's a lovely little agent that
runs inside your process or can run
outside your process and what it will do
is it'll tell you that a thread was not
able to run at any given point in time
and that tells you an awful lot about
why you could pauses in your
applications you can start identifying
when and where and you can correlate
with what your applications are actually
doing so we've got this clean
environment we've now got to run tasks
and we've got to measure and quite often
we've got a profile who here is used GF
visualvm anyone use your kit get
profiler all of those as profilers they
lie they do not tell you what's going on
in your code they're reasonable for a
certain class of problem I that class of
problems typically where you're going to
network or you're breaking out of your
program to do something else if you get
something to CPU intensive for memory
intensive
they do not find it because back to
those evil see if points I talked before
but that is when their sample your
program they only sample at CF point not
in the highly optimized code that your
jet has made for you it's no good you
need to use profilers like the new
jrockit Mission Control with the flight
recorder built-in slavs performance
studio analyzer intel vtune these sorts
of profilers actually do proper sound
plane and they will tell you what the
real issues are in your code you got to
move forward to that level and stop
using some of those other profilers so
one of the other things I like to do is
incorporate histogram so as I said
before histograms are a great way of
measuring if you put them into your code
so like whenever you're getting requests
in and stuff record the time of stuff
recorded in these histograms they're
really cheap they're really simple you
can get a real good view of what's going
on hits dr histogram is one of the best
for that it's not for high dynamic range
it works quite like floating point
numbers so you don't have to work out
what all the bins are in advance it's
very very efficient so what can I do to
get off this J curve even as my
utilization goes up well there's a
really old boring technique that works
really well it's all misunderstood and
that's botching everyone thinks that
botching is this thing that slows you
down and ours latency well if it's done
well it actually improves late and see
how can it do that we'll most people
backed by waiting for a time mode and
then applying a batch don't wait for
time outs with a batch what you do is
soon as something is available you
perform the action you go back and next
time you pick up as much as you can and
you do that by introducing something in
between so if I want to use a resource
in this case let's say i have got
multiple threads let's say i have tan
threads want to write something the desk
if they start queuing up to do it
because they're going to have to do it
under a lock one thing is going to take
one unit of time to do it the next thing
is going to take two units of time right
up to 10 units of times you're going to
mean response time or five units of time
now I if I take a different approach and
I put them all on to a data structure
and have a separate thread writing them
down to the storage my worst case
scenario is i pick up one thing off the
queue to begin with
I write it down to storage I come back
to the queue again and I take off the
next 9 and write them down to storage
I've completed the whole operation in
two units of time my mean is less than
two units of time and that is the worst
case scenario you often find in these
bursts scenarios that you get all tan
and you put them down in one go this
massively improves your utilization and
you don't go up the j-curve anywhere
near as quickly so my tip is start
looking for things that you can ama
tries and once you start thinking about
this you start finding them all over the
place it's a really useful way of
looking at them but the thing that
stands out from that is you have to
learn to go where synchronous and once
you start going as synchronous you can
have q's through your system you can
measure your cues and you can start
applying back pressure by bound in them
if people think they don't have qs in
their system you're so wrong because if
you used locks if you use an io most
things that you come across will have a
lock in front of it it will have a mute
acts of some means and guess what when
two or more threads go for something at
the same time is one gets it and the
rest joiner kyu Seok use become
something that are everywhere through
your code make them first class concepts
start dealing with them start measuring
them apply littles law to what's going
on not get a much better view of what's
going on in my system so back to my
thing is that earlier by plying back
pressure and not letting other things in
so let's say my storage is not keeping
up well these threads won't be able to
do any more work if they don't do any
more work they don't take stuff in from
the queue that will back up to the
network we won't be able to put any more
data on the network that will back up
these threads which will cause like you
to back up what's will cause the queues
are these threads here the back up that
are receiving network traffic from the
clients so that way I can apply back
pressure he'll wear through the system
or what's that look like well let's say
these guys are making hitched pay
requests on the front end well once your
threads are all busy on your cues are
all full return 403 server busy hour 503
server busy at this stage don't keep
accepting new request it's a much more
friendly where to be working so now
we've done that we're not going to run
always synchronous and it's so important
to run their synchronous i like to think
this is a way of getting out of your
going where you've got to be non
blocking we've got to go forward we got
to keep things nice and efficient get
off the synchronous way of working and
we can do this by stop hulking stuff so
the synchronous methods ends up with us
hogging we get stuck we get doing that
we can also start operating in some
interesting wire so now that I'm not
hugging now that I'm running their
synchronous and know that I'm not using
the operating system I can start using
things like lock three algorithms kind
of specialist ways of working
interesting ways of going forward we can
start doing these sorts of things kind
of a rush through a little bit to the
end here so let's say now I've got all
my state I'm running in this nice where
it's all free you get the nice big scary
PCP state diagram I'm not expecting you
to know this but the important thing is
for any given state machine what if you
could just read where you're currently
at you make them observable your state
machine becomes observable then you can
easily make progress the next steps by
targeting where you're at and making
decisions you can start watching up your
operations you can start monitoring you
can profile you can tune so as we make
our systems we design our code make it
observable put the currents data in a
volatile variable that you can read it
this lets you make your things
observable make good progress keep going
so here's a really simple thing that we
can do to get a lot of sins work through
our systems make them simple necked and
fast whenever they're not a sink let's
say I know sequence data into my system
so I introduced a sequencer I get lots
of traffic coming in once that sequence
i can send the same sequence to two or
more services those services can respond
if you're responding back in an item
potent fashion what happens are the
first one to respond you process it if
it's item potent you get another message
back well that's absolutely cool it does
muddy throw away the second one I said
one of these dyes I've got a very
responsive system because it doesn't
matter you start playing around with
this you can run things on different
memory settings different processors
start doing that and then all of a
sudden I'm whizzing back and
it does matter once lower ones faster we
still get responsive really quickly the
first one to get there is there as fast
if you want to read you replicate too
many other sites and that lets us go
forward and lets us go forward really
quickly final bit Alta Leon techniques
and algorithms is please go back and
learn data structures there's one thing
that'll stick with us through our
careers is data structures will not go
aware there's more to the world than
maps and lists you start getting into
loads of wonderful things like bloom
filters get less all sorts of
interesting tree structures and stuff
that's out there these things are dead
useful for how we end up working they'll
stay with you for your career much
better than the leaders verse in the
song library that's just out and that's
trendy so I'm going to wind up quickly
and closing something's coming it's kind
of interesting world that we're now
living in do use a common name is winter
coming what's really coming is the
Internet of Things last year for the
first time we passed the point where the
number of devices on the Internet is
greater the number of people on the
planet connected devices is now greater
than the number of people this is
continuing to go up we could be anywhere
up to 75 billion depending on your point
of view by 2020 so that's going to keep
happening so what that means is we kind
of control our arrival r it's the fact
that we can't control them we don't want
to because we want this business to come
we've got to deal with it so we got
through and when we design our
applications if we don't respond in a
timely manner things don't happen so
well so take this per person here they
haven't reacted quick enough and that's
going to cause a lot of pain really
think about how we start improving our
service times we just gonna keep
expecting to have our systems work the
way they used to and not perform or the
choices to go parlo but that's hard it's
really difficult it's much easier to try
and improve service times as much easier
to focus in on what we've got and on
that point I'll finish off
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>