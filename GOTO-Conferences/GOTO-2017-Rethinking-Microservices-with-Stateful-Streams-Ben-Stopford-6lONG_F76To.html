<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Rethinking Microservices with Stateful Streams • Ben Stopford | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Rethinking Microservices with Stateful Streams • Ben Stopford - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Rethinking Microservices with Stateful Streams • Ben Stopford</b></h2><h5 class="post__date">2017-09-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6lONG_F76To" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone thank you all for coming
on this final session at goatee I hope
you guys have had a a good conference
I've certainly enjoyed it so today we're
going to be talking about two subjects
which I I find pretty interesting which
is concepts of services mic receivers
which this whole tracks about and then
this concept of stream processing and
particularly actually around this this
idea of a retentive log so my name is
Ben Stopford you can get me on Twitter
there and I work at this company called
confluent anyone heard a confluent a few
anyone heard of apache kafka got a lot
more from that okay cool so this is this
is the company that runs it was set up
by the original authors of Kafka so we
sort of provide services around it and
I'm work as an engineer why I should do
a few things have a slightly strange
role I do a little bit of this this kind
of talking stuff and I work primarily as
an engineer on the distributed log part
there's a few different parts of Kafka a
man used to work at thought works and a
few other companies in the UK so yes
just talking you thinking about Mike
reservas there's been a really good set
of talks so far where we've looked at
event-driven approaches and we've looked
at requests driven approaches and we're
gonna think of it a little bit about how
these things kind of pan out over time
particularly from a kind of
architectural perspective so
microservices people like slightly
polarizing in terms of what people think
of them and there's a good reason for
this they can go very wrong which has
been talks about that too so for a lot
of people they're about splitting the
monolith so spreading the monolith is
important as no idea simply because
you're probably gonna start with a
monolith right if you're sensible you
will start with the monolith if you're
trying to get something delivers for
your business
if you quite often want to extend that
so I think Randy Shoop did quite and had
a quite a nice quote on this which is if
you're if you don't regret the
architectural decisions that you made
previously then you're probably over
engineering and I think that's very true
but this idea of just splitting a
modulus isn't really what mike reverses
are about from my perspective for me
it's really about this idea of autonomy
so autonomy it's a little slightly
abstract concept but what it really
means is that each service evolves
independently that's to say it is its
own master and having a sense of
Independence allows us to basically
spread an application across many
different teams so that they can all
move independently so its independence
this concept of independence
particularly around code bases release
process etc which really kind of give
services their value and it allows them
to scale so when we say scale I don't
really mean from the from the
perspective of petabytes of data or
billions of users or whatever I really
mean scaling from the perspective of
people terms like scaling your
organization as it grows because that's
actually one of the most challenging
parts I mean we'll be talking about
Kafka and K streams a bit in this talk
and these are technologies that really
can deal with scale in the sort of
traditional sense but actually one of
the harder problems is scaling in people
terms I mean if we think about scaling a
monolith we have many examples from the
pinup companies like Facebook through to
many other smaller examples I think
where this is a difficult thing to do
what happens as we grow but in fact all
companies are to some extent collections
of applications if you look at any
company over maybe a thousand people
you'll find there's a whole bunch of
different applications that have to
connect together in order to progress
some kind of business goal
and typically we'll do this with
something like enterprise messaging or
FTP file transfers these applications
will kind of talk to one another but
it's often an afterthought it's often
just something that we kind of bolted on
the end in an attempt to kind of make
this thing work if we think about micro
services or SOA which is sort of similar
certainly historically very similar
patterns they're really about this idea
of reacting for a multi-team
architectures architectures where each
service can actually have a degree of
independence so Pat Helen's wrote a
really interesting paper about it's
entitled data on the inside and or data
on the outside anyone read it no there's
a reference to it at the end and I would
really encourage it it's great paper but
he talks about thinking about services
in terms of the data that they hold
inside them inside the service itself
and the data that they expose and really
one of the things that we have to think
about when we move to a service based
approach it's actually the data that we
share with other services it seems kind
of obvious but actually we're not
typically that good at dealing with this
problem
so this external world the world that
sits outside a service and particularly
the data that exists within it is
something that we really should design
for so this idea of our independence
this idea of keeping service didn't
amendment comes at a cost if we think
about it from an oo perspective we've
all been taught about the concepts of
encapsulation probably had you've
probably learned about Parnassus at
university or learned object-oriented
modeling as you kind of grew up and we
know that if we have an object's it
contains States and behavior so here we
have an always orders objects on the
left and a statement objects on the
right and we know that we will
encapsulate our state and our behavior
behind a nice clean interface this gives
us loose coupling something which allows
us to change this is just this is a sort
of relatively intuitive thing to do now
if we use if we do this into a
monolithic application it's pretty easy
to get releases out so let's say let's
say we have a requirement from the
business and it involves changing both
sides
of an interface we can put those changes
in we can implement those JIRA's or
however we however we go about
implementing our requirements and we can
deploy the whole thing together and it's
pretty easy if we have the same problem
in a distributed world where different
teams run different services it's a
little bit more tricky because if we
have a change which cross cut several
set across cuts to services we have to
kind of think ranae's the releases of
these of these code changes so that it
all works harmoniously together so a
synchronization problem is a little bit
painful
so services work best where requirements
change is isolated to a single bounded
context let's take an example of that
imagine something like single sign-on
this is like a really good service
example it has a very simple interface
right authorites and the key point here
is that it's really unlikely no matter
what change happens to it in in terms of
our business requirements for a variety
of different services sorry unlike you
that any of them are going to affect
this single sign-on interface it exists
in a tightly bounded context a little
world unto itself but business services
services that we actually build and
change on a day to day basis these are a
bit different so to take a kind of
example from online retail if we had a
data set of customers that data set of
orders a data set of the product catalog
these will lightly proliferate into many
of our different services these are
effectively caught shared called
core data sets so they're just much more
there's much more intertwined
authorization sits over over over here
and tightly bounded context so the point
is is that the futures the future of
business services end up being far more
tightly intertwined simply because they
have these shared data sets that flow
and proliferate through different
services
so we need this idea of encapsulation
which we all understand intuitively so
that we can hide our internal state and
remain loosely coupled but we also kind
of need this ability to slice and dice
the share datasets we need to better get
access to them to write new services etc
and these two forces kind of compete
against one another in the services that
we build and the rub is essentially that
data systems databases Hadoop clusters
whichever one you want to pick they have
a little to do it with encapsulation in
fact quite the opposite databases which
are wonderful pieces of technology and
they wonder are wonderful pieces of
technology because they contort the data
that they hold into a whole variety of
different shapes and sizes so if we
think about a service we have an
interface which hides the data on the
inside so this nought there's less data
on the outside and there is data on the
inside whereas databases have this
massive amplifying interface that allows
you to contort the data that they hold
into a whole variety of different shapes
and sizes
this leads this kind of idea of this
data diet dichotomy so data systems are
really about this idea of exposing data
allowing you to iterate quickly and get
your work done the services are really
about the idea of encapsulation
the antithesis of that hiding data from
other people from other services so that
we can protect ourselves against future
change so who's heard this said
microservices shouldn't share a database
probably quite a few I would imagine
sami newman talks about this a lot
matter of fact he basically wrote a
whole book on it this is good advice and
it's good advice for that real reason
databases form one of the one of the
richest forms of coupling they have an
amplifying interface and they also
represent shared mutable state
we all know as programmers that we've
been trying to get rid of shared mutable
state from our from our programs for
many years so this is good advice but
what so what do we do instead for
independently import deployable
applications well one thing we do is we
temp we typically wrap our database in a
service interface of some description so
we have that data on the outside that's
nice and combined and this data on the
inside and we cleverly create this
interface that's nice and clean and
everything looks rosy so what happens
when we when we release these things
into production and we let them evolve a
bit over time so typically you get kind
of one of two eventualities and
hopefully these will these things will
resonate with you somewhat so the first
one is that we cannot just end up
growing the interface over time so you
might be adding a variety of different
parameters onto the end of this
interface so that we can kind of get
more precise data out of our data
service and this data sets on the
outside slowly grows over time all
alternative maybe we just add more and
more methods right so we I'll start off
with get or but get order and then we
have get ordered by user ID and then we
get get orders unfulfilled by ID and you
know two years later you look at your
service interface and it's got like 50
different methods all doing really weird
like kooky little things and you start
to wonder if you should have just
implemented a declarative language so
this is kind of one way that things kind
of go wrong and actually as your
datasets grow these problems compounds
these data services which have to serve
as many different other services have to
have to deal with larger volume that
volume that means they have to be more
specific in terms of the way that they
slice and dice data it means that they
have more data to vend they have scaling
issues so that's kind of option one
option one is we just kind of end up
growing this interface over time being
very specific about the data that we've
end option two is that we just give up
and we just move the
whole dataset so in this case we might
have a method get all open orders where
we just hold this service the order
service pull all of the open orders and
we just keep them locally in a database
we might also do this with a messaging
system and this gives us a couple of
advantages firstly it allows us to
perform joins and it allows us to kind
of iterate quickly on the data sets that
we've got but it actually leads to a
different problem if we go down this
approach and this is actually really
been Tobias messaging problem and that's
that we end up with many many mutable
copies in all the different services
around our organization and these things
diverge over time and one of the reasons
that this becomes really tricky is this
um what actually happens is each service
so data in companies is generally pretty
crappy it's just the facts of life right
data sets tend to be pretty crappy one
of the main reasons for this is that the
way that we interpret data is somewhat
subjective and big organizations have
whole departments which will kind of try
and work try and disseminate the debt
the definition of various different
attributes so that you can understand
what data means what this really means
is services end up actually writing
software which fixes the problems in the
data that they're getting converting it
to their own domain model cleaning
things up and by doing so they actually
become dependent on the very problems
that they're trying to avoid the very
idiosyncratic in those data sets and
this makes them very hard to change
because if you change something at
source then you've got a variety of
different services they're effectively
dependent on those those the previous
broken version of the data so it's very
hard to fix these kind of problems in
retrospect so the more mutable copies of
data that we have spread around our
different services the more data will
likely diverge over time so what we end
up with is this cycle of inadequacy so
we start over here on the left
you know we've got only project
we've done our design it looks really
cool and we're really happy with it
I've got design on a white board we do a
proof-of-concept it's still looking
pretty sexy and we have a start to put
it into production and things kind of
change and can't work out whether or not
we can change our services well
initially we can it all works quite well
so we just keep broadening this contract
and maybe we've we're probably doing
okay at this point so we probably
iterate a few times on that if we've
ended up creating a massive shared dated
god service somebody might notice that
they're spending rather a lot of money
in which case we'll end up going back to
this light let's encapsulate and
returning back to our Knights and
services through some cycle of
regeneration and alternatively we just
go frack it we'll just get all the data
so everyone ends up having a copy and we
end up with this divergence over time
and then the divergence causes a
redesign and we end up going around this
circle again and I've actually I've seen
this happen a few times in a few
different companies and the thing about
the thing about change is that when you
reinvent something when you do something
for a second time the thing that's
really important is that you actually
change the context in a way that implies
you're going to succeed the second time
so yeah yeah you might argue that doing
the same thing again and again in the
same with the same tool set is somewhat
idiotic in that you're you're you're
constantly trying to bang your head
against the same brick wall so the roots
out of this is to try and change
something we need to change the way that
we actually approach these problems of
disseminating data and controlling data
across a variety of different services
so these forces are fundamental
excessive
the divergence coupling they effects all
of the systems that we build there's not
much we can do about them because
they're fundamental all we can really do
is reposition ourselves on the tradeoff
curve between them so is there a better
way is there a way that we could do this
attack this problem in a slightly
different way so this is um this is a an
approach which been working on for a few
years with a few different companies and
it's actually similar in some ways to a
lot of things we've been talking about
today it's the first step is that we
build on a backbone of events so rather
than starting with sort of quests and
commands and queries we start with an
asynchronous protocol one that embraces
a synchronicity both in terms of a
technical perspective but also from the
perspective of real-world asynchronous
problems like taking it order and
getting it to your door and then we
layer in request where we're necessary
so requests driven approaches have high
coupling because effectively a single
call snack is coupled throughout the
dissemination of that call stack
whereas this event broadcast approach
which is like traditional messaging
providers with less coupling with
decoupling ourselves by adding a broker
and then using a broadcast protocol so
we want to start with something like
this
and layer the request response approach
on top so what we end up with is
something an event-driven architecture
essentially so we have an asynchronous
message flow in this case we have an
order which goes through a variety of
different states we have a number of
different services which are which are
collaborating to progress that thing
forwards all using an asynchronous
protocol and this is a this is termed
event collaboration and then we'll layer
queries on where we need to so in this
case we have a basket service which
responds to a user interface in a
request response mechanism and we're
gonna layer that on to our asynchronous
architecture so that's step one step two
this is the bit there's a little bit
different to a sort of traditional
approach is to make this data on the
outside the shared data sets the
first-class citizen so this idea of
there being data which we exposed to
other services and data lat which we
internalize so we're going to focus on
designing for this and making these data
sets available to any service that needs
them so stateful streams help us with
this so this is a effective effectively
a messaging system we're obviously
talking about Kafka in this instance
because it's the one that I know best
but there are others so we have stateful
strange streams at the center and we
have a variety of different views which
we're going to embed into our various
services and actually what we're gonna
do is we're gonna use this tool set as a
way for manufacturing views based on a
central essential canonical set of
streams so Kafka is the tool that helps
for this certainly from the point of
view of this talk categories our
streaming platform if you're not
familiar with it at the center of it is
a distributed log and then around that
there are ways to get data in and out
this connectors which will allow you to
to pull data from a whole variety of
different data sources or push them into
a whole variety of other data sources or
data sinks and a streaming engine which
you can bolt over the top or embed right
inside your service so the log a
distributed log is effectively a
messaging system it's a little bit
different to a traditional messaging
system because it's designed to scale
linearly both for actually from from all
three layers so regardless of whether
you're producing you're talking about
the actual log itself the distributed
log itself or occlusive services which
are consuming from it we can take a
single
type and spread it across as many
machines as we need to it's linearly
scalable with while retaining strong
ordering guarantees within a single
partition it also provides transactional
guarantees in the next release so that's
coming out this this this summer in a
month and this is quite interesting
because one of the big problems with
with building distributed asynchronous
systems is you really don't have a lot
of transactional guarantees and actually
you have to implement idempotence in
pretty much any voice in any
asynchronous service so having a
scalable mechanism for handling
transactions is pretty useful so what we
end up with this is this kind of service
backbone it gives us a few different
things we have the ability to naturally
low balance we have fault tolerance both
built into the broker itself as well as
built into your services because data is
effectively routed to different
instances if she's an instance of a
service fail we have this idea of
transactions which allows us to
implement both idempotence as well as
transactionally submitting different or
so missing different messages to
different topics and then bringing them
back together in a transactional way but
these these are the two things that are
probably most formative for what I want
to talk about today one is the idea of
the log itself it's a very simple data
structure right it's a it's basically
just like an append-only file and the
nice thing about append-only
files is they are oh of one for read and
oh of one for write and that's very
different to a traditional messaging
system so a traditional messaging system
is basically built on a b-tree and it's
a b-tree which means that you kind of
have to keep that tree in memory if
you're going to have efficient
performance or if you have a log you can
you can traverse it has a very simple
very simple interface there's like seek
or scan but you can you can implement
that in a very efficient way from asset
whether you're in memory or on desk
and then you have this other feature
which allows you to compact the log
based on a key and this is kind of
interesting so a log is literally just
an immutable stream of messages but you
can compact it based on a primary key
and when you compact a look what you get
is something that looks like a table so
this is an interesting place for us to
keep data on the outside data that lives
between our services somewhere we can
leave it and refer back to it later
so we would we might have our orders
customers payments and stock we might
keep those inside the distributed lock
and what we'll actually do is we'll lay
our views inside our services either
directly inside the service process
itself or is this sort of separate
service that we refer to inside some
bounded context this kind of starts to
change the way that we think about data
and the problem of managing shared data
in the service architectures that we
build so stream processing is a sort of
layer on top of this and this really
helps us do a number of things because
it allows us to introspect the streams
in a way that's kind of that's very
efficient and lightweight we're really
not taking a deep long-lived copy of the
data so a little primer on stream
processing assume processing is really
just this this problem of working out
how to bring together various different
continuous streams of data so it's a
machine for combining and processing
streams of events
it's a stateful stream processing is
actually a sub branch of stream
processing and the reason it's called
stateful is because it has a bunch of
tools which allow you to maintain States
and what that really ends up being
meaning is that you can treat streams as
an infinite thing which you chop up into
windows or alternatively you can treat a
stream as a table so
think which is deduplicated by a primary
key so we've always got this flow of
data coming towards us but there's a
stream or a table it doesn't really
matter and what we're actually doing
inside the stream processing engine is
just accumulating for some data and the
difference between a windowed stream and
a and a keyed stream a table it's just
how we accumulate that data so in a
windowed sense we might keep you know a
week's worth of data inside accumulated
inside our service or inside our
streaming engine or alternatively we
might accumulate that data by key and
actually have the entire data set so for
example if we wanted to have a lookup
table of customer information we just
accumulate the customer stream by
customer ID and then we can refer to it
within our service within our engine so
beyond that obviously you have the the
basic primitives of a database you can
take a view you can join different
streams together streams and tables you
can apply predicate filters you can
aggregate you can translate something to
your internal domain model it's really
basically just a database embedded
inside your service which allows you to
slice and dice these various different
streams because this is basically a way
that we can get access to and process
data from lots of different services
without having to a put responsibility
on them for providing scalability
properties etc but it also most
importantly it allows each service to
retain control of the queries that it
runs right so the data is shared but the
queries themselves they're totally owned
by each service so each service has
autonomy so stateful stream processing
has this idea of streams and tables on
the way in so here we have an example of
a query this is again an online retail
example where we're calculating how long
hates a payment to occur - to complete
inside a bunch of different regions so
maybe a payment takes I don't know a few
seconds to process in India and five
seconds to process in South Africa or
somewhere so it's that kind of query and
which in this case going to do it over a
one day window and we're actually
joining two streams which are gonna be
windowed and we're referring to a single
table all within the stream processing
engine so we have two streams on the way
in tables on the way in and then
likewise on the way out we have a choice
we can either have a stream on the way
out which is the result of that query
comes constantly updated or
alternatively we can materialize that as
a table we can query so we can put like
a rest interface over it and just query
it from a browser so we have this idea
so I guess yes at the important point
about this is that we're overflowing
data to rocks DB that's the internal
implementation inside the streaming
engine that's really just an
optimization so that we don't have to
keep all of this data in memory and we
cannot spread this over many instances
of a service so to take a little example
imagine we want to trigger an email
every time a prayer attainment is
confirmed so we send the user an email
that says your payments confirm for this
particular order that you just made we
might want to do a join between payments
which would be we probably treat a
stream orders which we would also treat
as a stream because those two things are
likely to come at the same time or
within a defined window and then we
might just use a lookup
against the stock table and the stock
table is so this whole thing would be
done inside this one service and we're
joining I said two streams and a table
and then sending an email so we might
not want to do this with a streaming
engine streaming engines have a an API
which you have to learn or we might have
a database we might have a requirement
for doing
ad-hoc queries something that we were
where we want something like movie
elasticsearch or maybe we want a
relational model whatever it might be
so we can always just go back or evolve
to using a standalone database which use
a connector and do the join there it's
just a slightly heavier weight process
because we're gonna have to then curate
that data over time but the interesting
thing about this is that if we take even
just this approach of the database if we
did this in a traditional messaging way
we basically have to keep all of the
data we have to keep all of the data
over time so that we can work out also
we can answer the questions that we
might need to answer in order in this
case to solve this problem and actually
any future problem whereas the nice
thing about having the shared streams is
that the connector can basically just
take the view that we need right now we
let's just take the the bits the the
very specific query a very very specific
materialized view that we need to solve
our problem today because we can always
regenerate it we can always go back and
regenerate it again
and actually a lot of the a lot of the
the mechanics of getting this pattern
right and I'm not going to go into this
today because it's a much bigger talk
but a lot of the mechanics of getting
this particular way are about having an
iterative process for rebuilding these
data sets that's really about having the
machine the machinery it's more like a
kind of continuous to put to kind--
continuous deployment approach so we
have this idea of shared storage inside
a look and we have this query engine
which we can embed inside our services
so data storage plus query engine is the
kind of database write data inside Kafka
query engine sitting on top it's kind of
a database but we had to have many
services right so we have one central
set of canonical streams and then we
have a whole bunch of different services
they're all using the same same
retentive set of streams but they're all
implementing their own views their own
interpretation of that shared data set
this is like a shared database it's
basically a database inside out as
anyone seen Mouse in plaquemines talk on
turning a database inside out we've got
a few yeah that's a really good talk and
I would encourage you to see that also
it's a kind of quite a powerful idea so
we sort of started with this idea right
micro-services shouldn't share a
database um this is kind of a shared
database but it's not a normal database
we can actually prove not by me by the
way and people have done a fair bit of
research to prove that event broadcast
is the lowest form of coupling and the
reason for this intuitively the reason
event broadcast is the lowest form of
coupling is because each service only
really receives a stream of events and
they can basically do whatever it wants
but if we centralize something that's
immutable we don't actually affect
coupling so this isn't any kind of
shared database we certainly can't make
a coupling put a coupling argument for
why this would be a problematic approach
so to share a database we should think
about turning it inside out what we end
up with is basically a machine which
allows us to create these kind of
materialized views so we have the orders
the payments the customers in the stock
these are the canonical data sets and we
have a variety of services we simply
have a process for manufacturing the
views that they need to do their job in
a very lightweight way in a very
lightweight manner you can always go
back to the shared streams if they need
it so let's look at these different so
I've got basically four different
approaches to sharing data we have
shared database service interfaces event
broadcast stream data platform and then
up here we have ease of change I can't
how easy how easy is it for us to evolve
our our system there exists a bit
accessibility how easy is it to get
something up and running quickly
and then data erosion issue of things
diverging over time because they're
effectively mutable so share database
it's pretty bad from ease of change
it's like shared shared mutable state
not good great from a day to
accessibility point of view because
anyone can just dip into it great from a
data of erosion point of view because
there's just one canonical copy service
interfaces for a little bit better from
the perspective of ease of change but
not much right because we still have to
if we have got a new requirement we
still have to get our service centers
for interfaces to change in order to
give us the answer to the new question
that we have from a data accessibility
point of view yes okay and from a data
erosion perspective it's get one
canonical copy and thank event event
broadcast great from the point of view
of ease of change because everyone's
basically got everything they can do
whatever they want
not so great from the perspective of
accessibility because if you weren't
listening to the message when it was
sent then you don't have it anymore and
from a data erosion age perspective is
poor because you have to maintain these
copies of you love shared data sets over
time but if we do use this kind of a log
backed stream data platform approach
it's easy to make changes it's easy to
get access to data and it's it's we
don't have this issue of data erosion in
the same way because we're actually
building these very focused views which
are backed by this shared data set so
for me good architecture actually has
very little do to do with drawing
something on a whiteboard or coming up
with the design and good architecture is
really about providing a set of
technology stack which you can evolve
over time that's actually the hard bit
Hobbit is going how do I start from
something small and evolving to
something that's complicated that's hard
for me request driven approaches aren't
enough and I think as that's fairly well
understood today hi coupling it's hard
to handle
physically asynchronous flows things
that happen these synchronously in the
real world and it's hard to move and
join datasets so we want to embrace this
data that flows between services this
data that sits on the design on the
outside and actually make it something
we're designed for so that we can give
our services independence all right so
each service has the freedom to change
and manage its own data based on but
based on a canonical set three building
on a single stream data platform that
everyone can access and share so this is
a mechanism for evolving and
architecture efficiently over time and
there are kind of a set of principles
that sort of back this idea up its wimpy
where we start simple lightweight and
fault tolerant immutable because we want
to build on retentive shared narrative
reactive because we want to leverage a
synchronicity I want to focus on events
which are coming now
evolutionary that's that's actually
pretty important is this idea that we
only take the data that we need today
that keeps this lightweight and nimble
and deep and decentralized so we receive
a receiver driven you can really talk
about that today but that says that's
that sir
that's what gives a venture of an
architect as their decoupling properties
we're avoiding coordination and we're
avoiding God services and that's all I
had to say there's a number of
references here we've been working on a
series of blog articles on this and
there's the turning the database inside
out talk which is excellent two papers
there from Pat Holland which are really
good and a couple of things from from
Jai and Neha who work at confluent um
thank you very much for staying for this
talk I hope this has been useful
if you have any questions then maybe
you've got some through the app you can
ask them now I'll be around for the
drinks and stuff so I love talking about
this so please come and talk to me give
me your thoughts argue whatever you like
I love to talk about it so please do and
I said you can get me on Twitter thanks
for watch</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>