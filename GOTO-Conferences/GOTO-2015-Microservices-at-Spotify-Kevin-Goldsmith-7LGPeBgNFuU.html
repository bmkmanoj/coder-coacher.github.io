<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • Microservices at Spotify • Kevin Goldsmith | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • Microservices at Spotify • Kevin Goldsmith - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • Microservices at Spotify • Kevin Goldsmith</b></h2><h5 class="post__date">2015-12-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7LGPeBgNFuU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is kevin goldsmith I am a VP
of engineering at Spotify that I am what
we call an alliance lead which means I
manage tribe leads if you know any of
our weird terms for teams I'm going to
talk about micro-services at Spotify I'm
not going to go I'm talking about lots
of different aspects about it a lot of
why we use them some detail but I'm not
going to go too deep in this but I can
take some questions around it so I'm
going to start this discussion with a
bit of a hypothetical thought experiment
let's say I wanted to build a large
application now this application is
going to have some requirements right
there's some stuff this application has
to be able to do it has to scale to
millions of users it has to be internet
scale it has to support multiple
platforms right so it has to work not
only on your mobile and on your web but
also on desktop and on embedded devices
on game platforms lamps it has to run on
it has to handle some very complicated
business rules it's not a simple
application this is a very very
complicated application with lots of
really strange business rules not set by
you set by others and it has to be
competitive in a fast-moving market now
there's two ways to be competitive or
there's two parts of being competitive
number one is you have to be able to
respond quickly when your competitors
make a move right you have to be very
nimble and very responsive better than
being able to be reactive it's better to
be proactive and to be the ones that
everyone is chasing right so ideally you
want everybody chasing you you don't
want to be trying to play catch up to
everyone else you need to be able to
innovate really quickly now this is a
thought experiment but it's a pretty
stupid thought experiment because I'm
obviously talking about a product a
product like this Spotify right this is
the this is
product I'm talking about now what those
are those requirements for our product
mean for us there's a few things one
today I'm using published numbers then
real numbers are higher in all cases
seventy five million monthly active
users an important thing to know about
what we think an active user is compared
to somebody like Facebook Facebook you
use Facebook several times a day for a
few minutes at a time you use it while
you're standing line at the bank you use
it you know while you're on the bus
people use Spotify for hours at a time I
think I don't have the right numbers off
top my head for just general length of
session but I can tell you the way that
our editorial team measures average
session length for their playlists is
like 23 minutes is their average session
length that's part of usually like a
several hour session people listen to us
all day long people put on white noise
and sleep with Spotify they stream white
noise to their computer or their phone
all night long and leave it on the
nightstand table white noise and pink
noise are actually some of our most
popular tracks on Spotify and I'm not
joking we're available in 58 countries
today and growing we add over 20,000 new
tracks into our corpus every day and
have forever then real numbers goes
lower it goes lower or higher but it's
always over 20,000 we have over two
billion we just published this number
yesterday actually two billion
user-generated playlists now it's
important I'm distinguishing between
user-generated which means the playlist
that you all create when you Spotify am
differentiating between that and the
ones we create if you know if you've
heard of discover weekly that's a
playlist we create for every active user
that's like 75 million playlists that we
create that's not part of this number
this is the ones that people sit down
drag tracks in and make a playlist so
it's a lot of data for a lot of users
if you know anything does anybody know
anything about the music business at all
like anybody working and like kind of
related to the music business okay if it
from the outside the music business
seems very simple or at least not that
complicated
when you are trying to license music in
58 countries the music business is
insanely complicated when you in Germany
click a track and Spotify to play which
track you are actually served is
actually determined a lot by complicated
business rules because for example when
we license Justin Bieber in Germany we
license it separately than one we
license Justin Bieber in the USA there
is a German version of his label that
licenses his music in Germany and they
negotiate differently than the US
version of his label the Canadian
version the Australian version every
time we go into a new country we have to
rely scences music you also hear you
have gamma right the music publishing
society their rules are different than
BMI and ASCAP and the publishing rights
societies in every other country so when
you click that track not only which
track are you allowed to play based on
your account in which country you're in
does that is that different for every
user but then who we pay and how much we
pay them is different for every track
and every user so the business rules
around this are actually really really
scarily complex but our systems have to
handle it we also as you're probably
aware if you know anything around the
space we have lots of competition we
have smaller competitors who could be
usually when we talk about that we think
about people being much more nimble than
we are like not weighed down by our
heavy weight processes but we're also
literally competing with two of the
biggest companies in the universe Apple
and Google these are our competitors
this is important and if you notice
something up here this is how
competitive this markets getting one of
these competitors are ready is gone yes
last week and now one of the other ones
has gotten bigger right this is a very
very fast-moving highly competitive
market so how do we support all these
crazy
Harmons for our application while still
like growing the service and real user
numbers pretty significantly
month-to-month we are growing still
incredibly fast so we have to do support
that growth we have to support these
crazy rules and we want to keep
innovating at the same time right all
these things are in conflict with each
other
all these requirements how do we do that
if you know anything about Spotify one
of the things we do I'm not going to go
through matrix I'm not going to go
through our matrix and squads and tribes
and that kind of stuff that there's a
micro-services track but what I'm going
to tell you like the important part and
how it relates to micro services is the
core of what we do is this idea around
autonomous teams and for us the
dictionary definition of autonomous is
interesting but specifically this part
of the dictionary definition of
autonomous we build these teams that are
full control over what they do right
that's the most important part the team
has to be almost no dependencies on
other teams that's the most important
part of how we get this stuff done and
keep moving forward trying to be faster
than everybody else so I used to be you
know before I came to Spotify I were to
add Adobe and I build a product called
Adobe revel which is probably no longer
in the App Store but it was until a few
weeks ago sadly but this is how we would
build kind of connected applications
right this is maybe how you build them
it's a pretty standard way of building
connected applications you have clients
probably a client team for each client
type you have a core library that
abstract some of the crazy business
logic probably also handles all server
communication to make that consistent
you have a server team you have an
infrastructure team that's how we did it
at rebel it's how a lot of companies do
it pretty obvious way to do that the
problem is if you want to build a new
feature in this kind of world well let's
say you want to build a new client
feature the client team asks the core
team please give us an API that lets us
do this the core team ask the server
team please implement this on the server
side so we can do whatever we need to do
in the client possibly you're adding a
new database type or something like that
now the server team has to ask the
infrastructure team that's a lot of
asking of other teams
right that's a lot of people trying to
get stuff on to other teams backlogs
let's say also like eventually the
client teams get bored waiting for the
server team they're like we know how to
do rest so we're going to just do it
ourselves
okay then they're tying to serve team
and then probably is there some product
person saying we have to launch this
feature in all platforms simultaneously
tons and tons of communication the
challenges around working this way are
that synchronization part it's trying to
get everybody building the stuff so you
can ship the feature it's because the
kind implementation depends on the core
implementation depends on the server
implementation depends on there are on
the infrastructure so we don't want to
do that that slows you down that slows
you down 80 to 80 different ways we
didn't want that so instead what we did
was we built full stack teams each of
our teams has back-end developers
front-end developers testers UI designer
product owner right what that means is
you want to add a feature to Spotify
you're on a feature team you want to add
a feature you implement the UI at the
same time you implement the backend the
developers are sitting in the same room
couple desks away from each other
they're in constant communication it all
goes out together and it's a dialogue
it's not synchronization so there's a
light there's still light dependencies
can't erase all dependencies obviously
we're not individual feature teams are
not putting servers into racks and our
DCs
we're also you know we actually to work
this way requires a lot of
infrastructure support and obviously
we're not submitting like 50 binaries to
Apple or to Google like somebody has to
package that all up and hand that off
but these dependencies are minimized to
the greatest extent possible so for us
again it's Spotify for us that means
there's 90 teams over 90 squads with
over 600 developers the numbers higher
this last published number in five
development offices on two continents
building the same product right building
the same app that runs on your phone
or on your tablet or on your desktop or
in your PlayStation so that's important
to know because this is actually one of
the reasons why we come to micro
services full have these full stack
autonomous teams require us to build our
stuff and very loosely coupled
architectures right so we don't have
these dependencies we have to build
things in a way that there is in strict
dependencies between the components we
build now I'm going to take us back into
a little bit of time I'm not going to go
as far back is as some of the earlier
speakers in the track I'm just going to
go back a little bit and this is the way
we build applications for a long time
connected applications you're an app
developer you don't have a huge team you
build a server component server talks to
a datastore that's the way we build
connected applications application
becomes more popular now all sudden your
back-end is starting to have some
scaling problems you identify oh it's
the datastore datastore is slow okay so
you shard it or you come up with some
sort of delayed replication system
whatever we know how to solve that
problem data datastore is slow let's
we'll fix that now you become more
popular well okay now it's no longer
your datastore that's a problem it's now
your server you have this big monolithic
server implementation and starting to
get some of the code pass or getting
slow well okay this is how we know how
to solve that problem we put a load
balancer in front of it create multiple
instances you know this is this is easy
we've done this for a long time now
Spotify is obsessed obsessed with
scaling it has been obsessed since the
beginning it's been one of the things
that's driven a lot of our architectural
decisions from the first moment so when
we look at this problem we've decomposed
we almost at the beginning decomposed it
to this problem right because if you're
worried about scaling to hundreds of
millions of users you build your system
in a way that you can scale your
components independently and this is
driven a lot of our direction with
microservices so when one part of your
monolithic service
or slow you have to re you have to
create new instances of big monolithic
services running on very big iron and
getting bigger all the time so if I pick
on Facebook because I like to pick on
Facebook around this there writing like
I'm massively monolithic server they are
having to invent new computer science in
order to distribute this binary around
right they have people who make it
easier for them to build this monolithic
thing we went a different way okay we
said we'll take on some different
problems because we think those are
easier to solve than actually hiring
PhDs to make our binaries get
distributed better so when we see this
problem if this let's say this white box
is doing a lot of fan-out to other
services or it's doing some heavyweight
computation we can just duplicate that
box or replicate that instances there
and then not have to worry about all the
other things that are actually running
pretty efficiently this is working for
us incredibly well and it has for years
we've been working this way for years so
microservices yay this part is the part
that's super important really easier to
scale based on our real-world
bottlenecks you can identify the
bottlenecks in your services and
replicate there or fix them there
without having to do massive rewrites or
actually come up with or actually having
to do like lots and lots of work to do
that
similarly there way to test right
because those tests surface is way
smaller and each one of these things
they don't do as much as a big
monolithic application so they're way
easier to test developers can test them
locally that's awesome right to be able
to test your service without having to
deploy to the test environment is super
great right especially if you're running
a laptop which way the developers are
they're easier deploying they're smaller
right that's super important for us
they're smaller they deploy really fast
and they're easier to monitor because
they're doing less so it's way easier to
monitor each of those instances they can
be versioned independently this is
something else we do a
and so this is super important as I said
before we run on embedded devices right
embedded devices manufacturers when they
want to create they don't care once
they've sold you your whatever in fact
this is the why things lamp that runs
Spotify in it once why things is sold
you this thing they're really happy they
don't really care about you anymore
because you're not going to rent the
lamp from them you own the lamp you're
not going to buy it again so you're that
sales done they don't necessarily need
to update it ever right it's kind of
like the Android phone problem right
older Android phones there's not a lot
of value in the carriers to update them
right same problem in embedded devices
there's very little reason for them to
update them which means that as long as
these lamps are sitting on people's
bedside table they're going to be
calling the same version of the API on
the back end they're going expect it to
work exactly the same and if it doesn't
work the same the lamp is going to start
is no longer going to work now the
people who are going to be angry the
customers you have this lamp they're not
going to be angry at why things you're
going to be angry at Spotify because
they're probably paying a Spotify
subscription and they just expect their
lamp to work with Spotify so the cool
thing about the way we work is when we
do a new version of an API we just
create a new server and we just run
those separately one instance per server
what that means is that to support this
lamp as long as this lamp is running
we'll have that version running in our
data centers some an instance talking to
answering to that version up until the
point that the last of these lamps gets
thrown away and then we can just shut
down those machines we don't have to
have those code passed sitting in the
new versions of the services we don't
add multiple for multiple versions into
the same instances which means we don't
end up with nearly the kind of technical
debt you end up when you we used to do
like you'd add multiple versions to the
to the same binary right and then
somebody some point later refactor some
code that it turns out like a version 10
10 versions ago was still using and
breaks that that's I've seen that happen
so many times we don't deal with that
and that's super powerful and very
valuable
the other thing that's kind of cool
about the way we work and at the scale
we're at is we're less acceptable with
micro services to large failures right
big services fail big small services
fail small right this is important when
you have an application like this so if
you start typing this is my favorite
service to kind of use as an example if
you start typing and Spotify in the
search box after the third character we
start trying to suggest things to you
right guess what the name of that
service is that gives you those
suggestions it's called search suggest
so what it does is it goes out and does
some personalized personalized results
for you based on your search history
based on the stuff in your collection
based on things you've listened to
recently so that one service is fanning
out to like a bunch of other services
any one of those fails this school you
just don't see those results if the
search suggests service itself fails you
type you don't get anything until you
hit return and then you go through the
other search service so it works fine
users may not even notice that so at any
point in time at Spotify huge
percentages are this doesn't happen but
it could happen
large numbers of error services could be
down and like all of our data centers
simultaneously and you and as you sir
wouldn't know it partially because it's
2015 and we built all of our things
assuming that services can fail all the
time we've learned that lesson but it's
also because each of these services that
could be failing is not doing so much
that it not being there is going to ruin
your experience with Spotify so that's
also pretty important so there's the bad
stuff about micro services as well
they're harder to monitor and I couple
minutes ago I said they're easier to
monitor I'm saying they're harder to
monitor well why are they harder to
monitor because we don't have because we
have thousands of instances running
right we have a lot of little services
running with lots of instances of them
so while the surface area on each one of
these is smaller than like a big
monolithic service
there's just way more of them so that's
a challenge for us discovery and
documentation tools is super important
if
you're a back-end developer starting at
Spotify next week you are a wash in a
sea of services that actually up until
recently there was no way to know what
services actually there were we were
pretty bad about that I have to admit
we've gotten way better at it but even
this next week not all services are as
well named as search suggests some of
them are these are services named by
developers with no and expectation the
people outside the company are going to
see them so there's some pretty weirdly
named services that have no context to
what they actually do that's okay but if
you're new developer trying to figure
out how to get album metadata and
there's no search that there's no
service that's called album metadata
retourner which it might be if you wrote
it in and you were in Germany as a joke
sorry bad joke on German sorry uh it's
really hard so we actually have had to
put a lot of work into actually making
it easier for people to find services
and to get documentation on them this is
something that we call systems II this
is relatively new it's only a few weeks
old or spending development for a long
time but it's only really become super
useful the last few weeks when you write
a new service and you check it into
github at Spotify you put a little llamo
file in there and then it populates this
so each one of these is this is just
some of the services right again for
example if you were a new developer
what does Keano service do you have no
idea
hermès java ib sim okay
bootcamp 42 B these are some of the
services that are running and you can
click on these you can get links to
their github repo you can get
documentation on the service itself the
system owner the squad owner monitoring
for the services so this is something we
added relatively recently that's made it
a lot easier but up until this point it
was actually a little bit it was pretty
hard you had to just walk through the
code and github to find stuff another
problem that we have but everyone has
when you talk about what's wrong with
micro services and why there's
so horrible what you always talk here
about is latency right because what's
happening is instead of us calling a
single process or a multi proc service
that goes and collects all the data and
returns it instead we're calling lots of
services and those services are calling
other services and there's latency that
grows through each one of these calls
yeah that's horrible the problem the the
benefit that we've seen to actually
working this way means that we tried to
figure out better ways to handle the
latency one of the things we did was we
stole this idea actually from Netflix
this wasn't our idea where we switch to
using these view aggregation services so
previously in the Spotify client liked
on your phone it would make calls out to
dozens of services and and then kind of
assemble all the data even some of it
didn't really need and then it would
kind of draw you a UI which meant each
one of these have their own latencies
and depending on the connection you were
on could be you know good or bad well we
cheated what we did over like a year and
a half couple years ago is we switch to
this other model this view aggregation
model so running in we have other
services that basically are one per
platform per view more or less that go
and do all this aggregation and return
exactly only what the client needs so it
helps reduce the latency pretty
significantly performance in the
application took a pretty significant
jump like the application has gotten
incredibly more responsive by doing this
it doesn't mean we're not inducing
though you can see by having multiple
servers it just means we're trying to
keep the connections between them looks
short and so it's faster overall we also
decrease a lot of the traffic to the
client so this is worked quite well of
course if you're going to write bad mono
lists you're going to write bad
microservices I think one of the
problems we have because our services
are owned by teams and lots of
developers own lots of services it means
there's fewer eyes on any part of the
code path so if you are writing bad
krummy code yeah you could actually it's
like bad smells can stay there
longer just because there's fewer eyes
on them because there's way more
services although honestly they're all
much smaller so hopefully it isn't it's
good trade-off but it's something I want
to acknowledge now again so these are
kind of generalized some of these
benefits are benefits maybe you already
realizing in your own companies if
you're using micro services but maybe at
a different scale than we are right so
if you're trying to convince your boss
like oh this is a really good way of
working and their boss says well
nobody's ever really done this at scale
like okay well we have over 800 these
are the active services we have another
several hundred that aren't necessarily
active we have 810 based on a couple
days ago I just looked 810 active
services that's about I saw the talk
this morning that's about this I guess
that's about the same as Netflix so
we're doing this on a pretty big scale
okay it's about 10 systems per squad
right so it's about 10 services per
squad about 1.7 per developer or tester
or whomever I didn't look to see what
the roles were I just looked to see who
had LDAP access to production just over
almost two services per everyone who has
the ability to touch a production
service whether they're testing it or
coding against it or just have been to
have access because they're in the right
LDAP Group that's over for all of
technology for the entire technology
department that's at 1.15 ish systems
per member so it's over one service per
developer or tester or agile coach or
manager in the technology group that
also means by the way that will so that
includes like everybody that also
clients developers who don't write any
server code so that gives you kind of
the scale we're talking about Matthias
Bjorn Hayden is talking tomorrow about
mobile development in the mobile track
so his squad his squad in Gothenburg has
like 60 services that they run that
they're responsible for and that's with
currently six back-end developers
that's the kind of way that's kind of
scale we're out this for example is part
of the metadata ingestion pipeline so
this is kind of the complicated this is
a squads owns this this is six
developers six seven developers these
are their systems that they're
responsible for I also wanted to by the
way we're pretty serious about open
sourcing stuff and one thing we've done
pretty recently is open sourced this
technology framework we call Apollo so
we've been doing this micro service
thing for quite a while a couple years
ago we started developing Apollo now
almost all of our micro services are
built on the Apollo framework it's does
a tremendously simple for us to generate
new services we decided that it was
probably useful for everybody not just
us so we've open sourced it it's on
github please feel free fork at will and
put in pull requests that'd be awesome
we are probably open sourcing a lot
we've already open source a lot of other
things but we're talking about doing a
lot more open source around this area so
maybe you'll be able to do System z or
some of the other things some of the
stuff we had to pull out of here like
one of the things this does for us is
when you launch your service it does all
the things - we have a service called
nameless nameless does all the right DNS
routing and stuff so we get all that
free versioning routing and all that
kind of stuff that might be something
we'd open-source some point later and
then Apollo the our Fork of it actually
does all that stuff for free so it makes
things pretty easy so in retrospect or
to sum up we've been doing this micro
services at Spotify for years we do it
at a pretty large scale we do it with
thousands and thousands of running
instances on we're still running on our
own DCs we also do stuff in the cloud as
well but
for our production environment right now
is all our own DC's more for historic
rather than practical or technical
reasons but we also you run these
services in our test environment which
runs on AWS so there's nothing
inherently that would mean we couldn't
do this as well in the cloud we have
been incredibly happy with it because we
have scaled stuff up we can rewrite our
services at will which we do rather than
continue to refactor them or add more
and more technical debt over time we'll
just rewrite them so a lot of our
services are on their fifth or sixth
iteration when we get to a scaling
inflection point all right so you're
rewriting a service it runs great on
fifth for 50 million users and now
you're at 60 million and it's starting
to to be a little bit slow we'll go okay
let's rewrite that we know more now than
we did when we wrote it let's make it
work for 80 million or a hundred million
because of what we've learned so we'll
do that create a new version direct new
clients to that version when the old
coins start stop using the old one just
because they've all upgraded we kill
that one and then we go forward this one
when we hit that limit we've now learned
more and we'll rewrite it again we do
this kind of stuff all the time because
it's really easy with this kind of
architecture and it's working incredibly
well for us so if you're having a hard
if you're at you know your company and
you're trying to convince somebody like
oh no no this is this is really great
they're like well now it's really -
that's a horrible idea you can point to
us you can point to Netflix you can
point to a lot of other companies and
say this is really working for them
they're super super happy with it with
that actually talked faster they meant -
I'm ready to take questions and thank
you
so sorry one thing - yeah I am also
around I'm not wearing a big red sweater
but and also yeah we're hiring by the
way so if for whatever reason though you
don't get a hold of me today you can
also pop me questions on Twitter okay so
thanks for your talk there are some some
questions in the online system one is
are your team's not siloed by being
autonomous how do you make sure teams
are not repeating what others are doing
rather than reusing yeah so there's a I
would actually say our teams are kind of
purposefully a little bit siloed right
we they're autonomous and they own a
mission the way we avoid two teams
trying to end up doing the same thing is
we're pretty clear when we create a team
we give it a mission that does not
overlap with anybody elses mission right
so there's really no reason why the
search team should be rewriting test
infrastructure they may write their own
test infrastructure but they're not
going to redefine test infrastructure
for the company it's just not part of
their mission so we're pretty clear when
we create a new team like what they do
and what they're responsible for and
then we make sure no other team the
company has that same or overlapping
mission so that's one way we avoid it
but actually to avoid kind of multiple
teams not learning what other teams are
learning to make sure we do that we have
this thing called guilds so guilds at
Spotify transcend the entire company and
anyone can create one so we have a
back-end developer guild a Java
developer guild so you plus plus
developer guild and then also stuff like
a Craft Brewers Guild and a photography
guild these are things that kind of
bring developers in multiple teams
together to talk about technology and
that's where we make sure like best
practices and knowledge gets spread
throughout the whole organization so
that we don't ever end up with two teams
kind of building similar technology to
support their missions okay another one
is how do you handle the deployment of
all your microphone services how do we
handle the deployment
and how do we handle the deployment for
microservices so we have so developers
deploy themselves so that should be some
ouch and something I should also say we
actually used to have an Operations team
that handled deployment and incidents we
stopped doing that a while ago
developers teams are now responsible for
their own operations which means also
that we built these really nice tools to
help them do the deployment developers
deploy themselves whenever they want to
and so what happens is there's a tool
that I love this name is tool there's a
tool called the provisioning cannon and
it's basically kind of a one button
thing that is a and shoot instances of
your service out to all the DC's and
create and provisional them on running
on our boxes so we made really good
tools to make it easy for developers to
monitor to respond to incidents and to
provision in provision and deploy
themselves okay so another one is how do
you handle altar is a ssin
authentication across all front-end
services if a desktop client talks with
so many services so one thing I didn't
talk about in our architecture but I
probably could have or should have is
all all client communication goes
through what we call access points so
that's where we do all the security
checks once you're there the access
point will send you out to whatever your
calls you want to make so the acts that
we have a rather than we didn't want to
create kind of a security nightmare or
each service was having to do lots and
lots of authentication on the client so
we do that through kind of these really
lightweight access points that redirect
you to the to the service you're trying
to get to but they handle all the
authentication okay how do you end the
multiple versions with branches yeah so
what we'll do so if you want a branch
say you wanted to make a new version of
the service that had a new API version
what you do is you just fork it and then
create a whole new service from that and
just rewrite usually the services are
small enough there's any kind of
reusable codes already in a library
anyway but for the most part yeah it's
it's pretty lightweight we make it that
these are all small services pretty
you just rewrite a new one I think some
we have some fairly older bigger
services but that's pretty rare okay do
you run integration tests with several
services and how do you deal with the
different versions then so one thing I
think we should be better about than we
are if I'm going to be totally honest we
do a lots and lots of really good unit
testing on a single service we do not do
great integration testing across
multiple services beyond lots and lots
and lots of clients automation so we
will run multiple clients we run them
with tons and tons of automation on them
and that is kind of a proxy for doing
good integration but I think that's
something we actually need to be better
about if I was gonna be honest ok and
what about so someone asks about a
performance testing with as so many
users how do you do with that so that so
one thing that's a that's important
about Spotify is we collect ridiculous
amounts of data around how users are
interacting with the application
probably way way too much actually data
all anonymized I got clear tons and tons
of anonymized user data but that
includes like a lot of stuff around
performance so we're always looking at
inner service performance we're looking
at client performance there's teams that
are 100% focused on that stuff and then
we're looking at how people you interact
with it so we're collecting lots and
lots of real-time data and then
analyzing it always so do you have
automated tests it's another question do
we have automated tests yes we have lots
and lots of automated tests I thought so
ok yeah and what about it can you
imagine working this way with developers
also sitting in India could I imagine
working this way with developers
outsourced in India yeah I don't see
particularly why that would be a problem
the only thing I would say is one thing
that has to be I think one thing that's
kind of important especially if you're
having your own teams be responsible for
operational responsibilities I think it
works pretty well because if you write
crummy code and you get woken up every
night because you're having incidents
you fix that crummy code so I think
that's actually kind of important part
of this if you had some other team
either doing operations or they were
doing the coding and you were doing the
operations there's that you don't feel
the pain but I don't see a particular
reason why like we do this between our
offices in New York Stockholm Gothenburg
and Boston and stuff and that was fine
so remote teams works ok outsource teams
ok should be ok so another question is
Apollo seems to be a java-based and you
say also so based on it isn't that a
little bit odd in a micro-services world
where you could use different languages
so we use different languages outside of
production so there's a reason we use
Java in production and this was a we
didn't use to actually make a rule
around what language you use that's
actually relatively new so most of these
services that Spotify runs on mostly
older versions soif I was actually
mostly written in Python the earlier
versions and we started transitioning to
Java over the over years a little while
ago we noticed that the services that
were having incidents consistently were
the services that were still written in
Python so we more or less said ok from
now on we're only running production
instant production services in Java
there's a couple reasons for it one is
actually developers move from team to
team and developers or code reviewing
between teams all the time and if every
team is writing in different languages
that becomes incredibly complicated
that's I think probably the main reason
we do also because we are like many
companies like when we find a tech
something that seems to work really well
we say this is what we're going to use
now because we don't want to deal with
the pain of stuff not working so that's
the other reason we know
Java works and well now that all the
production code is written in Java all
the production services are in in Java
that makes it really easy for other
people to jump in and help with stuff
behind the scenes we write in zillions
of languages right so where our data
pipelines are written some words using
scalding or so we're using Scala we're
using Erlang we're using lots and lots
of different languages at Spotify to
write our services we're just not using
those languages in production okay and
continue the discussion around
programming so someone asked whether
you're using go the programming language
go we are but not in production okay do
you have a bootstrapping tool per
language for service no so that's the
other thing
teams can more or less again like we
this is it was interesting when we made
this rule by the time we made the rule
about Java every almost every team is
already in Java so it wasn't a big deal
but for us to dictate a language was
actually kind of a big deal for Spotify
it's not usually the way we work
but what we have done for all those
other things like data pipelines and
stuff one of the things we've done is
we've said we're going to make it super
super easy to do the thing we think is
probably the best way to do it we're not
if you want to do it something else
you're totally willing to do that we're
just not going to invest a lot in our
infrastructure to make that easy so for
example the the thing I showed Apollo is
Java right because that's what we use to
do this we have other tools for Python
we've lots of Python stuff or that kind
of thing but now if you're we make it
really easy to do Java and if you want
to do something else that's fine but
then you have to do the work yourself
okay another question around the the
architecture do you have a separate view
aggregate for each device and each app
yes okay so each view is going to have
its own each view per platform so that
was an easy one yep how do you keep the
teams in sync and especially if one
teams depends on another one and
shouldn't there be like an architect for
doing these kinds of glue
coordination so we don't that's the
point I think is the teams are
autonomous so we don't really keep them
in sync they do each of them is doing
their own thing on their own schedule on
their own pace so we actually and part
of that is we have nearly some but as
few as possible dependencies so there is
a lot of the work that we've done to set
up our organization is to reduce the
amount that teams have to synchronize
between each other and dependencies
between them and micro service is
actually super helpful for them there is
another one that that I think is quite
interesting so how would you migrate
away from a monolith because I mean that
seems to be what everyone is doing but
you haven't spoken about it I think yeah
I think I've been right when I talk to
other companies like when I go and visit
other companies a lot of them tell me oh
that's really great for you but you kind
of more or less started that way and we
have this really important system that's
really old and we it's massive and
there's no way we can switch to micro
services because everything depends on
this thing and it's I mean it's I would
love to say well I'm not a consultant so
I don't go around and like make help
other companies do this I just have like
my own opinion about it which may or may
not be right but the way I would look at
this is every time I've talked to these
companies like what is this big system
doing well it's doing ABCD and E okay
well that's one two three four five
different micro services you could have
maybe you can pull this functionality
out then you pull this functionality I
was just decomposing I mean I don't have
a magic bullet for that but I hear this
all the time and to me when you're it's
easy for me because I'm outside it
obviously like I'm not in your company
and I don't have to deal with all the
the finance people or whatever but what
I'll tell you is if you can step back a
little bit really kind of look at your
system like a black box and just think
about what it does you can start to
figure out what you can tease out and
then sort of tease each of those threads
out and that seems to be a good pattern
I've seen work
other companies okay so it continued the
discussion about autonomy how do you
balance a team autonomy with
requirements that each service must do
such as monitoring so it's discovery
health checks locking correlation IDs so
there's very little that a service must
do but again that's also kind of
assuming developers don't care about the
services they write which is silly of
course they care about it they want them
to good to be good so you know nobody's
saying you have to monitor your service
there's no rule if you don't monitor
your service that would be everyone will
say like hey why aren't you monitoring
your service that seems stupid but any
developer who knows what they're doing
like and we hire people know what
they're doing like says it like we
should monitor your service we give you
lots of tools to make it super easy to
do that if you don't do it it's kind of
the question is why aren't you um so we
they have a lot of choices on how they
do stuff we don't tell them we don't we
dictate almost nothing but we make it
super easy to do it in the easy way and
so why do you need to innovate on
deploying services that's not really
something you as a developer writing
services need to care about if you have
a really easy way to deploy your service
you'll just use it because that's easier
and you can spend more time actually
coding so that's kind of we don't
enforce rules we kind of make it easy to
do the smart thing and then make it a
little bit painful to not do this the
obvious thing but no one tells
developers you have to do this we have
to do that ok with the exception of Java
in production so one rule we have there
is another more detailed question so how
do you do a distributed request tracing
request racing across all services I do
not have a good I do not have the answer
to that question off top my head I'm a
VP I'm not a developer something I have
to apologize for that
I wrote so that's kind of I I wasn't I
ended up in this track and in an unusual
way
but to say that so yeah it was my fault
so that I don't know what we're doing
about it so how do you do code reviews
across teams
oh so that's so so the way we do both
code review is actually an on call so if
you know anything about Spotify which
you may or may not we have a matrix
organization which means like the team's
being the vertical these full-stack
autonomous teams are the vertical on the
horizontal is sort of what we call
chapters and in the chapter would be all
the backend engineers across multiple
teams so for example the way we would do
code reviews is the folks in your
chapter who may be in your team or they
might be in another team that's part of
your organization they do code review
you all do code reviews for each other
if for some reason somebody in your
chapter can't do it there's the guild
and you can ask somebody in the guild
all the backend engineers are in the
same guild so there's pretty easy ways
to ask for code reviews we actually
require two code reviews of every
check-in you have to get two plus ones
to actually merge your pull request
that's something that we've done for
ever and ever
and it's worked really really well for
us so on call kind of works the same on
call rotations usually in the chapter so
you are not on call for all ten of your
services every night of the year you
have a chapter usually is like seven or
eight people and because you're all code
reviewing each other's code and because
all your services are actually quite
small you can cover each other's systems
pretty well right okay I guess final
question what is your biggest area of
technical depth our biggest area of
technical debt in the entire company or
in the backends I guess I think we have
we we have I think there's one service
in particular if you know Swedish it's a
service we call Sluss k-- which is a
Swedish for trash and that's a service
that kind of looks like a key value
store and it's kind of generic which
meant that it was a service where if you
needed something to be
in a kind of a persistent key value
store you just add it to there for years
now and so it's basically this kind of
mess data repository that owns way too
many important things just because it
was easier to throw something in there
than to make a new key value store yeah
so that's probably our number one kind
of technical debt piece okay thanks a
lot
table to talk thanks thank you the Quinn
Owens</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>