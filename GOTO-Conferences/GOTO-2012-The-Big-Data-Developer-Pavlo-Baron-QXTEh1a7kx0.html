<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • The Big Data Developer • Pavlo Baron | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • The Big Data Developer • Pavlo Baron - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • The Big Data Developer • Pavlo Baron</b></h2><h5 class="post__date">2013-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QXTEh1a7kx0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this is a talk actually about what it
means to be a big data developer and I
need to one you in advance that I'm
maybe too sarcastic for this world so if
you can take it could be difficult but
anyway i hope that i can tell you a
little bit about this these things that
are necessary to become a pic to be big
data developer first of all a short
disclaimer i will not flame where i will
not rent i will not bash any product or
do any similar here i will be sarcastic
but it's not that and it's just more
because I'm just I'm just too tired of
it you know after a couple of years
fighting but anyway I hope you enjoy
this so what I will do is this sort of
I'm not an actor but I will try to do a
sort of stand up comedy playing two
persons one person will ask stupid
questions the other person will try to
answer these stupid questions well I
don't want to get judged by who is who
this person is actually me but anyway
okay let's start it so this is the guy
asking questions so that's just hey dude
I'm a big data developer who isn't this
room is actually a big data developer
would say I'm dealing with big data as a
developer okay cool um what we have
enormous data and we continuously read
articles on high scalability calm does
that everybody read articles on high
scalability calm no you should well
actually you definitely should but the
problem there is that it's just a you
know sort of compressed information and
you need much more than that that I will
mention today um
but anyway I read it so come on so this
guy's has big data is easy it's like
normal data but maybe a little bit
bigger so let's discuss the size of the
data um what is this noise is it me I
think I did eat something but it's not
that bad okay the problem so is this big
how would you say this is the smallest
fish in the world as far as i could ask
Wikipedia um is it big this one is right
like its massive I can you know I can
compare it I can compare the size
whatever it means so let me define big
data just to be sure that it's big
enough it's petabytes of data every hour
on different continents with complex
relations and with the need to analyze
them in almost real time for anomalies
and to visualize them for your
management you can call this big data do
we agree on this what I learned today
this is Sun right anyway
is everything below this is Mickey Mouse
data so good news is that you can
intentionally grow this data can become
bigger and bigger and bigger it just
depends on use case maybe technology or
the common sense but let's the other guy
do is play his role no problem do it
data is data it's just same principles
so is it really same principles when you
deal with huge amounts of data or ever
they come wherever they come from so
let's consider storage first okay hey no
problem dude started such as a database
does it it does everybody agree on that
well I would dubbed it because the
storage capacity of one single box is
limited we all know this and the common
sense also knows this so it doesn't
matter how big you will grow your
machinery somewhere is the limit and
it's typically physics no problem dude
for sure you're talking about sequel
it's all grandpa's stuff it doesn't
scale a little it doesn't count anymore
we have this new shiny world so okay
that sounds to this one when you expect
this big data I've even learned this
pronunciation this big data because it's
big data but this big data just sounds
cooler when you expect big data you need
to scale very far let's build on
distribution and combine theoretically
unlimited amount of machines yada yada
yada on single distributed storage there
is no way around this except you have
invent something like a plague black
hole DB
actually there is as far as I saw there
is a back end for my sequel called black
hole it's pretty nice I mean somebody
already invented this I don't really
know what what it means and that didn't
play with this but anyway the name is
already already well kind of protected
no problem did no sequel scales and it's
cool that's the most important thing
this is the most important architectural
factor these days the coolness offer of
something don't forget I'm sarcastic so
those no sequel guys they achieve these
things through shouting you know it's
like shouting hides this distribution
stuff for me I don't need to take care
of old it hmm so building upon
distribution is from my experiences much
harder than anything you've seen or done
before I mean you just buy every
possible problem that you can buy with
parallelism but you also have the
network in there and those of you who
are actually dealing with networks and
things like that would maybe agree that
every single inch of a wire makes the
whole thing unreliable more unreliable
than anything else so the best thing is
that you would just run the whole thing
on the piece of xant and then that's it
but even that can crash and become slow
anyway it's the hardest thing that you
could ever do except you fed a crowd
with seven brats and walked up on the
water everybody Christian you or at
least has heard about this religion
doesn't matter it's not a religious
discussion here no problem did I just
have this cap you know does everybody
know hear what cap theorem is do you
really know in detail what kept theorem
is well actually I don't so uh
I think nobody knows in depth what
capture em really means so it's a lot of
discussion around this a term steve has
had a great presentation on that
explaining react and there are new terms
well actually it's not that clear but
anyway you hate no problem that I will
just pick two of three that's so simple
it's absolutely easy right well the only
thing that is absolutely certain about
distributed systems is that some parts
of it would just one day fail and you
will absolutely have no idea what
happens there that's absolutely certain
that's a given you can try to protect
yourself against this through monitoring
and through people who are bringing up
systems and stuff but anyway the pee in
this cap when you deal with distributed
system systems and when you start
distributing you have the pain to throw
your data on several machines on many
machines so p is a given your system has
to be a petition tolerant or it will
just suck so you will play with c vs. a
you know consistency versus availability
just moving the slider around it's not
the black or white it just depends on
the use case even if you would use one
of the available are awesome datos to us
who are implementing well sort of
dynamic paper like a sanrio react it's
it's not always just black or white you
will not say that for every use case in
your application you will do this it's
always consistent or it's always
available maybe it's somewhere in the
middle for this one part it's sufficient
to be inconsistent but for this one not
no problem dude this sharding this works
seamlessly and I don't need to take care
of anything
well let me dub this one again because
for example it's one of the hardest
challenges when you start distributing
shouting however you call it that when
you need to search in the data to
traverse through data when you have like
graphs for example this is one of the
hardest problems to distribute the graph
because traversals are not simple I mean
you need to keep your data together
somewhere and it's also valid for any
other thing because you will in any
system you will hit one node for example
and if this note doesn't have the data
it needs to know fast where the data is
so you have like it's not bigger one
that you used to jump to the two that
you have to access your data but
actually it will go to the next hop and
it's absolutely necessary to know where
it is in the whole system so there is
another hard challenge in this this
whole shouting stuff intent I mean just
going a little bit deeper than marketing
slides this is called nave hashing does
anybody know what near fishing is nay
fetching would be when when you have 10
machines and for hatching in order to
know where the data is you would build
like a hash function typically this
function with knife hashing would depend
on the amount on the number of machines
so whenever you change the number of
machines and the most positive way to
change the number of machines is to add
one the most stupid one is just to lose
one without you knowing that it's gone
but anyway you would have to change your
hash function because when you when you
go when you use module for example so
it's just complete completely different
function there and thus you would have
to rehash almost the whole thing you
just move the data around because next
time when somebody excesses this data
through the new function it will not
find in there you need to help further
and further and further so you need to
rehash
so the opposite of this is consistent
hashing anyway the trade-off between
data locality data locality is
absolutely important thing in dealing
with distribution when you process data
you need to be as close to the data as
possible consistency availability it's
much more than just see AP it's like you
know readwrite search speed and Layton
sees everywhere and so on and so forth
so it's a real hard trade-off you need
to consider it as a big data developer
wherever it might be you need to take
this into consideration ain't no problem
dude no sequel I mean I'm really
sarcastic right I wouldn't use this term
for to describe a technology because
it's just movement you know but anyway
no sequel will write a synchronously and
do MapReduce that's the magic word they
put the Mac produce to find data well
asynchronous systems are busy well at
least it's necessary to create a system
which is distributed where parts of the
system communicate with each other in an
asynchronous way through a message
passing for example that's the best
example so that there are even books
that are not even taking anything else
in account but a synchronous system so
the problem there is that with this
synchronous stuff is that it leads to
the consideration of eventual
consistency does everybody know here
different sorts of eventual Casilla
consistency like read your own rights
and so on and so forth I mean I wouldn't
elaborate on this one we can discuss it
afterwards if you like but anyway there
are different use case and it's again
use cases we don't want to have a
venture
consistence you just want to have
complete consistency however you call
this thing when you try to transfer an
amount of data from A to B well you
don't want to run into situation where
you have two accounts having same
transferred data or transferred amount
no problem did of course I don't have
money to transfer just like starting a
new startup no I'm just kidding I need
to store lots of data and I can from any
amount of data until no sequel and that
will deal with it automatically it will
just work did you ever experience
somebody who things like that like pick
product a and throw this data into it
and it will do well actually I did so
maybe it's not completely free from my
own experiences you well would you want
to throw your data which is maybe the
most important thing the most important
asset of your company somewhere in just
hope that it works what happens if you
just missed the target so concerning the
storage data locality redundancy
consistent hashing for a distributed
system in order to find data very fast
even with additional hopes if those
notes are not available it's still close
to bigger one efficiency eventual
consistency as a concept combined with
use case driven storage concept of
storage design those are key principles
in succeeding with huge data with
starring huge data mounts that's what i
would call Big Data development
everybody agree on that let's go on when
you have data data when you have data
you need to bring this data maybe to
your customers in whatever well
presentation form so it might be a web
application or something or just a an
API whatever so let's discuss this ain't
no problem dude it's always and I really
experienced people saying that it's
always the database being the bottleneck
in the whole chain somewhere behind the
scenes so it doesn't matter how fast are
you are you are up front it's not my web
service so well when you have thousand
millions millions however parallel
requests per second begging for data the
first mile that's mean that means from
your customer to the first part of the
whole chain this will quickly become the
bottleneck because I mean what will
happen there is stuff would just get
cute discard it all the packets you will
lose them you can even monitor this but
how much does monitoring help you if you
lose customers up front when you expect
people to use your platform whatever it
is in a way that they don't see the
difference between the big time and
normal time this will quickly become
your bottleneck just forget a database
ain't no problem dude I'll get my cell
buy myself some real sexy Harbor
yeah I bet you will that's my answer I
need to play these tools it's a little
bit difficult but well under the high
load when load is real high the problem
did that the limits that you will hit
our physics again I mean the whole thing
somewhere in parts or everywhere will
start to crack so what will happen is
you will just burn your hard disks your
boards will melt your cards whatever all
the wires and you will heat up to
maximum end and heating is a real big
problem still I mean not every company
is able to go to Sweden or whatever
Google went to and and you know to have
a known iceberg just for the next couple
of years anyway it's not about this sexy
hardware whatever sexy hardware means
but I really know people who are saying
hey sexy hot well it's at least not the
term that I would use well anyway it's
just about being able to replace this
pieces of this hardware having different
paths through the system so you
distribute the whole thing once again
you just grow in a horizontal way kind
of resigned redundancies and with
concepts that would allow you to lose
part of the system and to bring our new
power to bring up new parts of the
system however big it might be even
while your system is running that's the
very important thing what can happen to
you with things like knife hashing is
for example when you completely depend
on this in a knave way on the end the
amount of of the number of your notes in
the system you would have to bring
everything down turn on the new note
bring everything everything up and wait
for the complete rehashing
when or for caches getting warm and
stuff like that anyway when you want to
keep this first mile scalable you might
have to pay more money for that this
infrastructure becomes more and more
expensive and well the problem is then
that you wouldn't want to have it
automatically you shouldn't just accept
a vendor saying you hate you bring up a
new machine here well you will win like
five percent more performance here
whatever performance this performance is
a very difficult term test different
facets but anyway you will with this
machine which is like 10 millions a year
you will have five percent more
throughput here you will rather try to
press out the maximum of your machines
that you have of the networks that you
have and look for technologies how you
can do this a mr. Derek Ennis says again
greeting to Derek and Esther video he
says you need to saturate your network
so yeah that's the term ok let's go on
um no problem dude I will probably write
on my own of course or get myself a sexy
see 10k server does everybody know what
C 10 KS c 10k is a short form something
that is able to do more than 10,000
connections per second so apache HTTP
demon has limitations there are
benchmarks proving this so it's like
with 4000 or something around this
when you use this web server you will
start you will start performing the
expected way and there are servers that
go much further than that just think of
engineer or something like that so the
other thing is that I've promised that i
will not bash or rent about any project
any product here anyway you can build
there is a technology that you can use
to build something like that and of
course the first thing that a big that
big data developer should do is build
all stuff on your own well maybe you
will get this one or write your own one
but it won't help you when you users are
distributed all over the world the
problem is that the distance between
them in your first machine whatever it
is if it's load balance or something
it's just a big geographically it's
still too far away so if you have this
massive load up front you would have to
go for something like CDN does everybody
know about Syrians content delivery
networks Akamai well the peak times of
well peak times were web or internet is
being accessed the drive like maybe
sixty to seventy percent of the whole
traffic that they have real interesting
algorithms on optimizing routing and
things like that so you what you would
do you would use their infrastructure
you cannot pay for this infrastructure
in stores infrastructure because they
are already in maybe a thousand data
centers all over the world they are
always closer to your customer than you
they are doing dns tricks that put trick
your customer to a different server so
it's still w w your awesome startup calm
so you just want to push this load out
of yourself just the way
to somebody else when you have this pain
you will have to pay for this because
it's not that unexpensive ain't no
problem dude and I will push my whole
platform out to the cloud who actually
does cloud computing in this room I mean
you are an expert in this year of course
okay so you'd like provide us right no
just use it provider would you suggest
somebody something like this sure sure
you will well it's more flexible and
scales like hell of course sure you will
see the scalability actually in the bill
coming every month well anyway well what
happens in this cloud setups when you go
for them it's still a difficult problem
for example with easy to with easy to
and the whole infrastructure the problem
there is that basically you cannot
predict reliably how close you are
software piece of software that will
process the data will actually be to the
data it processes so for you it's just
completely transparent but it needs to
be as close to the data as possible so
you can process it fast if you start
moving stuff around it will just cost
you time and whenever in some different
cases whenever virtual machines just get
moved you can see it in your private
clouds however you call it in your data
center so just the world stops for a
while just for a little but think of
gazillions of parallel requests coming
per second of course you can force data
locality in cloud setups when you just
pay more it's sort of agreement of
course we can rent it no problem with
that anyway you need to have the pain
you need to have the money for that
data locality geographic speciality it's
very important in provisioning data
dedicated virtualization in context
prick computability content pre-compute
ability what would it mean you can use
cdns to host your data or host whole
applications or host pages that are
being pre computed by you so you don't
hit the database you don't get to the
last mile you just present some sort of
inconsistent data for a moment or for 15
minutes or something that's a way to do
this if you have this Geographic problem
and again it's use K is driven
notification you need to have the right
use case for the right cloud based
strategy and it's not again it's not
black or white those are key principles
when you're a big data developer let's
talk about processing of data hey no
problem dude it's just easy that's
classic MapReduce right you would
process everything with MapReduce even
your own grandmother well I do almost
agree the problem here is that does
everybody know this MapReduce pictures
and stuff yeah so you have different
phases like like map and reduce right so
the hardest phase in those to map and
reduce is called split it would really
suck if it would be called split
MapReduce it's not a cool name but the
problem that you have you have two huge
piles of data when you it just depends
on the product that you pick but anyway
either it's a file system or a virtual
file system or something you would need
to bring your data to this data store
and if you try to move around terabyte
of data
you just count minutes hours whatever
depends on the infrastructure no problem
dude so i will just this could be a
solution i would just right straight to
the data store of this MapReduce
framework it will just be fast like hell
afterwards it can be this would be my
answer but there is still about what if
in your map face you need to search the
data and you still have to think of data
locality so when you want to search data
you would probably go for an index or
something that's what everybody does of
course no problem dude I would just take
a cool indexing search library engine
whatever it will find my debt in a snap
let's ask the question would it really
do this you just need to imagine you
have a pile of data where well a big
carrots in there and it's distributed
over notes that you have no idea about
this about where the data actually is
which data you can ask for that when you
start indexing this you go through the
process that the pieces of index would
have to be on same machines because it
wouldn't make much sense to have index
here and here huge pile of data when you
may produce in search during mmm face
for example whatever face in your whole
pipeline there you would love to find
the data where the process of this data
is actually running on either through
hash-based key or key based when you
access or just searching for data the
availability and unavailability of
pieces here for system is that is
another problem with that so data and
index locality both together if you have
this use case
combined with filling this data store of
this framework directly or preparing
your data for it in a form that it can
consume it locally data locally on
different machines as well as the use
case driven technology usage use case
driven technology will use it don't
throw one tool on everything that's the
message here if you want it to work you
don't want to do this that's those are
the factors that you need to take into
consideration when you want to be a big
data developer orange just to get a
little bit better better through
learning so how about analytics does
anybody here do analytics statistics
machine learning yada yada yada cool
stuff I love it really hey but this guy
just says hey the MapReduce framework
will do this kind of magically
automatically just I mean just take it
and it does it it does on the Linux so I
can just put my data into this thing and
afterwards I can do analytics so are you
really sure that there aren't two
different basic use cases analyzing flat
analyzing data which actually is flying
in near real time and doing batch
analytics afterwards just consider the
time factor and it sounds a little bit
different right so do you really think
that this are both use cases for and
MapReduce framework
I would just keep this one actually as
it was in Amsterdam it was like the
favorite slide here those guys anyway
you don't want to believe in map
reducing for both use cases really and
again back to this time thinking real
time means time it really means time
there are two basic sorts of real time
it's either hot real time of soft real
time so it just depends on what you do
in the case of a problem how would you
manage this but anyway it's time it's
fixed so it doesn't mean that it's as
fast as possible or why well you can get
a coffee or something where you use is
still waiting for that it's acceptable
that he's getting the request the
response in five minutes and it's still
real time no it's not but anyway you can
call it near real time the problem here
is that when you consider the fixed time
in a distributed system something like
this will happen you cannot predict time
things in a distributed system really
you will just die in a fire here no
problem did that's the typical answer I
love it I'll get myself a sexy hardware
it will do everything sexy hardware does
everything of course you don't need even
your head your brain to in order to
install hardware just think of me being
sarcastic right but that you just start
throwing bad eggs at me I'm sure that
this guy will get himself some rocket
fast hardware so again you cannot
predict fixed and fixed i'm in mapreduce
it's impossible you cannot ensure the
completeness of data that's the next
problem when pieces when parts of your
data
I just have just gone you cannot
mapreduce this data because it don't
have it well if you think of redundancy
maybe then yes but still it's possible
to lose the whole thing that and
causality is another problem when you
try to to analyze for event causality
what happens after what it's a real hard
problem in a distributed system to you
know to make an order of things that
happen for analytics because for example
time is unreliable in a distributed
system and so on so if you need to
predict better while the date is flying
in for causality and stuff you would
need to CP your data is this term for
you CP complex event processing java
development java developers around
system like systems like a spa ok no
problem so what it actually does is you
have a sliding window going through a
through an endless stream of things that
happen you can define this window to be
like 10 minutes and within this window
you can analyze using a special language
which which is different for every
platform but anyway you can analyze how
things happened depending on what so
it's the typical example is like when
you when you hear a church bells and and
you see a woman in a white dress and and
rice is flying around so it it's pretty
sure that this is a marriage you know so
this causality chain here that's for
that and you can do this in real time
well in near real time again real time
things are real hard the most important
thing in this whole tool zoom is that as
of now your typical bi tool that you
would use maybe it's completely unaware
of no sequel stores existing around
that's one big problem really
because they would expect like you know
using jdbc odbc accessing this data
store and it's just a subset of data
that you can get out or at least a
subset of this protocol how you can
access the data because they don't
implement everything okay depends on the
solution no problem did my MapReduce to
can completely hide this for me I can
just concentrate on doing stuff okay
that's the killer argument I just want
to do this well about mathematics I
would just say that it's not possible to
call yourself a big data developer when
when you well when you don't work with
mathematics statistics probability and
machine learning and stuff like that
because that's what it is for any
analytical task you would do you would
need a whole lot of mathematics so you
need math statistics and stuff like
anybody in this world needs an iPad or
two or three I know people who have
three I don't know why excuse me they
have no kids especially this guy doesn't
anyway it's a sort of well let's give it
so the principle is separation of
immediate and post fact analytics and
complex event processing of data streams
its data flies by combined oh my god
let's cut okay combined with use case
driven technology usage instead use case
driven technology usage once again and
statistical knowledge are key principles
here in analytics of big data
processing that's big data development
so we forgot this visualization
visualization part ain't no problem dude
I have no idea about it hey me neither
have no idea about it really I mean it's
a science on it on its own but I know
people who are real deep into it and and
you know creating different sorts of
maps how to present big data I mean you
cannot do this with an ex oh
anymore sign it's just too much data you
need to group it kind of you need to
color it sort of and and so on and so
forth so just seek for this and you find
very many of them hmm so what's the
message so well he says I'm as modest
well I'm sometimes smartest I don't like
to be smartest but sometimes it's
necessary to convince people anyway um I
would just suggest following for a big
data development developer don't trust
and half-baked it blog posts how Beckett
Mick means in this case people who are
writing them are real small but they
wouldn't start with Adam and Adam and
Eve you know they have read all this
they have used almost everything in
those does everybody here in this room
have all these books you should get
yourself something really well actually
if you want to do networking you will
not want to miss this one at least
statistics and so on there is a there's
plenty of literature on it this is a
great one by the way to understand how
database systems work and of course a
tannenbaum from you if you big data
develop or not you should have this book
and this book should be just just should
look like crap completely used you know
there is no electronic version of it as
far as i remember but anyway let's let's
briefly talk about full stack do you
know this term full stack developer okay
it's a very emerging and it's getting
popular popular this term so the full
stack is all this and when I speak of
hardware here I don't mean only you are
int'l based system but all your load
balancers and even if you want to go for
real speed you will go for GPUs and and
you will do some fpga on some chips and
stuff so actually I cannot call myself a
big data developer if I don't do all
this but well I still learn so maybe I'm
just one step further to know what I
miss but it will take a lot of time to
know all that here time is a big problem
you know okay anyway mathematics
algorithms hardware storage very
important thing operating system the
biggest problem that i have with with
the Java world is not that the JVM is
crap the JVM is awesome but many people
just ignore the fact that it is there
and are in the situations where they use
hibernate for not even lazy loading
65,000 objects into the memory I mean
and fighting things with with half baked
on written caches and stuff you know
when you have on the operating system
level you have like three four of them
depending on what system you are running
you just need to know about this and to
trust into those guys who are
implementing kernels and stuff they will
be always better than anybody else
because they concentrate only on this
data stores abstraction our storage and
tools two chains that's very important
not one single two different languages
very important there are different
languages I mean well I should
this one no no no no problem about this
you can do with several different things
several different things even know but
there are platforms and languages that
better suit some use cases and you
should go for them and look at them and
just be curious about them the existence
of them check them out and not just
throw one single tool and everything and
say hey come on it will work I can still
think you're adopted words so it might
work kind of network distribution there
is a whole bunch of literature on that
it's a very complex it's real real very
complex area and statistics
visualization and all that so you need
math absolutely like an iPad and you
also when your big data developer you
need to know your point of pain there
are not many companies in the business
area i mean like let's just exclude turn
and so very special or medical
institutes they are also very special
doing in different dimension dimensions
doing pictures and stuff analyzing
animal anomalous and and all that just
exclude this from the enterprise of
business perspective of you or a point
of view you have twitter facebook from
google they have all the problems of
course they are the pioneers of all that
so they have also the data they have
also there's a whole pain here but maybe
you are in your company and that's very
important to understand maybe just don't
have this pain at all or just one of
these beans maybe you just need to
satisfy your customers coming in and
have millions of parallel connections
well the problem is it's boring when you
don't have this because you cannot play
with the technology with the core
technology well anyway you need to turn
on your brain and this is the most
important thing in your tool chain just
accept if you don't have these problems
just skip it maybe you should look for
another company okay
that's it thank you very much the Torah
of different tools and mindsets and
technology and play well you can see
well I mean curiosity is a very
important factor and the wish to learn
you need to combine a team to have a
team of people who maybe have different
focuses but you need this knowledge
overall at least you need to be aware of
things and you need to know that you
cannot know everything so you need I
mean you just throw anything somewhere
and you think okay it will store you
should go 12 steps further just down to
the ground seeing how things work most
of those data stores for example our
open source solutions just study the
code look at the code what they do I
mean code is readable even if it's
written in darling and still readable no
kidding erling is a cool language in
this simple so maybe you should have a
look at it one of the german guys will
have a look at it i hope so but anyway
you need to actually in order to do
something like this you will have to
grow your loan and your knowledge more
and more maybe with several people i
don't really believe in the concept of
somebody called data scientist because i
think that as far as as it gets down to
a developer he also needs to be data
scientist because i hate to implement
mathematical formulas that I don't
understand so I need to know what is
that one of the principles maybe not
that deep because it's all about
tinkering data out of something training
your classifiers whatnot but anyway I
need to understand this I don't want to
use a library or an algorithm written by
somebody else if I have to
run this in production later important
to speed I then the period of all the
ramifications of the data wouldn't it be
smart just to set the jobs our
anticipated way we hadn't known regular
distributed they then you should be able
to send the jobs to the day and then we
get results yes you can actually you can
but it's sort of similar because you
would send out jobs to to the machines
holding the data so where is the
difference I mean if however you call it
MapReduce or whatnot you will you will
wait for something delivering results
maybe pre-calculated precomputed so
that's basically the same concept thank
you very much guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>