<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • Don't Give Up on Serializability Just Yet • Neha Nerula | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • Don't Give Up on Serializability Just Yet • Neha Nerula - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • Don't Give Up on Serializability Just Yet • Neha Nerula</b></h2><h5 class="post__date">2015-07-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/utl3RyJE5SI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">right so I'm here today to talk about
this concept called sterilized ability
and so in this talk I'm gonna tell you a
little bit about what it means when you
want to use it when you might not want
to use it and I'm gonna talk a little
bit about my journey through the PhD and
how I got interested in this in the
first place so first a little background
on me I don't even know if I'm a PhD
candidate anymore because I defended
yesterday so that was very exciting
thank you I don't know what my formal
status is right now but I still have to
finish my thesis so it's not all not all
over I used to work at Google I was a
software engineer there where I worked
on a storage system and I also worked a
little bit on browser security and my
research at MIT has mostly been about
fast transactions for multi-core
databases and distributed systems okay
so another sort of little bit of insight
into what I do and who we are how many
of you guys know this handsome gentleman
James Mickens oh you guys I'm about to
change a world okay so you got to go out
there Google James Mickens and go read
his writings okay he is an academic
researcher from MSR Microsoft Research
and he just writes the most entertaining
things about computer science that I've
ever seen he also gives really great
talks but I think that this quote is is
really apt for the kind of work that I
do and the kind of work that people in
my group do so I'm gonna read it however
the most well just a little background
this is James sort of like theorizing on
in the zombie apocalypse who he would
want on his team okay so however the
most important person in my gang will be
a systems programmer a person who can
debug a device driver or a distributed
system is a person who can be trusted in
a Hobbesian nightmare of breathtaking
scope a systems programmer has seen the
terrors of the world and understood the
intrinsic horror of existence every
systems programmer I know who reads this
is like yeah this is so true so let's go
on this journey together this is gonna
be fun yeah so okay so let's talk about
this I want you to get from this talk
and understanding about what the concept
of serializability means in my why you
might want to care about it in your
database so how many people here work
a system that is backed by database okay
great and how many people have had
trouble or have had their lives in some
way made worse by this database yeah
okay so databases are complicated
databases are confusing and we need them
to perform really well unfortunately for
our applications so here are three
examples whatsapp sees a peek message
rate of over a million messages per
second this is a little disturbing but
it turns out Facebook is a fifth of all
of the pageviews in the United States
and Twitter has to handle many millions
of messages for mobile devices for
analytics and you know maybe we aren't
all running whatsapp or a Facebook or a
Twitter but you know we can dream we can
hope to get there one day and data is
only increasing in size well that's
pretty unfortunate because databases are
pretty difficult to scale so it's pretty
easy to scale application servers you
have more traffic to your website you
have more load more mobile apps and you
just add more servers it works pretty
well you can handle more traffic
databases don't work this way
they're stateful inherently it's very
difficult to scale your database and so
what people usually do to handle this
problem is you buy a bigger database and
then a bigger one and then a bigger one
and eventually you're spending a lot of
money on some really huge instance with
a lot of cores that you're not even
using all the time and you know
someone's like you need to spend less
money on your database you got to fix
this situation things are slow things
are bad and so the direction that a lot
of people go in is they move towards
distributed databases so smaller
databases we partition our data amongst
them you know and we can think about
things like elasticity and spinning up
more databases to handle traffic and
things like that that rarely ever works
in practice but it's a great idea and so
what we're doing is we're partitioning
our data amongst the different servers
and this is a distributed database so
here's an example of what that might
look like you might have your widgets
table and you're gonna take your widgets
table and you're gonna choose some
partitioning key and that might be the
widget ID or something like that and
you're gonna break your table up into
and you're gonna put them on different
servers okay so this is called a
horizontal partitioning and what we've
got going here is we've got a range of
widget IDs on each server so now
unfortunately our application servers
have to get a lot more complicated they
need to know about the range
partitioning they need to know how to
figure out where data is and they have
to probably talk to all the databases
too in order to put data back together
and do different kinds of queries now
when once we've moved to this model of a
distributed database we've lost a lot of
features that we used to have in our one
monolithic database we've lost secondary
indexes
we've lost foreign key constraints we've
lost query optimization because
oftentimes in this situation you don't
really have queries anymore you're doing
key value lookups you've also lost joins
and so all of that work has not just
disappeared it's now being done by your
web application server and so there's
two problems here the first problem is
that now when we add another web
application server it has to talk to all
the databases and we've got this problem
where we're adding a lot of new
contention it sorry connections and also
you know if you need to add another
server you've got to figure out how to
shuffle your data around and this is
really complex this is a pretty big
challenge it's hard to do this without
actually having downtime in your system
and you know god forbid you actually
want to change which key you partition
your data on that could be a nightmare
so this is kind of the setup this is
where I was coming from this situation
where distributed databases were be
starting to be used more often but they
were kind of terrible and came with this
huge set of problems so I was at Google
up until around 2007 and it was a really
exciting time because Google was in the
middle of building all these really cool
distributed systems and so I was super
excited to be there I was very into it
Google built MapReduce which influenced
a lot of the big data analytics that
we're doing today they have the Google
file system which was a direct precursor
to HDFS and you know they had BigTable
and it just seemed like this was the
future BigTable is a very large
distributed database that's a key value
store
so I was I was very excited about this
these ideas were being developed by
other companies simultaneously there was
Cassandra MongoDB Amazon published a
paper in 2007 about their database
dynamo which was very influential and
you know this was a really big thing
this is very exciting distributed was a
future so let's talk a little bit about
you know the pros and cons of that so I
went to grad school and I decided I
wanted to work in this area I wanted to
work on distributed systems because I
thought they were really cool and they
were the future and they were really
exciting ideas there and so here's the
reason that I think these systems got a
lot of traction first of all they were
in memory so there was no disk sitting
there imposing latency on your query
requests they were highly scalable very
highly scalable BigTable I know for sure
ran on tens of thousands of machines
they were transparently fault-tolerant
so when you're running with that many
machines failures are going to happen
and you don't want to have to wake
someone up every single time that does
happen and so these systems were great
because they replicated for failure
tolerance and they also provided geo
replication so these days we have users
all over the world and we don't
necessarily want to pay that 300 to 400
millisecond latency hit in order to go
get our data from halfway around the
world we want to keep data close to
users but we still need to keep that
data in sync and so a lot of these
systems provided these kinds of things
out of the box now here are the cons
these systems had a lot of faults -
there was no schema so moving from a key
from a database to a key value store you
completely lose any idea of a schema to
describe your data they require and then
that schema that organization that data
organization didn't just disappear now
you kind of had to move it into your
Kiro document design so if you're
setting up MongoDB you have to decide
what goes here what's an index what am I
doing with things like BigTable keys
were very special you needed to make
sure that they were created in such a
way that you could do range scans on
them these systems didn't have query
languages and
they at the time didn't support indexes
so I think some of them do now they
didn't have any notion of transactions
and they really didn't give you very
many consistency guarantees so it was
pretty challenging for someone to move
from a database to one of these systems
so let's say you've got your single
machine my sequel server a Postgres
server and you've got your like schema
that you very carefully crafted you've
got indexes and foreign key constraints
you've done all this stuff you know
you've you've ran queries you figured
out what you can and can't do and then
you're basically just gonna throw all of
that out the window when you move to a
distributed database which is which is
kind of unfortunate there's also this
cartoon which I love by Jay recursive
and is pretty old but it's still very
classic and it's this guy asking how do
I query the database and the other guy
tells him you have to write a
distributed MapReduce function in Erlang
like that that was the state that we
were living in at the time and we're
still we're not too far from the state
right now which is pretty unfortunate ok
so here's another thing we lose and that
I'm going to talk about for the rest of
the talk when we jump to a distributed
database we lose this thing called
transactions and some of you might have
been using transactions some of you
might not use them but you didn't even
realize you were using them so in my
sequel I believe auto commit is set to
true and so whatever you run on my
sequel query a single one you are
running a transaction and so if you're
running something like an update query
that's gonna look at a lot of different
rows and potentially change a lot of
different rows that's running with
transactional semantics in my sequel and
it's not okay to just drop transactions
out of the system and the reason for
that is because it's very difficult to
reason about concurrent interleavings of
the stuff that we're running in our
database we it might result in incorrect
and possibly even unrecoverable State
and so let's talk about an example where
that happened so let's say we had a
completely decentralized currency which
committed transactions but not the kind
I'm talking about financial transactions
to a log and there
no way to undo things in the case of
fraud so once you gave your bitcoins to
somebody and it committed to the to the
log you were pretty much screwed it was
done so this happened last year a
Bitcoin currency exchange actually lost
a lot of money and here was the code
about it the hacker discovered that
multiple simultaneous withdrawals are
processed essentially at the same time
and that the system software doesn't
check quickly enough for a negative
balance so this was kind of like spun as
a security problem I don't see this as a
security problem at all this is not a
security problem this is a very classic
problem that has to do with transactions
you want to be able to check the balance
and do the decrement together at the
same time you when you don't do
something like that you can have a whole
bunch of people checking the balance at
the same time seeing everything's okay
and then doing a bunch of decrements all
at the same time which makes you go
negative so this is a this exchange
didn't lose too much money but it's a
little bit worrisome that there are
people out there building software that
is addressing something as important as
financial data and our money and it's in
or in total in a system where it's
really seriously not possible to go back
and fix it and they're not really
thinking about these concepts so that's
the point of consistency consistency
guarantees helped us reason about our
code and avoid subtle bugs and that's
why I think this area is so important
and interesting so consensus consistency
is a very misused word in systems and I
think the reason for that is because
there were these two different fields
there was the database people and there
were the distributed systems people and
they kind of sort of ran in their own
circles then they did different sorts of
things and then you know these fields
have been moving closer and closer and
closer together and they're pretty much
overlapping right now but it's still
people coming from very different
backgrounds and so the word consistency
means different things there's
consistency as an acid how many of you
guys have heard this acronym before all
right a few of you okay good there C as
in cap as in the cap theorem there C as
in sequential causal eventual or even
strict consistent
there are many different consistency
models and now the thing is all three of
these words consistency are talking
about different things which is kind of
unfortunate so let's start with acid
transactions since that is what this
talk is mostly going to be about so this
is what acid stands for it stands for
atomic consistent isolated and durable
and again if you are running a my sequel
or Postgres database then you can run
acid transactions and you can get
serialize ability and this is what you
would type into a terminal in order to
do that so atomic means that the whole
thing happens or not so a transaction is
a piece of code multiple statements
multiple reads and writes and a topic
atomic means you're either gonna see the
effect of all of those or none of those
consistent in this context specifically
for acid transactions is an application
defined form of correctness so going
back to our Bitcoin example are a bank
example consistent here would mean my
idea that balances should never go
negative isolated is the idea that other
transactions so they might be executing
at the same time don't actually
interfere with each other and durable
means that you can recover from a crash
so what this means is that if the
database tells you that your transaction
has committed even if it fails as soon
as you get it back up again you're gonna
see the database at that point you're
gonna see the database at a consistent
point and that's going to include your
committed transaction so I'm gonna most
of this talk is going to actually focus
on the first three because I think the
first three are what are what have to do
with our other definitions of
consistency okay so serializability
serializability is a form of consistency
it's a property of a schedule of
transactions and one thing that I want
to stress very heavily is that
serializability does not mean serial it
does not mean that you're doing one
thing at a time it means that the
effects of what you do are like you did
one thing at a time
so here's a slightly more formal
definition the result of executing a set
of transactions is the same as if those
transactions had
one at a time in some serial order
that's all that serialize ability is is
that the return values from the
transactions and what's written to the
database and what you give back to the
user it looks like you ran them one at a
time which kind of makes sense and so in
our model in our acid model if we write
each transaction to preserve correctness
in our database that means when we start
the transaction things are an incorrect
statement at the end things are in a
correct State then that means that at
the end of running these transactions
concurrently our database is also going
to be in a correct State and that means
that we get to pretend like concurrency
isn't actually happening we're getting
the performance gains of it hopefully
without actually having to worry about
it when we're writing our code so here's
an example of what that really means
here I have two transactions the first
one is reading two keys ok K&amp;amp;J and just
ignore the syntax for a moment I know
it's a little weird but you know reads
and writes and this one transaction two
is adding one to the values for K and J
and so what we want to happen here is
that in a serializable execution we want
it we want to see the values of K and J
atomically we want to see both of them
at one instance in time and we want to
make sure that we do the ads correctly
and also atomically so let's say we
start out with k equals zero and J
equals zero what are valid values to get
back after we've thrown these two
transactions at the database anybody
right K and J equal to one another one
is K and J equal to zero so one thing
that could happen is that transaction
one looks like it ran first and then
transaction two and we get that means
you get zero zero back and another one
is the transaction two ran first and
transit then transaction one you get one
one back you should never get a 0 and a
1 back that's what's here lies ability
means now the benefits of this are that
we don't have to reason about
interleavings and we don't have to
Express invariance anywhere but the code
if we can write our transactions
correctly then the database will promise
to execute them for us in a way that
looks like one at a time
and I don't know about you but I do not
want a reason about n factorial lines of
concurrent interleavings of transactions
that can get very complicated and it can
be very difficult to understand what can
go wrong unfortunately serialize it a
Tom atom city is part of C realisability
I would say yeah it looks like the
transactions were atomic that they
happened at one instance in time in
order so serialize ability unfortunately
is kind of expensive it'll it really
does have serious costs to use this as
your isolation model on a multi-core
database this cost comes from
serialization and cache line transfers
between the different processors on a
distributed database it's also the
serialization or concurrency control and
network calls and so in order to enforce
this property in our database we have to
use concurrency control which usually
involves either locking or validation
and essentially some form of
coordination so in order to get around
this people often jump straight to
eventual consistency which is what a lot
of key value stores claim to provide
when they don't provide any I haven't
seen a key value store yet that really
provides something much stronger than
this so what eventual consistency says
is that if no new updates are made to
the object eventually all accesses will
return the last updated value so this
concept was at least popularized by
Werner
Vogel's from Amazon he has a great blog
and I don't know exactly actually
defined it or not but it was definitely
one of the first times that I've seen it
and this was how he defined it now I'm
gonna make a little bit of an edit here
because I'm sure you guys have all heard
that in a distributed system there's not
really a very good consistent notion of
time so what is the last updated value
mean your last my last some other
servers last the clients last who knows
right so we don't really know what last
means in this context and so it's not
really the case that it's getting
updated to the last value
we're eventual consistency just says
we're gonna get the same value and also
that key part up there if no new updates
are made to the object so when does that
actually happen in a system I'm
wondering who runs these systems where
you actually stop writing for some
period of time and you can give your
some undetermined period of time by the
way this period of time isn't isn't
bounded and you can let your your
servers kind of coalesce into the same
state this doesn't actually happen when
you're running a product and this does
nothing about multi key transactions so
despite the fact that you're getting you
know things are converging to the same
value you're not going to be able to do
the bank balance thing that I talked
about this property definitely does not
give you that so a slightly stronger
form of consistent well a much stronger
form of consistency is sequential
consistency and the best way I found to
think about this is it's what your
laptop is doing right now so you've got
multiple processors and each of those
processors has a cache and what's
happening is that your processors the
hardware is keeping your cache
sequentially consistent and I'm using
that term a bit loosely because
technically it's not even sequential
consistency but unless you're building
sort of locking protocols or things like
that we're not going to worry about it
it's close enough to sequential
consistency and so what's happening here
is that your processor is tracking what
cache lines it's reading and writing and
it's making sure that those stay in sync
across all of the caches so the what
sequential consistency means is that
when you're doing a series
reads and writes they happen in some
order the reads and writes correspond to
happening in an order and usually they
give you the feature as well that if
you're on a processor they look like
they're happening in the order the
processor executed them in so let's
start with this example right here so
here are three processors and they're
doing a series of reads and writes to
the same variable so the first ones
writing a then the second one is writing
be a writing B to X and then the third
one is reading a and then reading B does
this look legal to you guys does this
look correct
yeah okay well yeah maybe that's maybe
it was obvious but and the reason for
that is because the president we're
allowed to reorder reads and writes
between the different processors and so
this is a correct interleaving of this
where a we can just kind of shove that
right on processor to down and pretend
like it happened after the read now how
about this one we're writing a then
we're writing B then we're reading B and
we're reading a nope this is correct so
the reason that this is correct is we
can shove that right even further down
and we can create something that looks
like this so sequential consistency even
though it's one of the strongest forms
of consistency is still really confusing
I still get really confused by what's
allowable and what's not under this
model so there's an even stronger form
of consistency out there called external
consistency and I think this is
occasionally referred to as linearise
ability and the idea here is that it's
what sequential consistency promises you
you're going to see the reads and writes
in some order that makes sense but the
results are actually gonna match time so
to me this is the only thing that
actually makes sense in the world this
is the only way that I can actually like
reason about what's going on and one way
to think about it is as though there's a
little a little detective a little
external observer sitting there looking
at reads and writes and watching what's
happening so if we're looking in our
previous example where we had the right
to a and the right to B if those really
happens at different points in time then
an observer
could have come in and seen that the
right to be happened after the right to
a and so then if processor three read
them in this order the observer would
know that this was incorrect it would
know that since there weren't any other
rights to X that um that this was an
incorrect interleaving so this is the
strongest form of consistency
everything's matching this sort of one
copy things happen one at a time
according to time according to when
someone actually issues a read or write
so this is the type of consistency that
is being talked about when you hear the
words the cap theorem so this stands for
consistency availability and partition
tolerance it was first proposed by Eric
Brewer from Berkeley in 2000 at a talk
in pod C which is a distributed
distributed systems conference and it
turned out that it's a rephrasing of a
much older result FLP result and so this
was done in 1985 by Nancy Lynch at MIT
and some other people and okay so what
Eric Brewer was saying is that I mean a
sensibly what he said and what's quoted
is out of these three you get to choose
two but what he was really saying is
that partition tolerance is a failure
model sometimes your servers won't be
able to talk to each other and sometimes
you're not going to be able to know how
long they're not going to be able to
talk to each other and because you don't
know about you're gonna process reads
and writes potentially incorrectly
violating consistency or not and so
that's the choice that I think that he's
really talking about now one thing
that's important to note the FLP result
is actually very cool if you want to go
read this paper there's this this very
cool construction in it where they prove
that it's impossible to reach consensus
with a group of servers even if there's
one faulty process and the reason for
this is because this faulty process can
just always do the wrong thing at the
right time so no matter how close you
get to deciding amongst all of your
servers this one faulty process can step
in right before you're about to decide
and screw everything up now it's
important to note that both of the
ideas were based on a certain model of
both the network and failure so this was
based on the asynchronous model of the
network and like I said there's this
idea here that you might drop network
packets you might have errors a server
might start doing garbage collection and
pause but you can't tell the difference
between delay and failure so this is the
idea of an unbounded amount of time
right so it might be you you can't
decide that a server has died because it
might be possible that it was just it
was just delayed for a little bit more
than that amount of time so what does
this mean
what are these results mean and what
what do they mean for distributed
systems does it mean that we have to
give up hope that I should forgo any of
the rest of my PhD and rip up my thesis
and throw it away because we're never
gonna be able to do anything because
it's impossible to side stuff on the
Internet well here's an analogy that I
really like have you guys played candy
crush a little bit okay I haven't played
it though I played other Thailand games
and this is a category category games
called match three games and it's been
shown that this category of games is
np-hard
so what does that mean that means that
there exists no polynomial time
algorithm to solve all instances of this
game now NP and there's this famous
question of P equals NP and we rely on
the fact that certain things are are
np-hard be Kaleo love our cryptography
in order to function in the world now
does this mean that we can't play candy
crush saga no right I like it doesn't
matter that it might take you more than
the number of atoms in the universe to
figure out the answer to a very large
game of candy crush saga I can still
play candy crush saga on my phone and I
can still like occasionally beat the
game and I bet there's someone out there
who always beats the game and you know
is really good at this
and so I I really like this analogy
because it means that it helps you
understand what this theorem is really
saying and what it's saying is that it's
saying it's impossible to 100% of the
time decide everything on the internet
if we
can't rely on synchronous messaging we
can 100% of the time decide everything
if partitions heal if we know the upper
bound on message delays our system might
be really slow it might not be the most
ideal system we build but if we remove
this one part of the model then we can
and also just because you can't get to
100% all the time doesn't mean you can't
function at 99.99% a lot of the time
this is an impossibility result this
means it's impossible to do something
all the time we can still play candy
crush
we can still occasionally beat the game
and so that's why in my research I
haven't really focused so much on the
cap theorem instead what I really care
about is consistency versus performance
so in the day-to-day operating of your
system how expensive is it to maintain
good semantics this is a question that I
really care about answering and what I
want to know is how we can reduce these
costs while still producing a correct
ordering of reads and writes and can we
do this while still handling failures
and eventually making progress in the
case of failures so there's been a lot
of research about improving the
performance of these stronger forms of
consistency like serializability so here
are four different types of research for
techniques that I found really
interesting so one of them is to base it
basically get around the fact that you
don't know what time it is by trying to
force everybody to be very very close in
time so spanner a system by Google is a
giant distributed database and they put
atomic clocks in all of their data
centers so that they're able to bound
the skew they're able to determine
exactly how far it's possible for two
clocks to be off and then they rely on
this fact so that they don't have to do
as much coordination and communication
in order to do some of their
transactions
unfortunately spanner is still really
slow yeah so there's been a lot of other
techniques developed that are pretty
cool there's the theory of transaction
chopping and the idea here is that if
you know you're only going to be doing a
certain set of transactions if your
application is doing something and you
know what it's gonna be doing it's
always gonna be issuing the same kinds
of queries you can analyze that set of
queries and you can figure out if you
can divide those set of queries up so
that serialize ability is less expensive
and here are two pieces of research from
NYU where they've leveraged this
technique to get better performance in a
geo replicated system so they're able to
do transactions across a geo replicated
system then there's this idea of
commutative locking so I'm pretty I'm
pretty excited about community and this
is something that I used very heavily in
my own research and this has been used
in different forms there's this idea of
commutative locking and escrow
transactions this is from the database
community it's a very old idea abstract
data types and Doppel is a system that I
wrote that takes advantage of
commutative commutativity to do things
per core without communication and then
finally there are some systems that use
deterministic ordering basically if you
have a distributed database and you know
that people have to commit transactions
in a certain order and they all need to
commit them in the same order
you can just decide on that order ahead
of time and this can help you get better
performance in some cases now our goal
here my goal at least is to get parallel
performance when we're executing
transactions on a multi-core or a
distributed database and I think that
these are some of the ideas that we're
going to use in the future to be able to
do that
I think that it's important to think
about different concurrency control
schemes for popular and contended data
so something that's important to note
here is that if your transactions are
all on different data they're not going
to contend and serializability is not
actually going to be that expensive so
it's really going to be a problem when
your transactions are accessing the same
hot popular data and if that's really
the case are there special things that
we can do for this data to make our
transactions go faster
I think commutative locking and abstract
data types are really important and I
think interfaces that reveal data types
to the programmer are much better than
readwrite interfaces so here's an
example so whenever we talk about
transaction execution we talk about
reads and writes get
puts and that's what all of the cereal
box utilizability models deal with
however oftentimes when we are doing
things to our data
we're actually following a data type
specification where it's a counter and
we're incrementing it or decrementing it
or we're taking the max of something or
it's an index and we're inserting into
it with some sort of order and if we
know the semantics of the type of
objects that we are manipulating then we
can like some of them are actually
commutative so we can take advantage of
as some of the techniques I just told
you about to get faster transactions and
we can also do all sorts of other
different kinds of things with them and
so that's some of what I've been
exploring in my research is how we can
use these ideas to get faster
transactions so I'm gonna leave me with
a with well I'm gonna I'm gonna give you
this quote this was in the past banner
paper so I think it's really interesting
that Google kind of was one of the one
of the first systems in the no sequel
movement with BigTable they got rid of
all of the relational and query parts of
a database and then they came full
circle and they built spanner which is
an f-1 which is basically a giant
distributed relational database and
here's a quote from spanner we believe
it is better to have application
programmers deal with performance
problems due to overuse of transactions
as bottlenecks arise rather than always
coding around the lack of transactions
and so that's what I want to leave you
guys with that those are the takeaways
here use databases that are well tested
don't try to use the latest hotness on
your the most important part of your
system and your product use serializable
until it becomes a performance problem
so don't optimize too early and when you
get to a point where you have to make a
decision where you have to think about
giving up some of your good semantics
for performance think about exactly what
it is that you're giving up and what's
changing when you move to a system with
different models alright so that's
pretty much it thank you very much and
I'm happy to take questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>