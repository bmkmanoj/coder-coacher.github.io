<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • Fast Analytics On Big Data • Cliff Click | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • Fast Analytics On Big Data • Cliff Click - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • Fast Analytics On Big Data • Cliff Click</b></h2><h5 class="post__date">2014-11-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HVh6IYJtWHY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm cliff okay I'm going to talk
about Big Data I guess the the first
thing to do is qualify with people what
big is because everyone has a different
notion of big so i guess i'll open up by
asking a question here so how many
people here work with big data like to
so what a rush y'all do why are you in
this talk so how many people here let's
let's try again here how many people
here work with data sets that are
typically a gigabyte or larger in size
oh okay fine okay how many people work
with with 10 gig datasets enlarger okay
100 gig terabyte enlarger now we're
getting down to okay about 10 terabytes
a hundred terabytes okay now you're
getting up to what i would call big so
the petabyte right and and on it or i'll
stop here so the couple guys go go go
all the way up to petabytes and then
there's probably google people who will
keep going to exabytes so for what we're
doing a big well that was very exciting
to strike again there we go big is
somewhere between gigabytes and
terabytes in the sense that beyond
terabytes that's too big for an
in-memory solution conveniently so we're
building a platform it's pure java open
source you can get it straight off
github and the website has got a jar
file that you can download and go it
just takes one jar and it often runs and
it's doing it's a platform for doing
parallel distributed math and then given
a platform for parallel distributed math
what does that mean well besides being
to do sort of any kind of generic math
you want we're building high in math
machine learning algorithms logistic
regression generals little modeling
Grady boosted method random forests deep
learning neural nets on and on and on
bunch of stuff I'm so I guess I'll go
hand in hand with that so how many
people here are are using machine
learning at all or looking at it or
thinking about you doing any with
machine learning okay some amount and
how many people are using machine
learning on sort of a daily basis now
like like you're sitting at your desk
and your building models on a regular
basis so not and you go to hear have
plans to
go there where you're going to build
these models and try to buy them in
production so a couple of people so so
people out there making a plan okay so
this talk is actually I'm not going to
go deep into machine learning algorithms
in the math side I'm going to talk about
building systems platform for allowing
machine learning algorithms to be built
okay so I said this is a platform for
doing big data work so what does that
mean in this context it means almost any
job will you want on a big
two-dimensional table so anything that
reads or writes a single observation in
your data set a single row plus maybe
some nearby rose and maybe computer
reduction or produces a big data output
within that constraint the speed is
interesting because it's big and the
speed is typically data volume divided
by your memory bandwidth it's limited by
memory bandwidth frequently which is
like 50 gigabytes a second per node that
varies by hardware and by your original
setup and some of the algorithms require
more network traffic and so that's the
bottleneck and said memory bandwidth but
we typically see numbers that are you
know a thousand times faster than
anything doing a hadoop it's a pretty
low bar but we're still seeing numbers
that are ten times faster than anyone
else in the planet it's it's really fast
stuff so that opens up the door to doing
interesting things I mentioned the data
is compressed we'll talk more about that
later it's typically much better
compressed and what you get from like
gzip on desk and that goes straight to
the memory bandwidth data volume
trade-off for speed in the good
direction i'm doing math so I'm limited
to data that is math white numbers and
times and simple strings and then I said
two-dimensional table how big is the
table tables that are less like a
thousand columns wide are going to be
really fast like too fast that you can
hardly care up to ten thousand columns
work still fairly well and 100,000 or
bigger still works but the loops are
paralyzed the wrong way and it's not as
fast as maybe you'd like but the length
of the tables limited memory and can be
millions to billions to trillions long
it can be very very very long so more or
less you know couple terabytes of
cluster will turn
into you know tens of billions hundreds
of billions of rows and that's fine it
will all go fast okay so what can you do
with it now I'm going to step through
some examples here from a developer
point of view and the most common thing
we do is a MapReduce paradigm it's not
the only paradigm we support but it's a
common one because it scales out just
completely nicely so a map and reduce
will take something of a Type A in the
map call and produce something but type
B in this case it's double double and
I'm just computing a square and then a
reduction will take two of the B types
and make one here I'm doing simple ad
and so this is just sum of squares and i
write this Java code exactly like this
and I say do all on my data and it's
going to run at memory bandwidth speeds
so this will take a terabyte in you know
under a second and you know some some
tens hundreds of millions and how about
your cluster is it just like that so I
have a scholar version and development
it's still being whacked on it's not 100
complete but it's clearly about half the
lines of code and you can get the notion
really quick that you're just doing a
sums of squares here so it's kind of a
slick thing so I said before you know I
have as a map and reduce the map is from
a type 80 type B where the type a is
your big data it's your input it's
however many columns you've selected out
or how many thousands and hundreds of
thousands columns you have that's your
type a and you can treat it as strut if
you will but it's an array of structs
source obstructive arrays the two
notions are sort of interchangeable here
you know a is your big data but row by
row by row and your type B is your
output and there's two types of output
there's big and small big data has to go
right back into the cluster because by
definition the big is too big to fit on
one node so if you have a big output
that is on the order of magnitude of
your original input it's going to be a
distributed output it's going to live in
the clusters memory same ways your input
is you can also have a small data output
which is a plain old java object and the
reduction will take to such pojos and
produce one I'll do update in place
where possible and ultimately you simply
make a new instance of a MapReduce task
override map and reduce and say do all
and you're
regional instance comes back with
whatever your reduction was asking for
right back in that instance after its
visited your terabyte of data so here's
some examples of that here my type a is
a double pair x and y mi type B is to
this pointer an instance of linear
regression pass one which is computing
sums and sums of squares again and you
can see it's simply adding directly into
the Java object and calling reduction
takes this into that which are two of
the same type thing and folds them
together there's a scholar version it's
about half the lines of code a little
uglier on a slide but when I say it goes
right back into the original object i
made a foo and i can directly use it in
the next line of code to go compute
things straight off of foo and then
usually that feeds into the next pass I
just write the next piece in this case
of linear regression there's three
passes and you would say pass one do all
pass to do all pass through all a little
bit of algebra in and around the passes
and it's a done deal so there's an
efficiency thing here and you know to
get my memory bandwidth speeds out I
have to actually dodge all kinds of
overhead so under the hood whenever I
say map is actually a for loop that's
running over some chunk of data I'll get
more into what a chunk is here in a
minute but it's some collection of a
thousand to a million of your elements
that one core is going to do something
with and there's a simple for loop that
you can write yourself or you can let
the system throw in for you over your
own map call that chunk at CX at 6y
that's the decompression step talks more
about that in a minute because it counts
for speed so so much and then you do
your standard map introduction the way I
showed it a minute ago and those are all
just the same because you have a for
loop here if you have interesting
complicated map functions you can haul
loop and variants out yourself if the
Jets not going to do that for you can do
it yourself and when we do our own
internal mouth algorithms we will often
have map functions which are thousand
lines are more of Java code and we'll be
doing a lot of sort of manual
optimizations to haul loop and variants
out and things like that okay so here's
another quick kind of thing i can do i'm
going to count somebody instead of a
computer math and this is just a filter
you know if your if your input data
passes some particular filtering
condition count as a one or a zero and
the reductions add them up and thus
Collison tax is again a little bit a lot
shorter and a little bit cleaner and you
know I'll get counts that way sort of
however long it takes to shove the data
through the memory bus in addition to
doing accounts I can also collect sets
and this is a big gate output or I've
made a new output vector and I've said
do all passing in my inputs of my
outputs and then I can use it
immediately and the next step and some
were in the body of the map call I've
picked some condition and then set a pin
and that collected an output set that's
too big to fit on one machine typically
you know whatever the percentage of
people whose past the filter they got
shoved into the output set but I might
have an output set that's a terabyte in
size and if I'm filtering usually maybe
it's small but if it's big it's going to
be too big to fit on one machine so it
just the output just has to be
distributed so this is a little bit more
about the coding style here's a
basically a group buy or I'm going to
ask a question how many cars do people
own at different ages in their life and
the more interesting thing here is I've
done an allocation in the map call and
the key trick with that is it's a
private private allocation it's
privately done by one cor it's single
threaded access so in particular that +
+ out in the end it doesn't have to be
synchronized this is one core doing one
piece of work and there's no
synchronization involved but then every
core is running on a different chunk
late data and so you have a whole lots
of these little car ages arrays so you
have to have a reduction to roll them up
and I have a little utility that's going
to help just do you know array element
add there here's uniques this case the
difference is I've done a new in the
constructor instead of in the map call
and that meant to the time I called new
uniques right here i made an instance of
an object and when i say do all that
object got replicated around the cluster
with everyone sharing as much as
possible the one instance i just made
that's very useful for read-only sets
but if it's writable then you have to
handle currency in this case it's a
non-blocking it's a concurrent safe hash
set so i can have everyone just right in
so the map function simply jam something
into the hashtag
where the hatchets going to filter for
dupes and the reduction moves things
together and that's called whenever I
have to move these hash Jets are over
the wire but in so many lines of code i
can compute uniques and my data set
across the cluster okay so that was sort
of a quick whirlwind tour I'm going to
summarize the limitations here and the
main one is you know I can't do any any
machine level resource allocation
because that won't be clustered wide
sono IO in particular every member of
the cluster might have a different file
system view so you can't read or write
files sort of meaningfully there so no
new threads no locks there's no
distributed locks here no calling system
exit you don't have any global variables
per se instead any global static
variable becomes node local now if you
actually need global state then if it's
readable you put it in the constructor
and it'll be cloned around as a read
only thing for every every member of the
cluster every thread to use and if it's
rideable you're going to have to use a
reduction call to get the results to
roll back up and come back to you as a
truly global written result and for the
big state you just read and write
straight into the the big data same as
you saw reading you can write to it as
well and the typical paradigm here's are
going to run a one MapReduce step and
then do a little bit of algebra and then
run another one and another another
where the key here is that the typical
time to do tire pastor of the data is in
the in the modest number of milliseconds
so it's it's a different story than your
typical you know MapReduce paradigm okay
the strengths here is that the codes
running sort of distributed in parallel
without any more effort into the code i
was showing here it actually works just
as written you just say you know new
MapReduce task override MapReduce bang
done you write in a single threaded
coding style so you don't have to
there's no synchronization and there's
no talking to the cluster mechanics
somehow to do whatever set up there's no
concurrency issues there's no resource
management knobs you have to turn once
the cluster is up and running you know
all your maps reductions just do the
right thing
immediately there's no knobs needed for
GC or CPUs or network or data placements
know how blocks or hot locks you know
that's all taken care of under the hood
and and actually it works really well
okay so so that was like the whirlwind
tour of what I can do with it here let
me stop and see if anyone has any
questions about what I just said yeah so
they say so can I kind of feed data into
this so right now we're running on
static data sets where one of the first
steps to do is load data that said the
core architectures totally supports
streaming and we just hadn't had enough
users asking for it to put streaming
down so we add algorithms and features
based on what we have customers pay for
and as soon as you get a quorum of
people said I'll pay for streaming then
we put it in it's getting there
streaming will probably be here before
the year before next year you know a
year from now we'll have the streaming
support in there okay so let's take a
look at what how it actually works this
is an instance of fork join but it only
distributed fork joining so it's similar
to Doug Lee's fork join work i'm just
i'm actually using his fork touring work
sort of directly within a node and I've
extended it to work cross node and in
the way you look here's an eight node
cluster somebody somewhere is driving
this and has said you know making a new
MapReduce task and do all on some data
and then I do the sort of obvious log
tree fan out where I pipe the instance
of this object after the Constructors
run it's all set up to two as two
neighbors and they send it to their two
neighbors and so on until the whole
cluster has got an instance of this task
now as soon as the task hits a note it
also goes on the fork training work
queues and then they do what you know
fork trains been doing all along which
they start making clones of themselves
as long as the data set the referring to
is too large and they do it divide and
conquer and they split make another task
and so on until the amount of work
assigned to individual task corresponds
to you know one chunk of data and then
they all do parallel map calls across
the data and Fortran is really good at
doing load balancing and you know sort
of on the
on the work on the data so this will
slam all the cores on the box you'll go
one hundred percent cpu load though all
do their thing and then when they're
done each of the objects will have the
small data pojos left in the task
objects the clones of the task objects
that have been made all along and then
there's a reduction step starts going in
reverse we're a pair by pair as soon as
two guys have completed their mapping
steps to immediate reduction in place
and then those log to roll up make
another reduction until you get with a
single top-level instance per node and
then you run the cluster wide log tree
and reverse going backwards where the
poor fellow at the long into logs on the
right reduces up to his one friend and
the other two guys in reduced to their
friends they got their tasks from and
then in the next step you know the
reductions continue back through the
cluster until you comin with the final
reduction in to the original instance
and it's done so the time to run this is
typically log tree fan all through the
cluster parallel xq on the nodes logs we
fan in from the cluster and that turns
into some you know count of milliseconds
depending on how complicated your maps
and reductions are okay so it's pretty
straightforward not maybe some okay fine
so let's look at the data layout so this
is an interesting problem because people
I don't know it seemed to sweat a lot
about data placement data layout and
we're sort of doing something it's very
straightforward and has been working
quite well in practice so the abstract
API is I have a collection of vectors
for my data that's the two-dimensional
table thing where a vector is one column
and the column is treated like an array
but it can be longer than an int in
length you'll give me billions of
elements and lengths right I'm this fast
random access to any given element
although if one node asks for all
elements he'll get drowned in the data
it's too much right but he can pick
individual elements for free there's a
notion of the missing value this and is
in a it's very key to the machine
learning math algorithms to have a
notion of missing you your obvious do
something different on that and you can
write it or a pin so you could muck with
the size of the data sets or change them
in place it's all fine so I said I have
a single large vector it concept
it's full of typically daba daba
primitives however that's the conceptual
view it's actually compression is
whatever is going to be and it's all
geared for fast linear access it striped
across a cluster so here I have a four
node cluster i'm showing chunks of the
data will be on every member of the
cluster so it's not striped with one
vector holding a wind a vm holding whole
vector instead it's distributed across
the vector and it isn't fully in the
java heap so the data is actually in the
Java heap and we do a lot of work with
GC to watch the size of the heap and we
spilled the disk on demand do a bunch of
stuff there but the end result is if you
take the default collector and you said
it as large as you want will not have a
GC problem we don't see like we
routinely run with 250 gig gcx mx2 and
50 gigs on a 256 gig machine which max
GC pauses in one to two seconds every
now and then you don't get these giant
GC pauses okay so I said add a vector
typically actually have a bunch of them
and they're all aligned and they're
aligned this way so that one core on one
JVM can access all members of a set of
an observation going across at you know
memory speeds so while you can get out
the data with random access on any row
in JVM your fastest if you're doing it
in sort of a 1 cor going across on a row
and the reason it's set up like this of
course is that your data sets represents
some real-world feature set and here i'm
showing you age sex zip code ID car
whatever it's going to be and you know
conceptually is a array of structs or
struct array model and it kinda you flip
back and forth in your head which way
you want to view it and I can hand the
data to you in either way similar to an
our data frame so we can add or subtract
columns sort of for free it's like a
pointer change to do that so a common
setup is to collect in however many
thousand comms you have and decide what
you're gonna come you're going to work
with and you probably drop a bunch of
columns out and then you probably filter
and hack on a bunch of columns to build
out more features and then you pass it
on to the the heavyweight math I'll
gross but all the adding and dropping of
columns is all sort of for free okay
with in a column there's this thing
called chunk I'd mentioned earlier it's
it's just the unit of parallel access a
chunk is
typically a thousand to a million
elements in size it's stored in byte
arrays in the Java heat so plain old
byte arrays and it's where we do
compression and we vary the compression
strategy by the kind of data we're
observing in that chunk but the
guarantee is that or get or put and then
you give an element is just a few clock
cycles including the compression step in
this case the the compression is one of
the key fun things we do for speed
because we're moving the compressed data
through the memory hierarchy and
decompressing it and machine registers
on the fly it's sort of completely free
to decompress because you can't pull
data and fast enough anyhow we're all
blocked behind the memory bus so more
compression is actually better and then
of course every row across is sort of
owned by a single core at a given point
in time so that core can access every
member of the row like a single-threaded
like he has a java object in fact I'll
just build you a Java object of you want
one and then you know he can just
scribble at it a full Java speeds so so
it's it's his private data structure
that he can just mangle as he once and
then I said that single threaded on that
set of rows there's enough a chunk work
there to cover the cost of launching
enough enough threads together ething
busy but that's small enough that I get
good fine-grained data parallelism so
it's this sort of you know good blend of
just big enough to cover the cost of
launching or threads just small enough
to get good parallelism and so of course
what really happens is every core lights
up all in parallel all at once yep how
do I make sure the data is aligned I
load it that way so so when I inhale the
data sets I will start selecting what
rows are going to go on what machine
across the data so you know a common
thing to do is read in a you know
terabyte CSV file so CSV means is
variable length on every row it's all
kind of mismatched up now parse through
that and and build my compressed format
and and then make a decision about what
rows are going to go what places and
I'll group them together but I'll
carefully arrange them all note by note
by know that you have a compatible set
of rows across on a single node and then
there'll be one network shuffle so you
load a terabyte and it came from desk so
the network as you know thousand times
fast and disks the network shuffles
essentially free now and you're limited
by dis speeds right so to get the data
in so I'll do one shelf
one of the hood get the data where I
want it and then doesn't move again here
okay so there's a you know quick summary
a data frame is a collection of vectors
it's really distributed data frame it's
it's all the data throughout the whole
cluster and and then you know the
vectors are themselves a collection of
these chunks which is my unit of
parallel access a chunks of thousand to
a million elements and kind of varies
according to all kinds of funny factors
and then elements conceptually Java
double although it's actually how its
represented under the hood varies by
compression strategy and like I said I
have like a 15 different compression
strategies that I use right now and then
a single row is a single sort of
observation or instance or conceptually
a Java struct of those columns those are
things that you manipulate directly in
your map calls okay so so any more
questions on on this step damn okay so
let me let me break for second and do a
demo and I'll come back around to a few
other odds and ends and that's actually
most of the talk let's see if I can get
a demo going here so some in the world
I'm going to launch javed s jar xmax
okay my cluster came up but it's a
cluster of one of my laptop here um let
me get this coin so I didn't talk
anything about deployment but it's a
standard single jar file so it runs fine
my laptop I can cluster up on my laptop
by saying javed s jar you know ampersand
the background run it two or three times
a cluster of however mediaone my laptop
I'm going to run in the cloud running
ec2 it'll run on your own private
cluster a run on Hadoop but not on
Hadoop it'll read from HDFS or s3 or
wherever it because it's a simple jar
file you can just go pretty much
anywhere with it so I'm gonna start by
loading a data set here i'm going to
pick but i got your data sets let's pick
the nice UCI so we're gonna pick a thing
called cover type which is a forest
cover type is a pretty old data set that
people using machine learning a lot
as a demonstration data set I'm parsing
a CSV file I'm going to see the break
out of the initial lines so the the data
set is about seventy five megabytes on
disk as you can see if you read up there
it's about half a million rows 55
columns about 10 megabytes and RAM
that's the compression ratio between
disk and RAM going on there and then
I've got just a quick view overview of
the data set and it goes out you know 55
columns wide and whatever for this
particular data set the column one is
meters of elevation where they took the
sample and column two is like meters
distance to nearest water source and the
far right-hand column is the forest
cover type is a number from one to seven
those are the digits on the right there
that say you know 55 225 or whatever
it's going to be that's like alpine
forest or pine woods or meadow or you
know marsh or whatever it's going to be
and then there's 50 other features
floating around it's a big pile of datum
um I'm going to run because it's you
know it's a forest cover typing random
forest is a good thing here so this is a
machine learning algorithm this is
called random forest I'm not going to go
in a lot of details but I'm going to
build trees decision trees with random
weaknesses so I have a collection of 50
odd here I'll just hit the Go button
here take all the defaults 50 odd sort
of weak learners where each learner is a
decision tree a cart tree and then the
sum of them they vote together does
actually really good job of finding and
being able to run predictions on the
data set so if i run on here nobody's
not gonna do this one's beautiful we
have to wait all this guy runs to the
end so you take like another 30 seconds
here and i'll have something done
so while we're waiting here is a kind of
answer any questions about what this
means or why you would care what machine
learning is doing here see ok so the you
know the general notion is this
predictive modeling I'm going to build a
model of the real world based on my
input data set with the hopes that the
model let me predict new things with
sort of high high degree of accuracy so
I'm get this guy a little bit longer
here and then we'll we'll stop and take
a look at them so these predictive
models are used by lots of people for
different things but for instance it can
predict whether or not this credit card
transactions fraud or not whether or not
I'm looking at some sort of DDoS attack
whether or not i'm looking at a blob of
cancer on an x-ray or not you know what
is my you know estimated costs to ensure
the city of chicago from you know the
next hail storm that comes through is
going to wreck some percentage of roofs
in a neighborhood you know what's the
lifetime insurance costs of a particular
population of people there's all kinds
of things you do with machine learning
algorithms and they can be very highly
predictive so let me explain this chart
here real quick going down is the actual
type of the you know the observation
made on the data set of what the person
found when they did their ground truth
going across is what the model predicts
so the diagonal is where the prediction
was correct and off diagonals where the
prediction is incorrect so in about a
minute here or less I have something a
model that's about ninety percent
accuracy you can see about ten percent
fail rate way over there on the right
done on you know half a million
observations in a few seconds on my
laptop and that's a you know that's a
reasonable result for not much work
actually if I was to have a you know
cluster machines I could go a lot faster
or a lot bigger or both and I have a
model that does something interesting if
I were to run random forest a little bit
longer here I'd get a you know hype more
highly predictive model and use the
general trade off as some of the models
will build are very fast to build with
less predictive accuracy and some are
slower to build with more
accuracy random force is kind of in the
middle the other thing going on here is
that some of the models are more
interpretable so yeah here's my mean
squared error by trees that probably
didn't have any other numbers in there a
built too fast it's fine so that's
that's a unit used to decide when to
stop adding trees that you're not
getting any better random forest is sort
of in the middle of being interpreted on
what is in trouble of mean it means I
can look at the model understand why
it's predicting what it predicts like
logistic regression is very interpreta
bilawal but you can read off the the
different coefficients on a generalized
linear model logistic regression sort of
directly understand why it's making
predictions and that actually turns into
like government regulations people do
say FICO score card models if you're
getting credit allowed or denied in the
u.s. you have you told why you were
denied credit and so the government
basically mandated you do logistic
regression or variant thereof because
you can explain the results of it to the
in consumer that they got denied credit
because of something you goes the deep
learning model and it's highly opaque
you don't know why it predicts that's
also by far the best particular deep
learning models were like it's magical
Oracle's that will with extremely high a
core she made all kinds of fun
predictions so this is the model it just
built right now it's sitting in the
memory in my JVM if I want to pull them
all out and do something with it I'm
going to do something bad you're going
to take a second or a few i'm going to
torture my browser i'm going to pull
them all out as a piece of java code so
as a straight up plain old POJO but it's
big enough that when displayed in the
browser is pissa codes a giant pile of
text the browser is going to barf here
for a second there goes okay so here is
boilerplate we've all automatically you
know generated including instructions on
how to compile it and how to use the
model but the goal here is to let you
take this model and go into a production
setting so some are way down here after
all the boilerplate here's a treat so I
said it was a decision tree here's a
decision tree float prediction is equal
to well if you're missing that's the nan
check else if column 13 is or column 14
is whatever less than
this index and duh-duh-duh-duh-duh
somewhere down here at the bottom
there's an answer comes out okay it must
have been you know i'm going to pick two
or one or whatever it's going to be this
is class two out of my seven classes
right so it's a decision tree here rolls
on rolls on rolls on okay so the
decision trees get big and somewhere
there's a second and a third and a
fourth and a fifth and there's 50
decision trees ball buried in here and
and what it amounts to is if you run
this code with a ninety percent accuracy
it will predict the correct force cover
type and if I do the same thing for like
credit card data I'll have a model out
that if I run with some level of
accuracy it'll tell me I got a fraud or
not a fraud this particular all these
models do no allocation and their
coroner loops they run sort of finite
fixed amount of codes you can time them
and know exactly how long it takes if
you have a time-sensitive scoring issue
like a credit card swipe the guys
waiting at starbucks for his answer
right you have some number of
milliseconds you're supposed to report
you know thumbs up thumbs down that you
can count the number of milliseconds
this guy runs at that's what it runs out
and it's just sort of ready to go in
production right like that so yeah the
model runs on and on and on but it's all
sort of straightforward the stuff I do a
logistic regression the models tiny very
very simple and you can see we can run
the logistic regression here we do that
let's go new tab browse server so i'm
doing this from the browser um there's a
number of api's we support rest and JSON
is an obvious one and the browser was
built over the rest and JSON let's do
that get a parse goin okay yeah come on
done so that was much smaller data sets
only you know to megs and RAM 50,000
Road 40,000 rose uh generalizable
remodeling will do general and
remodeling okay so here is an airline
data set a sub fraction thereof and the
question is can I predict weather on my
air flight will be delayed or not this
particular data set has a lot of what we
call answer
an answer column is something that you
got after the fact typically added on to
data sets that's not available before
you get on the airplane so for instance
whether or not you're canceled in the
middle here is only available till after
you showed up the airport and discovered
the flight was canceled right so if you
predict with that the model will
immediately discover that every time the
flight was canceled you were delayed so
it's kind of useless so I'm going to
exclude that except I just did the wrong
thing here let's get rid of all of these
there we go now okay finally let me try
again here I'm easier to do it this way
so I do my shift click the correct way
here I'm looking for looking for control
click I'm looking for there goes don't
when diverted or cancel I do want
distance and the origin and death that
makes sense clearly don't want arrival
delays CRS is the plan the last time
that I want to keep actual lapse time I
need to catch because if the actual
lapse time is really long then somewhere
I got delayed I'm when he over to tail
nom and flight number because tail num
is a unique identifier on most of the
flights for this small data set and it
will simply predict if you got on a
plane with this following tail number
your flight was delayed or not because
there's not enough flights for that one
playing to to have it not be able to
tell otherwise and again get rid of the
actuals and keep the plan times and I'll
just go for the time the data eek the
carrier what time your flight will take
off your source and dust and so on so
forth come down here is this generalized
linear modeling covers lots of
interesting cases but binomial turns
into logistic regression which is a way
to you know do true or false i'm going
to just take the defaults there so
logistic regression I don't know people
want to go into how much math okay we're
done so logistic regressions one once is
actually pretty fast and scales very
strongly so that you can get a logistic
regression done on a terabyte in and you
know under a minute it's it's a really
fast model building it has an
easy-to-understand model but it's less
predictive so here instead of a ninety
percent prediction I'm getting around
you know seventy percent and you can see
sort of the
the red line represents a 50-50 die roll
and I'm better than 5050 so I'm getting
some predictive quality out but not a
huge amount but the other side I can go
up here and look and see what are these
numbers mean and and what happens here
is that the coefficients that are
positive drive the equation above the y
equals 1 over 1 plus map blah blah
towards the 1 and negative coefficients
drive it toward zero and so is delayed
being a one says the more likely that
you are to be you know the higher the
coefficient the more likely to be
delayed so taking off or arriving at
this particular Airport I know what that
is or taking off from southern guy are
both highly indicative of whether you're
delayed or not and if I run through here
there's new Chicago sort of Chicago
Midway's right there there's a few other
ones that come up pretty quick that are
kind of interesting destination Austin
and that's interesting okay fine I'll
quit looking at it but you can sort
through to me we start picking out you
know on the on the full size data set
SFO shows up right away as like a bad
airport to take off or land at so does
Atlanta and then certain airlines like
suck that you don't want ever be on this
airline and other airlines are perfectly
good so you can you can get something
out of the bigger data set here this
little data set doesn't have enough data
to pick up all those features but the
equation is this this is the that that
is the model right there so that's one
line of Dalek added you just drop that
in and in your predictive or your mobile
phone app and right away you can answer
out okay so that's sort of the quick
pass through the demo maybe I should
stop and take questions on this fri i
flip back to slides okay fine we're
going to we're going to go we're gonna
go on here okay so um so we've been very
busy here and the latest thing we're
mess around here is spark integration so
we're doing big data stuff and so spark
although we have very different sort of
models of data and execution it's clear
that that you know there's an
interesting integration going on here so
we're able now to move data back and
forth between spark and h2 o's data
frames and it's in process in memory so
it's actually pretty quick to go back
and forth and then you can take an h2o
data frame and turn it into a spa grady
and run spark jobs on it sort of
directly like the one line of code got
there and in addition the heo data
frames now support a scala collection
notation so you can just say for each
and put you know some kind of map kind
of code right in there and the codes on
github you need the the dev version of
h2o and then parry air is sparkling
water it's our combo this will probably
go away in another month or two all we
needed out of spark was a hook to call
h2o at startup time in the right places
and which we put a juror request and
spark people and they've taken some
version of it and so it won't be too
long for we don't actually need a spark
build okay so we can move back and forth
between the two that's all in memory
it's all in process there's no external
tooling needed mostly data doesn't move
it is the same shape because it's going
to be in sparks format in h2o formats
and then there's a difference between
eager and lazy I'll cover in a second we
do make a data copy but the hto data is
typically very highly compressed over
the spark data so that if you're running
a successful spark cluster right now and
you make a copy of the data the extra
member you need isn't a whole lot over
which are already consuming for spark
the you know this is a little bit of
layout spark builds up from blocks that
call partitions these are structures
that are limited as single JVM they
don't cross the machine boundary and hto
uses these chunks and collections for
every column you can have lots of these
in any given java heap limited only by
memory and then you know looking at it
this way an RDD is all of memory all the
data in hto data frame is all of it but
we're broken up by columns in the pink
for the vectors and r dds are broken up
by chunks there'll be a couple chunks
for every JVM it's a little different
format but we'll go back and forth
between the two different layouts with
just a java call to go back the date
around when we convert the h2o data
frames
version happens immediately it's
essentially the same as a spark
checkpoint you're going to run the argue
either typically lazy until you demand
an output h2o will demand an output get
its data in h 12 format then you can do
something with an h2o all those things
you do an h2 are all eagerly done on the
spot when you're done you will go back
to spark idd you say give me a 2 r.d you
get a you get a rapper object so it's
lazy at that point but you've checkpoint
it in h2o and then you can add our DS
all along you'll build up your pipeline
of computation and when you ask for the
spark result it'll pull the data out of
h2o converting a spark format and then
carry through the spark pipeline sort of
the way spark normally does things ok so
that's that's it i'm just going to wrap
up here we have a couple different use
cases people use the system for so no
distribution coding is basically whole
algorithms or whole vector math kind of
things often this is the rest and json
style interface and you're doing
something like loaded data set we run a
logistic regression get results just
what I demo in a minute ago and you can
drive this from our Python web base that
was the browser you saw their bash curl
scripting however you want to run it we
do have an active our community who uses
hto as a way to get big data on our and
they can type there are expressions like
they always have been doing an R and the
works all done on the cluster on the
cloud and the data set sizes are you
know thousand fold bigger than what they
can do an R um you can dive onto the
hood and write code in you know
MapReduce style that's pretty much good
for any sort of dense linear algebra any
time I'm going to pass over all the data
and do something and passing over all
the data and doing something is often on
the order of you know modest millisecond
counts so it's fast enough you can do
interesting things with a bunch of
passes it's not the standard you know
Hadoop MapReduce for speed it's much
much faster and then we support a bunch
more complicated stuff I'm not going to
talk about today but there's a
full-fledged key value store into the
hood and you can do kind of interesting
graph algorithms and stuff with that so
in summary most simple java just works i
right we have map functions that go to
the thousands and thousands of lines of
java code and they all just work and I
look like Java code runs like Java code
and it all feels right you know it's
fast
and in people everyone says fast but
we're running as the same speed as Java
code would run on a raise java arrays so
it's as fast as you could get out of
Java which is pretty much CPU bound or
memory bandwidth bound in most cases is
as fast as the hardware's going to go
you can write big data as well and it's
a little bit slower but not much because
we're doing compression as you do rights
I mean like I said we're typically
memory bandwidth limited would support
the full java memory model in the
distributed cluster there's no lazy
consistency saw exact it's off actually
fairly fast exact I'm although if you do
conflicting rights you're going to
follow the the Java memory model which
will not necessarily give you the answer
you want because it's pretty
loosey-goosey about ordering looking to
transactional updates which we do
internally a lot for command and control
kinds of things and then we're writing
you know big data distributed analytics
lots and lots of algorithms we've
written we're working on more we're
solidly working on 100 gig data sets
that's our show the nightly jenkins
tests actually each post push test is
now doing something on the order of
magnitude we're doing at terabyte scale
stuff now we definitely have customers
using terabyte scale data right now and
we have paying customers in production
and it's all open source and free so go
download and play for big data yeah
questions yes so you're working on this
and also the SE models a linear forest
and right clothes etc and unmaking them
off those yes so the current you know
interesting new model coming coming out
as cox proportional hazards which is
used typically for survival analysis or
lifetime analysis out of a collection of
a million people in your insurance pool
some people die off at a steady rate
after they've aged out enough and while
it may be harsh to think about people
dying it's what the insurance coming to
us for living for the last hundred years
and so they look at the large pool of
people and they say how many people are
old older older diet dead and then you
know we have insurance payouts and
what's the set of money and how much do
we need a-trak based on the age of our
pool and that's caca survival analysis
same thing actually works for lifetime
analysis of anything that you have and
then you breaks and you don't
so I phones what's the insurance
coverage costs for the collection of
people without you know I phones on the
planet when they die and you drop it in
the screen broke the longer you have it
the more likely is that you drop in the
screen breaks and you go in and get your
insurance payout so the guy he's doing
those insurance run that kind of
analysis on this on your car on you know
I don't know there's there's actually a
lot of uses for that particular one yeah
another question do what happens if Cano
goes down so we're not an H a solution
so we looked hard at that and we thought
about it for a long time and we know how
to do it but it's not worth the effort
right now so instead we went for sort of
raw speed and most operations are all
done within seconds to minutes and so if
a node goes down reboots cluster so the
cluster will come up in seconds loading
the data will take longer it's whatever
you know count of disks / mast max disk
bandwidth we can hit most describes to
eighty percent of theirs rated
throughput and a parallel data load so
without any trouble and then and you're
up again so if a node goes down it's
like a big calculator and it's no goes
down you shut the calculator off you
turn it back on again and you go again
so it's in that in that sense the
deployment model is very simple and
straightforward if we get to point where
people are running jobs that take weeks
or a week two weeks on clusters of a
hundred or more in size we'll probably
need H a solution and like I said I know
how to do it from a technology
standpoint hasn't been worth to do it
right yet q is any thoughts about
running h2o and bare metal versus the
via a link thought sorrowing ht1 bare
metal versus vm yeah so this is sort of
the obvious story about VMS versus h2o
we've been looking at that actually
right now a lot of people usually VMs
add some amount of overhead h2o will
burn all your cores and all your
memories sort of easily and so if you
add overhead you just slow yourself down
it's not like it makes sense to have it
share nicely with somebody else on a
neighbor because hto is not actually a
nice neighbor you say go it's going to
take everything and go and so it doesn't
work well it's not that it does
work at all you can work fine it just
it's not a nice neighbor for having you
know being on the same hardware we do
test and we run that way all the time
it's because it happens you know people
do it that way but it's it's perhaps not
the best way to set up a cluster for
running h2o but that said we run an ec2
all the time that's a very standard
setup for us and those are all obviously
virtualized mission that's all the way
yeah ok so it's complicated there's
essentially no connection but it looks
like there's a really big connection so
the the connection is most people's big
data is in Hadoop HDFS so will read from
HDFS as a parallel file source just fine
well read from s3 will read from local
drive or read from all these sources
including HDFS but we don't need to be
on a Hadoop cluster but we can so we'll
play nicely in the Hadoop ecosystem as a
funny kind of a mapper job if you want
to run us on Hadoop where the map sticks
around forever it never goes away and
then you know burns all cores when it
goes in his idol when you're not running
it so it's a kind of weird mapper job um
it's not very efficient in that sense
but it's not too bad in what really
happens is we go into somebody who's
interested you know in putting h2o in
production and they already have a big
Hadoop cluster lying around but they
don't have a spare bare metal cluster
lying around so we'll run on the Hadoop
cluster because that's the quickest way
to get a cluster operation going now
that said most Hadoop clusters have
machines that are spindle heavy and CPU
and memory light and we exactly want the
CPU and memory and and so it's it's a
you get a giant collection of tiny
machines and that's sort of not
necessarily the optimal way to run h2o
so you frequently better off with fewer
fatter nodes
it's going to come back but not that it
says anything so you know along the line
of fewer fatter nodes the what is what a
configuration for machine looks like
that is sort of interesting or works
well it doesn't varies by what you want
to do with it logistic regression is
strongly scaling and so you really can't
get away with a large sea of smaller
machines and it will do great random
forest typically has more network
bandwidth and as the trees get deeper
you have to build these histograms they
get exponentially bigger as the tree
gets deeper and suddenly have very large
data sets are getting slapped around
between nodes and so your network
throughput becomes an interesting
bottleneck and you're better off with
fewer nodes simply because there's less
network traffic you go too deep learning
the deep learning runs or neural nets
runs these neural net algorithms that
within a note are extremely efficient
using this hog wild algorithm their racy
and and and like sort of crazy fast but
the models all drift from each other
independently because there's no x86
coherency hardware across a node and so
that has to be taken care by shipping
the models across the wire to resync
them back up or they drift too far from
each other when you resync them they
sort of cancel each other out so you
want to keep resyncing them and that
turns into a network latency as opposed
to bandwidth issue so some of the
algorithms are sort of network tolerant
and some are less network tolerant and
so you kind of varies by algorithm
whether you want to have you know
whether you can get away with fatter
nodes or skinnier nodes and more of them
or fat or nodes and fewer of them all
right well that's actually you know
about time and and i'm done with slides
i have a lot of little fun extra add-on
slides that can go through people want
but when we call this the official end
and i'll be around for a little while
for Q&amp;amp;A here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>