<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • An Approach For The Internet Of Things • Christopher Meiklejohn | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • An Approach For The Internet Of Things • Christopher Meiklejohn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • An Approach For The Internet Of Things • Christopher Meiklejohn</b></h2><h5 class="post__date">2016-08-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zvS0iR4X8hU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Christopher Michael John
like Katie said I'm a PhD student at a
university a catalyst I live on in
Belgium we are in the French side not
the not the Dutch side and so yeah I
think so this is gonna be a research
talk so just heads up if if that wasn't
clear from the program and so I think I
successfully tricked everybody into
coming to see an inner things talk about
what we're really going to talk is about
distributed systems and so when I say
the Internet of Things you know the
Internet of Things is like kind of
buzzword we use it to refer to kind of
one class of applications where we have
a bunch of like sensors and we run
processing over them with things like
aim AWS lamda and stuff like that but
more generally what I want to talk about
here is is the idea of edge computation
because this kind of predates this
predates a lot of the internet-of-things
work by quite a bit and so what I mean
by edge computation is we're referring
to the the process of kind of pushing
both the computation and the data that
we compute at to the logical extremes of
that work so we're talking about devices
that are generating the data we want to
do the processing at the devices as much
as possible and so this could be mobile
devices this could be sensors this could
be anything points of presence data
centers you know they're all some degree
of the edge and if you come from the
networking world you know the edge means
quite a bit different thing than kind of
what we think of today when when it's
kind of framed in terms of networking
and so when I say computation what do I
mean
I mean arbitrary computation like any
program that I write and go on my
computer I should be with the data on my
computer I should be able to kind of
push that kind of computation the edge
right so we'd like it to be arbitrary
well see why that's difficult to do in
practice and finally I want to talk
about what one of these really
interesting about edge computation is
has a long history and in kind of
self-organized
self-organizing and resilient systems
and so if you're familiar with this work
directed or digest fusion this comes
from Cornell University in the 90s and
it's about kind of self-organizing
systems that can compute some sort of
result together in in a very resilient
manner and so I really like this work if
you follow me on Twitter you know that I
absolutely love like computer history
and stuff like that and so these papers
are interesting to read because this
work was originally framed under under
DARPA grants to Cornell's a Systems
Group and the idea was that you know
they were dealing with sensor networks
in the 90s they wanted to be able to
take an airplane and fly it over a
hostile battlefield or an area that they
could not get into drop a bunch of
sensors out of a back of a airplane have
some of them hit the ground some of them
will like keep working some of them will
break they'll self-organized into a
distributed system using an ad hoc
wireless network and then they'll
compute some sort of result in the field
and so that's a pretty extreme case of
edge computing it's something we don't
think about when we're trying to just
you know monitor the temperature you
know if your nest thermostat at your
home right it's it's a much safer less
aggressive environment and so there's
there's a good amount of history here
and it's very interesting reading if you
like this kind of work so to get a
little bit more practical we're going to
talk about an example about hospital
refrigerators and so many colleagues of
mine went to start a company that's
doing exactly this so I've I've spent
quite a bit of time thinking about the
problem and kind of exploring
alternative ways of solving it and so
what we're going to imagine here is that
we have a bunch of hospital
refrigerators I have sensors these
sensors are going to trigger kind of
alerts about the temperature whether
it's to warm to cold and maybe we want
to trigger behavior based on those those
events and so kind of the IOT reference
architecture that that we kind of look
at today and peter is going to talk
about an alternative one after this is
that we have a bunch of things and we
put them on the internet that's the
starting point to get where we need to
be and and what we would do is that
these things are going to like generate
events they're going to generate samples
and maybe we put this in something like
Hadoop distributed file system here and
then you know if this was 2007 it'd be
really invoked to use Hadoop to run
batch jobs over this data kind of
produce some results and then and
trigger some sort of thing but but we've
moved on since 2007 and so maybe now we
use something like spark or Kafka which
was talked about earlier
and so in this model we would kind of
process those events as they're
happening in real time and react to
those events as they happen so we can
imagine an ideal execution with this
kind of topology this reference
architecture that I've laid out would
look something like this so we generate
some simple we say temperatures fine we
send it to the data center and the data
center does nothing now when we generate
a sample that says there's a problem and
we transmit that to the data center
usually somebody gets called by a phone
or maybe this is the modern day version
of you know a paging system like page or
duty or something like that right and so
this is the ideal situation we want it
somebody gets call and all the things in
the refrigerator don't go bad and so if
we want to kind of start poking holes at
this we could say well I mean the first
problem is the idea of connectivity
right we all live on networks with you
know that use that you know that get
partitioned all the time as we've seen
from many talks earlier today and and
because of this I run into this
situation where if I generate a bad
sample and my connection to the internet
happens to be down and I can't transmit
that back to the data center then I all
I have is all I can do is buffer these
events and so if I buffer these events I
this is fine these devices usually have
enough to buffer the events but this
doesn't help anything in the
refrigerator right and so this is kind
of a this is kind of not desirable but
but something that we deal with that we
deal with all the time and kind of these
IOT systems that we that we kind of
deploy here and so what if I can't get
my events to Amazon IOT or my Amazon
service running on lambda or something
like that and so kind of the first thing
that we could do here is is we could
explore the notion of making local
decisions and so if we think about what
it means to make a local decision and
you know we have our event it gets
generated as a sensor it's transmitted
to Hadoop nothing happens everything's
fine if we generate a bad event if we
have the ability to take action to
perform local decisions then we could
just trigger the call from the device
right and so this is expensive obviously
because you need a bunch of backup
hardware but there is president for this
one of the companies that so PG&amp;amp;E for
instance can't send can't send electro
electro mechanical signal
for a paragraph that's overloaded
because the power grids overloaded right
so that's kind of a problem you know in
in the 80's and 90's so if you're in
your late 30s maybe or 40s you might
remember that you know it was very
common to have backup communication
mechanisms for things like iced tea on
lines and tea ones I actually worked for
a telecom that did this for fidelity so
they would have a backup system for
out-of-band channeling when when when
things would go down if you were you
know in the in the world of computing in
the 90s you know in in the late 90s
early 2000s when EMC introduced the sin
and then as Nazz devices would have a
modem connected to them with a phone
line and when a disk was generating crc
areas it would actually like call with
the modem to EMC and say hey this drives
about to die and then you know the next
morning at the office of EMC guy would
knock on the door you have a backup
truck and so this idea of local
decisions is not something that's that's
kind of new we've explored this before
and so the using local decisions to kind
of build an application around is really
expensive because it requires a bunch of
additional hardware and so maybe an
alternative that we could explore is
this idea of transitive dissemination
and so in this idea we could say well
maybe I have this sample go suit it's
fine the internet connection goes down
and now I generate the bad sample but
I'm connected via another wireless
protocol to appear and that peer can
trigger it can send the events on on its
behalf and and so this model should be
very familiar to anybody that's used
something like an epidemic broadcast
protocol that tries to rely on on to
give you very high probabilistic
guarantees on message delivery but
doesn't guarantee that it will work
correctly and so obviously you know
these epidemic for broadcast protocols
have been studied for a while and the
problem that you can trivially see here
is that you know to achieve high
probability in mass network emissions to
get you to 100% you potentially have to
send this down every single path you
know and there are entire groups of
researchers that are working on trying
to make this problem more efficient and
so then you alternate then you
additionally have the problem of well if
I get all these copies back at the data
center how do I know which ones given
that messages can go out of order how do
I know which ones are the most
up-to-date which ones are stale and
do I know which ones are duplicates okay
and so finally maybe we could look at a
model where we just send around status
right so rather than generating an event
stream and trying to guarantee that we
deliver with extremely high probability
within some sort of bounded amount of
time disseminate all the events
maybe we just disseminate state instead
and so a mutable state that describes
the state of the system and so in this
case we could imagine that with this
refrigerator here what the red circle is
is problematic but these two are fine
they can kind of disseminate this state
and build up kind of a overall state of
the system and in this model you know we
we additionally have a challenge of
ensuring that with the out of order
delivery everything can kind of look the
same so we have an equivalence problem
here and additionally we also have this
nice added benefit that any of the nodes
in the system can kind of make the phone
call to say that something is wrong and
so I will I will not talk about the idea
of how we ensure that you only get one
phone call
because that is a problem of
synchronization and I will kind of just
leave that exercise to the reader so I
talked about a lot of things and and
kind of what I'm trying to do is get you
to think about the idea of local
computation and I'm trying to get you to
challenge kind of the reference
architecture and think more critically
about the applications that you're
building and so there's kind of two main
points that I'd like to kind of drive
home here the first one is you know if
you don't just generate an infinite
stream of events that model every single
change in the system and kind of work
with a mutable state or aggregate state
then you can reduce state transmission
and this is very important because a lot
of mobile devices and sensors and things
like this
operate with very low battery very
little battery and the antenna is one of
the most expensive components in the
system to drive the second thing that
I'm getting you I'm trying to get across
is that the idea of making local
decisions if we can base a decision if
we can take action at the device we can
have a system that is in a sense more
alive it can be more resilient to
failures it can take actions and we
don't run into this situation where if
you know a bunch of our devices
that are doing something are
disconnected from the network where
where you can't do anything okay all
right and so if I want to talk about
replicated shared state away what I need
to do is kind of talk about consistency
and so you've probably heard a bunch
about this today already and you fight
heard a bunch about this in the past and
so what kind of just briefly do a quick
overview of kind of the ideas of
consistency of replicated data and so
the kind of the the first notion we'll
talk about is strong consistency and so
if we think about this the single system
image so if we think about one database
that we want to kind of have some total
order of events over if I have a data
item here represented by this circle and
I have two clients read that data item
concurrently now if c1 goes to modify
that data item and write it back to the
story using a compare and swap operation
this this will succeed in within this
kind of single databases first replica
will be updated now the problem that you
have here is that if c2 goes to right
and say while I read this green circle
I'm going to write this red triangle
this right operation is going to fail
because it's attempting to do this
compare and swap and the value has
changed from already's it and so these
are kind of this gives us this kind of
total order over the events and things
kind of change in lockstep you only read
them right and so this is kind of a
standard thing that we see in like kind
of strongly consistent systems like like
something like a console that's it's
backed by something like raft the raft
consensus protocol and so you know I'm
not going to diagram Paxos but I'm going
to diagram it just enough to show you
that if you have multiple replicas if
you have multiple copies of your data
items it gets that much more expensive
to make those guarantees it takes a lot
of time to get all of these nodes to
agree on an order never mind their
requirement that these nodes need to be
online and we're specifically talking
about systems today that we want to
operate while the systems are offline so
kind of at the opposite side of the
spectrum we have a notion of eventual
consistency which which kind of became
popular slowly over time and kind of
exploded in in to thin the kind of
mid-2000s in kind of a eventually
consistent system we have multiple
replicas
you know you can have clients reading
and reading these values from the
database and then if you have two
concurrent rights so client one writes
this Blue Square and then client two
writes this red triangle we can't
determine which one came first and so we
have to kind of assume that both rights
are concurrent and so this is very
similar to a system I worked on called
react and so in this case what we have
to do is have an arbiter the next reader
decide what the value should be it
should merge those values and then write
back to the dist data store the updated
value and so ideally the the thing that
you want to perform this write operation
to resolve these conflicts ideally you
want this to be deterministic kind of
the most trivial case you can have of
this is something like last writer wins
which just puts a timestamp with
everything and takes that that's
deterministic for a set of events but
but that doesn't necessarily model the
way we want the system to work because
it's hard to reason about kind of what
updates will win because you don't know
when things are going to get delivered
okay and so what I want people to think
is that kind of eventual consistency is
is kind of the model we're talking about
here we want to think of these devices
as as generating their own data kind of
owning the the copies of the data that
they generate and so we can think of
these three squares as being kind of the
events that are being generated and we
want to kind of share those events with
other members in the system in a
peer-to-peer way we own our own data we
compute with our own data and we want to
have the ability that these these data
items regardless of delivery order are
going to resolve to the same value
additionally when we think about the
results of computations we we kind of
want two properties here so we want the
property of merge ability and we want
the property of these computations
tracking Providence and and I'll go
through an example of what I mean by
that and so if we imagine that we have a
refrigerator that has scene events in
the system a and C and another
refrigerator that's seen the same event
a but a different event B now if we
distribute some arbitrary computation F
to these notes and F is computed with a
and C and F is also computed with a and
B
then the result of this computation
represented by a D Prime and D double
prime here should kind of track the
inputs that came from that computation
this is what I mean by Providence and so
kind of to put this in in a notion that
might be more familiar systems like
SPARC do this so that when when a Hadoop
job is kind of failing halfway and some
nodes get partitioned it can look at the
data structure is created and identify
the missing components it can't figure
out what those missing components are
and and kind of figure out where to go
get them and this is why SPARC is more
efficient than something like a dupe and
so the idea there is that it each of the
results of the computations is tracking
some data that is representing the
inputs that contributed to that
computation now additionally it would
also be nice to have the ability to
merge these so if I compute some partial
result and I compute some partial result
on another node I'd like to be able to
merge the results and so we could
imagine you know that I have some device
and I compute an aggregate over that
device maybe you know the average of all
if I'm playing a game and I track a
bunch of scores and and with those
scores I produce an average maybe I want
to share that average with somebody
else's average and combine it to create
a new average right and so we want this
idea of provenance so we know if two
computations have are sharing some of
the data and we know what derive that
data and we'd also like to have the
ability to merge them right and so this
is kind of like the ideal world you know
and and so it's trivial to do this if
you just store everything right and so
that the challenge here is to make is to
kind of try to make of this component is
to try to make this component efficient
okay and here what I'm just showing what
this D Prime and this D double prime
here is that we should be able to look
at the results and kind of order them
know some sort of relationship between
them whatever that relationship may be
okay all right so uh that's kind of a
pie in the sky idea imagining you know
what would make for a nice nice building
blocks for building distributed
applications that are easy and and very
nice to build correct that have these
nice ordering guarantees and we don't
have to worry about delays on the
network or offline clients or things
getting partitioned okay and so what I'm
going to do is talk about three three
kind of pieces of work here that a
preliminary
that we have and talk about kind of
whether you being used in industry kind
of where they've seen adoption so far
and then kind of make a vision for the
future and appeal to people to challenge
the way they build systems and so the
the first piece we're going to look at
here is called conflict clear replicated
data types so again I am the the CR DT
the the kind of CR DT talk of the
conference I suppose and and what C or D
T's are are data structures that are
designed for convergence for
distribution and deterministic
convergence and so we'll see an example
of what that means once we have these
data types that bring us all these nice
properties we need a way to combine
these into a programming abstraction
that people can actually use that they
can build and work with and kind of
operate with and finally we need a
mechanism for scaling these computations
right and so we're not talking about
networks of 30 nodes we're talking about
networks that we're running applications
on that have 10,000 nodes and so these
are the scale networks that were that
we're currently targeting and so it's
not exactly trivial to build a system
that can beckon scale like that in a
single cluster okay and so kind of an
overview on C or D T's so Co duties kind
of go back to 2011 there data types that
are so you know you would have in the
same way that you have sequential data
types and concurrent data types Co DTS
are distributed data types that come in
a variety of flavors here and so we have
things that look like sets multiple
different types of sets they have
counters registers things that are
called flags which are boolean 'z that
can only move in one direction and
finally dictionaries so this is the
recursive data type that can kind of
compose the others and what CRT T's do
are they provide a property called what
we refer to as strong eventual
consistency and that so means that if I
have a CI DT and I distributed I make
three copies of a set and i--i play some
events out of water I duplicate some of
the events I reorder some of the events
no matter the ordering guarantee no
matter the delivery of the messages and
it doesn't matter if I see duplicates
all of these objects as long as they see
the same set of events will have the
same outcome state and this is a very
nice property when you have a
distributed database that we were
concurrent systems writing to and before
I show an example one just to give you
an idea of where
has been used in practice you know a
league of the League of Legends chat
system from Riot Games uses CRD T's
we've seen we've talked to a variety of
other people that are using them for
instance opentable is using them and in
some of their point-of-sale service
systems the Danish healthcare service
the NHS in London so these are data
structures that have been proven to be
able to be used at scale
League of Legends especially the scale
there is is pretty massive obviously
familiar with the game and so to give
you an example of what this looks like
and I apologize for it being so small it
was much bigger over here we're going to
assume that we have three replicas here
replicas a B and C now what we're going
to do is we're going to model operations
on the set differently than we normally
would so we're going to model the
operations of the set as triples and so
this triple here is it's hard to see but
it's going to be a triple which is the
value one a set of unique operations
signifying additions into the set and a
set of unique operations that represent
removals and so here when I add one to
the set replica a will generate a unique
identifier which we're calling lowercase
a here and it will add it to the
additions so we'll say replica a at
logical time lowercase a has added the
element 1 to the set and then it will
propagate those results out to the
network now if C concurrently adds one
to the set that's a different operation
and so C will add one to the set with a
unique identifier one with one and the
unique identifier C lowercase C to
represent that and that you know really
these need to be unique identifiers for
for all of the replicas I'm just using C
to demonstrate to make it easier to to
follow the diagram and so when C goes to
remove one from the set it can only
remove the additions it's already seen
added and so C has only ever seen one
added the set by C so C can only remove
its own addition and so then it will
perform this remove operation and
replicate and so finally once all the
replicas see all the messages the merge
part of the merge process here for these
data items is that we take a union of
the the additions and we take a union of
the removals and now that we know that
the add set is a strict superset of the
removes that we know that one should
still be in the set now you might think
well this is really complicated to think
about
the intuition is is rather simple so
this is a set that biases towards
additions under concurrency but you've
got to set the biases towards removals
and the intuition here is that the ad
bias makes sense because normally you
don't add items to a set without without
this when the element the elements are
not on the set right that's kind of a
common programming technique and so
there's a challenge here when building
these data types that merge that you
have to find a distributed semantics
that that closely approximates the
sequential semantics of a regular set
that you would have in your programming
language and so it's a challenge to
figure out how to do that in a way where
it doesn't impose this massive burden on
the programmer and so kind of the other
thing here to notice is that you know
this is a kind of trivial example of
building a CRE T but you can see that
the garbage grows quite a bit it stores
every single update that's ever happened
and so there are optimized
representations of this that I could
have shown but it would have been much
harder to follow and so once we have
these data items this allows us to
replicate these things and currently
modify them and merge them but the
challenge now is that a user can never
make a programming decision based on the
user external value the one in the set
because if it did it would violate this
nice property of merge ability we have
and so what we need to do is we need to
build a system that allows us to program
with these objects where we think about
programming against the external kind of
value like the one is in the set without
actually programming with the internal
representation and so we are working on
a system that's a that's a library right
now for Erlang called a lattice
processing and this is a declarative
functional programming model for sets
only right now that makes the core data
abstraction kind of the lowest level
object to deal with B these objects
these CRT T's these mirja Ballabh jex
and because of this there's a challenge
in ensuring that when I take a set and I
kind of you know map it or you know do a
Cartesian product with another side or
something like that there's a challenge
that ensures that we need to make that
we need to make sure that all that
metadata I showed you that's internally
tracking this data structure to get that
convergence right we have to make sure
that we transform that correctly and so
that is a non-trivial task as we've as
we
slowly learned and are continuing to
prove is correct
and so to give you kind of just an
example of what this might look like
this is kind of like half Erlang half
not to make it kind of readable for for
a slide and so here we might declare
some set s1 and we might add three
elements to it and then map the outputs
of s1 into another set s2 and so this
would be like your normal functional map
operation which would say every time I
change s1 those elements will go into s2
and this is a product as a distributed
process that continues to do this over
time okay and so this is a work in
progress that we're working on this part
of my PhD work now I've shown you that
object before that one that if we go
back here that one that looks like this
and you'd say wow there's there's quite
a bit of information here this is quite
a large object to disseminate on the
network and so our colleagues are along
with us are working on trying to figure
out optimal ways to disseminate the
state so that we don't have to ship
around these entire objects and so our
colleagues in Portugal have come up with
this notion of Delta base state
dissemination and what this is going to
do is attempt to instead of send the
entire object just send a minimal
representation of the change because
that is going to be much easier to
disseminate and these are referred to as
Delta state-based CRD T's and so the way
the way Delta C IDT's work is that we
operate on the objects locally we make
our changes locally we batch up a bunch
of deltas or things that look like
differentials so similar to like a DIF
might look an R sink or something like
that and then we periodically broadcast
those out on the network to the other
peers obviously this is problematic if
you have clients that go offline and
never come back because your buffer
things indefinitely so we have to kind
of take practical measures to prune
things and and kind of evict clients out
of this kind of delta synchronization
process when when they're offline too
long and so we have to fall back to
sending the whole data structure which
is pretty common for anything that's
kind of doing mobile computing where
somebody dropped their phone and never
come back and so to kind of give an
intuition of how this works if we if we
generate a vent here
so if you have this green circle event
that gets generated and this blue circle
event that gets generated we also store
kind of a dictionary below which is the
updates that we need to send on the
network these kind of get accumulated in
in order and then when we go to perform
the synchronization we synchronize by
shipping just the Delta buffer if we can
unless that state needs to do full state
synchronization and then we prune the
Delta buffer and we continue to make
progress
so obviously this has some challenges as
well because I can't store a log of
10,000 users
I can't store a log of 10 to Delta
buffers these buffers of updates for
10,000 clients and so we have to deal
with the challenges of trying to figure
out an efficient way to do the state
dissemination that we need to do without
having to assume knowledge assume that
one node can know about everything that
exists in the cluster and so this
requires kind of rethinking the way that
you build a runtime system for a
distributed language and so there's a
lot of historical work on trying to
figure out how you can do something like
this and and so what we've decided on so
far is that we want to build a
dissemination mechanism into the runtime
system for shipping this kind of state
about objects in the language or the
model that's based on an epidemic
broadcast protocol and so these
protocols are nice because they support
high churn they support their extremely
fault tolerant and provide very strong
delivery guarantees depending on kind of
how you how you implement and take
advantage of the protocol and
optimizations aside kind of the the
basic way this works is similar to like
if you're familiar with the term gossip
protocol is that you perform pairwise
synchronization of state with every
client in the system so every time you
make a change you kind of fan out and
send it to a bunch of clients and then
you kind of pairwise do that until all
clients see all the updates the
challenges is that to make these
protocols extremely efficient and you
know this is work that's been going on
for for 20 years 30 years to make these
protocols extremely resilient and
tolerant to churn failures
these protocols can make very little
guarantees on message ordering and so
this means that messages may arrive in
any order you have to reason about which
ones came after which ones you have to
potentially tag this with cause little
information to figure out how to relate
these objects and so this pairs really
well with the programming abstraction we
built in with CRT tees themselves
because these objects are objects that
again
don't rely on message ordering to get
the correct result they're always going
to merge regardless of the output of
regardless of the message ordering and
so to scale this to our target number so
our target number for the end of the
year is 10,000 notes to begin moving
towards this target number what we need
to do is start with kind of a membership
overlay that doesn't store all of the
information in the cluster so not every
not every node in the system needs to
know about every other note but it does
need to guarantee that that what it does
know about represents a connected graph
so it does know about all it can deliver
messages to everybody in the cluster but
it may not know about everybody in the
cluster building on top of this we build
our broadcast trees which allow us to do
very fast dissemination when there are
no failures and we fall back to kind of
this peer to peer synchronization when
failures do occur in the network and so
for doing this in practice we use work
from our colleagues in Portugal the high
power view protocol for membership and
the plum tree protocol for broadcast and
so we build an overlay that kind of
resembles something like this where we
have a spanning tree through the network
and we have these kind of redundant
paths that can be used for state that
can be used for tree repair when a node
goes down in the system now each of
these nodes in the system could be a
completely transient node such as a
mobile phone or it could be something
like a distributed hash table so this
could also be backed by something that
has a stronger
guarantee of fault tolerance if we need
to make sure that data doesn't go away
because in our system we have to deal
with the fact that we're dealing with
data centers while also dealing with
mobile devices mobile devices might get
partitioned we don't care about
replication on mobile devices maybe for
fault tolerance it all depends on the
application and so this provides us a
flexible way to think about does
different consistency criteria and so
what we do is are we kind of layer or
work in between here so our
application works it's below it sits
below the broadcast primitives and
assumes that updates will be delivered
off the network to our runtime and then
kind of disseminates through that
network as well and so it kind of
roughly resembles a diagram like this if
you think about how the networking
architecture works and hopefully this
will get us to the numbers that we're
hoping to hit we'll find out soon okay
and so now they kind of just walked
through an application scenario we're
gonna walk through this idea of an
advertisement counter and so so our
original work two years ago was working
with Rovio entertainment on Angry Birds
but then we slowly started realizing
that a lot of our work is generally is
is more applicable to any kind of edge
edge computing scenario not just mobile
right and we can think of mobile is just
kind of a restricted case or maybe a
more general case of some of these
application designs and so for this
advertisement counter example we're
going to assume that we have a kind of
mobile advertiser that's going to
display ads on something like an iPhone
in our in our use case that we have from
Rovio they they want advertisements to
be displayed a minimum number of time
times that it's a guaranteed minimum
number of times they do this through
contractual obligation I guess and and
so they don't care if we over count and
so that's a very important part about
this application design and we're gonna
have clients go offline so clients will
have limited connectivity we still need
to record the ad impressions at the
clients offline and we need to
periodically synchronize these where we
don't lose any of the ad impressions so
just to kind of reframe that and in in
in different words it's a we want to
guarantee that we have a minimum number
of impressions but we don't care if we
over count and so the data flow graph
for our application looks something
roughly like this
and so we're just going to take a bunch
of kind of ads which we represent by CRD
t counters and we kind of grouped them
and just combine them with this notion
of contracts to make the example a
little bit more interesting that says
that only certain ads can be displayed
at certain times let's say I mean I only
certain number of ads can be displayed
like contractually like a different
parts of the
or something like that and then what we
want to do is just find things I have
valid contracts and so you know we can
kind of Union things and and kind of
compute the Cartesian product and then
take the filter which is equivalent to
just thinking about taking a lot of our
ads in a sequel table and doing a join
with a bunch of contracts in a sequel
table right and so this is how you would
represent that in a functional dataflow
kind of graph and what we want to do is
we want to send all of these copic we
want to take copies of these counters
and put them on the devices so that when
the device to send ads they increment
these counters so every time I display
an ad I increment the counter display an
ad increment the counter and then I
periodically send those counters back to
the server for instance and merge that
at the server and so every time a client
synchronizes with the server the server
is going to advance and it's kind of
view of the global number of counts that
have been done in the network and then
the server is going to disable ads and
we have a way of representing kind of
removals of ads in a way where it looks
like kind of a growth in states so that
program stays kind of moving forward in
in logical time and then periodically
clients can just synchronize to get the
updated count list right and so in this
application design there's an important
kind of notion here that everything is
it this is modeled monotonically and and
what I mean by that is just kind of
we're always accumulating information
that allow us to know if we see
something in the past it's all I'm
saying what's nice about the application
design is it supports arbitrary
distribution so any of those nodes in
that data flow graph I showed you before
could be running on any node in the
network I could run this computation
split across 50 different nodes it
wouldn't matter because I don't have to
worry about network anomaly is causing
the program to produce an incorrect
result and finally a notion of
divergence so divergence is going to be
a factor of synchronization so how often
the clients synchronized with the server
will let us know how far we're going to
drift off of the target number and so
you can kind of see similar realizations
in some of the Google data flow work
because they kind of have this same the
same notion of saying well we don't know
when the updates are going to stop
coming in so we need to do some sort of
incremental processing and handle
Avensis things are happening and so we
think of just kind of our server notion
is getting these kind of batches as of
events represented as mutable state
rather than a stream of updates where we
just keep sending things all the time
and that's kind of the - kind of
opposite sides of the spectrum so this
kind of graph just shows that you know
our server kind of interacts with all of
the clients in the same way the server
does peer to peer synchronization just
like something just like everything else
in this model we don't needs kind of a
special API or anything like that a
server is just a client that does some
additional processing that the other
clients don't do and so the idea here is
that servers are kind of Pierce they
might have additional code maybe some
additional security stuff to kind of
perform additional actions but any node
can make modifications to the
advertisement counters and kind of ship
this state around and cuz the second
notion here is that really when we talk
about servers what we're talking about
is we need a place to do something
exactly once we need a place to kind of
synchronize and do something exactly
once and and this is the notion of well
find all these advertisement counters
and eventually once I hit some target
number I need to charge a credit card
I can't let everybody charge the credit
card I need to synchronize and know that
I did that credit card charging at one
place and I did once and so we're trying
to think about computations where where
things need to kind of happen once and
where the rest of the system can kind of
bend to the way the to bend to the
requirements of the model and so what I
what I'm what I'm trying to get across I
guess you know kind of the general
overall theme of this talk and kind of
all of the crazy ideas that I've
proposed is the head or you know tried
to convince you of is the idea that
we're building up from zero
synchronization we're adding
synchronization when we need it rather
than trying to build a system that maybe
centers around something like zookeeper
and then say well how can i how can I
pull this apart how can I make this
system faster let me find the places
that I'm synchronizing that I don't need
to and so we want to kind of take this
bottom-up approach rather than this
top-down approach which is kind of using
these familiar tools and then trying to
scale the infrastructure once we've
built around kind of a common core okay
and so to kind of just like slowly
wrap up here there are a bunch of
challenges going ahead right so I a lot
of the ideas in here sound like they're
silver bullets you know they sound like
they're amazing the the problem here is
that some of the stuff is extremely
expensive to do still and so the first
one is is a notion of state explosion
and so if you if you're familiar with
the kind of the functional programming
fold operation fold is great because it
blows if you're building a language from
the ground up full blows open the doors
on what you can express in a programming
language and you can express a lot of
things in terms of fold and so if we
think of having a set and then we have a
counter and what I want to do is just
run a trivial fold over this set
assuming you know we pick an order to
fold so it's assumed lexa graphical for
the time being then if I'm going to
produce a counter that just represents
just the number of items in the set this
gets really expensive to store this
provenance information it gets so
expensive that storing the count
actually ends up being more expensive
than storing the set itself and just
querying for the count and so the
challenge here is how can we find most
the most efficient how can we figure out
a minimum bound for some of this a lower
bound for figuring out how to model some
of this stuff where we don't lose
information and so before we grow the
expressiveness of our language and our
programming model it's kind of a you
know it's it's growing into a language
it's a model now before we kind of
figure out how to add more operators to
the language and get nice syntax and do
all this stuff what we really need to do
is figure out some of these challenges
with state explosion additionally
security is a huge area that we have
some colleagues looking at but it's
really challenging you can imagine that
if we have this transitive state
dissemination that I might send data to
some node that shouldn't get it because
I want it to go somewhere else
and so we have some preliminary work
that tries to analyze programs and
figure out through information flow
control like can we find the most
optimal distribution path ensuring that
you never ever use a node that should
not get some information for transit and
so this is similar to you some of this
different this decentralized IFC stuff
that Andrew Meyers has worked on at
Cornell University and finally kind of
the way a lot of this stuff
works is that oh my god this is I got
cut off but this this is supposed to say
how restricted is programmable all the
operations must be associative
commutative an idempotent
away a lot of this stuff works is that
we form a lot of this around semi
lattices and kind of order theory and
and it's extremely challenging to do
certain operations when you need
everything to be associative commutative
an item potent for instance how do you
do something like a division right and
so uh things like averages choose end up
being really problematic and so we have
to figure out a way to kind of identify
where the trade-offs are and kind of
figure out clever ways of modeling some
of these operations and and provide kind
of explicit ways for the programmer to
say yes I know I'm giving up the safety
of this model but I need to do this
operation and so charging a credit card
obviously is not associative commutative
an idempotent for the most case for the
most part right and so you know we've
done a bunch of work and you know we're
continuing to do work this is a this is
kind of the the major work behind my PhD
thesis and and what I want you to kind
of take away again there's another slide
cut off for the next one what I want you
to take away is that I presented three
independently successful techniques
these at techniques have been proven
right so we know that the notion of
Providence has been in turn
widely successful in things like spark I
mean that's half of the reason spark is
so successful that's one of the two main
kind of contributions of spark we know
that see our duties are are very useful
in practice and we've seen them
successful we've seen massive success at
places like the London national hall in
the UK's National Health Service
services and the Danish health care
service and Riot Games where they
wouldn't have been able to build their
applications without CRT T's and finally
around the gossip protocols these
protocols have also been extremely
successful these protocols underlie
systems like Cassandra's metadata
dissemination and Reax metadata
dissemination so we've seen these things
work in practice at the scale of 100 to
200 nodes and so the challenges is the
challenging and the the challenging part
and the idea and this is the slide that
unfortunately got cut off because of the
sixteen by nine is that can we take
these three independently kind of
successful techniques that have been
deployed
in industry and used in practice and can
we combine them into kind of a cohesive
programming environment for the end-user
so that you have this nice library that
you could use in your java application
or your Scala application that allows
you to say this one part of my program I
want it to like be able to work offline
and scale really well but then you know
then I have to do the rest of the stuff
in like kind of the Java world of the
Scala world and so we can think of this
as kind of kind of this like kind of
monadic looking look at a distributed
programming I suppose and so that's it
thank you very much you know I'd be
nowhere without the help of all these
fine people in Europe so so I thank them
greatly for their support and so thank
you very much for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>