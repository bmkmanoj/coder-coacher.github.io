<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • Scaling Pinterest • Marty Weiner | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • Scaling Pinterest • Marty Weiner - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • Scaling Pinterest • Marty Weiner</b></h2><h5 class="post__date">2014-10-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jQNCuD_hxdQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">um I'd like to introduce it to the first
talk here and uh technical technical
session on unpin chest and we've been
running as part of q cons that we also
engaged and we've been running these
architectures you always wondered about
kinds of sessions and it's always great
to see these these large startups large
companies doing really crazy stuff so
here's a story of scaling Pinterest
scaling from a couple of computers
Martis has been with Pinterest from the
very very beginning so there's lots of
really cool stories from from scaling
Gary's not as technical you know all the
way through I was sitting last time I
watch this I was like like itching to
sew here how exactly did they do that
sharding are these more technical
details but you know on the others hand
is really nice to have this kind of nice
simple story and it's kind of sits in a
position to this whole conference you
know here at the conference we all talk
about all these new things all these new
fancy technologies and I really like the
message and this talk that you have to
keep things simple to make them scale so
please welcome Artie thank you ah hey
well I don't have to intro myself now
but so I'm from Pinterest been there for
a while today I'm going to take you on a
mystical magical tour of what it was
like to build pictures from the ground
up I'm gonna go through our evolution or
technology and I'm going to show you
some of the lessons we learned and many
of the many many mistakes we made and my
hope is that you won't make the same
mistakes but instead make all new
exciting mistakes
so first the evolution we started in
March 2010 and launched to the world and
sort of quietly I guess at the time we
were trying to figure out how to make
the website so what was the website what
are we building uh images here followers
we should have followers that seems like
a good idea
we'll need a feed will need aggregation
we'll need all these things
and what that usually looks like is
you've got two founders eating Doritos
and ramen and you got an engineer that
they found somewhere that's what we had
and it was really great it's a really
fun time because you're trying to feel
what you're building and you're moving
fast and the engineers gobbling it all
together and making it look right but
the underlying bits are just barely held
together but you're trying to you're
optimizing for speed of design over
engineering just yet so here's the three
it was Ben Silbermann Paul Chiara and
Yosh nella potty and and then for the
next nine months we were in what the
founder called we were in stealth mode
but not because we wanted to be going
for a little while a little bit
engineering little more product of
behind the scenes we had swapped cloud
services at this time if you recall
clouds cloud service has been around but
then they were starting to kind of
really get big and people were really
debating should we be on the cloud she
would be in data centers and I think we
swapped over to Amazon because it looked
new and exciting I think we may even had
some credits I don't know we started to
have a little bit of differentiation in
our stack so that we can I don't know
you know maybe have a little more
redundancy and things like that so we
had a ninja decks at the top and we
actually had four web engine running
Python we had a read read slave for my
sequel so that you can offload a little
your reads and a task queue so we can
send emails and Tweedy's and Facebook
E's and MySpace E's and live journals
and all that good stuff and then we had
a MongoDB
box because everybody has a MongoDB box
for counters and then and then I joined
around that time and then oh and here we
are this is a little example of what it
was like this is at our international
headquarters and early 2011 behind us is
the apartment that we worked out of in
the main area we worked and then there
were two bedrooms and one bedroom was
our international boardroom where we had
our grand meetings and the other room
was actually a bedroom so then the thing
happened thing took off and it was kind
of a crazy ride and we never slept or
anything and what happens during this
kind of a ride is that everything is on
fire we only had a few
engineers and everything gets hot we
were doubling every month and a half so
your databases are angry and people are
coming to your doorstep saying hey if
you just use my magic technology will
solve all your problems
so you're like yes yes gimme gimme gimme
and what happens is this and you're not
meant to read this you're just meant to
be terrified by it I'll give you a quick
rundown look we've got how many
databases we've got my sequel Cassandra
men based memcache Redis elastic search
and MongoDB and we have three engineers
so you have to be an expert in all of
these and and we were not so it was one
of those problems where each one comes
along it solves the problem for a couple
weeks
you're elated this is awesome and then
when it's when the honeymoon periods
over you start to cry a little bit so we
learned the hard way
it will fit every one of these will fail
so keep it simple when when the time
comes it's 3:00 a.m. and it fails it
sure would be nice if you could find the
answer pretty easily the other thing we
learned as we grew and we started to
become the biggest users these different
technologies is that whenever we were
the biggest user everything was a
thousand times harder because nobody has
tripped over those landmines for us and
then written tools as we tripped over
the landmines we wrote tools to fix them
so someone else could use them but we
were the first ones on so many of these
so lots of pain so here we are roughly
early 2012 and we had moved into Palo
Alto downtown and this was huge this
space was amazing there was an echo it's
crazy that didn't last too long so the
scaling kept happened kept happening and
around January 2012 we finished a
massive Rio Texas Oh roughly around
November October till 11 2012 we decided
to reconnect everything it was all on
fire it sucked it was too damn complex
so we said what works well then I'll go
through how we made these decisions but
we got down to first principles of good
engineering what works and what is what
is we actually won what are we actually
trying to build and we simply got rid of
most of it so now we've got my sequel
Redis and memcache and really we could
have done my sequel in memcache but
registers working so well we decided to
keep around for a couple really
important tasks so we got rid of all the
other things and then the ideal and any
scalability project is that you could
just
boxes of the numbers the financial
people hate me for saying that but but
then if you can hit that spot you've hit
linear growth or you've hit some kind of
growth where you can throw money at the
problem and that's kind of what we hit
at that point then we had the next
scalability challenge which was not
technology it was people so now you've
got more engineers so they're not all
doing everything all the time suddenly
you maybe you want to have one ops guy
and maybe you want to have two guys
building mobile devices and and one data
infrastructure guy or something like
that so we started to differentiate and
this is the beginning of a new era for
our company and it only gets more
exciting hair-raising so it got bigger
now we have to use the fisheye lens
so we knock down the middle wall and
expand it that way and now this space
the space was huge
we never knew yeah huge so we kept
growing in 2013 kind of linear growth
and uh we sort of did hit our
scalability uh the golden thing you want
out of scalability which is just push
numbers at it at some point SSDs came
out and we just kept pushing the number
of my sequel boxes up number of memcache
boxes up number Redis boxes up and it's
not quite that simple you start to hit
new issues number of connections becomes
a hassle Network all these other things
but in general we're past we were kind
of past the database mega pain point and
on to the next mega pain points and we
started to grow the company in terms of
people pretty fast suddenly to round off
all the corners you need product people
you need people to support them you need
lawyers and finance and and HR and all
these things it's only your company
balloons and the challenge now you that
we start to get is that when you hit
this 100 to 120 130 range people will
actually come up to you who've been
through this before and they'll they'll
say I'm sorry because you're going
through kind of a rough time what you're
what you're doing is you're now having
to make structuring your company if you
didn't already have it some company
start with it some companies don't we
sort of didn't so now we have to have a
improving communication feedback manager
structures things like this and that
always comes with some hassle because
this person wants to be manager this
person is a good manager but doesn't
want to be a manager you know all these
things and so any
structure you come out with will have
its own hassle and you're also going
through a cultural funnel because you're
bringing a bunch of engineers you worry
is the culture going the right direction
we could lose the culture now many
companies have what if we bring in 20
Google engineers all at once and they
change it into Google is that ok you
know these kinds of questions are
bubbling around in our heads so we
started to split it all up and now we
actually started to have individual
teams and this came with a lot of
struggle but but when it came out in the
end it felt really good starting to move
fast so you got data infrastructure
because now we have a full data pipeline
this thing needs to never go down
because the moment the data pipeline
goes down you're you lose your business
sense of the whole thing it's like a
captain who can't see outside the boat
we have a search team that was exciting
for a long time it was half a quarter of
me a business and platform to actually
support the business unit that was
starting to grow spam team because spam
became a problem web mobile growth we
started having a growth team so that
somebody is always looking at the
problem of growth are never one of our
number one initiative and then
infrastructure and operations team and
then a 65 non engineers meaning I hate
that term but can't come with a better
term product managers lawyers all the
the various other components that are
absolutely necessary to get this thing
rolling and then we moved to San
Francisco we ran out of space in Palo
Alto
thanks to Palin sure anyway but got big
this space would support us for the rest
of time but not not quite so we needed a
bigger space and now this is an old
picture and it's getting bigger so now I
think one of the biggest challenges we
have at this point is we've always been
in the same building and so the context
switch time or the cash load time of of
talking to the operations person for me
used to be this that's half a second now
it's I've got to go down to the other
side and I know that sounds like a
first-world problem but it impedes
communication by a significant amount
when you're not used to it and now we've
got another building and how about when
we have a building in New York and you
know these are the kinds of processes we
need to build in building culture around
a virtual environment and managing that
virtual environment all these things
have to be done very very well and we're
beginning that process
now so technologies this is a rough
layout of our stack at the very very
high level I wanted to keep it extremely
high level but it does give a good sense
for how we think of things we try to
keep it dirt dirt simple and we also try
to keep things a little abstracted so
someone co a-calling Pinterest comm goes
into ELB we're hosted all on Amazon so
we use a couple of their magic black box
tools and ELB is their load balancer
tool and one of the reasons we use it is
it's actually got magic hardware
underneath and if we need to deflect a
DDoS attack they can do that but then we
go into a software our managed load
balancer and the reason we do that is
that that gives us a little more control
ELB is to black box but at this layer we
can do things like shut off a University
where a PhD student is going a little
crazy which happened couple weekly then
we have our Python layer this purpley
layer here it's the business layer it's
it's currently Python but it's shifting
its uh if you come in from the web will
direct you to a web app where we'll talk
HTML back to you if you come in from the
iPhone or Android or somebody that will
go straight to the API the web layer
actually talks to the API layer now
exclusively that arrow now it needs to
go a little left and that's nice that
gives a nice abstraction structures the
whole tier task processing is done by a
system called pin later which is simply
on my sequel box where you put in tasks
a name of a task to run and some strings
and then a whole sea of Python processes
will pull them out with priority the
reason why we moved it to pin later if
you are in such the same situation you
can use something called Pyro's it uses
Redis and a similar operation it works
very very well we eventually decided we
needed a little bit more acid style
compliance and then every image on
Pinterest lives in s3 every user
generated pin and we get your initial
image and then we make a whole bunch of
different versions the small the medium
the square all that stuff and then we
front it with a CDN like Akamai and then
this is the part that's getting really
interesting as we grow we have this
intermediate service layer the the
yellowy boxes and they're connected to
the business tier with a connection
manager
system built around zookeeper we don't
have any load balancing between we used
to that was a hassle I would not
recommend it essentially what happens is
when a web app wakes up it says hey I
need a search service and this thing
will have a list of them say here
connect and they direct connect if one
of these dies on either end there'll be
a little scramble and we'll make new
connections if a new one comes up we'll
rebalance their - it takes away a middle
tier of hassle having an intermediate
load balancer is its own little
nightmare because you've got to say well
do I need 10 of those and you have to do
shuffling and then sometimes they just
don't work mess got rid of it and happy
ever since and then the service tier is
sort of where we're growing there's a
whole sea of different ways people do
this
Facebook is a classic example I don't
know exactly what it looks like today
right now but as of a couple years ago
they were one giant codebase with a few
services so they'd have big deploys it
would be a big thing and they had a
culture around that it worked it worked
well for them
Amazon's the opposite I think one
request Amazon's about 300 service calls
I think we're kind of going somewhere in
the middle I like to put it at a kind of
a boundary of a team that's one way to
look at it or roughly a product so like
the search service is this one nice
property you get out of this structure
is the search service is this massive
engine behind it you do a whole bunch of
nightly gathering of data you have all
these smart systems I don't care as long
as I can search for Star Wars and
crochet and get little nit Yoda's that's
all I want to see so search service
provides us that the feed service is a
service where you put pins in and it
goes out to everybody it should you know
handles it abstract sit away from me one
of the great examples of safety is the
my sequel service this is one of the
first ones we did and I kind of think of
it like the code on the Apollo 13 or
like on on satellites where it's really
dangerous code to touch one team will
lock it down write all the things around
it because what will happen if you mess
that up and this did happen is someone
will make a mistake and it'll be
innocuous at first and then eventually
something bad will happen and your core
data which for us ilysm my sequel will
start to get corrupted so we had a
situation and I'll show
a minute all of our things are started
and what was happening was somebody
would pull data out of one chart edit it
and put it back to the wrong shard so
now I'm just data started going all bad
so what you want to do is you that one's
a good one to have a service because you
can lock it down only these people can
touch it they're super careful if you
want to make any changes you've got to
go through NASA and everybody else and
you know and then we have things like
the spam service my team is now working
on that and one of the nice properties
of that is I get really fast speeds of
deploy we used to be part of the massive
you know kitchen sink codebase which is
to me everything we haven't pulled out
as a service and when we pulled out our
deploy times went from twice a day maybe
maybe not to once every 30 seconds with
tests and everything so it speeds us up
to all of our data is in these four
systems HBase is a relative newcomer
we've been using it for about a year and
a half now I think but we're largely in
my sequel sharted which I can go to more
if you guys are interested afterward and
memcache and Redis and then HBase is
something we're using for new systems
and I'll explain why and then also a
first-class citizen
although through it threw it on the side
is our data pipeline this is one of the
scariest things we build it's one of the
most important essentially this is how
we can see outside the windows how we
fly so everything that happens on the
site gets funneled into Kafka which is
like a giant firehose everything goes in
and you can drink from the fire hose
it's from LinkedIn really good it's in
the same vein as flume inscribed but
this one scaled very well for us
and then you drink from it we have a
system called set core which which
watches the logs chopped soms and then
puts them in s3 where now you've got
your magic eleven nine of magic
durability this system is a bit scary
because if it goes down anywhere along
there that data just evaporates and it's
very hard to get back so you you have to
have a really good strong team around
that system one else a nice thing about
Kafka's you can watch everything happens
on the site so we send it down for spam
processing or any kind of stream
processing so that if a whole big attack
happens we can react to it within
milliseconds
once it's an s3 you can do anything you
want with it because everybody talks to
us three these days send it over to EMR
that's
Amazon's Map Reduce or send it over to
q-ball that's someone else's Map Reduce
Amazon also has redshift which is a
flavor of Map Reduce that's a little
more structured you can't do as much but
it's much much faster so once you're an
s3 you're home free so if you're certain
the sharding I do have a talk on what we
did for our sharding you can go check it
out if you want to we could talk about
it afterward I go into how we landed on
this charting we did how everybody else
does they're different flavors and why
we chose this one we learn from many
mistakes and then made the next step so
around September tails in 11 or so when
everything was on fire and we wanted to
be architect we kind of had this problem
of how do we choose what to choose and
this this is how we sort of drove the
technology we'd use and then finally how
we drove the sharding technique we
decided on we kind of had this gut that
anything complex was evil so we didn't
want to do that and everybody I've run
into this problem all the time today is
I wonder why are you using these no
signal technologies why are you using
all these different really amazing new
technologies and this one person summed
it up nicely for me and then I had to
beat him up afterward he said well I'm
building the latest and greatest I need
to use the latest and greatest and ya
know if you need a flux capacitor to
make your time machine go and that's
brand new hot use that fine but if
you're building yet another really cool
social network or some other flavor that
you need a database well they've existed
for how long I don't know 60 years and
my sequel has been around for a long
time so use that it will do what you
need mostly so we made a list of
questions to ask ourselves so first does
it meet your needs well if it doesn't go
back in time and need a time machine
it's going to be a problem but most of
the time you need to at least make sure
that basic thing will will hit so one
example is my sequel can do a lot of
things but one thing it doesn't do well
is say like if you want to look up by
geography not good for that so you may
need a different solution how mature is
the product this is the biggest question
I think that most people get wrong
especially when they're building their
little tiny startup so I had to define
it what is maturity well to me maturity
is the blood and sweat that's gone into
something divided by the complexity so
blood and sweats like again my sequel is
a good example Facebook has poured in a
lot of resources into it they've beaten
that thing down they've seen every kind
of
that can happen and an example of why my
sequel an example of the maturity is you
can go and find any answer to it there's
a bunch of things that fall out of this
if you have a problem with my sequel the
answers out there and the complexity is
the other side of it so memcache an
example is a hash table with a socket
you can't get any simpler than that and
that sucker was mature roughly on the
week it was released and it's been
beaten down for several more years I
think it's about ten years old now so
that nothing just works kind of on the
flip side if you take a technology like
Mongo it's it's really growing fast
they've got a lot of blood and sweat
going into it but it's a bit more
complex and the blood and sweat compared
to something like my sequel much much
smaller so you've inverted this and now
you're saying let's go to market and
that's that's a problem I think we've
run into a lot so out of maturity comes
many properties and these are the next
couple bullets for one is it commonly
used typically when something is mature
people have used it there's people out
there so for example HBase which we now
use we couldn't hire for there's nobody
out there and if there are people out
there like we now have two homegrown
HBase experts we manacle them to their
desk and don't let them go versus my
sequel if you're in downtown San
Francisco and you faint and someone says
hey we need a my sequel expert there's
one out there guaranteed so so that's a
really big concern can you hire for it
it's a community active I'll tell you
the example that I remember with this
was it was 2:00 a.m. and I had a my
sequel problem I went on to Google and I
said you know replication key failure oh
and there was a million articles and you
go into stock exchange and it says
here's how you solve it and then they
call you a noob so like wow okay mature
versus something like mint base there
are zero answers out there I finally
found some little chat rooms somewhere
and the guy was asleep he was logged in
but he was asleep so that was great uh
how robust is it to failure well um my
sequel I'm going to keep saying mice
people this sucker I've never lost any
data on it and in the grimmest days I
remember the hard drive had crashed and
there was some other problem with the
machine I could barely login but I got
in and I found some random tools some
guy had written because he had this
problem and I slowly sucked out the
memory and got it onto another box and I
lived another day unlike some of the
other technologies as they grow
I don't want to pick on too many of them
but you run into a problem hard drives
have a little problem is a little bit of
corruption and we had the CTO come to us
for their particular technology and say
sorry data GaN fortunately we never
trusted anything besides my signal for
golden copy of our data so in all the
events when we had catastrophic data
loss on these other technologies we were
able to read arrive it next one is how
robust I'm sorry does it have good
debugging tools and this always comes
with maturity because someone trips
across the thing they want to profile it
they want to make it better so you get
things like like online backups
profiling all those things and men base
is an example I had I couldn't find any
of that stuff in my sequels loaded
anything you want it's there then you
have to get in the question about how
well is it scale well my sequel doesn't
scale past a box so that's the promise
that no sequel gives you is you just add
a box that's awesome I wish it would
work and it does work it's getting
better but but it's got its own set of
issues so my sequel we had to layer on
some some scalability to get it to go
more than one box and then finally the
cost justify we didn't really play this
game but many people have this question
about do I going buy the big IBM db2 s
or the Oracles of the world in our case
we didn't really have an option we were
a small start-up with no funding so my
sequel it was and I think even if I had
the money I think I'd still consider my
sequel but that is a question you have
to consider with anything you build so
why Amazon I've been asked as so many
times I thought I'd do a slide on it and
actually this goes back a little bit to
the days when everybody was wondering if
we should do data centers so it's
getting a little dated but I think it's
still relevant if you haven't used it
it's a variety of different servers and
you can run Linux or Windows you have
the option of managing it yourself but
they also have a lot of little tools so
if you want to not deal with your
database you can use RDS or dynamo and
RDS is like a Oracle or my sequel clone
and dynamo is like a big table kind of
clone or a Cassandra ish thing you can
use their load balancing which we use
their DNS we also use that they have
MapReduce if you wanted on day one we
used it for a long time but it had
scalability issues we had to get around
they have basic security firewalls and
everything built around it I'd say the
reliability is good I've been in data
centers I've been in AWS and everybody
likes to rag on AWS but
to me it's been very very good we've had
our big problems but we've recovered for
them the community is good and they're
at cheap but the promise of a new box
and seconds changes your whole world
especially from the old days when when
people asked about data center now if I
need a memcache box I can get it now I
can get it at 2:00 a.m. no question in
the old world I'd have to plan several
months ahead and hope I get out right we
use route 53 for DNS if anybody's
curious ELB for that first top level
load balancing works very very well
managed ec2 we use for most everything
else and then we use s3 for our images
in our logs Python we're primarily a
Python house but we're shifting slowly
I love Python I'd do it again if I had
to it's extremely mature I don't know of
any bugs we ran across the community was
really really good it seemed like our
dev was a lot faster I come from the PHP
world and I felt like it was a little
bit more structured and not too
structured to the point where I have to
make a factory factory factory just to
get a an object out seems like I can
move fast the libraries are very very
good I think one of the most effective
rapid prototyping tools we can build
with that's available the only problem
with Python is that it's it's an
interpreted language and the garbage
collector I couldn't find any info on
Google about something like is it
generational what is it so I went and
finally read the source code and the
source code for the garbage collector is
this long and if anybody's written a
garbage collector you know that's that's
too short so it was too short and so to
me that meant it was probably stopped
the world terribly and it was so that's
a problem with Python so you eventually
have to deal with that issue but I we're
just now dealing with it at the scale
we're at so I'd say if you're building
something new go with what works and
what's fast especially for your
developers so if they're Ruby people do
Ruby whatever works we're now moving to
Java and go for a couple of course
services and maybe for bigger systems
and that's primarily for the speed
reasons uh
we use Python for everything most of our
services and now Java and go we're
starting to use anywhere where that that
little that we really need to be fast
and that the variance needs to be low so
we're really tightening up our SLA so
that's like at our varnish layer for
load
balancing our search indexers are all
written in that they've got a lot of
work to do and they a lot of CPU
computation many of our systems by the
way just kind of turn around and do a
lot of routing so CPU computation
doesn't play in so you can get away with
Python just fine and then our high
frequency systems like the my sequel
services and our memcache routers and
all that stuff is now moving to Java and
if you haven't used go it's kind of cool
I kind of like it it's got kind of the
nice mix of Python and Ruby but it's
compiled and it's got a growing garbage
collector and the communities growing
very very fast okay so my sequel
memcache this is kind of the the
punchline of the thing these suckers
just worked for us they survived
everything we put against it every other
system in our stack has died in a
catastrophic way and I can go into that
if you like a little bit after it's
extremely mature both of them are
they're well known and well liked
everybody loves them everybody uses them
if you haven't used it yeah if you're
not just a user of it you may have
developed it and you know these kinds of
stories the response time to request
rate decreased for increased linearly
for us meaning that it wasn't just one
day where the thing got super slow as we
grew it sort of went down to her it was
intolerable so that was really nice
property a certain other technology we
found between 6:00 p.m. and 10:00 p.m.
we grew a little bit and we went past
some magic cliff and that was it and we
went from 10 milliseconds to 4 seconds
and had to get more RAM essentially
that's very hard to plan around and
makes me sweat so I love this property a
very good support so there's extra
backup for online backups works very
well you know top you can tell what's
going on inside my sequel Matt kit will
tell you bout my sequel and memcache and
the the community is very active
Facebook uses my sequel and puts back to
it at a very rapid clip we use it for
our primary storage of our main core
data all of our users boards pins
comments domains things like that and
now monetization and other major data we
separate our things into mappings and
core objects and I'll explain more about
that explain more about that in the
sharding talk there's a lot of
advantages between behind splitting
those two out and then we use it for
things like legal compliance data where
we need acid compliance because
otherwise
lawyers yell at me and then I get scared
redis this is the one that kind of
surprised us a little bit this is new
not as nearly as much blood and sweat
but the complexity is pretty small if
you ever look at the source code it's
kind of weird it's kind of cool but it
works it's very simple and works very
very well the community is very good
people are loving it people are putting
a lot back into it a lot of people are
adopting it twitter has it we have it I
think Facebook was Twitter messing
around with it a little bit so big big
people are using it right now and
hurting it in new ways uh the cool part
about it is if you haven't used it it's
a key value storage stores down to the
hard drive and it comes with a bunch of
different little technologies data
structures so if you need a set if you
need a list if you need a sort etcetera
a hash table or any of that kind of
stuff they support it and even cooler on
the cloud is the different kinds of
persistent modes they have your
durability mode so you can have store it
down to the hard drive now like my
sequel does you can never store it down
kind of like memcache so if if the Box
goes down it evaporates like a classic
cache or you can have snapshot and this
is a this means store your brains down
every 2 or 3 hours or whatever you said
and that's kind of cool especially on
the cloud when if your neighbor starts
partying at 3 a.m. and your latency of
your hard drive starts to go up if you
can do snapshot mode there's a pretty
good chance you won't store down on that
that time frame versus my sequel will
grind to a halt so we did that for our
main feeds we have a push model so every
time if I follow all of you and you
anybody puts up a new pin it will come
into my stack so snapshotting was a
perfect model for that because if i if
we have a hundred of these boxes and
mine dies I may go back for a couple
hours and back in time but then it will
replenish and and it really simplifies
the whole model and gives it's a
trade-off of that weird behavior for
much better uptime and it's also open
source so it means you can fix things
I've fixed many things in Redis over the
years they just as we hit the new
problems we use it for our whole
follower graph is stored in Redis and
this is our golden copy of our follower
data so we put a lot of trust on this
and we've been doing it for about three
years now so that means all of the
little
relationships and and pinterest follower
models kind of complex it's you can
follow boards or users then you can
follow users but not that board you know
these kinds of complexities we store it
all in Redis we store our public feeds
in there the reason we store our public
feeds in there is and not something like
my sequel is my sequel is still living
in the 90s it's one of its weaknesses
its living with the Macarena and
everything else it's a tree structure so
that means every time you want to insert
something you put something on the top
you have to walk a tree well everything
we build these days seems to be some
flavor of social network which is a
fight phone queue so put on the top read
the top 20 that's what it what Twitter
is it's so simple but my sequel is this
tree structure and so it is slow and
I've talked to them about it but it
turns out Oracle my sequel is built
around this tree structure from the core
up so it's a little hard to change but
if they can get that fixed I think that
could be a great solution otherwise what
happens every time you insert it's slow
it doesn't work very well for mass
inserts but registed Redis actually has
a true notion of an ordered list with an
o of 1 insert and it all sits in RAM so
it's blazing fast and we use it for
various caching mechanisms too it's very
nice for caching like if you want to
store someone's board to the list of
their pins you can make it a list of pin
IDs and you can say give me the second
set of 50 or something
so why HBase why after I rag Don this
kind of technology did we bring it in um
it's it's uh it's got a growing
population of core dedicated users and
it worked fairly well and it's got some
nice properties that we couldn't
replicate with my sequel or Redis one
big problem with Redis is that it all
has to fit Ram so if we have a really
good active rate say like it's 50% 60%
better or whatever that's still a large
chunk that are not using the site and
we're wasting RAM floor or if we show
people 500 of their feet and we want to
let it go to 10,000 can't do it
because it only sits in RAM and it costs
us a bundle so HBase has that nice
notion of an actual fast insert it's an
O of one a pin to the end massive fast
writing system so it now supports our
home feeds it's really fast
it's difficult to hire for we had to
grow our own HBase experts
it does have nice strong Hadoop
integration so if you want to do
MapReduce you can link in immediately
and the performance has been really good
big people use it like Facebook
so they've tripped over many of the
problems we've had I'm starting to
wonder if everybody's continuing to use
it I've started to hear stories that
maybe they're going away from it there's
questions about its scalability you know
the promise is that it has massive magic
scalability and it's efficient but I'm
wondering I'm starting to wonder if that
is actually happening and I'm wondering
I want to look at look at it over the
next two years and see if that promise
plays out the core ideas with my sequel
if we want to have a short in my sequel
we have say in number of boxes times 2
or maybe times 3 because we want backups
that's in times 3 that's pretty
expensive but for this one if you have a
cyclical ring and you do all this magic
sharing you can actually get it down to
in theory in plus 2 for backup or plus 3
for backup that sounds really cool
especially as you get really big I but
I'm wondering if the actual promises
playing out so I'll let you know more as
I find out so we use it for user feeds
rich pin details works really well we
put spam features in it this thing is
really operationally nice you can just
make a new table and it flows out
everywhere so that's kind of cool so you
know the promise plays out of
scalability then this is a really great
system to invest in and then we also
have a mapping of user relationships to
pins so what happened to some of these
technologies and mind you this is now
three years old so many of these have
matured way past what I want to use is
this is more of an illustration of what
happens when you use a product it's not
quite as mature as you like and all
these technologies failed on us
massively in one way or another and I
know that many of them have gone way
past their initial spot so I'll kind of
go through some of these and answer what
happened with them but I won't pick on
any individual one so did they meet your
needs for all of these we needed either
counters or search so yeah how mature is
the product well this was the problem we
didn't answer this question all we had
with these people coming to us saying
hey use this technology will scale the
moon for you all your problems are
solved sign up that is it commonly used
we didn't really answer that question
many of them are not in bases a classic
one
I think came from Zynga and I think is
now being kind of dropped under the rug
now it's not commonly used and it was
very hard to finding the answer
certainly nobody who knew how to use it
so what I ended up with certain
technologies was my cheat sheet of how
to use it and I eventually published
them for other people with Memphis my
classic favorite one was it had an error
message that says something like 0 7 EF
a B is opaque I don't know what that
means so eventually we had a cheat sheet
of that
well that means this and so this is what
you get out of it
kind of kind of dangerous at 3 a.m. is
the community active on many of these
they were not so there were zero answers
for any of these on one of them around 2
a.m. the thing just broke you know
there's no way to fix it you add a new
box started the magic replication starts
going and around 20% stopped okay what
do you know there's nothing to nudge
nobody's given you any extra tools to
nudge the thing has a bug what do you do
so what I did is between 2:00 a.m. and
6:00 a.m. when the Sun came up I got it
back onto my sequel memcache and then
problem solved how what is this gale
well these are that's the whole promise
these are really cool if they would work
doesn't have good debugging tools they
didn't many of these did not and there
were major problems around that so you
couldn't even find out what was going
wrong so you've got a problem you don't
even know what the problem is and you're
trying to solve it and they're all free
so there wasn't really a problem here
they all had weird different many of
these have different support contracts
and some of them are very strange that's
another thing to look at if you need
support some of these had such strange
support contracts we couldn't even buy
them it was either too expensive or to
mind eminently complex so if I had a
second chance to rebuild all Pinterest I
love going through this every once in a
while the first thing I would have done
is put logging on day one we did it in
terrible ways we had my sequel box where
we logged my sequel is a bad place to
log it's a tree who needs trees with
logging so what I would do is I put it
into Kafka and make a little thing to
put it into s3 problem solved then you
get MapReduce on day one learn how to
use MapReduce there's SQL 101 it's it's
easy it just takes a takes an hour I'd
also use stats T stats he's been really
good we're running into a couple of
scalability issues with it just now but
it's work
with us for the whole time it's from
Etsy essentially you say send a counter
that way it's a UDP packet so you don't
have to worry about that other system
breaking and you could send counters
bumps encounters or you can determine if
you want and you can send timers so you
can say this process took this many
milliseconds that's really simple setup
and really nice to have and then I'd
also log everything it will save you on
some day where you screw up my sequel if
you're storing all your stuff on my
sequel you corrupt your data
accidentally go to your logs the data is
in there from yesterday and you can
recover it you also get basic analytics
about how your site's doing and
recovering recovering from data
corruption the other one I would bring
up is uh Nagios or some kind of alerting
on day one Union or well isn't meaning
monitor monitor an idea is one of these
on day one because even though it's
designed to wake you up when there's a
problem it will get you sleep in the
early days one of our founders would
wake up every hour in the middle of
night
roll over check Pinterest good it's up
go down go back to sleep so when I
brought it up on the first day he was
like Marty I can sleep again I started
losing sleep now because it started
waking me up other things I would have
done we started around we started in
September on that graph when everything
was already hot so when you want decide
to make to shift to this charting
approach you've got the problem that you
already have a site you have to maintain
you're trying to build this new thing
and you're trying to transfer data off
so you're trying to take a little extra
load from that thing and put it over
here so that was a nightmare I wish we
had done it earlier it's a little hard
to know when but but my one little hint
to you is typically what happens with
any database especially using my sequel
is the first thing you do is say you
have a pins and a follower table you
take the follower table put it on that
box oh you have double capacity now
great well that's all I can ask for so
long so the typical thing you do next is
you start making read slaves and you
make more read slaves and you make more
read slaves the moment you make your
first read slave that's the hint you
should be looking at sharding right away
in fact the time bomb is already started
the problem with read slaves there's
many of them one of the biggest most
irritating things is that they make
these weird subtle bugs you put new
thing in my sequel you clear the cache
and somebody else reads before you
actually you know replicate the data
down to your read slaves and problems
pain in the butt the moment you start
thinking you need to do that
that's when you should be looking at a
starting solution email me evil will
party we also kind of fell into that no
sequel trap because of the problems we
were in everything was on fire uh I
would I would watch that closely I think
you may want to consider many of the
things I've been hearing from companies
it's actually the flip you would think
that no signal would be good in the
early days because it solves your
operational hassle for you turns out
many companies the time when you can
actually support a no sequel they kind
of excited hey we can have a few
engineers who can actually support HBase
and we can use the technology for its
it's good properties in the early days
when you have two and three engineers I
say stick with what's simple stick with
what will work stick with what will be
fixable at three and when the things
broken we also used a lot of weird
technologies for background test
processing there's a RabbitMQ classic
cluster Abell technology where you put
something on one end and it comes out
the other way too complex didn't scale
with us we eventually went to Pyrus
which is just a collection of boxes that
feed from Redis and we learned that
Redis works very very well so Redis is a
giant feed put stuff on the top you say
I want to run this task see a Python
processes takes one runs it couldn't be
simpler that simplified everything we
had the other thing I would recommend is
in the early days we also just had one
funnel of all tasks that's dumb
you should split it out into the
important tasks and the unimportant
tasks maybe even prioritize so one
example is if we want to create a new
pin and we will have a couple tasks for
making new images we need to do that now
but if we need to send out tweets that
can wait in fact I can even wait an hour
you know if the other things are broken
you want to make sure the hype things
work now I wish we had hired a technical
operations person earlier the moment
that guy came in and started doing the
chef and puppet steps where you puppet
eyes everything so you can bring up a
box by pushing a few buttons that made
everything a lot easier we used to have
these what I called Apollo 13 checklist
because what could go wrong and you'd go
down the steps to bring up on my sequel
box and hopefully you got them all but
it would also take a while so puppet
made it so that all of our boxes could
come up pretty fast if we needed to
change one property cross all of them
they would magically go across all of
them so it gave us scalability I wish we
had done it earlier and unit testing
might have been also a really good
choice
earlier rather than just kind of hoping
to the best and finally a be testing is
something I wish we had brought in
earlier it's very easy to add on what we
now call a decider you can build one in
a day a symbol version basically what it
allows you to do is say you have a whole
list of different kinds of properties
experiments or whatever you want and you
have a little drag a little slider so
you could say this brand new thing I'm
bringing in I wanted to bring it up to
5% okay it works now 10% okay it works
now 100% or if something if a
non-critical system is broken you have
kill switches so you can turn off
instantly so you can build the system
you can make a little simple wrap around
it I may even have an open source I'm
not sure and then the idea is that it's
in my sequel it goes up to memcache and
you say should I show this or not and it
gives you a little gives you a little
routing inside your code really
convenient last thing looking forward
we're now growing to beyond 400 people
we're now hitting things like how do we
live virtually how do we how do we build
out the next big thing how do we get
faster essentially how do we build more
Legos so that the we can build product
twice as fast and twice as good that's
kind of our current challenge so that we
can get that funnel going really fast if
we can have 400 experiments going at
once instead of say 100 that gives us a
lot more opportunity to see the next 5%
and if we can keep layering on those 5%
we'll go to the moon and then continuing
to improve our collaborations very
important because as we get bigger
collaboration gets harder and my final
little thing to you is just have fun
with this thing culture is a big issue
in many companies there's many that have
failed due to toxic culture they've
built and I felt and found that in the
beginning if you make the culture fun
people like to come there they like to
spend their daylight hours with you and
they like to have fun building things
with you so if you're building a new
startup kind of focus on that if you can
make sure that your people are having
fun because this damn thing struggle
hard enough so so no seriously have fun
take that heart are there any questions
zookeeper um yeah yes oh thank you so
the question is that there's a different
technologies mice equals simpler in
complexity and there's a Cassandra is
much more complex what about something
like zookeeper so zookeepers built on a
Paxos ish thing I don't think it's
actually true paxos for getting to
consensus and actually I wonder if it
was the right choice it seems to be
working well but it's gone through a lot
of hassle we use it primarily for
configuration management so which box is
live where and pub/sub and this kind of
thing I wonder if Redis might have been
a better choice you know close enough
eventual consistency this thing will
tell you the moment everything is is the
same but you lose you get split brain
problems you get all these other
problems so if you're going through the
exact same thing for configuration
management I think that'd be a very good
question to ask I know we've had our
hassles with zookeeper but at the moment
it seems to be working
I wonder if something like Redis or just
wearing an s3 might be fine you can do
some kind of a pub subby kind of thing
there are some kind of simple polling
more questions cool so if anybody's
interested I can go out here and talk
about sharding if you want to get more
into the Nitty Gritty or any other
questions you have thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>