<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Enabling Microservices at Orbitz • Steve Hoffman &amp; Rick Fast | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Enabling Microservices at Orbitz • Steve Hoffman &amp; Rick Fast - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Enabling Microservices at Orbitz • Steve Hoffman &amp; Rick Fast</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gY7JSjWDFJc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everybody uh thanks for coming my name
is steve hoffman and joining me is rick
fast we worked on a micro servers
project at orbitz together it's what
we're going to talk about today I'm
currently with Ray's calm and Rick is
now with expedia both great Chicago
companies both hiring check it out if
you're in the market so orbits the
travel website started back in two
thousand by the five major carriers as a
one-stop shop for booking travel online
we're going to do a quick architecture
overview history to kind of explain how
we went from sort of monolithic
architecture to services and then
finally to Microsoft's ins knocker a
Microsoft stock isn't complete unless
you talk about automated pipelines
otherwise you're going to be in a world
of pain and hopefully we'll have some
time at the end for questions all right
so like I said orbits started back in
two thousand by the major carriers the
architecture was pretty standard of the
day you had your web layer that served
up your web you know HTTP content talk
to a business layer which reading here
with the carriers the website launched
it was basically a single application we
did on demand releases by hand and we
were off and running and basically what
had happened was we had slapped the web
facade on top of the system you see in
this picture there's actually a bunch of
these are called GDS Azure global
distribution systems these are those
terminals you see the tellers typing in
at the at the airport that you don't
know what they're doing a lot of cryptic
commands all those paper tickets they've
been replaced those boxes of paper
tickets they've been replaced by
electronic tickets but effectively the
systems are still the same and so when
we're talking about talking to the
airlines to the business layer we're
really talking about communicating to a
GDS the first one that orbits hooked up
to in 2000 was called world span still
around today and in the 2003 timeframe
we hooked up to our second GDS which was
saber
now American Airlines actually uses
Sabre so if you booked an american
airline ticket and we booked it and
savor it actually saved the airline
money and so that was the economics of
it american airline around the same time
decided they wanted a consumer web site
so we were happy to provide the
conductivity Northwest also got into the
game and we provided connectivity for
them as well and pretty soon we saw this
architecture wasn't going to scale right
so in the 2004 time frame we switched to
a services model around a now-defunct
Java technology called genie and the
idea was that each of these services
would evolve independently we'd have
different release cycles and everything
would be great and we kind of went along
this line for a while we hooked up more
backends like Amadeus Northwest merged
with another carrier American hooked up
to Sabre directly so they went away
orbitz bought more brands like ebookers
and cheap tickets American Express
needed connectivity not going to show
you everything a lot happened over the
15 16 years of orbits history but
generally the idea is that your your
system is always evolving and so you
need to evolve with it so fast forward
to 2015 when we started the micro
services experiment orbitz had multiple
brands both websites and web services so
for instance if your familia they used
xml web service that we provide it to
them we connected to multiple backends
the entire system was comprised of over
500 separate applications running across
thousands of instances in multiple data
centers and deployments were more or
less daily and then of course at the end
of last year expedia came in and bought
the lot so as you can imagine
applications that evolve organically
over a decade tend to accrue a little
bit of process in 2010 orbitz was doing
two releases a year you can imagine how
well those releases went I'll give you a
hint as bad by 2012 which is when this
picture was taken it was about 18 days
from code to production and what's this
picture was trying to
what they were trying to map out was all
the steps necessary to get from code to
production and a lot of these were very
very manual steps and so a lot of time
was spent trying to eliminate as many
manual steps as possible they got it
down to about 1 to 4 days you also saw
Conway's law at play this is where
development had their tool which was a
tool called glue for deploying job
applications operations used CF engine
and the later chef and this kind of
mismatch between the two llege sort of
caused people to not create more
services so even though we have a
service-oriented architecture the pain
involved was just I guess more than they
could bear and they just want more and
more code in and so 15 years later we've
come full circle instead of having one
monolithic application we now had lots
and lots of monolithic services so one
of these services Rick was working on I
came over to my destiny said hey I've
got this big application I want to break
up and I want to do a sort of 12 factor
microservice architecture and I want to
use docker can we run docker so I don't
know let's see if we can figure it out
and so when we when we took this on we
decided well let's let's see if we can
shave off all that human process right
let's find a way so that at some point
human involvement stops and then it's
just robots after so we started with
this box that we called the Rick bot
right we already had chef in place and
the thing the theory was that we would
use chef to deploy doctor and then we
launched our applications on top of this
but if you've ever used docker hopefully
you have it comes with certain
challenges for instance if all three of
these were the same service and they all
listen that particular port they're
going to they're going to clash right
and so doctor has this network ad
network translation layer so that
everybody gets a random port and so each
of these services finding each other
becomes a problem you also have the
problem of like logs right if your
application is like logging the disk
it's actually logging inside the doctor
container where nobody can get at it so
we needed to solve some of these
problems
so the first thing we added to this
picture was a this box here if you're
not familiar with this is console it's a
eventually consistent service registry
from hasha Corp we run this locally on
every server that we run dr images on
and the applications basically register
and find other services via console and
when we launch the applications we tell
each application what its IP of the
server it's on as well as its external
port in this case it's 3100 to so that
when it registers with console sorry so
when it registers with console um
everything can be found right to deal
with the logging problem we had just
converted over to using an elk stack
elasticsearch with cabana and so what we
did was we dropped a log stash agent on
every box so that we basically bypassed
the default logging and we use the
network lager again because we knew the
IP of the machine we were on we would
just use a well-known port that we ran
the log stash agent on and the chef
configured log stash would forward it to
the appropriate out cluster and then of
course uh you got to have metrics orbits
you may or may not know but orbits
open-source graphite back in two
thousand eight we have a lot of graphs
and so we filed the same pattern what we
did was we dropped a local agent on the
box there wasn't a relay for this Rick
Rhoden it's basically a little go relay
called mr. pickles that is configured by
shaft to point to the appropriate
graphite cluster and again because we
knew every docker container knew its IP
it would basically connect to a
well-known port and then chef would
configure mr. pickles to forward it to
the appropriate destination okay so this
works great but you know if you were at
sams talk or anything before you know
that you know you're gonna have lots and
lots of these boxes you're not going to
like SSH all over the place and type dr.
run this docker run that what you really
want is you want something to manage
your doctor containers across lots of
machines
luckily around the same time that we
came to this conclusion mezzos had just
announced support first class support
for dr. and so this is a this is a
picture from the the memphis website of
their architecture you basically run you
basically have a master or hopefully
multiple masters that go through a
zookeeper election and then there's a
bunch of slave notes to calm agents now
and on top of that you run a scheduler
which then launches applications on the
particular mezclas agents in this case
we're really talking about using the
marathon scheduler so our picture looks
something like this we run the marathon
schedule it provides a UI and an API for
us to communicate with we find the
master the elected master through
zookeeper and it manages the slaves and
this is where we'll launch doctor so our
picture now looks something more like
this every one of these rick Bob boxes
becomes a meses agent and we use
marathon as the mechanism by which we
communicate through to then launch the
doctor containers on our behalf now
there are some applications where
they're not console aware and so they
have to be discoverable via a name and a
port and for that we used an open source
project called bamboo which communicates
with marathon to basically rewrite its
routing rules so we have a static farm
of a tree proxy boxes behind a load
bouncer and that's how the routing to
the services happens at the edges um and
so that's basically the setup right but
the the real magic really happens once
you start stitching this environment
along with other environments together
in an automated pipeline for that we
tried to use Jenkins and hopefully with
this little human involvement as
possible risk going to walk us through
some of that hello
so we had a pretty good use case for
microservices and that was in our SEO
landing pages on orbitz they used to
look like this they actually don't look
like this anymore they're now powered by
expedia but for the case therefore this
use case or this case study we're going
to talk about it from orbitz about a
year ago and this is an orbitz landing
page ideally if you google chicago
hotels this is close to the top we
strive to get it as close to the top as
we can if you google orbit chicago
hotels it will be at the top and these
pages are comprised of what we call
modules and these modules on the front
end or just these little squares of
content and they come from different
sources on the back end and the
applications they come from on the back
end is this large monolithic service
that I'm going to talk about and I'll
talk about a specific module in this
talk which is the editorial module in
the editorial module represents business
user contributed content copy and images
and it's it's retrieved from our CMS
system on the back end which was powered
by solar so our architecture looked kind
of like this the actual orbits website
on the front and then our content
orchestration service on the back end we
call it flex but for the for this talk
will is called the content orchestration
service and so it's comprised of all
these modules and all these modules are
are basically implementations of a Java
interface that will retrieve the data
that's merged with the templates on the
front end and so they're all kind of
generic this is a like a dynamic page
building system basically and
unfortunately though it crosses lots of
the lines of business that we support it
orbits so hotels search content air all
that stuff cars and so you've got a lot
of teams because we're we're kind of
organized based on mostly verticals and
so there's many data sources there's
many teams contributing and this diagram
only shows three but there's actually
more like 40 modules and probably like
10 teams contributing and so you can
imagine the types
problems we have multiple teams stepping
on each other's toes dependency hell
there's a major lack of resiliency with
everybody living in the same jvm it's
like you know somebody can overuse a
thread pool like you know use a bunch of
resources and basically ruin it for
everybody and so when you have all these
teams contributing it's easy for
somebody to take down the whole stack we
looked at the things like osgi and
modulation inside the JVM but none of
them really seemed to scale well and
especially weren't dev friendly and also
the worst part was one release cadence
so the typical larger service at orbitz
you know the deployment had to be
scheduled and typically it was you know
weekly some applications were you know
on the order of days but each release
typically contain a lot of commits from
a lot of teams and so what we decided to
do as you can imagine is we split the
modules out into separately deployable
microservices and the applications we
decided upfront that we're going to you
know rewrite our platforms so that
everything out of the box can kind of
support the 12 factor principles which
we could probably do a whole other talk
on but really what this means is simple
configuration stateless delivering your
schema as code with your applications
quick startup graceful shutdown things
like that and all these services are
just using HTTP with JSON or protocol
buffers but they're separate builds
their separate code repositories and
even separate technologies that they
needed to be and so the teams end up
looking more like this so the hotel team
has their module the search team has
their module etc and this allows the
teams to manage their own deployments
and releases and really it gives them
everybody kind of resiliency or like the
whole system can't come down from when
these guys because the content
orchestration service is now just making
API calls that these things and we're
using circuit breakers and timeouts to
ensure that one module can't affect the
page so you know when a module goes down
you might be on a landing page and you
might miss some copy or like an image or
something like that but overall the page
survived
and so the other nice thing about this
architecture was we decided to peel off
the back of this large service and so
the rest of the company doesn't know
about this you know we did and we didn't
have to boil the ocean to like rebuild
this we can kind of just peel one module
off at a time and so it was completely
transparent to everyone else anyway
before we decided to go continuous
delivery we had a process that looked
kind of like this and so our builds were
mostly triggered by check-in but some
were on demand only but we the worst
part was really that we built the same
code numerous times and so we call them
betas you might call them snapshots and
this makes sense for like libraries and
stuff but when you're talking about your
services you know we really want every
every build to be it doesn't necessarily
have to go to production but we want it
to be able to if it wanted to or if we
wanted it to and so the way we were
doing it was we were building testing
discarding building testing discarded
and then it's Tuesday and we want to
release so we call it 12 and then we
build it and we test it and we deploy it
and at this point it's got 20 commits in
it and if something is broken it's a
little harder to know what's changed and
so in the new world we want every bill
to be a release candidate which is why
we implemented continuous delivery with
Jenkins docker ansible and marathon but
first we want people to create new apps
when appropriate we don't want people to
keep tagging on things that don't fit on
existing services which is a problem
that we had so we wanted to cheapen the
or make it cheaper to create new
services and so this is the second time
we've seen this picture today actually
we used yeoman it's a it's a JavaScript
library for scaffolding projects
basically you just create a bunch of
templates it's usually used for you know
things more in the front end ecosystem
but it works fine for us we were
building Java Gradle projects and so it
basically would give us a skeleton
project with all of our kind of chassis
libraries baked in as dependencies so it
also would do things like prompt you
like
did you want to connect the solar did
you want to connect to a certain
database and you could basically
pre-configure some of your wirings in
and then the code base is obviously
generated and then we push it to a git
repo and at orbitz we use stash for that
and one of the nice things about stash
was that we could build in kind of a
gate at the beginning and we didn't want
gates but we needed something to you
know so we weren't just winging stuff
into production willy-nilly and so we do
that through the pull request and we use
a plug-in called the work zone plugin
for stash which basically protects
master and it makes it so we can't just
merge our code means we have to make
pull requests with a minimum number of
reviewers so the committer here he's
made the pull request and you can see he
can't merge and the reviewer can't merge
either but he can review the code and
approve it and then once he does that we
have bought that can actually merge and
so he's the only guy that can get our
stuff to master and then once that
happens the Jenkins stash commit hook is
triggered and then that's the last thing
that humans do in our pipeline unless we
explicitly configure a button later on
but I'll talk about that a little bit so
the stash commit hook triggers the
Jenkins pipeline and our first cut at
the pipeline had four step types and
they were just Jenkins jobs built using
the job dsl plug-in which kind of a way
to template out Jenkins job so that
you're not like creating them through a
UI which obviously wasn't going to scale
for the number of services we were
building and we had four step types
there was the build and push which would
actually compile and build your artifact
and push it to some artifact repo we
have the deployment step acceptance
tests and then our paper trail which is
change ticket creation and updates and
I'll talk about all these just a second
and this is a simplified view of what
our pipelines would look like we had
over like 20 environments we did a lot
of stuff in parallel we had lots of
partners that required us to be in
separate environments for example like
an American Express stuff like that they
needed their own stack so we ended up
with a lot of environments and so we
wanted our pipeline to be able to do
that fast and also in parallel
and so ultimately we build the
application the application name in the
version are really the only two pieces
of information the rest of the pipeline
needs so we'll talk about the steps now
the Jenkins pipeline build job pulls the
latest for master builds and runs our
unit test in our case most of these
services were springwood applications
built with Gradle and so our first use
case was let's just support Gradle and
so we do the Gradle build our Gradle
configuration file says hey this is
version 1 dot too now we say it's one
dot too but that's not what we're
actually going to publish we use the
Jenkins build number two curly excuse me
to create a unique release candidate
version right so we built the jar and
then we interpolate that value which is
say this is the 17th build so now we
have this unique version number and it's
a nice-looking version number that so we
know what order it was deployed and so
we know that 12 to 18 is a newer version
you'll see a lot of people using things
like get commit hashes which simplifies
this process but it also makes it so
you're like you don't know if it's a
newer version or older version anyway so
normally we would take our war file or
jar file and push it the Nexus or our
factory or one of those types of
artifact repos but we do what we do next
actually is we build a docker image and
so we use the Gradle integration with
docker to do that in retrospect after we
started having different types of builds
I wish we hadn't but we did but
essentially we had a very simple docker
file which used our base orbits Java 8
image and then we'd basically would copy
this newly-built artifact into the image
set the working directory and then set a
simple entry point and so now we have a
docker image and we tag it with our
unique release version so 1 2 dot 17
that's our docker image tag and it's
ready to get pushed we used to push them
to docker hub but we actually owned
artifactory and it has actually very
nice docker registry and so we push our
versioned artifact here and so the rest
of the pipeline is going to pull this
image by this tag
from artifactory and so the rest of the
pipeline can access it deploy step this
changed significantly as we worked on it
and our first shot was really let's just
use docker that's what we just wanted to
deploy with docker and Java Java
deployments have been kind of pain in my
ass for my career and so this was nice
it was nice that I could just push him
out there and it was pretty reliable in
a very simple deployment we used a tool
called ansible which is a configuration
management tool that's good for rolling
across toasts and executing remote
commands it has a nice docker module and
so we use that first and what this did
you can see here in this picture is
relying on like a fixed inventory of
hosts it rolls through shuts down the
old version launches our new version in
this case 1 2 dot 17 waits for the
health endpoint to come online we'll
talk about that in a minute and then
once everything's good we shut down the
old one we move to the next host we're
doing this in serial kind of took a
while but I was happy we we did a docker
deployment but there was a ton of
problems and we knew there were going to
be problems and we you know we felt them
soon failures are kind of weird you know
part of the part of a single environment
could fail the whole environment could
fail do you roll back just that host you
roll back the whole environment you
tried you try to retry we're doing this
llamo files just recently I think they
added kind of try-catch semantics in
ansible but ultimately it was a bunch of
logic that we didn't really want to own
and also we're just running docker I'm
Sam mentioned this earlier with things
like docker swarm it's not fully
supervising your containers after they
run so we're deploying these things in a
cloud environment where guys will shut
down VMs because they have to move them
to another physical host when docker
comes back up it doesn't know what it
was running because it's a new docker
and so we noticed like Microsystems just
disappearing here and there also we're
deploying to a fixed number of hosts and
so once we fill those hosts up
we have to create another pool and that
would make things a lot more complicated
and so you know the clear answer here is
we need to use the scheduler and so we
decided to try something new we stuck
with ansible because I think it's a nice
tool for doing deployments we built our
own ansible module that would do a
marathon deploy and I think and this was
this was a couple years ago I think
nowadays there's probably a one open
source one available for ansible I know
there's one for terraform but
essentially we can just give it the
image name with our release version
number of instances and you know some
configuration and then marathon can
handle all the complexity for us and so
this is my picture of the Rick bot
server image that Steve showed earlier
and we just added one new kind of local
helper service which is the mezzos agent
and so this allows us to participate in
the mezzos cluster and allows us to use
marathon to do deployments so same deal
our Jenkins deploy job pulls the ansible
playbook but this time the playbook
doesn't care about a rolling set of how
or a set of hosts it just cares about
where marathon is and so marathons just
an HTTP API that we can interact with
and if you think about it is if our
whole cluster of mezzo cents as a single
computer you can think of marathon is in
it d and it's just a simple HTTP API and
so our playbook really just calls that
with a configuration one thing to note
though is that marathon is asynchronous
so when you when you send it JSON it
says okay I got it thanks and it returns
and so we have to manage that our self
otherwise our pipeline is going to end
before the first environment even
finished deploying or failing so we end
up with something like this this is
weird HTTP soup pseudo code but
essentially we our playbook checks to
see if it exists the application itself
if it doesn't then we do a post with a
JSON document specifying the image
configuration things like that mostly
docker related if it does exist then we
do a put pretty much with the same
configuration
and then we get a deployment ID back and
then we have to pull this deployment
endpoint until that disappears and
that's how we know that it was
successful I wish there was a better way
but at least then there wasn't and then
we basically set a time out so if it
takes longer than two minutes then it
probably failed and so the ansible
module itself will then return but
basically we use the ansible module to
make it seem blocking to the rest of the
pipeline so that the pipeline doesn't
keep going one other piece of our
marathon configuration that I think is
important is we have different types of
a configuration that we deal with it
work a lot of the configuration that we
used to deal with it orbits like our
metrics are logging service discovery
stuff like that with stuff we had to
tell each application through
configuration files that's baked into
the infrastructure now and so chef
manages that for us when it stands up
our Rick bot agents right but there are
things that individual applications care
about like their databases or you know
some other value right things that our
environment specific and
application-specific we allow developers
to specify in a file in their source
repo with the application and those are
injected when we do the deployment as
environment variables and we have a
different way of managing secrets which
I'm not going to get to today
unfortunately so after we make the calls
to marathon it interacts with mezzos
getting resource offers to find out
where to place our stuff and only had
room for three servers here so it's
pretty obvious where they're going to
get placed but marathon starts up the
new version right so we've got the
docker image at 12 at 16 running in blue
12 17 running in green it does it in
parallel this time which makes our
deployments a lot faster which is nice
it does require you to have extra
capacity to support having two of
everything for a short period during the
deployment all of our applications have
health endpoints probably if you're
using spring boudoir drop wizard or
something in this point you probably do
too that's a first class thing in
marathon and so when we post our JSON or
put our JSON a marathon we tell it about
our health check endpoints and it
continues watching those during the
deployment and even afterwards and so
of our guys are okay we're at 200 and so
then we gracefully shut down in destroy
our old version and the graceful part is
the important part here the first time
we started playing with this we were
just killing stuff and you know we were
getting failures because connections
couldn't drain and more importantly
using service discovery like console
we're registering and D registering
application instances and when we kill
them they don't get a chance to do that
and so that was a mess but once we once
we got a grip on our signals works
pretty well and so that was one
environment and now we're next off to
the next environment so 12 17 is
deployed here and we'll just keep going
through our pipeline and doing the same
thing but the nice thing about marathon
and this is really the key for any
scheduler Kuber Nettie's will do this
for you I guess the newer versions of
docker swarm will do this for you ecs
and marathon they're continually pulling
your configuration so they know how many
instances of everything you need they
know the health check in points and so
they'll keep track of things single
instance goes bad disappears or if the
health checks returning like a 503 or
something it knows that it needs to
start up a new one and it'll do that for
you also in this case if the whole node
goes down then marathons still watching
and this is kind of a dumb case because
it's going to have to co-locate them
because I've only got two squares left
but it's going to go out talk to mezzos
again and find a new place to put the
resource and it found a place we check
the health in check excuse me health
endpoint again we're 200 and we're back
at capacity and that's huge I mean
that's that's the real it's the reason
to use a scheduler I think I mean the
API is nice it makes it easy to to write
resilient systems the next step type is
kind of a more loose step type as our
smoke and acceptance testing step teams
did this differently depending on the
type of application it was we always
encourage people to do unit testing kind
of embedded if the build time as much as
you could to get the coverage there but
as you go through the pipeline you might
do different types of functional testing
or black box testing a lot of teams
would actually use our service discovery
mechanism console to Joe
do like basic tests across a whole
cluster but not testing like really deep
functionality but just you know is this
thing working and a lot of teams now
will even do a canary deploy to
production run tests against that and
then once that's good then they'll
deploy the actual application that's
taking real traffic afterwards but we
made that configurable and then finally
is the paper trail all of our
deployments the scheduled ones legacy
deployments and these deployments
require change tickets in a product
called service now I'm sure some of you
have used it it's a ticketing system
it's very configurable we configure it
it heavily but essentially these these
steps we booked in our production
deployments with and what they do is
they pull information from JIRA and
stash to create all the information
about this ticket and they create a
ticket service now if the marathon
deployment fails then we close it market
has failed and if it's successful we
close it successfully and the nice thing
here is that it's completely transparent
to our release managers and two
operations and so when you put it all
together you get something like this and
you know after this pipelines run our
new version of the editorial modules in
production all of our paperwork's commit
or complete and we're ready for the next
commit and so if you build it they will
come we built this for a small part of
the orbits website but once we started
delivering things to production in 10
minutes other people wanted to start
doing this and so we had to scale it out
to other people not just for other
applications but we quickly found that
managing Jenkins jobs is unwieldy even
if we're using the jobs TSL stuff we're
changing pipelines a lot teams want to
reconfigure their pipelines and we found
that you know managing them as jobs was
hard and we couldn't represent that as
code until we started using a plug-in
called the workflow plugin which i
believe is now called the pipeline
plugin with jenkins 2 and what that
allowed us to do was configure jenkins
jobs using a groovy dsl and you can see
an example here is specifying a Jenkins
job that pulls the editorial module runs
the Gradle bill
old and then it has the sport 4dr which
is building our dynamically versioned
docker image and then you can see here
that we're pulling our playbook and
running our ansible playbook to do the
deploy this is still a static pipeline
and we didn't want everybody to have to
specify this either because we don't
want you know 50 to 100 teams with
microservices having to like write
groovy code to manage their pipelines
and so we took it a step further and we
said we want something like this so
think about like a Travis build or
something like that having a nice little
document that specifies your build in it
even in our case specifies our build and
deployment and other types of steps to
run in cereal and in parallel at certain
points and so we have this pipeline JSON
file that teams automatically have at
the root of their generated microservice
code and it specifies things at a higher
level so here we're saying Gradle build
which is the build step I talked about
and then you can see below marathon
deploy into the test environment we also
allowed them to specify things like
environment variables in this file so
they could kind of configure everything
about their application deployment and
configuration all in one place and so
then our workflow became a lot more
complicated but there was only one of
them and it was kind of managed
centrally so that we could kind of
standardized on the way the different
type of steps worked and so this was
nice it just became a little grosser
looking in the fact that we had to parse
in the JSON and then iterate through the
steps and then dynamically call these
Jenkins jobs but ultimately allowed the
teams to deploy whatever environments
they wanted and if they wanted to create
a new step type we would just have to
add a little snippet to the central
pipeline and so lots of teams started
getting on board and we started going a
little more polyglot than we had
expected and so people wanted to start
building Python apps and you know go
apps and stuff like that and so we
decided we needed to even take our
Jenkins set up a little further which
steve is going to talk about right now
so as you can imagine just like our code
are Jenkins environment had also sort of
evolved or devolved over time we had
sort of a you know standard setup where
you set up Jenkins you set up a bunch of
slaves and then over time you start
adding version after version of all the
various tools so you'll have you know
six versions of this and ten versions of
that and then you have compatibility
issues and it gets pretty messy pretty
fast not to mention the building of our
monolithic application was was so
destructive to the slave that they had
built some custom like take a snapshot
of the Jenkins slave run the build and
then run this script to take it back to
its previous state it was horrible and
so it looks something like this you
check out the you know you check your
code in it billed to a slave and then
you run your little time machine script
this was terrible because it meant that
every slave could only be used for one
thing right so it didn't matter how big
or small your build was you got the same
thing and so we wanted more flexibility
luckily someone had done some nice
Jenkins meses integration via this
plugin and so the setup became something
more along the lines of this where when
you did your commit it would instead of
just scheduling it straight to a slave
it would communicate with mezzos to
spend up a dynamic slave which was
actually a doctor image in a lot of ways
a kind of Jenkins microservice and so we
had a specific in a constant set up for
mezzo sleeves that were for doing
Jenkins builds like you see here this
env and the bill just proceeded as
normal and then when we were done
basically we could just knew the slave
and we still we got the same thing with
sort of single-use slaves and so we were
able to kind of build upon this so when
it came to doing a deployment we created
another kind of slave which would check
out a playbook and run it against this
particular Jenkins slave that had
ansible and the marathon libraries and
everything installed it would
communicate with marathon it would then
deploy to a separate mezzos slave in
this case the CIA environment and this
was all done via meses constraints and
parameters marathon and again when it
was done we would just nuke that slave
and the capacity is ready for the next
thing once you do that you start
basically scaling out your meses
infrastructure across network
segmentation lines or access control in
this case we got three pre production
environments we got a little Jenkins
environment off on the side and that
kept our builds from you know trashing
other environments we could also have
different access controls for production
and things like that and then you keep
scaling it out even further across
multiple multiple data centers or
availability zones and jenkins then
becomes this orchestrator across all
these different lines and so hopefully
to wrap up hopefully you see that we
haven't really talked a lot about like
the micro service aspect of it but
really more around the two llege that's
really around it right we learned over a
very long period of time that most of
the time you spend in dealing with lots
of applications across lots of machines
is the process and anything that you can
do up front to streamline it you know
figure out what process you want and
work backwards from that and investing
that time early on will will pay for
itself believe me you want to have you
know repeatable builds repeatable
deployments you don't want to be part of
it trust me and you know if you're
successful successful with your one app
it's going to take a while to get the
first app going but if you're successful
you have lots of apps and then you know
it won't be any that much more work for
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>