<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • Big Data: Challenges &amp; Opportunities • Roberto V. Zicari | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • Big Data: Challenges &amp; Opportunities • Roberto V. Zicari - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • Big Data: Challenges &amp; Opportunities • Roberto V. Zicari</b></h2><h5 class="post__date">2013-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aeNkhUu9j7g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the title of the talk is a big data
challenges and opportunities as a matter
of fact I will tell you three stories
and then you'll decide which story
you're like better so you know this is
big data I think we've been talking
about the data you know in the
conference already data comes from all
kinds of sources you know digital
pictures or videos or post to social
media and things like that and there's a
lot of them so there's a lot of data
coming up every day so I'll start with
the first story so what I would like to
do is to tell you the story our it is
told by the business perspective first
and then we'll have a little minute to
review it if you look at what the media
says big data is becoming a big thing so
if you think of McKinsey has a report
big data is a new frontier for
innovation competition productivity the
European Commission is saying data is a
new gold and they have an initiative for
opening up the data public data in in
the public sector so what do they do
them well first of all there's no even a
definition what Big Data is and I found
one that I found it very interesting
because it's very nice Big Data is
anything that is big enough so that the
existing software tools are now able to
handle it so namely the data bases or
whatever so that's a very nice
definition because as a matter of fact
you can use it as time goes so data will
grow the interesting thing is now the
data is fast produce and it's a lot of
data but it would be more there and
there will be more data that we more
tools and it's like here we come into a
situation where data is like data
science so the scientist knows that for
a long time because they have a lot of
data science and space but now the the
industry is experiencing all the amount
of data that didn't have it before
and the Kachina
so this that finish is quite good
because it's not defined in terms of the
size of the data the data sets will
increase in the future anyway and also
it varies about the sector's so it's
different you know it can range from
terabytes which is not big data for some
definition to pay the bias on something
like that so there is everywhere so
that's what McKinsey figure it out it's
not much really of a discovery here and
it's important for for what so what is
Big Data supposed to do well it's
supposed to create value at least what
McKinsey is is telling us so let that
review what is creating transparencies
discovery needs export variability into
performance segmenting customers so you
can target people better replacing
support human decision and making things
automatic and not manually anymore and
find a new business model that's what
they say this is one of the example of
how they you know they claim that this
could be the basis for competition
growth and individual firm and there's
an example here is a particularly tailor
that in a mention of the name that
because they dig into the data with new
analytics will actually improve the
operating margin of 60% now the question
is how do they figure it out sixty
percent now this is the kind of report
that the business is doing to define the
opportunity and then they figure it out
how do I measure that and they say well
I should consider only those action that
are related to the data and if in else
is not a 60 percent Somali quoting what
they saying at the moment but I think is
interesting they identify a few sectors
that are promising health care is one
public sector ministration is one but
there's a cash there because there is
not any incentive to publish data so the
data is typically not digital and they
need to be transformed and make it open
and unless there's there's a push for
the
they're not gonna do it global personal
location data well you know about it
there's a lot of risk because a lot of
data personal did all over the place
retail and manufacturing these are the
the errors with McKinsey has identified
the promising sectors McKinsey figure it
out the one the shortage and the
limitation about that is that there are
not enough talent people around
typically you need people with a lot of
expertise typically with the PhD in
computer science maybe in analytics in
data mining learning machine learning
and they're not too many around and that
could become a limiting factor so that's
what the business telling us they also
identify what they call issues I'll go
more technical on there but I just go
very quickly home on that part they call
about data policies which is typically
not the technical part you know how do I
store and compute the data once I have
them with analytics and you kind of at
Olympics they call it technology and
techniques
I like the techniques work because it
sounds very old-fashioned but it has to
do with privacy security intellectual
property liability access to data
because data is from different sources
and industry structure that's the
interesting thing that not all sectors
are the same an example here is a
there's no pressure in the public sector
so whereby in another sector they might
be driving by profit that's not the case
in the public sector although there's a
lot of potential there so typically if
you look at the challenges in big data
you can classify them in three
categories data process and management I
just went through a list I didn't want
to put them into the category and I go
quickly on that first of all is a volume
because the difficulty is out to deal
with the size of this data these are
some numbers that you can read
I was interesting there was a gentleman
from
before Twitter generate seven and more
terabytes of data every day and Facebook
ten terabyte vida so you can imagine not
to mention for example intelligent
sensors there's a something called the
Internet of Things the future way of
using internet with sensor intelligent
will drive and send you all kinds of
data all over the place so then all of a
sudden you will have a flood of data the
variety is actually an important factor
because you have a multi cyclical types
so not only you have structured data you
have semi structured data like and you
have raw data typically the the softer
tools that we have are kind of
specializing of the relation database
work very well with structured data with
text they don't work at all and we
rotate that they don't even know what it
is try to store pictures in one database
and you'll see that so the other
challenges they're not really technical
but it's important if you are interested
in your business on your development in
working with big data is data
availability is there data available and
very interesting a good process will
typically make a bad decision if the
data is not good so and that gives to
the data quality how good is the data
and abroad is a coverage you know if you
have a data that is indicating a disease
in certain location and some of the
location have bad quality data and in
some other location is quite good then
you may end up actually doing the bad
analytics because then you are
predicting something that is not based
on on consistent data that's a very
important applicability you see as an
example that a colleague of mine has
made about you know the flu and that's
really has to do with the better quality
velocity we would you know there was
exactly the presentation before talking
about string computing so it's clearly
an issue but I think the the most
important issue that is not much
understood is what they call the wrath
city so again I will cope with
uncertainty in position missing
information or even misstatement on
truth in data that's a lot of our issues
here and there a scholar is a is a big
challenge you know how to find high
quality data out there on the web the
people at Google they're working hard on
their you know this there's a group of
people that work on Google tables and
that's exactly their challenge no data
discovery once they have the data they
can aggregate it and present it with
some kind of visual so again determine
the quality of the data sets and their
relevance to the particular issue is
quite difficult and these are all open
questions so these are all areas of work
both in research and development that
should be addressed and combining
multiple data sets because that's
actually the difficult part again I'm
I'm going for the list data
comprehensiveness so are they aired with
our coverage an example let's assume it
you would like to analyze data it has to
do with geography there should be no
problem except that you may end up in an
area where you are not supposed to
gather data so how come you're supposed
to gather data why I'm not supposed to
gather data because maybe there's a
security issue there's a military area
and the data is not to be disclosed you
see these are some of the things that
are making things quite difficult when
you are trying to use the data for some
meaning and obviously personal
information the interesting thing is we
tend to be very concerned about our
privacy and what people do with our data
in the real world but when we are in the
digital world somehow is amazing our we
become very relaxed I have three kids
myself we are actually no more kids but
there are three teenagers who know even
their 21 19 18
Ricola grown-up girls okay and well
that's a generation where they use
Facebook and all kinds of stuff and and
that's life you know that's part of
their life not part of my life it fought
all their life so at the end of the day
you have a lot of personal information
in the net no matter whether the the
security of Facebook is restricting them
or not so it is very important that
actors you need to kind of sit on the
same table and try to define what is
reasonable in terms of some regulation
what does it mean actually a realistic
reconsideration what privacy really
means okay I think Facebook has kind of
I change it so interesting enough that
just you know this is like a developer
conference I think it is require
important here we shouldn't become data
dogmatism so yes there is important but
it will be like if you're trying to go
to a doctor and rather than
understanding what you have now the
doctors I have a computer you check you
out and then they give you a receipt or
I will tell you what kind of disease you
have I think it data is only one part of
the story and and there should be
something more about there even if you
analyze the data as much as you can so
that's you know there's a lot of issues
they have to do with the data and lesson
about the technology so the other
discoveries has to do with tools but
also is a very complicated ecosystem
with many actors have to play properly
the process well there's a lot of
challenges for example to capture the
data have to combine them together
especially when two objects have the
same kind of meaning but the different
kind of format transforming the data
into a way that it can be analyzed and
and and you need to apply rather
sophisticated technique and then apply
and modeling so that it makes sense to
work on the data and then interesting
enough which is very important once you
have a result
very few people will understand the
result so let's face it unless you have
a visualization that clearly shows it
then very few people outside the domain
expert will understand what you discover
so that's also there's a there's a lot
of work that needs to be done in in the
visualization of the results on a
management part we're talking about data
privacy security governance so but this
is not really new it is only that the
scale is so big and then everybody using
it because if you think this is a quote
from a colleague of mine Misha blog very
famous in in the database world in the
modeling and he's talking about data
warehouse this has nothing to do with
the new kind of a dupe things but at the
end of the day the two issues are legal
and ethical so it's only that we have
more data and data as part of our life
so it's becoming more and more like a
daily business so I'm taking a second
together to review this story that's the
story of the business telling us so my
my point here is is all about intention
what is the intention that we are trying
to dig into the data so your my
intention is good
it would be good my intention is not
good it would be bad and this is not
told so nobody will ever tell you that
they will tell you that no big data
analytics will increase productivity
innovation margin targeting people and
all con cancer stuff but at the end of
the day you can misuse the data in a way
or it is not actually sad so certain
things will not be said but it will be
done so you know that's that's all about
business I wanted to mention to you I
think he's in perform it was important
to mention it because you know the rest
of the presentation is more on a
technology challenge and that's the
second story and then I have a first
story the delight to tell you so these
are some of the example of what you can
do by analyzing data log analytics
everybody is talking about it that's why
the gentleman but with Twitter has
develop is lambda e Thatcher in the
presentation before because I can you
know do a lot of counts of computation
that fraud detection because you can
start looking at patterns in the raw
data and figure it out who is actually
for example using a credit card in
proper way social media and sentiment
analysis are important sentimentality
means I go on the web I look all the
tweets and I try to figure it out by the
content if there's a trend like you know
politically who's gonna win the next
election in United States
can I just by looking at the trace on
the net figure it out that that's called
sentiment analysis okay that's uh
there's a lot of research on there and
there's a lot of interest it's all about
try to predict or try to understand
what's going on with modeling and
management and energy sector energy
sector is quite interesting because for
example in Germany does well first of
all there's a European regulation that
says that in a certain amount of years
we will have intelligent meters in the
house only if in the world is
intelligent meters but you know the the
gas and electricity meter you have now
it will become intelligent because it
will understand how you use the data and
they'll send this data information to
some place which is the government and
the government will have all this data
and they will figure in or something
you see that's an example how you do
things with big data they could for
example figure it out that you shouldn't
do this thing or that thing
maybe when widgets act is definitely an
interesting area well yes sensor them I
had the pleasure to talk to a few people
about this topic so I quote a few people
one of them is Florian bass from EMC
green plan what he is saying is
basically say that if you have a lot of
data you know you need to put the
analytics to the data and not the data
to the to the procedure
which is basically what it was done
typically in the world relation database
world he also sent something interesting
that with terabytes there's no problem
you have a databases out there that are
handling terabytes did warehouse with
terabytes no problem when it goes to
petabytes then this gets very
complicated the cache about this
assumption is who really has a petabyte
problem in my in the enterprise now
because petabytes a lot of data so
anyway this is interesting because he
confirmed something that is called the
Grays Jim Gray was a scientist in
unfortunately no more here but it was
very famous in the devas world and he
started to work together with scientists
or trying to use database technology for
data science and he came up with some
laws for dead engineering and he said
take the analogy to the data that's
exactly what is happening now so the
issue is that now we are all kinds of
separate parts ETL business intelligence
reporting analytical different products
and it would be nice to have a seamless
integration of those it's not happy it's
not happening even with the new
technology and you'll see why so that's
a wish but in fact this is not really
happening one of the prominent that all
relational database technology has a lot
of problem with with this new big data
is that the underlying architecture
which is basically what my colleague
before also mentioned was not really
made for handling big data with
unstructured and then not formatted
parts so basically you have a technology
where the underlying model is not really
made for that and try to fix it it's
quite difficult that's the quote that is
me
so I well I asked a few people and I
also has
some people I think they have something
to say like they're not folders from
Amazon the CTO and said well that's what
he is quoting is basically saying that
the way they do analytics is different
it's like expiratory so in the old way
you knew exactly what to ask and you
have a schema and you were basically
getting something back now
they explore and figure it out what they
want of course it's also trying to push
their own cloud that's why they a couple
very much with cloud but anyway that's
exactly the same thing at the end of the
day you are exploring and by exploring
huge amount of data it takes time so and
that's actually the thing that in order
to explore the data you need the time to
do it so batch processing but that's
actually the issue here now batch
processing was very nice but when
business starts to think oh I could do
something useful with the data they want
in now so that's the funny thing you
know they're not happy with batch
processing they want real-time analytics
that's why now you have all this fade on
real tile analytical people Warnock
immediately to understand what the
patterns are of course this is actually
quite difficult because you will see
that the tools that we have are
typically batch processing and although
you can patch things together it's not
really obvious on top of the master the
data is semi structured or unstructured
whatever you want to call it and this
makes things quite complicated so how's
the situation at the moment if you you
know I just I'm a vendor neutral that's
a good thing about it or being a vendor
neutral I don't have to sell anything
so observe so typically when you want to
do big data analytics you have this
configuration you have a parallel
database or NoSQL data store kind of
connected within a dupe batch processing
okay so you can see all kinds of
variations all kinds of vendors open
they will tell
you know the solution is the best but at
the end of the day there are some
concerns about the performance of that
because you will have huge amount of
data store in the batch and when it
comes to the real time you have to put
into a peril database or NoSQL database
to do whatever they want so when it
comes to scalability the free experts
that typically are look at our data
volume we talk about a hardware size
which is no more a big issue because you
know hard with commodity and concurrency
we talk about in several this
presentation about they can't fear him
and eventual consistency all kinds of
stuff I had an interesting discussion
with bash or say I wouldn't have my bank
account eventually consistent updating
then we have a discussion on there so
and there's also a lot of interesting
things to hear so but let me just go
through the question is which and which
platform for analytics for big data you
need to understand that big data started
in two different words
I myself am from the data based
perspective but interesting thing is you
know big data in the database war
started in the eighties and and the key
was parallel databases unfortunately
Terral databases are neither necessarily
easy nor there is no even open source
implementation so the interesting thing
is in all these years that are actually
more than 20 the community didn't really
come up with an open source
implementation of a parallel database
but things like share nothing
architecture the clarity set oriented
queries and divide-and-conquer will
MapReduce stuff thing you have an Intel
data Tara data is the data base that is
actually you will see use in very
data-intensive applications
I have something about eBay and then
under sometimes the name new SQL which
is also not necessarily a nice name but
is basically reimplementation
relational databases where they
reemployment the entire internal parts
and then you have a no HP vertica IBM
the teaser atarid a task the data EMC
green plan the interesting thing you see
that you have always a big company /
another company they all been acquired
so Teradata acquires the data EMC apply
greenplum HP applied Vertica and of
course you know because all of these are
the technology that were like a new
implementation of relational databases
for large amount of data in parallel the
system world so the people that are used
to do system implementation networking
and and all that stuff they had a
problem and late in the nineties you
have you know the adieu p-- the
MapReduce you know you know of course
inspire and by the big web companies and
then you have the Google BigTable that
they use it you can't an Amazon dynamo
you can read the paper but you can't use
the system that is why you use a Pasha
dupe because that's open source so
that's the reason why so I'm taking some
concepts from my country collie Mischa
carry that is a quite famous database
guy to review the to stack the peril
database off the stack and then we
review the hardware software stack now
let's start from the bottom at the
bottom of of a relational database you
have a storage manager that is row or
column a record oriented and you can
basically structure your Storage Manager
like that then on top we have a
relational data for layer runs the query
plans and and understand about the local
storage manager and then the sheriff in
storage services and on top of the SQL
there's no other way to get into this
system as we asked
that's very important you can't break it
you know that's the thing you can't turn
it apart and decide to use only the
storage manager who I wish I'll use a
storage manager or you need to go
through the SQL that's actually the
problem is a monolithic thing and it's
not even easy to use because tell
databases and there's no open source
implementation so nobody can innovate
that's the thing is actually quite bad
if you look at the Hadoop software stack
it doesn't look good but it's completely
different because you start again from
the bottle you have this a dupe
distributed file system if you think
about it is a byte oriented file
abstraction files appears as a very
large contiguous and randomly
addressable sequence of bytes we don't
even understand that the boundary
there's no more like a record well you
normally you know you never break a
record in two parts you know here are
all bytes and then the system does not
understand that you know there's a
boundary that gives a record meaning so
you can naturally have parts of your
data that goes into other blocks on top
of that you have for example high base
there's a key value store and this can
be a less interesting thing that you
can't access that either directly by
client or via the adop Map Reduce so you
see is it's no more monolithic so you
can start
I was listening the presentation or
react or this morning that's a good
example they have a core and they can
just have it you know in the panel
database you can say I have a peril
database core and do what you want you
get the whole thing and so it doesn't
look nice but is actually no more
monolithic and the MapReduce and a dupe
is the only thing that allows you to
process a huge amount of data nowadays
so batch analytics you know and on top
of that the interesting thing that
people
that are very smart they realize that
not everybody saw smile and when you
start using a system that's quite
complicated they started to have a
higher level of obstruction and so in
the Hadoop ecosystem now you have high
level languages five Big Pig Latin J SQL
and these are high level languages
because it's easier to handle that in a
kind of I was a SQL style rather than
only programming in MapReduce so Misha
carry that is now a product guy I took
it on purpose it done that kind of nice
the pro and cons about Hadoop these are
the pros okay open source non monolithic
definite scale okay and and it has
support for replication and and you
don't have to have manual operation so
at the moment a dupe seems to be the
solution everybody's using it but I
think it is also looking and I think is
very important that we look at also
critically tell something that is not
necessarily a good thing
I slightly change the phrasing he was
much stronger I said questionable he
said something much negative rather we
say that the the points are first of all
it is questionable at least to have a
layer at the very bottom that has bytes
and then on top star having something
that looks like a declarative join okay
so because the level of abstraction goes
higher and then your bytes understand
and that's exactly the thing that you
have everything that is perform of
MapReduce okay and certainly if you try
to perform something like a joy with
MapReduce you can do that but it's not
necessarily done for that so and now
it's very interested in thing that I
think it was quite interesting is
somebody of you
the previous presentation lack of schema
the no SQL movement said well if I don't
have a schema more flexible true but my
know having a scheme I'm losing all the
information to them I have and in the
future it might become very difficult to
maintain those systems and Misha I carry
actually said that was actually one of
the early lesson that the database world
has to learn that really is becoming
difficult even to fix bugs maybe the
thing will be addressed but that's
definitely a dupe is not only good and
I'm actually I wanted to give you some
quotes here because I think he's saying
that the database community was leaping
no in all this year they really didn't
understand what was going on and then
they woke up and the other people the
system war had already implemented
systems had very little to do with the
database and this was the keynote that
he did in Berlin this year and that's
actually what is telling to the database
community so I think he's good because I
don't think it here is a mix and there
was a lot of discussion about you know
the pro and cons about that I think it's
always important to critically review
that the two projects that are happening
right now
Kerry is working on on the project
called asterix
uc-irvine he will come up eventually
this year with an open source Apache
style a doof variation of that
interesting to have a look for some of
you are more into the research part and
there's another project called
stratosphere at the technique diversity
in Berlin that is also trying to do any
it's slightly different implementation
of the Map Reduce thing just for your
information so much time since
everybody's using a dope you know let's
spend some time on my dope a dope is
really an ecosystem of projects as you
see that's what I was referring to you
know
pick 60% of this cases the Map Reduce
using the language that is like an
algebra Pig Latin
pig is really an engine for the four
languages hive was developed by Facebook
also inspired by SQL and 90% of the
cases of MapReduce I use like there and
there's a lot of other things so at the
end you have an ecosystem and that's why
I you have a distribution of doop
distribution so scale and processing and
structure data are the key important
things and then you can do all kinds of
interesting things with combining
machine learning
recommend I recommend in engines and
that creates the business opportunity
that we talked about before the main
limitation of a dupe is batch processing
and now some people are working on
trying to fix it that's the few other
limitation of a dupe one of which is for
example that is difficult to use and he
has a centralized node if you using the
Apache implementation you have a
centralized metadata store as a master
so if he goes down and when it comes up
it may take a long time but what other a
dupe distribution or do it they
re-implementing part of that and so you
have example certain a dupe distribution
commercial distribution like map are
they have reimplemented the the file
system so to make it different so you
have like MapReduce power but the
implementation is different and then you
have this dichotomy again you have the
analytics no MapReduce a dupe and then
you need the real-time part and then you
have all these key value stores and then
you can use for example no SQL database
like for example the gentleman mentioned
it before and we said a dupe doesn't
need a schema and I was curious and as I
asked actually they are not Vogel why
did you choose a Duke because amaz
using a dog that's his reply I find it
quite interesting this reply if you read
it carefully the first is a technical
issue know if you know a scale the
second is open source and the third is
basically saying everybody's using it so
better be there and then it's a three
years later we did the right choice
because it making a lot of business with
the cloud so he could well be that the
the ecosystem will start changing and
visit the intelligent vendor will start
adapting to a dupe and a dupe will
become like a commodity you have to
remember that there are other system out
there they call in new SQL I try
choosing one just you know to mention
one you know know is verticals now HP
but you can choose a green plan or
whatever they actually try to do also
analytics on big data and they are SQL
based so the column database engine and
you can argue on that but they provide
standard asset transaction so and then
it becomes very difficult when you start
looking all this system when you start
talking about performance because there
is not really a way to do a benchmarking
on them I just wanted to give you an
idea how for example analytics are used
by one large web company but this is
very specific is eBay the question is
how many eBay's are around there so how
many of you will eventually need it but
eBay definitely need it so I I had a
chat with one of the technical guy at
eBay that is responsible for food for
that and the main technical challenges
for eBay in terms of big data are they
all bandwidth you know the concurrency
in the workload management actually at
the end and the data movement between
the system you see there the interesting
thing that eBay this is not completely
actual
but very fast of ten minutes of five
minutes then seven minutes that's a good
very fast will skip anyway I can tell so
ebay is actually using free systems for
analytics the the one that is mostly
used or structured data is a terror data
so but then they have another system
where they need to have denormalized
relation and is again is the second
system is again is a terror data system
and they do user behavior they wouldn't
be able to do it with normalize data and
then they have a third system that is a
dupe for a structure complex data you
know and they do behavior user the
different is that the number of user
they are on a dupe it may be ten and by
about the other system are used by the
entire company for example eBay only for
the search engine thing they have more
than a thousand people working on that
so basically at the end of the day I
have a many more slicin that you can
when you will able to download you can
see they have basically come up with
several tools for several work now the a
structured data is definitely handling a
dupe and that's actually what eBay does
and these are some of the let's say that
the tools are they use as you see is the
high an easy thing you need a lot of
complexity also to manage all these
tools you know seamless integration is
definitely not really something that is
happening so and why do they do that
because they can then have a
improved by experience search
optimization I was shocked when you
mentioned how many people only working
on the little fence the only low window
will you do a search in in eBay buy a
protection you know fraud and a mobile
commerce that's what they do okay so who
are the typical abuser well you can
distinguish into class advanced user
maybe there are some here but how many
advanced user are there
or you need to have probably PhD from a
good school for the other one a dupe is
quite complicated so the vendors are
trying to expand or change the adult
distribution what kinds of improvement
and these are some of the challenges
that we have one of the thing is that to
bring a dupe in in critical business
project is now an easy task especially
if is if you don't have enough skillful
people and on top of the data data
protecting data against application user
errors is not an easy job
so what about the cloud the people
people in general are concerned about
putting their data on the cloud but but
this is like a do Amazon is doing a good
job with that and doing a good business
now what about traditional enterprises
here is an interesting quote I like this
quote very much because Mike
Stonebreaker is a very famous colleague
in database it the all kinds of database
we headed a group but they started many
companies in wonder which is called vol
DB vol DB is not focus on analytics and
then read the last sentence to
facilitate this architecture vol DB
offers an a dupe connectors so basically
even somebody that didn't like Adobe
dole is actually looking at the mark and
say well people using a dupe so I better
have a connector so I might skip some
from there you know this will be for
example coffee but if you talk to how
she be they will say they will have all
the data in the dupe and then use it the
the small part in in the real time NoSQL
and then basically send it back to a
dupe you have something like a connector
Apache scoop that is a tool where you
can basically transfer bulk or
relational data you
- a dupe that's what he thinks I think
that you know the a dupe would be our
commodity the only catch is actually
obviously no SQL base and this is no
obviously the only way I will skip that
part but just for you to know that is
very difficult to the benchmarking of
the systems everybody is claimed to be
fast scalable but there's no any easy
way to do a a benchmark on there because
for example the classical database
benchmark like TPC see they don't work
because they are fought for data
warehouses where you have an asset
transaction and none of these systems
have it so they don't qualify so to make
it short I think you should have a look
at this there's a there's a benchmark
done by Yahoo Research they are no more
there they all disappear but you can
actually have that as an open source at
the time to go on that well this is it
and with that you could actually have
comparison between various systems the
interesting thing about this benchmark
that the kind of query another typical
query that you have in relation database
a very simple read query and you have to
work load you know read only or read and
write and they are very simple but with
a lot of data and that's the only
benchmark that is out there that is not
vendor made because that's the question
if you ask the question to a vendor how
do you measure your benchmark I did the
benchmark but it's the vendor does the
benchmark is a vandal benchmark okay so
in here this is a neutral open source
paper so this is another interesting
quote how easy is a dope well it depends
it depends on how skillful you are but
that's really a concern know John
Schroeder is actually by the way the
founder of this and Matt are we
is a Hadoop distribution so maybe
there's something that mentioned need to
be mentioned that Enterprise Search
which is actually now a new way to find
in data and I'll skip and I come to my
five minutes I have this and then that
the the the ending of this story this is
what I found in a report of this year
from an analyst of Deutsche Bank in
London I just quote it I know paste it
in exactly as it is written no matter
what the truth is about this report
because nobody can really read the
future but you can see how conservative
this attitude is the very scary thing is
the number two so to what extent big
data is at the pad moment really in the
enterprise I think if the future will
show that data will come from all over
the place so you won't be able to escape
it but I think this is something to you
know consider especially when there's an
attempt to replace technology in the
place okay so that's take the time for
that and I'll have five minutes and I
use the last five minutes for something
that I'm very keen of telling you that's
the first story about deep data then
nobody is telling you or at least very
few people I think you know one movie
this is the other story about big data
story I like the story about big data is
the following that nobody or very few
people care about using data for
something that has a social meaning very
few people know all of them but very
especially in our research world on
developing there is sometimes not really
this interest and I asked myself why so
I'm I took this quote which i think is
quite good because you can say well you
can call it a common good challenge for
Big Data
I think that sounds quite nice because
there is an opportunity
first you can misuse if I can use it
nicely now the World Economic Forum you
know these guys are meet in Switzerland
in every year they even discuss about
big data with a report called big data
big impact new possibility for
international development that's
actually the exactly summary what is
interesting and they have a report that
you can download from the web which is
called big data big impact new
possibility for international
development and this is taking the
attitude of looking how you can do with
big data something for social and they
identify for example there is a an
initiative for un call UN Global pearls
and this is the website then there is a
global viral forecasting but you can
forecast on diseases and then plant from
and then I ask a few people you know a
few questions you know what are the the
main challenges why researchers you know
do not really look at that you know ask
them by the Google research and IBM
Research a consultant somebody micro
sorcerer
what I like in summary is what is
written Paul Miller is a consultant so
it's not really based on a company which
is important he said look what they say
about what developers should do work on
stuff that matters you know wow this is
a good thing
let's work and stuff the matter as well
and at the end of the day if you want
you can download the full report there's
a website you can download it I like to
you know finish with some thinking it's
always possible to follow and say ok I
use big data because my margin with
better my productivity will better I do
more money I develop her interesting
technology and there's a lot of great
Dane's here and great developers and
maybe some managers it would be nice if
we could also use some of our energy and
I say we because we're all there to
maybe go in that direction as well and
that's the first story that I wanted to
tell you which is not necessarily the
one that normally is told needed in
business conferences nor in
technological conferences well and
that's the end of my presentation</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>