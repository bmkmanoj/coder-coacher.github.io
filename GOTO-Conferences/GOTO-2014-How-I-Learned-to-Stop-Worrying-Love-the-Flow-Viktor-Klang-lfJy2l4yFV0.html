<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • How I Learned to Stop Worrying &amp; Love the Flow • Viktor Klang | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • How I Learned to Stop Worrying &amp; Love the Flow • Viktor Klang - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • How I Learned to Stop Worrying &amp; Love the Flow • Viktor Klang</b></h2><h5 class="post__date">2014-12-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lfJy2l4yFV0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is victor clang I work for a
company called type-safe i am happy to
work with a lot of really smart people
which is something that makes your life
really interesting and today i'm here to
talk to you guys about streaming and
stream program or stream based
programming so i have a lot of stuff to
say and we only have so many minutes so
i will try to just churn through and you
guys as soon as you feel like there's
something that needs to be elaborated
you just ask and let's settle that then
and there so let's start there's a quick
little agenda I won't read it to you I
think that will be just fine but just to
set through the the frame of things so
let's start I want to start by saying
that streams are not collections and the
reason I'm going to say this is it makes
a whole lot of difference to us as
programmers the definitions of the words
that we use so that we understand each
other and that we understand ourselves
so let me get through the streams are
not collections reasoning there was a
slightly famous Greek guy who once said
you cannot step into the same stream for
as you are stepping in other waters or
ever flowing unto you so it sets the
stage there's something something
related to time there right it's like
can't do it two times so what is a
stream in this context we're talking
about an ephemeral flow of data it's
something that is not static it's
something that disappears underneath us
so to say and what's interesting is that
a stream can be potentially unbounded in
length and the reason I say unbounded
instead of infinite is because
infinities have a whole lot of other
baggage that gets pulled into the
picture so think of it as something that
potentially never ends
when we talk about stream based
programming we're usually focusing on
describing transformation if we have
something that is flowing we can take
that thing coming from the upstream and
apply some transformation and push that
to the downstream like a filtering
operation or just passing data through
if we want to copy from one place to
another what's interesting about that is
we can sort of see it as a processing
Network that we can form we can have
different stages through the upstream is
going in that direction and the
downstream is going in this direction so
we have stages of streams that fan in or
join and it can also be split up and
what's interesting to see is that a lot
of stuff here could potentially be done
in parallel because they don't have any
sort of real tight coupling between them
so there's also a very interesting
amount of inherent parallelism that is
possible within stream processing so
let's talk about what a collection is
how many of you guys have ever used a
collection okay I should be good alright
so the oaks for dictionary which may or
may not be the source of truth for
defining computer science terms but it
says that it's a group of things or
people sort of sort of right group of
things or people and of course the
source of all the truth on the internet
says that a grouping of some variable
number of data items oh we're sort of
we're started going close rights is
getting a bit warmer all right so what
happens if you google for what is a
collection of course there's a
JavaScript library right it's the
backbone jss that a collection or simply
an ordered set of models all right fair
enough I guess
and if we're on the JVM which probably a
few are java.util.collection is
something that has a definitive size
like there is something like it it's
always some size there right know
exactly what's in the collection and it
provides some sort of iterator that we
can visit the elements in the collection
so we can go through it in some way and
also there's a few methods that describe
whether or not something is in that
collections like contains and stuff like
we can query if some element exists in
that collection so what do we expect us
users when we use a collection well if
we have an iterator and we don't change
the collection ourselves or like we know
that there's no one else changing it and
we use an iterator and we don't end
early we we expect to visit all the
elements of that collection because
otherwise I think there's going to be a
few broken programs out there and what
we also expect that if we have an
iterator and we take the first element
out of that iterator and we create a
concatenated iterator with the rest of
the iterator it should be the same like
we took the first and the rest put it
together again should be the same as the
first right and the same elements also
the contents of the collection usually
doesn't depend on who's actually
processing that collection so it has
some sort of does not relate to who is
using it and the contents does not
really matter or depend on when the
processing happens usually right so if
you get scheduled out for five seconds
while the GC is running when you come
back the collection isn't different
right it would also be quite weird but
when we observe a sequence it will be
very weird if
it mattered when we subscribe to see
that thing that would sort of go against
the expectations if we think about it
ask collection streams as collections
this will sort of be weird and also this
would also be weird like if we can't
process the elements fast enough and
that has weird effects that will also be
sort of weird if you think about it as
collections because that's not
necessarily something that we think
about when it comes to collections like
can we iterate fast enough or the other
way around when we can iterate faster
but we don't get data as fast like
where's the data that would also violate
our expectations so that's a very
important there are very important
distinctions between streams and
collections and setting the right
expectations is the first step of
understanding the difference so let's
have a look at what's new in Java 8
there's this new Java Java util stream
package and what Java util streams is
that they're not derived from collection
so there's there are some evidence out
there that screens are not collections
and it also says that streams differ
from collections in several ways all
right fair enough they don't have any
storage so they're not storing
intermediate results they are functional
in nature which means that they focus on
describing transformation using
functions and they are laziness seeking
and which is a way of saying that they
don't need to evaluate stuff unless
there's somebody down there that really
needs it so as you build up a
transformation you don't need to do any
work until there is something that needs
to be and that needs to use the result
and of course there possibly unbounded
so like a stream of all natural numbers
and they're consumable so Java Java util
streams are consumable you can only use
them once like an iterator and once
you've used it it's sort of now it's
used in now you need to go and get
another one
so strange versus collections like how
do we tie the bag together what's the
relationship between streams and
collections well you can take a
collection a collection is like a fact
it's like I know that we have X and I
can turn that into a stream that has X
right it doesn't really matter I Chris I
already know that I had X so we can
convert a collection into a stream but
the other way around is a bit harder but
it's possible but has some caveats right
so what happens if you have a stream
that never receives an element at all
like it takes forever to create the
first element of the stream when do we
say that the collection is done like do
we wait forever that's not necessarily a
good idea but you can do things like
within next time take everything that
you receive in the stream and bunch it
up into a collection and then pass it on
so there are ways that you can do it but
it's not a one-to-one mapping so saying
that a stream is sort of like a lazy
collection it evokes the wrong kind of
association so let's try to divorce
those concepts so let me just try to
give you a brief live demo of what I
mean let's see if this works alright so
here's some code here and this is call
it doesn't really matter that the the
language itself it's more about the the
flow of the operation so what we're
going to have here is we're importing
something called bang so this is a
interestingly sample with defining some
sort of model or domain where we're
doing something related to banks and
we're going to create something called
an actor system I'm going to explain
later what that means we're also going
to create a flow materializer and a flow
materializer is something that is going
to be able to take a description of a
stream and materialize it make it work
make it run and do stuff so let's start
by by describing our stream so first
we're going to have something which is a
function to transfer and transfer here
is just going to get
right a and random transfer between two
accounts so this allows us to have an
unbounded stream of transfers because
hopefully our bank is going to live for
a very long time there's going to be
transfers all the time an unbounded
amount of transfers so it's from an
account to an account with in some
currency and with some amount so that's
our so domain object so transfer here
creates a random one then for each of
these transfers we're going to transform
them using identity and that's not
really meaningful but it's just showing
what you can do identity means that take
whatever input you get and omit that as
your output so think sort of copy file
or something like that so we're just
going to going to pass transfers through
so that's a processing stage then we're
going to take ten of them or those
unbounded number of transfers and for
each of them we're going to invoke the
print line which is just going to print
out the system now so now we've
described a potentially unbounded stream
of transfers we're going to just pipe
some stuff through take 10 of them and
print them out and then at the end we
attach oncomplete which is when you've
done all this you want to shut the
system down and system shutdown is
important because all of this is going
to run concurrently and using all our
course so we pass in the materializer
town complete to say okay now we're
ready now we want to run this thing we
have described our processing pipeline
and we want to make it run so let's see
if we can if we can make that work where
do I have my pointer this is always
interesting does it work at all
did I click it all right i clicked it
alright so what did it do it generated
10 transfers and printed them out and
then it was done right so we can
describe a flow of information and
transformations on top of that flow and
then piping it somewhere else and then
running it but what's interesting is
that this thing didn't run in the
current thread it actually used all the
course in my tiny little macbook air but
it's decoupling the execution from the
description of the transformation which
is very important because we could
decide to run this on one thread or a
million without changing the code so
let's jump back to the presentation and
continue from there all right makes more
sense now we have both serve some theory
behind it and some code that did
something good good work all right so
one of the main problems when it comes
to stream processing is that you tend to
have asynchronous boundaries in your
program you have data coming in either
from the network or from some other
source that's running concurrently or
asynchronously with your program so you
have an asynchronous boundary with your
inputs and you tend to have an e
synchronous boundary with your outputs
and what do you want to do is you want
to try to get data across that boundary
without blowing up if the the sender the
guy on the upstream is producing data
faster than the guy that can receive it
receives it so if we want to do this and
do it safely we need to we need to work
on that so what are the possible
solutions for this so either we just
block we have asynchronous boundaries
but we're going to block just just now
I'm not going to do anything unless that
guy has either received or or
or a scent so that's not really going to
work because if we block then no other
threat will be able like that fred is
going to be hugged by this thing and
it's not going to do anything not really
good so what if we what if we just we
just let the sender the upstream just
push whatever he has and if we can't
receive it we're just going to drop it
on the floor or perhaps we can buffer it
that's up to some point but if he's
really faster than at some point you are
non your your finite amount of memory is
going to run out and you have to start
dropping things anyway so it's just sort
of a bandaid on the problem so that
doesn't work either so is there another
way of doing it well let's call that way
the reactive way which would be non
blocking like not like the first thing
and it's non dropping like the other
thing that's good as well we're not
dropping stuff and we're not hogging our
threads and we try to stay within the
bounded space so that we can always
reason about how much resources are
being used when we sort of flood the
system so how do we do that so if you've
heard about the reactant manifested
before this is not going to be new to
you but what if we could take a lot of
the the knowledge and ideas behind the
reactive manifesto and apply it to this
problem space can we can we solve can we
solve it in a reactive way so when we
started working on this problem we
realized fairly quickly that we can't be
the only ones that have this problem
like it can't be 2013 or what it was and
like everyone else has never never seen
this problem before so we we made a
couple of calls and we asked a bunch of
other people in the industry like okay
do you have something similar that
you're working on or have an issue with
so it turned out to be quite a few that
had the exact same issue so we created
this reactive screens initiative to
solve it in the general case so the
origin of motivation for doing this was
that we all in the
initiative or facing the same problem
that's a very good way of starting a
collaboration and if we're all building
tools for our communities we have a
reason to soar feel the urgency really
want to solve this problem and a common
solution for everybody would benefit
everybody so what if we could connect
things together if they follow the same
specification so you could use different
implementations that are good at
different things and hook them together
and get the benefit transitively through
throughout that sort of composition also
interoperability would be the best use
of the efforts right I mean if we don't
have these isolated islands then we'll
be able to reuse the efforts of
everybody and the end goal was to okay
if we can really solve this problem it
sounds like something that we could even
be able to put into the JDK if we try
hard enough like if we had a standard
solution for this then everybody could
get benefit right even though even
though standard library so currently
it's a collaboration between engineers
from a lot of different companies and
its really focused on the engineer part
where it's not a specifications group up
in the sky writing writing weird text
that is up to the mortals to sort of
decipher later it's really focused on
engineering so we have a lot of
different engineers that are working
together on this so i think that's super
super exciting so the goals are minimal
interfaces essentials only so if we can
make the the core the root of the
problem really easy or really simple in
terms of the surface area then we think
that that's going to be beneficial
because me sad there's a way less gray
area or gray areas and what we want to
do is make sure that the specification
of the semantics is rigorous so there
doesn't exist a lot of divergence down
the line where people interpret stuff
differently and you get things that sort
of work together but you also and I
think this is something that ever
standards committee or organization or
anybody who wants to push a
specification out on other people they
should write a test suite or you can
test your implementation using tests not
sort of did I understand the
specification right but really something
that you can execute over your
implementation to say okay now at now I
get it it's passing the tck as well so I
have understood the specification I
think it's super important what we're
not trying to do is that we don't want
to create a standard epi because if we
are going to solve this problem on the
JVM there's your Scala which is going to
want to have a scholar kind of API and
there's closure which is going to want
to have a closer style API so we don't
want to have some sort of lowest common
denominator and user API because that's
not fair to any user so if we were just
trying to solve the the core problem and
then the implementations can focus on
the API I think that's a very good
trade-off and what most people don't
think about when it comes to
specifications is that when write a
specification if you're not careful
you're going to rule out potential
implementations like if you make
requirements that aren't possible to
implement efficiently then you're
locking down the performance of all
implementations so you need to think
about that up front which is what we've
done so reactive streams is the the end
product of this initiative and they are
asynchronous and non blocking so that's
pretty good I'm walking is good and
there's a flow of data from the upstream
to the downstream and how we solve the
problem with being within bounds and
being non blocking and asynchronous is
that we have a flow of demand that goes
in the other direction so we know when
there's does this guy even want data we
want to minimize coordination and
contention this is important I think I
realized as I had written this
if any of you guys attended the Martin
Thompson's talk or or the reactive talk
the universal scalability law tests
exactly into this if we have
coordination and contention my
coordination in terms of coherence and
contention we're limiting the
scalability of the end product so we
need to think about that when we design
it also if we used in the thing about
the reactive manifesto if we have
message passing as a part of our
thinking when we design a specification
so that we not do this type coupling
between components then we should be
able to have different pieces of a
stream on potentially different CPUs
different machines different parts of
the world because it doesn't matter so
if we think about that as we design it
then we'll be able to scale out the use
of of streams all right so enough fluff
naming is hard this is where we ended up
there was a as you can imagine a lot of
discussions on naming there is like
consumer producer source saying you know
there's probably ten of these so the
upstream or the producer of information
or data is called a publisher and a
consumer or recipient of data is called
a subscriber so the publisher sends data
down to the subscriber and the
subscriber sense demand to the publisher
so the data elements flow downstream
demand flows upstream and what's
important is that data elements can only
be sent when there is demand for data
important really important so the data
in flight is bounded by the signal
demand because if the subscriber says
how much he wants then he's effectively
bounding how much can be in flight at
one point in time right and what's also
important is that the recipient of the
data becomes in control over the maximal
incoming data rate so let me just take
an example so if you were to go to the
store
and you want ice cream and let's say
that you are super hungry and you want
10,000 ice creams if you go to the store
clerk and say that give me ice creams
and he will just start like throwing ice
creams at you as fast as he can because
that's what he does and you have to yell
stop when you when you get to 10,000
there might be a case where you there's
not enough space around you to contain
the ice cream right or you might not say
no fast enough so he's already thrown a
couple of extra at you but that's not
really how we would solve the problem in
real life we need to have this demand
saying that I want ten ice creams I will
put it in the bag I want 10 more I will
put it in the bag right so the consumer
or the recipient of the data is in
control over how much data is is
acceptable to receive by that by that
consumer at any point of that so what's
interesting is that we didn't really
find a good word for it so we've been
describing it as dynamic push poll as
sort of a how works when you run it so
when the consumer can consume data
faster than the producer the publish in
this case can send it it becomes a push
model yes yes so so the question is does
the fact that there can be more than one
subscriber break the fact that there can
be bounded
yes so what's interesting is the next
there were the second slide after this
so let's get back to that so what's
interesting is that when the consumer
where the subscriber here can receive
data faster than the publisher can push
it or send it it becomes a push model
because the publisher can always send
right it's an important implication but
it's also the other way around so when
the publisher is faster or can send data
faster than the subscriber can receive
it it becomes a model because the
subscriber says okay now I'm ready for X
and what's super interesting is that
during one time it automatically
switches between these modes depending
on the scheduling artifact or the
availability of data in the system or
the possibility of writing data to
somewhere else so you don't have to do
anything to get this nice benefit of
going as fast as they can at any point
in time so what you could do is the
demand if you only said I want one and I
want one is the only thing you can say
that would be quite inefficient because
you would have to say if you were to
order 10,000 ice creams you would have
to say that I want an ice cream 10,000
times so if we allow you to say I want X
number of ice creams then you're more in
control over how many times you say it
and how much you're willing to store at
any point in time so batching the demand
means that you can also batch the data
right you can store it somewhere you say
I want 10 and then the publisher can
then create 10 you could do that in
parallel if you wanted to so going back
to your example a very common use case
is that you're going you're broadcasting
or sending out information to multiple
recipients very common so you see here
both the flow of data and the arrow so
that is the flow of demand and that goes
up screen so when you split data where
when you send out data you're
effectively merging the demand between
these two things if he wants 10 and he
only wants one you can only perhaps
request one from your upstream because
that's the aggregate demand but what's
interesting about the model is that the
model doesn't say that all subscribers
are entitled to data so if at any point
in time the publisher feels like this
subscriber is not keeping up he's
dragging us down I'm going to I'm going
to error him out and say that you need
to keep up in the future if you want to
want to have my data so that's that's
how it solved but the inverse is also
true so it's very common to have
multiple sources of data and you want to
merge them in some way you want to like
have all that data from there I all the
data from there and I want to look at
that data in aggregate what's
interesting is that when you do merging
of data you're splitting the demand
you're saying that okay I I want five
things so I will request from each I'm
splitting the demand to my up streams
let's symmetric so what does it look
like in practice this specification so
there is for interfaces how many of you
guys are urging them / Java people
enough this an interface just sort of
specifies what you need to implement
right this is this is the signature that
you need to implement in order to be a
publisher or a subscriber or whatnot so
the publisher interface only has one
method this subscribe takes a subscriber
doesn't return anything because it could
use message passing it could send
something to the subscriber but if we
return something strict there then it
would have to wait in order to be able
to return that thing and a subscriber
has four different methods I I usually
call them signals because it's signaling
to the subscriber that there is either a
subscription for you or there's data for
you or there's an end up stream or
something went wrong in your upstream
and we need to abort so it signals to
the subscriber and the
actual instance of the subscriber that
processes these signals could live
anywhere doesn't matter so the the
common thing between a publisher and
subscriber becomes the subscription and
the subscriptions in control over
requesting more elements and canceling
the subscription perhaps you have an
unbounded streaming and in the case
where we did the live demo and you only
want 10 items then you want to cancel
your subscription after you have
received your 10 items so you want to be
able to bail out early but this could be
the entirety of the spec if we wanted to
but it's sort of like the SK I
Combinator's is that there is another
thing that is so useful that everybody
is going to implement it themselves and
I call recall that the processor and
it's something that is both a publisher
in a subscriber so I think function
right if we only have getters and only
have sitters that would be sort of a
boring life so we want to be able to
have functions that can take both input
and produce output so everybody's going
to want to use that so we threw that in
there and it's super simple and it just
sort of creates this common vocabulary
so how does the actual handshake work
how do we set up the screen to work well
first the subscriber gets put into the
subscribe method on the publisher and so
like here here am I i want to have data
and the publisher creates a subscription
and passes that in to unsubscribe on the
subscriber so that's the handshake you
want it to have data here's how you can
signal demand and and cancel it and once
you've done that the subscriber can be
begin by requesting more information
from the from the publisher and the
publisher is done free to send elements
downstream it's pretty nice and the
subscriber can request more elements but
it's very important to realize that
there is not a first you need to request
then he needs to send everything then
you can request more that's not how it
works so you can request more at any
point in time that you feel like okay
now I have extra space or extra room for
more elements there's not an acting
protocol and then the
you can send more elements so what
happens when the stream completes like
how do we propagate the end of file for
instance well we first sent elements and
then the publisher is free at any point
in time where where the stream is closed
or has stopped normally to send the on
complete signal and that is super
important because if you were to require
the subscriber to have signaled demand
in order to be on complete call it might
never get the own complete call so this
allows you to terminate as early as
possible and there's a somewhat of a bog
/ bad condition in TCP where you have to
register for either reading or writing
in order to get the the closing of the
stream especially when it when it fails
and this allows you to always get the
completion signal no matter it you don't
need to do anything special for it and
if there is a fatal failure from the
upstream where there's never going to be
more data because things went really
wrong when it just sends the on error
signal and that passes through any
exception that you want to pass through
saying that this is this is what went
wrong inning to you need to deal with it
alright so so type safe has a a project
called acha which i have been a part of
creating and what type is it in order to
implement this reactive streams
specification and the interfaces we
implemented on top of akka and akka has
a unit of computation called an actor
and an actress or like a process that
communicates using sending and receiving
messages and they're sort of purely
reactive because they don't execute
unless they have messages to produce and
they don't need to block so it's a very
good for us a very good fabric to
implement the actual stream processing
on top of and they have an address so we
can send stuff to them and they have
sort of a mailbox where the message is
output for the mail for the actor until
they can receive them they have some
sort of current behavior and they can
change the current behavior as they
process messages and they have some form
of local storage which means that they
can also do some some computation of
store
intermediate results and they're only
scheduled to run when there is a message
so if there's no data they don't consume
any cpu resources very important and the
interesting is also I realized yesterday
that I read and reread godel escher bach
the book da be it's actually funny
because i met ola beanie for the first
time yesterday and he also remember that
he was together who recommend the book
so read the book and a huge thing in the
book is recursion and was interesting
with an actor is that every actor has a
parent actor so it's recursive every
actor has a parent and at some point you
need to terminate that recursion right
if every parent like it that where does
it end and outside like at the top of
this where we try to determinate this we
have the actor system as you saw in the
code that sort of the encapsulating
thing and outside of the that sort of
bubble of time and space lives the actor
that's called the one who walks the
bubbles of space-time and since he's
outside of the model he doesn't need to
have a parent but he is the parent of
the other the topmost one so that's how
we term it just just an anecdote well
and every actor if that means also that
they don't have only have one child one
sub act or they can have n number of sub
actors so it sort of forms like a tree
and you can spoil an actor's at any
point in time which means that it's a
dynamic tree of processing so they
process one message at a time so they
can receive messages from many actors
but only process one at a time and
there's no locking in between it's just
the mailbox that grows in that case and
what's interesting is that the overhead
of one actor is about 450 bites anaka so
that means that if you do the math you
can have millions of them on a very
small amount of memory so like there's
like 2.5 million per gigabyte or
something super important and also acha
has a clustering feature
so that we've tested it so it scales at
least two two and a half thousand nodes
so if you have millions of actors x 2500
nodes that's a lot of actors and it
ought to be enough for anybody right but
there might be used cases where where
you need more what's interesting is that
with this multi-core era where we get
more and more course if I create
millions of actors on this machine and I
would have millions of core on this
machine I would be able to use the
course which is super important well if
you want to learn more about actors
there are two papers that I recommend
reading you can you can check the slides
later if you want to add more more
information so what's next for acha
streams so our choice was what I demoed
that's sort of the dsl for acha streams
and what the flow materializer does is
that it creates actors from your
description of your transformation and
runs those actors so what can we do with
this streaming acha streams API well the
current API is sort of minimal we are
trying to condense it and keep it small
but of course we need to grow it to fit
a lot of common common use cases and
also naming is hard right either you
take names as they are established from
research or you take names that are more
intuitive to anybody that does know
about the research but then it's hard
for them to go read the research because
they don't have the same name so we
still sort of a work in progress to find
the right names and we have a new API on
the way that's called flow graph which
is also something we're talking with
where you can describe a complete
processing graph immutably and share
that and plug in inputs and outputs at
all end so you sir reuse the
transformation in different contexts
where you have different data sources
and plug different graphs together and
build up this sort of description of
transformation build a vocabulary of
transformation that you can reuse and
what we also want to have is really good
both skull and java ee p is because once
you have a job api any language on the
JVM can can use that so it's it's a
requirement so the back pressure of the
of the thing like we need to be but we
we need to communicate backwards that
you can't send stuff now that's the
demand that becomes the back pressure in
the system and what's interesting is
what we potentially could do with the
back pressure is that we could have it
self-tuning at runtime so since every
processing stage in our our surf
pipeline or graph of transformation we
can no information about when we request
more and we receive more like what does
that look like over time well what's the
behavior of the of the thing in front of
me and also we can also know how our
latency for processing stuff ourselves
like does that change over time like
what's the relationship how am I
contributing to the overall time and
also it can look at the request from the
downstream right I can see the the thing
that is consuming my data how does that
behave what's the latency between me
satisfying the demand and receiving new
demand and also trends in the demand
right is it running in lockstep or it's
asking for one I give it one it's asking
for one I give it one like there I can
know I can know about the behavior of
the thing after me and there's different
kinds of behaviors so you could have an
emergent self tuning back pressure that
varies over time where you only have
local information at every step and the
changes will ripple through in the
entire pipeline dynamically super
interesting would love to do this we can
also do something called Operation
fusion and what we could do is compile
time as you declare your your flow of
things we could use scala macros to
rewrite your transformations to optimize
so let's say that we would have a filter
and plus buses concatenate here we have
a filter operations that take some input
and emits some of that input and
drops the rest so I only keep the red
apples and drop all other apples and a
map operations that transform one to one
like I want to peel the Apple and just
send the peeled Apple downstream well
the interesting thing that there's an
operation for that sort of composite
behavior that's called collect which is
match some predicate perform some
transformation and instead of running it
in two different steps we merge it into
one single step so we collapse it into
two but we can't really do the other
thing right because if there is some if
you do the transformation and there's
some sort of side effect happening then
rewriting this would change the
semantics of the program so that's where
there's a question mark could be
something that you open into saying that
I know that my transformation is safe so
you can apply this this optimization
anyway but we can also do this runtime
so let's say that we have any type of
stage and after that stage we have an
identity stage like transform just pass
data through we can just drop the path
data through because it's not really
doing anything we can just omit that and
also the other way around if you have an
island estate in something else we can
drop that too so what's interesting is
that if we have an expression like we
filter stuff then we have drop until
some condition like drop stuff that
doesn't match and the first time it
matches it's always going to type things
through so that means if we follow that
with a map operation a transformation
once at one time the condition triggers
it becomes an identity stage perhaps it
drops until there's a big enough Apple
and then it sends all they're all apples
through since it's an identity stage we
have this rule we can drop our entity
right we can drop identity and now
becomes filter + map we have the filter
+ map rule that says we can collapse
filter + map into collect so
runtime we could optimize the
transformation but we can also do
operation deletion we can remove
operations so for instance a fold is a
transformation that only ever emits one
result so if you follow a fold with a
take operation where take is take n
number of items and n is bigger than one
it's not going to do anything so we can
just drop that it will never it will
never have any difference in the program
drop zero like drop nothing doesn't do
anything and of course as we know before
if it doesn't do anything we can remove
it as well and we can also do interstage
simplification so let's say that you
have this thing and at the end is take n
and we know that take n once that
becomes identity we can remove that
operation this leads us to if we have to
take in there we have some execution
optimizations so let's say we have take
em at the end we know exactly how many
instructions take n is going to need and
the user cannot pass in any kind of
function in there it's completely sort
of fixed so that means that we don't
have to run asynchronously we could run
it synchronously with the same operation
the preceding operation or the
succeeding operation we just fuse that
on to the end and make it run on the
same thread at the same time or just
after it but we could also do it stuff
like distributed streams so we could
have a transport protocol where we have
the sort of this interface with sending
demand and receiving stuff if we put
that and sandwich that on top of network
protocols we could run that over like
TCP UDP or any bidirectional channel we
could also do maxing of streams re using
multiple reactor streams over one single
transport and D marching at the others
other side but what I am super exciting
about is the possibility of being able
to materialize a flow like use you
describe the the transformation and when
you pass in the materializer
it can say okay I will run this part of
the flow on this machine and that part
of the flow on that machine so you could
scale out transparently your program
super super interesting so let's do
another live demo just to wrap things up
let me see let's see here can we I think
we need to run through this a bit so so
what we're going to do here is that we
have the same thing that we had before
right now the transfer we unbound a
stream of transfers and then under we
have another flow which is a clock that
every second it's going to admit a tick
to have two different inputs or two
different things that we can get input
from and what we're going to do if you
skip the 412 30,000 462 what we're going
to do is we're going to take these two
streams and join them with a sip
operation and sip just means that you
get the pair like one of that and one of
that in a pair and then we do map so we
transform it and we just retain the
second part of the pair so we're going
to discard the tick we don't care about
the tick just keep the transfer then
what we're going to do is something
called map future and map future what
that is going to do is it's going to
call a webservice in this case and it's
a web service that does some sort of
conversion between currencies so what
we're going to do is the currency that
we have in the transfer and the amount
of the transfer we're going to send that
off to the web service to get the result
back and we get that result in a future
so a future is an object that represents
a value it that will be will be there in
the future and when we get that thing
when we get the value from the web
service we're going to transform it by
creating a new transfer with the URL
mount and the new amount
in Euros so we're going to convert every
transfer to euros everybody with us oh
and we're going to create a publish out
of that and we're going to create 3462
of them so we have 463 38,000 462
different streams right a lot of them
you know the reason why the number so
what I have calculated that using the
materializer will create 26 actors for
every one of them and 26 times 3462 is 1
million and 12 so it's close to a
million so 1 million actors we're going
to create and then what we're going to
do is we're going to create an N way
merge taking the 30,000 462 streams and
join them together into one and we're
going to group that so we're going to
create a collection of at most is 1
million elements in one second so that's
the batching thing that I talked about
earlier transforming a stream into a
collection so then we have a collection
of elements that happened within a
second what we're going to do is for
each of those windows that collection
we're going to analyze that collection
all the transfers that happened within
that window we're going to analyze them
and analyze is a very very naive thing
that just creates a string of the number
of transfers that happened and the
average some in euros of those those
transfers and it just creates a string
out of that so down here after analyzed
we have a stream of strings that we're
going to print out all right so we have
a lot of streams that call out to some
web service to translate them and then
we join all of them together create a
window of everything that happened every
second and then we're going to analyze
that and print it out right and we have
more than a million actors and at the
end
and unless we actually execute this line
nothing is going to happen because up
until now we've just described what we
want to do and unless we add consume
there with the materializer nothing is
ever going to happen so let's see what
we can do here like I don't see much
from my own screen so let's let's run
this thing so what you might hear is the
fan on my macbook air as its tormented
by my program and generates over a
million actors so if I if I 30 years
from now have 1,000,000 course in my
macbook air that would be able to be
used for this thing so this runs within
bounded space we didn't have to do
anything to get that bounded space we
didn't encode that anywhere and it's
transitive with the back pressure and
this program is just going to run on my
computer and the important thing is that
once we have the bounded space when we
deploy stuff into production we can
start to reason about how much we can
deploy into one machine it's not like we
have to get woken up at 2am because
there was too much input into our system
super important to get this boundedness
of work in our system and everything
here is non blocking like it's if you if
we can see see my but we can't it's it's
here but if you could see what's uh if
we exit the you see my course up there
it's sweating a bit so it's using all my
course I didn't have to do anything
specifically to say this is how you need
to do this in order to execute this so
it's really divorcing the concern of
execution from the concern of describing
transformation
so if you want to you can run this on a
single core if you want so if we start
tormenting my poor little machine can we
see the that's there is that the X all
right and we're almost back almost back
so if you want to get your hands dirty
want to try this out you can have a look
at reactor streams org for reading more
about the actual specifications and
things you can have a look at the code
there is an artifact published that it's
the current 040 release we're going to
aim for 10 though as soon as possible
because I think we've stabilized the AP
is in the specification we have an
experimental version of a car screams
that was released yesterday if you want
to have a look at that we also have
something called an activator template
which activator is a learning experience
you can download you can have a template
that generates all the things around you
and you can just start hacking
immediately if you just want to try
things out and throw code at it and
that's all I have and thank you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>