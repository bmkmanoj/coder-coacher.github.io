<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • JavaScript Performance Through the Spyglass • Vyacheslav Egorov | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • JavaScript Performance Through the Spyglass • Vyacheslav Egorov - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • JavaScript Performance Through the Spyglass • Vyacheslav Egorov</b></h2><h5 class="post__date">2016-08-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/r76ZjdzFExg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is rich Slav friends called me
Slava
all people are my friends unless they
write JavaScript no anyway so I'm here
to talk about JavaScript performance and
this talk is a little bit different I
looked at the schedule and all talks are
mostly about using javascript and so
they're targeted to JavaScript
programmers but my talk is a little bit
from the other side from the inside the
trenches it conveys my perspective on
JavaScript performance based on my
experience working on the v8 I worked on
all kinds of different virtual machines
in my life starting from Java VM and I
also worked on things like v8 which is a
JavaScript engine inside chrome and dart
VM and the lower GT even but this talk
is concentrating on my experience on v8
so and why is it important to know how
v8 works well that's because all the MS
are very complex and to understand the
performance you really need to
understand how VMs work so for example
this is a stack overflow question from
today I edit it right in the morning so
somebody is very curious why on the node
chair so how many people use nodejs okay
a lot of you my condolences so why are
no js' let based for loop is slower than
there are based for loop by a
considerable amount maybe four times
it's an empty loop so why empty loop is
slower so the overhead of loop in itself
is considerably higher and usually you
have no choice but to ask a question on
Stack Overflow if you encounter the
problem like that but in reality if you
are prepared at least a little bit then
you can have a choice you can go try to
profile what virtual machine does so you
can at least try to read the code that
it produces so maybe the code the
virtual machine itself if you can cope
with C++ and don't in the bucket for
that and
not many people can cope so for example
the wheat has this optimizing compiler
called crankshaft and if you want to
deal with crankshaft you can use a small
tool that I wrote that allows you to
read the code that it generates and
using this tool you can look at what
happens with that for loop and it
probably is a little bit hard to see but
this is the code of the benchmark in the
form of the control flow graph that is
used by the optimizing compiler and it
doesn't really what matter what really
actually happens but you can see some
suspicious things are happening because
you have the loop with four wire up
there and and somehow there is a very
big loop down there which is
considerably bigger and what's most
auspicious is that there is a loop
inside the loop if you use the let
binding in the for loop this is really
strange
and actually the led bindings were added
after I already departed from v8 so I
was curious myself what's actually
happening and the way I was trying to
resolve this I didn't know actually what
is happening so I got the source of the
v8 there are instructions how to get it
and I'm built a debug version of v8
which is very easy to do one make for
example a linux little bit hard on
Windows but development on Windows is
hard anyway you cry all the time and
then I run this version pass the test
file to it and I asked it to print the
abstract syntax tree that it builds for
this code and it looks very kind of
scary it doesn't matter what actually
happening I will I will say it doesn't
matter what actually happening a lot in
this talk because you really need to
concentrate on some points in the code
and not pay complete attention to what
is happening and if you just start
reading through it then immediately what
jumps is that there is your normal loop
variable I and then there is this
strange temporary and unfortunately it
doesn't fit into the screen but it says
dot for
so obviously your original code did not
contain any variable called dot four you
can't even create a variable called dot
four in JavaScript so this is
immediately really strange so you go and
oh yeah here is a smaller version so you
go and you just grab the sources of v8
for these dot four thing and you find it
somewhere in a strange header file
called SD value factory and there is
this thing called dot underscore four
and you say okay maybe I should grab for
that now
and you grab for that and you find the
code in the parse and it doesn't really
matter what the code is it's just enough
to know that it's in the parser then you
go read start reading the code around
this places where it mentions dot for
the dot underscore four and you
immediately find a helpful comment so
the v8 sources are relatively well
commented so you can find this comment
there which says okay we have this for
loop with the letter cons binding and we
rewrite it as follows and what follows
is a huge rewriting rule for this and so
in the comment and immediately you can
see where the loop inside the loop comes
from there is a there is a loop inside
the loop here there is this loop and
then there is this loop and I can't
really give you insight into v8
implementers brain why they decided to
implement it this way but I guess that
was because they wanted to shoehorn the
led bindings into the like old esteem
and as a result they got somewhat
inefficient implementation of that for
the for loops so hopefully they will fix
it and things like JavaScript core or
like Firefox as far as they know don't
have this the SpiderMonkey they don't
have these issues
so eventually v8 people probably fix it
so there are two points of view here is
that internals are important and you
really need to know them but there is
also a point of view that internals are
completely irrelevant and you don't
really need to know them and both of
these are valid so I don't want to try
to persuade you that you need to know
everything that happens within those
many many many many many many many many
many many lines of
class code but you need to have the
understanding on a very high level
because when it actually matters it
matters a lot so for example here's a
chunk of code from some Russian like
social site where people talk about
programming
somebody tried to measure something and
they wrote this code and everybody here
can read JavaScript code so I'm not
going to go through that and explain
what it does and they run it they call
this fine function many many many times
and they discovered that you can do it
on v8 own note like five thousand times
per millisecond and that sounds pretty
cool and then they try to do this they
said okay what about if we catch this
array outside of the find function oh
actually outside of the index and
outside of the find function and how
much does it improve performance and it
does improve performance now you can
call it 14,000 times per millisecond
which kind of makes sense you don't
allocate array every time you call the
index or find and then they wrote in the
this post they wrote that oh that means
a reallocation is expensive and and I
came into the comments I like coming
into the comments and telling people
that they're wrong and I said what about
if I make a function that makes an array
and instead of moving the array
allocation out I kind of move the
function that creates this array out and
I will start calling this function in so
logically speaking it should again drop
the performance down all the way down to
like four five thousand operations per
millisecond right
but actually doesn't it it actually is
almost as fast and this is what people
do when they see this they are very
surprised and here the choice is to
profile like you can use on Linux for
example there is a very helpful tool
called perf which is a
think that can hook into the cpu
performance counters and aggregate the
statistics for you and v8 has certain
levels of integration with this tool so
with the with the newest kernels you can
even get the annotations like per
assembly line essentially per machine
code instruction but with an older
kernel you at least get the per function
statistics and what does performance
counters show here that there are a lot
the names of functions in c++ they're
very long right they don't fit on the
slide but you can see that there is a
lot of stuff related to creating new
functions here it's a lot of C++ code
responsible for that because it says new
function you function you function new
closure so on so forth and if you
profile the version where the array is
allocated in this small helper function
you discover that there is no such thing
happening there but there is instead a
thing called fast new closure stop and
what happens here is very kind of
bizarre is in v8
there is this thing and written in
assembly essentially that can allocate
closures fast but it does not support or
did not support allocation of closures a
fast allocation of closures if the
closure contained an array literal
inside so what actually happened let's
go to the code back to the code when you
had an array inside of the index
function the allocation of an index
function itself which happens every time
that you call fine function was very
expensive it had to go to the runtime
and do all the stuff in the runtime and
as soon as I moved the array allocation
out then the allocation of the index
functions was sped up considerably and
even though that you're allocate boss a
function and and the array every time
that you call this the allocation itself
is not that expensive because it happens
in the new space it's just a bump of the
pointer it's very very fast ok so if you
run this code on the newer v8 and people
the v8 people actually taught this
handwritten piece of assembly how to
allocate closures with literals inside
so now both cases are equally fast ok
here's another very interesting example
from stack overflow
somebody wrote cha-cha 20 cipher in
JavaScript and it does not matter what
your 20s it's just some number crunching
stuff II yeah that's essentially it it's
it's the main part of the chat warning
and there is a helper method called
quote around here which is called a lot
of times in the loop and the person who
asked the stack overflow question he was
actually curious what happens if he
takes the implementation of this quote
around and in lines it into this
function that calls it so he measured
the the version without inlining and it
was like 20 megabytes per second I don't
know whether it's fast whether it's slow
for a cypher it doesn't matter for the
purposes of this talk then he takes this
implementation of this function what
around and just puts the the code into
this loop manually completely inlining
the entering this loop into a huge mess
of the arithmetic and he runs it and of
course the expectation is that it runs
faster well it actually runs ten times
slower so he makes the face like that I
guess and it's not very happy and he
says I heard somewhere that inlining
must improve performance why does it not
happen and I looked at this example and
I said well you have this function there
which does certain things to the numbers
and what if you apply the awesome GS to
it and awesome GS is this interesting
standard way you put the user as
meditation into your code and then
suddenly everything looks very runs very
fast so as I said put the annotation use
the Osman your code like that and the VM
will speed it up and lo and behold they
did speed everything up back to the
basically the same performance that was
there before and of course you need to
put an additional comment there which
says that to the people who don't
understand anything about performance or
virtual machines otherwise they will
remove this so and that's that's why I
really need to know things usually when
we say performance we used to mean
used to mean that you go to GS perv
dot-com and you write some micro
benchmark and you know everything there
is to know about the JavaScript
performance but you are a Jesper kind of
diet it's now in this perpetual state of
rewrite and you can no longer use it so
do you work for those of you who don't
know it like how many people ever used
Jesper from their lives here not that
many oh that's good that's good actually
you're not spoiled you're not your minds
are not corrupted by this thing so it
was this site where you could write some
code and then you press the button run
test and you get like operations per
millisecond for those tests that you
wrote and other people could also run
those tests and then the data was
aggregated and you could see in
different browsers different OSS some
average performance numbers but the
problem is that people write very
strange benchmarks and they don't
understand how to use it correctly so if
you base your decisions on this you get
really strange decisions for example
somebody tried to write this test which
tries to measure what's the fastest way
to count letters e in the text and they
discovered some very fast ways on
firefox you can see so this is
operations per millisecond or per second
so the higher the better and firefox so
there is this purple line that like is
huge like it's faster than anything that
you saw before in your life
it's like tesla of javascript and it's
this split thing up there so the fastest
way in firefox apparently to count
letters e in the text is to split it by
letter e and then take the length of the
array and subtract 1 it returns the
right result kind of but it's kind of
mmm what what actually happened here and
the other one that is also fast in in in
all the browsers not as fast as the
first one of course but still very fast
is to iterate over this string but for
iteration you don't use the normal like
for I from zero to length of the string
you start slicing the string with a
slice and this is kind of again strange
because what I would expect slice to be
slow
so it's really strange results and if
you take it like for the face value you
will make strange decisions based on
that and you shouldn't do that so you
should instead doubt everything like
literally everything you should doubt if
you're if you're going home we should
doubt whether you took the right bus and
of course you should doubt the micro
benchmarks so the micro benchmarks is
this like think that you try to use when
you want to measure what's the cost of a
single operation and how do you do that
well of course you write the code like
this you take the current date then you
perform the operation then you subtract
the current date you subtract the
previous current date from the current
current date right and you get kind of
the length okay simple enough but what
if the operation is faster than the
clock usually would expect that the
assignment to a variable for example
does not take a millisecond even right
well maybe it used to take a millisecond
back in the 90s but not now so the easy
answer is just to repeat the operation
multiple times and then divide the time
by the amount of iterations that you
took or divide the amount of iteration
by the time depends on what you want to
get as a result so this is the
arithmetic from the school but maybe not
everybody went to a school so I will
repeat a little bit so you want to find
the cost of the operation marked C so
you if you repeat it 10 times and then
you write it by n then and goes away and
you kind of get the cost right
arithmetic 101 so if you will look at
the example with a slicing of a string
one by one then what happens with this
example what makes it so fast is that
the GS per for the benchmark GS which is
this library underneath the GS perv it
generates the JavaScript code wrapping
this into the loop and maybe if we spend
an hour just staring into this
JavaScript code we'll figure out what
the back is the back is that you arrive
with an empty string on the second
iteration of the outer loop right you
slice the string slice slice slice it
ends when the string is empty and then
string is never initialized you just
repeat with the
with empty string all your operations so
repetition starts with a string equals
to the empty string and we wanted to get
mass like that but we actually got the
mass like that so we repeated an
operation once and then we repeat it
nothing n minus 1 times and then we
divided by n and you got the cost of
operation divided by N and the cost of
operation is very small number and
amount of repetitions is very high so
it's approximately it costs nothing
right so very simple math this is a very
common example of benchmark Jaso jesper
misuse when people don't take into
account that operation should take the
same amount of time every time you run
it and but it's actually not very easy
to achieve that in the modern world so
for example here another case from the
GS birth where somebody tried to measure
what's the fastest way to convert a
string to a number and it turns out
again on the firefox the fastest way to
turn string to a number is a double
tilde so if you know the intricacies of
the JavaScript semantics the tilde is a
bitwise negation right and the double
bitwise negation is not a negation at
all right it's just identity so when you
declare ok double tilde for the win
let's convert all parse int to the
double till them they're considerably
faster of course the answer is nope
javascript compilers the just-in-time
compilers they're relatively clever like
dogs they're not smartest people but at
least they can do move to your code and
then you can observe how the program is
running and they can optimize while it
is running so your math from the first
grade no longer applies really you get
at least an optimized version and
non-optimized unoptimized version
optimized versions together and then you
repeat to several time the unoptimized
version then JIT comes and optimizes it
so it gets a little bit complicated but
it actually is even more complicated
because there can be multiple optimized
versions like Safari for example it has
several optimising cheats and it tears
up
as your code runs and becomes hotter and
hotter and if you factor in all kinds of
costs like GC and compilation cost and
the math really becomes messy and you
can no longer use a simple math you need
to go to the University and take some
analysis so so you need to outsmart the
just-in-time compiler to measure
anything really and if you program
languages like Java for example then you
already have tools to do that for
example the Java has the Java micro
benchmarking harness gmh and it it had a
lot of engineering put into it to
outsmart the hotspot but in JavaScript
you don't really have these tools and
benchmark J's does not do much to
outsmart the JIT I will show you some
examples of optimizations now on the
slides but you should really understand
that I am going to show you them on the
source level but it's not how they
happen in reality
so they happened on the more complicated
intermediate representations but for the
purposes of the talk I'm ignoring all
the compiler theory and I'm just doing
what I want to do so what happens with
this benchmark with double tilde is that
the dog the dog JIT which is smart
enough it sees that I never changes in
the loop it's a constant really the eye
is assigned some constant string and you
just put it in there and then it says oh
but I know how to apply the bitwise
negation to the string full of numbers
and it does it and it does it again and
you end up with just assignment of the
constant to a variable which is not that
expensive it's essentially zero and this
is optimizations called constant
propagation and most of the cheats all
of the cheats these days and they do it
so you can say okay I can trick the
compiler I can trick the constant
propagation what about if I make the I
non constant just generate some string
based on the current time for example
and of course the JIT will be all
panicking and run away but no this is
the wrong answer McFly this is not going
to trick the compiler
so the compiler will observe that while
it is not a constant it actually is a
loop invariant so there is no reason to
perform this operation again and again
in the loop you can just move it out and
perform it outside of the loop and you
again measuring nothing you are
measuring an assignment of some pre
computed value to a variable of course
the computation is now happening in the
code but it happened in kind of outside
of like what you are measuring this is
called loop invariant code motion again
most kids do that
they can go further forward and then
they can observe that nobody is actually
using J variable and if nobody is using
J variable then J zero is not used
either and if nobody is using Chi zero
then maybe I is not needed anymore
either and there is no side effects from
computing the two string of the date now
so it is all completely dead so your end
up with an empty loop and of course then
you can even ask if you are like if
you're very smart kind of dog you
can ask why then there is a loop right
and then the loop also goes away and
this is called dead code elimination and
the bet is in the sense that it does not
do anything observable anything useful
anything used by the subsequent code and
the example from before with a split of
a string Firefox is actually smart
enough to understand that splitting is
constant string by the variable by the
sum call other constant string and
taking the lengths of the resulting
array it doesn't actually do anything
observable from the outside so this is
all completely removed and eliminated if
you assign this value to the two
somewhere then it's a different
situation so what you should have
figured out from this small intro is
that optimizers they eat micro
benchmarks for breakfast and for lunch
and dinner and in the night they also
wake up and eat them and some people
really want to create a to proof micro
benchmarks that the optimizer scanted my
recommendation is usually to not
that at all like just find something
more useful to do fix some bugs in your
code but if you really want to at least
try then you should follow some very
simple requirements you should try to
avoid this very common optimization so
wait constants and loop invariants in
the dead code and for the love of God
verify the results of the benchmark that
you are doing because many people just
write some benchmark but actually not
doing what they expect to do at all
because they don't check the results and
checking the results also combats dead
code stuff because you're now using your
computed values so we can try to fix
this benchmark by doing by applying
these rules we create two non constant
values and we start swapping them in the
loop to confuse the compiler so that the
variable we apply the double till the
two is no longer loop invariant and then
then we check that everything is kind of
good to use the result of the double
tilden operation and this is not bad
usually should be enough to actually
measure something but if you have a
really smart compiler and our hope is
that in the future all the compilers
will be very smart
then it's potentially still not enough
because they what compilers can do is
that they can take this loop and say hey
what about if I do twice as less
iterations but I repeat the body inside
twice so some compiler C++ compilers do
that all the time to small loops and
what happens now is that you swap and
then you swap again and that's as if you
never swapped anything so the values
again become loop invariant for this
loop and because they loop invariant you
end up with some dead code and some
invariant code and again not measuring
anything so this optimization is called
loop unrolling v8 doesn't do it this
light is actually like from 2012 I have
been giving this talking variance
variance since 2012 and it still doesn't
do it but I want to induce paranoids in
2012 in two people the fear of God the
way you should approach the codes like
you saw that VMs are relatively good in
optimizing things
so you should approach your javascript
code with the presumption of performance
so the presumption of performance is
that if you write reasonable code it
should be reasonable fast and if it's
not reasonably fast then the VM is at
fault not you but people also have the
thing called confirmation bias which is
they think that something is slow and
they write some benchmark and the
benchmark service and the their code is
reasonable but but their performance of
this code is bad and they say okay I
know this feature of the language is
slow so I will not go and I will not
tell anybody that this is slow I will
write a blog post that will warn
everybody else from using this language
feature for example there is this
presumption that prototype chains are
slow and people write benchmarks like
that they say okay I create a deeply
nested prototype chain how many people
know what object.create does I'm just
checking that nobody's sleeping there
how many how many people know what
object what how many of you write
JavaScript actually okay so not that
many I see I see
so object.create creates the object with
a given prototype and then if you nest
that you get a deeply nested prototype
chain there is a least tributon on this
slide as well so how many people know at
least at least yeah okay okay yeah
hooray for lease so and then they write
this triply nested loop for whatever
reason it's triply nested I don't know
and then they access this property on
the prototype of a prototype of a
prototype of a prototype and they
measure how much it takes outside of
this function call and then they write
another function that cashes the value
of this property outside of this loop
and you just use the variable inside and
then the state-of-art benchmark driver
is in the game as well which calls this
function the measures how long it takes
and you measure it and you discover that
indeed if you look up the property on
the prototype many many many times it's
eight times slower than just looking it
up once and caching the result and you
write a blog post this is literally
taken from a blog post which says don't
use prototype
they are slow with a big stamp on it
then of course I come and I say ok how
about I make it harder what if instead
of putting a normal property on the
deeply nested prototype I put the getter
on the prototype and of course everybody
heard that getters are slower than
normal properties and then there is a
prototypes in the thing so it should be
many millions billions times slower
right turns out it is actually faster to
make it a getter and people are really
flabbergasted by this and they say you
are a dark wizard or some sort but you
can look in the generated code so this
is the screenshot of my tool called our
Hydra which shows you what code is kind
of generated this is an intermediate
representation used by the compiler and
you can see that if you make it a getter
then suddenly it's inline into the loop
it says enter in line leaving line and
there is nothing in between those so the
because it was just returning a constant
right and then that constant is just
added to this counter thing again and
again so somehow once we made it a
getter it got in line and the same
wasn't happening with the normal
property it was doing this it was
calling this very scary thing called
load named generic and looking it up
every time again and again in the loop
so it turns out that the blog post was
based on the very very very old v8 which
could not handle the properties defined
on the prototypes as well as they could
kind it could handle the getters defined
on the prototypes so gathers it could
inline and everything was very good but
the property it would look up through
this very slow process again and again
if you try the newer v8 then everything
is actually kind of good and the
performance is the same because the
newer v8 can handle that well and when I
say you were it's from 2013 maybe
somebody was using note ole dot 4 dot
something which used the nation to eat
ok everything is good everybody
rejoicing and the the thing that you
should observe here is that the
prototype chain traversal code that goes
up the prototype and finds where the
properties good loop invariant code
moved somewhere out of the loop and
you're not measuring anything again I
essentially so okay let's do something
completely different
well actually the same so what if I run
the same benchmark twice like I measured
the same function twice for the first
benchmark and for the second benchmark
as well expectation is that nothing
changes maybe it becomes a little bit
faster because there is no cost of
optimization anymore
turns out no it's actually become slower
for some strange reason and I was
showing you before that compilers are so
smart and everything is good and you can
dance and be happy and write whatever
code you want but actually compiler is
also very complex so they have bugs of
all sorts inside and that's what I am
trying to show you that they're full of
bugs and you should be ready to face
those bugs so what happened so we can
again use the error Hydra and we can see
that there are many versions of the same
function produced by the optimizing
compiler and the red color means it the
optimized and I need you optimized again
and then there is a purple color which
also means you optimize but a different
sort of the optimization and then it
produced a version that is that was used
after that and did not be optimized
again so it's a the optimizations
reached a stable point but when we look
at what kind of stable point it reached
we find that it's a very strange stable
point so the counter variable which
accumulates the the some of our optional
prop access it is now boxed so you load
it from the thing you add it and then
you box the counter again and again and
the boxing so v8 uses this
representation of numbers of doubles
that is boxed so you store them in a
heap object allocated on the heap and
you store it in there and you allocate a
new one every loop iteration of course
that is expensive you produce a lot of
garbage and that's why it stabilized on
a much lower version so you can dig
through the bead bug tracker and find
the back
and what was happening is that the the
there is there was at the very end of
the benchmark the the line that was
printing the counter and it turns out
this line was causing all kinds of
shenanigans happenin in the v-8
internals and there was a workaround for
it you just need to hide the fact that
you are aiding this variable to
something for example you could have
forced converted it to string with a
method call and then this inference that
tries to figure out how to represent
variables in the code like which ones to
put on the machine registers as a
unboxed doubles for example it would no
longer be confused and then everything
would run just again just fast this back
was fixed so don't put two string
anywhere so most of the box that I'm
showing you they're fixed so you don't
have to use any work around here is for
something unfixed kind of so I can stack
overflow question somebody tries to
measure what is like how much it costs
to use getters and setters on the object
and they write this benchmark and I will
accelerate a little bit to make it on
time and on the newer v8 it's three
whatever three I know is it million I
can't read numbers million right yes
three million operations per second and
on an older v8 it is considerably faster
like it's eight hundred million right
and of course you are again very
surprised what's happening here because
pendo v8 people work work work and make
everything slower right maybe it's time
to retire so if you take this and write
it like remove the benchmark J's from
the picture and you just write this
function that that's what the benchmark
does define some getter setters and try
it in a loop again and again and you run
it once this function and you run it
twice and it slows down by a factor of
100 on v8 so yeah again our helpful
guide that is very surprised there there
is a thing called the fast
representation of objects in v8 and
there is a
to check whether your object has a fast
representation is to use these internal
functions you need to build v8 and run
through the special flag but you can
check it and if you run it with a
special FAQ and use that percent as fast
proper testing you discover that when
you call this function the second time
the object that is created is doesn't
have a fast or property representation
and that's why it slows down so much
there is a complicated thing happening
here I don't want to go into the theory
details but how you could work around it
is to call object or create on this
object
so there is some internal machinery that
says if you put an object as a prototype
to another object here when we are not
even using there is out of object dot
create then the v8 force converts it to
the fast representation again so you can
call it once call it twice does not slow
down anymore call three four times does
install then slows down so because there
is some polymorphous limit reached and
polymorphisms limit reached in the
generated code okay you can say okay I
will go to the old-school methods of
creating getters and setters I will just
write it manually like the our ancestors
did in 1900 so it turns out that's okay
but it's still not as fast as the the
fastest possible case right but at least
it does not have these strange drops
it's kind of predictable so you can then
turn to an object-oriented approach if
you were ever a Java programmer then you
can say okay object orientation is the
answer to everything if if one class is
not enough make two classes in the
factory and here I create a box which
boxes our value and then I have getter
and setter on this and then I create
this box oh that there is a yeah that's
how it is so it's it's almost equivalent
to our previous code and that's actually
the fastest way to do it in v8 v8 is
really optimized for this style of
object-oriented programming where you
have some classes and you have some
methods on the prototypes and stuff like
that so I don't know you're happy I'm
kind of happy but I know depends on your
programming style maybe you have taught
that
so that's why it's important to know
fundamentals and you can look into other
VMs as well so this talk is about to be
8 mostly and about to be 8 bucks but it
doesn't matter that you cannot do the
same for the other VMs so javascript
core you can build javascriptcore on the
Mac easily just check out some source
and build it and it also has this
drop-off when you call the first version
of function it's very fast very fast
then a little bit slower and that's
because you can again you can ask it to
print you the intermediate
representations that the compiler uses
in all kinds of stuff
and look at it and like if you know how
to read the v8 disassembly and stuff
like that it's easy to read the
JavaScript core stuff as well so there
are a lot of code I don't want you to
read it all I will remove all the stuff
that it doesn't matter but you can see
that in the first version it's just
check some preconditions and then it's
in lines the set called and Inter lines
the get call as well and it says known
kohli so everything is good but after
the third call in the third call that
says ok I will no longer do any inlining
I will just start calling through the
generic call method and then that's why
it slows down because there are some
counters like polymorphism limit reached
that's why it's slower ok the short time
we have a little bit of time I will I
will I will be fast so somebody tried to
measure whether the function call is the
faster than the method call or the other
way around so they created a function
that does some hashing of some strings
doesn't matter what it does and then
they put this function on the prototype
of some object in the Java Java style
again so you have an object with a
method an object doesn't contain
anything just this method and then you
compare you run this function on some
strings and then you run this method
which is just the same function on some
strings and it turns out that the
function is considerably slower and the
method is faster so object-oriented
programming wins again Oh to to the
object-oriented programming so we can
ask this question and please don't show
anybody what I'm going to show you now
it's it's
us go to attendees so if you take this
benchmark prototype setup function which
was setting up our benchmark and you
just add these strings inside there like
this video JavaScript with this one
weird trick and then you run it again
then it's the same performance between
function in the method so obviously
something is not really right here again
our helpful phase so if we look in the
IR hider what was happening is that
function benchmark was actually never
optimized at all so it was running in
the unoptimized version which is
considerably slowed and optimized
version and that's why it was slower and
there's an explanation but I all
summarized it for you as yada-yada
yada-yada
so the the the thing is that function
call and the method call were done
differently in the v8 and the
optimisation just was not triggering it
was not counting the right thing to
count and adding the adding the stuff
that I added just cause it to be
optimized because it started counting
the right things this is kind of fixed
so shouldn't be a problem anymore again
and but the thing is that we again
measuring the empty loop because if we
look in the ire Hydra what's happening
the whole hashing code was moved out
because the strings that we were hashing
were constants so all the XS is that the
constant indices like lengths is a
constant of if it's a constant string
and stuff like that everything was moved
out some of it were computed and fold it
so can it be the other way around so
this is the last kind of slides because
I'm running out of time so I work on
dark now and I work on the dart VM but
the people who do the compiler from dart
to JavaScript they come to me and ask
questions sometimes because I used to
work on v8 and they they were compiling
the for foreign loop into JavaScript and
they were producing this code and there
is this helpful check in there which is
checking that you are not modifying the
array because that's that's the the job
this dart semantics and you want to
throw an exception when you modify right
here in the iteration and they said
there is a check and it of
cost something so if we comment it out
its 18% faster and can you help us and
make it somehow 18% faster without
commenting it out and I said well no
problem
you take your code and then you write it
like that
so you replace the method call with
something strange here so if you know
just keep semantics and the coma
operator returns the the last evaluated
expression which is this function to
call and then you call it and this code
is never executed because this is just
an exception that never gets flying out
gets to fly out of this code because you
never modified the array and somehow it
makes it faster like I measured it and
it really makes it faster and just to
summarize so somehow we speed up the
code by 18% by replacing the method call
with some strange stuff that looks like
a person who's one I was like something
else strange happened to it and and in
the code that never execute and we sped
up everything so what is happening there
is that we does not know what so this
code was never executed and we it really
relies on the information collected by
the executed code to optimize it and
because it was never executed it doesn't
know what's happening there and the
method non executed method calls and non
executed property accesses were treated
differently in the v8 so I replaced the
non executed method call and within non
executed property taxes and it just
helped to optimize it it helped it to
understand that that was never executed
and helped it to understand that the
lengths never changes because like v8
can see that there is only array access
and no functions calls and in the in the
code so it can just assume that this is
all that code and make it that and yeah
so this is explanation doesn't matter so
I really recommend to take a look on
your algorithms first and then start
micro benchmarking and wait a second so
there was a first example with this and
a strange use awesome directive again we
can look at it in the our high drain
discover that the
v8 was continuously optimizing the
function where we inlined all the stuff
inside and there was the up market in a
strange place saying strange things and
if you see the code like that if you see
the situation like that where your
functions continuously being the
optimized and optimizations never
stabilize then you should just go and
file a v8 back really because it's a v8
back not your back and there is a gain
explanation doesn't matter there is a
more sane walk around and put in this
huge comment so the reason why the huge
comment worked is that because we ate in
relies on the size of the source code to
make inlining decisions due to some
historical artifacts which would be
eventually fixed I suspect there is a
much sane work around instead of
disabling inlining is to just add some
truncation to this load from the array
so please never assume that language
feature has to be slow do go and demand
from your like closest senators or
whatever and people in the parliament to
make jaws good features fast right the
bugs in the v8 bug tracker and VM people
are really your friends I am your friend
even though I'm not a v8 person anymore
so remember to rate this session
whatever thank you very much for your
attention
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>