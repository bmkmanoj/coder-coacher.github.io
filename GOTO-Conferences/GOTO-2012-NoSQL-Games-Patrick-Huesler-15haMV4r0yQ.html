<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • NoSQL Games • Patrick Huesler | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • NoSQL Games • Patrick Huesler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • NoSQL Games • Patrick Huesler</b></h2><h5 class="post__date">2013-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/15haMV4r0yQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone my name is Patrick I work
at Google and today I will talk about a
few systems that we built for our games
the databases that we use the mistakes
that we made and how we got out of it
but first let me ask you a question do
you like to play because I think we all
do because it's a natural things we
learn things we train and we can also
interact with each other there's a big
social component to playing we learn to
deal with other people and it's also
good for us to deal with other people or
our animals for instance and it's what
we do here at rouga we do social games
and casual games so we target users that
normally don't play video games reasons
like my mother she loves to play one of
our games and those games are for
instant like monster world where you
have your own beautiful garden and you
can help your friends and with their
garden and help them watering their
plants or diamond ash which is our most
successful title where you have we have
a few gems on the iPhone or iPad and
then you have to click the colors that
are the same that disappear new ones
come you played it for 60 seconds and
try to read the high scores of your
friends so now get to the technical part
of it the those games they provide some
challenges for us and to give you a
context of the numbers that I'm talking
about is monthly active users for
Diamond there so that's a number of
users that play that game in like that
gave me a lot of 30 days and that's the
number of users that play that game
yesterday so what does that mean in
terms of more our kind of thing is
requests per second so that boils down
to six thousand five hundred cars per
second for the game monster rule they
came in through the back end and that
might not be a big deal in general but
in our in our use case it's a problem
for the database
and the reason why that is it's right
having which means pretty much all the
requests or let's just say the majority
of requests that come in need to write
to the database so our biggest problem
is scaling out the rights to make the
database work with that and we have done
so in several iterations over several
different games we try to evolve in our
architecture to make that work and to
give you an idea how in case you didn't
know how the classic setup works for us
is we have one user the place it goes to
Facebook on a desktop clicks on a game
and it loads up the flash client that
talks to our a game back-end and all the
talks are reporting back-end which is
important for the business analysts so
they can do some data with it improve
the games run IP test stuff like that
the same we have for iOS games as well
so the first two games that they've
built that was actually before my time
were brain bodies and bubble Island and
they ran on lamp well actually not
exactly the thing they used nginx but um
and is a bit hard to pronounce so the
bottom line was my sequel does scale if
you looked at the numbers before from
Diamond - that thing actually still runs
on my sequel and it runs nicely so I say
for certain use cases
it definitely scales if you have to
write people if you have an engineer
that sets up the Machine correctly that
gives you fast machines and configures
them in a way that they'll fly and if
you have engineers that right smart
queries and think about how to insert
data and how they carry data without
killing the database now moving on to
another game the next game Monster World
they decided to build this one on ec2 in
the cloud in order to be able to handle
big spikes in traffic because Facebook
it can happen that all of a sudden your
game goes through the roof and they use
rails instead of PHP but my sequel was
still there and this is a fairly
standard setup that we
have they started out with my sequel but
shorted initially because they knew they
had to shot eventually so they wanted to
do it early on so they get that
technical problem out of the way so
there's a big story about how this game
scale and what the problems were and our
lead engineer yes / has a great story
and I think he calls it six weeks in
hell and it has a lot of gory details of
what exactly happened but I'm just
giving you the gist of it the many new
blocks the first fuse were actually
quite easy to manage and they always
found solutions to go around it but
eventually they ended up with something
like this now I imagine you have two or
three people that need to run this thing
and also write the backend code so
that's a classic death of thing we're
talking about so and those two people
have to write production code do advance
in a game and make the game better but
also run this thing on ec2 and the next
step is so the Hat is eight shorts and
it still it wouldn't keep up with the
rights so what I started doing after
that is well they don't know they
started thinking well the next logical
step would be going from eight to
sixteen but if you add if you sum up the
slaves as well you end up with what's
that 16 and 32 certainly two machines
did you need to handle and if one goes
out one master goes out you have to
switch to the slave and come up to the
new machine so that's a lot of
operational work and so it turns out
monster world was a different use case
it was not an arcade game so the lesson
learned for me personally here is arcade
games are not forming games like monster
world is considered the traffic and the
backend and the databases so they
figured well this is not going to work
if you want to like spend some time
actually coding apart from just dealing
with operational hazards so the light
that a tunnel that they saw eventually
was Redis
because Redis is extremely fast they had
right problems right so needed to deal
with rights but is also durable in some
sense not as extremely as my sequel but
you can have it persistent if you want
so what is outside of doing is they
started moving the slow parts out of
read out of my sequel into Redis
especially the parts that if you look at
a forming game you normally plant
something in monsters is a monster
garden so you plant something gonna pull
out your harvest that you plant it like
this cycle and this is what caused the
most traffic so they moved it out to
Redis and then gradually moved out more
and more it was hurting them and this
eventually saved the game monster world
in the back end and so they could move
on and start focusing more and other
stuff so the next game is the one that
actually worked on it's called happy
hospital and from what I've learned from
all the problems they had in a cloud and
needing to have tons and tons of
machines and having such huge
operational overhead we wanted to go
dedicate it because in my understanding
the cloud was just too slow for what it
cost us
there was like IO was terrible and the
CPUs weren't the fastest ones and they
got expensive and expensive but we
wanted faster machines as fast as we
could get as fat that I oh should be
fast the network should be fast so we
don't have to deal with this terrible
latencies between the applications of
rent and the database because the higher
the round-trip is the longer you
eventual response time is going to get
if you fire up more than one query right
so we decided to go with Redis and we
came up with this so the idea was have
very few machine but big machines and
just have everything as fast as we can
and we stored all the data instead of in
a classic bicycle way of taking the
users data and the users has in happy
hospital for many of patients so instead
of putting the patient's all over here
and the rooms all over here and the user
here but then we need to use the user's
rooms
have to go here through all the rooms
and look down and then you need the
patient scroll down here we would have
one compound user so you have register
key the Valiants already hash and
everything that belongs to one user is
in there which turned out to be really
powerful because we could easily move
that user around between environments
from staging to the production and act
for impact and force which was extremely
good for debugging for testing because
it could just have a bunch of JSON files
or llamó files that just restored to
your sister to your staging user for
instance and and see the world like the
user that had a buck right and so and
also in case of shorting that's quite
easy because you have to choose short by
one key and you don't have to short all
your models as well and so we were fine
and the family was fine as well but we
ran into some robots too and those were
the interesting ones
first of all ready started to do
something weird because we would
expected a growth of memory to be a
linear to the growth of new uses take a
map right every new users is a certain
amount of memory and it should grow
linearly it did that until a certain
point and all of a sudden and many read
his memory girls just went through the
roof
and we didn't really know what was
happening and we couldn't really
reproduce it in a development or staging
environment he was just happening of
production so eventually and we also
figured out if he would restore ready
stamp into an empty Redis database then
it would consume much less memory than
the actual production database did so
that started become a problem because we
were slowly running out of memory on our
machines which we thought it would hold
up for maybe half a year or a year
another thing that trip does is all of a
sudden we had stalls in our system all
the way through the stack so on the top
and if you look at response times from
your application and all of a sudden II
which is stall for one second response
times go up and then cripple down again
and some time later on the same thing
would happen and we were looking at all
the things and all the cues that were in
there and we could
figured what it was until one of her
engineers said well whatever it is it
has a clock and it turns out it was
periodic what was happening in those
stalls so as it turns out there is a
scenario in how red is does the
background safe for the fork that it
does can stall for up to one second so
this is what we were experiencing it was
and the problem was also we would run
that on the master database our
backgrounds dump for backups instead of
the slave so we moved it over to the
slaves and that problem was solved but
eventually we could no longer do dumps
and first year the quick trick is always
make sure that you check whether your
off-site backups are actually working
and they're actually pair and actually
have a certain size in in memory to make
sure that they are there because for us
they just started disappearing luckily
we were monitoring them so what happened
I think it was my misunderstanding of
how copy and write works because I
thought well Redis is forking a process
and then it uses copying right to write
out the dump of its data so it never
should reach the point where Redis uses
twice the amount of memory that it
actually that is already using I thought
that would never happen as it turns out
under a certain load this is actually
possible so what happened to us is the
operating system scheduler started
killing that backup process before it
was done writing so that's two the
biggest wrote what we hit with that game
and it was terrible because now we had
no backups and we had a master in his
slave but if anything happened to them
they were no longer persistent if
anything happened to them it would lose
all that ada so after I've done some
crying it was actually really sad about
that I wasn't even sure if we were
actually get fired for a stunt like this
because of I think it was my fault
anyways I didn't get fired and we came
up with a solution that I call restoring
demand what we actually did is we
started archiving users because we
figured a social game you might have a
lot of zombie users as I call them
they start playing the game once and
they never come back but they still
occupy space in your memory or you and
also users they start playing for 20
minutes they don't then it don't play
for a couple of hours and then it come
back and play again but in the meantime
in most of the time you don't need those
users unless some of their friends is
interacting with them so what we do we
will archive all users and only when you
started playing we will load them from
disk into Redis then they would play
there so essentially it's like a write
through cache you can think of it that
way but after dumpling we've write you
to disk again and that worked remarkably
well because instead of having millions
and millions of users in your register
base you only need a few hundred
thousand basically for the user that
actually playing concurrently right and
he eventually moved that to disk store
which is an experimental branch of Redis
that never made it to production and is
not officially supported but we ended up
using it and it works really nice for us
we even start a wrote a version of it
that instead of writing to disk writes
to s3 but we never tested that in
production so moving on in the evolution
of our games the next one was called
magic land which was a medieval city
builder kind of things and the guy
although who had to design that system
he worked previously on monster world
and he decided to go stateful and here
in a great idea how cadets of course a
Captain Obvious statement but
essentially he wanted to get rid of the
database in the sense we were using it
so each said well we do it stateful and
we write to as three in the back end we
add to a three as our storage and we run
everything on ec2 so no database scaling
and write issues because if you are
stateful we can write to a three
sometime if you want to we don't have to
execute the writes all directly right we
can unload them up and send some state
to a three and he decided to build it in
Erlang for various reasons first Erlang
is well known for being excellent that
distributed computing accent that
concurrent
computing and it's really cheap in
Erlang to get new processes right
because they're not system processes so
you can have thousands hundred thousands
maybe millions of processes of modeling
machine and if you look at that stateful
context you will see why that matters
because let's look at how it would work
on the back end if you start playing the
game you come to Facebook you log in the
flash plant loads up and the issues a
login or set up call to a coordinator
note which assigns it a worker process
response one which then acquires a lot
to make sure nobody else is writing to
that user on s3 at that time and then it
loads that get fetches the data from s3
loads it into memory and keeps it there
and from then on the client keeps
talking to that worker and all the
rights go directly there and the
synchronous in the background we also
write it to a three so that we don't
lose most a lot of the data in case
everything dies and logout works
basically in Reverse the desciption
timed out in some way the lock gets
released and we write the user back to a
stream and that is an excellent solution
and I think a remarkable one from those
two guys that built that because it uses
very little service on ec2 and it is
stable and it scales extremely well I
think at some point they managed to get
the whole game through one application
server but it was just for bragging
rights that they don't do this in
production most of the time so is does
everybody know what Facebook's Spartan
project worse Facebook is part of
Project the way I understand it was
essentially that Facebook wanted to push
html5 mobile and wanted to create an app
store on their iOS and Android
applications right so the only thing
they could do this of course they had
web views inside their iOS and Android
applications where they could launch
other apps and display them so and then
they want to have mobile games that's
why the partnered with us so the idea
was to write a mobile game
and we decided on an extension do
magical and that's called Madeline
Island the problem there was the that
lines that was given we were only two
people at the deadline we were given is
in more than weeks instead of months so
I started getting nervous and they were
talking huge numbers in expected traffic
they were saying I don't know it was in
the in the millions on the first week
and I don't know I was just huge so
after I was really confused because
there Here I am
there were two people what a product
manager the other one is me we need to
build an html5 game on mobile which is a
feat by itself but I also need to scale
it and make sure it handles the traffic
on a scale that was never seen in our
company before I ended up using nodejs
because I thought well since I'm using
it in a client already and we probably
will hire new JavaScript programs to
help us out which we eventually did we
had some great guys to pull that off so
I decided to do no chess in the back in
his wall so we can seamlessly switch
back and forth so that was settled so I
needed the database that requires very
little operational effort and that
scales without sharding so I don't have
to deal with just I add one more
actually to add two more to scale with
the problem and that has master's
replication and yeah so and it could be
this needs to be the fastest kid on the
block because which I was rebuking also
we could write to the local storage but
he also could have a synchronous saving
so it was fine it just had to be
reliable and no chairs is actually since
its event that quite fine which is
sticking around and waiting for a
database response so I chose react and
that was good enough for for that yeah
need on ec2 and I said well that's good
enough for now
and this it turns out it was good enough
for good because here's the kicker the
project never really took off in a scale
that
thought it would be so it meant to an
extent that it actually got stopped and
we released the game open source as a
mobile as a retention iam attention into
pocket Island so and everybody needed it
but anyways through with that
operational time that I had with react
have few lessons that I've learned first
of all it used react is a simple key
value store it didn't do any fancy with
it I didn't do any fancy data structure
complex data structures or complex
queries that would come along with it
essentially it would store something
like that and so my experience in
general ryoga's everything was a tank it
it's fairly fast it was a lot fast and
expected and it's super robust like
little to no operational hazard he was
just fantastic and it was exactly
actually what they said it would be so
it was really happy about that and the
fact that you don't have a single point
of failure in your database it's just
amazing because I don't want to get up
at 4 o'clock in the morning because the
master just died now I need to switch to
the slave and set up a new master and
deal with this with via let's just say I
have 5 notes if one note dies I probably
will just sleep through it and restore
it in the morning I'm going to be fine
another learning is I use spit cast as a
storage engine and that might not have
been the best idea but I never had to
pay for it because it wasn't we never
reach the amount of use but it would
have become a problem via because the
u-men you Keys need to fit in memory and
if you remember what I said earlier
about a lot of zombie uses that we have
and a lot of people that would just dry
out this html5 mobile game or it's it's
nice and then they never come back
so there's a lot of user like that and
you keep them around in memory as a key
at least so I thought eventually that
would have become a problem
lucky for me it never did and yes I'd
this is an older version of react I
don't know if that's true for new
versions of react so what I wanted to do
I wanted to back up my system and if
that I did is about just to MapReduce
over a whole bucket
essentially what react did is it will go
through all the keys and remember react
is a distributed system so he had to
coordinate a lot and then returned all
those keys and I almost killed a cluster
like that but it didn't so I was lucky
there and this is a development problem
or less than a production problem but
locally especially in America limit is
fairly low in the 200s and react when
you say react started he comes and said
well you should adjust you limit and I
was like yeah it's fine because I kept
thinking connections connections India
can accept I wasn't thinking file
descriptors that via means to write to
you because I didn't think how leveldb
for instance works it has a lot of files
open files that it writes to all the
time so the problem was even though it
says you should increase the limit it
doesn't tell you that it didn't start
HSS went fine but it's not saying I
couldn't start on the standard out you
have to actually go through the lock
files to figure that one out and here's
my favorite story so Cooney actually has
used react alright so actually I think
the story is new is new to do okay so
with vehicle you can attach to a note
and get a shell and you can change you
can call functions and change modules
and change codes it's because it's
Erlang but what happened is what I
normally what how normally get out of an
Erlang shell is by typing ctrl C twice
so what happens if you do this so every
I get that shell is you just killed them
out and so I killed a whole staging
cluster before I started to figure it
out I was already upgrading on
production and I think I killed one or
two nodes until I figure that out so
yeah that's not thing so how am i doing
I wanted to go yes the next thing we
started working on is monster mobile
remember that farming game that I talked
initially about it dad had scaling
problems with my sequel build my sequel
and eventually moved to Redis so this
this time I had plenty of time on my
hands
to get this one right so I thought but
well up until now I think I'm still good
so I looked really looked into how other
mobile games are doing this because it
was our first farming mobile title in
that sense on iOS that we are working on
so I looked at other titles from Singha
and other competitors how they are
managing the protocol how to deal with
flaky connections how to deal with
clients being offline how they deal with
that you cannot have the same request
pattern that a lot of the desktop
clients have where like every little
action trigger is a request to the
server but you can only do this with a
broken connection on 3G or all over just
setting up an HTTP connection is quite
expensive you can't just go around and
set up HTTP connections all the time
it's just not going to work and and also
looked at how cheating works because
normally on on facebook we would look at
the back and it just validated
everything on the back end because if a
lot of control over there and on iOS
people you have a local desk and people
can write to that and so looked at they
would cheat and turns out it wouldn't
attack the protocol the cheaters that
whose videos you would see you on
YouTube they would attack the local
storage and change configuration values
and other funky things like that lesson
learned for me was first of all I know I
was client is not a flash client I but I
also learned that my monster real mobile
game is is not an arcade game so I
couldn't just copy and paste what our
arcade mobile game was doing so
essentially it was asking
that question do I need a database at
all I didn't want to go stateful because
it didn't expect to have a lot of
traffic because here's the thing on an
iOS device you are in full control so
you can only have a local disk so you
can batch your updates in let's say once
a minute or go in higher depending on
the demands of the game so you don't and
essentially so the database what that
means the database doesn't need to be
really fast they can be fairly slow it
just needs to be extremely robust but
you can even take it a little bit
further and say well let's just keep the
back and simply do not the staple thing
because it's wonderful but it adds a
little bit of complexity just keep it
simple and stateless and just write
directly to s3 right just have Erlang or
nodejs or whatever sitting as a thin way
around a3 and just reading and writing
for 3m make it shake check some stuff so
that thought definitely crossed my mind
and I'm not sure what are we gonna do
this but it's definitely not across my
mind and also I forgot the slap was
there sorry the next question I have
came up but do I even need a back-end so
in attempt to optimize myself out of a
job
I could actually try something like this
and essentially have no data base in
that sense for that I need to scale no
operational overhead
no back and I need to write and just
manage everything from the client in a
smart way at this point I'm not sure if
this is actually possible but I will
certainly look into that and I think
I've finished really early today okay
that's all I got</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>