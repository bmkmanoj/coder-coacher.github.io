<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • The Microservices Journey from a Startup Perspective • Susanne Kaiser | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • The Microservices Journey from a Startup Perspective • Susanne Kaiser - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • The Microservices Journey from a Startup Perspective • Susanne Kaiser</b></h2><h5 class="post__date">2017-03-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1BKp616GPQE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm talking today about the journey to
micro services from a startup
perspective and when we talk about micro
services we hear and read a lot about
success stories from very large
organizations such as Netflix Amazon
eBay Spotify just to name a few and when
you look at those success stories you
might end up saying well well that's
great what's there just let's copy that
well the problem is that what works for
one company well might not work for you
so the journey to micro services is
different for every company and as
Adrian Cockroft the former Netflix chief
our cloud architect said is if you just
copy the results you might miss and skip
the process behind and these results and
to the fact that you will not receive
the the results that you that you want
that you expected so the journey to
micro services a deal was trade-off
there's no fast golden rule that you can
easily apply and just to give you a
brief overview what we do and who I am I
am in Susannah Kaiser the CTO of just
software we are located at some holy
Hamburg here in Germany nassif Germany
and what we do is just software we
develop just social that's a b2b product
providing providing app for
communication collaboration and an
integrated solution to simplify and
support teams working together more
efficiently and let's go first before I
go to our our journey let's go to the
beginning and a couple of years ago when
we started our startup and everything
was really small and cute in a way so we
were working with one team on one
collaboration product which was
implemented as one technology it was
what was one
code unit in within one technology stack
so the thing is that after a while
everything was in evolving so our team
got bigger and bigger we added more
features to our product and the number
of users increase so that's supposed to
be very fantastic but on the other hand
we reached a point where everything was
feeling kind of clunky and and slow and
from the organizational perspective it
was the case that everything took too
long to get things done for example our
meetings or discussions our decision
were more time-consuming than before and
the thing is that also that we really
well-defined responsibilities no one
feel responsible fed responsible for
example when it back arised so the
process was becoming slower and slower
and our productivity suffered while
adding more and more features to our
product the usability became kind of
complex and so the usability and user
experience suffered from continuous
feature amendments so instead of solving
our users our users problems efficiently
we were kind of confusing them and also
related to our software architecture
everything was tied together so we had
no opportunity to release independently
and fast and when we do have changed one
part of the system might have affected
also the other part of the system so
there was kind of need to evolve that we
need to shift and split things up so we
had division that we then split our one
collaboration apps and multiple smaller
collaborations apps so taking care of a
specific problem and solve this one very
well and so we established separate apps
for sharing documents for communicating
in real time for for communicating for
exchanging knowledge information in news
so the idea came up to have different
collaboration apps for a specific use
case in the meantime we will also
our one big team in smaller smaller
teams and two was a long goal was the
goal in the long run to achieve well
autonomous teams with well-defined
responsibilities because what we want to
achieve is that every team can work on
different parts of the systems without
affecting the other system or without
affecting the other teams so and so what
we did is that we then assigned to each
team specific parts of the Pacifica
collaboration F so that they can work
develop independently depending
dependently on the on the product itself
so but we I still have to face that we
still have a monolithic software
architecture so how do you can we
reflect also that we have achieved in
our organization structure in our
product now along our software
architecture so we have to wanted to
split then in our our software
architecture and micro services because
we to receive autonomy you had to get
rid of all your dependencies and so at
first glance it looks pretty easy well
yeah let's go ahead let's do that let's
introduce micro services but the problem
that arises is that micro service has
come with complexity and these
complexity might not be obvious at first
glance because now you're you're
communicating over in a distributed
system you are now communicating over
the network and the network itself is
unreliable and slow so your request
could be very slow and you can for
example wait and if it definitely for a
response
it could block your resources and you
can also it can also fail at any time so
you have to add complexity to d2 fail to
handle your failure and also in a
distributed system
it was micro services you are
partitioning your data that means that
you are losing your transactional
consistencies across services so
you have to deal with eventual
consistency as well and since you are
now not only dealing with one modulus
but instead was multiple moving parts
your operational tasks are getting more
complex so monitoring and deployment is
getting more complex and also like
debugging has become tricky because you
have to know to figure out like you have
to consolidate your legs locks across
service boundaries and in fact that you
when you communicate over the network
you have to establish api's and how to
make sure that these api's don't do not
introduce breaking changes so that you
return that didn't result in a brittle
system and along with all these
complexities is that your skill set and
also your tools are different than
monoliths so this has been just it as
well and another question that arises ok
and the moral is here always have the
authentication authorization in one
place and now if you deal with multiple
parts of my micro services how shall the
authentication authorization
authorization be handled and for sure to
develop efficiently you don't want to
want to deploy all these micro services
in your development environment instead
you need your development environment is
more complex than monoski world there
are not only not complexities that come
along with with micro services but also
with the transformation phase from
monitors to micro services because I'm
tingling and mandalas is really really
hard work and it's really time consuming
so the transformation process takes much
longer than anticipated so when we
started with it
it took at least double the time that we
had originally anticipated and we also
miss deadlines that was a very very big
disadvantage and also like if you have a
core functionality with a lot of
dependencies it's not really easy to
untangle that one and one thing that you
need to consider is that you still have
a running system it's a brownfield
project because you have your money
mills and you have to maintain and run
it while you are transforming it so you
have to share
results in terms of in terms of capacity
but also in time and which is also very
important is that all involved parties
has to be on the same side they they
have to agree and be on board that you
are going to do this past that you're
transforming your models into micro
services because if you don't are if you
are not on the same board then it's much
more time-consuming
well reflecting all these complexities
and challenges you might end up saying
well is micro services world or
Microsoft's architecture good for me
well it depends so definitely down
partition without a good reason and it's
not a good reason just to introduce it
because it's trendy and hard right now
and you need to definitely find a good
reason to say well we would like to to
to go this path and for us the reason
was that we it was organizational driven
because we're splitting up our product
but also our or our teams and smaller
teams and we want to have them working
on different part a different part of
the system at a different speed and we
want to make them we want to enable them
that they can make their own decisions
what how to implement it and when to
deploy it so we want to release changes
quickly and independently without
affecting the entire system well when
you have decided well let's go this we
have find a lot of good reasons to start
with it
now the question arise like how to start
how do you have the huge monitors in
front of you and like oh gosh how can I
just pull out it was a single part of it
and what we did before we did it we had
to keep some key concepts and minds that
it's really important for for doing for
doing the step because the key concepts
of modeling good micro services is that
you have to guarantee a high quizon
within the service itself and a loose
coupling between those services if you
don't follow these key concepts then you
won't achieve the bed
it's of microservices to have
independent parts to be developed and
also released and that means for example
if you change a part of your system you
want don't want to affect we affected
other parts of the systems and if you in
terms of like high cohesion was in the
service it means that related behavior
shall sit together so when you change
one this behavior you don't want you
want to modify it at one single place
you don't want to change the behavior
it's totally different part of the
system because this means that you have
to rebuild and redeploy the ID all the
services that you have changed and then
your deployment process is tied together
again and that's quite that's exactly
the opposite that we want to achieve so
loose coupling and high cohesion are
very important for for keeping in mind
while extracting micro services out of
your monolith and I was talking about
high cohesion and related behavior shall
sit together and one thing is that you
can then try to identify the bounded
context I guess there's a workshop on
Wednesday talking about demand and
driven design and it's kind of a
principle from remain driven design that
you try to figure out the boundaries
where a specific responsibility is
sitting in a specific behavior is
sitting in and you want also create kind
of likely understanding what shall stay
consistent in which part of the system
can be developed independently so these
bounds concepts are very important for
extracting marker services out of
immoralist because they provide a kind
of seam and where you cut out them the
the part of the micro services are from
and for us the bounded context world our
collaboration apps and so we were
thinking okay let's start with it and we
have product driven bounded contacts and
so we started with extracting one
collaboration app and that was our just
drive
application and so what we did is the
just drive application calibration Eva's
taking care of document management and
so what we did is that we established
this or implemented is as a coexistence
service to the monolith and it's not an
exact equivalent to the original
functionality but instead it has more
features and instead has its own
different user interface because we were
working on user using UI new its
improvements and also has a different
data storage so what we did and we
rebuilt our own service as a coexisting
one but from scratch which was not good
by the way and so what do you do so this
is then that the the just drive app is
then has a date or authority over the
documents because you exchanging
documents in our test drive app and it
remains it keeps the data authority on
the documents that means it's the only
service that can read but also write on
this on this data and for each document
you have an author and the author is
information the profile information is
an extract of the profile information
and to avoid that you retrieve every
time when you're displaying a document
the profile data from the from the
monoliths because that's the one that
has a data authority over the profile
data and to avoid requesting this
profile information every which are
every time you're displaying a document
you we are keeping a redundant copy it's
not an exact copy to the extract of it
so and this is ok as long as you do not
undermine the data authority of the
monoliths that is this one is still the
one that modifies the profile data but
now you have a new complexity because
you have to make sure that these data is
and thing and to do this you have to
establish a kind of linking mechanism
what we do is that we are using as an
Cronus communication style between them
and the and the new service and there's
a sour message broker so we're using
Kafka for that one and so whenever a
profile is updated in the Monell is it
gets published to the CAF it's a very
message broker and our model is then
subscribing to this event and updating
its replicated data accordingly so well
that was good at the first part of it
but after a while it took too long to
get this restricted because what we had
what our problem was is that we did too
many steps at once so we established a
new user interface because we're working
on our UI and UX approvement we also
establish a new data structure we
because we had some requirements that
our document data structures obsolete
and we have to to add some some new
structure as well and we also introduced
new technology and in the meantime we
still have to maintain our system and we
have to extend our skill set accordingly
to move too much all these complexities
so it was really heavy undertake and we
decided well this is not the way that we
can extract our services it's really
time consuming and it's really
frustrating because you're not getting
results very quickly so we followed
different strategies then so with the
next collaboration app we are with
splitting them in steps and we were
zipping it from top to down and this
probably something that works for us by
we well I don't know if it works very
well
but for also it wasn't was a good
approach because when we the next
service that we were focusing on was the
drive they the chat communication system
and what we did is that we were
extracting the web app first so
introducing moving the web client to its
own service and introducing then a rest
client and the rest api to you to
retrieve the data from the monolith so
that's only and it's not the final final
processes it's just the start of the of
the extracting process and but
this allows us to to develop to develop
on the web app independently and employ
it independently and iterate on the on
the UI independently so we get four fast
results that was helping us a lot so the
next one then is that we are then
zipping it a little bit more down and
then we came to the Westons logic and so
when extracting business logic it's it's
kind of more work to do because you have
to untangle the dependency in the
monolith and I'm saying that the
dependency in the monolith means that
the interfaces that you are talking
we're talking to you into your new
services has now to be addressed as API
stand so you have to establish api's and
if it's a be directional communication
style then you have to to introduce it
on both ends and so this kind of like
yeah more more work to do than just
extracting the web user interface so but
what we still see here that's this step
that they still share the same data
storage so when coming to the next step
we were extracting we were splitting the
data storage as well and if you do this
then you as I said in the very first
beginning about our drive app you want
to like to keep data redundant because
you don't want to request the monitors
for every information of the other
domain in a synchronous way so we would
like to keep data redundant and this
means that we have to keep them in
things that we have to establish a
mechanism to keep those dead ends in so
in he again that's a synchronous
communication that we introduced then
when we split the data storage but also
you know communicating over over Kafka
to publish modification events
okay the question in general arise is
like which one of the own contexts shall
we use first to extract marker sense out
of it and well there are kind of like
rules not really rules but kind of like
ideas how you can do it it's the first
one is like go for the low hanging
fruits it used the one that are easy to
extract and these are mostly those that
have either not so coupled in the
modulus itself that has very little
dependencies or that are already
communicating as in chronically so and
if well you are starting with those that
are easy to extract you can learn while
you go so that means that you can extend
your work skillset during the extraction
process and also become collecting
gathering experience in in micro
services but also in the transformation
process itself then another idea to to
pick those that would be good candidate
for for the next step of the extraction
process is that you that you identify
those that are giving you the most
benefit for example these are those that
are for example changing frequently
because if you extract those you can
develop and deploy them independently
and release these changes these frequent
changes independently without rebuilding
and redeploying the entire system so it
gives you a lot of benefit and the other
one is identify those modules in your
monolith that are using different
resource requirements for example is
this one taking a lot of disk space or
is it other taking a lot of memory
consumption and you can then split those
services in order to have a system that
you can easily scale because if you
extract them in an independent service
you can then scale it independently
and another thing is stop making the
models bigger and it's kind of like yeah
very checking
okay we have to meet a milestone just
edit to the monoliths and but the
problem is here while you're carving out
your bounded context from the model is
to make this monoliths shrink you're
then adding new functionalities on top
of it and so the transformation process
itself is slowing down because your
Manos is not getting smaller and so
approach could be then to implement new
features as micro-service and when it's
whenever it's applicable so and after we
have extracted your services which was
also very very important is that you get
your CI and contingent integration and
continuous development delivery pipeline
ready because you have to dealing now
with more moving parts and it's very
it's crucial to establish automate
testing automate and delivery and so for
for every deployable
artifact you are establishing your own
pipeline and this pipeline looks like
like a monolith but what we are
reflecting in this monolith and in this
pipeline for micro services are the CDC
tests the consumer different contract
tests and what they care take care is
about your API because now your do you
are working in a distributed system and
your your micro services are working
every in API or communicating over an
API and the problem on the complexity
that that could could happen is that you
introduce breaking changes unconsciously
and then this result in a brittle system
so the consumer-driven contract tests
they let you identify potential breaking
changes and what we use is
is a chess framework called detect have
you heard of Peck before is someone
using pay okay
because Peck allows you to divine a
consumer contract and a consumer
contract is a negotiation between
consumer and API provider and this
consumer contract is recorded impact for
example when you are running your
consumer tests automatically it what
what it does is that it generates a
provider step out of the consumer
contract and delivers the responses the
consumer expects so you can use it these
consumer contracts to add two different
levels so you can use it as a provider
step and provide us up on the consumer
side but also you are generating could
on the provider side you could also
generate requests out of this contracts
and to test then you're generating
request and test it against your API to
check whether the responses are
delivered by the provider that the
consumer expects so mmm another question
is where you for example keep this
consumer contract stored it could be for
example part of your repository but what
we have established is a PAC broker that
allows you it's a it's a central storage
space storage server we can upload your
consumer contact and also tagging and
versioning it so it allows us to access
just either to store it but also to
retrieve it from up on the provider side
so what is pretty different for us and
also challenging for us and this is
probably not applicable for for other
companies is that we are deploying in a
polyglot environment that means that we
are providing our solution as a SAS
solution in the cloud but also there's
routing it is on-premises products as
well so we have to do a lot we have to
face a lot of trade-offs between those
environments because on the one side we
can have a service instance per house
with just guaranteeing isolation
isolation for every service instance and
on the other side on the premise
solution we don't want to have it
because we would like to reuse us use
our resources efficiently we have to
deal with specific requirements or
specific restrictions on the on the on
premise solution so there we are after
you deploy it as multiple service
instances per host or even per process
for example if you run different
services in your Tomcat application
container and so these are kind of like
contradiction or requirements and we
have to somehow face it so what we
establish is then that we are providing
packages for for Debian and also for as
operative packages and what what we
these packages are then executing
ansible script and these are
provisioning than your system so
whenever you for example you install it
on your on-premise solution and you then
ansible takes care that it it it deploys
your micro services on the same tomcat
instead of establishing a new one
so in the beginning I was mentioning
like the complexities between concerning
design for failure and so now we get a
little bit into the details because if
you communicating over the unreliable
and slow Network you have to to take
care that you introduce a timeout
handling because you don't want to have
requests that are waiting indefinitely
or blocking resources so you need to
establish timeout handling and in
addition it's also a better approach to
provide for bags in terms of when your
service is not available anymore
other other thing is that you have to
consider to to keep your system
resilient is to limit you a number of
outstanding requests and this is kind of
like that you measure the threshold how
many requests outstanding at a specific
threshold then you return a failure upon
a response immediately because it's kind
of indication that additional requests
the terms will be pointless and also
circuit breaker it it's kind of like
monitoring your error rate and it's
tracking your number of successful
Kressel and your failed request in a
specific special the quick circuit
breaker kicks in and from that point on
the additional request attempts will
then return and very quick and fast
failure instead of waiting indefinitely
so and this decline itself then do after
a specific time out then do recent
release requests and if they are were
successful the circuit breaker gets
deactivated this sounds really complex
and with the good pointers and you were
that there exists libraries for this and
you don't have to reinvent the wheel so
that's something that we also have
experience like oh damn we have to take
we have to take care of so many so many
complexities and we were like kind of
like reinventing the wheel in every case
and then we was
singing well wait a second we are not
the only one having this problem so
there must be libraries around so we are
now try to establish then implement
history in in this and our environment
we have not we're just about to start
right now and so I would be very excited
if some of you have already experienced
this business library and to share
experiences about this and as I
mentioned also one of the complexities
is that your developers needs to have
full access to to fall to a runnable
system and it should be yeah equivalent
to your production environment and so
what we provided is that we are for
every developer we are providing an
development environment consisting of
vagrant and VirtualBox where each
microservices microsomes is established
and implemented so that you can still
implement or work on your in your IDE
work on your own service and all the
other services are already installed so
you know your your developers don't have
to to take care of the setup of each
single micro service from scratch
so it's a summary yeah due to the fact
that we have to deal with a lot of
complexities and challenges don't
petition without a good reason and if
for example if you're just a small team
and and you always deploying your your
products as one one chunk and you don't
need to need to release changes quickly
then you can also implement a monolithic
and modular ma notice it's it's a it's
reasonable stitute and to do it this way
as well so but for us it was very
important to release changes quickly and
we also wanted to reflect this in our
organizational structure as well then to
carve out your micro service from your
monitors you have to identify your
binder dominant context first that
allows you to identify those themes that
you that you can cut out your your
specific parts of your monolith and then
don't do this mistake that we did in the
beginning because like going doing too
many steps at once do the split process
and many manual steps and for example as
I mentioned for example and zipping from
top to down starting from top to down
and then and gather your experience
along the way get your building delivery
pipeline ready for from day one because
this is very crucial in general for
development not only for site micro
services but because now you're dealing
with more paths and moving parts and
this is more crucial than before and
which is also like sometimes you have to
keep in mind that it's not only a
technical shift but it's also a cultural
shift towards DevOps culture and if
you're not willing to do this cultural
shift as well it won't work for us
either
and so not only a product but also a
software architecture is made with
laughed sweat and tears to be honest and
now our software architectures also made
with micro services and thanks a lot
thanks a lot for the talk so there are
some questions from the app one is how
big was the monolith in lines of code
less than a million we had roughly how
three hundred to four hundred thousand
thousand lines of code approximately
cool it's the car itself it's Java it's
running in Java education okay and the
next question is how long did the
transition to micro services take it's
not finished yet it will take forever so
well we started well it's kind of like
there's no real kickoff like now let's
do micro services it had already started
with small ones like one year ago but if
you have seen like these like the
collaboration apps are more like a
self-contained systems because they
provide their own your user user
interface they have their own business
logic and provide their own data storage
so they are a lot larger larger parts of
it and this we are doing since the
beginning of this year size seems to be
pretty important so how many developers
yes not so many eleven okay what library
are you thinking of using for
implementing the circuit breaker yeah
that's the history that we are now
facing so if someone has experience
isn't that one or just another one just
let me know I will
let's share experiences okay did you
start to cut and paste the code from the
monitors to the micro services for the
chat app yes that's the next one but we
are right now working on it that's the
one that we are zipping from chopped it
down right now how big is the team how
what are the roads you mean I like the
team size reflecting micro-services it's
the same team so we are they do both
they are do the micro-services
transformation but also I have to
maintain the current system that's also
the the complexity that comes along
during our transformation thanks Johnny
so it's the 11 people that you were
talking about before yes
can you elaborate on the boundary
context yes there is a workshop as well
on Wednesday about the mind driven
design but and the bound context itself
it's it's kind of like giving you a kind
of domain model usually consists of
several bounded contexts where in which
is a model applicable so and it gives
you kind of like responsibility and with
its explicit boundaries around it and
it's my kin wherever you have some
behavior then it's consistent that
she'll stay consisted in one piece it's
kind of a yeah symbol or it's kind of
like reflecting that you can put a
bounded context around it so from this
one quite interesting how do you handle
deployments while maintaining inter
microservice API consistency um that's
where we introduced the pact framework
the testing framework that's the one was
the CDC consumer-driven contract testing
test frameworks and we make sure through
this framework that we are not
introducing some breaking changes
because whenever for example when you
run your a new test and you introduce
breaking changes for example you removed
data from your from your API that you
consumer was expecting then the the test
will fail on provider side because the
providers still grab this consumer
contract from the pec broker server and
then it will identify that those that
the API that's now actually the current
state of the ape the current state of
the API will break because it's
different from from the consumer
contract that's what they can you my
expected but so I mean the CDC's will
tell you that you have a problem but how
do you solve it how to solve well
because we are we are not that large
and we are talking to each other yes
sounds interesting and unusual talking
about talking so for talking between the
micro-services a monolith why did you
use rest and not a message queue we use
both of it okay probably I did not point
it out so we have not only synchronous
communication and but also as a chronic
communication as well and yes there are
still some rest api is established
between those and usually it says yeah
do are synchronous communication between
across boundaries and rest this is
synchronous communication style so we
are using it for example for uploading
documents where we sometimes where we
need to have a specific we are waiting
for specific results on the other
service so more technical one how much
memory or RAM does a developer
workstation need to start up all the
vagrant machines its provisioned with
for cake the very grave machine okay
that's just one machine the the
background boxes there's one machine
because it keeps it all in one yet but
it's better to have a little more six I
would say can you hear me yes great it
so from your slides it looked like you
were splitting the monolith based on
product boundaries or feature boundaries
rather than component boundaries and
looking back do you think there was a
good idea just to give an example you
mentioned this mechanism to copy user
profiles to the new service wouldn't
have been a good idea to maybe to
isolate the user profiles into its own
service and once that's sure yes these
are just good starting points and then
you can from that point on you can split
it into additional ones so the next one
is then the profile service that will be
extracted operative it so that's not
it's just only
yeah yeah reflecting our progress but
it's not the the final state that you
want to achieve
I see the questions would you start with
micro services next time around all this
it depends depends on you really on your
on your skill set that you have if for
example all the experience that we now
gained we would start with
micro-services who not thought was a
modernist first because it's yeah it's
also what you want to achieve yeah okay
so thanks a lot thanks for the talk and
for answering all the questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>