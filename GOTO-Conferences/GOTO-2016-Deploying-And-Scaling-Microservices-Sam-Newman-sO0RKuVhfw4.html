<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Deploying And Scaling Microservices • Sam Newman | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Deploying And Scaling Microservices • Sam Newman - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Deploying And Scaling Microservices • Sam Newman</b></h2><h5 class="post__date">2016-08-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sO0RKuVhfw4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we may as well kick this off this is
a talk about deploying and scaling micro
services
my name is Sam Neiman I work at a
company called thought works if you know
about what we do are we have a booth
outside where we're also be giving away
copies of my book please buy my book
anyway I'm going to talk to you really
about a bunch of stuff today but we're
going to start off as as Laura did in
her security talk this morning try to
set down some framing principles this
talk is a talk about deploying and
scaling micro services and a lot of the
technology we rely upon to do that is
changing rapidly and that means that the
information I give you today about
specific technical implementation
details will be different in a few weeks
so I want to give you some sort of core
principles some ideas to take take with
you
so it's concepts are an artifacts which
I think are often underlooked because I
think that framing will help you
evaluate the new technology that comes
out and then with the last section we're
going to be baking off effectively three
different platforms that can be used for
scaling micro services in the form of me
sauce kubernetes and docker swarm we're
gonna start at the beginning we're going
to start by talking about these core
principles and we should start off with
my definition of micro services I talked
about these things I tend to talk about
things like business motto business
oriented services I think when Fred
talked earlier he talked sort of about
business capability oriented services
that sort of my mindset as well the
thing I really focus on though when
talking about what makes micro service
is different to other forms of
service-oriented architecture I do think
micro services are just a form of my of
service-oriented architecture what I
they tend to focus on a lot is this
independent deployability most people
who pick a micro service architecture do
so because they want to go faster
they'll talk about autonomy and
sometimes that's nice to give people
autonomy but the end goal is to short on
that cycle from concepts of cash how do
I get something out into a production
environment more quickly now if you
don't focus on your making your store
your services are independently
deployable what you end up with is a
dish
system that has to be deployed in one
big lump and that is a significantly
worse place to be than with a monolithic
system so you need to be in the
situation where it's the norm that you
make a change to one service and deploy
it by itself into production environment
without having to change anything else
this is going to be the core of what
allows you to go fast there are
exceptions to every rule but they should
be exceptions the rule is you should be
able to make a change to a service and
deploy it into a production environment
without having to change other services
that's how you're going to go fast
that's how you're going to go fast
safely and a lot of what I'll talk about
today are sort of other principles that
reinforce this idea of is if it's only
one thing you remember from the talk
today it's by my book if there are two
things is by my book and remember about
independent deployability now we're
going to think about the second problem
we've got which is this which is how do
I get software from my laptop into a
production environment how we do this
changes how we think about deployment
and the especially deployment of micro
services some of you are probably
familiar with structures like this this
is a deployment pipeline or a build
pipeline it's a model of the stages that
are software moves through as it goes
from say being checked in to going into
production you might have a CI tool or a
CD tool that allows you to represent
this flow in an actual tool or if you
don't this is probably a process that
some of you have in your heads and we
have this idea with these systems that
oK we've checked some source code in
that source code then is typically built
by a CI tool and so it might be a
compilation step or a packaging step and
then we have effectively a deployed
artifact and it's that deployed artifact
that's going to be put into our
different environments so what we think
about a build pipeline or a continuous
delivery pipeline like this what we do
is we think about taking the artifact
that we build the beginning of this
process I mean move it through these
stages and we are validating it as it
goes so if my tests pass it's good
enough to go to UA T if it passes the
UAT process which could be a manual
process it's good enough to go into our
performance and Sol and so forth until
hopefully we
end up in a production environment I'm
hoping that so far this isn't rocket
science it will be quite familiar to you
there are some things are that people
often get wrong one of the first things
is you really need to focus on your idea
that you have one artifact for all
environments so when you put that
artifact into UAT or performance or prod
if you rebuild that artifacts over and
over again what you're doing is well
just blatantly wasting CPU cycles
because if it's a same source code and
the same build process why do the same
thing over and over again the other
reason why it's important when you build
that artifact once is if the artifacts
you build and test is not the same as
the artifact you then build and deploy
there are some things that can slip into
the deployment does a build process it
contains behavior of the application I
know there since C and C++ programmers
in the room you'll all know about the
power of different flags to change the
behavior of programs in production even
if you're not using macros you can even
do this with Scala scholars brought as
many many things warm laps during
compilation cycles long treatises on
type theory
it also has brought us compilation flags
to change behavior of things like string
interpolation so build your artifact
once and once only it therefore follows
that the configuration for an artifact a
thing you deploy has to be separate from
the thing you've built so when I deploy
and artifacts in a dev at a UAT a
performance environment I have my
artifact I have the configuration for
that artifacts and those need to be
separate things and that's actually
quite useful because then you can have
control around who has access a certain
configuration sets so that plays nicely
in sort of very corporate EIT
environments and also just in very
sensible maybe everybody shouldn't have
the production passwords on their laptop
type environments you should also use
the same deployment process everywhere
because if the deployment process your
user deploy into production is different
to you 80 is different to dev you will
find defects later on and that's pain
what you want is by a time e deploying
to production to have used that same
deployment process like a hundred times
on your release candidates you'll move
through your pipeline and so when we're
thinking about
different deployment platforms I like
picking deployment platforms that allow
me to have some of the unified artifacts
Universal artifacts and allow me to have
a unified deployment process this is
often what I will use I like writing
this up as a script I will use things
like fabric or something else and this
is this is the core of any sensible
service deployment process I think it is
a command-line interfaces because
command line interfaces are very easy to
automates they're very easy to type in
they're very easy to copy and paste
I don't like buoys I like GUI for
telling me about stuff I don't like
gooeys for controlling stuff the first
word is deployed I think intent is good
in a CLI and then we have this the
service names is the service that we
want to deploy we then specify a version
number or could be a build label this is
the version of the software we want to
put into a given environment and
normally what I'll do is I'll script up
other things so this will putting it for
an artifact repository or I might give
you the word local local means it's
going to look on your local machine for
something you just built this makes it
very easy few choose exactly the same
deployment process for just deploying
your local laptop as part of dev or I
might say latest and latest we'll go and
find the latest green build and deploy
that and finally you have this concept
of an environment a location into which
I'm going to place my service this is a
CLI that can be used by developers by QA
by production people I've been using
variants of this for about 10 years
before I even it was even involved in
micro services I still think this works
then again when we come and look at some
of this stuff it's interesting here
we'll see different deployment platforms
make this easier or harder this concept
of environment though is sort of worth
diving into what do we mean when we talk
about environment
now environment often means something
about restriction who can and can't see
your service but for for this talk the
things only in a focused on is really
what we talk about when we talk about
the topology if we think about what an
environment might be for my service in
for example UAT
it might be a couple of services load
balance talking to a database and so I
deploy my service maybe on two machines
and it's sort of its production issues
but it's not exactly the same as
production because having copies of
production environment is often quite
spensive as we tend to defer that until
further on our pipeline we start hitting
our performance or our production of
Romans though we all have a different
topology now we have to have a topology
capable of handling the full load and
also handling that with an acceptable
tolerances around things like failure
mode so here for example I might have
for knows perhaps low balance of cost
and availability zone or some different
data centers talking to say a standard
sort of primary replica database setup
so the topology of the environment is
changing same artifact the same
artifacts all the way through but the
configuration of that service in a given
environment is changing the topology is
somewhere else and so when we're
thinking about our deployment platforms
it also follows that we need platforms
that allow us to represent that topology
in some nice easy way and I'm hope I'm
not saying anything out of order when I
say and of course however you define
that topology should be something which
is version controlled because if you're
not version controlling stuff rigorously
right now please don't do micro services
go sort that out first so sir eyes those
core principles so independent
deployability is king that is the most
important thing if you can independently
deploy your services in production you
have probably got a lot of things right
one artifact for all environments so
void really doing the same thing over
and over again and trying to the same
deployment process everywhere now let's
dive into the power of the talk that is
often the thing that people think is
very boring but I think thinking that
artifact is really interesting I spent
many years in build systems this is the
my biggest source of pain was how we
tended to package up our applications so
let's talk about artifacts what do we
want out of artifacts what do you want
out of artifacts for our services we
want it to be something it's easy to
create so good start easy to create when
it's be easy to deploy with Mercury
services what we're often trying to do
is as Jim Lewis James Lewis sort of says
you try buy options at micro services we
like to keep our options open and one of
the things therefore I like with my
artifacts is that they abstract
out the underlying technology stack I
shouldn't care for the point of view of
someone deploying the application I
shouldn't care whether or not it's a go
app a Java app a scholar app or a junior
app just I just just deployed it runs it
should also be something that is good
for a developer focused person and good
for an Operations focused person I am a
proponent of DevOps
I hope I don't have to tell people in
this room that DevOps does not mean no
ops if you're not if you ought to talk
about that we could talk about that
offline let's start with the probably
the most common artifact I see and that
is tarballs giant bundles of stuff how
many people have seen the after-effects
of say oil spills as the oil washes up
on the beach you see like wildlife dying
trapped in this sort of giant ball of
stuff well that's what developers do to
code all the time right we just bundle
it all together and we sort of just go
ah it's kind of you know it was easy to
do cuz I just dropped all the oil in the
water but it's kind of difficult to do
anything with now here you deal with it
and we throw these things over to
operations people like me who then catch
it and go I hate you with a passion
because now deploying these things
becomes difficult the issue is these
things aren't really easy to create you
know to ANSI vfc a directory and I'm
done as easy I can do that now I've got
to unpack it I've got to change
permissions on files I've got to create
users to run the processes I've got to
install any dependencies that this
software has on that operating system
I've got to move it into the right
locations this this is the reason why
people end up with like large puppet and
chef scripts because there's developers
are creating them tarballs and you have
these huge complicated manifests and
people go or puppet and share for
rubbish and it's like no you're rubbish
so no they are not easy to deploy
sometimes they can abstract out a tech
stack it depends but it's not great for
that
they might be good for developers they
are definitely not good for operations
people most operation people I know
think these are terrible hide
and that this is part of why developers
shouldn't be allowed nice things let's
talk about spat specific artifacts here
we're talking about things like gems
we're talking about NuGet packages pips
those sorts of things these are the
these are the sort of artifacts specific
to your technology stack some of these
are suitable for deployment of services
some of them need a bit more work we
need a bit more help certainly out of
the box something like by itself a jar
is not gonna be any use as a service
deployment chocolaty NuGet might i mean
that could so that could install
services for example in windows
they are normally fairly easy things to
create because developers in that tech
stack know that tech stack they know the
tooling around that stuff you're
probably already using these artifacts
to create shared libraries amongst your
development teams they're not easy to
deploy necessarily the biggest issue is
that they don't do a very good job of
abstracting out a technology stack
you've now got to deal with the
packaging system for whatever artifact
you're using this becomes especially
problematic when you have technology
stacks that use tall trains that require
large amounts those tall chains to be in
the production systems there have been
ongoing issues for years knowing how to
get Ruby deployment right in production
environments having issues with leaving
things like our VM lying around which
can themselves be security problems
again good for devs good for UPS I don't
think operations people really like this
stuff again as this thinks is a bit lazy
I think it's fine for shared libraries
but in the whole I tend to avoid sort of
text back specific artifacts the one
exception I'd be making a little bit
recently is around go just because it's
go right and therefore it's great
because go right go go go go go go go
dakka dakka go go go Kodaka Kodaka
Kodaka micro services go docker in all
seriousness right go does create these
very tiny little statically linked
binaries they're very nice and easy to
deploy I have a little bit of a soft
spot for them but they still have some
of the same issues and other types of
service deployment you've got to put the
artifact in the right place you have to
setup your users for it but nonetheless
it's sort of you know currently
interesting aside
let's move on to something a little bit
more interesting let's talk about
operating system specific deployments
artifacts think about what deploying an
RA package a package looks like on say
Ubuntu hey I've got my come on I've got
get install and I've got the name of an
artifact so this could be if I've set up
my own in-house artifact repository
server this could just be in the
repositories list things like
artifactory and nexus can often
masquerade as OS specific artifact repos
it's quite simple right looks a little
bit like this doesn't it the mapping
between these two things isn't that far
off makes it quite easy to create that
wrapping script these things are really
really good because we have a whole tool
train on our operating system even on
Windows for managing packages we can
check that the thing we think we
installed is actually the thing we
became installed we can look in query
versions we can define dependencies so
that when we install a package we
automatically install the dependencies
as well and these things become very
very easy to install organizations that
use this as a unit of deployment when
they create these packages well tend to
have very little to no puppet or chef
requirements because most of what
they're doing are just package
installations this is a great way of
actually reducing the amount of stuff
you've got your manifests the issue and
the reason we don't use this is partly a
because developers are in charge and
developers are not operations people but
really these things are not easy to
create there is a package manager called
fpm fpm is sort of an abstraction layer
over different Linux platforms the idea
is you can use one tool chain to create
it for different packages on vacuum
right under the hood it's not too hard
to work with but even the developers
understood how painful creation of these
artifacts was because they call it fpm
which stands for effing package manager
I think you could probably guess what
f'ing means they are though really easy
to deploy you just you run that command
hosting these things is pretty easy
nowadays they do an excellent job of
abstracting out the technology stack
do you care what my sequel is in do you
care what nginx is written you just
install the package and it's running
it's either running or it's not it's a
services there and what isn't you do not
care what the tech stack is good for
devs good for ups operators people love
this stuff
developers tend not to because they
don't really understand the operating
system that they run deploy into in
production even in this day even in this
decade of easily accessible
virtualization in the Indian we have
vagrant we've got docker that we can on
our laptops developers still insist on
developing on a different operating
system than they deploy onto which is
just crazy and I think in this in this
sort of year inexcusable
get over yourself developers develop
your software in the same operating
system you're gonna deploy on docker
makes that very easy very good makes
that very easy to do in a very safe way
this doesn't solve all of our problems
though because we have other issues that
can crop up so I've got my build process
and I've created a couple of artifacts
and I'm starting a journey on since
microservices old things so what I'm
thinking is okay
so I'll take one of my old machines that
I use have my monolith on and I'm going
to take these two built artifacts I'm
gonna put them both on to the same
machine and it'll be good right I can
use my nice command lines it could be
wonderful and all my dependencies get
installed but then we start hitting
sorts of issues that come up when we
start deploying multiple services onto
the same host onto the same sort of
execution environment we have little
things like you know the diamond
dependency problem issues like okay one
service needs one version of a of some
other dependent thing and a different
service needs a different version of a
thing and we want this to be the case we
want to be able to change these sources
independently we weren't about to deploy
them independently but we can have
situations where the dependencies that
we have on that machine clash this
happens all the time so now we've got a
situation where we're not always sure
that when we update dependencies of a
service that we go to deploy whether or
not they'll clash this actually happens
even simply with things like puppet and
chef popping a chef want to be
controlling the whole box that sort of
pushes you to having one set of puppet
scripts or chef scripts for the whole
machine not per service for the whole
machine because when you start trying to
run these different runs
they can't run at the same time so you
can't do independent deployment very
easily we have other issues that occur
when we coexist multiple services on one
machine we have simple things like they
let me write code and I checked in some
of my normal normally highly performed
code that chews up all the CPU very
effectively and I wipe out everything
else on that machine because there now
is because of resource salvation in this
situation we've allowed the side-effects
of a new version of software to impact
other services this doesn't give us
confidence in doing independent we're in
situations like this when we have
side-effects of occur as a result of
deploying we start putting more
processes around our deployment we start
thinking actually we should test all
these things together and deploy them
together when we have issues like this
it's not helping us achieve independent
deployability that's why most
organizations that use micro services a
scale end up moving to this kind of
setup they find a way of creating
isolated execution environments and
having one service per little sort of
environment this could be a virtual
machine or as we'll talk about in a
minute a container normally what people
do here is sort of traditionally this
would be where virtualization comes in
this is where you do start like a type 2
hypervisor this is what Xen KVM VMware
Amazon when you launch an image on
Amazon that is a type 2 hypervisor you
have a physical machine you've got a
thing called the hypervisor devices high
poly that manages isolation of things we
cannot the great names in tech don't
weigh them so meaningless and these
things come up quite nicely one of the
really nice things you can do with these
is is you can actually create custom
images as well so your deployment
process can be I can was spin up a
virtual machine then I'm going to
install the dependencies of my software
that I'm going to pull my service and
that's my deployment the other thing you
can do is say no actually what I'm going
to do is create an artifact which is a
whole virtual machine image so out of my
build process I might get for example as
Netflix do get a custom Amazon machine
image these custom images can then be
launched on the hypervisor
so rather than having this deployment
cost of installing the dependencies
every time you do a deployment it's a
one-time build cost it also allows you
to spin the services up very very
quickly and you can also get nice side
effects because it makes it very easy to
use immutable infrastructure so rather
than ever touching that machine you blow
the Machine away and spin up a new VM
the issue again though with operating
system packages is they're not always
very easy to create these these custom
images Amazon AM is are pretty
straightforward ah as even a tool from
Netflix called the a manator that makes
it a bit easier but the tool trains with
things like VMware are a bit painful and
if you're targeting more than one
virtualization platform it quickly
becomes a bit unmanageable there is a
tool from hashey Corp called Packer
which can really help it gives you the
abstraction on top of that and if you're
already using puppet chef ansible or
salt as part of a configuration pipeline
then you can actually use that to help
you create the virtual machine image
using Packer deployment can be easy can
be hard again on Amazon spinning up a
custom ami is trivially easy spinning up
a 20 gigabyte video built and then
you're moving across your network to the
chagrin of your NOC who are banging down
your doors wondering what the hell's
happening to their supposedly resilient
ten gigabit backplane um anyway these
those you don't always make yourself
popular with this stuff they do though
do an excellent job a really excellent
job of abstracting out the underlying
technology stack which is really useful
for us in the service and again I only
actually care now our operating system
I'm working with to some extent I
launched the VM my service is there
everyone's happy
good for devs good for ops the build
times for these things can take time you
often what you're doing is you're taking
what was a deployment cost of making a
build cost building custom AMR is for
Amazon can take 15 20 minutes they
usually depending on what they're doing
with them and if the only way you've got
of actually spinning a service up is to
wait for your build process to create an
ami that's really going to hurt
developer productivity so if you are
going to adopt things like custom images
those artifacts make sure your
developers have some other way of
launching their software so just from a
fast feedback point of view now one of
the downsides these sort of
some images he's not a concept it's the
cost there's a long time taken to build
these things often the spin-up time
could still take a few minutes and on
some virtualization platforms can take
10 or 15 minutes that's certainly what
you see with some of platforms like
Rackspace the other issue is that the
cost in terms of the amount of computing
resources that go into managing the
isolation of these virtual machines
there is a hypervisor that's maintaining
separation between these virtual
machines each of those virtual machines
is running a full operating system stack
which might be overkill when you want to
alter in the same OS that's sort of why
people got interested in the micro
service of space around containers
specifically because containers give you
a way of creating isolated execution
environments more cheaply at its core
the idea is that rather than having a
hypervisor that's handled separation you
use your operating system kernel the
operating system kernel is already
responsible for keeping things isolated
and what we do is you just ask the
kernel to do more things and so when you
spin up containers they're different
Forks on the process tree and so it's
one kernel for the whole machine so
you're not reproducing the kernel
overhead you also don't need a
hypervisor anymore one other interesting
quirks of hypervisors is the more VMs
you pack on the machine the more work
the hypervisor does so often with
microservices we're creating smaller
things that need few resources that
means it can become very cost
ineffective to have lots of small VMs
and so again you end up spending more
and more money on fueling the hypervisor
rather running your services we don't
have that overhead with containers the
isolation isn't as good as hypervisors
but it's much more efficient building
these things or tins to be fast and
spitting them up is lightning fast the
issue historically has been there's no
real standard around the tool chains for
containers you had lib contain libvirt
which was sort of getting their lxc was
around and is still around today and
that's what I started using first on
Linux machines these ideas have been
around since Solaris zones open vizsla
and stuff like that the issue is we
never really had the concept of things
like artifacts for containers so
everyone would build Artic there
contains in different ways that means
spinning
the container was always effective yer a
sort of a scripted deployment process
rather than run this thing that's
actually why docker has become popular
because all docker did initially was say
okay this container stuff looks quite
useful but at all train around it sucks
even simple things like running five or
six nxe containers on my on my machine
is very easy to do but I have to set up
all the networking for it that means
you're doing iptables
when friends do not let friends do
iptables so docker came along and gave
us an image it gave us artifact
repository - gave us a nice tool chain
around creating effectively custom VMs
but in a much smaller much more
lightweight way and so we can much more
rapidly provision stuff the cost of
these isolated hosts is isolated
execution environments is much more
reduced you can now much more densely
packed these things that much more
cost-effectively do so and the effort of
building them that a computing resources
is reduced so these things are pretty
easy to create some of you probably
played aware apps your how many people
have built a docker their own to the
dock omission probably about 25 percent
right I actually think the process is
quite nice it's just a line it's just
like it's like a script it's very
familiar if you had unscripted
configuration of machines you're not
trying to deal with the complex to you
that some of the declarative
provisioning systems deal with like
popular chef because these things are
the only ones shot if you want to change
your software version you build a new
container you throw the old one away you
deploy the new and I saw the docker
mindset which actually simplifies that
deployment process there's a couple of
gotchas but by and large I think it's
very straightforward they are
ridiculously easy to deploy they are
very good at abstracting of a technology
stack you don't care it just runs and if
it's not running the dock the container
exits right good for dev good for ops
developers love this stuff I've
developed as love just getting random
stuff off the street and shoving it in
production machines and going oh that'll
be fine operations people there people
that sort of think maybe the site
working is a good idea maybe uptime is
something people should care about now
docker is a production ready I knew
people running docker in production for
at least a year before docker themselves
said its production ready
but one of the gaps it's always had
really is around the multi machine
management so docker works really well
on a machine you talk to a docker engine
it manages isolation on your machine
that's great if you didn't run this in
production yourselves so you have to
have something on top so you can use
script this yourself or use one of the
platforms making a look at some of those
platforms now so again we're gonna do a
bit of a bake-off what are the things
we're looking for from our deployment
platform we talked about before we need
some way of separating our artifact from
our topology so any platform that gives
us this is is you know it's doing well
we want handling lots of services that
will not be easy that's great and what I
mean this I mean it mean just oh I can
deploy a lot of them I mean things like
managing desired states and ensuring
that desired state is continued or
something called autonomic platforms and
something I did is supposed docker
images we like them they work really
well so platform and supports docker
images is going to be doing quite well
for us as we can look at docker swarm me
sauce and kubernetes let's start with
docker swarm dr. Swann's probably the
newest and I'd say least mature of these
offerings in a way the architecture of
all these platforms is pretty pretty
similar so here we have something called
a swarm manager it talks to things
called swarm nodes and what you do is
you run a command line interface and you
talk to the swarm manager they're really
nice thing is that the API the command
line interface for talking to the swarm
manager is it's very very familiar it's
almost the same as what you would use
talking to a single docker engine
commands like docker PS dogger logs
running that against the swarm manager
working exactly the same way as working
at the docker engine and therefore if
your user using single node docker
working with a swarm becomes much easier
it's about 70 percent of the commands
work from docker engine on to the swarm
manager so the nicest things it does and
when you say you go to you go to the
dock at the swamp manager and say I want
you to deploy the this many things and
then you give it some addition
information you might tell it about how
to distribute that load and it handles
that for you in common with other types
of platforms or scheduling systems as
this is a sort
is is it would support things like
scheduling strategies for example we
might have something like bin pack so if
bin pack what the swarm manager is going
to do is try and density pack every
single node before it moves on to the
next one so it's really going to try and
eke out every last bit of resource this
is very good when you're looking for low
you really highlight utilization if you
in an environment where you'd like to
turn off some of your managers say some
money bin packing could make a lot of
sense for you you can also do spread so
we've been packing read densely packs
things together it may not do what you
want in terms of spreading load across
multiple machines which is what you
might want to for doing things like
improve resilience e so spread will
distribute your load evenly all this
different mission all those images you
spin up and get spin spun up across your
machines is an even way as possible
there's also more complex things like
affinity saying this thing can should be
deployed next day so I shouldn't be
deployed there's even a random editing
strategy for docker swarm which I think
exists because people thought they could
do it I can't think of a good reason why
you'd want to randomly distribute your
crap all over your production
infrastructure but I don't know maybe
it's a Friday and you're all bored I
mentioned that docker swarm supports the
docker command line it also supports
other parts of the tool chain one of the
things it supports is docker compose so
docker compose as a rigid open-source
project called Ivy but it's a way of
defining effectively a configuration for
a stack of containers and how they're
related to each other so in this example
we're spinning up to docker containers
one which is going to be our web node
now we have some port mapping going and
it has a link to another node which is
Redis this is going to spin up to things
this is a quite a nice waste of effect
to be defining your service topology you
could use this to define an entire
environment but that wouldn't make much
sense to me because we want independent
deployability so I would use one docker
compose file per service and vary that
per environment but you can run this on
docker swarm and it puts all the things
in the right place this is sort of like
you know cloud formation if you've used
that on Amazon it's you'd use it in a
similar way and it's a very nice way of
standing up stuff quite quickly so
there have been some issues there with
docker swarming is fairly immature
historically it had two big issues it
didn't rebalance and it didn't restart
fail containers what I mean by that is
if you lost the whole machine and that
machine was running loads of containers
docker swarm wouldn't reapply the
original request you made you say I want
25 of these things the machine died took
five of them with it and docks will just
carry on happily that has now changed it
is now starting to take on more
characteristics with being properly
desired state systems that manage
designs they have to keep checking you
asked me for 25 of these in a spread
sharing strategy and that's still what
you've got that stuff has only just come
in in the last couple of releases now
for me this is quite core to a system if
I'm doing using a shredding system I
want it to be desired state I want it to
maintain a known state for me I don't
want to have to keep checking in getting
this right is difficult and so the fact
that there's only just coming in Dhaka
swamp gives me a couple of causes for
concern
it does play very nicely with the rest
of docker tool train if you're a
developer and operations person who's
come from a single node it's gonna be
very familiar so now a lot of low
ceremony that's really nice lighting
that's excellent probably the big
problem for me is a lack of case studies
so when I was doing the research dist
talked at the beginning of the year I
really hunted around for a long time
trying to find case studies and I found
to found one case study from O'Reilly
who were building a platform as a
service on top of docker swarm and
O'Reilly were my publishers that's great
so I could reach out and have a chat to
them
there's also Rackspace for using docker
swarm to create a pairs what I actually
found out was that I Riley were actually
building their solution on top of the
Rackspace pass and so what actually was
to case that it was actually one case
study really when you got down to it
this if you're using if your production
deployment infrastructure I want more
than one case study out there right I'm
a risk adverse individual let's talk
about me sauce me sauce has been around
a long time you can learn more about it
from Stephan Rick's talk later on which
you should definitely go to and get a
bit more detail about how this works so
I won't spend too long on this form
structure so far we have a master the
master talks to what used to be called
me sauce slaves but thankfully have been
changed to meet us agents and at this
point we have to deal with the f-word
and in the context of me sauce that F
word is frameworks and frameworks really
confused me because when I first came to
me sauce my understanding of the
framework is it's something I use to
help me use something else
so hibernate is a framework for not
understanding databases right you know
that sort of thing spring MVC is a
framework for implementing web apps
whatever that might be
frameworks and meat sauce cents are not
the same right and this is annoying to
me because we do this all the time in
computing we say we have a name a def at
a term a definition a commonly
understood definition and then we reuse
it somewhere else a completely different
name how many people here have used
puppet before a few right in puppet
there is a thing called a class do you
know how many how many instances of a
class you're allowed one right that's
not a class that's a singleton but we
didn't use that word did we because
pockets so a framework in me sauce it's
more like when I'd say as a plugin it's
actually to plug into sort of a pair you
have a scheduler and that's a thing
you're going to talk to when you want a
certain type of work scheduled across
the nodes and you have an executor and
the executors the thing that lives on
the host itself and manages environment
in which that job will run so I might
for example use Hadoop scheduler and me
sauce is been traditionally used for a
lot in data processing so people running
Hadoop spark jobs storm jobs that sort
of stuff me sauce is excellent at that
because this plug-in model so I would
talk to the hoodie pleasure to share my
jobs
I would then run executors or my me
sauce nodes and they would manage those
sort of jobs running there and I can mix
and match different plugins different
frameworks and when we talk about micro
services are and we talk about docker
sorry we talk about docker deployments
on me sauce who are actually talking
about using a a a plugin called marathon
so marathon is there for long running
jobs marathon is what is doing things
like the desired state management it has
all the same support around things like
scheduling strategies and affinities
that you'd see in docker swarm in fact
more
this plug-in idea is quite annoying fit
your head around there one of the things
it's useful for straight away is it
allows you to run mixed workloads so if
you're looking for a a sort of
production scheduler that's also going
to be able to handle your data
processing and you've got Sam's the jobs
you've got spark all that sort of stuff
you could run you're on the same
clusters you could mix all that load in
together using this plug-in model and
that's a really nice idea because then
you you don't have to invest in
different types of deployment platforms
of different jobs that's kind of nice
you could also create your own plugins
as your own frameworks so this is one
called aromatic aromatic is a very
simple framework what it does is you
basically launch one shot containers on
docker and then they die on me so they
die it doesn't seem terribly useful so
you literally issue a curl command it
spins it up and it shuts it down
why would you what you want to do that
how many people here have heard of
Amazon lambda right a few Sam as a man
that is excellent but one of the best
things around a platform as a service
that Amazon have done so if you don't
know what Amazon lambda is it's
unrequested on an event it will spin up
a very short-lived task execute that
after then die so you can implement
these things in Java although it's a bit
pointless could take some spinner or
JavaScript you could hook up the API
gateway and this stuff is amazing
because it's great with security point
of view as Moore's mentioned earlier if
a thing isn't running you can't crack it
it's great from a pricing point of view
because you know you'll use your you're
not running things don't need to run
aromatic is sort of half of lambda
already on your me sauce you've almost
got your in-house service architecture
with just a few lines of code so my
sauce really good if you were on other
workloads excellent for that very
powerful very widely used and there are
lots of case studies out there and
you're gonna see one later today the
seriously you can throw a rock and get
good case Lucy reruns on me sauce it's
one of the first big sort of internal
tech details we've been getting out of
Apple was them talking about how they
build their own framework for handling
Siri which I think runs a tiger at
30,000 nodes or something crazy like
that
but there aren't more moving parts here
this was not a piece of technology that
was designed specifically for handling
deployment of these kinds of
applications it's a bit more
general-purpose and the ecosystem around
meat sauce gets quite confusing
especially when you start throwing in
things like D sauce which is a data
center operating system coming for
missiles fear but nonetheless I think
actually if the conservative choice this
is actually not bad at all
let's talk about kubernetes now our last
platform in some ways this is the oldest
in some ways it's the newest so
kubernetes is and is not google open
sourcing their internal container
technology and shading platform because
they're not stupid and that's a
competitive advantage to them what
they've done is they've looked at the
abstractions they use internally and
I've done a cleanroom
re implementation of that in the
kubernetes project so you'll see these
things called pods that's what they use
internally they have an open source Borg
which is their internal system they've
sort of given you a detuned version and
so they wouldn't actually make sense to
you because so much of that intelligence
was tied to internal technology as well
so you have to think about if the
abstractions are Google abstractions
that work well for them and I tried and
tested before Google you are not Google
you might be Google if you're Google why
in the room you've got this at home go
home but it's also very new so this is a
brand new project really it's been
around for not a very long period of
time model is similar right we have an
API server I talk to things called
cubelets
they're kind of cute little qubits have
a command line I talked to the API
server to do stuff and then I sort of
fudged some details here because then we
start getting to talk about what it is
we actually deploy or the unit of
scheduling with marathon on me sauce
what we schedule is a docker container
docker swarm we're shooting a docker
container with kubernetes we're actually
deploying pods just like a piano pod a
pod is got lots of little things inside
it and you deploy the whole pot right so
a pod is a tightly but hang it's like a
tightly coupled collection of containers
or metadata or data volumes
now I wouldn't use pods with lots of
things in them I want independent
employability therefore I am nearly
always going to only have one thing
inside my pot and less I'm also putting
say configuration files in there as well
some of you may have seen Fred talk
earlier he sort of talked about some
cases where you might want to have more
tightly coupled two or three things and
have always deployed as a unit together
that might be a situation where I'd use
a pod but I do those things very very
rarely so I don't actually think for
micro service architectures that the pod
concept makes much sense to me it feels
actually like a lot of people using it
as performance optimizations in some
ways this simplifies affinity so it is
these trading platforms you can often
say on this thing deployed next to this
thing with pods you're effectively
enforcing that but always deploying them
together but because it's a unit of
deployment I worry that you've lost that
independent deployability so I would
almost you know I would think of my pod
as my build artifacts and everything in
it is pilot build artifacts pods are
themselves also they're not the mortal
of the unit of scheduling so these
things will die and get spun up again
and kubernetes will make sure the number
of pods you want is what's running so
this is sort of useful but you know I
always like thinking of the service the
logical concept of the service has
primacy if that's what I want to think
about that's what I want a reason about
I want to deploy a service on to think
of a service topology kubernetes has
services though which is useful so this
is a definition for a service in
Kouvelis users Jason it does support
Yammer but all their examples in Jason
because they don't like us I think
that's the reason given so anyway this
is a name I've got a version for my
service and at the bottom I can see some
that standard port mapping stuff used on
docker compose examples here's my
topology right it's sort of emerging
here I can talk about how ports are in
there you can also talk about how things
are related as well the current way
things start getting a little bit
different a bit of interesting is that
bit in the middle that's selector and
that selector stuff says this service is
made out of pods that
match these criteria and you've sort of
got this loose coupling thing going on
which kind of makes sense I guess it
feels elegant but elegance with adding
complexity and I'm not quite sure why
but nonetheless I was thinking okay is
is this is not too bad it's a mapping of
metadata and set of pods this is
effectively the topology but the problem
is you don't scale well we'll get to
that in a minute the way this works is
on your load you have your cube loop
which is the thing that's the API server
talks to you have a thing running on
that machine is well called the service
proxy and as you're deploying things
that serve as proxies sniffing for pods
that match its criteria as they are
you're one of mine I'm gonna map your
ports and that's how it handles it and
it's a service proxy can also do also
interesting things like effects you do
load balancing for you as well where I
start getting annoyed with this concepts
and these abstractions is I logically
think I have a service I want to scale
that service up I want to see the health
of that service but you don't do any of
that stuff with the service in
kubernetes what you do is you go over
here and do pods you scale pods you
don't scale the service and it that
doesn't really sit well with me and it
doesn't it feels like an odd abstraction
it feels like an abstraction for a
workflow I've never seen used outside of
Google anyway so cool bellies simpler to
set up the me sauce I'd say just have a
few fewer nodes it is a little bit
higher level abstraction so the bit
closer to a pass this sort of service
this topology idea is more baked into
the platform with docker swarm it's sort
of an add-on tall with me sausage sort
of doing it yourself in a way exists
this topology idea gives you that sort
of high level abstraction which some
people like I spoken to people that work
with kubernetes and say after about six
months or a year it really sticks it's
like I don't really want to wait a year
for that stuff but anyway they're
confusing it is very new right there
have been issues with it there were
issues sort of around Christmas time
with getting it to scale beyond 100
nodes for example which is pretty small
for these sorts of platforms although
that's very plenty for most people there
are lots of people who support this core
OS support this Microsoft were really
one of the first people to get out in
front and center with kubernetes we're
running it out
cloud so you can get manage keeping in
these clusters running on all kinds of
platforms like you can to be fairly nice
sauce redshift have rebadged their
fitfully they they got rid of open
shifty too because it wasn't very good
and they're open chef III is basically
kubernetes with their stuff on it so
you're getting a lot of commercial
companies now that are supporting this
this is very much also Google's play to
ensure you've got cloud portability so
they want you to be using platforms like
kubernetes to have portability between
Amazon and Azure and all those sorts of
providers so people know you do ask me
you know what would I use if I had a
choice between these three tools well
firstly and the key thing to understand
is there's not just three of these
things there's like about 30 I used to
work on thought works putting together
think of the technology radar and every
six months or so we would get together
in a room and look at this stuff and
there'd be more and more platforms
coming out that was sort of sit in this
space at different levels of abstraction
it's a very confusing space so I think
if you're looking to pick your own
platform you probably shouldn't just
have these three on your list maybe you
need to have some more but the way I
think about it I hope I've shared some
of my rationale about how I break these
problems down these are the tools I've
been sorted try to look at this talk and
I probably still want to be top of my
list although not swarm to new are too
little known about it I'm not feeling it
right now
I baked these two off probably if I was
more conservative or if I had more mixed
workloads I'd be picking me sauce over
kubernetes kubernetes is still a little
bit painful on the operations side I
think compared to me sauce but don't
take my word for your context is
different I would always advocate not
listening to what people conferences say
and taking it word-for-word if you're
interested in this stuff it's thought a
few of these things give them a go most
of these tools you can just spin up on a
hosted provider you can go to google and
spin up meat sauce clusters for example
very easily just fire them up and have a
go and you can be up and running on with
marathon in sort of less than half an
hour to do bake these tools off I'm
hoping what I've shared with you is some
guidelines that you can use
sort of have a look at that ecosystem
and make rational judgments as these
things change we've talked about core
principles we talked about the
importance of independent deployability
this idea of having one artifact for all
environments and that why they why the
docker image docker container may well
be that image artifact for us thoughts
about why won't the same deployment
process everywhere we talked about why
docker images as artifacts are a good
idea and why they become so popular
people have fixated on oh look I can run
a one-line command that pulls down some
arbitrary code from docker hubs and run
it on production infrastructure and so
that's the good thing about his stuff
and that's not again friends don't let
friends install images from public
Locker hub we also talked about sort of
the criteria about how I spent a
platform the thing I will say is this is
a shiftings and Fred talked earlier that
we're still learning how to build micro
service architectures well even from the
theoretical side of a modeling side or
the API design side and we're also doing
that while sitting on top of technology
that itself is still moving around
things will go wrong technology you pick
today will just you'll discover has
major problems with it further on down
the line if you're going into this world
except that you're going to have to get
quite good at validating what's good for
you and sometimes saying you know what
we're gonna leave it six months on the
other hand you might decide to jump all
in I have got time for some questions
there is a discount code for my book up
there if you buy it from O'Reilly direct
you get discount code I'm also do a
podcast which you can get Saten even to
i/o for such podcast so thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>