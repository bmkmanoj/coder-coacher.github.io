<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2014 • Make Sense of your Logs: From Zero to Hero in less than an Hour! • Britta Weber | Coder Coacher - Coaching Coders</title><meta content="GOTO 2014 • Make Sense of your Logs: From Zero to Hero in less than an Hour! • Britta Weber - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2014 • Make Sense of your Logs: From Zero to Hero in less than an Hour! • Britta Weber</b></h2><h5 class="post__date">2015-09-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8vgTBIoF8zs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey so the stock is actually gonna be
about locks and the reason why it is
about locks is that when I sign up for
this talk I actually thought I would be
speaking in the Deaf office track okay
so so I thought okay what's good for a
deaf abstract locks is always a good
topic okay and then I figured a few
weeks ago that actually I will not be
speaking in the deaf abstract but I'll
be in the big data and analytics track
and then I thought well maybe I should
change my talk but maybe not because if
you're interested in big data and
analytics then you probably have the
problem that you have to ingest your
data somehow have to transform it
somehow and then put it somewhere else
and this is a pain okay and this talk is
gonna be about how to ease the pain for
you okay a few words about Who I am I'm
an elasticsearch core developer and
joined elasticsearch one and a half
years ago and I basically joined because
everybody's doing big data and I also
wanted to do big data yeah I wanted to
do data analysis and which I'm really
super interested in although I must
admit that doing am now doing a lot of
things that elasticsearch so a little
bit of everything and one thing that I
would like to mention is if I have no
off spec round whatsoever
and this is bad okay and this is bad for
two reasons so the first is I'm gonna be
touching many ups related topics in this
talk so ask me all you want about
elasticsearch if you want to know
healthcare integration on your 100 note
tester that runs on AWS okay we'll see
about I'll be able to answer okay but
there's a second reason why this is
really bad and that is because that
these two things interested in data
analysis especially a big data analysis
and not having having a ops background
actually conflict because some of the
biggest data that we are having lying
around right now and that diet needs
analysis are log files okay okay so
how do we solve this problem if you're
somebody like me and you want to analyze
log plans so I'm going to give a very
brief introduction to the out step it is
elasticsearch log stash and Gabanna in
just a quick poll who of you have ever
used that quite a few huh okay good so
this is gonna be very very superficial
really beginners introduction and what
this is all about okay so maybe those
will just raise their hand might not be
super interested but then I'm gonna talk
a little bit about analytics with
elasticsearch and then I'm gonna show
you Cabana for which just came out a few
weeks ago and still not better which is
the visualization part of this whole
stack okay so what is up so it's
basically three components first
component is log stash which will just
allow you to easily ingest data
transform it and output it to some other
application it has many inputs many
outputs I'm gonna talk about this in a
minute but the output that I will be
talking about is elastic search the
search engine and document store and
then once you have your data internet
and in elastic search because elastic
search really is more or less a back-end
what you really want to look at your
data okay and the software that we have
an elastic search for that it's called
Cabana okay this is the three components
but first let's talk a little bit about
data so when you're dealing with data
and I like to loose if you talk about
data that comes in the form of maybe
messages or documents just one unit and
mainly contains a text so in dealing
with data and you're really lucky
then maybe you pull your data from some
sort of API for example you could pull
it for IRC or from Twitter if you get it
from Twitter what you get is this nicely
formatted JSON document that has all the
different entries in lots of information
and you can easily take it and put it
somewhere else or find out what this is
all about but if you're not so lucky
then you might actually be looking at
lots okay so let's look at some logs so
what's going on a PI system so it's my
system lock right now right so it starts
off with a timestamp here okay next
comes the hostname I've really renamed
my computer I think then there's a
component that actually wrote a lot in
this case HipChat and then comes some
message the hotel and the system
administrator or somebody who wrote the
software what's actually happening right
now okay and this is basically
unstructured text okay so what this
looks like so if you want to ingest a
syslog um you would probably have to
write or usually you would have to write
up a your own little tool that parses
the timestamp the process the host name
the component and then maybe detects and
puts it somewhere people now have
different applications they write
different logs and in this talk I'll be
talking about two data sets so I'm just
going to show an example of this so one
is this lab here so that's that's very
differently it doesn't start with the
timestamp it starts with this number
here so this is really just an ever
increasing ID appending comes another
number so who can tell me what this is
it's also timestamp it's a UNIX
timestamp yeah awesome right so and then
you have and this is a Wikipedia
xs-large so this would be the request
and component okay so if you want to
pass this also in addition to the
sizzler that we've just seen yeah that's
right another little script that passes
all this stuff even right okay so
another log sign I think I'd have this
prepared so so anybody else what this is
okay so one of these entries this is an
Apache log
okay so starts off with an IP e thing
would come user identification again the
timestamp the get request the referrer
here so we referred the user actually to
this resource and then this is the
resource that was actually requested
here oops and then comes the actually
agent that was tell us something about
what browser for example that user uses
whatever else is using and so on and so
forth and as you can imagine I mean such
an Apache log holds an awful lot of
information along the AP even think
about
you can get from the appear can get an
idea where this uses from it might not
be accurate but you can add a deal where
it's from you can know what kind of well
browser versions you actually have to
support on your web page by just looking
at the agent and all that but you still
you would have to ingest all this data
okay and that's a pain okay so and the
way you can do this isn't in a much in a
way it's knocks - knocks - in a few
sentences it's a ruby application that
allows you to collect enrich and store
data in a very easy way and it's
particularly designed for large data
that is if you have get your terabytes
of data that are constantly flowing in
okay and horizontally scalable means as
more data flows in all you have to do is
add more nodes and more locks such
instances and this will just scale and
the best thing about it it's open source
it's a fetch it - okay so so the basic
idea of locks - is that it has three
components which are completely modular
so you have an input component which
would just connect to this resource that
actually passes the message okay this
can be a data store it can be a
particular log file it can be actually
any file it can be some monitoring
system it can be some queue like
rabbitmq all these different things
take some messages from there and then a
second component is the filter the
filter would allow you to parse these
messages are to enrich them with for
example information about the GUI PE or
about the user agent as you seen in this
log before mutate this message transform
it and then I'll put it to somewhere
else and this is the third component
locks it has many outputs so you can
write to various data stores email pager
2d various chat programs and so on and
so forth and the cool thing about it is
really modular so if you want to switch
out but because just because you change
your data store you don't have to change
the input in the fill they really only
have to change the output okay so just a
quick example on how this would look in
practice so it should what
sorry okay if you download locks - what
you would get yeah it's just this tar
file okay and all you have to do - the
locks are started it's just two things
the one is just an packet okay so it's
just unpacked this thing and the second
thing you would have to do is you have
to write a little config file that tells
locks - which input component which
filter and put output component actually
to use okay so let's look at the most
simplistic config file that you can
think of and that would be so let's
build this isn't one in this case we say
okay and the input is just stand it in
so once luster starts it just read
messages from standardin
yeah you'll do nothing but them and then
just output them to stand it out okay
what might have seemed very interesting
but I'm just showing you for
presentation purposes and the only thing
I'm saying I'm changing years
give me the debug output so just so that
you know what's actually going on okay
and that's the start locks - or you have
to do is say start locks - with this
command point it to the actual config
file oops and then we'll start and start
reading whatever you type into the front
line okay so for example if I know type
hello it will do several things with
this message so first of all there's
going to be a field that's called
message and that will contain whatever
you typed in there a version which just
was just an internal thing of mock stuff
so we can ignore it for now it
automatically at a timestamp for you
that is one was this message actually
received and at the hills name at which
hostess was actually received okay so
this might not be super interesting
because really what we're interested in
is what how can we get more information
out of for example this Apache not that
we see before okay so just once more to
show you this oops this is really what
you wanna pass so what you would need is
the third component that I was talking
about before this is this filter
opponent okay so how could a filter for
such an Apache lock look like and then
again I'm just gonna show this so here's
how a filter could look like it actually
love to go into all the details and
explain all the different goodies that
come with these filters and how to use
them but I can't if you have a different
question about what the different
filters can do and what kind of
different filters we have you can find
me later at the elasticsearch booth and
ask me or one of my colleagues but for
for now all you need to know is the most
important filter for parsing log files
is the crock filter which is just an
easy way to define sort of regular
expressions and then we have two more
field to see which is the date field but
that if it actually powers an actual
date from a string at the GU IP filter
that will enhance the IP that was passed
with particular information and the user
agent which will make sense of this ugh
and file ever seen before
okay and output is again sent it out so
just to show you how this works again we
can start locks touch again as we did
before point it now to this different
config file and then just copy and paste
any of these lines into our standard in
and then see what's happening okay so so
here's what's actually happening so it
takes this message again puts it into a
field that is called message but then
this much more with it okay Paris is
declined IP it passes the timestamp what
the Bucharest actually was what was the
HTTP version status code how many bytes
were transferred who was the referrer
what was the agent and and about the
agent not only what is the agent but
also what is the name of the agent what
operation system was support what's the
major and minor version right all the
interesting stuff or for the GUI think
it's not just the GOP it also gives you
the different country code that gives
you maybe sometimes even a city name and
might even give you an area locations
such as magnitude learning
let's do the longitude which might not
be exact okay but if you have lots of
this laughs might give you some
information so and the message here is
well I cannot go into the details
oh yeah oh right and then of course I
mean it tries to power it right but if I
just press return it's not really a
message so they'll tell you oh no no way
no can do
okay so the message here is you have
these these log files you have lots of
them and all you have to do is to write
this little contract file that I showed
before that it's really really short and
so then you can get all this information
out of it okay awesome
okay so so before lockstitch has many
different outputs such as data source
email and so on and so forth if you want
to know everything about these outputs
either you can find us at the booth or
you can also just go to the locks -
homepage and step check out the
spectacular documentation so the luxor
documentation is spectacularly well
written I think okay so I can just go
there it's a huge list that tells you
everything that you need to know about
all the different inputs and outputs
okay but the output we will be talking
about it's really just elasticsearch
okay so when it's elastic church in a
nutshell again it's written in Java
Shaima free so that's why I'm at Dino's
eco track here Jason base meaning it can
just ingest everything that comes in
Jason and then the one thing and I
really apologize that I don't have this
on the slide it's based on the scene
okay you can think that fantastic search
basically is distributed the scene and
we'll see in just in case you don't know
it's this search library this is Apache
search library okay so it's a search
engine and just like laughs - it was
really designed for scrolling and
scaling horizontally meaning that if you
store your documents with elasticsearch
and more and more data flows in all you
have to do is add more computers
starclassic search nodes and this will
just scale and again it's open source
fetch it to license okay so let's take a
look at how elasticsearch looks like
again if you download elasticsearch just
let lastest you will just get a
towards that far this is the better
actually 1.4 GHz mod yesterday that I
forgot to download it so I'll just be
demoing the better which which is also
cool but and in just like before all we
have to do is just say antar it and
that's it
that's all you have to do and then you
can start elasticsearch now for the
purpose of this demonstration I told
before elastics filters the back end and
you can communicate to eat for example
via HTTP and curl yeah but does it's not
really nice to show alright so I'm just
gonna install one more thing for another
six so just to make it easier for you to
see what's actually going on and that is
Marvel there's a plug-in for
elasticsearch that would allow you a to
monitor elasticsearch and then also
comes with a nice user interface that's
called sense and that makes it easy to
execute queries against the last
exception okay
Marvel is unfortunately not for free
although it is free in development so
okay and then again just like last -
it's just started with this command and
then it'll run okay so so I said before
and I just wanna prove that this is
actually the case and you can talk to it
via KHOU so for example you can say okay
is it really running for default it'll
listen on port 9000 207 training it yeah
here it is okay so here's the status
protocol 100 is there the name will
always be some Marvel superhero
if you don't define anything else okay
so in this case the tomorrow man so
sorry I have no clue who that is good so
so this is sensor which I opened just
for demonstration purposes
okay I said before to just everything
that that comes in JSON format so for
example here would say okay I give you
this document it just has a field text
and in this text is hello and the way
NSTIC such organizes documents isn't to
sort of logical units the first one is
an index and you can roughly think of an
index as one database and then a type so
you can refer think of a type as a table
okay this is deep
basic Association so in this case I
would say okay I call it index test
index and I call the type doc and I give
it an ID say okay hello I says okay the
document is there and then when I was
search for it again I can execute a
query for example this would be a match
query just give me all the documents
that have hello and the field text they
would say yeah you used a document okay
if you look for hellos say no there is
no document so this party up the query
part this is actually the important part
that elasticsearch unless the search
query language is really huge very
powerful comes in many different ways
and I will not be talking about it at
all so if you want to know more about
elastic search how it works
it went there really good introduction
and now all about the query language and
all the different goodies that come with
it you read this book it's elastic
search the definite of guide by my two
colleagues Clinton Gormley and second
return and this is really the best
introduction you can get into an elastic
search okay all right so now we're
actually ready to ingest this log file
that I've shown you before and put it
into elastic search so now first thing
we need again this config file okay but
what allow us to parse and story to
elastic search so the contract file
would in this case just look like this
oops oh yeah oh yeah right again we take
it from standard in we have the same
filters that we had before and the only
thing I have to change here is the
output circuit output is now
elasticsearch using this protocol HTTP
extra you do not even have to add this
line I'm only adding it because there
later on we'll be showing Cubana for oh
and keep on a foreign it's elasticsearch
one before but locks as per default this
is a search I think 1.3 some node so
this is the only reason why I'm putting
this year usually you don't have to okay
and then to just get stuff into lastik
search to do is just cat
was a sample law and we pipe it into log
stash and again point it to the config
file which was in this case the this
config file and that's it and now all
these lines from the sample log will be
read by log stash will be parsed put
into this format that we've seen before
converted to a JSON document and then
sent to elasticsearch for indexing okay
before we look at this there is actually
a few things that we need to know the
first is as I said elastic search
organizes documents and indices and what
locks - this is per default it's it says
ok just create one index per day and
whenever a message comes in and that has
a timestamp check out which days this
would fall into and this is then also
the index that this document would be
indexed to ok this is just pretty fault
ok so let's see if we already have
something so if we search yeah there's
some documents so far twelve thousand
five hundred and eighty five and then we
can see it's actually the same documents
that we've seen before just now in JSON
format and indexed and elastic search
and now we can start asking questions
for example how many people that visit
my website use Windows uh-huh five
thousand one hundred no four thousand
two hundred so far and how many make
uses less interesting so what's the most
often used operation system that's an
interesting question right and and you
don't want to go I mean you don't want
to go through all the different
operations
raishin systems that look like okay but
Mac and Windows and whom what else do we
have instead what you want to do it so
we only have it the other way round when
have it give me all the operation
systems that that we have and then count
and give me the maximum so which one
which one was much often requested and
this is what we call an elastic search
and aggregation again to explain the
details of the segregation I have to
refer you to this this wonderful book
but for not just a brief how it works
you say okay you define an aggregation
give it a name and in this case it would
be terms aggregation so the the
information about the operation system
for example we have it here in the user
agent not our ass so I say okay just
aggregate all the terms and just use a
user agent at Westfield and then count
and then what you would get back is
something like this okay
other I never tried that operation
system okay and then we have Windows 7
Philip Bailey knocks false by Windows XP
and so on
okay um there's of course more things
that you want to know you might also
want to know okay just how many requests
per day right so how many requests per
day you would then use a date histogram
it's called date histogram again look up
the syntax in this wonderful book and
then the snake histogram Oh see okay per
day in this case just count all the
documents that would give me the account
for 24th of my for the 25th of May and
so on so forth but you could even go a
step further could say okay tell me what
was the most popular request for each of
these days or no wait tell me what was
the request that caused most traffic on
this day okay so you would end up with
something like this here and then I can
explain it I'm just gonna execute it so
this is also an aggregation and they
would create for me one result per day
and then per day again it would split
this up into the most popular requests
which would then be sorted by the sum of
the bites that were actually transferred
okay and then we can take a look at that
and frankly if you look at that it's all
nice that this is possible but it's
actually not much better than the
original log file that we had before I
mean so which was the most popular
request on the 25th of June no idea okay
so which brings us to the last component
that I actually want to talk about that
is Cubana okay which would allow you to
actually visualize all this stuff so
qivana again in a nutshell
so in browser JavaScript application
based on angular flat and t3
and it speaks to Allah so search via
this aggregation requests that if that
you've just seen before but allows you
to actually construct this application
requests in a with a nice user interface
and then also makes nice visualizations
of these for example bar charts or pie
charts if you into that I'm actually not
but yeah until again it's open source
it's for free ok so yeah that's maybe
one more thing I should tell say about
that I said before it's a better right
so that's about it and I'm gonna demo it
now you gonna for right and well the
thing is it is a better but the GA is
still a little further away and in the
latest better that it was released not
only is it a better but also some of the
really good goodies that will come
actually missing in this last release so
so what I did for you is I just will
give you an overview of what's about to
come and I created a little Cabana
branch for you and we're just merge
together all the different pull requests
that I like it's an overview and I'm
gonna run from this branch okay so now
we're entering the danger zone so ok ok
I started it already right - up and
running usually what you would have to
do it's actually again just a dark stash
in elasticsearch you just download it
from from the webpage you get to sting
your answer ends are and tar it just
start it just like the other ones and
they were just drawn okay well now I'm
running from grunt so let's see how that
goes ok so let's do search
sorry keep on a FERPA default lists on
for 5601 when you started up the first
things gonna ask us okay so what index I
actually what index you want to look at
now we just index log files and as I
said before they start with the locks -
pattern locks s - and done something
else so I'm just gonna leave it here the
default it last me for a time field name
and you can actually say okay I want to
look at data that
has no time base events but for now it's
time base event so we just don't leave
it this way create it next thing is
gonna show me okay and this is all the
fields you have in here and then there's
three tabs that are basically important
and I'm just gonna talk about two of
them um
first is the discover tab which is just
a part that allows you to say to figure
out where your data actually is okay so
no results fault that's a bad thing
maybe my data's just a little older okay
so let's take a look hmm so I could say
okay just show me what was going on in
the last 15 years on my last session
disease and then we see how okay
something's gone in here a pregnant
repentance off I was from 2012 and it
would give me a nice overview over what
kind of data it is so this was just
aggregate depending on what the
resolution is and what it can actually
render per day or in this case I think
it's per 12 hours how many requests came
in and you see this nice seasonal
pattern here so I would expect that this
is actually during the week and this is
on the weekend because on a weekend
people have other things to do then no
visiting web pages okay so this is how
it looks like and then it will show you
also show you all the different messages
for example here it would give you a
nice overview of what you have in these
fields and it'll show you the same thing
in JSON which is really ugly so you
shouldn't use it and so this is okay the
message here is this is a basic idea to
give you give you an overview of what's
actually in the data but if you want to
know more like the things we just seen
before what you want to do is you
actually want to go to the visualize tab
and create a visualization from that
okay here's how this would look like so
first of all I say okay I want to look
at the lock session exist before and
let's start off with a vertical bar
chart okay which would in in the
beginning be well very uninteresting
because it really just tell me how many
tokens are currently indexed and then we
can add this aggregation that I was
talking about before for example we can
say okay
mega dates histogram on the x-axis time
stem field is actually the types of
field make it daily histogram and then
let's see how this looks like okay and
this is basically roughly what we've
seen before
actually I'm wondering if I can just
yeah that's better
that's basically what we've seen before
okay same similar seasonal pattern but
not much interest not more interesting
than what we've seen before so the next
thing will be to look is for example say
okay what was the traffic how many bytes
were actually transferred okay so to do
this you can configure the x-axis confer
ixs configuration Oh y-axis
configuration and say it just sum up all
the bytes per day okay and this looks a
very different and you might notice
everybody that this is much different
from what we've seen before we would
expect if the if the traffic sort of
follows the same pattern as the as a
general axis then this should be okay
we'd expect that this would be the case
but it's actually not the case there's
something else going on here I'm just
gonna quit this makes me nervous so the
next question we could ask is okay so
which was the resource that actually
cost this traffic
what was the resource okay so if you say
it's just okay split the bars and sort
it by terms and by terms I mean by terms
I mean the requests so let's take a look
what was actually requested and this is
actually rather interesting because we
can see here that all this traffic or
most of this traffic which actually
caused that one single fine and now you
can guess where this log is actually
coming from so this is actually the out
the old log stash web page okay so that
all of this was caused by this one file
this is a little weird so the next thing
we might want to figure out is okay so
mmm
what's the agent who downloaded that
stuff was that weather people clicking
on the web page was there maybe a
seminar going on right and all the
5,000 students that were connected
online on the world don't let him know
such at the same time or what was
actually going on so let's look at the
agent just gonna yeah good okay this is
also rather interesting so what this
tells us is that it's basically all just
chef Klein's and I mean this year okay I
would argue that maybe this is okay
somebody had a chef client running as I
said I'm not an ops person okay so but
this here sounds that it seems a little
fishy to me okay so the next question we
could ask is ok who who does that maybe
I can figure out a phone number and call
these people and say hey you screwed up
your chef configuration there's a
different kind of personalization that
we can actually use for that and use a
tile map so tile map would tell me
something about where in the world stuff
is happening Oh though as I said danger
zone
apparently this doesn't work with the
resolution of my screen so I'm terribly
sorry if you want to see how the time
app works and how it can give you a
wonderful overview of where in the world
people have read your webpage then
please find me at the booth later and
I'm gonna show it to you but here
unfortunately it will not work that's a
shame okay but it's actually not so bad
because I'm running out of time anyway
so okay good
actually I said before I want to show
you two data sets okay the one was this
Apache lock and the other one was just
Wikipedia access lock okay so I cannot
actually show you this Wikipedia access
log or index it on the fly because it's
a little bigger than then the thing
which we've just seen so I prepared it
for you okay so I'm just gonna start
another elasticsearch instance so what
I'm going to do is I'm just say
give it a sip such a little more heap
space I forgive it about it's probably
too much but better safe than sorry
against are lots of searches before but
pointed to a different data directory
just to make sure that the data that
actually index is loaded okay okay
while this is starting up I'm going to
talk a little bit about the actual data
set so first of all the the Apache lock
that have seen before if you want to
play around a little bit but lock says
you can get it for free on this webpage
semi clip EUCOM which is the old webpage
that actually hosted locks - apparently
in 2012 and the other data set that I'm
going to show it's a wiki access lock so
you can get it from Vicki bench - tu and
this is a spectacular data set any one
of you ever seen that No Wow
okay it's two months of access to
Wikipedia it's actually designed for
it's actually designed for benchmarking
your own web server so you can download
this and you can get a Wikipedia dump
and then execute all these comments
against the server and see if it crashes
or not okay this is what is originally
designed for but from a data analytics
perspective this thing is pure gold it's
amazing what you can see in that okay
and I'm just gonna show you and not the
whole data set because the whole data
set us I don't know 500 gigabyte or
nearly a terabyte and just gonna show
you the dinars from the German Wikipedia
and only those only the page is very
requested okay so everything that starts
with to either Wikipedia a table slash
wiki this is what I'm gonna show and
it's gonna be 70 million entries okay
see if we here yet yeah it looks good
okay so first you gotta do is reload
Cubana
this is actually different index so this
is this is the name of the index where I
had it and then again we have to figure
out where our data actually is so let's
see where this data actually is okay
here's our data oops sorry
okay can zoom in a little bit zoom in a
little more and then we have this a
similar thing that we've seen before
okay
so we have a weekly pattern apparently I
would argue that this is probably during
the week and this is on the weekend okay
and then also daily pattern in this case
so this is probably day this is probably
not probably day and probably night and
many reason why this is much more
impressive here than it is for this
Apache log is because this is really
just German Wikipedia so we would expect
it's really just Germans that access
this stuff or maybe a few from from
another country but maybe not so okay
and then we can ask asking questions
about this in this case just one month
of German Wikipedia for example you
could say okay so what was the most
popular page okay and the title here is
actually wrong so what I'm doing here is
I say okay split the lines by the most
popular page overall for weeks so this
was worse first aggregate all the pages
over these four weeks and then tell me
okay how are these how do does the XS
evolve over time and there's already
something interesting we can see there
okay
maybe not so interesting is the fact
that the main page is the most requested
page but this is interesting here so
what is this
anybody knows oh you can because this is
my friend over here okay so this is a
third of October which totally explains
the snip here okay nobody in Germany
does anything other than partying so oh
maybe not okay but this is not it's not
super interesting because the main page
just renders everything else invisible
and so second thing we would probably do
is get rid of all these Wikipedia meta
pages like portal or since the stuff at
Wikipedia or was special or what's pizza
or so so let's filter them out and the
way you can do this in command as you
can just define a filter on this top row
here it's a very easy query language so
it just tell you okay and a page should
not be main page the page should not be
special and so on and so forth and then
what you would get is this here and this
is a little more interesting now this
gives us the most requested pages but
you can also see some
here okay so this peak here for example
I mean this is something that we've seen
before people want to know about the day
of the German reunion here is a peak so
this is heroes héroes is a serious
amateur who a few knows this heroes know
okay
and the 10th of October was actually the
first air date of the first episode of
the first season of heroes in Germany
and he was the second episode okay so
they can see this fix it and I assume
this is king of queens but I wasn't
interested because I don't really into
King of Queens oh okay
this is also interesting here okay this
is Myanmar so anyone knows why there's a
peak for Myanmar
at this point yeah okay they had a
revolution at this part and the
revolution had the peak at this point
okay and you can see this actually also
easily if you look at different several
related topics
something that is related to this you
can see okay Myanmar has a peak hunter
also sort of has a different behavior
here okay and onion soon Suki is a
different behavior here and I probably
dis pronounce this but and be coup which
is a Buddhist monk okay so people were
looking up this word very frequently
okay so this is all interesting but
there's actually another way we can look
at our data so instead of asking him
what was the most popular page and then
plot this over time instead tell me what
was the most popular page per day that
is bucket at first per day and then give
me the most popular term for these
things okay so and now I'm just going to
show this in a different view it's not
gonna be in a curve view because as you
can imagine if you do it per day you end
up with way more terms way more curse
and this well I'll tell you anything so
I'm just gonna look at the table view
here it's another nice fancy feature
that's coming out with key banner
okay enter so tell me I pay agents of
September apparently the main page
what's the most important page Jim
Morrison was requested here well this is
this interesting and we're going to look
at this in a minute
19th of September only just meter pages
20 September only just made of pages and
so on and so forth again not very
interesting
however so what we could do now is we
could do the same thing that I explained
before go to this thing here go to this
tab here define a different query that
put filter out all these things but
there's actually a smarter way to do
that and it's instead of instead of
doing a term aggregation here and just
count the popular pages we could ask
okay don't just tell me how many
documents contain this term or how often
this page was requested but tell me how
much more of them it was requested on
this day and compared to all these other
days okay and this is what we call a
significant terms analysis in terms of
analysis so again we would execute this
on the on the page the page field and
that would then show a very different
pattern so again Jim Morrison apparently
was popular on the day right but then we
see something else I'm actually not even
know how to pronounce it and I cannot
tell you why it was popular on the day
or Howard Carbondale and the 19th of
September there's something interesting
apparently the 19th of September was an
interesting article on the day and it
makes a whole lot of sense right
how specifically might seem a little
weird to you but then you when you go to
Wikipedia and look up the article of the
day what you will find is that yeah this
was the article of the day and the 1959
okay so this alone makes a whole lot of
sense you get the 20th of September the
20th of September ISS expedition was
extinct also the article of the day I'm
not absolutely sure yeah all that sort
of stuff so this is interesting but now
let's focus on let's focus on Jim
Morrison okay why was Jim Morrison
interesting on the thing and I could
tell you already Jim Morrison did not
die on the day did not have a birthday
on the day there was actually nothing
and was not the article of the day okay
so let's take a closer look
so I can again look just at this bucket
aggregation so how many requests of Jim
Morrison per day we're there and then we
will zoom in on this on this particular
date okay so let's go back to the
discover tip because this is where I can
brush currently cannot question over the
last have and look for just look for
just Tim Morrison and then let's zoom in
just on this particular part so the bit
is here okay now as we go back to the
visualize tab we can see it a little
better okay and this is again the part
where the danger zone starts here would
be a very impressive timeline that would
tell me something about how many
requests at which time of the day okay
and unfortunately and now can I prove it
to you because it's not rendered I
promise I'm gonna open an issue and
we're gonna fix it but virtually nuts
and this is really a shame because the
funny thing here is what you would see
is that this is actually 20 to 1 a.m. in
the morning this is actually at
something like half past 10 okay so
something happened between half past
10:00 and 1:00 a.m. in the morning and
then I looked at TV program from 2007
and buy that and check it out there was
actually when they aired the film it's
it's yeah and I've heard that amazing
okay and this is all just with a few
mouse clicks at 70 million documents it
runs on this little computer few mouse
clicks wait a few seconds and then you
can figure it out and then all you have
to do is Google a lot to figure out why
this is actually a significant term okay
good okay that was it for the danger
zone if you come out the backup sites in
case it would have happened so so one
thing you should know about I mean I was
posting a little bit with the 70 million
terms and all that um terms aggregations
can be inexpensive okay
so if you actually plan on using this on
your own cluster or even run it in
production and there's a few things you
need to know and I'm just gonna briefly
mention the passwords hoping that you
will later on look it up if you actually
use this so first is a filter to filter
query cache is a new feature comes in
one for that will make it easier to to
store results of queries
and not execute them again if it's just
the same query
wormus is interesting and dark violence
which is a way to actually not have to
load everything into memory but just
memory map everything okay this is also
a rather new feature that you should
really check out and then and I'm just
putting this on a slide because I'm
telling everyone without us of what I'm
talking about before you move to
production with elasticsearch watch this
video there is a video that tells you
everything about what you need to know
what you need to take care of things
that might nuke your cluster is all in
this video okay
watch it okay so I talked a little bit
about the basic aggregations but we
actually have a little more so we have
for example okay
our percentile aggregation which is not
yet integrated in Cubana bots on the
roadmap account analogy aggregation
significant terms I showed and then
coming soon there will be much more okay
so we're thinking about maybe adding
derivatives correlations between curves
for example this medium our thing would
be cool if it would happen automatically
okay but it's not sure yet there is an
open issue if you have something that
you totally want to see in elasticsearch
a sinking Wow
yes with this algorithm and this data
would be awesome to have check out the
github issue its issue 8110 and drop us
a note and say ok when I have this this
is the algorithm I would expect and let
us know what you think because this is
really work in progress
ok and that's actually all I got
we're hiring needless to say I think
everybody's hiring and then
elasticsearch the company I mean was
talking about software the company were
actually offering support and trainings
so in case you're running unless the
session production yeah there's a column
ok that was it these a lot
I have three questions written down is
there anyone wanted to pose a question
now so what happened every what happened
- hey predefined convict fires is the
first refined config files sorry I
cannot read that anymore
I saw four locks - everything yeah
actually there is a I don't think there
is anything on the locks - back page
there's examples for I think there's
definitely one for Apache log um one
thing that is nice about locks such as
there is a big community around it so if
you if you google a little bit you will
probably find a config file or sample
config file for everything just because
the community is so great and writes a
lot of blog posts about it and offers
everything to the rest of the community
so yeah what do you you want to say
something about that I'm pretty sure on
github in the log stash can trib
repository there are also some sample
config files there as well
ah alright awesome thank you okay how do
you get locked right scattered across
multiple hosts to locks - do you need a
messaging you yeah you can use a message
thank you for that
I mean this is the Luxus can read for
forever thank you there's also another
thing that's called the locks the
forwarder but please don't ask me about
the technical details of that find me
later at the booth there's me and more
colleagues and then we can figure it out
but yeah this is this is the basic idea
you would use a queueing system of one
way or the other for that okay the very
last can it be used for monitoring or a
sofa analyzing for both and so the the
original development of Capano was
actually a system administrator was
Rashid Khan who thought it should be
really easy and this is why I actually
built keep on a3 and it was for
monitoring you can configure it so that
it will load automatically every five
seconds over one minute or something
like this and I'll just always display
your grass okay so
also alerts yeah okay alert okay so
alerts currently not know so currently
you can you can visualize your stuff
but there's no automatic alerting if
it's a message based on writing of
course you can just lock stuff for that
I'm just output something to page it 2d
but there is nothing like basic
threshold in the potential okay now send
a mail because memory pressure is above
this and that's threshold yeah okay see
it huh for the next talk and thanks
again for Britta thank you for coming
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>