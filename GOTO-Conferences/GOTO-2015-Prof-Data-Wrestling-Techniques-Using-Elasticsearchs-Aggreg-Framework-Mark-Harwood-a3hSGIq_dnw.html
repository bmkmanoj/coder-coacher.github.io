<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • Prof. Data: Wrestling Techniques Using Elasticsearch's Aggreg. Framework • Mark Harwood | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • Prof. Data: Wrestling Techniques Using Elasticsearch's Aggreg. Framework • Mark Harwood - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • Prof. Data: Wrestling Techniques Using Elasticsearch's Aggreg. Framework • Mark Harwood</b></h2><h5 class="post__date">2015-06-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/a3hSGIq_dnw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">afternoon so yeah my name is Mark how I
think he's Lizzy I am a long term mr. on
the leucine project so I've been
involved for about over ten fifteen
years however long it's been going and
joined elasticsearch about a year and a
half ago and I work primarily on the
aggregations framework and to use that
horrible expression I like to eat our
own dog food so I like to use elastic
search on real data and try and solve
some real problems so the subject of the
talk today is to really just run through
some example data sets that I've played
with over the years and give you some
indication of what sort of challenges
they throw up and what sort of things
we've done to resolve them I've thought
it data wrestling technique because I
don't really feel too much like a data
scientist there's too much sweating and
swearing and grunting involved for it to
be called a science I the earth I think
of it more like wrestling data so let's
um let's skip through some examples of
how we've used this but before we get
into this what I wanted to do is just
kind of rewind a little bit and just
replay some history how many people here
are familiar with elastics that you use
elasticsearch so healthy number of
people so I'll skip quickly through this
but search has evolved over the years
you know we used to know it as an
interface like this you don't actually
have to read the words here you can
probably recognize it you type in 2.5
keywords into a search box you hit enter
and then you get 10 blue links that's
kind of the user experience but we had
to do behind the scenes with all sorts
of engineering to figure out how to do
this searching something as big as the
internet for as many users as there are
on the internet so there's some
engineering challenges in the back but
we're all about giving you the best what
we consider to be the best links first
menu page through them and it gets a bit
boring so websites again you'd have to
read the text on this slide here web
sites like Amazon introduced these kinds
of interfaces where we would still have
the search box up there in the top left
somewhere and we'd still get our 10 blue
links but we have this thing called
faceting so
we would actually summarize the million
or so results that you would have to
page through to see ordinarily we'd
summarize them here on the left-hand
side and say well here's how many hotels
have Wi-Fi here's how many hotels are in
this particular region his how many four
star hotels is how many in a particular
price range so predominantly we would be
seeing sort of structured data here on
the left-hand side summarizing the
millions of hits that we had in the
search engine and also giving you the
ability to drill down and say I'm only
interested in four star hotels in this
price range and everything else so
people are very familiar with this
facets I got us a long way but then we
introduced the idea of aggregations and
multi-dimensional summaries of our data
so again the analytics interface of
today's sort of a multi dimensional in
nature each of these visualizations can
represent more than one dimension and
we've got the full drill-down
capabilities and in some respects the
search box disappeared we weren't
necessarily interested in 10 blue links
anymore we were just interested in
saying all this data in my business
summarized it all tell me what's going
on in my business so a lot of people
come to us for this experience well
necessarily get me the 10 blue links but
summarize what's going on in my business
because we got so fast summarizing the
sort of Amazon type experience taking
them you know millions of hits and
putting them into different buckets of
four-star five-star hotels price
brackets geographic brackets we got very
good at summarizing data very quickly so
that's what people come to us for now a
lot of users actually don't bother with
the search so much and just actually
want to summarize everything that's
going on in their data but they still
search when they click on these
different sections of the visualization
they're still narrowing down and
drilling down into the data so we still
have elements of that in there so to
summarize these if you go to the last
except Asia certainly used to have on
the banner on the heading real-time
search and analytics so what we mean by
that is we're optimized for real time we
do the give me the 10 blue links help me
find the best documents and we use all
these strategies here some of these
dating back to the 60s you know fuzzy
matching tf-idf relevance ranking a lot
of people thought about that for a long
time all of that stuff goes into ranking
and get helping you find the best
documents and we've got all the tools in
the box to help you with that plus we've
also got the tools to help you summarize
all the data that you get back from
these searches or indeed if you just
want to look at all of the data we can
do the analytics to say what are these
documents collectively tell me about my
business you know is it on the up is it
on the down you know we can summarize a
lot of data very quickly there's a
little bit of a problem potentially with
the meeting of these two technologies
it's just a cautionary note for you if
you're performing analytics on the
results of searches bear in mind we're
not talking about sets of data in the
normal sense our fuzzy stuff over here
is capable of producing what I would
think of as being a fuzzy set things
belong to it to an extent so the first
slide I put up i think was somebody
searching for organic meat now it's
organic or the word meat preferably both
there's gonna be a whole bunch of
documents that contain both those words
and there's going to be documents that
contain just one of those words ranked
to somewhere below so we're actually
summarizing potentially in our analytics
things that aren't exactly what we were
looking for especially if you throw
things like stemming into the mix as
well so my opt the ick off the end of
organic and end up with organs that's
clearly not something that somebody was
looking for organic meat might
necessarily be looking for so there's a
cautionary note here when you fuse these
two capabilities together make sure your
end users are actually looking at the
pie charts and the bar charts with full
awareness of what it is that went into
that set of documents they're looking at
I've got some examples coming up later
about how we can do what i would call
fuzzy analytics so we're doing analytics
but on fuzzy sets of data so there's
some examples of that coming up so
that's the background that's where we
are where we come from where we've
arrived and now going to look at how
we're going to use this on various
different data sets so the first data
set I have is all of the UK housing data
so in the UK we're quite lucky in
there's a lot of open data now you can
get all this stuff freely if you're
interested in the URLs you probably
can't read that but i think the slides
are going online you can get hold of
this data if you want to the one thing I
had to do to help tidy up
data was I had to take the post codes
here and look up the latitude and
longitude of each of those post codes to
understand where this house is so each
house looks like this it's got a
postcode house type locality location
County and a price that's kind of
interesting so what I wanted to do was
try and summarize the housing market in
the UK I can do that with one query in
our elastic search using an aggregation
here so there's a sort of structured
nature to aggregations and it uses
nesting inside of the Jason and in some
respects some people say this stuff
looks complex compared to say sequel but
I actually quite like the nested nature
of the way aggregations are represented
because it reflects the nested nature of
the structures that we're producing so
the top level you know we're breaking
all of these house notice there's no
query here by the way it's not search
it's just pure analytics what you're
saying take all the housing market
summarize it so we don't have a search
here although we could have if we wanted
to and we're saying summarized by
location and then within that within
each and every location summarize prices
and various of the different things so
let's have a look at what that produces
um so this is a visualization which is
drawn entirely from the data is produced
by this query so this is using google
earth and actually when you sort of tilt
the earth and then spin it round and
look at it you can clearly see a divide
between north and south in the in the UK
here so we call it the UK north-south
divide cunningly and what I would have
chosen to do here in the top level
aggregation we're using a Geo hash with
the precision of five to give is little
towers hear that how many people are
familiar with geo hashes two three okay
so what I'm stood right now is probably
described using a geocache which is GC
p73 QTL something and if I use 12 digits
it would actually fit inside the glass
this glass of water here so the
technique is um take the globe divide it
up into 32 cells
and I'm it's currently sat in cell G
here in Amsterdam and then within that
cell we then further break it down by
another 32 so we get G B and then so on
and you keep going down and down and
down and by the time you've done 12 of
these things give into a little glass of
water so it's quite a good way of our
breaking geography down into different
cells so i'm using a precision of 5 i
could also use a description which is
based on miles i could say at this many
miles square or whatever and it will
find the nearest geocache that fits them
so that gives us all of these various
different towers here and that's our top
level aggregation then within that we've
got another couple of attributes here
this is the multi-dimensional nature of
the aggregation we've got the Geo and
then within that we've got color and
height so let's look at what goes into
that so the height of the tower is the
median house price for that particular
location so the median house price we
can click on each tower and we get the
number of what's the median house price
this use of median and percentiles we've
got a whole blog on that averages can be
dangerous so if the gentleman walking
into the room now was Bill Gates then
all of a sudden as an average we would
all be millionaires that wouldn't have
been the case before hand but that's the
nature of ravages and how they can skew
things so percentiles are great for the
third actually doing these kinds of
summaries take the median if you're
doing logging analytics and things like
that people are often interested in the
99th percentile you know what's the
worst end sort of experience that people
are having in terms of response times
and my website so cracking a aggregation
their percentiles to help you summarize
the data if you're interested in the
mechanics of that then go have a look at
arm there's a talk by Ted Dunning it
Berlin buzzwords called TGI jest and it
tells you how we're terribly clever
about being efficient and computing this
stuff at scale next thing we've got
going on is the color of these bars
you've noted i'm using the terms
aggregation here to summarize within
this cell what is the most popular size
one meaning I only want one result back
what is the most popular County name
that is used to describe houses that are
being sold there and that actually gives
us that draws for us if you like the
shape of the counties in the UK because
we're taking the most popular one in
every location and the net result of
that is you can actually get a map and
see where the boundaries of all the
counties are another interesting blog
post here which is kind of relevant and
related it's not a elasticsearch one but
it's a different one it's about
crowdsourcing boundaries of areas
there's a county in the UK called
Middlesex which seems to keep changing
his boundaries some people think they
live in it and some people don't so it's
a bit nebulous now if you crowdsource
data when you take enough data in then
using an aggregation like this you could
begin to figure out what the crowds tend
to think of as being you know the lay of
the land so it's an interesting
application here i think flickr did this
some time ago with the tags that were
put on images they would actually create
shape files for those of you who are in
the GIS world they would reverse
engineer shape files based on the tags
that people were attaching to
photographs so crowdsourcing locations
so they would figure out what the
boundaries of Chinatown were in a
location based on the other tagging
information is in the data so it's not
another nice little the use of
aggregations on large data sets as you
can find the most popular thing so again
I pick the most popular town that's just
useful for you know when you click on a
particular bar the counties are quite
large so when you click on a particular
area because it looks to have a very
high spike you can say that asia yeah
sino that place that's the place with
the racecourse where is really expecting
saintly expensive to live this terms
aggregation again the terms aggregation
we're using here size 10 so we get a
nice little table in here which is a
breakdown of how well sort of houses are
in this location of the terraced houses
are they detached or at least semi
detached so you can get all of this in
one query and it will run sub second and
then you can spend the next you know two
hours wandering around in Google Earth
you know finding the cheapest most
expensive place to live so highly
multi-dimensional data summarized really
really quickly
other interesting thing about geo is it
can be something that you can use to
join data sets together so you know
ordinary ordinarily when you try to join
data sets you looking for some key you
know a foreign key or a primary key or
something like that well geo is some
kind of universal thing that we're all
constrained by you know like space and
time you know we have to exist in these
dimensions you know so we've
collectively decided they can as an
industry we're going to represent geo in
certain ways and we now to translate it
from that long into you know other
coordinate systems and so on so it's
possible to take data from one data set
like this one which is house sales and
say okay let me take all of the towers
here that went above let's say 500 k is
the median house price so that what does
that give me that gives me an indication
of which of the affluent areas in the UK
so now I can take the results of that
query and I can use it as a filter on a
different data set so here these are all
the Geo hashes that represent the
affluent areas and I'm going to hit the
crime data set now UK crime I'll go all
of the UK crime records and way on my
laptop here and say what's different
about the crimes that go on in the
affluent areas because i'm using this
filter here which is affluent areas only
and what that tells me there's a line
here of the UH normal expectation will
get into this in a minute but that tells
me that in the affluent areas theft is
up because they're nicer houses to rob
they got more stuff in them so people
will steal from them an antisocial
behavior is down because they're
terribly nice in these sort of locations
so they don't tend to be antisocial okay
a different data set lots and lots of
data sets have connections in them
connected things banking transactions
emails phone calls you know even
documents are wikipedia articles have
connections to other wikipedia articles
it's interesting sometimes to have a
look at these from a graph perspective
and i don't mean graph in the sense of a
bar chart I mean graph in the sense of
nodes and edges it's actually a whole
area of computer science it turns out we
can actually get this sort of data the
shape data out of elastic search and
here's how you would do that you end up
querying the Enron emails again notice
I'm not using any query here but
crucially I could be I could have sent
an email and said only find me the
emails that are talking about the dodgy
company called Raptor the Enron set up
who was talking about Raptor because
that's a dodgy vehicle I want to know
who set it up when and who were they
talking to you so that might be the
query that goes above this but what
we're saying is summarized for me the
top 50 senders of emails about Raptor so
that would give me these nodes in this
diagram here potentially these are the
people who are most frequently talking
about project Raptor and the second half
of this query is nested an aggregation
and who were they talking to so that
completes the other half of the the
transaction who's sending the email and
we're going to summarize the recipients
field here too and we want their 50 most
common correspondence from the results
of this query you can get these graphs
structures here and that'll give you an
indication of the different cliques and
hubs in a business that are talking
about these various different things we
can choose in this diagram here for
example to draw line thickness as the
volume of emails that were exchanged we
get a document camp so we can use that
to draw the thickness of the line
equally we could have used the number of
our documents to size the circles in
this diagram to say well how many emails
with a sending busy very you know
this guy but we can do more than just
summarize the lines that connect things
if we put another sub aggregation in
here if this was banking data for
example I could be summarizing the
financial value of the money that was
being transferred between each pair so
every line in this diagram could have
rich amounts of data in it summarizing
financial volumes may be summarizing
temporal data in a month by month how
many emails were being sent between
these two people about this subject
there's a commercial piece of software
called key lines which is quite adept at
doing these kinds of visualizations and
it includes timelines across the bottom
so it's a sort of thing where you can
scrub the timeline and as you were
moving across the timeline it'll animate
bits of the network they show you well
some people started out back in two
thousand over here and then by 2001 you
know the talk of raptor had spread
across the business into these other
areas so you can replay all this
information and again it all comes out
of one query to elasticsearch so it's
another way of having multi-dimensional
data come out of elasticsearch in one
query summarizing millions if not
billions of records top tip this was
something that I put in holistic such as
a result of trying to do this type of
analytics which is we ordinarily we just
take every single document that matches
our queries we throw them through the
aggregation tree as I like we like to
think of it and it expands the whole
tree and then prunes it to just those
things that you wanted the top 50 the
top 10 or so on this is one case where
we want to collect the data in a way
that differs from the defaults the
default is this depth first approach we
take every single document we expand it
out into a bucket and then we expand it
out into the sub buckets and then the
sub buckets beyond that this approach
what we don't want to do is inflate a
picture of the whole of enron what we
want to do is first pick the top 50
senders not all people in the Enron
network and having established who are
the most prolific senders only then
push the documents out to find out who
their recipients are so it's a different
way of collecting data internally the
results the end results are exactly the
same but the interim working state that
we need to keep reduce the final figure
is different so you can read up about
that we have separate blogs that talk
about collect mode and Brent first
versus depth first most of the time
you'd have to worry about it and depth
first works fine this is one scenario
where breadth-first is actually sir
tickle you want to turn on yes neo4j
will chase down near 40 very fast
they're great what they do the trick the
graph databases do which is index less
traversal so if you want to follow a
connection it's you don't have to look
it up in a b-tree index or anything it's
like I've got a pointer bump on there
right so but what they don't do is have
aggregations so if you and I have
exchanged you know a billion financial
transactions each one of those is an
edge or a document and it's going to go
scurrying off the ring you still gonna
do a disk seek or if you're lucky enough
you've got it all in RAM but that's the
thing you can't shut it you can't show
up the graph if you go and read the
neo4j CTOs blog about how to sharda
lastic neo4j or how to scale it he
actually quotes me on there talking
about how tell tish yeah how to scale
neo4j and the short answer is it doesn't
really scale yeah and they talk about a
thing called cash sharding but that's
really putting all of the data on every
single machine and then just routing
queries to a machine in the hope that it
will be warm in terms of the cash so
that they haven't really
yeah so I mean that I would say there's
two there's two differences i think
potential or two potential differences
with the way and i think such approaches
this and the way near for judges well a
three if you include charting we do
shining one is the the aggregation the
ability to summarize huge volumes of
transactions as a single edge a rich
edge if you like a super edge is gotten
kind of like month by month you know
volumes whatever but that's one thing
the other thing that shows a lot of
promises is the fact that we know which
connections are meaningful relevance
ranking and everything else we know that
we shouldn't actually be going off down
certain avenues in the graph because
they're just super connected and we know
the frequencies of everything our IP
address is whatever you put in to
elasticsearch we count it we know how
many times it occurs and we can use that
to actually explore the meaningful
connections and graphs rather than just
the abundant ones that's how i would
describe that but um another example
data set movie lens data recommendations
this this could apply to anything that
is people have bought x people who
bought x also you know tend to buy why
you know it's that kind of deal so this
is a user we've got a bunch of product
codes here which are just movie IDs so
this is like a typical sort of thing you
would find in a product recommendation
type scenario you can download the data
and you can play with it here's the URL
but each of the documents in our system
are one of these are used around his
preferences now just to explain how
elastic search can be used to do
recommendations along the x-axis here
I've got the percentage of all users who
liked a movie in a selection of movies
here drawn as dots on this line so let
me explain what's going on here
seventeen percent of all people like
Forrest Gump this is true of this data
set and probably true of you know people
in general I think is one movie just to
the right of it so there's twenty three
percent or something people like
Shawshank Redemption okay so these are
just popular movies star wars is
probably looking around here too now if
I take a completely random sample of
people and I'm taking ten percent of
people here as an example and I plot
their tastes
on this axis we shouldn't be surprised
right seventeen percent of my random
sample also like forest gump
twenty-three percent of them will like
Star Wars it would be unusual if it was
anything else right if you take a random
sample of people why would their tastes
differ from the mainstream that that's
not unusual that so these when I do
these these queries you know plot a
random sample against everybody else I
always see the dots line up on this line
you know number of males number of
females bomb is always on the line movie
choices whatever if you take a random
sample they're the same as everybody
else so that's dull what's interesting
is when you have a non random sample so
what happens to these dots when I take a
non random sample of people and that
gets interesting there's a non random
sample of people i'm looking at all the
people who like the movie Talladega
Nights don't know if you know the movie
will ferrell movie and i'm using the my
favorite aggregation significant terms
aggregation which is the thing that
looks for the things that have changed
that are we call uncommon lee common so
it doesn't tell you how you people
who've watched Talladega Nights they
really like this movie called Star Wars
you know you should go watch it but we
everybody's seen Star Wars we know about
that one so we're looking for things
that are different from everybody else
so this is the set of dots that we have
here you can see a hugging the left hand
axis here and there moved away from this
fairly faint line here which is the line
of normal expectation so less than not
point 50 cent of all people like the
movie anchorman I should I find that
stunning but anyway in a set of
talladega like as twenty percent of them
like anchorman so we've gone from you
know naught point five percent that's
what we would expect if we took a random
sample in our Talladega likers it slept
up here this red arrow shows the extent
of the leap is gone up to twenty percent
that's a huge increase in popularity
right that's the signal that we use to
find common tastes
so you know somebody who clicks on
talladega nights you'll give them the
recommendation at the bottom hey you
ought to go look at anchor man because
we've seen this big shift in popularity
tip avoid analyzing poorly focused sets
so if I took all of my recent movie
watches as a data set be some things in
there which are kind of baps rare so
Talladega Nights blades of glory and
then I might have watched Star Wars for
example now if I look at the set of
people who have these movies in common
any one of these movies then I'm going
to pretty much match everybody because I
mean has anybody here not seen Star Wars
Wow okay Ponty throughout yeah my wife
hasn't either she's trying to make keep
it that way you know she's not sure I'm
avoid it we would match just about
everybody using this query so it doesn't
make any sense to do analytics on a big
set let's see what happens actually if
we do analytics on a set of a query
which includes blades of glory talladega
nights and Star Wars my nice little
cluster of dots here which all these
movies are moved away from hugging this
line here and they've started to bend
towards the mainstream what we're doing
is we're diluting the signal although
Star Wars watches that will a lot of
them kind of watch chick flicks and
other things that perhaps I'm not so
interested in so we've started to dilute
the signal that was in the original data
so how do we solve that how do we get a
representative sample of users well
let's go back to what I started with
we're the search guys we know how to do
relevance ranking what is that that's
information theory this is stuff that
dates back to the 60s we use inverse
document frequency scarcity what does
that mean people who share my rarer
choices like Talladega are ranked more
highly than people who share my
mainstream tastes like Star Wars another
one term frequency people have watched a
movie choice many times repetition of
something repetition of a signal as a
person who's watched Howard Baker 27
times he's probably interested in the
same to the movies
length normalization people who've got
encyclopedic lists of movies well of
course they're going to share some with
interests in common biggest have seen
many so let's use length normalization
coordination factor that's another
ranking heuristic which is how many the
things did I ask for did I find in this
in this person set of movie choices
these are all the heuristics they're
going to search relevance ranking
ordinarily we would use them on text
what are we doing here we're using them
on numbers raw numbers but some numbers
have meaning in the same way that Star
Wars is about the equivalent of the text
word the it's so common is actually not
worth anything in terms of ranking so
let's put it all together in
elasticsearch 2 point 0 we removed an
old restriction which is we would
automatically assume if you were
searching for numbers that relevance
ranking had no place we wouldn't rank
and score things based on how rare they
were how rare a number was because that
people don't do that but actually we've
just found the need for that so we've
turned it on so now by default if you
search for a field and it happens to be
a numeric field we're going to relevance
rank on the scarcity of that to support
use cases like this one if you don't
want relevance ranking on your numbers
then you should do the thing you're
always supposed to have done in the
first place which is to use filters and
not not queries so we're kind of being
consistent in the messaging now queries
are for ranking filters offer you no yes
no tub decisions another change I only
want to find the people like me not all
the people who've seen any of my movies
so we have a new aggregation called the
sampler aggregation which is where you
get to pick the top n matches of you
know from your query and subject that to
it to analysis or analytics so we're
going to use the significant terms
function here on just the top matching
sample of content from this query and
finally are in earlier versions of
elastic search excludes we're written
using reg X's and they perform terribly
and
it's been fixed we can now have a raise
very large numbers of things and it's
much more efficient in terms of
filtering these things out so what i'm
doing here is i'm saying here's on my
movie choices find people like me and
tell me what they like excluding the
stuff that i told you about the movies
i've already seen question yep diversity
yes thank you that you asked about can
my sample be diversified you UT that up
perfectly sir I couldn't have paid you
paid you money for a better vet a
question the there is a change that went
into leucine which allows you to die to
diversify on a choice of filled for the
benefit of everybody when you take a
sample it's important that it's a
representative sample so people do this
one they do marketing exercises they
don't want to sample from a particular
ethnic group or gender they want
diversity in the sample so there's an
additional setting in the sample where
you can say diversify on field x that's
got to be a single value field by the
way it can't be a multi value field but
you can say for example I don't want
lots of products from the same supplier
I want a maximum of three products /
supplier so you pick the supplier field
set the value 3 and that means we don't
get a sample that includes anymore no
more than 3 results from each supplier
so yeah diversity diversification is an
important part of sampling yeah now this
is coming into point oh this is part of
the sampler aggregation yeah so the end
product is you get these things which
have the left left axis here faster more
accurate more relevant suggestions that
was 30 milliseconds to do that
particular query there is rare that you
actually make an optimization of
actually improves the quality or
something but this is where the next
good cases is a nice trade-off if you
look at less people it runs quicker and
you actually get better results but
there are limits to aggregations I've
got a separate talk on this and that
goes into much more detail about this
but to tear up an example data set that
I had was the amazon marketplace they
have reviews in the marketplace that
look like this so we have a reviewer and
he's reviewing a seller and he's saying
five star seller and the date and
everything else is that fraudulently
can't tell you've got to look at that
reviewers behavior over time from a
single data point we can't kind of make
any sensible business decisions so what
do people normally do they they take in
streams and fire hoses of events and it
all arrives out of sequence because you
know the red dots here represent one
user's activity in the blue dots are a
different users activity and they're all
jumbled up with each other cuz they're
just arriving in time series order when
a person visits the website when he next
visits the website somebody else might
have come in in between you may wait
wait five minutes before he comes back
again so trying to do any kind of
reasoning about information here on log
based data when it talking about the
behaviors of users is very difficult
because all the data is just landed in
time-based order and it doesn't really
help us get a complete picture of red
user or blue user the hardest question I
got asked was what's the duration of
average web session on my website so how
Christ how would you figure that out i'd
have to get every single web session
load it up into memory find the first
time stamp the last time stamp subtract
the two that give me a duration oh my
god that's going to blow up memory on a
busy website that's a lot of web
sessions so pay-as-you-go model to
computation costs don't put it all off
until query time and try and do all
using aggregations periodically every
day every hour every minute maybe ask
for a sorted set of the latest events
sorted by entity ID so entity is
typically you know a person or a web
session and then add it to a little
profile that you keep either for read
user or blue user just as a regular
elasticsearch document but in a
different index
is what we call an entity centric index
it's not a log centric index it's a
summary of each user's behavior over
time so we take in every minute our day
whatever the latest batch of stuff and
we add it to what we already know about
that user and we can use a groovy update
script so this is all standard stuff in
elasticsearch scan scroll API being used
here update API with the groovy script
running here and we're attaching new
events to existing profiles drawn here
as a little booklet which summarized
people's behavior over time you can play
with the data I've got it here that's a
bunch of scripts you can download it the
raw data is the stuff that we saw
earlier we have a buyer and a seller and
a rating and a date and we load that
into our event eccentric index so this
was an event there was a review that
occurred at a particular point in time I
run my Python script here and what this
does is it creates this entity centric
index over here so it organizes the
information reorganizes it around the
individual and it derives a new set of
attributes here which are actually quite
useful as it turns out so let's have a
look at the anatomy of what that script
that groovy script looks like so we
start off we're given our historical
record this is the profile for user 27
or whatever it is elasticsearch invokes
the script and gives us the stored state
of user 27 it also tells us if this is
the first first time we've ever seen
user 27 so we can initialize some
variables if it's a create operation the
meat of the logic here is a for loop it
takes the latest batch of events the
latest reviews in this case and attaches
them to an existing set of data and
maybe it's just incrementing a counter
here total number of views plus pass
this is the interesting stuff this is
where I give the guy a character if a
reviewers made less than five reviews I
call him a newbie if he's been entirely
positive about a seller or sellers in
general and he's only ever given review
to one seller I call him a fanboy
because you just can't
saying you're great you're great you're
great the reverse is actually an
interesting character he's called the
hater this is a guy who repeatedly buys
from the same seller and says you're
terrible you're terrible why would you
go back to the same seller repeatedly if
he was so terrible any ideas competitor
yeah saboteur now we only see this
because we're looking at his behavior
over time right I didn't find any haters
in the Amazon data but I did find a lot
of fanboys oh sorry there was a build
there yeah you stick your script once
you've honed and refined it and you can
get quite creative about what goes into
this these scripts the business logic
and then we can run a query on our
reviewers new shiny index that we've got
a reviewer index and I can say let's
have look at all the fanboys and we'll
use that significant terms algorithm
again to say who were they reviewing who
are these people bigging up you know
saying you're great you're great so we
search for all the the profiles that
have been tagged as fanboy and we try to
figure out which sellers are kind of
unusually represented in that data and
we see here there's a seller 187 he's
captured seventy percent of the fanboy
market and yet he's a minnow he's got
like not point one percent of all the
reviews in reviewers in the system so
he's got a customer base yeah if all the
customers out there he's captured the
market in fanboys so what are these
fanboy accounts doing well if you look
at them over time look at the way these
things line up ten different accounts or
giving five star reviews to the same
seller and they all happen to do it
within five minutes of each other weeks
or months apart going on four years now
you could call that fraud amazon refused
to be drawn on that was fraud or not but
you wouldn't find this unless you looked
at behaviour over time and clearly this
guy latched onto the fact that in order
to hide his activity he had to create
multiple accounts and create multiple
you know review records and they would
all you always use different language
and stuff like that
but you spot the anomalies when you
start to look at behaviors over time
another quick example UK car road
whether this test so it's 37 million of
these things are the cool demo T's
standing for ministry of transport their
tests did this car pass or did it fail
if your car fails the test you're not
supposed to drive it anymore until it's
fixed you can drive it home you can get
it fixed and then you bring it back but
let's have a look at our what we could
do with three separate records for the
same car so this is test one when he
failed a little bit later two days later
he came back and failed again and then
some time later we got a green he passed
tests one two and three so what
attributes could we derive from this
information so we've got the test date
on the x-axis and we got the Milo meter
reading on the y-axis you're doing
something a little bit strange first of
all in two days this guy managed to
drive you know 100,000 miles that's not
physically possible so you can detect
that and there's a lot of readings like
that and the groovy script can then do
something which is called Levenstein
edit distance calculations so it looks
at the two numbers and it figures out
actually probably what happened there
was somebody missed a digit when they
were reading it there's a character
missing between these two two readings
and that explains why the car did a
hundred thousand miles in two days so
that's one thing we can we can find the
bad data we can attach a a date or time
period the number of days it took
somebody to get it fixed so it's the
difference between the first fail and
the subsequent pass how long did it take
so this during this time period it was
in a failed state driving around some
people you should expect miles to go up
not up and then down again so somebody's
been rewinding the Maya lammeter so
there's again another interesting thing
you could derive this little difference
here this is how long the distance that
this car was driving while in a failed
state the difference between when it
failed and when it subsequently passed
so you put all this together these are
attributes that we now have new insight
from
on car objects as opposed to just
looking at tests what can we look at and
reason about well if we plot the days
taken for a fix so you know between
north and 365 days so the longer you
have between failing and then
subsequently passing well we would
expect you to drive more the distance
that you've driven would expect to be
kind of linear the more days between the
two events then the longer you've been
driving around some of these people been
driving around for a very long time with
cars that failed which is kind of
illegal that bit in the middle that's
just weird six months exactly 180 days
there's a huge spike in the middle so
what's causing that has anybody got any
ideas no what's different about our
public transport vehicles is they have
to be tested twice a year because it's
deemed to make them more safe okay so
the answer is taxis this is what amounts
for this this particular bump here if we
again use the significant terms thing to
tell me what's unusual about that spike
this is the set of car makes that are
highly correlated with that particular
behavior they are taxis so what taxis
are doing is they're failing their test
and then they're thinking well this is
my living I've got to carry on driving
and I got another test coming in six
months time as well just keep going so
the give the government initiative that
was designed to make the cars but safer
perhaps has made them less safe because
the frequency at which they're being
tested just means that they don't bother
they just wait for the next one they
skip a test if you like so that's an
example of what you can do when you look
at behaviors over time and reorient your
data around entities rather than just
looking at log records so just to
summarize and recap averages are
misleading the Bill Gates example use
percentiles medians think about the
fuzziness of the set that you're
actually analyzing so you know how fuzzy
is your query well you know what sort of
results are you actually trying to
summarize here
against is not the same as popularity a
recommendation engine that only ever
told you hey a lot of people like Star
Wars wouldn't be a good recommendation
engine you've got to tune in to what's
significantly different about certain
sets of people or data you can't get
into trouble with aggregation sometimes
when you're using memory so consider the
use of breadth first for some of these
kind of big social network type things
and also consider reorganizing your data
away from log records into entity
centric indexes that summarize behaviors
of people they're the things that are
you mark it to their the things that can
represent fraud and risks to your
business so it's often useful to
reorient your data around those that's
it any questions
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>