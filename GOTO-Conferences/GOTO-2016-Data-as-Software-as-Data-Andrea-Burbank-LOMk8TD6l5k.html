<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Data as Software as Data • Andrea Burbank | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Data as Software as Data • Andrea Burbank - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Data as Software as Data • Andrea Burbank</b></h2><h5 class="post__date">2016-07-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LOMk8TD6l5k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks everyone for coming as she said
I'm Andrea Burbank I'm a data scientist
at Pinterest if you're not familiar with
Pinterest it is a visual discovery
platform to find new ideas and try them
out in real life if you haven't tried it
you should the people in the last talk
had free Lego giveaways I'm giving out
free membership to Pinterest to all of
you try it out after my talk you can go
to Pinterest calm if you aren't familiar
with it as an example my sister called
me yesterday she threw a birthday party
for my six-year-old nephew they filled
water balloons with water and put tiny
dinosaurs inside froze them took them
out and now they had dinosaur eggs that
they got to crack open and the
six-year-old thought it was amazing so
this is the sort of thing you can find
on Pinterest as for me I joined
Pinterest in 2012 we were about 50
people then or about 800 people now so
during that time I built a lot of data
infrastructure did a lot of data
analysis built our a/b testing framework
and now I'm doing sort of ranking and
relevance kind of stuff so I've touched
lots of pieces of the data stack as well
as doing software and that is why I want
to talk to you today about software and
data and how they depend on each other
so also I have a tendency to talk too
fast I'm going to try to speak slowly
but if at any point I speed up feel free
to raise your hand and I will try to
slow back down it's not because I'm
nervous it's just good I talk really
fast ok the theme of this talk is
turning data into gold and I actually
object a little bit to this theme
because when you think of turning
something into gold
you're basically digging something out
of the ground that already exists you're
polishing it up and turning it into
something valuable but the thing with
data is it doesn't already exist
something creates the data it is not raw
material and if you think about what
creates the data
it's your software it is the thing that
you are running every day that's going
to create the data and you need to put
as much thought into the process of
creating the data as in to turning that
data into so-called gold um the other
thing that bugs me is people say oh you
know I don't care about that big data
stuff er well I'm just a data scientist
I don't know anything about software and
I think the key to success
is to marry those two together no one
can be just a software person or just a
data person to be successful you have to
understand a little bit of both and so
with that data as software is data the
idea is better software for better data
better data for better software where am
I going with that this is sort of what
I'm going to talk about it's a journey
that Pinterest went through so we went
from having no logging at all to logging
but no insights in correct data to do
this stuff we can't control incorrect
data I do this stuff we could have
controlled but didn't understand messy
data correlation and causation with data
and how software can make daily easier
who is coming to Pinterest this is the
most basic thing you might want to know
who's coming to my service how many
people are there perhaps how many
billions does that make me worth etc but
if you think about the way a service
works it looks something like this so we
have a varnish caching layer that caches
our most common requests and then passes
through requests that don't hit the
cache to our ng API and ng-app servers
they talk to each other they pass back
data to the user this is great but when
I say data in this context they're just
readings from something like a my sequel
database and in practice it might not
always be a my sequel database it could
be HBase or whatever else but they're
reading data that's persistent that's
what we need to run the service so we
have a database of users we have a
database of pins because from the very
first day Pinterest existed when you
come to Pinterest we have to know who
you are and what pins you have but what
we don't have is any way to tell any
more than that so when I joined
Pinterest in spring of 2012 we were
experiencing exponential growth in our
user base and the way we knew that was
that we had this database of users that
showed us the number of registering
users is taking off but what we wanted
to know was why and if you look in our
database we have this database that's
serving our service we know a lot of
things about the users we know their
first name we know their last name we
know their email address
we know their gender we know when they
signed up we know whether they want us
to email them every day or once a week
or once a month but what we don't know
is where they came from were they
referred by Google were they referred by
Facebook were they referred by the New
York Times did they come to us from our
homepage
did they land on a pin that I found
exciting did they land on a friend's
page and join Pinterest we just had no
idea what was actually driving these
users to sign up and so we started to
hypothesize why are we so popular right
now is it because we're invite-only and
so people think we're exclusive and more
desirable is it because we're
invite-only
and so it's all the same bunch of people
who are all here reinforcing each other
is it because we spread through word of
mouth to a small community is it because
Facebook is sending us a lot of traffic
is it because of all these new dirt news
articles I keep seeing about Pinterest
we had no clue and so we did what anyone
would want to do if they wanted to track
something like daily active users this
is from Wikipedia I won't review the
whole thing but basically if you ever
want to become a very large company
which Pinterest has from the start you
have to have some idea of who's visiting
you because that's how people like
advertisers want to actually buy
eyeballs and so you need to know this
many people visited today and did
something substantial but if you look
again at our architecture nothing here
tells us any of these things we can tell
you how many pins were created today or
how many users signed up today because
there's the persistent things we need to
run our service but we can't tell you
anything more and so it we added a layer
of Kafka and for those who aren't
familiar with Kafka it's just a logging
service through Kafka we could write
down what requests we were getting this
sounds super simple but actually knowing
what requests are coming into your
service how long it takes you to serve
them what you're returning is super
valuable now you know gee it looks
awfully slow in this place or it looks
like we're returning a lot of 404 why is
that or things like that but in addition
it means you can start to count who's
coming so there are requests log look
something like this
for every user it has a time sorry for
every request it says what user made
that request what time that request was
what path that hit what status it
returned and a whole bunch of more
things I'm not showing here and so from
that we said hooray we can finally count
who is coming a Diu is a user who hits
our service with a request awesome my
talk is over well it's not that simple
so if you think about how email works
usually when you send someone an email
you want to know whether they opened it
and not just did they click and go to
Pinterest but did they even bother
clicking on the subject line and saying
hey this looks like interesting content
to achieve that we embed a tracking
pixel in the email as most people do
that fires a little request off to
Pinterest and says I opened this email
and we know when it was opened and so on
and so if you look at our router our
request hog it's not just those requests
for a pin and a repin and visiting a
user profile but there's this little
request on the bottom for the tracking
Jeff well that's not a real user that's
somebody who opened their email on their
phone or on their laptop
they didn't visit Pinterest at all they
just got an email from us and so now we
revise our definition of da you to users
with only a subset of certain requests
so that's great now we actually do know
who's coming we can count them and we
can write a query to do so so we say ok
you are a da you if you are in the
requests log and your pass is not the
tracking Jeff and it's on this day
hurray
and so we make a graph that looks like
this and we can actually see how many
people are visiting Pinterest each day
that's really great because we can see
oops we kind of broke the service on
those two days we can say hurrah
it looks like overall we're growing but
we might have more questions who are
those users is the same users every day
or new users trying us out for the first
time do people come back often and so we
come up with weekly active users well
that's fine we can just query our
request log again and say in the last
week did this user visit on a
that's not the tracking Jeff these
people are weekly active users four
monthly active users that's fine did
these people visit in the last 28 days
and visit a path that's not the tracking
Jeff and for yearly active users did
they visit in the last 365 days on a
request that's not the tracking Jeff
well this is starting to sound kind of
expensive if I'm logging every single
request and going back over a whole year
of it every time I want to do this
things are not going to go well for me
and so instead we create derive tables
again this is super obvious but
tremendously valuable and this is where
software engineering helps you create
the data you need so instead of just
counting the people who show up every
day we can write a partition to a table
and s3 that says on this day this user
was a d8u
and it looks like this so on this day
this user had action type 0 which I've
arbitrarily decided as da you
they had 135 requests this person had
200 requests this person had 3 requests
and so to go back to our questions who
are the users is it the same users every
day now I can find that out we can write
a new table of xd7 users people who
visit each week we aggregate from our
lovely unique actions tables there's no
parsing through these requests and
trying to do it it's just one row per
user runs in three minutes I have this
lovely table of weekly actives I have a
table of monthly actives and now I can
see how often users are coming to
Pinterest this sounds really dumb but if
you imagine being a company where your
service has fallen over most days for
the last six months and you've mostly
just been working to make sure that
people can come to Pinterest having
actual data that says these are our
users this is how often they're coming
back is super useful and so here here's
some data where I've you know erased
some axes and so on but we could see a
lot of people were actually coming to
Pinterest everyday a lot of people were
coming only once that week we wanted to
know what happens the following week and
now we can look at that too because we
have these derived tables and we can run
a quick query and say well people who
come every day this week tend to come
every day next week people who only came
once this week those are the people that
we might not be losing
those the people who might not
understand pinterest and that's an
intuitive idea but having the actual
data in front of you lets you then
target those people figure out what's
different about their behavior and do
things to help them understand pinterest
and stick around more here's another
view of the same thing in case you like
this view better but basically you can
see the same trends where people who
visit there's sort of a magic tilting
point where you can see if people are
visiting less than maybe three ish days
a week they tend to move next week
toward less instead visiting more than
that they tend to be even stickier next
week so people who visit six days a week
probably will come back every day next
week maybe they just were sick one day
and so on we can ask other questions so
I wrote this query for this talk but I
just wanted to see if people come back
came back one week a year ago how many
weeks did they probably come back this
year and most of them come back every
week and so we can see that yes the most
common is they came every week the
second most common is they came every
week except one the third most common is
they came every week except two this is
crazy and this is stuff we can't figure
out without this request logging so what
did we learn for Pinterest we can
actually count our users and Pinterest
is incredibly sticky what did we learn
about data is software if you log
requests you can get a lot out of it but
you have to know which requests count
you probably want to create derive
tables you're not repressing your logs
over and over again and having things
like weekly and monthly aggregates can
make your life a lot easier for
understanding things like how people
turn in and out of the service because
it's not that every day I wake up and
brush my teeth and say gee should I use
Pinterest today it's that I've built a
habit out of it and I come back over the
period of weeks or months counting is
hard but now we can count we've
completed part one part two I mentioned
one reason for counting which is that
eventually if you want to sell ads you
need to know how many people are coming
there are other reasons like maybe I
want to try a new feature and see how it
worked and so one reason we want to
count is to know how we're doing so
here's our graph of daily active users
we talked about how we could find these
outages we talked about how we could see
the growth let's apply that to a real
situation fall of 2012 we had just
launched Pinterest to be able to sign up
with an email address so instead of
having to sign up through Facebook you
could just go to pinterest.com enter
your email address which you are going
to do after this presentation it's gonna
be great and join the network in
addition we just released our first iOS
and Android apps so instead of only
going to your computer and finding
Pinterest now we're on these lovely
pocket computers well that's until we
wanted to see what outcome that would
have on our metrics and so we could see
that when we did these things
sign-ups completely went through the
roof this is awesome all of our hard
work is paying off we can see that not
only is there a bump when we launched
what we call open signup but there's a
second bump when we launch our mobile
apps we are getting the users that we
hope were hoping to get hooray
but we started to wonder what was going
on and I hope it shows up up there we
especially wondered what was going on
when a few months later that jump in
signups went away and we said well we
still have open signup and we still have
iOS and Android apps what's going on and
so we took a closer look at our data and
we looked at the people signing up and
we had candy 21 387 at hotmail.com and
Deborah 29 43 at hotmail calm and 2
dancing 91 46 hotmail calm and I bet you
could name about 300 other users that
also signed up
and similarly later on we had an visited
to do at Yahoo and cutter tofu at Yahoo
and all these email addresses that
probably no one you know has anything
like that and so yes what we were
measuring was spam we had no idea what
was going on with our business because
of all these extra people who aren't
actually people who are obscuring our
metrics the thing with spam filtering is
that it's adversarial what I mean by
that is that every time you come up with
it's a pattern that you identify what
the spammers are doing there
going to adjust their tactics to not be
caught anymore and then you cope with a
new rule they adjust their tactics and
you're constantly fighting with each
other
except the nice thing about doing it for
your internal metrics is it doesn't have
to be adversarial anymore you don't have
to block their behavior you don't have
to let on at all that you know they're a
spammer you can just say it's lovely
that you're doing what you're doing we
are not going to count you when we're
trying to figure out how our business is
doing and so you could take the
adversarial part out of the equation and
actually understand your business again
and that's what we did so here's some
examples of some of the people we could
catch so these are a number of users in
our database who have this lovely quote
I was born botanical with the soul of an
animal now if you sign up for Pinterest
and that's your description that seems
perfectly plausible people like to put
quotes in their descriptions of
themselves but if you look at their
email addresses again we see this common
pattern where it's a capitalized first
name and a capitalized last name and
then some letters and then yahoo.com
these people were spammers and we can
tell they're spammers because they're so
similar to each other here's another
example these are people whose username
is a common female name followed by a
digit followed by two letters or
sometimes followed by two letters and
then followed by a digit and so you get
the idea here like we're not computers
we're very very good at pattern
recognition we say these people look
like scum we need to get them out
sometimes it's a little bit different so
and this is a user from China who has 50
boards but no Pitons every single one of
these boards is a phone number for an
escort service in a different city in
China there are hundreds of users like
this so that their profiles will show up
on Google and when you see their a
little snippet it will tell you how to
call this escort service these are the
sorts of people we're fighting with the
good thing is we can do that so we have
this database of users and we can write
rules that find these people people who
have the same description as each other
people who meet these certain regular
expressions in their email address
people who sign up with the same name we
can do the same for pins people who sign
up with the same weight loss - person
who create the same weight loss image
people who only pin from a single domain
people who pin the same link
turn over people who pin the same image
as someone else whose famiiy people who
created boards called I'm da boss and
make lots of money we had a whole bunch
of those for a while people who delete
and create boards within a second people
who sign up for the same ip as people
who only signed up via email I could
keep going forever clearly there's a lot
of these but the point is for everyone
you can just write a simple rule and it
looks something like this insert
overwrite table pen oolitic stop
possible spammers data great that's just
a spot in s3
this is for all the people whose first
name is good Jehovah - and his last name
is Fahad you dinner this sounds
ridiculous right I can't even say these
words but they were I don't know 200,000
of these people who are messing up our
data and so all we do is say great this
is dumb but I'm just writing a partition
we're solving our data problem with
software engineering and so we write all
of our rules and we populate this table
of possible spammers data and it's just
a workflow this is a screenshot from our
workflow tool it runs every night we
figure out who's violating our rules we
decide not to count them in our metrics
we integrate that with the real-time
spam service which actually is
adversarial and does have to block these
people's activity and so on I won't talk
about that today but I'm happy to answer
questions about it to create a new table
called spam users and now we can filter
spam and so we take our spam users table
we take our xt7 users in our XD 28 users
table which count weekly actives and
monthly actives
and we can stand filter them so we have
spam adjusted versions of those tables
we actually know what's going on and we
create spam filtered versions of a lot
of other tables and now finally we can
have a clue about our business so here's
a workflow for that again it's a
workflow it has dependencies it has jobs
they run every night simple software
engineering problem and we're good and
so instead of this graph of signups we
have this graph look we actually did
increase signups a lot with our launches
we just also let it opened up the doors
for spam and so once we leave out that
little dotted line of spam now we can
actually measure how we're doing we can
say hey these two weeks of effort we put
in provided
return-on-investment we can tell whether
opening up our service in other
countries is a good idea
etc data is software aggregate your data
build workflows it's software
engineering to solve these data problems
we can finally count who's coming again
and have a clue what's going on let's do
this again daily active iPhone users so
fall of 2013 we saw a step change in the
growth rate of people visiting on iPhone
hooray we wanted to know why and said
well it's likely because we were
featured in the App Store and indeed
we'd been featured in the App Store by
Apple for launching on iOS 7 but how
much of that jump would that explain we
started to dig in further what else
might have happened so the mobile PM
wrote a email to the entire company do
you have an iOS device check out
featured in the App Store we launched
3.0 of the iOS Pinterest app today which
represents a huge amount of work from
the mobile team an inspired combination
of iOS 7 and Pinterest visual language
the interaction model has been carefully
designed transitions gestures and
information architecture we're
painstakingly thought through that
doesn't sound like we were just featured
in the App Store that sounds like we
rebuilt a whole bunch of stuff so what's
special about iOS 7 well if you look it
up what changed in iOS 7 says Mac world
well not that long ago
iOS apps running in the background we're
essentially stuck in suspended animation
unable to do anything until you've real
aunch them all of that changed in
October 2013
thanks to background app refresh a
feature introduced with iOS 7 so when we
say we completely revamped Pinterest to
be integrated with iOS 7 what that means
is we use their background fetch feature
to get people's home feeds ready for
them before they open the app now this
is a really good idea so instead of
opening your phone and seeing a blank
screen you open your phone and there's
already lots of inspired pics for you
that you can see right away
and so we started to say well this
change in calculating iPhone da use is
probably inaccurate because we know
we're doing a background fetch but we
also were featured in the App Store
there was some real effect how much of
this is real
how much of this is due to a change in
our logic so we talked to the iOS
developer and he said only this one
handler is being called on the
background the only thing we do is fetch
people's home feeds and so great we went
back to the data and someone did an
analysis and said preliminary analysis
shows background checks only contribute
a small fraction of the da you gained
great really that jump that jump is
really big if you're a data person and I
know I said everyone's a data person so
you should all be data people you're
looking at that and you want to be 100%
sure that that gross is real because if
that is real you should be trying to get
in the App Store every week this thing
is the biggest growth vector you've ever
found and so a few weeks later we said
you know this just it continues to gnaw
at me I really want to make sure 100%
this explanation we found only accounts
for a small amount of the growth and so
what did we do we took that request log
that we talked about for every single
request for every user we know when it
happened what it was a request for etc
and we can make a graph this graph is
very small and that's intentional you're
not supposed to see all the details but
the big dashed line is when we launched
our new iOS app and hopefully you can
see that to the right of the line
there's a whole bunch of blue and green
streaks to the left of the line you
mostly see little red dots blue and
green streaks little red dots what's
happened here is we annotated each of
the handlers that was called so v3 home
feed v3 get pin v3 get user v3 get
notifications hundreds of these handlers
each for different API endpoints and we
just colored them until there weren't
any streaks anymore that were red just
using data to try to figure out what's
going on this is what the analyst said
you can easily see long smears of blue
which are background hits to
mostly v3 home feed these we knew about
already and we did the developer
mentioned that there might be these but
after the 3.0 app is launched you can
also see long smears of green triangles
and there are no corresponding smears of
green circles before october 18th this
makes me believe that when in the
background the app can do more than just
fetch a v3 home feed data as software
software engineer says it can't do this
we look at the data it sure doesn't seem
that way something else is going on
software engineer takes another look as
per our discussions yesterday I actually
confirmed iOS applications can be
launched to do a background fetch even
when they're not visibly running and not
appearing in the multitask tray this
could explain the green streaks were
seeing so we took an explanation of
software we looked at the data tried to
figure out what's going on it only
encountered for a smell not to the
difference we looked again it informed
how we're understanding our software
data as software building this loop now
that we know that we can build estimates
of different deu counts when we exclude
different handlers so if we just exclude
da the home feed one it's the red line
and if we exclude those green ones as
well and so on and so we can get a sense
of what our actual growth is depending
on which handlers we think are fake but
more than that we can try to build that
into our software and so instead of
having a request log that looks like
this we add another column and if the
request is being called in the
background it says so if the request is
being called on a foreground it says so
and now we can count these people as da
you because they had an active request
and exclude these people from da u
because they had a background request
again it all sounds simple everything
I'm talking about you're like well of
course you should do that and yet
actually doing it is difficult and
actually knowing you should do it is not
always obvious either and so we just
adjust our data job we say ok app state
is not background and active we don't
want to accidentally not count as da une
one on old versions of our mobile
clients so also app state is null
now we have a new count of who's a daily
active user on our platform and we can
see now our revised count goes down a
lot a lot of that jump was not real in
practice we could have backed filled
with data we didn't it's complicated
anyway I can talk about that as well if
you like but we now have a real count of
how many users are visiting Pinterest
again and we can see that we learned
about what our software is doing by
looking at the data data as software we
actually learned something about how our
app work that we hadn't understood
before from looking at the data we
understand how activity changed at
certain points of time and we can now
add explicit logging that helps us
understand what's going on counting was
even harder than we thought but now we
have a sense of who's coming to
Pinterest part three avoiding data chaos
so we talked about how the app and the
API now right through Kafka the request
log in the raw API request log so we
have a log of every HTTP request coming
in it's going great in addition to that
of course now that we have this
capability we say well let's write down
some more stuff maybe every time someone
signs up we'll say what method they
chose to sign up was it Facebook or
Twitter what IP were they coming from it
sign up what other information did they
give us and so on for every event that
happens for every pin you know did they
do it via the bookmarklet that they do
it via the button where is it coming
from and so on and in practice there's
more and more and more of these logs
this is a really really good thing
because it means that the developers
care about the data it means they want
to know hey I'm building this thing if I
log something I'll be able to find out
more about it but it also means that you
have all these topics piling up and you
need to sort them out the same things
happening underneath so we're writing
all these logs and all these logs go to
s3 and we talked about how important it
was to have derived tables to be able to
count things like daily actives and
monthly actives and from the monthly
actives and creating the possible
spammers into spam adjusted and spam
adjusted monthly actives and so on this
whole story makes sense and this story
makes sense too but in this story makes
sense as well
but soon it turns into this we have I
don't even know how many tables I hate
to think probably thousands tens of
thousands lying around in s3 we don't
know what's in half of them we don't
know who put them there we don't even
know which ones are business-critical
all the time and so if you're not
careful you soon land and data chaos the
other thing someone else actually talked
to on earlier today is JSON so go back
to the start of my story Pinterest is
about 50 people the service is down a
lot of the time we are really just
trying to get by and also build in some
logging to have a clue what's going on
and so we pick JSON JSON is really nice
because you can read it look there's all
some stuff about this pin you can add
stuff to it later when you realize you
forgot an important key you just add it
in but it has some downsides as well so
if some feel doing it is no longer
useful if it's been deprecated you don't
really get to deprecated it it just kind
of sticks around and doesn't do anything
for you if you add a new boolean key
later it's the how you will be null on
the things that you haven't touched in a
while because it wasn't ever populated
if you're doing sort of lazy updates and
it won't be backfilled into the meaning
of null is false see for some things and
truthy for others and parsing it is very
slow so yes we have this get JSON
function and you can get field out of
the JSON but it takes a long time and so
in practice you created some derive
table that you're gonna reuse every time
but every time you then add a new key
you're taking away the advantage of JSON
which is you now have to update the
schema of your derive table and write a
new query so instead of JSON we started
moving toward thrift thrift is just
enums and structs and so we have a
struct for a pin promotion that has all
these different fields that are integers
that's lovely they're integers and
strings that could just as well be JSON
more or less except it's you know
serialize differently and so on ah but
the magic of thrift is that you can
actually nest these things so you can
have a pin promotion action event that
has a pin promotion inside of it that
has nested information and so you have
this very well typed well-defined data
structure
that you can mine as deep as you want to
find the connections you need and none
of this did a chaos of things floating
around randomly so I just described JSON
as untyped and thrift versus as strongly
typed if you're a software engineer
maybe that sounds familiar
data as software maybe the things that
we need to do to keep our software in
check are the same things we need for
data so the problems we talked about
proliferation of tables no clear data
ownership duplicated work streams lack
of schema how can we turn that into
order things like a central data
repository key shared tables across the
whole company deprecation of unused
tables tests for data accuracy self
documenting tables well if I cross out
the word data and tables in all of these
and instead siccing the word code and
software this sounds like exactly what
we've been doing in software engineering
forever we have a central code
repository you track things into it we
have key shared functions across the
whole company I'm not going to
re-implement sorting any more than you
are we deprecated unused code we have
tests for accuracy of our code we have
self-documenting code and so to avoid
data chaos the solutions the same as to
avoid software chaos data as software
software as data okay
I'm gonna pause for a second here what
have we talked about so far we added
Kafka logging now we have a log of all
the requests coming in we have a clue
what's going on we cloned our databases
that we could do analysis offline of the
pins and the users that were serving at
real time but without adding extra QPS
those databases that will bring our
service down we built workflows to
aggregate those data and to derive
tables we defined we dropped out a quest
that weren't important like email
tracking so we can tell who our actual
users are we squash spammers we learned
a lot about how our app actually works
we got to ignore some phantom requests
that were happening when nobody was
actually there we created schemas for
our data added tests for data accuracy
where did that get us back where we
wanted to be at the start just being
able to count our users and have a clue
what's going on
so where do we go from here well we want
to count it internet scale remember the
goal was to know how we're doing did you
go back to this problem we saw this big
jump and usage and said well it's
probably because we were featured in the
App Store
that explanation is perfectly reasonable
but where does it fall on the spectrum
of certainty so if you are arranged from
correlation to causation a lot of the
things you say are much closer to the
correlation end than you think they are
the goal of a lot of the data things we
do is to move closer to causal inference
move further to the right on the
spectrum an excellent way to do that
well sorry ah here's an example per
capita cheese consumption correlates
very strongly with a number of people
who become tangled in their bed die
because in their bed sheets each year I
could make a story for that if you eat a
lot of cheese then you get indigestion
and then you don't sleep that well
you're tossing and turning in bed you
get tangled in your bed sheets and you
die
that story is not entirely unreasonable
this story also not entirely
unreasonable maybe if it is very very
likely that being featured in the App
Store will lead to a growth in our users
but how much is my lovely story
correlation or causation and so here you
come to a/b testing which I would argue
is the best way to scale the sort of
data and software with your company to
know what's actually going on so quick
refresher you take the population you
divide it into control and treatment you
apply your treatment to the treatment
you don't apply your treatments they
control you measure what happens so here
you have control enabled you divide them
in two you measure your outcome you want
people to turn blue look more people
turn blue in the enabled group we should
ship it but should we a lot of things
come in here I've given a lot of talks
on maybe testing I'm not going to talk
about it a lot today I'm happy to chat
about enough here
some simple things you want to know was
that different statistically significant
a lot of people say the thing on the
left
that's mumbo jumbo you're talking about
whether it's significant
not it of course it's up like that is
the definition of up is that there are
more of these people well the problem is
if it's not significant it might not be
real it might just be due to chance
what about novelty effects it's actually
true that a lot of people just click a
button because they don't know what
it'll do but the goal of your company is
not to add new buttons every day's that
people will click them because they
don't know what they'll do it's to
create long-term change where people
understand the value of these new
features user segmentation it might
affect some people more than others
pitfall of this is you can go down a
very long road of segmentation where
you're like oh no it looks terrible for
men and Kazakhstan turns out there's 8
of them you probably don't care you have
to be careful with this and lastly
randomization errors the enabled goop is
so much higher well it turned out that
you randomized people incorrectly and
the enabled group was better before the
test began as I said I'm not gonna go
too much into this but again in the data
as software theme this is where software
can really really help doing the right
thing should be easy doing the wrong
thing should be hard build tools that
make it easy to do the right thing with
data and do it hard to do the wrong
thing only show significant effects
differentiate novelty versus long term
effects let your tool allow people to
compare important user segments without
clicking on a million dropdowns show
them when there's randomization errors
and so much more
counting an internet scale please do a/b
testing this was a very short segment of
a longer talk but I'm happy to chat
about more data as software where are we
now leverage a/b testing scale insights
with tools doing the right thing should
be easy doing the wrong thing should be
hard see the whole talk counting is hard
we got everything wrong at every step of
the way first we didn't have any logging
at all no clue what's going on
then we logged every request as a DA you
well we shouldn't be certain things
don't count then we had spam interfering
with metrics it took a long time to
parse our logs we had background fetches
I didn't even talk about things like
browser tabs that people leave open
screen savers that are running widgets
that people install on their websites
all sorts of things where to do the data
correctly you have to know how the
software works you have to think about
well gee if you put a widget on the
webpage should that count as a DA you or
not what does that look like in our logs
and all sorts of things I didn't talk
about what a per people in what country
are they in those questions are harder
than you would think
data is unruly if you don't have data
governance you're gonna end up with
chaos the solutions are similar to those
for software use thrift used wrongly
type things write tests and lastly for
scale decisions do a/b testing automate
metrics collection make it easy to do
the wrong write make it easy to do the
right thing and hard to do the wrong
thing and build a culture of
experimentation data as software
software's data you can't be good at
data without understanding the software
your software will not be as good unless
you have data thank you
I should have time for questions yeah
there's at least one question Judy up
how have you solved the problem for
legitimate users from a SIA that we're
posting manic pins but it was weren't
relevant from the user for in Europe
sorry how do I problem you showed with
the user from Asia hello it's solved or
did you block them probably yeah through
a user from your okay so the question is
how do we prevent sort of spam from Asia
from affecting people in Europe we
actually want to prevent spam from
everywhere from affecting users anywhere
that's more in our real-time systems but
it's actually fairly similar in that we
try to use the data to detect mass
action and prevent people who are taking
mass action from having effects on
people that's very general if people
have more specific follow-ups I'm happy
to follow up offline but basically you
can detect these things with things like
machine learning or just handwritten
rules and then the main goal is both to
get correct business metrics which is
what I talked about and also to make
sure the experience at Pinterest is
positive for our users which means
preventing those people from
disseminating their content any other
questions I remind you of the free
Pinterest membership available to all
thank you for coming thank you for
yourself</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>