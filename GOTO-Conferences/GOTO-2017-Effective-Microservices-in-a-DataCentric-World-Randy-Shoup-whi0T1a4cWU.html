<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Effective Microservices in a Data-Centric World • Randy Shoup | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Effective Microservices in a Data-Centric World • Randy Shoup - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Effective Microservices in a Data-Centric World • Randy Shoup</b></h2><h5 class="post__date">2017-06-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/whi0T1a4cWU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">great thanks that well it's great to be
back in back in Chicago and Beckett go
to so I'm going to talk about effective
micro services in a data centric world
and I'll tell you a little bit about my
background the background of the company
that I work for and then applied that to
some learnings that we've had and
building out micro service architectures
so just a tip that you know where I'm
coming from I'm the VP of engineering of
a company in San Francisco called stitch
fix and stitch fix is a clothing
retailer that is sort of revolutionising
retail by turning it on its head and we
like to use we like to think of our
combination of the business model as
combining art with science and if that
doesn't make sense that's fine but it
will in a couple of slides before that I
was sort of a roving CTO as a service
like my friends would call me I was
helping a bunch of companies scale their
organizations scale their technologies
and in large part or in often part the
technologies involved adopting micro
services and migrating monoliths to
micro services before that I was
director of in director of engineering
at Google for Google App Engine so App
Engine is Google's platform as a service
and at the time I was there was the
world's largest one you might be
familiar with Roku or some other
platforms as a service it's kind of in
that same that same space and then
earlier in my career I was chief
engineer at eBay for about six and a
half years I and during that time I
spent a bunch of effort helping to
migrate eBay through a couple of
iterations of their fundamental
architecture cool so I want to tell you
a little a little bit about stitch fix
and this isn't to tell you that you
should buy your clothes through our
service which of course you should but
it's basically to give you a background
on the business model because that will
let you have some insight into why data
is so important and why micro services
are a good fit for for what we what
we're trying to do so the business model
at stitch fix is basically the reverse
of standard retail so rather than going
into a store or into an online online
e-commerce site you fill out a pretty
detailed style profile about yourself
you tell us all the things you like all
the things you don't like your price
preferences your style preferences all
sorts of information that you would tell
maybe your friend
or your sister or your brother and for
somebody who is going to choose clothes
for you so you give us all that
information we send you five hand-picked
items in a box that we've selected
exactly for you and then you keep what
you want and you return what you want
and obviously you keep the things you're
going to pay us for them but if you
don't keep them they come back for free
and no problem so behind that model is a
lot of art and a lot of data science
so we've make we made such an such an
investment into data science that we
actually have a one-to-one ratio between
engineers and data scientists so we have
roughly 70 to 75 engineers in the team
that I work on we have roughly 75 data
scientists in our algorithms team and as
far as I'm aware this ratio is unique in
our industry I mean I'd love to hear the
counter example but so far I believe
it's the only company that sort of a bar
scale that has sort of equal parts
engineering and data science and what do
we do with 75 data scientists right well
we actually apply intelligence to every
part of our business so we apply it to
the buying process which in retail is
called merchandising we apply it to
inventory management so we actually have
physical warehouses and the managing the
inventory there is done with data
science we do it with we apply to
logistics optimization so shipping out
to get things to the people at the right
time at the minimal cost we do it for
styling recommendations which I'm going
to talk about for the next couple of
minutes so the recommendations to the
human stylist that actually choose the
clothes that are going to go into the
box for you and we also do more standard
things like demand prediction trying to
think trying to figure out how many
clients we're going to have going
forward
how many fixes or how many boxes in our
terminology we're going to send out that
sort of stuff and the key thing though
across all those all those areas is that
we have humans and machines augmenting
each other
the humans wouldn't be able to do as
well without the machines and machines
wouldn't be able to do as well without
the humans so let me give you a quick
example of a working example of what we
call styling so styling is choosing
those five items that go into the box
for you and we have about 3500 human
stylists that are all around the United
States they look at they look at our
inventory and out the other and they
choose the five items for you and then
they
send them to you in the box and we hope
that you enjoy them but actually behind
that is a bunch of algorithm a bunch of
algorithmic work and a ton of data
science so we as soon as you have signed
up with the service with that detailed
profile we can immediately cluster you
with about with a bunch of other
customers that have similar tastes and
then we run a ton of machine learn more
to make predictions about what you're
going to keep and what you're going to
return and those algorithmic
recommendations go into the interface
that the stylists use to choose the
things that we send and then the in the
stylist is not just taking the top five
things that we recommend the stylist is
doing what the humans do best putting
together an outfit deciding that this is
a thing that a software engineer in San
Francisco would wear or this is
something that a Manhattan socialite
would wear basically the things that
humans are good at and the machines
aren't yet so for the rest of the talk I
want to talk about how we work at stitch
fix focusing mainly on the micro
services but and I want to talk about I
also want to talk about why that way of
working actually works so modern
software development to my mind is sort
of made up of four different segments so
there's and it's not just technology
right it's organization its processes
and practices its culture and its
technology and as far as I'm aware or in
my personal opinion the ways that I like
high-performing teams that I've worked
in the approach those problems in this
way so in organizations we like to have
small teams with well-defined areas of
responsibility for processes and
practices I'm a strong believer in
test-driven development and continuous
delivery for culture I'm a strong
believer in DevOps meaning that
everybody owns end to end the services
and applications that they build and
finally I believe strongly in micro
services so I'm going to go through
those first three super quickly to kind
of give you a baseline and then the rest
of the talk is going to be about micro
services though they promised so quickly
on the small teams so so the
high-performing organizations that I've
worked in sort of the Google's the
eBay's of the world do this so they have
small teams that are aligned to
particular business domains so a team
has a clear well-defined area of
responsibility and a team
typically typically maintains a single
service or application or a set of
related services and applications teams
are cross-functional so within the
boundary of the team they have all the
skill sets that they need to do their
job now I'm not saying they have all
skill sets in the world like we don't
make our own hardware we don't make our
own operating systems but all the things
that the team needs to get their job
done are all within the boundaries of
the team and of course those teams
depend on other teams at statistics for
supporting services supporting libraries
and tooling on the processes and
practices side again we're strong
believers in test-driven development and
continuous delivery
so for test-driven development we do
this because tests help us to go faster
right we're making an invest investment
upfront in writing tests as we write our
software but we do that because the
tests have our back right they there
they're there to make us to let us know
when something is broken because of a
change that we made and it actually ends
up increasing our development velocity
rather than slowing it down tests help
us make better code so it gives us the
strut the confidence to be able to break
things and it gives us the confidence to
be able to refactor in a very courageous
way when things when we want to do
something different with the code and
finally tests make better systems so it
allows us to catch bugs earlier and it
allows us to fail faster so the other
test-driven development has been
practiced by a bunch of people for a
long time but widely in the industry
relatively recently so often you will
hear from managee types and I have
become this person so I'm sad but you
often hear from managee types that we
don't have time to do it right and I say
wrong I say we don't have time to do it
twice so if we do it right at least
right enough the first time as Brian
Cantrell also said in today's keynote
then we're going to be in a much better
position to build on something solid
that already exists and be able to move
forward in a great way and in fact this
is maybe counterintuitive but the more
constrained we are in terms of time and
resources the more important it is to
build right the first time right if we
don't have much time and we don't have
much resources we should spend our time
building a small number of really great
teachers rather than a large numbers of
sort of half implemented ones and just
to be clear when I say right I don't
mean perfect right you build the 80/20
rule of the thing that you actually need
to get done you do it well high-quality
move on the implication that that has
for us at stitch fix is a really
interesting one
it's the first time that I've worked
I've been in the industry for a long
time as you can tell they're looking at
me and we basically don't have a bug
tracking system so bugs are fixed as
they come up so we have a system that
that shows us the bugs that are
happening in real time in the
applications but when they come up we
fix them and and we have a really solid
system from then on we do of course have
a backlog it's not like we don't know
what we're going to do next week next
month or next quarter we have a backlog
but those backlog are new features right
features we want to build technical debt
that we want to repay it's not a whole
list of things that that are broken in
the current set of features does this
kind of make sense yeah it's really cool
and I and I really love working in an
environment like this which is why I
bring it up so the other angle about
processes and practices are is
continuous delivery so most of our
applications that stitch fix are
deployed multiple times a day and when I
used to talk about Stephanie Bay when I
first started giving conference talks
about in 2006 I was talking about the
eBay architecture and some of our other
practices at eBay and it was the first
time that eBay had sort of opened the
kimono and talked publicly about what
they were doing and so one of the things
we talked about beyond the architecture
was eBay used to release the site on
two-week train so they would leased the
entire site once every two weeks and
just to give you a sense of how far
we've come 11 years ago that was amazing
people were like oh my god you released
the whole site every two weeks and now
if I said that at this conference people
would be like oh my god you released the
site every two weeks so we're doing it
multiple times a day this leads to more
solid systems right so it allows us to
release smaller units of work it means
the thing that small units that we
release are faster to repair easier to
diagnose if they go wrong and they're
smaller changes that for us to be able
to roll back or to roll forward and it
also enables us to do really rapid
experimentation small experiments that
are really cheap so we can do a/b
testing and a
testing and ABCD testing because it's
cheap we are able to rapidly iterate and
experiment really quickly and I bring
this up because that's going to be super
relevant to the data stuff that I'm
going to talk about in a moment but the
last sort of pillar that were built on
is the cultural aspects of DevOps and
what that means to me is end-to-end
ownership for a team of the of the
service so a team owns a service from
design through deployment all the way to
retirement and the team is responsible
for all aspects of the service right not
just the features but also the quality
of the service the performance of the
service the operations of the service
and the maintenance of the service it's
basically the reification of the Amazon
idea of you build that you run it ok so
that's the background and now I want to
take the rest of the time to talk about
micro services some of the ways that
we've approached the problem some of the
things we've learned in migrating are
monolithic database into individual
services and also some patterns and
practices that you can take back as
you're trying to maybe mana migrate your
monolith in the same way ok so but first
it is it can be easy to think that at a
conference where there are lots of talks
about micro services that that we would
be saying you know you should do micro
services for everything that's actually
not true and I'll give you some great
examples in fact there are examples from
places you've probably heard of so at
eBay depending on how you count it's on
it's about fifth-generation of its
architecture so it started famously as a
monolithic perl application written over
a three-day weekend by the foundry in
1995 so he wanted to play around with
this new web thing and and try it out
and he was a devotee of economics and he
was like well I'll build an auction site
he totally was not planning on building
a business this was like a fun little
hack not hack day like hack three days
hack long weekend for himself but it
ended up becoming a business and you
know now he gets to live in Hawaii and
stuff so that's pretty cool so the next
gen the next generation of eBay's
architecture the v2 of it was a
monolithic C++ application and that its
worst it was about it was 3.4 million
lines of code in a single DLL yeah don't
do that
you want to talk you think you have a
model desk yeah 3.4 million lines of
code in a single DLL oh and they were
hitting the compiler limits on the
number of methods per class which is 16
K so seriously don't do that ok the next
iteration v3 was a reimplementation in
java and it wasn't micro-services but
you can kind of think of it as many
applications so take a take a page or a
series of related pages on the site like
the buying part the search part the
selling part etc and they can make a
java application out of that and then
there were a bunch of shared databases
underneath and then the neck of the
final iteration of eBay's architecture
and where it is today is you could you
can basically describe it as a set of
polyglot micro services so twitter has
gone through a similar evolution so
depending on how you count Twitter's on
about its third generation so it started
famously as a monolithic rails
application which my friends at Twitter
tell me they call the monorail which
that's a clever name the the next
iteration after that was breaking out a
bunch of the front-end into JavaScript a
bunch of the backend to the into
services written in Scala and then now
it's fair to characterize Twitter as a
set of polyglot micro services Amazon
has gone through a similar evolution so
Amazon isn't so clean on the defect of
the divisions between their architecture
they went wide relatively early but they
did start out with a monolithic Perl C++
application which actually you can still
see in some of the product detail pages
if you go to Amazon if you ever see
obidos OB idos in the URL that was the
name of that original monolithic
application and of course for search
engine optimization like they don't want
to change the pages because of PageRank
and yeah anyway so they started to break
up their back-end into a
service-oriented architecture lots of
services written mostly in Java and
Scala and now I think it's fair to
characterize Amazon as a set of polyglot
micro services so there are two things
to take away from this one is all these
places didn't start out doing micro
services but all of them ended up doing
it does it make sense so when you're
starting out and you're still trying to
figure out whether whether there's a
business model and whether you have
product market fit and whether you're
delighting your customers this is not
the time to build a distributed system
later on when you've reached when you've
reached massive scale that is the time
okay so now that we've sort of put
microservices in a bit of an
architectural evolutionary context what
is a microservice so to me the micro of
microservice is not about the number of
lines of code that's in it but it's
about the size and scope of the
interface so a micro service is
single-purpose it has a simple
well-defined interface and it's modular
and independent and this if this looks
like good just simply good it capsulated
class design you're right
that's exactly what it is and if it
looks like this is the right way to
build services and why did we need to
say micro you're right as far as I'm
concerned micro is nothing micro
services are nothing more than
service-oriented architecture are done
properly but the other element of micro
services which is where a bunch of the
initial so uh so are-- architectures
went wrong is a critical element of a
successful micro service is that it has
isolated persistence in other words the
Micra service the only way to get in and
out get the data in and out of a micro
services database or whatever
persistence mechanism it uses is through
the published interface does it make
sense but that's isolated there's no
backdoor way to get in to get into the
database from the outside how does that
work Randy I'm glad you asked so I'll
tell you a little give you a couple of
patterns or a couple of approaches to
how you might be able to do
persistence in this way so the approach
were number one is that the team that
builds and operates the micro service
also operates its own data store right
at stitch fix we use Postgres as our
back-end so we have a bunch of services
where the team is essentially
maintaining its own own instance of
Postgres but you could replace that with
you know my sequel or whatever
persistence mechanism you like another
approach is to use a persistent service
right so store to your own tables or
your own space within a persistent
service operated on your behalf by maybe
another team at your company or maybe a
third party cloud provider like Amazon
dynamo or relational database service or
Google spanner or something like that
and the key the idea here is it would be
operated on your behalf so you don't
need to know how to scale into
the persistence mechanism but what but
the key thing that you want to have a
service owner is that your slice of that
multi-tenant system is yours and it's
isolated to you and nobody else can get
in there does this make sense what I'm
talking about
yeah no it's good all right and again
the critical out thing that we're trying
to get here is the only external access
to my data store of my service is
through my published service interface
so speaking a bit about interfaces I
want to make sure I want to talk a
little bit about how to evolve
interfaces in a micro services world so
a lot of people have maybe worried about
well how do I version micro services and
isn't this really complicated and it
takes a little bit of discipline but
it's actually not as complicated as you
might think but key the key element that
you want to you want to have here is you
basically as you are developing and
evolving the interface of your service
you just simply want to make sure that
the changes that you make are both
backward and forward compatible and the
the main goal here is that you want to
make sure that you never break your
clients code but Randy you tell you but
you say to me I sometimes I need to make
breaking changes sometimes I need to
make iterations on the on the interface
great so the way to think about that is
something called semantic versioning
which probably most most of us have
heard about so the semantic versioning
is like major version minor version
patch major versions are for breaking
changes minor versions are for additions
to that changes in behavior like adding
new methods and patches are for things
that don't change behavior at all but
are just sort of improvements in bug
fixes or performance or something like
that so often and if you have a
situation where you you want to do
breaking changes to your interface
you're often maintaining multiple
interface versions of your service and
that actually might mean maintaining
multiple deployments does it make sense
so if I want to go from version 10 dot
whatever to version 11.10 be maintaining
an instance of version 10 and an
instance of version 11 and if I do that
what I want to have is a pretty explicit
deprecation policy with the clients that
are using my service in other words I
want to I want to go to this client and
I say look the new version is version 11
I'm going to keep maintaining this for
you for I don't know three months six
months whatever is the friendly thing to
within your company but notice that the
DevOps
idea that I own and operate the service
myself forces me to have a really strong
incentive to help my customers wean off
of version 10 onto a version 11 does it
make sense why I'm saying that I'm
maintaining both version as long as
there are customers that are clients of
mine that are still using version 10 I
have to continue to maintain it and
continue to operate it but as soon as I
can get that last client last a client
off onto version 11 then I can shut it
down and we're good so I and my team
have a really strong incentive to invest
in helping customers to move from
version 10 to version 11 does this make
sense what I'm talking about
cool all right so the next thing about
data and micro services are sorry the
next thing is about extracting micro
services from a monolith so at stitch
fix we build most of our applications in
rails and we have a bunch of back-end
services and go
so rails really wants you to build a
monolithic application and thankfully we
avoided that temptation so we have about
40 different applications for different
areas of our business and different
different functions of people in the
different areas but we did but our
version of the monolith problem is we
deal but we did build a monolithic
shared database so I'm not sure you can
know I guess you can see better than I
can see but that's basically it has all
the interesting entities of our system
right so the shared database that stitch
fix has information about clients it has
information about items shipments which
are those fixes or boxes that we send
out it has information about the
metadata information about the styles
and SKUs that we sell and have in our
inventory it has information about our
warehouses etc etc etc and so all the
different you know all the 40 different
applications more or less are using
information in this shared database
obviously this is a single point of
failure this is a single performance
bottleneck it's kind of a challenge as
we go forward to being able to move
really fast in parallel and get the real
benefits of those small teams the
continuous delivery etc this is the
bottleneck that that slows us down so
we're going to try to get rid of it and
we'll talk through the pattern of how
we're approaching that so the goal here
is to decouple applications and services
from the shared database so I'll take a
little bit smaller example like let's
imagine just so that there
won't be so many boxes and lines on the
screen so let's imagine we have two
applications and three tables in the
database so the first step is that we
want to create a service for let's say
clients right so clients were a thing
that we have in a table in the shared
database so first we create a client
service then we have the applications
that were that used to be directly
talking to the table now talk to the
service and now we extract that that
table from the shared database and now
now we have a standalone service that's
not dependent on the shared thing and
now we just keep doing that over and
over again so we rinse and repeat as we
like to say in America so we create an
item service on top of the core items
table we create a style service on top
of the SKU table and now these are our
new micro services right so we can now
draw boundaries these are you know the
real extent of the micro service is this
interface boundary that surrounds the
application sort of business logic a
part of the service and the database
does this make sense simple steps now I
showed you that in like whatever one
minute and it might it might have you
might have thought that I was making it
seem like it was easy it's not at all
easy right what's hard is uncut encase
transactions that are dealing with those
individual tables so there's actually
quite a lot of work associated with this
but once you have sort of decoupled it
you kind of you can follow through this
this pattern or this sort of a pipeline
of work that I just thought I just laid
out cool ok so now I want to talk about
some ways of dealing with data in
microseconds that we have found to be
really effective at stitch fix and
things that also we have done at Google
in the past as well ok so one thing is
how do we deal with shared data right so
in our in our monolithic database there
are a bunch of tables that are used in
lots of different queries and in lots of
different applications and how do we
deal with that so the problem here is
that the monolithic database makes it
really easy to get access to the shared
data right I can just join with it in a
secret in a select statement I can I can
query it in applications that are using
a database and so the question is where
does that share database go in a
microservices world
and the principle here is that there
should be for any piece of data a single
system of record within the overall the
overall ecosystem so every piece of data
is owned by one and only one service
okay now that service implementation
probably has replication and so on I
don't mean a single like machine but I
mean a single logical service that's
that's the responsible party for that
particular piece of data and that
service is the canonical system of
record for that thing
so there's once you know in the ideal
micro service ecosystem that we are
moving toward at stitch fix there will
be a service that owns clients another
service that owns items another service
that owns orders etc every other copy of
that data in our system every other
place that we represent a customer or an
item or whatever is a read-only
non-authoritative cache take a moment to
like let that sink in does it make sense
there's one place where like I can
update it and that's where it actually
lives any other place that remembers
anything about items is just a cache of
that thing
so if you keep that in the back of your
mind now the the approach is that I'm
going to suggest are going to make more
sense okay so the first approach is is
kind of just lookup synchronously right
I want to find information about what's
the example here about about customers
and so my order service needs
information about customers like maybe
it needs the address to know where to
send the order or something like that
and so the idea here is that the
customer service is the system record
for customer information and the order
service synchronously calls in to that
service that customer service in real
time does it make sense this is pretty
simple
okay the second approach is leveraging
more event-driven techniques like we saw
in the keynote yesterday from Martin
Fowler
so in still the customer service owns
customer data but instead of the order
service calling in a synchronous way to
the customer service maybe the order
service needs to do some complicated
stuff with the with the customer
information maybe and needs to have a
cache in order to do that efficiently so
let's imagine that the customer service
sends events when customers have changed
so said in something like customer
updated events
the order service is listening to those
events and the order service then takes
that information and cashes it locally
and then it can now use it to do it's
fancy stuff do this make sense
so the pattern when are calling a
synchronous event plus a local cache and
recognize that if I ever asked the order
service about the state of a customer it
would be wrong right it would be
potentially stale and by the way the
order service should never have an
interface like I just suggested but but
all the information in the order service
about customers is potentially slightly
stale okay and then approach 3 is for
things that for data that's relatively
immutable and we have a lot of this at
stitch fix right it's sort of immutable
read-only metadata like size schemas so
that's a thing like numeric sizes for
women right size to size for size 6 etc
small medium large extra-large you know
for other types of sizes that's a size
schema and that's the thing that doesn't
change very often right it's rare to
have some new like extra extra extra
small or something like that colors are
similar thing for us fabrics the US
states right I hope to god there's
nobody who implemented a micro service
that serves up you know what are the
values of the human of the US states but
yeah so don't do that so how do we do
with that again we use a lot of these
application stuff in in Ruby so that's
why I have a little you know gem diagram
of gem icon there but like if items
service needed information about I don't
know colors and stuff and then style
service needed it and receiving service
needed this would just be a shared
library this make sense cool ok
so that's those are the choices that we
have around shared data and what I am
not telling you is that one of them is
better like we actually use all of those
techniques that stitch fixed for various
various things it totally depends on the
how often the data changes and what the
use case is ok so now I want to talk a
little bit about events and I want to
talk about events now because that's
going to give us another set of tools to
do the next set of patterns that I'm
going to talk about afterward so I am a
strong believer in the thing that Martin
said in his keynote and didn't get a
laugh even though he totally should have
which is events are a first-class
concept construct in in programming and
we don't take advantage of it as much as
really should so if you ask Wikipedia an
event is a significant change in state
so it's basically a statement that some
interesting thing has happened in our
system and it might be that you know
when I produce an event it might be that
nobody's listening it might be that one
consumer is listening and it might be
that many consumers are listening but
the idea is that the producer produces
the event and typically consumers are
subscribing to that event and updating
themselves or doing something
interesting based on the event being
propagated I strongly believe that
events are the underappreciated you know
redheaded stepchild fourth fundamental
building block of building big systems
right so we all know the three-tier
architecture where there's a
presentation layer and application or
business logic layer and a persistence
layer I think we should add to that a
first class concept of state changes in
the form of events in it and this is
intuitive right events represent how the
real world actually works right so if
you think about finance
when Deutsche Bank and Bank of America
are trading currencies like they're not
doing a distributed transaction right
they are they're producing a bunch of
events on both sides sort of at each
other and then they reconcile at the end
of the day I was I was trying to as I
was putting this talk together I was
wrapping my brain to try to think of
what's a really visceral example of
events in our own lives well here's one
how about the exact the actual software
development process so I'm big fan of
continuous delivery but raise your hand
if every time you press ENTER in your
IDE that it gets deployed to production
does anybody do that
no so because that'd be crazy right that
would be totally crazy
so when I hear people say it is hard to
reason about systems where something
happens in one area of the system but it
hasn't yet propagated to another and how
do I deal with that just think about
what you guys do 8 hours 10 hours 12
hours every day does it make sense yeah
and anything that you call a workflow
that is exactly in a set of events does
it make sense a workflow is a thing that
I do that produces an event and then
there's another thing and another thing
in the workflow another thing in the
workflow etc okay so I'm using this
to use this as a building block as I
just said to build some interesting
things that we can do with microservices
so events in microservices go very very
well together so just like the
synchronous part of our interface events
are a first-class part of the interface
of a service so a service interface
includes the obvious stuff which is like
the synchronous request response part of
it right so whether it's rest and JSON
or G RPC or whatever thrift your request
responses like that's the obvious part
of the interface but any events that
your service produces any events that
your service consumes those are also
first-class parts of your interface and
also any bulk reads and writes that you
do into into or out of your persistence
mechanism like for ETL purposes or
analytics or something like that those
are also part of your interface so again
I said this before and I'll sort of
stadion because it's so important the
interface of a service is any mechanism
that gets data in and out of a service
does it make sense cool okay so now that
we have micro so now we have events on
our micro services now let's put that
together and do some fancier stuff so
joins right when I have a monolithic
database joins are super easy so the
problem here is that I get used to that
right so I get super all I have to do is
edit the from clause in my sequel
statement and magically I've joined
together a bunch of tables in my and
bunch of different entities in my in my
system when I split those tables up into
separate services this becomes a lot
harder so I'm going to give you some
techniques to do with it
so the first approach is a service that
materializes that join or materializes
that view for you so what do I mean I'm
let's imagine that I want to produce
something like an item feedback service
how did people at stitch fix feel about
items and with the items that we sent to
them pieces of clothing and which which
people did we send them to and let's
imagine we're going to use that for some
algorithmic purpose later on so so the
way that we would do that is the item
service is going to produce events when
items are updated or added in the system
the order service is going to produce
updates and events about orders changing
in our system and we're going to be
accumulating that in our item
Veck service and basically denormalizing
the join between items and orders so in
this model we're not doing the join in
real time at query time we're doing the
join we're reifying we are denormalizing
the join ahead of time does it kind of
conceptually make sense what I'm talking
about yeah so the idea is that we're
maintaining a denormalized join of items
and orders together in our local storage
in our own little you know database or
whatever for for this service and this
you use this technique when you have a
really high number of rows on each side
so it's called high cardinality and
database terms so if I have a lot of
items and a lot of orders for those
items this is a great great thing to do
so if that seems really weird what I
just suggested I want you to take a step
back and think about that actually there
are tons of systems that you use that do
this exact technique so almost every no
sequel approach approaches things in
this way right in relational databases
we make it super easy we write
individual things and then we
reconstruct things together on at query
time and foremost most no sequel
approaches the way that you do it is you
actually compute at write time
the queries that you want to run so you
can't run arbitrary queries but you but
you compute at write time you do the
denormalization at that point and that's
exactly the technique that I just
described another example is the wording
that I actually used here so lots of
database systems Oracle etc I have a
concept of a materialized view right
maybe it's really expensive to join
items and orders together because there
are so many and so what you what you
tell the database system to do is like
maintain that view for me denormalize
all the time and let me query it Korea
quickly search engines work this way
right so anybody uses elasticsearch has
has this tool in their toolbox analytic
systems definitely do this right any
data warehouse or analytic system uses
this technique and actually log
aggregators right things where you bring
individual logs together that's also a
join like this is this this making sense
feeling feeling more familiar on this
slide maybe than last one okay cool so
here's the next technique the next
technique is to just simply join at
runtime right so join between join in
the application or
ever client service you're using so
let's imagine that here the use case
here is I want to show an order history
page so I want to show to one of our
customers all the orders that they have
plus some metadata about themselves like
maybe their name or how many boxes we've
sent them or something like that so the
idea is we're going to first query over
to the customer service and get the
customer record for you know this
particular customer and we're going to
query over to the order service and
we're going to say find me all the
matching orders for this customer right
super simple this makes sense yeah and
how we would do this in a monolithic
database this would just be a join with
two tables in the from clause you're all
good now we're just doing that joining
in the application right we're just
simply going one place to get one half
of the join another place to get the
other half of the join and this
technique as distinct from the previous
one is best for cases where I have a
single customer so a single you know on
one side of the join and multiple orders
on the other side of the join so in
database terms the other last one was M
and M to N and this is in 1 to n join
this makes sense cool I'm sure this
seems pretty familiar to people but if
it didn't you do it all the time any web
application that goes to one that is
like constructing a page out of two
different sources of data does this
right so if you if you display a page
that has anything more than one query
typically you're doing a join like this
right you're going one place to look up
one pipe piece of information one entity
and you're going another place to maybe
get the detail or look up another entity
that that's related this make sense cool
all right we're going to step it up one
notch so I want to talk about workflows
and something called sagas so the
problem here is transactions write
transactions are awesome they are so fun
and so easy and so well easy a
monolithic database makes transactions
across multiple entities trivially easy
that's what that's one of the great
things about about databases right so if
you're using a relational database you
get acid transactions I can update a
customer record and all of her orders at
the same time all in one transaction and
it's easy once I have split those
tables in two separate services this is
really hard so the answer is very much
not do a distributed transaction with a
two-phase commit please don't do that
that that will kill your scalability and
be a real performance bottleneck but
still you want to have all this thing
happen all at once or not at all right
that's the a and acids I want to make
sure that I upped at the end of the day
I update the two entities at the same
time or alright or I don't update either
them okay so the trick here is that we
want to turn that transaction that super
easy transaction into a slightly more
complicated but scale but more scalable
thing called a saga so a saga is a bunch
of trent of steps connected by events
that are basically a state machine so
we're going to model this transaction
across multiple entities as a state
machine of individually atomic events
and once we do one event we're going to
do the next part of it we're going to do
the next part we're going to do the next
part of it okay so the idea is to
re-implement this thing as a workflow so
let's imagine we originally wanted to
update a s and B's and C's all together
into single atomic transaction in a
distributed microservice world you can't
do that in one go but you can at the end
of the day at the end of this workflow
make it happen or not happen so the idea
is you update the a it sends an event
that update we're going to update the B
that sends an event we're going to
update to see that sends an event will
update the D you get the rollback by
doing what are called compensating
transactions or compensating operations
in there in the reverse in the reverse
so if we had a failure along the way we
would send events to kind of back
through this chain back the other
direction through the state machine and
kind of undo the stuff that we need to
do right so we remove all the DS that we
added or tried to add that then that
says events and that's going to remove
all the fees and remove all the B's and
revolta etc does this at least
conceptually make sense yeah it's way
more complicated than a database
transaction I'm not going to tell you
otherwise but it is at least it is at
least an existence proof this is a thing
that you actually can do this is so this
is the way that you can get the kind of
transactional characteristics
something similar to track transactional
characteristics when you're in the micro
services world so again I want to bring
this back to something that's maybe more
visceral for people so if you think that
no system works the way I just described
not true there are actually a ton of
systems that work this way and they're
really complicated basically anything
that you talked about as a workflow
works the way that I just described
so payment processing right the it
starts with you know please take 50
dollars out of this customers credit
card and put it in our account there are
a lot of steps that go into go into that
flow those steps are not done in a
single immediate atomic database
transaction right we do not go across
all the different credit card issuers
and banks and acquiring stuff and all
that kind of stuff that does not happen
as a transaction that does happen as a
workflow or a series of steps or saga of
individual things connected by events
and anything that we talk anything we're
we talking about approving something
like an approval workflow like an
expense report maybe in your in your in
your company that flows through you
needs to be approved by X person then Y
person then Z person this is exactly the
same thing right and if somebody at the
end of the chain or at the highest level
of the chain says nope Randy's expenses
like that you know that Jazz Club sorry
that's not something that we cover
more's the pity but okay so that goes
back down to the chain and like gets
rejected all the way down and maybe if I
can convince my boss that no it there
actually was a business purpose for my
going to the Jazz Club and yeah then
maybe you can go back up again does it
make sense yeah yeah okay cool so it
would not be stock would not be complete
without at least a nod to serverless
right so one of the interesting one of
the maybe obvious use cases for service
techniques functions of the service is
exactly the thing that I just described
so an ideal use of functions as a
service or what we're calling server
lists is this super lightweight product
processing that we do in that in that
chain of events in one of these sagas
right so let's imagine that a does its
operation B does a little bit of
processing
and then you know sends another event
she does a little bit of processing etc
those are super lightweight logic like
those are probably examples of you know
ten lines of code twenty lines of code
really simple if you get right down to
what the logic is they are stateless
right they are triggered by an event and
then they do their operation and then
they produce other events out the other
side and and so they're perfect for
something like Amazon lambda or Google
Cloud functions or something like that
this makes sense yeah it is obvious but
let's just say out loud Micra services
are not the answer to every problem
services server list is not the answer
to every problem but these are just
tools that we have in our toolbox and a
healthy company now and in the future is
probably going to be combining
techniques that use micro services for a
bunch of stuff use serverless or
functions as a service for a bunch of
other stuff it will be possible to do
something all in let's say functions in
the server list track yesterday there
was a great example from one of the
presenters about like the built this
whole company and its whole elearning
system around events that's awesome
but the vast majority of systems are
going to be a combination between
different techniques just like they are
today okay so I want to running out of
time here so I want to just sum up a
little bit
so again modern software development
doing things quickly and effectively in
a in a highly data centric environment
like we have at stitch fix so for
organizations I strongly recommend that
the way that we move quickly and
efficiently is with small teams with
well-defined areas of responsibility in
terms of processes and practices
test-driven development and continuous
delivery in terms of culture a DevOps
culture of ownership and you build that
you run it and then finally leveraging
the tool at least at a certain scale
leveraging micro services as your
technological approach here so thank you
very much I would be remiss if I didn't
let everybody know that stitch fix is
hiring
so those 75 engineers we were hoping to
grow a bit more you there the link is
here we are hiring an application
development and data science and
platform everywhere we are based in San
Francisco but actually more than half of
the engineers on my team are remote and
they
from all over the United States
sometimes from their houses sometimes
from smaller offices offices that we
have in in other parts of the country
like Pittsburgh and Austin so feel free
to grab me personally you know while I'm
here I'll be here for the rest of the
day you can tweet at me at Randy shout
or connect with me on LinkedIn we'd love
to if any of this stuff sounds really
interesting you want to learn more about
it I'd be happy to talk with you so
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>