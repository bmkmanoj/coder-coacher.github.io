<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2015 • Wings Stay On or Replacing On Site Search at Wikipedia • Nik Everett | Coder Coacher - Coaching Coders</title><meta content="GOTO 2015 • Wings Stay On or Replacing On Site Search at Wikipedia • Nik Everett - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2015 • Wings Stay On or Replacing On Site Search at Wikipedia • Nik Everett</b></h2><h5 class="post__date">2016-01-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ubj_IfH8BCA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so the subtitle of this presentation is
not there the subtitle of this
presentation is how we replace search on
Wikipedia I didn't break anything and
since I'd like to keep you guys on your
toes there are two lies in this subtitle
because it's just more fun to lie right
so the two lies the first is when an
engineer tells you did it they didn't
break anything no no no no no not at all
I broke plenty but we broke plenty
rather it was a group effort anyway the
other part is in the Wikipedia so I see
this is windows so we said we were
placed search on Wikipedia but that's a
lie I mean we did that but we also
replace it on wiktionary and on wiki
data and on commons and wikiquote so
there's lots and lots lots and lots lots
of different wikis right there there are
888 or something if my notes don't lie
right there's some file week at work
count it right but that's in about two
hundred sixty five languages and you
know I could go on and talk to you about
how we don't how we don't speak all
those languages and how we rely on Lucy
for language support they could tell you
about all awesome features that we used
but that would be an unconscionable
waste because I did exactly that when i
was talking at this presentation which
you can go watch and you'll get the blog
post link so you'll be able to go look
up the other presentation or you could
just internet stalk me or whatever and
you can find this presentation it was a
double right we had I had my friend Chad
who worked on the project with me there
now let's see ah so
I've lost my place oh so yeah I was
going to tell you about why we chose
elasticsearch but I don't have time for
that I gave this presentation last night
and I went way over and so I'm cutting
things out but uh the the well I'm going
to talk about half of why we chose
elastic source the part of the reason
was the community this is this was our
old product or our old search it didn't
have any community it relied on this
which is undocumented impossible to
install but i'll skip to the part that
sort of more interesting we really
really liked elasticsearch cusp of
automatic short allocation and we
probably liked that more than we were
more excited by it than we really
deserve to be because with the old
search system we had forgotten how to
move shards from node to node right the
documentation was gone so figuring out
how to do it would have been an exercise
in digital archaeology which sounds
really fun but it isn't right and we
were we would totally have brought down
search right I mean that thing would
just like restarting a node would bring
search down for 15 20 minutes so we were
really quite worried that we would have
broken it if we tried to move things
around I also would have told you that
we loved the analyze and cat api's and
they're great and they are great but I
don't really have time to talk about him
so skip um then there's the other big
thing that we jumped on liked about
elasticsearch was the api for creating
the mappings so at the time when we were
working with we started we started
working with solar and soul are the only
way to create the mappings at the time
was to copy a file onto disk in some
place and that was just sort of painful
and it just wasn't the way the
application wanted to work wait so the
mapping API that elasticsearch has is
was way better and I know that Solar has
gotten better at this so this is not
really a fair thing on the other hand
this is this link doesn't work skip it
but pretend this link show is
documentation for a link for an API and
solar so it turns out that there's an
API and it was an API in solar and I
think it still exists where you can do
an HTTP G
at to delete an index and I you know
found this a peon I was like oh that's
really cute and I copied it and paste it
on over IRC to my friend Chad who in
that other conference was standing right
there on his great suit anyway so he
moused over the link and his IRC client
was like oh let me give him a preview of
that link nuked his development index
and I felt like a really awesome troll
for the day and we walked away we were
done no more solar for us right um so
let's let me go back to what I to the
point of this talk right the point of
this talk is about how we didn't break
anything see I lied again right we only
broke a little bit of stuff and how we
did that right this is an elastic search
track so you know I'll talk a lot about
elastic such as well and i just did talk
a little bit about it right so you're
getting your money's worth but yeah i
want to talk a little bit about how we
didn't bring search down for forever and
didn't screw too many people didn't
break their workflows things like that
so the first step and i don't have a
beautiful picture to show you for this
one is that we made the decision not to
fork elasticsearch we actually as you'll
get I'll get to later we submitted a lot
of patches to elasticsearch we found a
lot of things but forking elasticsearch
would have been a nightmare from like
like keeping our own branch of elastic
search and back porting all the patches
would have been a nightmare for us
because we were a two-person team right
we just didn't have time for that there
was plenty to do while we waited for
elastic search to cut the next release
and even then I have never sent a patch
to elasticsearch maybe a documentation
patch but every every pass that I've
sent it isn't documentation they've ya
know said ah do it this way rename this
it make this more consistent or oh god
what are you doing that's totally not
the right way to do that and that's okay
right that's what code review is for but
if I had just like merged that into my
local branch I would have diverged quite
a bit from the elastic search release
and that would have been really painful
so we didn't do that and I'm really glad
about it
so the next thing that we did was we had
beta communities these are communities
that were super excited about the new
search probably I have no idea why they
were so excited like they were super
excited so Italian Wikipedia was yes we
will take your new search as fast as you
can give it to us and they reported so
many bugs and they worked with us while
we learned elasticsearch and they were
great they're wonderful folks on IRC
they were translating stuff from the
wiki communities village pump and
getting it to us we had so many bugs
that came from them cadillon Wikipedia
as well same deal and finally meeting
wiki org mediawiki org is one of the
traditional canary sites that the
wikimedia foundation uses for figuring
out whether we're going to break things
when we when we do deploys every week
the first sight that gets things the
media wiki org and we're always on
mediawiki org right we're looking at
documentation i linked you to a couple
or I brought up a couple of links on
mediawiki organ I'm going to go back to
it on mediawiki org it's it's we we we
hit it a lot right and I've broken
search on mediawiki dot org far more
times than i would like to count i get
it two weeks ago it's that's but that's
okay because that's what it's for it's a
canary right but it's also neat because
it's mostly system administrators and
developers they go on there and so great
at filing bugs man I mean it's not just
like it broke it's like it broke when I
did this and this so it's really this
has been a great canary site for us
right so the other thing that was great
was that right around the time after we
gone to Italian Wikipedia I may be
getting my time lines off a little but
right around the time we get to the
Italian Wikipedia some other folks at
the foundation wrote this beta features
plugin see there's mediawiki org okay i
had i did a search on me to wake it out
order to get here so I'd have known had
five broken search anyway so this beta
features thing lets users opt into a
certain to a certain thing right to a
certain feature so we let users opt in
to search right we gave him an option
called new search and they could click
it and then they go into our search and
they could beta test it and it was
awesome turns out the beta features was
actually written for visual editor or
hover cards or something I kind of
forget so the
is beta features are really only useful
for people that log into Wikipedia and
most folks that log into Wikipedia our
editors not all editors log into
Wikipedia some just contribute as IP
addresses as it ons right but others log
into Wikipedia and they they can set up
their beta features right so to define
my terms a bit editors are people that
edit Wikipedia if you've just viewed it
your reader that's what we call you it's
okay readers are good we like them
editors are good because they contribute
and this is kind of a silly term wiki
gnome it's probably not the correct term
but I've heard it bandied about if it's
kind of hard to read but that upside
down ! up there says this page contains
humorous material and should not be
taken seriously but I'm going to use it
anyway in this presentation because
there really are people that go in and
what these guys do is they they go and
make some minor change right so there
are like templates that have to include
a date and if you use it doesn't fill in
the date well some of my go fill it in
for them or if a link is a reference but
it's to a website well we don't like
we're reference links to websites we
like reference links to internet dot arc
Internet Archive right so that if the
site gets obliterated or someone takes
it down or whatever we want the thing to
live right we don't want dead links and
references or maybe if a reference is a
dead link they'll mark it as it did like
my god like these are great things
they're great for the health of this
information source right well my
favorite wiki gnomes are people that do
searches for spell checks partially
they're my favorite because a couple of
them were beta testers and they jumped
on me because I broke a lot of their
things right so it turns out that these
people who do searches for spell
spelling corrections are really really
interested in or rather they're really
really sensitive to stemming you know
what stemming is anyone not know
swimming is okay all right stemming is
when stemming is when you take the
grammar out of a word so if I wanted to
search for wiki gnomes well I you should
that should find wiki gnome as well
right without the S on the end and it
gets more complicated it gets a lot more
complicated than Russia right or in
polish or some some of these languages
which are heavily decline right
anyway so these wiki gnomes they want to
know they want to find just the word
right just the word that they're looking
for so they're also really sensitive to
something called transclusions and let
me define that so ah that's unreadable
that's unreadable too but it's better so
Wikipedia and wiki's in general
certainly mediawiki wikis are made up of
wiki pages which include templates which
can include templates which can include
template which can include templates
which can include templates some of
which are written in Lua right because
the old templates were actually like
touring complete except it was really
really slow and so someone said wow
you're writing all this hard code in
these templates these are horrible hacks
let's just give you a language that you
can write your code in and Luis nice and
sandbox and whatever right that doesn't
matter let's move on from that but what
matters is that these pages are include
templates and sometimes people want to
search on wiki text and sometimes people
want to search on the actual output of
the page and these curly brackets mean
invoke a template and stuff like that
right uh let me move on this is by the
way the template source for linking to a
YouTube video I have no idea like I
haven't read it Oh God so I was gonna
yeah let's get that right alright so we
had these beta users right we had these
beta is beta we had these beta wikis
right well that's useless if you don't
actually learn what they want right if
you don't listen to them right so I told
you they file bugs right so I'll give
you an example one person filed a bug
they said I need to search in the wiki
text of the document right that's that's
that template stuff that i showed you
well so the point the reason we did this
search project like the search project
has three count em three requirements
search wikitext or sorry search the
rendered text of the page for regular
users keep the index
to date in a real-time way in real time
was not well defined but that's okay and
three was nobody can hate you those are
those are the three requirements right
which is pretty normal I guess anyway so
this is the render text to the page
right if we if we take out you know the
pretty pictures and the tables that are
over here and the links and things
that's the render text of the page right
so that's what most people want to
search well some folks want to search
the wiki text to the page remember my
spelling correction people they care
about misspellings in the wiki text
because that's what they have to edit
that's what they have to fix right I
don't care about they don't care about
the rendered texture I mean they do
that's ultimately why they're doing it
right but that's not what they need to
find so when we deploy to meeting we are
rickey.org someone said I need to search
on on the wiki text and we said okay
that's that's absolutely true so we
ended up having to render so we ended up
making this in source operator which you
can't read but uh zooming in a slightly
less hideous way so we made this in
source operator that would search in the
source see how the text the highlighted
portions are actually wikitext and when
you compare that to sourcing and to
searching in the page this is actually
the render text so to do that though you
have to actually index both parts so
this is the giant JSON blob that we
store for that Barack Obama page and if
you'll have to believe me because I'm
not going to zoom in because it'll break
everything but you'll have to believe me
that that's the text of the page and
that's the source text of the page so
they're both available and for those of
you playing along that aren't in this
room you can follow that link and find
and search for quote text and quote
source text and find it and if you have
a nice browser that renders JSON nicely
then it won't be a giant wall of text
it'll be two giant walls of text so
turns out but that's not what the person
who filed that bug wanted turns out when
we saw when we did that when we gave you
the
operator and deployed it they said well
that's not right so I wanted to find
this page manual WG namespace aliases
when I search for namespace aliases and
you know you it's hard for me to see so
I'm sure it's hard for you to see but
they were searching for namespace
aliases like like camel case right lower
case and capital A for Nancy's aliases
but that's not what the page is right
the page is GG WG capital namespace
aliases and that WG is in there right
and a normal search is just going to go
the wgs alias alias that's a word so we
had to make search work on Pascal case
camelcase Nate case in constant case so
by the way the reason this worked is
because this page is made up of a
template that includes the knit includes
namespace aliases camel case just like
that so if you're searching wikitext
you'll find it and that's what the old
system did that old system search
wikitext kind of it tried to use Java to
parse wikitext and it was halfway and it
halfway only halfway worked but all
right so ignore this for a bit the other
thing that we broke this we seriously
did break was used to be able to search
for food ash bar and only get food ash
bar if you search for food spacebar it
would find food ash bar and food space
bar the food ash bar search behavior was
really useful for people who were
looking for misspellings where the dash
version is a misspelling and we didn't
implement it we just broke it and the
reason for that is that it would have
required a custom analyzer that would
have been now after after a year I would
have been able to ride it but then we
couldn't even deploy plugin so the
elastic search cluster because we hadn't
figured out how to do that yet from
opera from an operation stable and I'll
get to that but so we said look we can't
do that instead we made him this regular
expression search that was hideously
slow because it was a
it was naive right it would just search
every every document so I mean yeah
there's sharding and but no it was it
was they would take minutes and by that
time varnish had timed out this was
hideous the moral of the story is
eventually we did deploy plugins and
eventually we got a trigram accelerated
regular expression search which is
actually quite quick I mean not quite
quick it still takes 10 or 15 seconds
but that's totally ok for this use case
I promise and so we did eventually solve
the problem but we never gave him the
dash syntax we just got nice regular
expressions which people have been
asking for anyway so anyway getting to
this pit do this stuff the reason so
it's not useful to implement all these
features if you didn't break them right
so we had to do a bunch of testing on
this and in fact the old system didn't
have tests didn't have integration tests
and have unit tests didn't have any
tests so I was determined we wouldn't
leave that we wouldn't leave my
successor you know if in 10 years
someone has to rewrite the search again
they're gonna have a spec right so there
are like thousands of these things and
the interesting thing is that when you
write these integration tests they don't
just test they don't just test code you
wrote my test elasticsearch they test
plugins this tests the ICU plugin this
is I don't actually know what that where
it is this is 3 or this is four
different representations of the same
word when you normally when you do not
Unicode normalization they're the same
but when you don't tune in Unicode
normalization they're different and you
want when you search for any of these
four to find to find them together right
they're the same you want you want the
lastly search to think of them in the
same term that's what this test is
asserting right we also have this test
over here where we assert all kinds of
stuff about highlighters and that goes
in and asserts about the way the
highlighting works and the highlighting
extension that we rode and that has a
lot to do with the queries an elastic
search and there's this huge interplay
of things that we're testing here right
but what matters is ultimately that the
right answers come out of the search
system so unit testing this would have
been silly because unit test
anything like this would have been silly
would have been silly to go oh my search
system produced the right JSON and it
sent the right JSON to elasticsearch and
if elasticsearch sends it the right JSON
then we render the page correctly this
doesn't give me any confidence that we
haven't broken anything so elastic
searches is involved in all of the tests
of our search system and this gave us
the confidence to do things like upgrade
from Eau de at nine to 10 to 11 you know
and I'll beyond right and yeah we had to
fix some things we had to change some
things but these tests actually caught
everything right when we upgraded and
break anything it was great it was
perfect it's awesome and I feel really
proud that these things worked right so
next the next thing you have to do is
citizens Wikipedia right this is a
pretty big site on the internet right
there lots of users you may not know
that there's on site search on Wikipedia
that's okay right most people come to
Wikipedia through google but we get more
than a million searches an hour and we
get like like several million like five
or six million prefix searches an hour
it's pretty decent size right so we have
to do some load testing so I threw
together a quick and dirty python script
which if you follow the link on the page
it's not very long right it's like a
hundred and four lines including
documentation and shebang and all that
and it's probably wrong but we used it
anyway to try to try to test the this so
we were playing the old searches logs
right and we wanted that you know we had
sampling so we would do things like I've
replayed five percent of English
Wikipedia searches against the new
search index and it would work and the
very but the very first time I crashed
it at twenty percent I didn't crash
elasticsearch I just overloaded it so
when we first when we did the first
employer we only got up to twenty
percent of the search traffic that we
actually were going to we actually
wanted to handle in the end and that's
not enough right I mean yeah that's just
not enough right so we had to break out
the you know tools to figure out what
was slow right so elastic search comes
with this hot threads API which is
documented in this link
these are all links by the way in the
blog post I'm not just bringing up
random web pages anyway the hot threads
avi is great but it has this tendency to
measure which threads are hot and then
those those threads have then it
measures then it gets the stack traces
that are going on in those like the hot
threads like the three or four hot
threads that's great except if those
threads are hot during the measurement
time and then they're not hot during the
stack trace collection time and when
you're when you have hiked we reload as
opposed to high indexing load this is
very common that the threads that you
get in hot threads are actually not hot
anymore but time you look at them so the
only thing you can do is actually hit
elasticsearch with j stack or it does
the same thing as when you see quit a
java application so when you see quit a
java application on standard out it just
spits out all of the stack trace right
it sits out although like hundred
threads that elasticsearch has running
at the time and well we don't want it on
standard out right so we hit it with
this thing called a stack which spits it
out to the standard out of j stack the
command and so then we pipe that pitch
it in a file then we wrote a bash script
used talk to count or use grep I think
actually to count the number of threads
that were hot in certain places so it
didn't it didn't actually count the
threads that were doing a lot of things
it just sort of we just ran it a bunch
of times and said oh this time when we
ran it there were six threads doing you
know doing work on the fray suggest sir
and there were three on em fell and they
were two on this right and then we would
run it again and then again and then
again and it would just sort of count
where the threads were and then the
threads that were spending the most time
in these various places we're probably
those were probably the slow places
right and there you know the JVM has
safe points and all this other stuff is
so that's kind of it's complicated right
how that works but for the most part we
figured out that we had some issues one
with em Bell so we filed this issue ah
and it turns out that it's just and it
turns out that we could fix that on our
own it was
one line change plus some comments and
that made the script scoring like an
order of magnitude faster thanks em bell
then we had some other changes we had
another change which we fixed up stream
which we have streamed into
elasticsearch some speed ups for the
phrase suggester right we found these
out by doing what I said by doing the
basket by wrapping and finding whether
we were spending lots of time and some
things we had to do on both sides so we
we had to implement multiple Reese cores
in elasticsearch then we had to use them
in our extension right so for those we
waited after we implemented an elastic
search like a month or two before it
came out anyway the other interesting
thing about this is that we did this
load testing in production because we're
a non-profit we really can't afford to
have two huge clusters to do the load
testing with so it was just very careful
to not crash the bait crash hit while
the beta testers wrong there and I'm
pretty sure I accomplished that but I
mean that's what you get when you don't
have that much money I mean wasn't we're
not that bored but still fix that please
anyway wow that's hard to read so the
other thing that we that was super
important for us was mediawiki vagrant
mediawiki vagrant I'm so big so vagrant
is a system that sets up a virtual box
vm and then runs puppet against it
that's not about all that's sort of
interesting there that's basically what
it does and the puppet the puppet stuff
that we we made so the whole setup
together we made mediawiki vagrant its
job is to set up a virtual machine that
is very much like our production cluster
it's like if you scaled down the
production cluster and only ran it on
what machine so it can do things like
host multiple wiki's in fact when you
set this up for us for testing search
right you add the little thing you say
yes I want to test search you take a box
when you run a command that takes a box
for you basically it will download the
ubuntu vm spin it up clone the code set
up apache install my sequence all
reticence tall elasticsearch install
some extra elastic search plugins create
a couple of wikis and index them
and that actually takes quite a while
because puppet is slow but eventually it
gets set up and then you can use this
stuff right and you can you can use your
development environment and this is neat
because if you break your development
environment you can just destroy it and
rebuild it and it doesn't like your
development environment is no longer
canonical right what's canonical is the
development environment that the puppet
scripts setup just really cool right
because then if production changes or if
if you can't reproduce some issue new go
and you can fix puppet and destroy your
vm rebuild it and then if you can
produce the issue well that's awesome
right now your development environment
is more production like it's more it's
it's as broken as production which is
really important right so this this was
sort of an important way of us of us
getting users to the point where they
could run all those fancy integration
tests that we wrote no performance
testing because well you can't
performance test it in vague rant but
just no virtual machine inside your
laptop is not gonna be anything like
running normally so I would hear talk
about plugins let's see oh wow I
actually do have time so we ended up
doing a lot of development or
development in plugins eventually so we
were sending patches upstream to
elasticsearch at some point we started
doing a lot of plugin development inside
so I we do started doing a lot of
plug-in development inside the wikimedia
foundation and the goal here was so that
we could iterate more quickly so we
didn't have to wait for a release and
this worked right this was this was
pretty cool they ultimately wrote a new
highlighter because the ones all the
ones in lucene don't work that's why
there are three of them because no one
of them works we wrote this grab bag
which contains a bunch of backports so
you know I said we didn't backport a lot
of stuff from elasticsearch again I like
to keep you on your toes we did right we
backported a few things from elastic
search and we did it in plugins but it
was pretty rare we had to be pretty
careful with it right and we also added
things like that's where the trigram
accelerated regex searches things like
that and then we made this Swift
repository which is where you would do
snapshots and the reason we did it is
because the last rehearsal in sports
ifft because it's not that widely used
but we use it so there you go right and
the reason you do product and reason you
do plugin development is because either
the feature doesn't belong in core
elasticsearch you have to move quickly
or your back warning things or all three
right and in I just showed you actually
three exam like an example of all three
went through anyway I wanted to talk
about uh what okay I wanted to talk
about what's been what's been painful
about this process what worked right
I've talked a lot about like the things
that work like having integration tests
and having vagrant and submitting
patches of to elasticsearch this works
really well right plugins like they all
work great with what did it work very
well is uh actually have a list of four
it was three last night that I had
another one so rolling restarts and
elastic search until one dot six they're
hideous it took us three days to do
rolling restore right so that's like
taking a note down and adding and adding
it back that takes like two hours three
hours per node and if you're willing to
script it we're willing to have some
watch it around the clock and you can
get it done in less than three days but
we're always a little bit wary about
these things and so we wanted to have a
person after every node goes down have a
person go look and say okay everything
is still good do the next one to the
next one do the next one and then at
some point your person goes to sleep
right so it has to take three days for
us to do it when once we get to
elasticsearch 106 will have seen flush
which will let us do the rolling restart
way faster but we still have to be
careful because we're doing an upgrade
we want to make we're doing an upgrading
you can't downgrade elasticsearch so you
want to make sure that if you don't do
them all really fast and then get
yourself broken right so we're going to
do one and then wait and then do a wall
but anyway the next thing is next thing
that didn't work for us was jobs or
rather they did work for us but their
pain in butt so to get synced flush to
work to get Singh flush to be a faster
only restart you have to not write to
the index ad during the rolling restart
process so to do that you have to have
you have to queue up all of your rights
and in fact we already had this because
i am so pair
annoyed about bringing down wikipedia
that we started pushing all of the work
that we do all the times when you talk
to elasticsearch unless it's in a search
we do it asynchronously we don't even do
it after we flush the requests this is
PHP by the way so we don't have threads
right we do it we push the things to a
job queue and then we pick back up pick
them back up on another system and this
is really useful because of
transclusions and you know when you have
a template that's used in a million
places you want to go and upgrade all
them so you have to go explode out the
job queue and things like that and run
all those you couldn't do that in one
process anyway but just to get just on a
regular user change we pitched that
thing into the job queue right and to
test this testing this asynchronous
thing is a huge pain right and I told
you right I'm really passionate about
making sure that we have integration
tests that actually work right and prove
things and that's it's just so brittle
to like add things to the index and then
wait for things to go through the job
queue and the job queue sort of would
break on us and they change something
and it's just like ah so anyway the
other thing is that some bugs and
elastic search can bring down all the
nodes or we can bring down lots of nodes
there are ways to send bugs to their
ways to send poison queries to
elasticsearch I won't tell you what they
are intentionally but certain wild card
workers basically can do it right and
you can consume tons of memory and that
will knock out every every shard that
got the note or that got the query and
english wikipedia is shorted seven ways
we have 31 servers so 31-7 is oh
and you know I get paged it's the middle
of the night is this unhappy right this
is bad right and I don't actually know
the answer to this I don't know the
solution other than don't let the wild
card queries bring you know consume that
much ram but that's the obvious solution
that's not the answer right the answer
is in some way to prevent yourself from
making these mistakes you don't want to
pay me to paint yourself into a corner
right and finally installing plugins
didn't work for us and the reason for
that is that
are nodes in production don't have
access to the internet so the bin plugin
script doesn't work by the way your
nodes in production shouldn't have
access to the internet either that's how
you become an IRC bot in the open proxy
malware sir don't do that right this is
bad right so everyone has to negotiate
with their operations team their dev ops
team whatever you call them they have to
negotiate a way to install plug-ins
right you've probably already negotiated
a way to install Debian packages or RPM
packages or whatever it is that you do
to get elasticsearch install but plugins
don't come that way come another way so
you have to figure out how to get those
installed and that's a different way
from how we deploy everything else at
the foundation so we had to negotiate
and get the right things and get them to
install things you get the appropriate
permissions and this is I think a pain
point many people are going to suffer
because that's the way operations
engineers are they're paranoid and they
should be paranoid that's their job but
it's difficult right so I wanted to wrap
up I've taken you on a presentation
journey I want to wrap up with the the
the the things to which we owe our
modest success and by the way I'm
actually actually think that we really
did succeed with this project even
though we did break a few things and I
and I think we succeeded because we
replace search live on a huge website
didn't bring it down right and no one
hates us no one hates me right you'd
think that this is the internet right if
I had broken something someone they'd be
you know hordes with people after me but
now so yeah we were successful right and
we owe that success to the beta testers
right they quickly pointed out problems
and they worked with us right we roll it
Bo it to the phased rollout approach
that we did and I didn't I kind of
skipped over that right we went wakey
wakey when we did this rollout we were
intentionally quite slow right we owe it
to these canary deployments to mediawiki
organ to all the people to use it we owe
it to lots of integration testing we go
to load testing we go to vagrant for
helping us build production like
environments and to actually having the
the willpower to set this up when I
started the foundation meeting with heat
vagrant didn't exist right and that's
been built in the meantime
we owe it to the plug-in infrastructure
and and to the elastic search folks or
the elastic I guess they're gold now
folks who built the thing and helped
work with us and reviewed the patches
and to the folks that helped us gland
patches and leucine and things like that
without that we wouldn't been able to do
it right and yeah so like that's how we
didn't break that much stuff when we and
we did this change right and I'm excited
by it and now I'm done talking so this
is where you clap and then ask questions
so yeah if you have questions go ahead
yes talk about being brave stuff for
like features the best that people in
whole system fontaine so yeah this the
the information retrieval question and
we did it by with the beta systems we
did it with the beta testers and for the
most so that's like touchy feely stuff
right for the most part they said yes we
find we're it is at least as good or
better in all cases but that's not
really that's not enough right that's so
sort of the two sides of it right and we
did one side and we didn't do the other
and we're starting on the other so the
first side is that we went and got a
number of canonical test cases a number
we created a bunch of tests documents
and it searches and made sure that they
appeared in the order that we expected
them in to appear in and that works an
integration test for it they don't fail
that's great and that's that's nice but
it's not really good enough right but
that's what we did and I'm sticking to
it right what we didn't do is we didn't
instrument and we didn't instrument how
good a job we were doing because that's
an incredibly complex problem how like
just doing a search right and figuring
out whether the user like to that search
without asking them right because we
don't have nobody's going to fill that
in or the people that will fill then and
are totally going to skew our documents
right they're totally going to ski the
reports so you have to do it without the
user doing it right and so for a long
time we weren't willing to do that we
weren't willing to essentially spy on
the users right so we're actually coming
to the point now where we're willing to
do very very careful anonymized data
collection but this is long after we've
done this replacement right so we don't
know we can't compare it to the old one
but what we can do is start to do this
collection and then we can start to
graph how good a job we're doing right
but how good a job you're doing it
searches very it's a very very hard
problem there's a lot of academic papers
published on the issue that I haven't
read and I have a delegate right I have
a person who and I said you go and
figure this out for me please and then
you know come up with a plan I don't
care how many people it takes right now
so the trick is remember how I said
there were two people on this project so
two of us replaced the old search right
when you have two people you do not have
time to measure this stuff you don't
have time to instrument it it's just not
know but when you have 10 or 15 that
becomes a lot more possible you can
start instrumenting things you can start
building the systems you can start
building the event logging and and pull
the events together and you can start
building systems where you're actually
able to calculate this metric and that's
what we're trying to do now but yeah
it's going to be a while right i mean
that's a hard problem and there's also
you know there's existential precedence
like you know are we doing a good enough
job anonymizing the data as it comes in
right things like that because we really
really really don't want to spy on you
really like we don't but we do want to
make search better and that's really
hard and I don't have the right answer
right I know that that's other other
than we're going to try very very hard
did that answer your question
no no that so here's the thing right we
we do things like phrase Reese cores and
incoming link counts and boosting
different parts of the of the page and
we also do other things like we let the
community change boot like we let them
configure boosts so the community has a
thing with it they'll go and they'll put
a template on the page if it's a good
wiki article or if it's been a featured
wiki article or if it's a good picture
is so for Commons this is really
important they put good picture and they
put bad picture on pictures that are
just like garbage and by garbage i mean
like low resolution and like yeah not
like distasteful anyway so we gave give
the community the power that can
configure how these templates boosts the
search results this works pretty well
right but going much beyond that doing
anything fancy like topic analysis or
semantic networks or even just changing
the relative weight of the temp of the
title to the text you need to do his
instrumentation first because you have
no clue whether you've made it better or
not right unless you can instrument how
good a job you're doing implementing you
know semantic analysis topic modeling
maybe it's fun but it wait it could be a
huge waste of time could be huge waste
of CPU resources you have to check right
you don't know right and you certainly
don't know how to tune the parameters in
in in the you know in the various
ranking algorithms that you have unless
you're measuring how good a job you're
doing right you can you can do it and
then look and go looks better which is
mostly what I've done and I admit that
that's like kind of not a good way to do
it so yeah so the trick is any time we
go beyond what's built into Lucy nor
elasticsearch any time where we want to
go and develop something interesting and
new we got to make sure that it was that
it's useful right so you know we may end
up doing things like prototyping it and
then playing the searches against it and
seeing whether the results are better
but we'd have to use the kind of thing
where we said oh this this user now is
clicking on
right remember the Facebook guy right he
was he was standing where I am this
morning and they've started to get they
have this data right uh you know and
what his team was active with his team
told him was that oh it's actually
easier to do the quick win things to do
things like have users pin results at
the top and just pin him right maybe
well I actually want to go steal that
idea and give my communities basically
give logged in users the ability to
click a button and say when I search for
this that's what I mean bring that up
right that's kind of what Google had one
of those for a long time and they
abandoned it so who knows right I mean
everyone's sort of playing catch up to
you know the big guys that that have you
know thousands of people working on this
problem when we have two or four and so
you know you have to but it meets the
advantage we have is that we're going to
open source everything right everything
we do is open source I'm not I'm not
going to go bring up all the links but
there are links where you can look at
the mapping configuration that we use
you can look at the analysis
configuration you can look at the
parameters I showed you the link where
you can look at the the source document
that we have for every article you just
append ? action equals Cirrus dump to
any article you want and it'll spit out
what we have in elastic search for it if
you do a search you can append a search
URL parameter and we spit out the search
that we sent to elasticsearch and if you
happen to have elastic search with all
of our data you could send it to but
really this is useful this is useful
debugging tool for me and this is a
useful tool for other people to go how
did they do that thing or why is this
thing working the way it is that's kind
of silly right we want people to be able
to file a bug and go and that's not the
right way to interpret my query dude so
yeah we could only do that with this
thing anyway um sorry that was kind of a
rambling response but I guess rambling
is pretty normal for me I hope it works
for you guys any other questions yes
no way using my sequels your prize our
primary storage because the wikimedia
foundation has used my sequel as its
primary storage before it was the
wikimedia foundation when it was servers
running mediawiki in Jimmy Wales
basement right it's been my sequel it's
been my C equals from my sequin will
continue to be my sequel until the earth
dies heat death right mais you know we
will be my sequel forever we don't have
constraints we don't have foreign keys
because they didn't exist when we
started where that hardcore oh and also
once you have a huge database there's no
chance you're going to add them without
taking the system offline and by the way
I'm sure there's stuff in that database
that violates those constraints and we
just have to hope that it doesn't matter
because with 10 years worth of mediawiki
development who knows what's in there
it's kind of scary production is fun
yeah so we don't use Eliza search as the
primary store there were times when we
started use a glass to search where we
had accidentally nuked things this was
mostly our fault and not elastic
searches fold we so we have a very
complex repository where we configure
the config parameters and for for a
while our extension defaulted 20
replicas because we wanted it to work
properly with people that just
downloaded mediawiki and it was the only
way that you would get the cluster green
well that's stupid now we default to
auto expand to 23 replicas that's much
better but that's beside the point at
some point we screwed up the
configuration and went down to zero
replicas on the production cluster and
then we nuked one of the shards to do a
disk upgrade or something and lost it
and that's not last i searches fault
that's totally our fault so yeah that's
when I earned this t-shirt because I
broke I brought down the Italian
Wikipedia for a while I think I actually
brought it down in the sense that they
couldn't edit the pages anyway like
you're standing up which means I'm I am
I
so I like the IR question because it's
the hard one that we haven't solved yet
and so yeah you get your nice thing by
the way the answers to your question
aren't in this book unfortunately it's
it's a really hard question and I would
love for there to be open source stuff
around this and we're going to work on
it
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>