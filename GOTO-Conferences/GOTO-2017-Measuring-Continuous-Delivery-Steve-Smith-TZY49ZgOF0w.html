<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2017 • Measuring Continuous Delivery • Steve Smith | Coder Coacher - Coaching Coders</title><meta content="GOTO 2017 • Measuring Continuous Delivery • Steve Smith - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2017 • Measuring Continuous Delivery • Steve Smith</b></h2><h5 class="post__date">2018-02-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TZY49ZgOF0w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so Sven is too modest he was just
quietly telling me how he's an expert in
choosing the wrong tools and I think
we're all very good at that
I certainly am so hello I'm Steve Smith
and I'm going to talk about measuring
continuous delivery let's have a show of
hands how many people here think
continuous delivery is hard okay thank
you
the rest of you are liars please leave
the room
now I have successfully got the audience
against me I can continue so yes I'm a
continuous delivery consultant Pro don't
leave ace in London I'm a trainer I am
the person that you phoned when Dave
Farley is stuck on a plane and just
Humbert is leading and have a protest
march in curry in California
I remember what yesterday's protest was
so I'm also the author of a book
measuring continuous delivery which was
on lean pub Animus osage of equal
experts which is a network of
independent delivery consultants which
is now worldwide so what does it mean to
actually do continuous delivery right so
continuous delivery is a set of
principles and practices and the list
keeps growing and we don't seem to prune
very many the way I've the way I think
of it is that you're in a state of
continuous delivery when the stability
and speed of your release process can
satisfy business demand if your release
process cannot match business demand
then you're in a state of discontinuous
delivery okay there's a phrase that I
invented I run out of source calm and
literally just is like antonyms of
continuous it's a thing now it's mine
okay so
continuous delivery involves a lot of
technology changes okay so this is a
list I cooked up a few lovely people at
my dining room table a few months ago
let's pick out just one of these right
so database migrations how many people
here work in an organization where
database migrations are fully automated
okay
a lot of hands we're not I'm suspecting
dishonesty or optimism
okay well that knows good series that's
what I kind of expected right so let's
think a little bit about that okay so
there's an implementation cost to doing
that right it's a difficult with a
difficult problem to solve even though
they were awesome tools like fly way out
there now to do it for you I know
there's some risk involved in doing it
but if you think about the value of that
work right if you think about how that
will help improve stability and speed of
deployments then clearly that's going to
benefit you and your company okay
now apply that same kind of value cost
ratio to all of these practices because
you probably do need all of them I'm
afraid and then as spam was mentioning
there's just there's so many tools that
the tool space was just exploded for
continuous delivery you know there's
about 19 different logging tools now you
know there's loads of cool monitoring
tools and the good news is it doesn't
really matter which tools you use
because as long as you avoid the really
really bad ones you're going to be fine
okay so you can use kubernetes you know
that's good you can use docker that's
good you can use solaris as owns that's
good
just don't use WebSphere ok don't use
WebSphere ok applause of course and the
one thing you don't want as a speaker is
one person applauding because that shows
that it has no commercial future so
anyone here from IBM no they never come
despite my repeated invitations so
there's also a lot of organizational
changes that need to happen this is
obviously much more contextual and yes
these are all more valuable and more
challenging than the technology changes
that you need to make so let's pick out
every one dealing on call so who here
works an organization where everyone
does on called for production developers
testers product owner business analyst
for the other one ok I can put my hat so
my last client we did that actually
which was really cool like people were
showing the phone
it was very different immerse the
company worked and it was you know
worked it worked really well I've got so
I'm a real big fan of it so again think
about the kind of the cost of doing that
right I think about the contractual
changes that needed the persuasion
efforts to get people on board with it I
think about the risk of it going wrong
of you know someone sleeping through a
sleeping through a call as I did a
couple of weeks ago pager UT beat me and
I carried on sleeping but also think of
the value of that work right think of
the shared sense of responsibility
everyone would have for the production
services the increased situational
awareness that people would have okay
and again you you need all of this stuff
okay but it's not these changes that
make continuous delivery difficult okay
this is a list of stuff that we can all
understand a reason about okay what
makes continuous delivery difficult is
applying these changes to your
organization to your own circumstances
and constraints okay your organization
is a complex adaptive system every
individual has a limited amount of
information even the continuous delivery
consultant who might claim otherwise you
can only reason about the cause and
effect of an event afterwards okay this
is what makes continuous delivery
difficult it's different in every
company every single time this is why I
don't have a partner I'm product and I
never will
and I keep committing to that on camera
to stop myself in doing it sometimes I
mess it with Dave Farley recently and I
would see now I'm never gonna do a
pipeline for Damon I quite agree Steve
but I'm doing one of my spare time would
you like to get involved absolutely not
because Jenkins is just fine anything
that's composable that will let you
build stuff in your circumstances is
what you want okay so given that that
the kind of the core challenge we have
continuous delivery the need to apply it
to your own company what's our best way
to adopt it well we need to continuously
improve the stability and speed of the
release process okay we need to
increment then iteratively improve okay
in a way that doesn't break things
unnecessarily in a way that gradually
delivers benefits until we can start to
match business demand
the best way we have to do that is to
use the improvement Carter so I'm sure a
lot of people familiar with it my Grover
came up with it from his work with a
Toyota so with the improvement Carter we
establish a set routine around our
existing ways of working we repeatedly
use the same four step process to
gradually introduce change into the
organization
so we've step one we create a vision of
success we paint a picture of some sense
of future economic prosperity we try to
attach some kind of sense of urgency so
that people feel inspired and want to
contribute to the program we've stepped
to we grasped our current condition we
understand how far we've traveled on our
journey we try to get a sense of how far
we still have to go and that's
qualitative data stories narratives
interviews experience reports and it's
also quantitative data whatever kind of
numbers we might have lying around
we've step three we need to establish a
target condition so we want to create an
intermediary milestone we want to try to
achieve something on our journey toward
success in a reasonable period of time
we're attached to that some success
criteria and a time horizon and then in
step four we iterate towards our target
condition we use the plan do check act
cycle from dr. Deming and this is where
we run different experiments in parallel
we tried different technology and
organizational changes that try to drive
us towards a speed or a stability
improvement so I've mentioned dr. Deming
in about eight minutes Dan off would be
very proud of me forget my meantime too
Deming down and the last time he saw me
he's doing a talk today and I defy him
to do Deming faster than eight minutes
without cheating you can't just like
start by saying oh hello I'm Deming or
something Harbhajan not allowed so if
we've understood the challenge behind
continuous delivery and the process of
adopting it at least a process that
gives us a reasonable chance of success
the question now becomes more how do I
know if I'm doing well right how do we
know what our vision of success should
be how do we know what our current
condition is how do we know what
intermediary milestones we might want to
work towards
and how do you know if stuff is working
how do we know which changes are having
a positive impact which are having a
negative impact in which you've had no
impact at all and the best thing to do
is to measure of continuous delivery
it's to measure the stability and speed
of the release process and by doing that
you can reduce the uncertainty that's
all around you and it empowers you to
make better decisions as you gradually
adopt continuous delivery you can
understand if you need to improve
stability or speed next you can
understand they've shared on quarters
had a positive impact you can understand
if automated database migrations have
helped or hindered you so if that sounds
hard
it shouldn't because the hard works been
done for us see you've probably aware of
a dr. nicole forstman and just humble
and friends work on the state of DevOps
reports so that's like an annual
worldwide survey that's published so
DevOps as a thing makes me very sad but
these reports are absolutely awesome and
over the years they've uncovered all
kinds of interesting things they found
that organizations that deploy more
frequently to production are twice as
likely to exceed market share
productivity and profitability
expectations
they've also published some
peer-reviewed academic research that's
proven that as you improve stability you
also improve speed and vice versa ok so
for many years big IT companies have
told all of us that you have to choose
between stability and speed you can't
have them both those companies have been
lying to all of us because they want our
money they want to bathe in our money
essentially and I'm you know I'm sorry
to be the one to break the news to you
so why do we care about this news
lapentti knows delivery leads the strong
IT performance
well the means by which they've arrived
this new knowledge is something that we
can use ourselves so the cold occurs and
friends have always used the same
measures of stability and throughput so
with stability they use change failure
rates the percentage of changes that go
wrong and they use failure recovery time
the time it takes to recover from a
failed
deployment or fail change and we're
throughputs they use lead time which is
the time it takes to make a change
and frequency which is helped and you
make a change so these are two
measurement pairs there's a really good
book called how to measure anything by a
guy called Douglas Hubbard I really
strongly recommend and in the book he
talks about how you can form
clarification chains with measures you
can take something intangible like
stability and make it tangible by by
combining different measures together
these measures are quantifications of a
thing at a single point in time so you
have some new information as a result of
that that's a value to us but we can
increase the information value by turn
them into metrics right so if we measure
these things on a regular basis not just
once a year in our organization then we
can understand trends we can see if
particular changes are improving
stability or speed or if they're
starting to harm our progress and on top
of that if we apply the same temporal
properties to them and apply them to the
same visualization then we can see how
these metrics are changing in relation
to one another ok so that's what I call
indicate as an indicator is a
visualization of emerging trends in a
problem domain ok so a couple of years
ago I had the idea of taking the colon
jezza's measures and turn em into
indicators so we'd have a deployment
stability indicator where on a regular
basis you would visualize together
deployment failure rate and deployment
failure recovery time measurements and
with deployment throughput we visualized
together deployment lead time deployment
interval measurements ok now it's
important to have these measurement
pairs together ok because one on its own
it can be very misleading so for example
with deployment throughput I visit a lot
of companies that tell me how amazing
the deployment lead time is and they say
you know Steve we can you know we can
deploy in three days like that's really
great how often you do it only once
every few months you know well why is
that oh you know there's all this hard
stuff you don't know if you know there's
this person you won't talk to me he
won't you know he won't deploy things
for me you know so you can maze too
often it happens
so one of these on their own is not you
know good news so for example it's all
very well saying you know our deployment
failure rate is tiny Steve you know
nothing ever goes wrong oh you know how
long said fix it oh you know a couple of
weeks you know when it does go wrong you
know everything is on fire you know we
don't know what to do okay and on top of
that with the exception of the toilet
failure rate which is a bit special
because as a percentage deployment
failure recovery time deployment lead
time at the Florin interval I also
thought about measuring variance as well
as the average so if we look at the
median and the standard deviation of
these then we can understand if an
improvement or a regression is
consistent or if it's just a flash in
the pan all right so that was all very
hand wavy there's a little bit more hand
wavy stuff so a deployment stability
indicator looks like this this is a
generic technology value stream I made
up here is some code going onto a branch
please don't do that when I'm looking it
really makes me sad too the whole
branching thing then you emerge into
master it should have been in the first
place what took you so long why didn't
you do that in the first place you know
what why why why and then this is going
to turn into a branching talk I need to
be very careful here there's a bit end
about branch and got luckily then I'll
be running out of time I'll have to
speed up so then you put the code into
your build server and you know you build
it and that works you know good for you
and then you do some stuff please don't
do too much stuff too many test
environments you know don't try to have
you know few of them you know three or
four and then eventually goes into
production and it goes bang it wasn't me
I didn't touch it and that's your
deployment failure rate right that's
your percentage of production
deployments that go wrong and your
deployment failure recovery time is the
median and standard deviation in time
between the start and end of all
production failures in your time period
all right so here's our deployments
throughput indicator so again you know
code on a branch Yetta Yetta put on
master it belongs and then deployment
lead time is the time between you
successfully building an artifact and
its subsequent deployment into
production and deployment interval is
the median and standard deviation in
time between production deployments now
don't let that cute little arrow fool
you that's where you will be losing a
whole bunch of time I'm willing to bet
okay so that's all super duper but what
we really need here is an example right
and a few lovely people I have one so
between 2014 and the spring of this year
I worked for about two and a half years
in a UK government department as an
Operations team lead it's responsible
for two-thirds of all transactions in
the British government my team was
responsible for delivery enablement and
release engineering so that means we
tried to help teams to adopt continuous
delivery and we also owned the build and
deployment tool chain we were operating
services on a secure multi cloud
platform as a service there were 60
teams with about a hundred different
digital services and about five hundred
micro services by now it will be loads
more and those are predominately written
in Scala and run in a couple of
different cloud providers okay so the
value stream in this government
department is very pleasing there were
two testing for Amazon one is optional
so code goes to an integration testing
environment that's optional if you
really really have to do some end-to-end
testing please don't do too much you
know you don't really want to spend too
much time you know it's a bit theatrical
of the best of times there was a
mandatory performance testing
environment and then there was
production functional testing
exploratory testing that happened on
local workstations okay so every team
followed the same value stream every
team also had the same fully automated
release process and every team was
cross-functional every team had
developers testers user researchers
business analysts product owners the
other one I always forget so the normal
constraints weren't there right so as I
said at the start your company has a
complex adaptive system you can't know
in advance what's going to happen when
you make a particular change but we can
have some heuristics okay and in my
experience the normal constraints around
continuous delivery are you know really
long challenging value stream broken
release process teams aren't
cross-functional so there's lots of
handovers
none of those were a problem here okay
so my team came along and we were like
you know we'll help everyone do
continuous delivery and we didn't even
know where to begin because the obvious
constraints weren't there on top of that
when you have 60 teams you don't know
who to help first you know this it's a
very real challenge you know I'm sat in
London you know working on my team
should I help the team you know sat next
to me waving hello Steve we just had
lunch together remember me I've got this
problem or should we help the team in
Lincoln you know stuck on a pig farm
you know like screaming for help like
why won't you help us you know we're
dying here or you know do you help the
team stuck in Fishguard you know they've
been there for a year you're just
writing code have they deployed yet no
if they just keep writing code you know
so now I made up those locations it
wasn't Fishguard it was actually
Aberdeen but it's a very real problem
okay how do you know which teams to help
when they don't know that they
themselves need help okay so we I
decided that we should measure
continuous delivery across the whole
department to understand which teams
were doing well in which teams needed
their help the most so the way we did
this was we created a vision of success
in government that's very easy you wait
to a Friday lunchtime when everyone goes
out for lunch you write a compliments
page that says we will do these things
and then on Monday you pretend it's
always been there and then you wait for
someone to come and tell you often when
they don't come you just keep doing
things like that you know I don't
recommend you do that I recommend you
actually create a vision success with
business stakeholders like it growing up
and understand you know like where your
problems are not just go quickly at
lunch and let's let's say we're going to
do all these deployments so what we kind
of set out was a deployment failure rate
of 5% with a deployment failure recovery
time of under a day and a deployment
lead time of 7 days and a deployment
until the 14 days so in other words we
want you to deploy to production at
least once a fortnight and under a week
we want you to take less than a day to
fix stuff when it goes wrong and it
should go wrong no more than 1 in 20
times now you might find those numbers
aggressive you might find those numbers
conserve
if you're a British taxpayer might be
very scared all of this but as I said
right at the very start continues to
live it is about satisfying business
demand okay so that means that it has a
dynamic success threshold okay there is
no magic number so if anyone starts
saying to you you must have ten deploys
a day then they're misguided okay it's
really about what your organization
needs in order to satisfy demand okay
and this was pretty conservative where
we were out as a department okay
so what we're trying to do here is we're
trying to divide our teams into two
cohorts those teams that are practicing
continuous delivery that we can learn
from and amplify their successes and the
teams that are in a state of
discontinuous delivery where we can help
them out and dampen their failures so Oh
too fast okay so I still feel about the
implementation because you know people
like that stuff okay so we had one
really big advantage okay there was a
deployment database already if I had
just a load of JSON full of numbers that
no one cared about except for me for
like six months I was pleading with
people I don't tidy the data don't
delete the data you know I had it backed
up all over the place I knew one day
it'd be really useful and then I think
eventually I wrote a python script or
something to suck all the data out are
turned into CSV shoved into a Google
spreadsheet generated some graphs and
then wait to see some important people
and said which teams do we think are in
trouble today and we all wrote down two
teams I think and I said okay this is
what the data shows and we found out
that one of the teams we'd all suspected
was in trouble the day two agreed
one team that no one had no one had
thought of looked like they were really
getting into trouble and then someone
famously said like oh yeah they did
phone up yesterday and asked for like of
disk free times bigger than have a team
and at that point we were like quickly
phone them put someone on the train
let's go and help them okay so based off
that initial success with you know me
and some Python and my pythons terrible
and some spreadsheets you know we
couldn't got a couple of developers in
from my team to scholar it up create
some micro-services actually create a
proper grown-up service around it
treated like any other production
service run it on the platform and we
created
like a little intranet website where you
could go and look at your team and you
could see your services there were some
very careful rules behind it that I kind
of set out from the outset so you
couldn't compare one team against
another team because even though we all
know that's a bad thing to do you kind
of can't resist it I obviously did it
myself privately one's gonna hahahaha
you know and then of course I had a
graph showing 60 teams with a lion graph
of 60 lines it's just meaningless anyway
so I learnt nothing also you couldn't
look at any information about
individuals you know eventually somebody
came along I was like this stuff will be
lovely for my performance program Steve
and I kind of lied and said there's a
lot of lying today and they still didn't
their goodness me
and I said you know all I'm really sorry
Sam it's just too hard to collect
information on the people actually
pushed the button in production Jenkins
doesn't give us that date or time
they're all you know that's a shame yes
it is a shame yes so of course I went
home think I did a really good thing
today for 700 people and they'll never
know okay so that was kind the website
that we created and it was always framed
as look at this thing to help you
understand for yourself how you're
getting better
it was never framed as we're checking up
on you because you know that's what
people are kind of scared of in
government okay so how did we do so with
our deployment stability indicator we
found that we had a small number of
teams where their deployment failure
rate was 0% and their deployment
recovery time with zero days so they're
doing really well but the majority of
teams shared an average deployment
failure rate of 10% managed took the
five days to recover from a production
failure so you know on average that's
not great something goes wrong takes to
a business week to actually
comprehensively close it down that's not
amazing
we actually found one team that had a
deployment failure rate of 67% so two in
three deployments are going wrong you
know I think I called them up and they
were we were wondering when you'd phone
us Steve we're having all these terrible
problems you know why didn't they throw
at me first
so okay hit so here's an example
indicator okay so how does it work so
the x-axis is time we've got a six-month
duration here the left Y axis is
deployment failure rate and the right Y
axis is deployment failure recovery
timing
so it's 15 so this was the apples team
they're not really called the apples
team I've done this talk enough times
now I've actually forgotten which real
team it was which is really handy so the
blue line is deployment to failure eight
and the red line is deployment failure
recovery time do we have any colorblind
people in the audience because I if so I
can go over this with you afterwards if
you just let me know okay so this team
was doing very well right so we want the
lines to trend down towards continuous
delivery their deployment failure
recovery time improved from nine days to
zero their failure rate improved from
20% to 0% now this is the point where
all of us including me start having lots
of crazy theories about how they
improved okay and this is where things
get a little bit dangerous okay there's
this idea in one of the senior Calista
labs books about us I called the ludecke
fallacy like the danger of trying to
extract a qualitative story from a
limited number of quantitative data
points it's very very easy to look at
this and go you know or they clearly got
better at testing or something right but
what I really want to stress here is
that when you measure continues to be
when you use indicators
it's about pinpointing where you need to
have conversations okay indicators and
measurements don't tell you the stories
you need to know they tell you where to
go and find the stories okay so this
team clearly we want to talk to because
they've made a significant improvement
all right and when we are went to speak
with them they were just kind of
surprised they were very they were the
best performing team at that point in
time and there were two things that were
different there automated acceptance
tests were better than other teams they
were much much faster much more
consistently made a really big
investment in it but also their
monitoring dashboards were better than
anyone else they had a someone on the
team had really got on the bug for
gravano if I remember rightly they tank
crafter love really lovely dashboards
compared to the default dashboards which
weren't anywhere near as good so we
exported the JSON from the apples team's
dashboards we wrote a scarlet DSL around
it we generate the dashboards for the
other 500 micro services and just
injected it into production and then
with a nothing a couple of days every
team had the opportunity to improve the
their situational awareness of
production okay we would never have had
new tool without the insight that we got
from this conversation we had derived
from our measurements okay all right so
we've deployment throughput we had a few
teams where they were deploying to
production once every five days and it
took them a day so if you recall that's
faster than our vision of success so you
know they're doing really well and the
apples team was one of those teams so
you know that's another nail in the
coffin of you know you have to choose
between stability and throughput but a
lot of teams were averaging together one
deployments production every 21 days and
it took 12 days to do it I think there
was one team where they had a deployment
lead time of three months and their
deployment interval was blank what does
that mean someone shout louder factor
almost they had one they had one
production swarm it took them three
months and then a lot over they know
that was a bit hard let's not do that
let's not do that again
the tall man won't find us you know so I
found the minute no but they'd they'd
had some actually some free hard
promises off to be fair to them but yeah
so we had you know a lot of teams that
we could help here
so this is an example throughput
indicator for the bananas team they're
not really cool bananas please don't
look for a team core bananas in
government there probably is one that it
won't be these people so on the right
x-axis were still time the left works
this is days we've got deployment lead
time in orange in the dark orange that's
the median and deployment interval is in
green and we can see this team was doing
pretty awesome stuff they're doing one
deployment every three days in March and
then by June
they'd slowed down to one every 14 days
okay and again this is the case and I
really fell into it this time this time
I was like doing more enter and testing
I need to go and find them and help them
and then of course I went to I got on
the train I went to meet with them and I
kind of sat down with him on the train
I've been rehearsing my stuff about
end-to-end testing and I kind of sat
down it wasn't interesting at all it was
everyone on the team was different so
you know I sat down and said you know
who are you
and they said never mind us who are you
and I said the worst possible thing
insane Brit and I said I'm from London
I'm here to help she never goes down
well if you're outside of London I won't
be doing that again so when they forgave
me I found out that basically the entire
team had decamped to some trendy music
started but obviously fold in her three
months when they ran out of money but
you know I wished him well I'm sure they
were sure they're happy so what we found
was this brand new team they had to get
used to each other they had to get used
to the platform which was a pretty
complicated thing to use they had to get
used to their own digital service but
expecting a team to keep up a cadence of
one deploy every three days it's pretty
demanding in that situation right they
weren't doing anything wrong they slowed
down because they were just kind of
getting the hang of things and I would
never have known that if I hadn't
actually taken the time to go and visit
with them and we spent some time doing
some training with them I think we sent
some people in from other teams with
have you ever chat with them about some
stuff and some time after this I think
they got back to one deploy a week I
believe okay so there's more so it
occurs to me that the measures that the
Kohl and chairs have talked about the
last couple of years they aren't
actually specific to deployments we can
reuse them we can reapply them to
different aspects of the technology
value stream in order to better
understand different parts of our
delivery capabilities so an obvious one
to look at as builds okay so continuous
delivery mandates that you have an
always releasable codebase okay that is
something that is very hard to get right
okay and what should I say here so this
isn't just about you know having a
super-awesome Jenkins or team sit or
whatever is you want it's about branch
by abstraction is about feature toggles
it's about being able to build very
frequently and the interplay between
build indicators and delivery and
deployment indicators is really
interesting if a team is building very
frequently and they're not point
frequently then they're probably going
to be in a good position to give it a
try if on the other hand the team is not
building frequently and they are
deploying frequently then something is
wrong somewhere and it's time to you
know like get ready with the
productive time to get ready for a
production failure probably okay
although that situations pretty rare so
we can have a build stability indicator
of build failure rate and build failure
recovery time and a build throughput
indicator of build lead time and build
interval so build failure rate is the
percentage of builds that fail on your
build server and build failure recovery
time is the medium and standard
deviation in time between build failures
and build throughput is build lead time
which is the median and standard
deviation in time between a commit to
master and its subsequent successful
build on your build server and build
interval is a time between builds on
that builds successfully happening okay
and again that little arrow is where you
lose an awful lot of time and winning
Tibet so we again had a vision of
success around this we asked teams to
build at least once a day to do it in
under an hour and to have fewer than 1%
of builds fail and when they did fail to
recover some failure in under an hour
okay
so we had a small number of teams that
again had they build failure rate of 0%
and zero hours between a funny about
failure and resolving it which was
awesome but the vast majority of teams
shared an average build failure rate of
19% and I build fade recovery time of 4
hours so one in five builds are failing
and it's taking half a day to fix it
one particular team had a build failure
recovery time of 12 days I like to think
that whenever their builds went red they
just kind of stood and went what should
we do let's go on the holiday when we
come back it'd be all right
let's do that we'll kick off Jenkins
from the beach they'll all be fine why
did they do that anyway so I never got
to the bottom of that so this is our
build stability indicator
okay so build failure rate on the Left
y-axis build failure recovery time on
the right to y-axis our charming purple
line at the bottom is be able to failure
it which was very low for this team
but our bill would fail recovery time is
ten hours and it's staying pretty
constant so there's a general heuristic
if you've got a time-series metrics a
lot people will notice from production
monitoring as well if you have a time
series metro a long period of time and
it is consistent activity and the line
is not fluctuating at all then something
is going on right even a system or a
person or something is meddling with the
data okay because it's a stochastic
world out there stuff is highly variable
always changing and why would this be a
case why would every build failure
require 10 hours to be fixed right that
just feels wrong so I had a chat with
the team lead and it turned out that
their acceptance test took 10 hours to
run and whenever they became pressured
to get build-out on time for someone
they would do the build first and then
run the acceptance test afterwards while
the testing was going on and that caused
a couple of problems so first of all if
you push those slow tests down the value
stream then you're not going to feel the
pain of the slow build so you're not
actually gonna feel the need to actually
improve on it okay that's why it stayed
at 10 hours they weren't that fussed
about it wasn't when it was slow it
wasn't actually impeding their day and
also what it means is that if those
accept assess fail later on and the
testers already started their work then
that testers work has been wasted right
because they can have to wait for a
fresh build so we had a chat with the
team we actually parachuted someone in
for the apples seemed to help them not
literally you're not actually allowed to
do that in government I asked but I had
to settle through you know just getting
someone to walk across the department
actually sit with them for a couple of
days and you know who knew if you spend
a couple of days improving you I accept
most tests and turn them into API
acceptance tests and writing some good
unit tests it's amazing how quickly a
ten hour bill can become a one hour
build you know who would have thought UI
acceptance test would be such a bad idea
not as none of us do that do we okay so
with our build throughput indicator we
found that all of the teams all 60 teams
averaged zero hours between a commit or
master and a successful build which is
which is pretty good
I mean yes we all want a ten-minute
build well if five minute build now give
us 2017 but even getting to a point
where every team is you know producing
builds in under an hour that's pretty
awesome but we had a handful of teams
producing build every two hours
the majority of teams were producing one
built every two days okay so that's a
lot of teams that are creating any help
with their builds so this was the
oranges team this is their build
throughput indicator they left Y axis is
ours and that's covering them build lead
time in yellow and build interval in red
so our build lead time stays about zero
stays about zero hours as we'd expect
because we saw that all teams were
sharing zero hours for lead time but
their build interval here is really
interesting right so what's happening is
in January they produce one build every
four days in February they produce one
build a day and then they regress back
to four days okay that's a huge
improvement in one month all right so in
my book I call this trend too fast too
frequent like that can't end well
right continuous delivery is a hard
thing to do there's a lot of stuff
involved and that's going to end in
disaster
if in a month so this is actually fairly
easy to guess what happened I didn't
want to guess what haven't a team goes
that much faster in a month it's
happened to all of us they were
crunching ta deadlines right so someone
basically said we need more builds for
more testing in the Sprint in February
because we've got an important business
event in March right that's not a
government thing that just happens
across the industry right so the team
crunched they cut some corners they made
improvements to their tests but after
February they were you know they went
back to the way things are used to work
for them and I think by the time my team
kind of learn
all of this I think it was kind of April
time and you know we had some long
chants for them you know about how did
it feel when you could build this
frequently you know we know you can do
it because you've done it previously we
could show them their own activity and
we helped out with some of their
acceptance testing we also spoke with a
product owner about flowing work through
the team and trying to avoid these big
crunching events all right so our final
indicator is a code indicator so
continuous delivery is dependent upon
continuous integration its 2017 but
still I have to say continuous
integration is not a tool it's not
having CI and the URL of your build
server please don't do that really
really upsets me continuous integration
is a team practice where everyone
commits to master at least once a day
okay
that is something that is hard to do
okay it's simple but it's not easy
the Cole and Jesuits research has shown
that there's a statistically significant
correlation between continuous
integration and continuous delivery -
which most people in the continuous
livery community just went yes good
that's what we fought okay if teams are
pushing code frequently to master and
building frequently then they're able to
deliver frequently okay so we can't
really look at code stability because
when you fail to merge a change to
mainline trunk master what everyone call
it main lines not really superset those
failures attract in your local working
copy don't normally tracked remotely but
we can look at code throughput okay we
can look at your mainline commit lead
time the time it takes you to go from a
branch to main line and we can look at
mainline commit interval which is the
frequency of commits to mainline so our
code throughput indicator mainland
commit lead time is the median and
standard deviation in time between a
push to a remote branch and the
subsequent merge of that commit onto
mainline of course you could just commit
that straight to master in which case
the number you get would be zero and
then you and I can be good friends you
know you should definitely do that
mainline commit interval
is the frequency of commits to master
okay so continuous delivery has a
dynamic success threshold continuous
integrations is static
okay we've said that we want the team to
be committing to master at least once a
day so we asked for each team to commit
to main line at least once a day and to
commit to master from a branch in under
an hour okay and the numbers we've got
were really surprising so first of all
every team to my surprise was merging
from a branch to master in under an hour
on average obviously there was some
really horrific standard there was some
horrific variance around that but on
average every team is doing what we'd
like to see part of that might have been
the branching strategy so ironically I
was responsible for the branching
strategy for all of these teams and I
really pushed the github flow and not
trunk based development which will sound
a little bit crazy but previously the
department using git flow which is an
abomination and completely incompatible
continuous integration continuous
delivery bunnies cats anything that you
like just can't do it with git flow so I
really strongly encourage teams to move
to github flow to really sort of feature
branches because I felt that that was a
smaller jump people to make them to go
straight to trunk West development you
know for 600 or developers that would be
pretty hard change so everyone's going
to mainline pretty fast that's not bad
at all right but just as I said right
back at the start it's the two measures
together in a measurement pair that you
need okay that one number one it's only
pretty misleading unless I told you that
we had I think three teams three or four
teams that were averaging two hours
between mainline commits and that the
apples and bananas teams were in that
okay but the majority of teams averaged
five days between mainline commits which
was way higher than we expected all
right so a lot of teams that need help
with the way that they are shaping their
code the way that they're pushing their
code the way that they're designing
their code all right so this is our
final example this is the pair's team
pears pears pears never going to get
another pair so handed it I have
genuinely forgot on which team this was
which is awesome because this story
makes me sad whenever I remember it why
are you making me tenets so uh you're
not really so the main line lead time
here looks like it's not a lot
okay that's zero hours that then becomes
thirty four hours so they went from an
average of zero hours between branch and
mainline to over a day between branch
and mainline the reason you can't really
see that on that graph is not because
I'm incompetent it's because the y-axis
is so enormous and that's because over
six months this team went from pushing
to master wants every four days to once
every twenty seven days right so once a
month we push something to mainline now
it would be very easy here to be
extremely snarky and basically say you
know you're doing all wrong
you know how dare you and go set your
dusty stick and you know really thrashed
them into pushing to mainline more often
but this team were a little bit less
experienced aside from the tech lead I
think and they you know they needed help
they needed some understanding and we're
in this particular in the case of this
particular story I think my team picked
it up as late as May I think by the time
we heard of this they were already you
know they were already a point where
they were pushing once every three weeks
or so and there had been some signals
they had asked for the build server to
build branches which is always a bit of
a signal that people are straying away
from the continuous integration okay
just as a tool can do something doesn't
mean you should do it so when my team or
I or one of my sock puppets you know
someone on my team I was like go and say
these things to them in this exact order
just so they didn't think it was me and
I was obviously me all along you know we
said you know building branches it's you
know it's incapacity of consumers
integration what can we hope we were the
design of your code I think it would be
fair to say they were a bit sad at the
time and you know we should have really
picked up on that so you know I think
now but what we found was ended up
guessing the Traynham to visit them
there in the north of England and they
have been given two pretty hard goals to
reconcile right they've been told you
have to reallocate your service to meet
this impossible
standard that you know I didn't really
understand and also they had to keep
shipping new features right and as in a
relatively inexperienced team when you
have to do these two impossible things
you reach for the branch ring reliever
right because that's what the tool gives
you by default it's right in your face
like would you like to branch you know
especially now with stuff like github
where the tools are so lovely and it's
just so easy to do it's very tempting
you know I understand that so we kind of
sat down while the team and I think it's
fair to say by the point where they were
pushing once a month they accepted that
there was a bit of a problem and we
encouraged them to loosen branch by
abstraction we stole someone from the I
think was the oranges team that we stole
where they went and sat with the team
and really worked hard to find them I it
seems in the code to put some toggles in
place and gradually it started the
numbers did start to come down
afterwards there was actually a really
awesome a thing that happened I had
nothing to do with where there was the
third-party service they depended upon
in production and it was just going down
for emergency maintenance I'm always
very suspicious when a production system
when you get that email say we're going
down for emergency maintenance we'll be
right back
you know we talk what is this emergency
anyway so when this happens the the
product owner repairs got very excited
you know what we're going to do what
we're going to do do we stop the service
and so one of the most Jeanne members of
the team kind of put the Hanuma said
should we just toggle off that call and
return some blank data which they did
the service stayed up taxpayers could
continue to use it the government
department continued to cut cross there
everyone won just because they had that
little bit extra adaptive capacity that
little bit of extra resilience as a
result of putting in some tog words into
their code to try to push the mainland
more often okay so that was just a
really great success story that I take
absolutely no credit for that was really
awesome
so I've shared with you five indicators
five stories from a massive three-year
transformational program within the
British government where continuous
delivery has really got a really strong
foothold since I've left now teams do
all of their own deployments at all
hours through production and I've been
told that there's way more successful
there were some really awesome stuff and
measuring continuous delivery has really
helped okay
Tim's being in a position to understand
if changes are working or not
teams being able to understand looking
at other teams improvement journeys what
changes work and what changes don't you
know that's a really really powerful
toolkit to have okay because there's so
much uncertainty in your organization
there's so many different options that
so many different levers you can pull
and when you're in a position you can
rapidly experiment and then measure the
success or the failure of those
experiments then that's something that's
really really useful okay so improving
the monitoring dashboards helping teams
up to speed with a lot of new changes
helping teams with beach toggles and
improving the resilience of their
service in production there's more
things that directly happened as a
result of us taking this approach and
there were many many other things that I
haven't been able to share with you
today that were indirectly or directly
linked to us having this focused
approach on understanding how teams were
progressing with continuous delivery so
it's really nice it was actually we were
60 teams right because now nobody can
say this possibly can't scale or what
are you talking about
you know so the scale thing's gone okay
so this can work with one team 60 teams
600 teams okay if the data is there if
the tooling is right then you can
measure continuous Rivet across the
department across a company across a
group of companies and understand and
help teams understand for themselves if
they change their maker being successful
or not okay so that's my book it's on
lean perb I'm happy to take any
questions thanks very much for coming
along and I'm around for the rest of the
day if you a bit more comfortable asking
questions you know one to one situation
but thanks again for your time thank you
how go get the time are you I had no
idea of time then how did I do we have
one minute one minute for I don't know
20 questions in my I need to talk
quickly sorry don't put this on me
without using branches how do you easily
deploy two distinct deployment
environments like development staging
and production that's not a one minute
answer without using branches okay so
feature toggles are really really
powerful if you want to test different
functions in your service without
interfering with other stuff I'd also
chat I'd also ask why you need a
specific environment to test a specific
feature but it's easy to stay up here
and throw out answers that are
contextual and don't apply to your
company whoever asked that I'll come and
find me afterwards and I'd happily told
you about that more detail
thus team size have an impact on
failures or recovery time yes yes it
does
so there's no such thing as like an
optimum size that's how seven plus two
thing the pizza team thing that's what
one company did and work for them
but I do think there is kind of it
depends it depends on the team on the
surface but there is an optimum size
where people are able to kind of respond
to a production problem and also still
continue to other work but I do think it
also comes back to this idea of having a
releasable codebase and having a
functioning service in production I
think that's a bit of a mindset shift
for engineer's managers the executive
this idea that the most important thing
is not shipping new features it's
keeping things in a working state it's
keeping builds flowing that kind of
stuff okay last question in a micro
service environment would you measure
per service or per team good so I didn't
cover lis both okay so the way the way
you can do it is you measure stuff for
services then you find out which teams
own those services and then you can
aggregate your team measurements based
on the services okay so if one team has
free services you do free free there are
some measurements that's wrong you look
at this free services individually and
then you use the same on
the lying data for the team level
measurement okay you just need to avoid
doing an average of average if you ever
find yourself doing an average of
average then something is wrong so I
actually really suck at maps I really
suck at quantification of a lot of this
stuff there's Gokul Troy McGinnis who
does this stuff way better than me but
some really simple measurements and just
being up to link services back to teams
like you can uncover some really
powerful information okay thank you very
much thanks very much everyone thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>