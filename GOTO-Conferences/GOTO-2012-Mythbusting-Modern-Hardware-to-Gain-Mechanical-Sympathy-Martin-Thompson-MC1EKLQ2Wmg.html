<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • Mythbusting Modern Hardware to Gain 'Mechanical Sympathy' • Martin Thompson | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • Mythbusting Modern Hardware to Gain 'Mechanical Sympathy' • Martin Thompson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • Mythbusting Modern Hardware to Gain 'Mechanical Sympathy' • Martin Thompson</b></h2><h5 class="post__date">2013-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MC1EKLQ2Wmg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the last talk of the day in
the Mythbusters track in and guess it's
the last talk for most people through
here and unless they go to the keynote
later so hopefully everyone's not bored
of talks so far I'll try not to be too
boring towards the end of the day so
what am I going to talk about well sort
of following on the Mythbusters
tradition we want to look at a number of
things and come up with one of three
conclusions either the myth is confirmed
we agree with that it is possible the
myth is plausible it could be possible
given certain constraints or the myth is
busted in that it is a total myth and we
shouldn't really be taking it too
seriously now we were doing this for a
number of things in the day but since
I'm a bit of a petrol head and this is
where mechanical sympathy came from
because Jackie Stewart the racing driver
actually came up with the term it's not
my term I blatantly stole it and reused
it in this context and Jackie Stewart's
term was that you get the best out of a
racing car when you have a good
understanding of how that car works you
don't need to know how to rebuild an
engine rebuild the gearbox but you need
to know how it works to get the best out
of it and so he talked about the driver
working in harmony with machine to get
the best out of it I believe that is
very similar in computer science and us
getting the best out of the hardware we
have that we're using for computing now
this is a Formula one steering wheel it
looks terrifying compared to a normal
steering wheel you find in your car but
actually it's really not that terrifying
if you just get over the initial ooh so
many knobs and dials because it
effectively steers and you've also got
gear changes here on the side but all
the rest of these knobs are just small
tweaks to what you can do like you can
adjust then the mind of rails you're
using so that's the rev limiter setting
and what that will do is that let you go
faster but you'll probably burn out your
engine and have it less reliable and
you're gonna use more fuel so you just
make it trade-offs and how you're doing
different things seamless brake balance
you can have the brakes go forward or
back and it's just in a racing car you
get more of these knobs to play with we
have them in computing if you work on
something like the Linux operating
system you get used to assist control
you'll be changing knobs and
stuff like this generally they set
fairly good but we may go off and change
some of them but on trying to say is
some of the stuff is not as scary as you
think and the myth I want to try us
today is can normal developers
understand the hardware we're working on
I've never given this talk before so
there's gonna be interesting
people may think at the end of it yes we
can understand this it's not that scary
or you may go holy crap we don't wanna
ever touch this again hopefully not the
latter
so are we these birds of little brain
that cannot understand what's going on
in here actually it's a it's an unfair
representation of homer because if
anybody is a big Simpsons fan you'll
actually realized he is really
intelligent and he has a big brain he's
got a pencil shoved up it and it's
causes his problems he had the pencil
take it out when I was so he was really
smart for a while but he didn't like it
or oh we like this little guy Cartman I
Cartman is kind of interesting because
he's really intelligent but he's lazy
and he's like the typical modern society
is Lizzy doesn't want to put that word
in I want everything yesterday
unfortunately you could almost draw that
conclusion about computer science than
our use of hard work at the moment we
don't put the discipline and the
professionalism in to understand enough
about our platforms as you would in
another discipline so let's look at that
as a question as we go through are we
just being lazy or is this just too hard
to understand so first myth let's start
off with who thinks
CPUs are not getting faster hands up if
you think they're not getting faster
that's only a few it's kind of
interesting cuz the vast majority of the
industry keeps saying that
so let's look into it in a little bit
more detail herb Sutter famously wrote
the the free lunches over there that a
lot of this gets talked about and he's
actually got some very very good point
in this and it mostly comes down into
thermals inside CPUs as we run them
faster and faster they need to get
hotter and hotter and heat dissipation
on the skill that we're dealing with is
Incred
be difficult to solve so we can't have
CPUs getting hotter that's fundamentally
there she not that they can't get faster
they can't get hotter and clock speed is
the main thing that makes it go hotter
because of the voltages we drive for
that but I'm going to show you the clock
speed isn't everything
so here's a simple test let's word split
the text of Alice in Wonderland how long
does that take on a few CPUs over the
last few years so let's go back to core
to zero and we're at 2.4 gigahertz and
we've got we can actually word split
that an entire book 1400 times a second
that's pretty decent really but look
what's happened as we've gone forward
and CPUs clock speed my pointers giving
up clock speed is going down even
because we're worried about those
thermals quite a lot but look at
operations per second it's going up
that's just over the last few years and
I've tested some later processors that
trend is continuing but I'm not going to
stop there because that's not the whole
story which is kind of interesting
so let's look inside a CPU and
understand why some of that is getting
faster well this is the Sandy Bridge
microarchitecture this is typically what
most laptops are you combined today
unless you're getting one of the very
latest in which would be getting is Ivy
Bridge Ivy Bridge is the same
microarchitecture is this but add ice
rink to make it a little bit smaller so
this is the current state of the art for
Intel x86 processors now inside a
processor at university we're all taught
we have one CPU on one our thematical
logic unit hang on
I'll have a three of them here this is
one processor one core you've actually
got parallelism happening inside CPUs so
instead of going faster we go parlor on
the go and parallel inside but we've got
actually six ports along here because
we've also got a lot to do with on the
right hand side here loading and storing
data and you'll notice that the little
red boxes that are outlined those are
actually new the
with Sandy bridge over the previous
architecture which was Nehalem and to
make these things go really really fast
we've got to feed them because we can
have multiple instructions per cycle
going on it is completely possible to
have three mouth operations going on at
the same time as to load addresses being
are at loading store address has been
generated and storing off a piece of
data so that is impossible to get right
up to six operations per cycle actually
going on so the clock speed does not
equate to operations actually being
executed because these things are
happening in parallel if you look at
house well which is the next generation
on from this we're going to eat ports
wide and we're going to have more data
paths for getting data in and out of
these and you'll see quite shortly why
because feeding these is actually the
problem now I'm making a statement I'll
go in the back it up in a second but
just a couple of things to note and here
especially around performance you'll see
up here you've only got one divide and
one jump onlys so code this heli branch
to code that does a lot of division and
remainder operations doesn't go as fast
as straightforward arithmetic using plus
minus and multiply so let's dig down
into what's actually going on so back
here all of these different things
inside the CPU have got counters see if
you use a grid telling you what's
actually going on there cut counters for
everything so we can profile this and
tell what's going on one of the tools
that lets us out those counters is per
stat on Linux so here on the run really
simple first up there's loads of bells
and whistles you can pass in to this so
if you type perf list it give you pages
and pages make sure you pipe it two more
because you're going to need it of
everything you can profile and it's
everything from CPU counters to what the
operating system is doing to what the
i/o systems are doing I can give you
lots of what's going on now what's the
very basic I'd put on me running the
Alice in Wonderland text through this to
tell me what's going so this is Nehalem
architecture first of all couple things
to note 2.8 gigahertz is what it's
actually at but it's actually running a
three font area that's caused a turbo
boost
so it's running faster it's allowed to
clock off when it's not running all the
cores at the same time now you'll see
that look how many cycles of the front
end on the back end so what do mean the
front end and back end front end is
fetching instructions in decoding and
radley to be executed the back end is
executing them and retiring those
instructions after they're finished the
front end and back end this processor is
idle a significant chunk of the time so
if we made this faster would our
previous example have got any quicker
no whether we CPUs are getting faster
not as almost irrelevant they're ready
faster than we're feeding them and I'll
get on to the next myth why we're not
feeding them fast enough but let's just
give forward another architecture
generation of the Sandy Bridge so
running the exact same test through
sandy bridge look our cycles idle has
dropped we've dropped down from thirty
and thirty eight to twenty six and
twenty one and on turbo booster this is
a two point four Aiken turbo boost
higher because you get turbo two in
Sandy Bridge
so they're actually this is a
like-for-like comparison on exactly the
same piece of work
notice the time 5.9 seconds nine point
eight to complete the work for one see
that's actually 25% more going across
the generations why is because of wasted
cycles the actual instructions per cycle
is not as good on the previous
generation and you can keep going back
so all of those figures that we same the
reason why it's got faster by generation
is not to do with clock speed is to do
with how we're feeding those execution
units inside there and so that's a kind
of interesting observation so RC if you
use getting any faster I'd say it's
plausible but not what most people think
it's getting faster because we're
feeding them faster not the Custis CPUs
themselves are significantly faster for
a long time with in having CPUs that had
just been stalled idle
to be fed and we're not getting data to
them quick enough this one's interesting
memory provides random access hands up
who thinks memory gives us random access
hey I got what 10 20 percent the room
there I said yes most people don't think
so the name suggests this random access
we all believe we get random access
well what's kind of interesting is this
is the cost of creation at the end of
the day to get to me in memory is fairly
expensive so we want to feed those CPUs
really fast like we've been talking
about but how do we feed them really
fast well we need the deer to be close
to them by something that's very very
quick and this is the classic memory
hierarchy where at the very top we've
got the CPU registers and buffers inside
those cores those are incredibly quick
like the latest generations now even a
register to register move doesn't cost
anything because those instructions are
getting decoded it's actually just re
not be in the register file it doesn't
even move anymore so it's free when
you're inside the core effectively and
you get buffers in a night which we'll
touch on on a second then we've got
layers of caches and caches get bigger
and slower bigger and slower so every
time you need to deal with more data the
complications in addressing the larger
size means it gets slower so what this
effectively gives you if you design
really well is you've got the cost for
the largest and the type of memory being
permanent storage but with the speed of
access of the fastest if you're feeding
it all through correctly and also an
interesting thing that isn't talked
about so much but is commonly really
important is par the par to access a
disk is so much more per operation than
part to access an l1 cache and you start
seeing modern processors are starting to
shut down different things you'll see
that on the little sandy bridge-e
servers now if you transfer memory from
a networked hard to the sea
you it used to always transfer to mere
memory now we can transfer to the cache
okay that's much faster and much slower
lately but that's not why they're doing
it the reason they're not doing it the
other way by transfer the memories they
don't have to par the memory up to do
that and transfer it in so we're trying
to keep those thermal Stein everything's
a bit dropping the thermals now I'm
getting this power but it's time
so you see here we've got the classic
sort of speed versus cost but really
important is par so let's look at a
little bit what's going on so we look
inside a CPU core nice these blue boxes
at the top or the equivalent of what's
inside the CPU core what does it do
talking right from that point well first
of all it talks to the memory ordering
buffer because when you've got all of
those colors with those cores having all
those execution units running at the
same time you've got a lot of
parallelism and the one thing you don't
want to do is stall because you see
those instructions stalled it was a big
issue so it tries to predict branches it
tries to prefetch data try to do all
sort of things it's fact literally
execute it hits a branch and it goes
based on my data I think it's gonna go
this way so I'm gonna assume that branch
comes back is true and I'm going that
way in your code and continue executing
until it finds that little divert that
guess was wrong it has the back out and
threw it away to deal with all of that
and make that progress it uses the
memory ordering buffers for this because
it can make progress and as long as it
doesn't show that progress to the
outside world that's completely fine a
scratch pad and if it finds that
literally that it's gas was correct it
can just let that go and those buffers
provide that but those buffers are
really cheap to look up data in as well
so there's a fully associative queues we
can just look up the data what's going
on if you in concurrency those are the
big problem that you're worried about
seeing the latest versions of data and
families to attend in my course later in
the week we'll talk a lot about what's
going on inside there's that's where the
sliced titled memory ordering but we see
for going further right from there we're
dealing with l1 l2 l3 cache I've got
these little things on the side called
the line feed the line fill and the
right combining buffers those are really
interesting especially when you go to do
a write and you get a cache miss you can
just continue to write to those and you
can continue to make progress and you'll
start
after a while that their biggest cost is
an instruction miss followed by a
redness followed by right and that's
counterintuitive most people don't
realize that rights are actually less
hassle than rates because rights can be
buffered reads you got a guess an
instruction this you've got no hope
because you don't know where you're
going next so let's get a little bit
more scary what's the cash flow system
look like if we dig into it well it's
just the same as the last slide but it
was a bit more detail so our memory
ordering buffer is talking out mostly to
the l1 data cache so our l1 is actually
split data and instructions pretty small
only 32 care piece for those and it's
organized into what are known as cache
lines cache lines on an accident are 64
bytes so when you write a single byte or
single bit or a single word you need the
cache line to be available there your
currency of data exchange within the
processor itself so you're transferring
64 bytes at a time in a night and those
are effectively hash tables they're just
Hardware hash tables that if you get a
Miss it has to be pulled in the close
would call something to be pushed out as
well and that's your nearest thing to
the core nice and fast nice and near
like some of the things were fading into
notice the double arrow feeding data in
that was a Sunday bridge edition with
Nehalem there only was one oral feeding
in and that's actually one of the
fundamental reasons why that last
algorithm was faster on Sandy Bridge and
was before because I had another data
path to feed and make that go faster so
what else is interesting is these guys
over here this is where the random
access question comes in the memory we
deal with is virtual memory it's not the
physical memory we run on operating
systems that virtualized what we're
doing so the physical address needs to
be worked out based upon the virtual
address that we're doing that's usually
looked up on a page table that would be
an incredibly expensive operation to
look up through a tree of structures for
every given address so we use these
think of TLB the translation lookaside
buffer and they're a little hardware
cache
of content-addressable memory that looks
up one to the other very very fast
if you get a Miss on one of those you've
got to go to the page table and so
that's what makes your memory access
really much much slower so imagine is
something truly random access whenever
you're gonna miss on the TLB versus not
getting an S on the TLB it's got to make
any huge difference we've also got the
pre factors and what the prefetch is do
is they look for what you're doing and
try to have the data available for you
and if you want to do it so the obvious
one is the streaming pre factor so the
two pre fractures in each of these
layers the streaming pre factor that's
he runs also here it just looks for a
part of your streaming through memory so
imagine you're walking through an array
reading bite after bite after bite it
sees you're doing not and is prefetching
that data hiding the latency before you
get there it's really simple really
really fast
there's also the instruction pointer
Prefecture which is more clever and it
actually looks at how you're walking
through code on your branches and if
you're walking through memory in a
predictable fashion it can prefetch as
well so it's now purely on stream if you
forward backwards it can have also two
gaps in it as long as it's predictable
but the predictable missteps are called
strides and those strides must be less
than the page size of the operating
system because the TLB entries map to
the page size to the operating system so
the where TLB entry works is it's the
page and then the offset inside the page
so it won't spawn across pages and pages
are typically for care they have been
but we realize in the big memory systems
they need to get bigger so if you're on
one of the latest versions of Linux you
actually find you're using transparent
huge pages so it's actually gone to two
megabyte pages and it's not using it if
you're in the low latency Spears this is
a nice kind of interesting gotcha
because as it has to do garbage
collection and compaction of those even
sometimes get huge pauses that you're
not aware of there's it makes the space
up to give you two megabyte pages at a
time but so it's doing that and you can
similar things further right so these
things are prefetching for you and get
in
right now as I get out further to the l3
l3 Nye has got a ring boss on it and I
see everything outside of the single
core which is their l1 and l2 cases I've
got a ring bus this ring boss is
incredibly fast and it talks to chunks
of memory in the l3 these are normally
organized into two megabyte chunks this
has got advantages and that you can
actually power off different chunks at
times as well so again going to that
power budget and there's also a link
from here to which is known as the
system the agent which contains the
memory controller says we actually
talked out to me in memory but also the
qpi controller will we talk to other
sockets continuing other CPUs and so you
can imagine the difference between I'm
accessing a piece of memory that's in a
cell here versus I'm going across a
memory channel or qpi Boston somewhere
over there to another machine or another
socket that's dealing with it at the
same time so this is not going to give
you uniform access to your memory like
we've gone out what's actually in memory
itself again it gets even more
interesting is so the memory controller
first up has a write buffer in it so it
can actually take a write request
acknowledge it back before it even
writes it and then can queue those
things out but then it writes across
multiple channels down to multiple banks
of dims that are in your thing and so
within add-in they've all got banks is
usually between zero and it within the
bank and you can per those up together
and so you'll hear rank and rank is just
multiple dims working together with the
banks aligned so they all become the
same unit now within this we have a res
of memory and they're typically for care
by one care by 16 bits so you 2 bytes in
each cell now to read or write one of
those cells you can't deal with it
directly what you've actually got to do
is take that very very tiny charge
that's in that cell and connected to a
sense amplifier the sense amplifier was
is effectively what's known to the row
buffer and it amplifies the charge that
it sees
positive or negative up to a charge
that's very easy to read by the rest of
the circuitry so what you end up doing
is you read row at a time into the sense
amplifier in a rare which is the real
buffer then you access the column within
that so you start to see a pattern here
that if you're just walking through
memory touching one byte in each row
that's gonna be nowhere near as fast as
walking through memory the other aware
so think of it sort of horizontally and
vertically where you reach a row and you
go sequentially through a row big
difference because it's a read in the
sense amplifier I thought stasis in your
row buffer to read or written and all of
these channels can work in parallel so
the reason we've got the channels in
parallel and multiple banks is the cost
of this gets hidden because it does
things in parallel by pipelining so as
one read requests gonna it sends the
next one straight away it doesn't wait
for it goes to a different bank if it
can't and so whilst one returning the
ones she just keep the boss flooded and
keep it full at all times now what's
this all mean when we actually take it
together and really measure it the first
quota finds really really interesting
Richard sites who was the architect of
the DAC alpha processor the Raptor which
others time was acknowledged to be the
fastest processor around it was actually
probably the first dual purpose
processor because it runs hot you could
compete and cook on it at the same time
but he made the quote that we should
stop making faster processors it's not
the problem the problem is the memory
subsystems that's where the real action
is we gotta make those faster I know
people are starting to wise up to this
and writing things faster but it also
makes a difference for us in that we've
got to write software that accesses
memory in a friendly manner and what
does that mean as far as figures go
well here's using this is Sandra tool if
I just completely sequentially walk
through memory it takes me three clocks
to get the l1 data
11 clock cycles to l2 14 to l3
6:9 and seconds now why are they going
to now seconds from clocks because mere
memory is not to remember the clock
speed of your processor there's certain
physical characteristics to how the pre
charge cycles work that a text set
amount to time for it to happen so you
measure that in nanoseconds you need to
work out this is a three gigahertz
processor three clock cycles is one
nanosecond to get to your l1 data so
sequentially I can get through memory
really really fast that is the raw
figures that doesn't even account for
pipelining so as your deal is being
pipeline through a system like that
that's even hidden unions see those
three clock cycles it becomes
effectively free if you walk through
memory sequentially Nick step up I
mentioned pages so the pages deal with
the translation lookaside buffer but it
also deals with the operating system and
how it cuts everything up into virtual
memory pages if I walk around completely
randomly inside the page as opposed to
just sequentially walking through it l1
and l2 they're just a fixed hash table
and how they work so there's still the
same number of clock cycles l3 goes up
and the reason that goes up is because
we are connected to a whole bunch of
different points on the boss and I see
each gap on the boss's an extra cycle so
you can randomly go in around them plus
the TLB misses start matching because
there's no TLB cache on the l3 so you
start getting those and the prefetches
stop working for you so well so we get
more latency now l3 and start getting
significantly more latency on memory
when we miss at that stage in the same
page so things like the sense amplifiers
and all are lit up and things are all
great for us at a certain level
let's go completely random walking
around our Haven well again l1 l2 sim
stories user
well gue starting to get serious now in
l3 and look what's helped me and this is
the fastest CPU available around of a
moment for a memory subsystem that's me
and stream you just go back a couple of
generations even just go back to
Westmere and you just add another 20 to
30 nanoseconds on that go back to the
court who architectures and it's a
hundred and fifty nine a second
for a lot of the memory to get accessed
I thought right so it gets a lot lot
slower so how we access memory really
really matters you know what what's that
mean when I sort of says like the
daughter is a branch miss our
instruction this is your worst case next
thing is a redness is the next thing and
then a right
well writing Haley Browns code guess
what's gonna happen it's gonna miss more
often because it's just too much data to
keep track off well we start looking at
reading and writing memory let's work
through things sequentially if we can
but that's not always the case we can't
do that but let's work on chunks of data
to gather that a cohesion makes sense
this is actually not weird this is just
good design keep things together that
are cohesive don't couple things that
are not cohesive I find people say you
don't need to know all of the stuff
that's go away in fact actually good
coupling and good cohesion design next
most of this just work and I find a lot
of people write code that actually come
in your profile it runs incredibly fast
and really nice just because they employ
good principles in design and if your
code is completely random and runs all
over the hey've well it's gonna be slow
so the data tells me memory being random
access lying is kind of busted and
besides the fact I hate the name random
because it's not cracked it's arbitrary
our industry's terrible at naming things
okay core drives do they provide random
access everyone says they do teep
sequential hard drives are random Sewer
hands who thinks hard drives are random
access one two three four okay this is a
nice low number this because people
really know are they're just worried
okay car drives spitting rust we kind of
know them quite well so the little disk
spins around an arm that reads it reads
actors and stuff on this that's going on
first up most people are not a worth a
thing called zone bit recording who's
hard to zone bit recording before seen
it here
nah - there's a big difference between
writing to the ardour tracks and the
inner tracks of a disc let's do the
density and so what the discs do to get
more data on them is you actually put
more sectors in the outer ones than you
do in the inner ones it doesn't need
angular velocity is exactly the same but
you get greater density so what does
this mean will you think about it for
one revolution you're gonna see so many
more factors pass underneath so you're
going to get greater throughput this is
a graph of a debt Western Digital Raptor
tenncare disk reading the outer sectors
- the inner sectors this is one of the
fastest hand care discs on the market I
can get 220 megabytes a second in the
audio tracks and I get to the inner
tracks I'm done - nearly 140 that's a
big drop-off so that's just throughput
changing and this is one of the better
discs that are right there for doing
that we'd say well spin this faster is a
better way of doing it that's kind of
true but there's limitations or not on
the fastest discs are on a still 15 care
and haven't got any faster for a very
very long time so what else is in the
hardware this kind of interesting well
we queue of commands to this because as
their arm sweeps across the disk we
don't want it to go completely nuts and
it implements elevator algorithms
Dalbert let's do this but also so does
the hardware and so if it's passing over
something will reorder the queue so
it'll get the thing on the web past it
also does lots of other quite neat
things as well and it also does
prefetching again it's starting to see
the same patterns pick up again over and
over again so it realizes you've read
something I'll prefect something but
also what's the unit of currency now
we'll the minimum you can read a right
is a sector sectors were typically 512
bytes for a long time they're now moving
almost exclusively to for care why have
they done that
it's very easy to miss when the star to
stop us on this it has to leave a decent
gap between each of the blocks on desk
they'll realize when it picks the start
and stop all those gaps start to add up
especially as disks bigger and bigger so
you actually lose the ability to store
more data by actually making the blocks
larger and having less gaps we get more
data on a desk so we're not moving to 4k
so that means 4k is the minimum we can
read or write so we read a write a byte
you read a ride for care now if I write
a bite to a file that's already or
something in it I could have read the
whole thing first
I changed the byte and write the whole
thing back again I can't just change the
warm bed and one byte so I'm dealing at
that level of currency at the stage so
what goes to make up an i/o operation
well there's four things to it really is
the command overhead itself that's all
the electronic processing that happens
inside the disk and it's fairly small
and electronics are getting very very
fast the cost here is dominated by the
physical media itself so the two major
components of that are the seek time
that has to seek backwards and forwards
and look at the rotational latency
well the seek time of the arm from sort
of outside to inside in the best discs
you're looking at around six
milliseconds so did you a full sweep
right across so on average you're
probably going to look at three
milliseconds is the cost of the sweep
because you gonna sweep halfway on
average laptop drives much slower they
don't work quite as well see you looking
at sort of anywhere up to 15
milliseconds this is Kenneth
state-of-the-art doesn't what's lower
discs aren't their rotational latency
something spinning to 10,000 times a
minute it actually takes six
milliseconds to do a fault rotation so
it's going to take you three
milliseconds to get around so what's
this all add up to well let's take the
final bit which is data transfer well if
I'm ever idea to enter art you find that
in physical disks the media transfer
dominates the cost and getting a type
extender faces got so fast so if you're
in SAS or SATA 3 you're looking at
of 600 mega Sakon through interfaces no
problem no but the physical media can
only deal with one the 200 megabytes a
second typically on or off that media
so that's we've dealt with sequential
let's look so sequential I can get only
outer tracks off at an care desk 220
mega second let's go to random they
access a for care block so I'm not gonna
try format so I'm gonna just do a full
for care blocks reads or writes my
average latency to get one you start
adding up its pan milliseconds that's
only a hundred IO operations per second
you can get through to physical spinning
desk
what's your throughput when you're at
that sort of random read that's less
than a mag a bit a second that's not
very impressive you'll actually find
that the circuitry so if you write a
test to do this you'd get a bit more
than one mega second you might get two
because the circuitry is quite smart so
all that's what circuitry all this
giving you is a doubling up of the
theoretical maximum by trying to be
clever so randomly accessing a desk
I would starting to died well what can
we do to improve it there's some tricks
look at one arm how about having two
Donal half the seat time let's make it a
lot more expensive some discs keep
multiple copies used to be a scuzzy
trick for a while so I've got less
rotational latency but again
everything's got a consequence remember
back to the racing car you turn a dial
you get one of fact it's always got a
consequence the consequence is it's
gonna take a longer to write done you're
gonna have less capacity I mentioned we
have got command queues and those are
getting better and smarter but they're
very limited in what they can do the one
thing that we can actually do quite well
is put a cache in front to it which is
kind of reasonable for the reading it
actually really wins out for writing
because it's back to the reading you
don't know what you're gonna get
and that comes hard to predict it but
for writing it's perfect because you can
just acknowledge the studio where the
danger is this is persistent storage so
it must be cieth we used to have battery
backed that's not even an issue anymore
because batteries fail over time now
we've got great big capacitor
and the capacitors Novak them and
they're really reliable and see if not
even commodity SSDs and stuff you can
buy a laptop over there with an intel
SSD just completely mainstream and it's
got a capacitor that Sears all my data
so if you just pull up all right
it still see if for writes and so it's
acknowledged and sent it see if the disk
now through all of this I haven't said
anything about the operating system the
operating system there's lots to make
this all batter but let's assume you're
doing synchronous writes to disk so
you've turned on F sync or F data sync
when if you're writing through the
operating system can't help you you're
going through and you're at the mercy so
if you're writing to idea base where you
must have your data the operating system
is not helping you you're dealing with
this and you'll see some databases
playing funtastic transaction numbers
they're not committed a disk they're
only inside the operating systems page
tables that is not reliable so I think
the deal is telling me again
this is not random access sounds like a
disk is really a big tip that's fast
because actually you think of it you can
seek backwards and forwards to the tip
this is doing exactly the same thing
just a bit faster it's not true random
access ah something's come in to the
rescue
we have SS days solid-state drives not
solid state disks do they provide random
access your hands please oh we got more
hands for this one still not a lot but
so has an SSD work well it's arranged in
the logical blocks some are different
for the nose but this is sort of what
the typical ones work like you typically
have a 2 megabyte block that's been a
very rare of cells again looks a bit
like memory now each of these little
cells you'll see MLC and SLC which is
whether this multi level or single level
single level means you can store a
single bit and the way it works is it's
got a voltage or it doesn't have a
voltage that's simple but the fact that
you can have range to voltage
so let's say I can have four different
voltages so 0 1 2 &amp;amp; 3 I can store two
bits this is what an mlc cell becomes
there's new ones coming out now that
have got eight levels so you can store
three bits so we can I say get a bit
more density in there but you'll find
there's some interesting side effects to
that so most of the consumer stuff that
we use is mlc because it's all about
just making it sheep and getting the
density they're not the way these are
organized is we could address every
single cell but that would be really
really expensive so the circuitry is
designed so you access a row at a time
until you can only read or write a row
and any given time so and that's
actually called a page and these and
it's typically for care for what we're
dealing with at this stage there's so
mariette care as well so just depends on
the model but let's say for care is your
typical and you'll find it's quite a
common when we're working with so if I
read or write
a single for care thing that's great and
it actually works really really fast and
it doesn't matter where it is because
there's no physical moving parts so I go
through and I just write and I write and
I write and this is great and I go back
and I read and I read and I read this is
all I see everything's looking
completely random this is grid what
Holmes if I delete something so I've got
a big file now that let's say it's not a
big file it's even just a simple it's a
fork a file that I extend to be it care
I can't go back and modify that what I
have to do is at the market is deleted
because I can only agree as a whole
block at a time
why is that the way SSDs work is there
right every cell to be a one and when
you put data into it you turn off the
bits that you don't want and if you try
to write ones to everything it's very
easy to turn off of a specific bit
because you dream this is the circuit to
drive current in the turn of bed on cans
to light up the cells around it and so
you can't accurately write a single bit
so if you want to arrears
you have to write ones to the whole
thing
this becomes interesting so I've got an
array as a block but I can read and
write single page now what's the effect
of that is I can end up with bits that
are marked as deleted because you don't
wanna reuse the whole block every time
because these things have a limited
number of cycles sometimes when we can
thousand times you can read and write
they are arrays these cells over and
over again and the word ID you don't
want your desk to wear.i too quickly so
what they do is they just mark them as
delete it and then the copy the data
that they need to another block and the
new data that goes along with it so
that's always he got a cost now what
happens is after a while you disk ends
up fragmented and you've got to go
through and delete all of those ones
that actually were marked as delayed it
uncompact and you've got garbage
collection going on well people say well
what about trim but all trim does is a
hint from the operating system to the
desk that is not using something anymore
so it allows it to Alma tries the cost
ID but it doesn't solve the problem and
what the cost ends up meaning is you get
this really bad behavior at times so
here's a really nice clean disk this is
actually the one for my one of my
laptops which is an Intel 320 this is a
two year old disk and it can read out
over 200 mega second and write at nearly
200 mega second absolutely grit then
what happens when it fills and I start
deleting and modifying stuff read is
still looking good but right starts
going all over the place and that's
because of this garbage collection and
the readwrite cycle as it's dealing with
compaction and the fragmentation going
on and this is a really well behaved
desk some discs are terrible in that the
the performance just falls off to
nothing and you have to reformat them to
get them to recover this we're getting
better and better all the time where
drives as this is happening over and
over again and we're going to be a
careful of there's a term called write
amplification is where sometimes small
rights and cost so much copying to occur
that the writers are
five - what's actually going on so just
to cover this very quickly the random
rights can help performance because the
more acts you're raising the blocks we
seem to have great random performance
for reads an interesting imagine if
you're only ever doing append-only right
you'd never have to raise that could be
really quite good so what's the axle
performance figures measured off the
desk so let's let's actually hurt when
these desks with lots of loads so let's
hit it at 40,000 times a second
our average and so this is with the next
read write balance our average on the
read is sort of around a hundred to two
hundred microseconds you see people
getting claims of 20 microseconds and 30
microseconds you don't see it in reality
with any sort of reasonable load these
are more realistic figures you're
looking at about 100 to 200 microseconds
is what you'll actually see through some
of the better disks at the moment and
the max goes up because they get tangled
in the GC but look at the their rights
again it's looking really quite good but
starts having problems and the averages
hide a multitude of sins because the max
start getting pretty bad thanks to
latest theater the art disks are still
pausing for up to half a second at times
they're not so good discs pause from
multiple seconds as they're tried to
sort out fragmentation on this so the
view is this random is it not random I
think plausible I think if you just read
from disks you do get perfectly random
access if you just write in an
append-only fashion again you get
phenomenal throughput and good random
performance if you mutate in place
performance ends up starting to hurt and
becomes very very random so on that note
you've had a whistle-stop tour through
some modern hardware do people feel they
understand a little bit more
are you more baffled is it possible for
us to understand this stuff and write
better code sure hands to who things we
can actually on
just found this stuff in a write better
code very good how many people think
there's no hope for us to understand
this stuff and we should just take note
and bury our heads in the sand it looks
like a result oh thank you very much
hopefully haven't kept you too long if
you've got any questions just fry them
away</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>