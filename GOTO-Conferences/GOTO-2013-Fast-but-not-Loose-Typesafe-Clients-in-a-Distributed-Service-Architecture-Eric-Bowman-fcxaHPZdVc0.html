<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2013 • Fast but not Loose: Typesafe Clients in a Distributed Service Architecture • Eric Bowman | Coder Coacher - Coaching Coders</title><meta content="GOTO 2013 • Fast but not Loose: Typesafe Clients in a Distributed Service Architecture • Eric Bowman - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2013 • Fast but not Loose: Typesafe Clients in a Distributed Service Architecture • Eric Bowman</b></h2><h5 class="post__date">2015-10-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fcxaHPZdVc0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">any guilt is not really a very popular
in Europe it's fairly popular in the US
and it kind of arose out of a kind of
recession and and there was a big glut
of global or kind of luxury brands they
couldn't get rid of their merchandise
and so guilt built a business around
that it's it's an e-commerce site with
it looks fairly predictable in that
regard but there's a lot of dynamic
behavior behind it sales change all the
time and as Steve indicated there is a
stampede effect when particularly more
popular brands go on sale and there's a
lot of scarcity around it and is
typically really kind of soluble items
and they're they're incredibly cheap or
at least fairly cheap essentially
everyday is a little bit like the Super
Bowl where there is this moment when
everything changes and people cut on
their lunch break particularly in the
east coast of the US start hammering on
the site and this graph doesn't even
really capture how intense it is because
it just doesn't have enough resolution
it really is down into the kind of
millisecond range we see this incredible
surge of traffic go through the system
and it really kind of brings unusual
non-functional requirements in terms of
how we have to deal with things and it's
been a lot of it's been a challenge in
what started as kind of a generic
website ran into technical challenges
which sort of shaped the future of tech
in the company in kind of good ways very
quickly as a tech company that these are
the kinds of things that we're using
these days and part of this talk is a
little bit about the the path that we
took to get there yeah I'm not going to
talk a huge amount about this stuff
although I wanted to give kind of a
sense of where we are in the tech
landscape and we've really embraced
microservices pretty seriously kind of
based on on the outcome of what I'm
going to talk to talk about today so
when we started it was a very simple
website really and I say we this is
actually before my time rails in
Postgres and and they are amazing
and they're really probably isn't a
better way for start-up to get going
quickly and there was you know there was
sort of a vision around this business of
all these brands have this stuff that
they're going to destroy and could we
actually create a market for that and
and we did create a market for that and
it went pretty well and growth was was
very quick in kind of 2008 into 2009 and
then we signed up a shoe manufacturer
that probably known as room has heard of
maybe one or two people I had certainly
never heard of it I I don't actually
have any shoes that look like this but
there are people who really like these
shoes in fact a lot of people and this
was a very very hard brand for us to to
get a deal with because it's been very
sensitive about never having their
things on their their merchandise on
sale and so we have to sort of a
delicate dance to get them to sign up
and then everything really had to go
well for them to be happy and so getting
this friend was a serious coup and it
absolutely destroyed the site i mean it
was just I mean it was down for hours
and you know people crying and it was
not not a good day for the tech team in
particular and so basically we had to
figure out or they had to figure out
still before my time what it would take
to actually make this work and what it
was actually going to take was something
like 300 servers and keep in mind that
there's this incredible pulse of traffic
that lasts for 10 minutes so you either
have to figure out how to spin all this
capacity up which at the time was still
a little bit risky or you have to have
all this spare capacity around but it
just seemed like an awful lot of
hardware for this unusual use case so
there was a bit of a bit of a transition
in the company fact they called they'd
hired a lot of really good Ruby
developers and and people who really
really loved Ruby and basically the
decision was made to look at this guy
which was the JVM and it was a very
traumatic event actually for a lot of
the Ruby developers and in fact they
occasionally get called the Ruby Taliban
now because it was
just like the idea of running the site
on six servers even though that seemed
good it wasn't Ruby and so there was a
little bit of a transition that happened
in terms of the tech culture at gilt and
a pretty small team of maybe five people
ended up rewriting kind of the entire
tech stack into this very kind of
coarse-grained kind of service-oriented
architecture they broke it up in very
sensible ways users and orders and
payments and products and the cart and
page Jen was a sort of a page generation
services you can imagine in the front
end there's the thing they called Swift
which was basically a JSP engine and
incredibly simple really and it worked
and they actually could could have made
it through the louboutin sale with
something like six servers so they were
happy it was all looking good and
actually it's kind of a nice
architecture you have a front end server
and it can call out to lots of server
services and compose basically what it
gets back from those services to form a
page and that's kind of something that's
not that hard to get our head around and
it's sort of you can break the work up
into teams and the kind of the core idea
here was that all these service calls
were made using futures and if you're
lucky you can make all these future
calls very early in the request and by
the time you actually render the page
all of the data is there and it goes
really really fast and it did go fast it
was everything was low tech and and
surprisingly fast actually I'm still I'm
surprised at how well the JVM works at
least under certain certain conditions
um so it was really i mean we're looking
at a very old-fashioned architecture
reliefs just like a three-tier
architecture not not really sexy it in
any modern way but you know it's kind of
engineering and very reliable and what
we saw kind of emerged was you know this
front-end that we could do a lot of
caching and could do some computation
and orch
strating these calls to different
services and the service steer kind of
got a little bit heavier but though it
was still cashing and trying to protect
the database and the the kind of early
very coarse-grained service oriented
architecture really did a pretty good
job separating concerns and then we had
a few different databases although one
very very important database which was
very hard to scale and fusion-io
actually saved the day there postgres in
fusion-io is amazing but that's a
probably different talk and sorry it
sort of colored this because sort of hot
we think of this kind of the temperature
of these things and there's both the
kind of run time temperature and but
also the development temperature so the
you know things are working well the
front end is super super busy and we've
got that scaled across a lot of machines
and it tends to be CPU completely maxed
out the service tier kind of pretty busy
but hopefully not as busy and basically
at this point if we hit the database
it's probably going to be a site outage
we would never hit the database when
we're busy insurgency enough the it also
kind of reflects the where the
developers are most busy there's just a
constant set of features and bug fixes
and browser compatibility and stuff and
and the code turns all the time in the
front end and and similar to the kind of
access patterns it slows down as you get
closer to the data so looking kind of it
at how that data moves through the
system what we started doing in the
in-kind the initial version of this
thing was to basically make it very
similar to Ruby and the reason for doing
that was basically performance and I
when I this is kind of about the time
when I joined I had never seen anything
quite like this and I'm still a little
bit in shock over it it was the most
low-tech Java I'd ever seen I came in
from kind of a spring and a maven and a
more enterprise-e background and this
was pure JDBC and like the simplest
little library that could turn result
sets into hash maps and then all this
oh that was operating on these hash maps
and then using an early version of
jackson to turn it into JSON and then
from json back into ash maps and it was
fast is about the nicest thing I can say
about it I mean it was like Ruby in a
way only much more horrible I mean
there's just no type safety anywhere in
the system really and and the thing is
like Java programmers expect some degree
of type safety and so it's there was a
kind of an impedance mismatch over how
the system was architected and how
people could deal with it and kind of
what happens there is you get something
I think of is like designed by accretion
where you can't actually ever take
anything away and so people just keep
adding these we can't it's impossible to
track down who's using what you can try
and search the code and I mean you can
sort of get there but it's expensive an
error prone to figure out who depends on
these different values particularly if
they're named something stupid a name
that appears everywhere so you can never
remove things and things just kind of
grow and basically in here the chaos
gets out of hand surprisingly quickly
and so that was kind of not great and
then I meanwhile we're kind of scaling
this whole thing across a bunch of
services and there was just this
incredible sort of divergence of meaning
and and it just got worse and then we
wanted to try and add people into the
system and basically no wonder that
could really understand how the system
worked it was not a happy onboarding
experience I think for a lot of people
and so there was definitely a problem
there are some things that we really
felt were going pretty well definitely
the system was really really fast and
the service decomposition wasn't wasn't
horrible and there was an implicit core
model that we'd really seen work and at
this point guilt was selling a lot of
different things including wine and all
these things had very kind of
complicated sales models and all that
I've had been figured out and all of it
was working and so there was something
definitely really good there but we're
failing pretty hard kind of on the org
scaling part and there
no real discernible api's anywhere that
anyone could see and and basically the
implicit core model that was good was
also because it was implicit it was
incredibly hard to reason about and lots
of bugs were made about bad assumptions
and it was you know it was it was a mess
and so we were pretty sure we knew what
we wanted which was a core data model
and an API is everywhere and it's not
rocket science or at least one hopes it
isn't rocket science I think what's
interesting to me is everybody talks
about wanting this stuff but actually it
never seems to happen or less rarely
than it should so I was actually pretty
happy to get to be involved in this um
but there is one thing that worried me a
little bit and that is that it starts to
feel a little bit like our PC and tying
it into Steve I've been reading Steve's
work for a long time and I'm not sure
how familiar you are with Steve's work
but he was a serious proponent of korbo
which is a distributed object model and
all about our PC and he swung quite far
the opposite direction and wrote quite a
lot about it and if you're not familiar
with his writings you should definitely
check it out um but despite this little
niggling concern in the back of my mind
we decided to carry on and so we had
basically all these core services that
were the sort of legacy and they were
all built completely differently and
actually internally they were sort of
exposing json-rpc endpoints and and all
the problems I sort of highlighted
around type safety but they were working
and we didn't really want to touch them
but we were really stuck it was very
hard to upgrade the system in a
meaningful way mostly because of the
reasons i highlighted earlier so so the
approach that we took was a little bit
it actually felt wrong but we decided to
try it and so we had all these these get
repos and and my my instincts kind of
told me that we should try and
big clients for the individual services
with the services and we had sort of a
group think about it and and decided
that we would start out not doing that
despite the fact I was pretty sure that
was wrong but I figured we could fix it
later and so we made just this single
repo that was basically the entire core
data model and clients for all the core
services and this a sync client
framework that would hopefully make it
relatively straightforward to create
clients so by client is I'm really just
talking about like a Java interface that
defines the end points on a service and
you call into this thing and it makes a
call to the service and you get
something back also known as our PC but
there were a couple of things that we're
doing a little bit differently then at
least it had been done in the past we
were building everything around futures
as I mentioned and that was one of the
things that made me feel like maybe this
was not going to be a disaster the core
model the data model we built in as well
so everything that kind of was needed
across the site which up to that point
had been hashmap sweet ride to actually
nail down what it was we found bugs and
we went through and fix a lot of bugs
and a lot of services and it came
together actually pretty quickly and it
seemed sort of okay but what it didn't
really offer a very good solution for
how we were going to continue to add
services and services with clients going
forward and actually sort of an
interesting case where the tool shape
our thinking we were using aunt at that
point and aunt is very hard to use
particularly to do something like a
multi-module build where you can think
of a single logical kind of project that
can publish multiple jars that was it
can be done but we weren't doing it and
it this was a point where we sort of
realized that actually the decision
to to put everything in one jarred been
partially motivated by the fact that it
was hard to do anything else as is so
often the case so we did fix that and
and so we started to think about a
single get repo that had like a service
and a client and kind of a core which
I'll talk about in a minute and we
wanted to make the the services restful
as much as we could and we decided to
write the the clients in Scala which
we're really liking for a bunch of
reasons that I won't get too into and as
I mentioned everything was going to be
future based and then we made the kind
of controversial at the time decision
that we were actually going to hedge our
bets in terms of the kind of protocol or
the kind of the sort of encoding of data
between the client and the server people
talk about schema first or they talk
about code first and there are lots of
code generation tools and soap and all
this crap so we didn't do any of that we
basically looked at case classes in
Scala which are this incredibly compact
representation of a simple class with
with virtually no behavior and there are
kind of tools that can turn an instance
of a case class into JSON or build that
instance from json or database or
whatever its is very very a kind of
elegant and simple way to represent the
kind of thing that you might represent
in a schema and again it was quite quite
a controversial move that ended up
working out quite well so we put we put
that stuff in what we started to call
the core and the core essentially was
like a firewall to avoid having any
client code depend directly on service
code and vice versa um kind of some
earlier attempts at gilt to create
clients the client had actually just
been part of the server jar and and so
if you wanted to talk to the server you
had to actually link to the server
itself which led to unspeakable horrors
in terms of dependencies this is kind of
just a simple thing as a simple model
for how
people could start to learn how to do
this in a way that we despite the fact
that it felt an awful lot like RPC scene
may be okay and so what it starts to
kind of look like at runtime is you have
some consumer of a service and it's it's
basically dependent on this client jar
and the client jar is dependent on this
core jar and the client knows how to
call this service but the clients would
sort of think of is like an embassy for
the service it's very much not really
owned by the consumer in any way and we
tried really really hard to keep the
details of its implementation from
leaking out into the consumer which is
actually pretty hard on the JVM and stop
the JVM is not good at that the jsr 291
it's kind of looking at this kind of
thing in osgi but it in practice it
ended up not being a huge deal although
the fact that we weren't releasing this
to the world probably reduce the amount
of crazy things people did with the code
and so we built this kind of ecosystem
around this stuff that seemed kind of
nice we made it really easy to write
functional tests for the services by
writing what we're essentially like unit
tests of the client API and then we
wrote a way to capture service responses
and kind of fiddle with random number
generator seeds and and be able to
replay the conversation in a relatively
simple way and that actually gave us a
ton of leverage in terms of quality and
also being able to deploy really quickly
without worrying too much and as part of
this we came up with a very simple way
to essentially express that one
particular get repo depends on tests
that are in another git repo that sort
of fell out of this work and we did some
work around making things kind of
upgradeable on a live system and and
also one of the one things that we've
really struggled with
I don't know how many people work with
customer like public-facing web sites
but regression testing is kind of a big
deal particularly in our case because we
have such a narrow window every day I
think if something goes wrong during
that time it's not good and things like
selenium sort of work but we've we've
really struggled with how how to avoid
just paying teams of people to test the
site manually and one things that came
out of this was the same functional test
that could be written by just simply
annotating them in a using a convention
you can actually emerge from all the
code the set of tests that does a pretty
decent job of kind of quickly testing
this is two men to end and that was kind
of exciting we also ended up doing a
bunch of work around just automating the
process of upgrading to the latest
version of all the shared libraries and
we've we've actually started to become
kind of compile farmers which I wasn't
quite expecting and I kind of wish I
could go into more detail about this
stuff because this is the hard part of
writing library code in Java which I
haven't heard anyone talk about in a
long time but there's things like
fragile base classes and actually I
think it's Scala and Scala it's even
worse and I've I've sort of heard
occasionally over the years about how
Google is spending a huge amount of
resources compiling all the code all the
time and I now have a much better idea
of kind of why they do that things are
not intuitive and fragile in a way that
you would think that they wouldn't be
and it's a little bit the dark side of
the JVM in my view and and things like
C++ and there was kind of another
component to this that turned out to be
pretty important which is using
zookeeper and zookeeper I've heard it
come up a couple times already today
zookeeper if you don't if you haven't
used it it's painful to use it first but
it absolutely has got to be the most
significant contribution to distributed
computing for
mortals certainly for a while it is a
kind of this distributed directory
system that has kind of the triggers and
guarantees and yeah I'm not a zookeeper
expert but what I love about zookeeper
is that it's API essentially is very
well constructed to prevent you from
doing something stupid and everyone I
know whose approach is really annoyed by
the API until they realize that it's
actually that every single thing is
exactly the way it is for a reason which
is kind of a good feature of an API but
by making the decision to kind of have
zookeeper everywhere what became
possible is suddenly this coordination
between our our services and our clients
so that they could essentially configure
themselves automatically by starting up
by just simply knowing how to find
zookeeper in any environment whether
it's your development laptop or some
stage machine or production it all works
the same the convention is completely
standard and so things just sort of work
so for example what URL does the service
live at the end of is not actually
something that needs to concern the user
of the API they may need to know that
and it needs to be easy to set that but
in terms of just being able to quickly
do something and also have it be sort of
predictably working the same across all
these different environments it's been
pretty amazing and because you can sort
of subscribe for changes to zookeeper it
becomes straightforward if you need to
do a kind of global configuration change
to everything if you know for a variety
of reasons it doesn't happen all that
often but when we need it it's kind of
amazing so it ends up just being this
level of indirection which is extremely
powerful compared to kind of more a more
spring approach I guess it also lets us
do kind of a circuit breaker it's a way
to to tell all the clients to do
something without them necessarily
having to
here for without the service that
they're talking to necessarily being up
and so over time we found this to be
really quite powerful in terms of kind
of partitioning the concerns across the
different people who who sort of matter
and ultimately trying to make it it's
kind of easy to just get the job done
which are trying to do which is very
often build an application that depends
on data so we're feeling pretty good
about what we built this stage and
pretty happy and in the back of my mind
I kind of have this vision that Steve is
saying this is completely wrong you
people are insane and the reason that
Steve feels that way thank or at least
part of the reason is his kind of
summarized this guy Peter Deutsch put
together this pretty famous eight
fallacies of distributed computing and
thieves are in quite a bit about how our
PC one by one kind of violates these so
I tried to kind of go through them in
this system and try to understand how
how this approach was doing and actually
I feel I feel like the reliability stuff
is largely solved through futures it
does put some burden on the programmer
and it's possible I'm missing the point
although the future is particularly naka
and I guess they're now in the Scala
standard library are really quite
sophisticated in terms of how you can
represent failures and carry failures
through the system and I feel quite good
about how how that's leveraged so number
two is latency is zero again and I feel
like futures hide that fact or make it
not a concern so I'm going to boldly
claim that that's resolved and been with
his infinite is another one of these
things that uh it's sort of a sort of
and I'll get to that I'll get to a
little bit more detail about the sort of
part
but ultimately for me this feels like
much more service design issue than an
RPC issue and if you'd build services
that make sense that's not such a big
deal the network is secure is obviously
yeah whatever that means in the modern
age apparently no network insecure but
that one actually zookeeper has helped
us out a lot in fact we've been able to
use you keeper to do things like keep
kind of shared credentials in a somewhat
more secure way if not absolutely secure
then then what I've seen in the past
which is usually in a plain text world
readable file somewhere in the file
system so i feel kind of okay about that
and definitely feel good that there are
a lot of options that we have the
topology doesn't change so that you know
this is the idea that you just assume
the network is exactly always going to
be the way it is now and zookeeper has
already helped us out a whole bunch
there and the idea of having this
configuration that you can subscribe to
so I feel pretty good about that and
there's one administrators sort of
similar there can be as many
administrators as you need but there is
only one zookeeper at least one that
matters and so I feel sort of boldly
okay with that as well transfer cost is
zero is another kind of a sort of it is
definitely the case that when you have a
lot of things kind of on your mind and
you're trying to move fast and get a lot
done that you forget about it basically
it's impossible really for most people
to keep in mind how much things actually
cost in terms of network and what are
the different orders of magnitude
differences between doing something kind
of on board and off board so there are
problems with this however it is sort of
captured in the service design hopefully
and in practice it hasn't been a big
issue except in one particular case and
so I'm going to boldly call it resolved
in quickly
probably no surprise I actually think
the homogeneity of the network isn't an
issue so much either although that
probably has a lot to do with the fact
that we're running our own data center
and we when we do things like move stuff
into ec2 we have a fair amount of
control over how that happens so that's
vaguely resolved so I still suspect the
Steve's not convinced by this and he
wrote a pretty interesting paper called
convenience over correctness which is
obviously the goal art well so that
basically the art his argument is that
our PC is really much more about
convenience and correctness and I think
that that's true correctness is
fantastically hard most of the time and
I sorry I took just a little look at
what it would take to make a tiny part
of the problem that we're looking at
correct and it's a little bit of a
distraction but this is this thing that
keeps coming up for me and so I thought
I would bring it up in public about all
the different data models in a system
and how often people cut corners here if
you think about just the very simple
kind of three tier architecture and how
data in kind of the extreme case if you
wanted to handle every possible case you
know there is the data is modeled in a
database and it gets loaded into a
service and very often it needs to be
turned into a different slightly
different model so that it can be
computed against in a reasonable way and
then it needs to pass out of that
service and and there's a net another
model to represent it on the wire and
then it gets picked up by a client and
translated possibly into a client data
model where there might be more
computations that are done against it
and then finally it gets exposed outside
the client into what may be a completely
different model data model again very
often you want to keep the client data
model private so you can change it and
then public data model
you want to be much more careful about
changing so yeah that just mean right
though there's five different ways of
representing the same data that in the
extreme case you need to be thinking
about and in things like the RPC
frameworks of the past some of these
were generated by by software from some
description and I think probably a fair
number of people over time have have
tried to understand how we can use
inheritance and code generation and
different schemas to simplify all this
and kind of my my conclusion is that
that that's basically impossible that in
the general case it just can't be done
in a way that's actually usable we
though we have tried so basically this
stuff kind of works in practice but in
theory it's completely broken and it's
just yeah business goes on and so I
would argue that we are still doing a
couple things that are a little bit
different that actually matter so the
the absence of machine generated stubs
turns out to be pretty important it gets
people so you know this this was the
case in our mine it was a case in RPC
and I remember actually looking at rmi
about ten years ago and having just no
clue how this stuff worked and no like
it was kind of amazing that there could
be this code that absolutely you
depended on it was sort of generated and
had to work perfectly and it was
difficult to get any kind of coherent
description of what it was trying to do
or and so we haven't taken that approach
all basically we're using a kind of
inheritance in Scala to mix in a lot of
different behaviors and trying to make
it so that you do actually supply all
the code that matters and it's all
accessible to you and at every point
kind of in the stack there's the
opportunity to do some kind of
customization if it's needed most of the
time it isn't
and that kind of ties into that what I
was calling embassy oriented programming
where rather than having these stubs
being this thing really you're thinking
of the client is part of the service and
then we sue keeper there's a lot of
indirection and as I mentioned the type
system support for failures is is pretty
amazing one and I think we're only just
starting to understand what that means
kind of the Scala world but there's a
lot that does suck about our PC and I
think some of the stuff is kind of gone
in a way and some of it is still here
remote objects like what I obviously was
taking off object orientation had taken
over the world most people here are
probably too young to remember that
every single thing was object oriented
and all of these objects were mutable
and these incredibly complicated
programs were created and part of that
contract was that you could sort of not
care where the objects were if they were
local if they were remote and I think
that's gets right at the essence of the
claim that our pc sucks that that
doesn't work of course I don't think
that's what we're doing and failures
were pretty horrible and and item
potency is something that we strive for
but it's really really hard and a lot of
the real problems that other people have
had or require things like transactions
which again are hard in the simple case
and very quickly become almost
unbearable in a distributed system
luckily that's not a problem guilt as
very often so we ignored all that and it
was pretty easy and it was felt like
there's a lot of success in fact in back
in April we measured how many services
we had created a guilt and there's like
about a hundred developers creating
services at a great rate it's actually
over 300 now and they're all kind of
following this model and everything is
working pretty well with one sort of
glaring exception
and that is basically it's like the if
all you have is a hammer everything
starts to look like a nail so there was
a little bit of corner of the Gil world
that I wasn't paying as close attention
to as I probably should have around how
batch jobs were done and like a really
big part of kind of gilts model is
around personalization and we know a lot
about what our users look at and what
they like and and and because we have
this sort of scarcity a model that's
actually really important why people
like to come to the website it's very
easy for people who would probably buy
something to not actually see it and
there's a lot of algorithmic work that
can be done to make that situation
better and a lot of that algorithm
algorithmic work happens in batch jobs
and this sort of ties back into the two
sort of things I was not completely
comfortable about and this idea of the
law of instrument which is basically the
name of the phenomenon when when all you
have is a hammer and so what basically
what happened was that it was these
clients were so easy to use and they
would work just anywhere and it was
trivial people were essentially doing
the equivalent of enormous batch reduced
jobs making the service calls and they
didn't really consider what was going on
which in this case was like hammering to
death on line production customer-facing
services and kind of leveraging the
power of computers to bring down the
site and so really you know it's like
these clients are all about pulling data
and that that's I mean that's kind of
what so is about like one of the things
that people talk about is kind of one of
the downsides of service-oriented
architecture is that you you lose the
ability to do joins and databases a lot
if you really bulkhead your services
which people tend to think is a good
idea then you end up having to join in
the service layer and that has its
problems it doesn't always scale very
well
and so if you really kind of want to
scale on a batch mode you're rather than
pulling the data to the code you want to
be pushing the code to the data so when
there's tons and tons of data moving it
is expensive the code is much much
smaller and that's kind of what
MapReduce is all about and grid
computing and these days spark is this
thing that's running on Hadoop that's
all about that and it's pretty
interesting so but we sort of stuck not
a point where it feels like so of socks
or maybe it's because it's our pc i'm
not sure but we're trying to think about
kind of a next generation of so and what
does the service really start to look
like for us and i think probably other
people and there are a lot of trends
that i think are pretty interesting and
relevant and this this idea that the
basic crud model for databases is a bad
idea we're starting to embrace pretty
pretty much everywhere we can we built
some tooling around this and there's
cotton we're not unfortunately in a
position to used atomic although I wish
that we were because I think it's kind
of amazing we've also moved hugely
toward event streams as basically kind
of event sourcing as a way to represent
the way system or basically the way data
is changing over time and obviously
batch processing has been become huge
lambda architecture is sort of important
to us Nathan Mars grave and gave an
amazing talk last year here about that
idea which I won't go into too much and
there's a there's pattern command query
responsibility separation I think it
stands for which is basically
identifying that actually lots of times
it matters quite a lot to separate the
reading from services from writing and
writing is fairly rare often whereas
reading would might you know in a batch
process for example reading would be a
huge event happening billion
of times and so you know where we
started kind of them from here the
simple thing and now we're actually
ending up looking at services that start
to feel a little bit something like this
which doesn't feel simple to me but it's
so is kind of where we are in terms of
how can you actually create things that
scale and do all the stuff that you need
done which is you know data has to go
into the data warehouse and has to be
available to batch jobs and real-time
data needs to be subject to complex
event processing and made lots of things
need to be pre computed so that services
can look them up and I mean this is
basically you know what just a single
service starts to look like whereas a
year ago for us at least the service was
like a servlet container running Jersey
and that maybe had a database behind it
and hopefully not so we're yeah it's
kind of an exciting time what's
interesting about this model is that we
don't actually have to break any of the
clients to implement this stuff that
part seems to work fairly well but
anyhow that's that's kind of all I have
have a few minutes for questions ran a
little bit under the time I was hoping
to which is good um yeah got some
questions right oh let's see could you
have scaled the Ruby on Rails system by
only changing the architecture and not
the language / technology so it's hard
to know the answer that question for two
reasons one is I wasn't there there is a
lot of people talk about whether Twitter
could have scaled I think really ruby is
not a very efficient runtime it just
isn't even on the JVM and so it's I
think it's not a question of could we
have made it scale so much as did we
want to pay the money to scale it did we
feel like it was worth it
and also and I think also you know
performance wasn't the only thing as we
were scaling the organization at the
time we were finding Ruby hard to scale
across as you got sort of multiple teams
trying to work together and were you
know I think collectively were big fans
of typesafe programming as a way to help
that can you say more about emergent
regression tests yes actually we're
putting together a talk on that there's
quite a bit of interest in that this has
been sort of a ongoing struggle for us
as I mentioned we have a process
internally when when something goes
wrong that's customer-facing we have a
weekly meeting that's that's basically
to kind of talk about what was the root
cause and talk about what went wrong and
basically the obligation now coming out
of that is that you if you are kind of
on the hook for one of these issues you
not only have to fix it but you also
have to provide the automation that
would have caught the bug if at all
possible as part of the process of
fixing it which is one of those things
that seems kind of obvious but it's
people don't tend to really do that in a
committed way and we're genuinely trying
to do that in a committed way and the
other thing is that we've moved we're
really trying to move away from kind of
dedicated QA people and sort of an
internal ethos around decentralization
the problem is that things like a global
regression tests say are these
inherently centralized things and we we
tried to build a team that would kind of
manage library of global regression
tests and it didn't really work very
well for a variety of reasons and so
basically we decided rather than try and
centralized a team to do that what we do
is try and have the absolute minimum
amount of centralization required which
was basically some kind of convention on
on the tagging and then some relatively
straightforward machinery that could
search through the code and Phi
all these tags and assemble all the
tests and run them and then over time
you know as people go through the the
sort of process of fixing incidents that
that library emerges and gets bigger and
the things that are actually causing
outages are the things that we're
actually writing tests for and there's
just like a convergence toward quality I
think the other thing is that you can
only get so as a system gets bigger I
think you can only get so far with
testing in real time and so we're also
just now starting to ramp up kind of a a
much more ambitious way of looking at
the real time kind of clickstream data
and other and log data and centralized
log data and trying to detect things
that are happening that are bad as
quickly as possible rather than try and
catch every single thing as an example
you know for if we're looking at logins
by user agent if there's some
particularly weird version of internet
explorer or whatever that's failing that
the way to see that stuff is to actually
pick it up in real time and then try and
fix it rather than try and focus on
we're going to test every browser all
the time so we do some cross browser
testing but not extensive enough to
catch all these issues and and now it's
kind of clear that we have to fall back
to a kind of event processing to catch
that stuff okay um did the Ruby folks
not bring any test methods along with
their hash map design so yeah the Ruby
code base still exists by the way
there's a this is a huge code base and a
lot of it is internal facing and it's
actually a very high quality code base
but in the holy god oh my god we're all
going to die moment of porting
everything over to Java what didn't
happen was the same degree of testing
that had happened in Ruby and that was
regrettable I think it's fair to say at
the same time I think it's
to imagine the amount of pressure that
that team was under like the business
was literally on fire so yeah we were
still feeling the pain there if you're
running see QRS with the vent sourcing
what do you use for event storage so I
wouldn't say I think it's fair to say
that we aren't yet doing that we are
using Kafka and we're using rabbit and
we are now basically catching a lot of
events from a few systems into Kafka and
then basically doing an ETL operation
into a data warehouse but is this is
super early days and I think there's a
lot of interest in that style and we've
done some of our own implementations
with some mixed success but the the
events or stuff that was done for acha
that I guess has now moved into acha i
think is really interesting although i
have no opinion yet on how we would
store that stuff to do the does the
audience like the fact that the speaker
trolled the track host since i was told
i do want to make a comment good um no I
think what you're doing is fine because
the problem that I was writing about is
the the convenience is that you know RPC
was invented to hide distribution from
the developer if you go back to the very
early writings on RPC the point being
made and it's actually in the text of
these writings that says today we don't
have distributed systems developers
people don't understand how to do that
so we have these library developers so
if we can make the programming look like
writing a library then we went and you
know they're sweeping under the rug all
those hard problems the the eight
fallacies and ordering and causality and
all that kind of stuff and that's the
objection and clearly you're not doing
that because you're thinking about the
distributed system and you know writing
it specifically as a distributed system
it was with significant irony that I
realized that we
had in fact been burned by exactly those
issues that on the batch yet processing
yep so yeah vigilance is required and
and yeah I long agreed with that the
idea of not making it look like local
code so that you could actually grep the
code and find everywhere that was making
a remote call and it's hard to keep that
kind of diligence up yeah there was
question there yes as you build up that
many services if I understood correctly
you're using skala cases kiss classes
and provide typeset client-server
communication yes what is your
experience been in the in the impact of
coupling your systems like that as you
try and change like add new fields data
types in those types of things right so
the question is about duties are
coupling how does it work out using case
classes to share data between the
clients and servers um so yeah there are
gotchas although it works pretty well if
you add fields and very often when you
add a field you have to make it an
optional field and so the the clients
that are out there are prepared for are
prepared to ignore data that they don't
understand and in practice that works
quite well although a lot of people go
through some cognitive failure which
leads them to delete something but the
test catch that really really quickly
having said that it has become clear
that we have to be really really
aggressive about trying to upgrade
things as quickly as possible if we
leave things I mean they're running fine
there'd be no reason to upgrade them
other than it's just sort of a policy of
constantly upgrading everything to the
latest as quickly as we can consider
moving to just primitive data types so
that you can so a server can iterate and
push out new data without all the
clients having to rub just to pick up a
statically typed object um yeah so the
question is that we consider I guess
what was it allowing just the data to
fall across without the tightest
wrapping them so that you don't have to
rev all the clients every time right so
yeah that was sort of how it started out
I mean that's basically a type-safe and
type unsafe approach and yeah we found
it hard to maintain that code it does
work you can do it that way but as long
as we maintained backwards compatibility
of the data which is not it not actually
that hard in practice as long as you
don't delete things and then you add
things carefully it hasn't been
necessary to upgrade those remote
clients although we do choose to kind of
for other reasons but more related to
dependencies with say the configuration
system then the actual data
compatibility ok there's more questions
Eric will be around but please let's
thank our speaker thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>