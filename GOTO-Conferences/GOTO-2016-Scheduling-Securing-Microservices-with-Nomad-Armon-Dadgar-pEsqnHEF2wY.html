<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Scheduling &amp; Securing Microservices with Nomad • Armon Dadgar | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Scheduling &amp; Securing Microservices with Nomad • Armon Dadgar - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Scheduling &amp; Securing Microservices with Nomad • Armon Dadgar</b></h2><h5 class="post__date">2016-12-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pEsqnHEF2wY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">brief context my name is Armand you'll
find me all around the internet is also
just ARMA
so relatively easy to find and as Nikki
said I'm one of the co-founders and CTO
of Hacha Corp
we're probably better known for our
tools than for the company name many of
you are probably familiar or have used
vagrant one of our more popular tools we
also make terraform and packer and vault
among sort of others that that Nikki
mentioned for folks like HMRC but today
I want to spend some time talking about
nomad which is our cluster manager and
scheduler these are both sort of
relatively new domains so I'd like to
sort of explain what a scheduler is as I
think this really makes it a lot more
useful when we when we talk about nomad
and its design what it's trying to do
that's sort of the highest level what is
the purpose of any scheduler a scheduler
ultimately is there to map a set of work
on to a different set of resources right
and so a concrete example something that
we're all relatively familiar with or if
we're not familiar with we still use
without realizing all the time is a CPU
scheduler so if you think about any
machine it has some limited number of
physical cores right there's a physical
reality that we have only you know a
core to maybe it can hyper thread and
they're sort of virtual threads running
but effectively there's a physical limit
to our resources but there is a sort of
an unlimited number of workloads that we
might be able to place on this right
it's not inconceivable to have thousands
of threads as scheduled on an operating
system and so this responsibility of
mapping this workload whether it's you
know web server threads kernel threads
multiple applications on to the physical
resources is the job of our CPU
scheduler and so over time the scheduler
is trying to optimize for different
things right for most types of CPU
schedulers they're optimizing for some
amount of fair use of the CPU if it's a
desktop scheduler for you know you know
OS X it's gonna optimize for some level
of responsiveness so it's going to do
things like shuffle around what's
actually running on these physical
resources either to give fair access or
to optimize for responsiveness
this is just one example of you know a
scheduler that we use and interact with
all the time
the reality is schedulers are absolutely
everywhere but their function is
basically the same it's just their
mapping different sets of work on two
different sets of resources so our CPU
schedulers are mapping virtual threads
onto physical cores you can really think
about clouds like ec2 or OpenStack Nova
as effectively just being schedulers
mapping VMs onto hypervisors you have IO
schedulers that are mapping it onto disk
spindles and the sort of the examples go
on and on when we talk about cluster
schedulers they're sort of domain as
mapping applications onto a set of
servers and so generally speaking why do
we use schedulers at all what are the
advantages of using them in sort of any
setting it really comes down to three
different primary benefits one of them
is higher utilization of the underlying
resources whether that's a hypervisor
whether that's a physical CPU core it's
a decoupling of work from resources and
it's better quality of service we're
better sort of depends on your
application and the techniques we use to
get there as sort of shared across every
type of scheduler to get higher levels
of utilization of our resources we do
things like bin packing so if you think
about a hypervisor you have your
physical machine with 64 cores you spin
up many VMs on it you're packing
multiple machines onto the same workload
you're effectively bin packing we do
things like over subscribe so it might
be that you actually allocate more
memory to those VMs than the physical
machine has RAM because you're sort of
you're relatively confident that your
VMs aren't going to consume all of the
memory so we can sort of over subscribe
the system and are optimistic that we're
not actually gonna exactly is exhaust
resource we can do things like job
queuing so if we have more work than we
have physical resources we can put these
in a queue such that the moment we free
up some available resource we can start
running the new thing right away so we
minimize idle resources then we can talk
about sort of the decoupling benefits
this is done through a few different
folds one of these is it allows us to
abstract away details of the physical
resource away from the logical work in a
way that sort of frees us from having to
care about some of the underlying
details so for example when we're
writing an application we don't really
care you know
what is the cash alignment of my CPU and
I sort of sharing a cache line and am i
crossing a memory boundary these are
sort of details that are abstracted away
from us by the CPU scheduler it sort of
allows us to work at a higher level
without having to be sort of bothered by
low-level abstractions and so the way we
ultimately get this is by having an API
contract that our scheduler gives us and
standardizing around that and the last
one is better quality of service and
this really depends on sort of what the
goals of your application are if my
application is something like Hadoop and
I'm just running you know thousands or
hundreds of thousands of shards of work
you know my quality of service is
something around you know what time does
it take to complete all of my workload
right and so the best scheduling
decisions are probably something around
fair use if I'm a desktop scheduler and
I'm scheduling you know Safari and an
interactive interface I care more about
responsiveness so I care about the
latency of user interaction being
handled right and if I'm a moon lander I
may I want I really care that like my
rockets fire in times that I don't crash
into the earth and as opposed to you
know did I fairly schedule the
background you know data upload process
so the notion of quality of service
really depends on what your domain is it
could be very different what you care
about but ultimately it really comes
down to three different mechanisms that
get applied one is priority so you know
is the you know is the rocket you know
firing program a higher priority than
the telemetry upload program then like
make sure it gets priority access to
resource its resource isolation so even
if you have multiple resources multiple
physical resources do you have low
priority work that's happening to steal
resources away from high priority work
so we want to be able to isolate that
and its preemption so if all of a sudden
high priority work shows up but we're
busy doing a bunch of low priority stuff
can we get rid of the low priority stuff
and switch over to doing high priority
work so the concept of using something
like a cluster scheduler to achieve
these the goals isn't new and it's been
done for decades Google for example has
famously used boards since the early
2000s if we really think about AWS as
just being a giant scheduler it's been
about a decade there Netflix and Twitter
more recently both have their own
schedulers that they use so this
sort of a common approach we see done at
large-scale to achieve those sort of
efficient economies of scale so with
that background that contacts that sort
of brings us now to Nomad which is our
cluster scheduler so when we're talking
about a cluster scheduler it's from
important to remember what we're doing
is mapping applications onto a set of
servers right and so in doing this we
have a number of goals in designing and
building nomad one of these things that
it's easy to deploy applications that
being sort of the primary function of
the scheduler to that it's operationally
simple to run this thing right so as you
might imagine schedulers much like you
know CPU scheduler your cluster
scheduler ends up being sort of the
center of the universe for your data
center and that you want this thing to
be built for scale but you don't want
nomads to ever become the scalability
limit of your broader application so
where this starts for us is with the job
specification is how do we provide that
workload how do we specify what
applications we actually want nomad to
deploy and where this starts with nomad
is a job file the job file is meant to
be human editable human readable simple
to write and understandable and this is
a totally valid example of a job file
that would deploy Redis so here we're
defining a job naming it Redis basically
specifying a docker image to run and
providing some set of resources we want
Nomad to dedicate to our job before we
sort of get into the details of it
what's really important with the job job
file is that it declares what we want to
run right and importantly what it
doesn't do is tell it where to run it or
how to run it these are details that are
left to Nomad right and so this kind of
goes to that second goal of a scheduler
which is how do we decouple the work
from the underlying machines right and
so if our developers were telling us
specifically machines to put it on or
how it should be run there's really no
decoupling the developer as there is no
abstraction basically so the key here is
by it's not allowing people to provide
this this is what allows Nomad to
abstract the work from the underlying
resources so now Nomad can make
decisions as to where the work should
run how it should run
should it be preempted should it be
moved somewhere else and so this is
super key to have this decoupling this
abstraction so that we can have an API
contract in terms of the type of
workloads you might run on top of Nomad
it
you know the example happened to be a
docker container but it can sort of be
anything right from nomads perspective
we're mapping a set of applications
whether they're containerized or
virtualized or standalone onto a set of
servers it doesn't particularly matter
what the type of application is
underneath it's gonna use some amount of
storage compute and networking and
that's really what we're accounting for
here so if we're running containerized
workloads like docker and rot I'm sorry
docker and rocket those are supported
natively virtual workloads through Q EMU
standalone things like Java jars or
statically linked binaries are all
supported today the system is designed
to have a very flexible pluggable
architecture so it would be you know
relatively straightforward to extend it
to support Windows Server containers
then hyper-v the list sort of goes on as
you might imagine so where this really
starts for our developers is with this
declarative job file it allows them to
express their workload in an
infrastructure as code weight these job
files are meant to be version control
but they're meant to be peer reviewed
you can do pull requests and talk about
you know and use your normal code
process to to evolve how you define and
run your application and importantly it
lets us remove imperative logic so as we
do things like move from AWS cloud to
Google Cloud or shift traffic from
on-premise to the cloud these things can
then be abstracted away from nomads
because we haven't imperative lis
declared dependencies on them but what
about our external dependencies what
about all of the other things an app
needs to do other than just start right
so questions around ok how do we connect
to our service discovery registry how do
we health monitor these applications how
do we get applications secrets to them
how do we deal with our stateful
applications they don't particularly
like being moved around and so there's
sort of answers through each of these
but the answer is gonna sort of be
repetitive in the sense that you put it
in the job file you you declaratively
define what you want to do and it'll be
nomads job to make sure that takes place
so for example how do we declare that we
want to be part of a service discovery
index well we declare that with that the
thing that we're running our application
is a service itself so we might say we
have a service we'd like to expose our
HTTP port to the rest of the cluster and
by the way we have a health check as
well so please hit the health endpoint
every five seconds and that's going to
tell us whether or not this app is hell
so what this will do when we provide
that specification to nomads is all of a
sudden in format that oh by the way you
need to interface with console and
notify it that the thing we're running
is a service and why don't we just do
this automatically well it could be the
case that what we're running is a batch
workload so for example if we're running
a massive Hadoop job or SPARC job it
might have you know tens of thousands of
shards that aren't services we would
ever want to discover right so it would
just sort of be it would just pollute
the namespace of our service discovery
if we were registering sort of anonymous
batch jobs so we want to be explicit in
registering services when it makes sense
so here what's gonna happen as nomad is
very much decoupled from console they're
just interfacing with each other over an
API so Nomad is constrained to just
caring about scheduling problems so no
matter is going to schedule the
application potentially placing it on a
client alongside any number of other
apps that it's been packed and then
because we've declared this as a service
Nomad will inform the local console
agent that by the way app 1 is a service
and here's this health check information
and then console will take care of
registering the service monitoring its
health and making sure this is available
to the rest of the cluster one of the
advantages of decoupling these concerns
is that we can have applications that
live inside of Nomad and are scheduled
and are sort of living the dream
alongside applications that are you know
legacy VM based monoliths that are
interfacing over console so console can
sort of act as a shared service
discovery bus for applications inside
and outside of Nomad so that brings us
the secret distribution sort of at a
high level when I say secret
distribution it's really answering the
question of how do our end applications
get their API credentials for example to
talk to a you know s3 database
credentials SSL and TLS certificates
these are sort of highly sensitive
pieces of information that our
application needs and sort of the worst
case scenario but you know it's very
common very easy to do is you know well
you just you know pass it to the
application as an environment variable
right the challenge is if we're moving
towards things like infrastructure as
code this file lives in github right or
this file is being you know templated by
something else that then is pulling from
gap and so ultimately this thing is
living in plain text in systems that
have very limited access control
right so how many developers in your org
have access to github and can go and see
what the user name and password to the
database is so becomes very hard to
reason about who has access to these
things how do we restrict that if it's
compromised who could have made use of
that information it's very hard to
reason about it and so for this reason
we created vault so it's a different
product we make its goal is to really
tackle the secret management space and
do so in a way that gives you audit
ability gives you fine-grained access
control ensures secrets are encrypted in
transit and at rest so we don't have a
lot of time to go into the details of
all but at a high level you know its
goal is really doing secret storage it
has you know ways of dealing with static
secrets it can dynamically generate
credentials for things like databases
and cloud providers and it does it in a
way that gives you rich access control
auditing and client authentication
mechanisms so at the highest level
though much like any type of database
there's sort of a login flow to it you
know before you can request data from my
sequel you first establish a connection
and do a login sort of handshake vault
is very similar in that respect so
anytime we're starting to talk to volt
there's an initial login flow where
we're providing some set of credentials
to it if that goes well vault will
return a vault token back to us very
similar to you know how a web server for
example would return a session cookies
does great you've logged in and every
time we perform an operation we're
basically threading that cookie that
token back into vault and if you know
we're still logged in thought we'll
bother to actually respond to our
operation so the question then becomes
great as we're moving into something
like nomad how do we get these tokens to
our application right and sort of a bad
answer would be well just provided in as
a you know environment variable and
paste that into the Java file the reason
this becomes a bad answer is it's
effectively only one degree of
separation away from actually just
putting the database password in there
right so it's like yes the database
password might be living in volt but the
credentials needed to access the
database password are living in gap so
we're still only sort of one step shy of
it so as we might expect the answer is
well there's a declarative block that
lets us do this and get away from this
problem so the latest version of nomad
allows us to instead just specify what
set of all policies our application
needs
so we
actually need to now provide the vault
token itself we just say we need a token
that has at least this level of
privilege and we can put in a list of
all of the policies that we need so if
we know okay my app needs access to s3
and you know the database we can define
a policy for this and just say my app
needs that policy by the time it's
running
so then what vault will do is take care
of doing the flow around that for us at
runtime so that we don't have to hard
code these secrets anymore so when our
users submit a job now to nomads they
can provide their own vault token vault
will then I'm sorry no matter will
verify does Armand actually have
sufficient privilege right like if I'm
only allowed to perform 10 operations
that I just submit an application that
needs to run with root level privilege
in the system and then all of a sudden
we can reject that and say no you're
trying to basically escalate your
privilege in the system so no matter
what make sure we're not escalating our
privilege if we're still relaunching an
app with you know equal or less than
privilege than ourselves then nomads
will go ahead and schedule the
application somewhere so great it places
Apple 1 alongside some other apps and
then there will be sort of a transparent
flow to our application where no matter
will take care of generating a token
with sufficient privilege and renewing
it behind the scenes and it will just
magically appear as an environment
variable to our app so our app still
just gets its vault token as an
environment variable but we didn't have
to hard code that anywhere and this is
sort of in memory everywhere and never
sort of no real risk with it
so the key to the native integration is
we can remove these secrets out of our
job files and just talk about policies
which are safe to store inside of a
version control system our secrets never
hid compliance disks so Nomad goes out
of its way to sort of use temp FS
systems and keep things in memory and
even when it's writing to disk it's real
still writing to ram and we can minimise
the trust throughout our system the
Nomad servers don't have to maintain
tokens clients are maintaining things in
memory things don't have to live in
github cool so what about our stateful
applications those that we can't easily
just knew can blow away the way I like
to describe statefulness is really not
as a binary it's not totally stateless
or totally stateful is just really the
spectrum right and different apps will
live in different places on this so for
example you know on the very left we
tend to have applications like API
servers web servers caches they tend to
be sort of more stateless than not with
some things like cash you might actually
imagine inching them further up towards
the stateful side of the spectrum
they're not totally stateless in the
sense that they are accumulating some
state it's just that for most purposes
we don't really care if we lose them
they're sort of ephemeral by design and
then more towards the middle of the
spectrum you have systems like HDFS
Cassandra Mongo these are you know what
you might call cloud native databases
they expect to be running in a
distributed setup they expect to have
their data replicated you know n times
across data in the cluster and they
expect machine failure and that's sort
of the fundamental difference is their
design expecting machines fail at the
very end of your spectrum you have your
traditional sequel system so my sequel
Postgres ms sequel you know go down the
list and these systems are highly
stateful they basically expect that if
they call right and F sync that data is
never lost right and so as a result the
way we can handle each of these this
sort of goes from easy to very hard
right and the reason it goes from easy
to very hard is that the very hard end
of the spectrum you have assumptions
that you know sort of don't match
reality right the the reality is if you
call F sync and then assume you'll never
lose data again well you've sort of made
an assumption that hardware never fails
and it turns out that's actually not the
case especially if you're running in a
cloud environment that hardware
definitely fails at an elevated array
and so the promise that these
applications are expecting the API
contract if you will is very onerous
right it's very onerous to maintain
whether you're running physical metal
because you have to go out of your way
with raid controllers and multiple disks
and sort of doing redundancy underneath
the database because the database that I
called F sync your problem as the
operator right so this is sort of hard
on a non scheduler world and it usually
requires a lot of hardware to sort of
hack around this but then as you move
into a scheduler world this contract
becomes especially tough right because
now we don't necessarily have the luxury
of raid controllers and redundant drives
on every single machine or using storage
attached networks we're running in a
cloud there is no storage attached
Network everything is ephemeral and
things fail all the time so this
contract becomes a lot harder so in
practice what you have to do is work
around this by sea using distributed
file systems so you provide
what appears to be you know a local
filesystem but really underneath the
hood you're trapping out two EBS EBS is
writing to multiple disks it's using
erasure coding so it's going above and
beyond to sort of simulate the fact that
hardware doesn't fail so we're nomads
really tries to focus is sort of on the
easy and medium today and says you know
let's punt this to RDS and you know the
traditional mechanisms that we've sort
of battle-hardened for it and really
focus on you know the non stateful to
the moderately stateful those that kind
of work with reality a little better and
so if we're just working with those type
of applications what we can give Nomad
is a hint that says you know what my app
isn't totally stateless I'd prefer not
to lose my data my data is sort of
sticky alongside me and what this will
let Nomad do is it gives it the right
hint that says you know what if I do an
application update or something and I'm
changing you know my version one to
version two I'd prefer not to lose my
data so what no matter we'll try and do
at this point is great I'm upgrading
from v1 to v2 if we can place the app on
the same machine then we'll simply move
the data between the two tasks so stop
the old version of Cassandra move the
data start the new version of Cassandra
great we didn't have to do a full
rebuild of that Cassandra node by you
know we're pulling in data from the
other replicas if that machine is full
or for some reason we've changed the
constraints of our jobs that no longer
fits there for example we've gone from
version one of Cassandra to version two
but we changed it and said actually we
really need to be running on you know
the version for Linux kernel and
Cassandra was previously running on
version 3 Nomad can no longer put it on
the same machine because we've changed
what's where what machines are eligible
so no matter we'll find a new machine
that has available space to run it and
it will copy the data from the old
machine to the new machine before
starting Cassandra so we can still sort
of allow our data to migrate alongside
of our app and avoid doing a full
rebuild of of Cassandra State
potentially the key is this is a
best-effort replication so if we
actually just lost that machine the
machine that was running Cassandra
previously just died AWS decided to kill
it then there's no machine to copy it
from so Nomad will still find a new
machine and start Cassandra somewhere
else but it won't be able to migrate the
data so the key is the sticky migration
is really best effort right and so if
we're sort of running these medium
stateful apps that connects
spectin tolerate data failure then they
will begin their expensive application
level rebuild of state so from the
perspective of trying to make
deployments easy this really starts at
the top with declarative job files it
goes into making sure it's flexible
enough to support different workloads so
if you have existing VM workloads or
you're migrating to containers or you're
like I just have a lot of stand-alone go
binaries and I really don't need to
bring in more heavyweight tooling then
you can run all of that through Nomad
sort of flexible interfaces by bringing
in things like console integration vault
integration and sticky volumes we can
kind of handle a much broader set of
application needs and challenges so this
brings us to the operating side of this
thing it's great from the side of
developers that it's easy to use and
they can get the power they need but our
operators still are running this thing
our platform teams are responsible for
it so how do we make their life easy
this starts by making sure Nomad is just
a single binary with no external
dependencies so whether we're running it
in client mode and we have thousands of
instances of it where we're scheduling
work on - whether we're running it in
server mode that's actually the brains
of the cluster it's a single binary
we're just flipping flags on it there's
no external state storage like at CD or
zookeeper it's self-contained as a
system in terms of trying to learn about
how do we build a system that's
operationally reliable where we really
started was by building on the
experience we have with surf and console
these are two other tools we make surf
is a cluster management tool and that's
much lower level it's really a gossip
based peer-to-peer tool that gives us
three major features one is membership
which is what other machines are in the
cluster this is sort of a tricky problem
when you're in a highly elastic
environment where servers are coming and
going you have autoscale groups things
are failing so the failure detection is
key as it lets us know okay which
machines are both in the cluster and
currently alive and the last one is an
event system so how do we broadcast a
message to the entire cluster
efficiently this gets to be a more
challenging problem when you have
hundreds or thousands of machines and so
surf has been proven at very large scale
it's a pattern battle-hardened sub
system that we can sort of use and build
from the san diego supercomputing center
has a 10,000 node surf cluster that gets
used that uses
serve fastly uses a to orchestrate their
CDN so it has a lot of very large-scale
production usage and what we can use
this for underneath the hood in nomads
is to give us very simple clustering in
Federation so if you've ever used consul
and did you know a consul join you're
like yeah that was kind of easy we don't
have to think about what were the IPS of
all the thousands of nodes in our
cluster that's handled by Cerf
underneath the hood and so we can bring
that over to nomads and make it easy to
run these very large-scale clusters the
other thing we started with was console
which is our service discovery tool and
really it provides a handful of
functions service discovery you know
where'd are the API servers are they
healthy what's the configuration of
those API servers are the future flags
you know enabled or disabled are we
doing shadow traffic it lets us do
high-level coordination so for example
distributed locking and it operates very
similarly in the centralized server
distributed client model so what we were
sort of able to learn and bring in as
learnings from console is how do we
support multi data center setups how do
we have a robust raft implementation so
that we have these servers that are
highly available they have consistent
state and making sure that the system
actually operates with you know in a
central server distributed client model
at very large scale so we're able to
borrow a lot of our learnings from
console when building nomad and so that
sort of high-level operational goals
with it was making sure it's a single
binary with no external dependencies as
self-contained as possible part of this
is you know reducing how you have to
reason about a potential failure right
if it's a system that depends on many
other systems and you have a failure
it's sort of hard to debug and
understand where this lies where if we
can make the system sort of as
self-contained as possible it can
minimize the surface area we have to
reason about and it's highly available
out of the box so there's no special
configuration you just run multiple
servers and it will replicate as highly
available if the clients know how to
discover the new servers and deal with
that and so the last one was building
for scale and this goal is really to
ensure if you're putting something like
Nomad at the heart of your datacenter
right it's scheduling your services it's
scheduling your batch jobs it can become
the central bottleneck by which all or
their orchestration is waiting on so how
do we make sure the system is sort of
never the limiting factor to what you're
doing and so where we started on was
building with surfing console but the
problem with this approach is neither of
those has any
scheduling logic right like they provide
low-level tools to know what what's in
the cluster and high-level system
architecture and multi data center
routing but they don't actually schedule
jobs which turns out to be the
bottleneck so our return to was research
to understand you know what is the state
of the art what's been what's being done
out there and who can we learn from and
the two groups that are sort of most
influential to our thinking is the
berkeley ant lab where they've done a
lot of work on things like MapReduce and
yarn and Mase's and spark as well as
google which is probably you know the
preeminent scheduling shop in industry
and so there's probably four major
papers that mostly influenced our
thinking one of them and the biggest and
most important really is Borg
it's Google's Google's primary internal
scheduler to date and so this paper was
instrumental in sort of how Google
thinks about their large-scale workloads
their latest enhancements to Borg come
in the form of Omega which is an updated
system architecture and then from amp
lab we learned about sort of how do we
do very large volume low latency
scheduling of batch jobs from Sparrow as
well as from ASOS how do you do flexible
you know flexible scheduling when you
have different types of workloads so you
know your service is very different
types of workloads than that then things
like batch jobs and so what we ended up
with was sort of an architecture that
looks very similar to console if you're
familiar with it you have a set of
central servers they elect leaders
internally so that you know in this case
we have our leaders in the middle it's
being it's replicating data to the other
servers the other servers are forwarding
a request to the leader that's sort of
servicing most of the operations and
then we have clients that span many
different data centers so we can have
you know datacenter one two and three
all slaved into a single set of central
controllers if we need to distribute our
controllers across multiple regions for
example you know maybe we have a US
region and an EU region and we've
decided that that's how we decide we
want to split our workloads we might
split it and say we have these two
separate regions and in the US have
three data centers and in the EU have
three data centers but have separate
control plains for them but although we
have separate control planes we still
want our developer to be able to just
submit a job to any one of their data
centers any one of their regions and so
the server's federated so the developer
doesn't have to think about it they
don't have to worry about which is the
end point for the EU servers versus the
US servers they just submit it to
anywhere and it will forward the request
as appropriate one of the goals of
having the sort of multi region
architecture is that it also becomes an
isolation domain for failure right so
you can think about this sort of like
the bulkhead of a ship right if we take
an outage in one of our regions the
other regions should continue running
and so this gives us sort of an
isolation boundary that we can reason
about okay where do we want to draw our
failure lines and so because we can have
any number of data centers per region
it's sort of up to us to be able to
model how we want to do this so maybe we
say you know what we're gonna run one
set of servers and have 12 data centers
around the world and the entire world is
our isolation boundary basically if we
lose our central servers we won't be
able to schedule anywhere or we might go
to the very opposite extreme and say
every single data center should be fully
independent it should be its own region
and scheduled separately and great we
can do that if we want to have a
one-to-one ratio of data centers to to
region controllers that's fine so
ultimately though the design goal of the
system was to be able to support
hundreds of regions because we have you
know users and customers that have you
know several dozen data centers and so
what if they want to model each one of
those data centers as an independent
failure domain that should be within
scope for nomads each of these data
centers should be able to handle tens of
thousands of clients and each region
should be able to handle thousands of
jobs running and so the ultimate
architecture of the system is most
heavily influenced by Google Omega this
means it's optimistically concurrent in
its scheduling meaning it can make many
parallel scheduling decisions at the
same time and it uses several controls
to make sure this is done safely and
part of this changes the way we do state
coordination so this means the system
keeps all of its state internally it
frees a free sort of external
dependencies from the system but allows
us to support service and batch
workloads even though they have separate
types of logics several separate types
of quality of service concerns and so at
the very highest level the kind of
mental model the architecture of the
system is basically only these four
types of nouns there's two sort of
inputs from the outside world to nomad
one is the job workloads so developers
were submitting work to the system what
are the things we need to run this is
sort of the work and then there's
operators who are providing resources so
what are the
underlying loads nodes that we can
actually schedule work on to the mapping
table of work onto resources is through
an allocation so we've allocated work to
a machine and the way Nomad decides
these things is by doing an evaluation
so we sort of evaluate okay a developer
has submitted a job what is the
available resources how many resources
do they need let's create an allocation
that map's these things to each other
and so roughly we can think about an
evaluation as taking place anytime
there's a state change so something has
changed in the real world and note has
booted you know a job has been created
updated deleted notes have come and
failed allocations have failed or
finished so something has changed in the
outside world and now we need to
reconcile that we need to sort of bring
the world back into the desired state
and so the way we can think about the
function of an evaluation or I'm sorry
the function of a scheduler is it's
really a function that takes an
evaluation which says hey something
about the world has changed please
generate a set of updates to the world
that will kind of bring us back in line
all right so in sort of Tara forms case
it might be you know the sort of
equivalent is you've asked for a VM that
VM doesn't exist so Tara forms job is to
create the VM nomads equivalent is you
just asked for ten web servers there's
currently zero web servers so its job is
to create a new web server somewhere so
this is how we can think about sort of
the job of the scheduler but the logic
within the scheduler of deciding where
we place it you know the constraints we
want to do the quality of service we
enforce this function is sort of free to
do things however it wants as long as
it's ultimately generating these
contracts and so you might imagine
having different schedulers in the
system that prioritize doing things in
different ways so in ads case today we
ship with three distinct types of
schedulers the service scheduler the
batch scheduler and the system scheduler
and so a batch scheduler expects that
the things that's going to run terminate
eventually right and it also expects
that it's doing extremely high volume
things that are generally relatively
short-lived so you might actually want
to do things like optimize for placement
speed over finding the perfect placement
all right so if I have tens of thousands
of machines in my cluster but you submit
a Hadoop job with you know a million
components you know what's more
important finding the kind of perfect
location to run each one of those
million pieces of the job or just
quickly scheduling them because most of
them will be finished in ten second
anyway so the batch scheduler
prioritizes doing things faster where
the service scheduler assumes that your
jobs will be long running and so it's
worth spending more time to find an
optimal placement so at a very high
level without really diving into it the
system architecture looks something like
this these are totally internal details
that if you enjoy nerding out with are
kind of fun the pipeline in terms of how
Nomad deals with things is things happen
in the real world so there's external
events jobs are being created nodes are
coming and going so on and so forth that
results in evaluations being created so
these are things that the scheduler has
to deal with and reconcile those all go
into a queue so the evaluation broker
queues these up and then applies
priority ordering so for example a high
priority job is more important to
reconcile than a low priority job so
this thing will provide some sort of
ordering and and then we'll fan-out the
work to any number of our servers so for
each server that we're running every
single one of its cores is running a new
scheduler so if we have you know
reasonably beefy hardware we might be
making hundreds of scheduling decisions
in parallel so this will get fanned out
across these things that are
optimistically running at the same time
there's no pessimistic locking taking
place each of them can be invoking
different types of logics so the service
scheduler the batch scheduler a custom
scheduler if you want and then they're
submitting plans and so this is how we
deal with the fact that we're
optimistically making decisions and
there's races right if you're making a
hundred decisions in parallel you might
race and say you know five different
schedulers or designing to put work on
the same machine so how do we fix that
up and this is where the optimistic
coordination comes in is in this plan
queue all of our schedulers submit plans
and they may be fully applied meaning
great you've submitted you know your ten
web servers there's free room on those
machines start running those things
create the set of allocations or it
might be the case that there was a
conflict so there might have been a race
and you place that webserver alongside
an API server because another scheduler
beat you to it and there's no more space
to run your web server so that might get
rejected so the plan queue allows sort
of partial admission of scheduling work
and it will kick things back to the
schedulers if they don't get things a
hundred percent right the first time
around so for very large scale workloads
they can sort of you know nine hundred
out of the thousand might get scheduled
in the first try 99 out of the next
hundred and the second try and that
final one in the third try and so allows
us to make progress even though we're
making the
sejin's sort of in parallel so
ultimately as a result of the sort of
higher level architecture it's very much
heavily molded around Omega what this
enables us to do is make these
optimistic scheduling decisions and and
do these hundreds of decisions in
parallel without sacrificing the
correctness of the system so you don't
have to worry about Nomad sort of you
know quadruple over allocating and nodes
resources it sort of corrects for this
stuff internally so that the user never
sees sort of the issues with the
optimistic challenges so the question is
you know going through all of this work
of building a system that's
optimistically concurrent and dealing
with all the stuff in adopting the Omega
challenge the question becomes you know
did we succeed right we spent all of
this engineering effort like was it for
all for naught so a challenge that we
sort of brought up for ourselves as to
say well how do we know this thing
worked how do we know it actually met
its design goals because we set out
these lofty goals where we said we want
tens of thousands of clients and
thousands of jobs running and can the
system actually do that and so we came
up with a challenge for ourselves that
we called the Nomad million container
challenge which was basically can we
just run a million containers on the
system and see how fast we can do this
right and so sort of the ultimate stress
test of the system and so we worked
closely with our friends at GCP to do
this they gave us a bunch of resources
so we spent up five thousand machines on
GCE and then basically submitted a
million containers splitting and saying
we're gonna submit a thousand jobs each
job will contain a thousand containers
so that we're sort of creating you know
a slightly more difficult workload for
the scheduler to have to deal with and
so that the end total should be that
we're running a million containers and
what we saw is that the system performs
admirably is that it actually does give
you that nearly linear scaling of having
an Omega architecture that it is able to
make you know the hundreds of decisions
in parallel and so even though we
submitted this you know absolutely
brutal workload for the system it's
still able to schedule all million
containers successfully in under five
minutes right and so it's sustaining a
scheduling rate of you know initially
it's a little bit faster but eventually
a plateaus down to you know north of
2,200 per second and so this is pretty
pretty pretty impressive for a system to
be able to maintain this and so you know
one of the things that you might notice
is actually the
there's random dips as it's going is
that it's not a perfectly monotonic line
one of the interesting things you
actually see when you're doing you know
benchmarks of this large is that things
fail in the real world so as Nomad was
running through this and scheduling all
of these jobs in practice we saw that
machines were failing in GCP so Nomad
would schedule it onto a set of machine
that machine would fail and so then a
new machine would be brought up and then
I would reschedule the work so no man I
was sort of fighting the fact that some
of the machines were being losses it's
doing this along the way we also
uncovered some bugs in the docker engine
just because the the rate at which we
were scheduling to the engine was so
high that exposed some edge conditions
and so we had lose probably about two
percent of our docker engines due to a
race condition and so Nomad would also
work around that so it would detect that
those machines and jobs have been lost
and rescheduled them so there's these
sort of bumps as it's sort of losing
machines and rescheduling but even
though it's losing machines as it's
going it's still managed to finish the
million containers in less than five
minutes and so I think the most common
reaction you know for us this was sort
of like mission accomplished the system
did what we promised it would do but I
think the general reaction we got from
people when we talked to them about it
was you know something along this which
was like well 640 KB should be enough
for anybody why would you ever need a
scheduler and I can do anything remotely
close to this and you know the way I
like to think about this was you know
what if I you know what if when Oracle
created the you know one of the original
our DBMS systems they said well it can
do two transactions a second like why
would you ever need a database they
could do more than two transactions per
second right if you're if you have tools
that constrain what you can actually
build on top of them then you're not
gonna go and build Twitter on top of a
database they can do two transactions
per second right like it's going to
constrain the possibilities of what
you'll do above it so when we're
thinking about workloads of this scale
we're thinking about things like well
what if you have lambda function type
workloads
right what if you're doing batch jobs
with millions of pieces what if you want
to reach the point where you don't even
have long-running services and you spend
them up on demand so within the request
flow you're starting and stopping your
services because your scheduler is fast
enough to do that so why bother reserve
resources for a machine that does five
requests a day and so to get to those
levels of capability you can't do two
transactions per second you have to be
able to operate at these really immense
volumes and so you know this you know
many of those things are sort of you
know phantom
future workloads that you're like okay
well when we get there we'll cross that
bridge so one of the interesting things
for us to be able to talk about is a
customer whose workload has actually
even bigger than the c1m challenge and
so shortly after you know we published
this benchmark we had a chance to talk
with the folks at Citadel and for those
who are unfamiliar they're the world's
second largest hedge fund they manage
over 160 billion dollars and they
reached out to us and said you know what
the c1m is sort of cute but we have some
workloads that you know our own
scheduler in-house can do and it's five
times larger than what your c1m does and
we're curious if it will hold up to our
challenges and we're like okay
interesting this is sort of a very
different type of workload because it's
heavily batch so you know in our
benchmark one snowman had scheduled the
docker containers they were long-running
services they didn't have to be
rescheduled
we're in there's it's a continuous
stream of batch work that's being
finished and so it's not that we can't
nomad can schedule it and be done
because 30 seconds later that thing
finishes a new workload arrives for the
system and so in their benchmark they
ran a cluster with 18,000 CPU cores and
for five continuous hours basically
continue to submit workloads to this
thing and nomads continue to schedule
them at over 2200 containers a second
the entire time so I'll let you do the
math on that but just in the course of
their burnin for their workload was
something over 40 million allocations
and so sort of at a high level this is
Nomad right it's a it's really designed
to be a cluster scheduler and provide
those key benefits of higher resource
utilization decoupling of work and
resources better quality of service and
to do this in sort of a way that is
making it easy for end developers to
self-service and deploy their own
applications so how do we sort of free
them from having to talk to our
operators and just let them submit
whatever jobs they want at the same time
there's people at the other side of this
who have to keep our platforms and our
systems operational so how do we make
their life easy so they're not being
paged continuously and then how do we
build it for scale so that you know when
you do try and build lambda style
workloads and batch style systems on top
of this thing that it's not your scale
bottleneck and so that's no matter in a
nutshell that's all I had thank you guys
so much
I think we have probably like three
minutes for questions yeah I I will
start with the first question that comes
through the app is no man nomads aware
of the utilization of the underlying
resources in other words can nomads help
me avoid hot Resources by moving jobs
around that's a good question so it's
sort of two different questions one of
those is is no matter where of it the
answer is yes so for any job you can
basically ask nomads give me the
allocation status and I'll tell you you
know memory usage CPU usage etc and
would give you kind of fine-grained
resource utilization the second question
is can nomads basically automatically
mitigate the noisy neighbor problem so I
happen to be co-located with an app that
is really noisy can it just move my
application to deal with that not today
it's certainly something that could be
done we sort of have the right feedback
loops for it we have the underlying
telemetry there is just you know no
process today by which it's it's looking
for these hot spots and doing a
migration but it's a very conceivable
that it could and because it's all API
driven you could actually imagine
building an app yourself that does this
right like you could just simply query
you know for your allocations you care
about figure out okay it has a noisy
neighbor I'm just gonna like you know
taint that one and make Nomad force
Nomad to basically migrate it somewhere
else so you could imagine actually just
using its api's today to build it
yourself
hello so in the example you gave of
using KVM as a underlying platform when
you speak about the slightly stateful
applications you spoke about the ability
to copy data does that expect shared
storage in order to work ah good
question so the question is when we're
doing the copying of data for the semi
stateful applications does it expect
their shared storage no so it doesn't
depend on using like a SAN or network
attached storage is it's literally like
copying the data from one machine to
another machine it doesn't expect that
there sort of have some underlying
shared storage and so prefer to keep you
on the same machine so nomads will
rather not copy over the network and
instead find room on the same machine
but if it has to or you've changed your
constraints then it'll slave it over the
network
any other questions awesome well thank
you guys so much thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>