<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2012 • Liquid Metal: Taming Heterogeneity • Stephen Fink | Coder Coacher - Coaching Coders</title><meta content="GOTO 2012 • Liquid Metal: Taming Heterogeneity • Stephen Fink - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2012 • Liquid Metal: Taming Heterogeneity • Stephen Fink</b></h2><h5 class="post__date">2013-03-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PCMRYXN3xmU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm going to talk about the liquid
metal project this is a project at the
from a team of us who we have up here
we're at the TJ Watson Research Center
in Yorktown Heights New York this is the
main IBM Research Lab so I'll start by
telling you something which you already
know which is in the last decade or in
the last eight or nine years ago
frequency scaling pretty much came to a
stop clock speeds on computer chips are
not getting faster very much anymore and
so the only way to improve performance
with chips going forward to exploit the
more transistors is to exploit
parallelism and so we entered the
multi-core area error which we're all
familiar with so let me just start by
asking question how many people here
have explicitly programmed for
multi-core with threads or any other
parallel so almost everybody in the room
has programmed for multi-core and I
assume by a multi-core you mean
something like Intel x86 bunch of course
small-scale multicores one thing which
you might be aware of is in the last few
years become very hot is that there are
much more high degree multicores called
graphics chips GPUs and over the last
especially in the last three or four
years these have become very popular
here instead of having a few two or 48
cords we have several hundred cores
perhaps thousands of hardware threads
which can run concurrently and people
have realized that for lots of workloads
you know it's much more efficient and
you can get much better speed and
parallelism by programming with the GPU
and so we've seen the birth of GPGPU
using gpus for general-purpose computing
how many people in the room have done
GPGPU programming program general
purpose computation on GPU all right we
have one well so GPUs are a fairly
radical solution then perhaps for you
but if you want to get even more radical
than a GPU in the world where we have
you know very hard power constraints and
difficulties with scaling you can
actually design your own hardware so if
you actually want to design your own
ASIC your own chip you need a lot of
money and a lot of time and a lot of
expertise fpgas our field programmable
gate arrays this is hardware basically
programmable hardware where you can
create a hardware design and dynamically
loaded onto a chip and effectively
a lower entry barrier to design your own
hardware and it doesn't run as fast and
it's not as dense as a custom ASIC but
it is a way to quickly prototype
hardware and once you can design your
own Hardware you have your freedom you
can exploit extremely massive bit
parallel structures if you can do them
you can get rid of a lot of interpretive
overhead you don't have to worry about
memory system hierarchies and things if
you so if you can design the hardware to
match your application you can get much
better performance as I know in the room
program did not using FPGAs 3 so this is
actually I would have expected more
doing GPUs in fpgas but not in this room
so this is the world we're going into
for those of you who haven't used GPUs
or FPGAs you may be wondering do you
really need to worry about this why do
you need to worry about it let me start
by a few motivating examples that we're
seeing from a lot of our customers and I
believe you'll be seeing more and more
over the first few years so there are
few models of how we might want to use
these specialized devices perhaps the
easiest one to understand is using a GPU
or an FPGA as an accelerator so the idea
here is that there's some part of your
workload which for some reason perhaps
it's massively parallel you know data
parallel floating-point computation
which GPU is can be done very
efficiently can do it higher performance
lower power / flop or perhaps there's
some other kind of specialized
processing which an FPGA can just do
more efficiently than your CPU and so a
natural thing to do is just take part of
your calculation which is running on
your cpu ship it over to use the devices
and accelerate or run fast or run more
power efficient ship the results back
and so forth so infective lee you're
using this as an engine to offload some
part of your calculation this is
extremely popular in the HPC world for
things like oil companies doing seismic
simulations also in financial world if
you look at the top 500 supercomputer
lists many of the of the highest
performing HPC supercomputers now
include GPUs at every node simply
because they're the most powerful data
parallel floating-point engines that are
available today and we see lots of other
applications which can exploit FPGAs as
well when we start talking about FPGAs
there are they have other advantages for
which lend themselves to other kinds of
deployment models
one which is very popular is using FPGAs
in what we call a network appliance
platform so FPGAs you can plug your i/o
such as your Ethernet or serial bus or
let's say your network straight into the
FPGA and now you can design your
hardware so you can design your own
network protocol to exploit very low
level structure you don't have to worry
about say running a custom operating
system stack and so forth so you can get
extremely low latency turn around and
very high throughput at the network
bandwidth level as long as you basically
by moving your computation very close to
your own custom network interface and so
there are a lot of the FPGAs are used
very much in telecom industries of
financial industries networking
applications effectively for low latency
and high throughput network applications
the idea here is that most of the most
of the computation will come in and off
the network directly into the FPGA
occasionally perhaps some
general-purpose computation or control
logic will will execute on the main
system but the FPGA is the main locus of
control of your calculation another
model which is sort of the same idea is
using what we call storage appliance so
in this case the idea is if you can put
an FPGA very close to your storage and
if you can architect your system to have
much higher bandwidth between your
storage devices say your flash or
solid-state disk or even your regular
disk you can putting processing logic
very close to that you can get higher
bandwidth and so if you can do some of
your query processing very close to the
storage you can reduce the bandwidth
demands on your system bus and you can
basically offload a lot of your data
processing in your big data world very
very close to storage so this is what we
call the storage appliance platform this
is very popular right now in big data
database applications and so forth and
it just has advantages IBM certainly has
some offerings in this as well as other
companies so I hope I've convinced you
that there are good reasons and to use
these architectures and we're seeing
more and more of these coming along yet
most of you in the room had very little
or no experience with this one of the
reasons why is because doing this kind
of heterogeneous programming is not very
accessible
mainstream software engineers today one
of the reasons is that each of these
devices demands a different programming
than model right now we're all very
happy programming in high-level
languages like Java or Python or
whatever for our CPUs and they provide a
very high level of distraction if you
want a program general-purpose GPUs the
current state of the art is to use
something like khuda aur opencl these
are fairly low level c api's and subsets
of c which allow you to use the GPU but
you still put the program in a much
lower level of abstraction you have to
learn a new programming model you have
to manually manage communication between
devices you have to be very careful
about structuring your computation to
use the GPU effectively so its base but
relatively painful especially compared
to our experiences and programming
general-purpose processors however
programming GPUs makes is like heaven
compared to programming FPGAs those of
you who have done this no you know so
first of all programming FPGAs most of
the people who program FPGA is they
actually don't say they program so you
don't actually program hardware you
design it your hardware designer and you
create a design and you load the design
onto the chip and we don't even think of
it and that's why these are design
languages VHDL and verilog and there's a
very these are very extremely low-level
uh basically circuit design languages
that describe your computation you don't
have any benefits on the FPGA of an
operating system or device drivers or
memory managers or any of the things we
expect you know the system software
which makes writing programs easy so
it's really a very low level very
painful experience compared to writing
software to be able to use an FPGA in a
heterogeneous system so what we're
trying to do is see how much we can do
to fix this so the goal of our project
it's called liquid metal and we have a
programming language called lime and our
goal really is to try to make using
heterogeneous systems with a mix of
these kinds of devices accessible to a
mainstream software engineer programmer
so that programming a heterogeneous
system with a GPU or FPGA is not that
much harder than as it is programming in
a high-level language on a cpu today
and we think that if we were able to do
this you know we can make these
platforms more accessible and the
platforms are coming right now a very
small elite of customers are using them
we expect that due to the trends in
physics in the the just benefits of
using these architectures there'll be
more and more demand we see this coming
along so we're going to try to make it
more accessible and more like mainstream
programming so our strategy is we have a
single language called lime which I'm
going to described in some detail which
looks very much like Java we have a
pretty rich tool chain to try to make
programming in line resemble programming
in Java as far as the richness of the
tools and we'll show you some of the
details of how we go that and also we
try to provide especially runtime
systems to on which the system runs to
hide many of the low-level details so
that'll be the the content of the talk
is describing our approach to this
before I go on that slide I was I'll
skip any questions before I get into the
details of the programming model all
right so let's talk about liquid metal
so our philosophy is that there is a
single programming language line but
we're not going to guarantee that we can
compile all of lime to every device so
there'll be different subsets of the
language that can be compiled to each
device depending on the constraints of
the device so what we have is the line
compiler will spit out more than one
version of different it's a task based
program model of different tasks
depending on what sort of devices it can
exploit for each piece of code so all
the code can be implemented on to run on
a JVM we have a we can spit out JVM
bytecode and run on a cpu so you can
write a full program using any of the
language constructs into the run on
bytecode if you want to run a particular
task or look particular routine or a
particular part of code on a device such
as a GPU or an FPGA the compiler may or
may not be able to generate code and so
the philosophy is that the compiler you
can write general code and then the
compiler has exclusions the compiler
will give a lot of feedback on reasons
why it's not being able to compile a
particular piece of code down to a
device and we try to make it as easy
possible then for you to refactor your
code to satisfy the compiler
tell the compiler more information that
it needs to know in order to generate
good code for a particular device and
I'll get some details of what this looks
like so what is lime lime at a very high
level is very much like Java but we add
a couple of other constructs which allow
us to compile down to these devices
which basically fall into two categories
one is isolation so isolation is extra
information to Java code which allows us
to move computation around and I'll show
you some of the lime constructs the
local keyword and immutable types which
enforce isolation on parts of the
language which allow us to accelerate it
and then a lot of it's about abstract
parallelism so we in order for the
compiler to exploit highly parallel
hardware and to move and schedule
computation around it needs to have a
very good idea of the parallelism so
we're not taking a super compiler
approach we're not asking the compiler
to infer parallelism from secret
sequential code or anything like that
instead what we have are fairly
high-level constructs into the language
that the programmer can use to tell the
compiler this is this style of
parallelism and once things type check
the compiler knows okay it's a way for
the programmer to communicate parallel
structures to the compiler and then the
compiler can map them down to the
hardware so I'll describe these in a
little more detail so our development
experience as I said is the idea is
we're very big on an incremental
migration incremental refactoring so the
idea is you would prototype your code
basically in Java so you could start by
just getting a functional version which
works in Java and to do this it's it's
we have an ID which is the clips based
ID gives you all the benefits of the
Java development environment including
editors and debugging and so forth and
then we want you to incrementally
migrate the parts of your code you want
to accelerate by adding these lime
language constructs which express the
parallelism the compiler needs to know
any questions yet before I dive into the
details
bound face they look like one lower
Presidente's 10 um this is quite
different from x10 yeah I can get some
more details I used to work on next ten
as well it's a let me present what we
have then I could describe one thing at
a very high level we're not trying to
solve distributed computing problem
we're not trying to express clust
distributed memory cluster computing
which is the main focus of X 10 so we're
not trying to scale out to a cluster of
workstations our constructs are really
designed the ideas you have a single
node with devices attached and managing
the details of particular devices we're
not solving the problem of how you would
communicate across nodes to do that you
would use message passing or x 10 or
some or some other model but that's
outside the scope of what we're trying
to solve any other questions good move
stuff
please stand up um so it's different for
different platforms basically there'll
be a set of platforms which we support
and we can handle those so for the GPUs
we basically compiled down to opencl
which is a standard for GPU hardware
which is supported more or less by
nvidia and ati and intel and all the
vendors for fpgas there are really only
two vendors that dominate the market
which are xilinx in Alterra and we have
support for both their tool chains so as
far as GPUs and FPGAs are concerned
we've got most of the world covered yeah
that's our problem so that's what we do
for for for a living yeah that's what
that's what that's why what what we do
for the rest of my life is any other all
right so let's try to i'm going to try
to out so i think the best way to try to
describe at least the flavor of the
programming language in a very short
talk is just by walking through an
example so i'm just going to walk
through a simple example which is an end
body simulation also i'll show a demo
later so we'll just consider one
iteration of an end body simulation the
ideas here we have a bunch of particles
perhaps you're familiar with these kind
of computations these particles might be
think of say stars in a galaxy or maybe
they're molecules in a fluid and they
interact by some force gravity or sores
or some other physical force and we
basically have an N squared calculation
where in each time step each particle
will exert force on every other particle
according to a physical law and so
there's two there's basically three
steps in the calculation you generate
the positions of the particles you
compute the force on each cup on each
particle and this is the expensive part
of the calculation this is the naive
algorithms or N squared so each particle
effects every other particle and then
there's a force accumulator once you've
computed all the forces you sum them up
and then you'll move the particles to
their new position and then repeat so if
we wanted to do the accelerator model it
certainly makes sense very at a high
level since the force computation is the
N squared computation the rest of them
are
linear will want to we'll start off by
saying we want to accelerate the
computationally the force calculation
and that's what I'll walk you through so
you start as I said you would start so
we'll imagine that we already had Java
code to do this and started with
something that looks like Java code the
n-body class the first thing we do is we
need to tell the compiler all right
we're going to have a task parallel
structure so we we need to tell the wine
compiler okay we've got three tasks here
this is the structure of our computation
and these are the boundaries at which
we're going to want to move code around
so the way we do that in lime is we
apply a task operator which is a new
keyword to a method declaration of a
java method so the code highlighted in
green here is saying what we're doing is
we're setting up three tasks we're going
to turn the particle generator method
into a task the force computation method
into a task and the force accumulator
method into a task and these are just
regular Java methods ignore some of the
strange arrays stuff for now and then we
have the connect operator which is the
equals greater than sign this
virtualizes communication so this is
saying that the output of the first task
the particle generator will be fed as
the input to the second task force
computation the output of that task will
be fed into the third test which is the
force count accumulator now so so what
we have here is very much a test
parallel coarse-grained dataflow
programming model and that's at least
the outer level of parallelism by which
you'll express your computation so for
the rest of talk I think at least on
this example I'm going to focus a bit on
the GPU model of doing the data parallel
force computation but one thing I want
to point out at this point is this task
parallel model is very much in the
spirit of hardware design where you see
layup data flow graphs of blocks and
there are constructs in the language to
do splits and joins and matching and
rate matching and so forth so very often
for hardware what you'll do is you'll
set up a very fairly rich task graph
which maps down to hardware and this is
the kind of thing that runs well in
hardware cause basically each task if
it's in a pipeline like this corresponds
to a you know a pipeline cycle and a
pipeline design and so it's very much in
the style of how people do hardware
design
I hope that's clear I'll go through a
few more slides and then clarify
anything that needs to be clarified so
once you've identified the task graph
which is what we've done here on the
programmer will tell the compiler that I
want to relocate a particular task and
we do that with these relocation
brackets and that's basically telling
the compiler that in this case i want
this force computation task to be
accelerated and then depending on how
i've configured the the compiler
toolchain it will decide which which
back end it's going to try to so let's
assume for the moment for this thing
that I've configured my tool chains that
accelerated tasks the compiler is just
going to soon try to send the
accelerated task to the GPU now what
happens is once you start to use these
ensure breast brackets some of these
stricter type checking rules come into
play so at this point the compiler will
start to check that this force
computation task not only that it's
legal line but that it actually obeys
the constraints that it needs to know in
order to ship it off to the GPU so in
particular the the properties i
mentioned before about isolation and
parallelism and so in particular you'll
have to start telling the compiler
information about the force computation
task so I'll get into a little that over
the over the next few slides but just to
show you the method declaration in
particular I've said local this is going
to basically enforce that this force
computation is isolated so we don't have
a general shared memory model each task
runs in an isolated address space suit
they can't read and write random access
memory across it the only sharing of
state between tasks is through the
inputs and outputs and so the local
qualifier guarantees checks tells the
compiler to check that that property
holds we also have these immutability
constructs which I'll describe a bit on
the next slide I believe yeah so let's
look at the code a little bit more so
what do we need to do in order to get
this computation running on a GPU so I
mentioned the local the local says this
is an isolated task what that means is
basically the compiler at this point is
going to check that the inputs and
outputs to this task our values that is
their immutable so that they can't be
written and write and written or read
or observe to changing by other parts of
the system while it's in transit we're
going to enforce immutability which
basically tells us this value property
for the inputs and outputs so that funny
array syntax we have on the inputs and
outputs of force computation basically
this is saying that the input to the
force computation method which we're
turning into a task is an array of four
tuples of floats so it's a two
dimensional array but it's immutable so
nobody else is going to be scribbling
into it while this task is running and
it's going to return an array of three
tuples which is the forces which is also
immutable the local keyword also tells
the compiler to check that this this
calculation is transitively isolated
some particular can't read or write any
global mutable state so with that
information the compiler knows that it's
safe to relocate this task and that it
won't change the semantics of the
program in any way now we also have in
this case using the map operator the @
sign so what we're going to do is this
is a data parallel application of a
function it's the same as map if
familiar with math and lists or any
other functional languages which is
basically saying take this function in
this case the force function and apply
it to each particle to each element of
in this case an array so basically this
tells the compiler that we have a data
parallel computation and by once again
using the same sort of annotations local
and immutability we guarantee that there
are no in effect loop carried
dependencies we guarantee that this
actually is a data parallel computation
and that we connective ly if we were
running on a cpu we could execute these
in any order and that means that we can
use the GPU hardware we can fire it off
in parallel and we don't have to worry
about sharing or the semantics so this
is basically the type checking here
tells the compiler okay this is safe to
paralyze and it won't change semantics
now if you violated these type checking
you'll get feedback during the IDE
saying this this this method needs to be
local or this parameter is not a value
or so forth and so the process of going
from vanilla Java code to something like
this that can tell that can be
accelerated it's effectively a dialogue
with the compiler where the compilers
starts to give you warnings and you can
go go in and fix them
and it's really not usually if your
algorithm is parallel we think it's not
not to Burnham's and to do that so I
mentioned the map operator this is a way
of describing data parallel operations
we have you know fairly rich well the
syntax is really pretty simple but it
allows us to express a bunch of patterns
of data parallel operations so we can
map an operator so when we say that C
equals a at plus B this is mapping the +
operator to overreach of the arrays a
plus B and this does what you expect as
as illustrated here is that will do an
element wise edition of the array a and
B producing an array see there are many
other patterns you can use to do data
parallel so you can do you can apply
methods in parallel so this float C
equals map the add function a and B is
exactly the same as mapping the operator
and then you can also do lift lifting or
promotion or Kareem which is for example
you can add one to each element of array
and by using this pound sign it's
basically saying lift this scalar value
into array as an argument to the map
function so that's the basics of data
parallel computation I hope that was
clear any questions before I go on right
so what we have here I showed you the
code for the n-body calculation those
are if you had programmed with opencl or
CUDA you know that this is much higher
level and much more abstract than the
current state of the art of programming
using in this case opencl where you have
to do a lot of system level q set up and
compiling and management and manual
communication you know we're programming
a much higher level of abstraction for
the GPU um if we're programmed for the
FPGA it's if we're a somewhat higher
level of abstraction for GPUs we're in
incredible orders of magnitude much
higher level of abstraction than you
would write for an FPGA if you're
programming an HDL or Verilog you know
even very very simple user logic which
you would Express in Java expands to
very complicated state machines with
low-level driving wires and concurrency
and checking signals and timing
in verilog it's really a very low level
program model just to express the user
logic and expressing the user logic in a
heterogeneous system is not really the
hard part the hard part is managing the
communication between the FPGA and the
host computer it's not like you take an
FPGA and plug it into your USB and and
and the operating system recognizes the
device and and you start using it
transparently you know this is really a
raw hunk of silicon it doesn't come with
an operating system doesn't come with
anything so you have to write or find or
acquire the IP to implement the device
the intimate the drivers on the hardware
and to have some sort of run time system
on the hardware to do all the routing
between that and so it's really a
tremendous amount of work just to build
the basic communication infrastructure
between a device and a CPU and so the
gentleman asked a question you know this
is the kind of thing that we have done
and we've done it once and we can do it
once and then our compiler can reuse it
so we have built the runtime system
which manages the communication our
compiler knows how to use it so
basically the programmer doesn't have to
worry about it and doesn't have to know
it and this is we think a tremendous
value compared to the alternative of
doing this from scratch yes
so we're compiling Verilog we're
compiling directly down to hardware and
then our hardware our circuits basically
the hour runtime system is calling it a
virtual machine is perhaps a little too
glorious it's a bunch of cues with
control signals and arbitration logic
basically to route data from the pins to
user logic so you would take our user
logic which basically exposes the number
of wires and then this is what our
compiler does I mean the programmer
never ceases the program are just writes
local into fubar what happens to the
compiler you know generates Verilog
which exposes a bunch of wires our tool
chain hooks up those wires to the wires
of our internal cues and then our cues
talk a protocol over say PCIe to our
hosts we have drove I drivers that know
how to talk to the same protocol yeah
that's right I mean it's really a bus
and and control logic any other
questions so we think you know compared
to the alternatives of programming FPGAs
we think we have a very compelling story
as far as productivity we're saving a
tremendous amount of work just in
writing the code but a lot of the story
here is not just about the code it's
really the tool chain as well which
especially for fpgas but even if even
for for GPUs where the tools and the
tool chain and the the process by where
you use these devices right now the
state of the art is you know very
primitive compared to what we're used to
with software it's almost like going
back in time before the before the
invention of Fortran and try to show you
you know somewhat some of our tools do
and what the alternative would be with
the current practice so for example
compiling the code you know in our
system basically compiling the code we
have Eclipse windows and launchers so
you would open up a in our ID you would
open up a specialized compiler wizard
and which would walk you through how to
configure your compiling
for the FPGA and then you hit compile
you know the alternative using the tool
chain that as people do right now is
really relatively painful you have to
use multiple tools it's all vendor
specific you have to write a lot of
logic and test bench just to drive your
logic to test it work with simulators
and and so forth and you know it's a
very long tedious process that takes a
lot of expertise and this is part of the
reason this is a large part of the
reason why it's very hard to get started
using FPGAs you know it takes a lot of
expertise just to learn a particular
vendors tool chain and then if you learn
the xilinx tool chain that's not
necessary going to help you with
learning the Alterra tool chain or if
Intel is going to come out with
something so it's all but you know
completely different from what we're
used to in software where it's almost
like going back in time to the days
before programming languages so once
again as far as productivity I think you
know we're making it much much easier
you things that would take at least days
if not more to learn how to do and to
configure and get working you know we
hide behind a wizard and the system
drives automatically debugging is a
similar story you know in our system if
our compile if our system is implemented
correctly the semantics of the program
do not change whether you run them
whether you run it on the bike code back
end or whether you run it on an
accelerator the program should produce
the exact same answers as long as our
system is correct which we hope it is so
you can debug your user logic simply by
debugging it as if you were debugging
Java code so we have a Eclipse D Butler
integrated into eclipse where you can
set breakpoints and introspect and do
all the nice things as if you were
debugging Java code you know what people
do with hardware design today basically
is they look at waveforms and this is
the level of debugging which is the
alternative today so you know we're at a
fairly much higher level than that now
we do our tooling will allow you to run
on a simulator very easy and produce
waveform so if you want to for example
look at performance or look at what's
going on you can get to the flow level
information but for the vast majority
program is we anticipate they would
never have to know that you know that
there are things such as clocks and
signals and times and they could do the
debugging at a much higher level
abstraction once again the whole theme
here is trying to bring current practice
in this world much closer to what a
mainstream audience would be comfortable
with running code is is much the similar
story you know running hitting a run
button the ID as opposed to learning yet
another tool which is what you have to
do to to load bit files onto an FPGA and
execute them and so forth so there's
just a tremendous gap in the experience
of using a modern tool chain versus what
the current state of the art is and
using these devices today synthesis so
in order to compile a Verilog file or
VHDL file down to a circuit you you run
a synthesis tool this is a vendor supply
tool which basically lays out the logic
does the the design and tries to obey
tries to satisfy clock constraints and
layout constraints and floor planning
and come up with the design so this is
learning yet another tool this takes a
very long time to run synthesis on even
smallest user logic modules takes many
minutes running for big designs can take
hours often you'll run a synthesis and
it will just fail because the tool
wasn't able to satisfy some timing
constraints or some clock constraints so
this is yet another area where we can
try to raise the level in the experience
we've decided to do this by a cloud
service we call lime Forge so are now
for regular development you can develop
simply on your desktop you can run in
the simulator on your desktop and and do
all your development when you're
actually ready to deploy onto a physical
device you need to run the synthesis
tool you need to generate the physical
bit file to load onto the device and now
you have to do synthesis what we do is
to make this place pay folds we've
implemented a cloud service which
basically you submit your job where you
ship your logic and the program over to
the cloud and our cloud will now run
whatever it takes to try to synthesize
this so basically it will in the
background the cloud will spawn out a
bunch of different synthesis jobs and
search for different clock frequencies
try to maximize your clock frequency
there are lots of ways of tuning
constraints to the since
tool so our cloud will search through
different constraints and apply
expertise which are the experts on our
team have learned over the years on how
to drive these tools so basically we try
to have a somewhat intelligent
intelligence maybe not the right word
but a pretty advanced system in the
background using a lot of resources to
try to get the best results from the
synthesis tool and once again take the
burden of learning how to drive this
tool correctly and how to optimize this
tool away from the programmer and try to
do it automatically and we do this in
the cloud because this is very resource
intensive and so that's why we farm it
off as well as licensing issues it's
easier to just install one copy of the
vendor tools so that's it i'd say at
least i've tried to give you a flavor of
what we're going for at least four
developer experience and what the system
is like i could talk a little bit about
the implementation any questions at this
point before we have we have we have
sale yes no skill of the people you have
years now it is to let go away pink or
let's say the relevant well we'll be a
bit like a Sunday programming or you
could think of it I'd like to say yes
we'll put them all out of business you
know will and I think for the vast
majority if we're successful we will
there they'll always be I think reasons
why you want to go down to the to low
level design for some parts of your code
or for some applications as far as our
system is concerned there's a few
reasons why you might want to so we've
got a very high level programming
language we've made it very easy and
safe to express parallelism but we've
taken away some of the power to express
arbitrary forms of parallelism from the
programmer so you can't express
arbitrary concurrent structures in our
language so if you want to write an
arbitrary design with queues which are
communicating on a clock by clock level
and be very careful you can't express an
online in our line
which so for that you'll have to drop
down to a lower level and that's for
example how we implement our own runtime
system we don't our own crossbar cues we
don't dog food that we we drop down the
other thing is just the question of
what's the maturity of the compiler
right now there's a tremendous gap
between our source language and what
we're compiling to and the compiler is
still relatively immature compared to
compiling to assembly language on on a
cpu we're even that took basically 30
years of development you know we're
solving a much harder problem or
compilers just not that mature yet so
you're definitely going to give up
something probably if use our compiler
either performance in terms of clock
cycle a throughput or area more likely
our designs may not be quite so space
efficient one thing we do have in our
system is what we call lni on native
interface so we do export interfaces
whereby you can write your own logic and
Link it in to our system so one possible
use model is if there's a write
everything and if there are less
performance critical parts of your
design perhaps logging or some control
structure or some debugging code or rass
code you might want to write that in our
system but for something which is really
super performance critical you would
write that by hand and then link it into
our system just like you would do
perhaps native code into Java today we
can also go the other way we can simply
export Verilog from our tool chain and
you can take that Verilog and do with it
what you want so you can use our
compiler basically as a legacy to gates
just as a higher level there are a lot
of design thing and then you can plug it
into your own infrastructure and worry
about so it can go either way it depends
on how much pain and and how much
performance you want so let me talk oh
yeah I should get to a demo shouldn't I
so well I don't know I might regret
saying that so as I said are the
implementation is something we call a
lime virtual machine the line virtual
machine basically is our abstraction of
a virtual machine of an operating system
and parts of it will run on the CPU and
parts of it will run on each device and
basically the job of the virtual machine
is to take code load it run it and
manage the communication between
different tasks and so this is basically
our whole runtime system this is a lot
of the effort we put in I think I'll
switch to a demo at this point so
actually that n body calculation which I
described has actually been running well
I've been giving the talk so here
running the n-body calculation I'm
actually right now running it on the JVM
and I guess the key number to look at
right now is I'm getting about a Giga
flop a second and if you look at the you
probably can barely discern the motion
of the particles it's going pretty slow
so I can tell the runtime system migrate
the force calculation which in my code
was this compute forces method I can
this is a panel exported by a runtime
system and if we tell the runtime system
migrated to GPU so what our system just
did was take all the state of that
running tasks ship it over to the GPU
connect all the all the cues and dat
emotions so that the data is now going
over to the GPU and back for the force
calculation task and now we're running
on the GPU i'm getting i guess about
five gigaflops now hopefully you can at
least see the data moving of course will
be much better on a machine with a much
better GPU but the point I want to show
you is that the the runtime system is
very dynamic it can move tasks around
because we have isolation things can
move and everything just gets managed
transparently this is all the same code
which happens to be running depending on
where you want to run it if I had an
fpga attached to the system I could move
it there as well so I just want to show
you a demo that you know it's that easy
to test moving things around in to write
code which works across different
systems
so in particular when we're using the
GPU such as the demo I just showed you
the real structure of the virtual
machine looks like this we've got part
of the when I was running on the GPU
just now part of the code was running in
java bytecode we have a/c glue layer
which both talks J&amp;amp;I to the JVM to get
data from there also manages the GPU
device we using the opencl api so data
will get shipped from the JVM to this
see wrapper layer which is then managing
the opencl colonel on the device and
handling all the low level protocols and
things used in opencl so that's what it
looks like for for the GPU you could
imagine similar structures although it
takes a lot more work for the FPGA I
won't get too much into details of
performance I guess I'll just try to say
the bottom line of performance so you
know I think we have a very good story
as far as productivity I think we I
think we make a pretty convincing claim
that you can be much more productive at
doing this kind of programming using our
system the question is how much are you
going to what are the performance
consequences of doing that because using
these devices in the end is all about
performance or power performance so
there are two ways to ask this one
question to answer this question one is
to say if you're a Java programmer and
you start to use the system how much
faster than your original Java are you
going to get that is how effective can
you be in using these accelerators or
perhaps if you're an expert you're the
question one knows how much slower is my
program going to be if I use this system
than if I had done it all by hand by an
expert so we have to evaluate both just
I have numbers I probably go into them
into details the current status you know
we're not trying to build a magical
super compiler our compiler is fairly
sophisticated I think our group at IBM
is very strong in compilers and we'll do
about as well as as we can but you know
on some benchmarks line performs just as
well as a hand coded alternative and
some not we have to worry a lot about
communication computation ratio in
particular so this is what we're working
on all the time I think our performance
story and the GPU right now is pretty
good I won't go into these in too much
detail but there was a paper we
published this year at PLD I programming
language design and implementation
conference and you know for a suite of
GPU benchmarks you know we're able to
get the dramatic speed speed upset other
people
reported on using a GPU using our lime
program for variety benchmarks and at
least for opencl for the GPU on these
benchmarks were within a ballpark of
what you expect of what hand to encode
would do so we think for the GPU for a
pretty large number of codes we have a
pretty good performance story the FPGA
we're still working on on the
performance story I don't have any
numbers to show for basic combinatorial
circuits basically things which don't
involve loops or state I think our
performance although maybe not necessary
aria is as well as you can get in this
effect that we can get single pipe
single cycle pipelining of the circuits
so we can for any tree of combinatorial
logic are our compiler will produce a
pipeline design that can produce one
output per cycle which is as good as you
can get we might not get this we might
use more space but we're getting closer
for complex circuits right now the
compiler is worse about a factor of ten
so we're doing a lot of work on Io
pipelining and we're pretty hopeful that
we're going to be able to get this down
to within the ballpark within within a
factor of two or four of what a hand
tuned of what a expert programmer could
do within the you know the the within
the foreseeable future so and then you
know you'll have to decide whether or
not that the productivity versus
performance trade-off is worth it for
you right now I think these numbers will
just get better over time as our
compiler is mature we also are working
on there's a lot of issues which i won't
go into here as far as supporting
high-level languages features such as
dynamic memory allocation or virtual
method object oriented dispatch or
things like that we're working on ways
to support these up these language
features on the devices we had one paper
recently describing how we're going to
do garbage collection on the FPGA which
is an excellent paper that also was in
the pldi conference so that's definitely
worth checking out if you're into this
kind of design thing so Martinez what's
the summary as I said I think we have a
pretty good story on productivity we
have we've shown that we can have
high-level concise abstractions for
parallelism that mapped to different
devices we've shown I believe or at
least our experience is that the
experience of using this is not much
harder
java and so orders of magnitude better
than the current alternatives um and I
think we have a good chance of
delivering on our goal which was the
general solution for mainstream
programming of these systems now of
course as I said there's no magic you
know we're not going to design a
parallel algorithm for you this is not a
parallelizing compiler you still have to
be able to design your algorithm in a
way that's amenable to parallelism we're
not going to pretend that one version of
the program runs well on all devices you
might have to write different versions
of a method in order to satisfy the
constraints of different devices we're
not trying to run vanilla Java programs
on accelerator you do have to go through
a migration step dialogue with the
compiler this is an internal research
project right now so this is not yet
available to the public i'm not prepared
right now to make any announcements of
this i'm hopeful that this will be
available in one form of the other
relatively soon but for the moment this
is an internal research project we're
piloting it with various groups within
IBM to get experience in heart in the
system and we're hopeful that this
experience will validate our hypothesis
that this is a productive way to go so
I'm done and i'll stop here and answer
any other questions you have if you're
an IBM customer we might be talk to us
we might be willing to do some sort of
one-off first-of-a-kind agreement if it
made sense so we're willing we are doing
this or we're in discussions of this
with several customers there's no reason
why we couldn't do more so we're happy
to engage as as I said though it's not
open public so it has to be done under
the appropriate legal agreements yes
right so for right now both the GPUs so
let's see for the GPUs right now they
plug into PCI things now both Intel and
AMD have integrated GPUs on the same
core so the FPGA is there's a number of
ways to communicate right now yes our
main system is communicating over PCIe
there's also Ethernet is a possibility
we also have for prototyping we have a
UART serial connection so there's many
different ways to talk to FPGAs there
are also vendors that do memory bus
attached to fpga designs either with IBM
or with Intel so there's a number of
ways but yes are our primary platform
right now is PCIe other than the devil
as usual are there any interesting
stories of applications with benchmarks
interesting story as well one thing we
have fun with internally is that it
seems that most of our compelling
applications are evil so we have a lot
of stories from you know applications
from high-speed trade frequency high
frequency trading from Wall Street and
weapons and you know so I guess that's
the story with all sorts of high
performance bleeding-edge technology the
early adopters tend to be people with a
lot of money and power I try to
concentrate on the non evil applications
please talk to classic and right I'm
from offering as to the VJ more
effective this is you think it would be
possible to use this power or three from
pure fiji any projects in this line
language Ian about love yeah so I think
of the scenarios i started at the
beginning of the talk with my personal
opinion is the one that's most
attractive is this is network-attached
and in this case basically in the common
case all the computation will be done in
the fpga and i guess in theory you could
cut the cord and just detach the fpga
but in practice that's usually attached
to a host because you have to boot the
device and load things for that but yes
in in most of these applications the
idea is and you know we've been we've
been doing some of these applications
especially encryption encryptions an
interesting one I'm sorry I'm zipping
gzip gzip is a very interesting
application but the idea here is that
we're doing some filtering directly on
the wire at the speed of the wire
ignoring the whole software stack on the
cpu for very low latency high throughput
now gzip is an interesting application
there are stet I don't know if you're
familiar with the out the algorithms but
there are static and dynamic versions of
the Huffman algorithm and people are
doing fpga compression apps but their
compression algorithms
generally speaking that you do in
software which use dynamic Huffman
schemes are more effective with
compression but they're extremely
difficult to map to hardware because
they use dynamic structures yet this is
the kind of code that in theory well
actually in practice that we can express
in our language and if our compiler is
able to code it is able to implement it
well we can actually use a better
algorithm so I think this is an example
of the possibilities of we're using
higher level abstractions perhaps not
counter-intuitively you can perhaps get
better performance by using a better
algorithm even if the operation by
operation code generation for very low
level stuff is not as good as doing it
by hand the ability to use a smarter
algorithm at a high level may actually
be more productive as far as the bottom
line story so that's one application
which we're pretty pretty pretty happy
with going on in the FPGA and their
number of others</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>