<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GOTO 2016 • Nomad: Scheduling Applications at Scale • Seth Vargo | Coder Coacher - Coaching Coders</title><meta content="GOTO 2016 • Nomad: Scheduling Applications at Scale • Seth Vargo - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GOTO-Conferences/">GOTO Conferences</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GOTO 2016 • Nomad: Scheduling Applications at Scale • Seth Vargo</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hllS4Gutams" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I want to talk a little bit about
history and how we got here and then
where I think we're going and how Nomad
can solve those problems and
specifically how it's different than the
other schedulers we've heard about like
Apache mesas and kubernetes so my name
is south fargo i am the director of
evangelism at haji court i'm an engineer
by trade so I previously worked at a
number of engineering companies you
might recognize my face from the chef
community I've done a lot of work there
and more recently I moved into kind of
this full-time role of evangelizing all
of the Hashi corporate tools how many
people are familiar with how chic or
cool I like when the numbers go up
because that means I'm doing my job for
those of you that aren't familiar with
hashey core we make a number of
open-source tools we're an open source
company so we make vagrant which is a
really popular tool for automated
development environments Packer which is
a tool for building automated machine
images surf and console which is our our
service discovery and distributed
key-value frameworks built on top of
raft and gossip terraform which is our
infrastructure provisioning and
management tool so our infrastructure is
code vault secrets management nomad
which is what I'll talk about today
it's our distributed application
scheduler auto which is our opinionated
take on all of our other tools and Atlas
which is actually not open-source that's
our commercial product but like I said I
want to talk about nomad today so what
is nomad how many people have heard of
nomad cool I don't know if you got this
yet but I like engagement so I might ask
you to raise your hand lots of times
it's okay thanks for bearing with me
so Nomad is a globally distributed
optimistically concurrent scheduler and
we've heard this word a number of times
now scheduler but like we've never
really defined it in the context of
technology so I want to take a step back
and if you look up like the dictionary
comm which is you know the source for
everything
definition of a scheduler it says that
it's a program that arranges jobs or
computers operations into an appropriate
sequence so I thought that was too
specific so I took it a step back even
further and I wanted to get the
definition of a scheduler
in the real world so a scheduler is a
person or machine that organizes or
maintains schedules so that's pretty
pretty standard schedulers are just a
way to map a set of work on to a set of
resources so this is a scheduler this is
Microsoft Excel circa before I was born
and at the very basic level a
spreadsheet is a scheduler it provides a
way for us to map values wrap resources
onto values so I don't know why my
clicker is misbehaving so traditionally
if we take a step back into like the 90s
for a little bit everybody had their own
data center and I used the word air
quotes there because some of the data
centers were like the closet in the
office and you gave all of your servers
names because you had to because you had
to refer to them so in this case I've
chosen star wars-themed
you always picked a theme like parts of
the eye or your favorite movie that way
you could remember them so in this case
we have four servers we have Skywalker
Vader Leia and Solo and you or an
operator would put applications onto
those machines so in this case we have
some Python and we have some go and then
you know Ruby and node.js
take up a lot more memory so they have
to get their own server but you also
have that like you personified these
servers so for example you knew that
that Vader was was not good so you also
had your little spreadsheet you know
your Excel spreadsheet or whether it was
like a little notepad and you taped it
up to the wall and you have the IP
addresses and the MAC addresses and then
that notes column and like in this case
Vader just randomly kills applications
haven't really figured out why don't
really have time to address the issue
but we just know that happens so when a
server goes offline or catches fire the
operator was responsible for
rescheduling that application and/or
fixing the Machine minimizing downtime
so in this case we might reschedule the
Ruby application on to Skywalker we
might reschedule the Python application
that no
application and python application on to
solo you know we're still not at
resource maximization but we don't have
downtime anymore then when that server
is repaired so whether it's replaced or
repaired we have to make sure that we
update the spreadsheet because it's
going to have a new IP address and then
we say like you know rebuilt on you know
420 2016 and then we can start
scheduling applications back on top of
that and this workflow was quite common
and then we later learned that if you
give these operators pagers they become
highly available and you don't have
downtime anymore
so this obviously does not scale it
didn't scale in 1990 it doesn't scale in
today's modern computing infrastructure
but it doesn't scale even worse because
we've moved into the era of cloud we
moved into the era of on-demand
applications we've seen a lot of talks
about like lambda and serverless
architecture so having an operator have
to make a decision about where an
application should live doesn't scale in
today's architecture so let's talk about
schedulers that have been around for a
really long time so I think this was
brought up earlier but a CPU is ask it
like the CPU scheduler a part of your
operating system it's responsibility is
to map particular processes to a CPU so
here we have a four core machine I have
four processes I have the kernel itself
Apache reticent bash and that's all
great but let's pretend I have like a
two core machine so no longer do we have
a one-to-one mapping
instead we have those resources that are
vying for the same core so it's the
schedulers responsibility to prioritize
those so in this case what has the
highest priority anybody want take a
guess the kernel right because if the
kernel is not running neither is
anything else so in that case the kernel
might get a dedicated core and we might
be shifting between the other three
applications on the second core but we
might also move the kernels operations
to the second core at some point because
that's what this scheduler does and the
way it makes that decision is up to that
particular scheduler in the constraints
that's given
so schedulers aren't a new concept
they've been around for quite a long
time so CPU schedulers map threads to so
the work that they have to do is threads
they map them onto physical cores in the
context of like ec2 or open stacks Nova
we're mapping virtual machines the work
is a virtual machine and we're mapping
that onto a hypervisor in the case of
like Hadoop yarn we're mapping jobs big
MapReduce jobs on to a series of nodes
and then more recently where we start to
see the word scheduler pop up it seems
like it's been defined a little bit more
is a cluster scheduler and that's where
we're mapping applications things like
docker containers or a golang library or
a Java jar onto a series of machines or
servers some of the advantages of a
scheduler are very straightforward so
high resource utilization how does a
scheduler accomplish high resource
utilization well within the context of a
cluster scheduler increasing resource
utilization is done using techniques
like bin packing oversubscription and
having a consistent queue for jobs the
second and kind of more important thing
that a scheduler does for us is
decoupling the work from resources so
everybody always talks about higher
resource utilization because it's the
easiest way to sell a scheduler you can
walk into your manager and say we're
gonna cut your cost by 60% and they're
like cool make it happen and they don't
care about the actual technical
trade-offs of doing so what schedulers
have actually enabled us to do is
decouple our work from the resources
themselves so no longer are we operating
system dependent are we particular
versioning dependent instead schedulers
provide us an abstraction of the work
that needs to be done and that
abstraction is important because it's
consistent across any type of job so it
doesn't matter whether you're doing a
large MapReduce job or running a single
static binary it doesn't matter if
you're running a batch job that's
processing some billing information
that's relatively short-lived or if you
have a long-running service like Redis
or MongoDB that needs to be up all of
the time
it also includes certain API contracts
and standardization across them so one
of the great things that docker brought
to the community is standardization
around containers providing a consistent
and reliable API wrapper around
containers and schedulers are doing the
same thing around applications right
they're providing that standard API
around how to deploy and manage an
application at scale and lastly and and
somewhat intangibly is better quality of
service and this is something that's
really hard to measure
everybody talks about 9s how many 9s do
you have how many nines do you have
right but there are intrinsic things
that we can't measure with quality of
service like you know the 99th
percentile latency or how does your
application behave for a user in like
sub-saharan Africa who has 56 kilobytes
per second can they access your
application how do you measure that and
schedulers provide us with a better
quality of service coupled with things
like CD ends where we can scale out
globally to make sure that our
applications are accessible all across
the world we do this by providing job
priorities so earlier I gave the example
of Redis versus some billing
architecture one is a batch job that
runs for maybe 15 minutes one is a
long-running process that's critical to
the application without priorities a
scheduler can't make the decision of
which is more important so it'll just
randomly choose or do like whichever one
was submitted first and that's obviously
not ideal because as an operator or an
engineer you're aware that the MapReduce
job that's doing all of the billing
could probably wait a day or an hour or
even 10 minutes if it means that the
Redis job or the MongoDB job can
continue running because the application
the customer-facing application or the
business facing application that is
critical has to be up and running and so
do all of its dependencies so if a
scheduler doesn't have the ability to
specify priority and say a a is more
important than B then those decisions
can't be made adequately and you can't
provide a better quality of service the
second is resource isolation so you
don't want your applications clobbering
on top of one another and this was a
problem that was experienced a lot in
the past 20 years it's the reason why
tools like chef
puppet and ansible and salt are so
popular or because we had to install
like 20 different applications onto a
machine and without containers or docker
containers or container is Asian
we had multiple Ruby versions multiple
Python versions managing all of that was
quite a nightmare and that's where
configuration management really became
popular because managing that by hand
was difficult schedulers and containers
and VMs provide us with that level of
resource isolation so we don't have to
worry about you know Ruby 192
conflicting or through B 193 and is our
application using the correct version is
our Ruby and conflicting with the system
version right these are so many
questions that have come up and
schedulers are really solving those
problems for us schedulers also aren't a
new concept I think they're a new
concept to what I call mid tier
organizations but they're not a new
concept to large scale organizations so
we already talked about like ec2 and
Nova being a scheduler it's a VM
scheduler but Amazon also has ECS which
is their container scheduler google has
borg and then some of the research
google did also for sparrow that our
berkeley did for sparrow as well and
then twitter and netflix netflix has
tightened twitter has their own
scheduler then these have been around
for quite a long time they're more
recently being evangelized and
popularized because they're becoming
more relevant as we move to cloud-based
architecture serverless architecture and
we start focusing on applications
instead of machines themselves so when
we talk about nomad and this might be a
little bit redundant but nomads also not
alone in the scheduler market so you
have kubernetes meso as meso sphere
container ship at i/o and a number of
other schedulers are continuously being
created the primary difference is Adrian
said between nomad and all of these
other schedulers is that Nomad is a
scheduler Noma doesn't provide you with
service discovery it doesn't provide you
with all of the bits and pieces so the
reason Nomad is is really popular is
gaining in popularity is that it's
really easy to adopt into your existing
organization without taking an entire
bite of a particular pie so you can
adopt Nomad and continue using your
service discovery tool whether that's
console which happens to be something
that we make or something like zookeeper
the integrations are there for it so you
can adopt purely a scheduler avoiding
that vendor lock-in and avoiding all of
the refactoring that might need to occur
to bring your applications into the
modern era another key difference
between nomads and these other tools are
the types of applications that can be
scheduled and I'll get into that a
little bit more so there are three main
points of nomad that I want to discuss
the first is the cluster scheduling part
the second is the deployments and then
the job specification itself so this is
the job specification and we think this
is what sets Nomad apart from a lot of
the competitors we have a very human
friendly configuration it's JSON
compatible so you can generate this as
JSON and the Nomad parser will actually
read it as JSON so you can still do
machine readable and machine output but
we have a very friendly config that
looks a lot like nginx if you're
familiar with it key value pairs very
human friendly support for comments very
version control friendly and they're
designed to be checked into version
control because your job files are
really the description of your
architecture in this example we have a
job named Redis at the data center that
we want to run Redis in and the task
specifies how we want to run it where we
want to get it from what driver we want
to use and the set of resources
optionally that are required so in this
case we're specifically saying that this
docker container which is just pulling
down the latest reddit Redis image needs
1/2 of a CPU and 256 megabytes of RAM
with a minimum network of 10 megabits
per second the job specification
declares what to run and then nomad's
responsibility is to determine how and
where to run it
so in this way Nomad is responsible for
abstracting the work from the resources
themselves
so one of the really interesting things
about nomad is the design and the
architecture because it's similar but
also very different from some of the
other schedulers out there so
understanding the goals and constraints
of a problem are super important when we
consider the design at Hashi curve we're
a academic white paper focused company
we have ideas and then whenever we're
ready to build something we go out and
research all of the latest and greatest
academic papers scholars who are writing
about these particular topics for nomad
we had a number of requirements and
these included support from multiple
data centers and multiple regional
deployments we wanted the flexibility to
support any type of workload so not just
docker containers not just VMs we
already talked about why job priorities
were so important we wanted to use the
latest and greatest bin packing
algorithms and it needed to be able to
support large massive scale twitter
scale Google scale Facebook scale but it
also needed to be operationally simple
such that you could get it up and
running in one or two commands when we
talked about scaling I wanted like
better define scaling we want nomads to
be able to scale to thousands of regions
with tens of thousands of clients per
region with thousands of jobs per client
and I'll talk about the million
container challenge in a little bit and
how nomads performed but if you do the
math that's a million jobs in designing
nomads we turn to our own experience
first
so Hashi court makes surfing consul
which are distributed systems which run
at scale in production today surf gives
us cluster membership gossip based
peer-to-peer membership detection
failure detection without a single point
of failure or TTLs
and it provides an event-based
distribution system it works at massive
scale tens of thousands of machines in a
single data center its production
hardened for a number of years now used
by some of the largest companies on the
internet and it's operationally simple
console is built on top of surf and it's
our service discovery framework it
allows for a distributed key-value store
it also up and also includes a number of
features that are important in a
distributed system such as a centralized
locking mechanism and it uses an
architecture for centralized servers and
distributed clients
console itself is multi datacenter aware
it uses raft for consensus it also has
been tested at incredibly large scale
and is very production hardened so surf
and console provide us the gossip and
consensus layers and there are also very
mature libraries with powerful design
patterns the problem is that they all
lacked scheduling logic and in Hoshi
Corp we believe in the UNIX philosophy
which is you should do one thing and you
should do it very well and it doesn't
make sense for a service discovery
framework to be responsible for
scheduling just like it doesn't make
sense for a scheduler to be a service
discovery framework so for this we turn
to academia we wanted cutting-edge
technology we wanted cutting-edge
research in the field of scheduling and
we primarily were guided by work done by
Google and amp lab at Berkeley so a
number of people have talked about Borg
already but Borg is Google's internal
scheduler I think part of Borg also runs
Google cloud platform and then Berkeley
Sparrow was coming out of Berkeley which
was a similar scheduler so doing a
survey of the the current academia at
the time schedulers are either
optimistic or pessimistic based on their
concurrency and they choose whether they
want to manage state internally or if
they want to push it off to something
like sed or some other service and then
they're either single or multi level so
Borg is fixed in its scheduling logic so
like the schedulers built into Borg
whereas Omega has a pluggable
architecture so you can bring your own
scheduler some schedulers are highly
optimized for batch workloads so billing
processing one-off jobs they like five
ten twenty minute jobs whereas others
are more optimized for long-lived
services Redis database java application
etc so ultimately nomad is more inspired
by Omega than it is sparrow it's
optimistically concurrent which allows
it to make scheduling decisions in
parallel across many of the servers it
uses internal state for coordination
both for speed and to make it more
operationally simple we didn't want the
first step out of the box to be go
install this other tool so that we can
store state we wanted us to be
operationally simple out of the box we
focus on both service and bachelor
clothes
and we're going to support a pluggable
architecture for drivers very shortly in
starting our system design we turn to
console console is very familiar we knew
it scaled this is consoles architecture
at a very high level so up on the top
you can see all of the clients the
clients communicate with each other over
RPC on the LAN gossip they communicate
with servers those servers are
responsible for three things they're
responsible for responding to RPC
requests they're responsible for leader
election and then they're responsible
for forwarding requests to the leader if
in the event they are a follower within
a data center the nodes talk to their
local servers but the server's also
participate in the win gossip or the
public internet gossip
so across multiple data centers the
servers are communicating with each
other over the public Internet or some
resemblance of the public Internet in
this architecture the failure is
isolated the failure domain is isolated
to the data center so your failure
domain is the data center itself and
this was because each data center is
completely isolated from its peer
knowmads architecture is very similar
but we've introduced this concept of a
region so a region is a collection of
data centers and has only a single set
of servers so whereas in console each
data center has a collection of servers
in nomads each region has a collection
of servers and each region is a
collection of data centers so as an
example you may have you know Frankfurt
Frankfurt Ireland and I don't know
Scandinavia as your three data centers
right those are where your data centers
are physically located that region might
be Europe so region is just a higher
level abstraction of a data center and
the reason for that is that scheduling
decisions typically aren't made at the
data center layer they're made at the
regional layer so the way I like to
describe it is that I like to think of
data centers as countries and regions or
continents that obviously doesn't map
one to one but it gives you a good
representation in your mind in the multi
region architecture the region is the
failure domain and the server's within
that region communicate with each other
so if the United States region goes down
the failure domain is isolated to that
continent the European region
is unaffected so the region is the
failure domain this allows for enter
more data centers per region giving the
user the flexibility to run a single
region per data center or hundreds of
thousands of regions or hundreds of
thousands of data centers per region as
if the world was one global region the
region is also the scheduling boundary
so you can't schedule a job across
multiple regions in one submission you
can schedule it but they're isolated
jobs they're independent of one another
otherwise that would break the failure
isolation so next it's important to talk
about
nomads data model and this is critical
for understanding how the system
actually operates
so users submit work in the form of a
job or a job file and that can either be
done via the API so you can build your
own tooling around it or you can use the
command line directly the client nodes
are joined to run the tasks and then the
mapping of the tasks from a job to a
node is done with what's called an
allocation that's a glossary term and
these allocations are only created by
evaluations an evaluation is basically a
state change in the system so some
example evaluations are like I added a
new client node a node went down I've
deleted a job I've created a job or an
allocation failed meaning a previously
successful allocation is now failing and
that might be like let's just say
somebody ripped a hard drive out of a
physical data center physical data
center node and now you have less disk
space that you originally did that
causes a reallocation to occur so in
this way if we want to get technical a
scheduler can be modeled in functional
notation as a function that takes an
evaluation and returns a set of
allocation updates or an array of
allocation updates and those allocation
updates are either created updated or
deleted and that evaluation itself can
be a specialized system so it can be a
service job a batch job a system job
whatever it might be in this way it
allows our architecture to be pluggable
so this means this makes nomad an
omega-class scheduler it allows it to
support the pluggable schedulers with
custom logic but makes use of the
internal coordination and state system
the heavily inspired
council architecture allows it to
support multiple regions and multiple
data centers very easily and at massive
scale in terms of the client no mat is
written in go and it relies on clients
to actually run tasks the client
supports a broad range of operating
systems including Windows Linux FreeBSD
and Darwin the clients make use of
fingerprinting which if you're familiar
with like chef or puppet it's like very
similar to like Ojai or Factor
collecting data about the system that's
used in the scheduling decisions and
pushes that information up to the Nomad
servers so here are some examples of the
fingerprinting so for example the
operating system kernel OS version
Hardware how much CPU do we have both
total and available memory disk the
capabilities of the system itself is
darker installed is Java installed is
console installed and then the
environment itself is a ton Ibis is it
on GCE is it on OpenStack etc the
information from the fingerprints is
provided to the servers to constrain
task placements and support bin packing
so for example in human terms you would
say this task requires Linux and then as
a result implicitly docker and PCI
compliant hardware and all of that is
expressed as a constraint in the job
file it's codified it's in version
control when you say that my task needs
half a gig of ram and exactly one CPU
core that's expressed as a resource ask
in the job file again codified in
version control you get the same type of
collaboration that you have on
applications on infrastructure changes
tasks drivers live on the client and
they're responsible for executing tasks
and providing resource isolation so
right now Nomad has support for docker
and rocket on the containerization level
but one of the things that sets Nomad
apart from some of the other schedulers
on the market is that it supports more
than just docker we support virtualized
workloads with kimu and KVM and we
support standalone static binaries such
as Java jars or go binaries so when your
only requirement is you need the JVM or
you need something that's capable of
running an elf binary nomad can schedule
that directly for you without the
overhead of darker without the need to
install docker on the system free
additional resources to maximize
utilization we are also actively working
on support for Windows Server containers
Xen and hyper-v and c-sharp standalone
applications so these are some of the
things that you might see in the
pipeline ultimately Nomad is designed to
be incredibly flexible and it does so
with the pluggable architecture
fingerprinting drivers and the job
specification format it's a single
binary like almost all hashing core
products so you curl you move it and
user local bin and you give it
executable and you have installed it
it's usually four lines of bash code or
100 lines of puppet and there's no
dependencies absolutely no system level
dependencies and it's highly available
so one of the last things I want to talk
about and I hinted at it earlier is the
Nomad million container challenge how
many people have seen the million
container challenge cool one person so
we recently did a thing where we
scheduled a thousand jobs with a
thousand tasks per job on 5,000 hosts on
Google compute engine and they
graciously provided that for free so if
you do the math there that's one million
they happen to be docker containers
running across 5,000 hosts on Google
cloud platform and we added constraints
to all of the jobs some of them
arbitrary just to make scheduling more
difficult the hosts themselves all had
different resources so some were like to
mega our two gigabytes of RAM somewhere
64 gigabytes of RAM just to make the
scheduling as hard as possible so the
results were pretty amazing as you can
see from this graph nomads scheduled
over a million containers talk about why
over a million in a second in just under
five minutes so if you can't see that
that's two hundred and sixty seven
seconds is when all of the containers
were scheduled what's interesting to
note is that Nomad actually scheduled
more than a million containers this is
due to at that scale you have like
actual hardware failure like machines
literally died we also found a bugger to
in the darker daemon that have since
been fixed that at that massive scale
caused race conditions when Nomad
detects that a job is no longer running
it reschedules it that's one of the
responsibilities of a scheduler so this
is not only showing the massive scale at
which nomads can operate but also at
that scale it's still capable of making
fundamental fed scheduling decisions
like is this job still running no okay
I'm going to start it again
so Negative Nancy always says no one
would ever need to schedule a million
containers like someone was thinking it
who was thinking it yeah someone was
thinking it so I'm about to show you a
video and this is a video of a stress
test performed on an Airbus a320 so
that's an airplane as you can see the
wings Bend significantly further than
any traditional flight would ever
encounter
however there's pretty much added safety
and confidence and knowing that this
airplanes wings can bend to I think it's
like 39 degrees or something beyond
their parallel right that's pretty
awesome I'm gonna show you another video
now so this is a video of another type
of plane maybe there we go
this is a another type of plane which
snaps the wing at about a 17 degree
incline so I ask you this both of these
scenarios are highly unlikely but which
plane would you rather fly in so do you
need to schedule a million applications
no but which scheduler would you rather
use the one that can only schedule a
thousand or the one that can schedule a
million at scale and be highly
performant as with everything we do
hashey Corp is Nomad is open-source and
so is the contain the million container
challenge so hi should Corp slash c1m on
github is the million container
challenge all of the scripts are
open-source you're free to run this
against Nomad or any other scheduler not
just Nomad so to conclude no Mehta is a
globally distributed optimistically
concurrent scheduler it uses the goal
was ultimately to allow users to
dramatically increase their resource
utilization
d couple applications from resources or
work from resources to improve their
overall quality of service so that's all
I have I do have a really quick demo but
I don't know if I have time oh you do
you do have time ok look at that I was
hoping he said no thanks Adrienne cool
so what I have here is a really small
font size so what I have here is a nomad
server that's running somewhere on a
cloud somewhere so I have this job
that's running code web app and I'm not
going to dive into the job file but it's
just a docker container there's two
docker containers one's an H a proxy
load balancer and one's an Apache -
serving up a static web page and it is
currently running if I ask for more
information so we can see that we can
see that I screwed this up a bunch of
times cuz you get the whole job history
but you can see that it's currently
running on I scrolled too far it's
apparently running on this particular
node here so node whatever DC whatever
and I'm intentionally doing this without
service discovery so if there were
console installed on these machines it
would automatically discover them for us
but I'm doing this like pure nomad so
what I can do is I can ask for the
status of that node by its ID so I can
see everything that nomad knows about
this node so it currently has what 750
megahertz of the CPU is is currently
allocated 512 megabytes of memory 600
megabytes of disk and there are 0 IO
operations currently taking place so
what I want to show you is so this is
running on 1000 11 this is where I have
to cheat and log in the AWS console to
get the IP address of this node what I'm
gonna do is I'm gonna kill this node and
I'm gonna show you that when this node
goes down nobody is going to detect that
and reschedule it on another machine so
what did I say this was 111
all right was it Tina
I don't know thought 8 yeah see I'm not
a sysadmin okay so that note is now down
and it's shutting down and as soon as
Nomad is made aware of that nodes
departure from the cluster and this was
a docker container so we also have to
wait for docker to report that the the
service is no longer running and
unfortunately that's a pull based model
now push based model right now so we do
have to wait for the node to leave the
cluster once the node leaves the cluster
nomads gonna automatically schedule that
job on another machine and it's going to
do that using the same bin packing it's
gonna do another allocation so when we
run the status again we'll see another
allocation take place which will form
another evaluation and hopefully it'll
schedule it on another machine so let's
take a look at this job status ask for
the total status so it says the job is
running so let's ask for the status of
the webapp job so here I don't know if
you remember but 9dc8 20 was the
previously running node that was the
node that we killed now it happens to be
running on a node 3 7 2 4 so for example
if I run a nomad node status with no
arguments you can see that that happens
to correspond to node 57 doesn't really
have a name or anything special so it's
this last node in the list you can see
that nobody's reporting our original
node is down so it's no longer
accessible and this application has been
rescheduled on to that particular
machine in this case 57 so if we hit up
57 in the and again I'm doing this
without service discovery but if there
was service discovery you would be able
to use you know DNS or some type of
interfacing to query this but this is
actually that application itself so this
is that application that got rescheduled
on to this host 2 or 3 docker container
is running behind a load balancer so
nothing really special I don't have a
really cool demo but I think it's super
cool to showcase what happens when an
application dies and a scheduler is
responsible for May
be sure that it's still available to the
client that's all I have any questions
there are questions yes crap so there's
a question here about Windows support
okay
you mentioned that all your different
applications run on Windows right but do
you have any examples of it running
companies running on Windows in
production and at large-scale definitely
not at large scale for nomad we do have
a couple companies who are running some
small skill stuff on Windows the the
nomad client runs on Windows but the
nomad server has to run on a Linux based
operating system right now that's
something that we're trying to get rid
of so we can schedule work on to a
Windows machine but right now it's not
possible to run the server on Windows
there's some UNIX isms deep down on the
core that we have to remove before we
can pour that to Windows I don't have
any examples of companies that are that
are doing it that I can really talk
about but we do have an examples repo
it's open source on github where you can
see some of the job files and things
that are like Windows Windows specific
ok so do you see a future for
configuration management like chef and
puppet I do I think that chef puppet
ansible salt I think they still play a
very vital role in this space I think
that but a different role yeah I think
they're just gonna play a very different
role I think as we move towards
containerization this notion of having
multiple applications on a system
without some type of resource isolation
from either a scheduler or a container
engine goes away so things like chef and
puppet become crucial for provisioning
the initial systems building like these
larger cluster machines the clients
themselves that have you know gigabytes
of memory gigabytes of RAM
you know CPU disk space and provisioning
those initial instances I don't think
we're gonna see them as popular in like
the cron based or you know you run chef
on some interval or puppet on some
interval I think it's gonna be purely
more of a bootstrapping process and then
we're probably going to rely on the
schedulers or the container engines to
make the decisions from there okay so
you mentioned the concept bin packing
a couple of times what exactly does that
mean there are three people who ask this
jeez okay so the easiest way and I'm
gonna steal Kelsey Hightower's
description the easiest way to describe
in packing is Tetris so how many people
have ever played Tetris right so that's
been packing next question okay yeah so
the idea is that in bin packing you you
reserve effectively large chunks of
space such that you don't you don't
always pick like the best solution in
Tetris you don't pick like the piece
that exactly fits instead you pick the
piece that forms another shape that you
know might be coming in the future so
you might form a gap that perfectly fits
the T shape even though you don't have a
t-shape on your map it's not the next
one then when the T shape becomes
available you can clear three rows out
instead of just one so that's kind of a
the metaphor I like to use there good
metaphor so do you do you have any sort
of visualization of the cluster like we
saw with DCOs previously so we do have a
visualization tool it's currently in
beta it is not going to be open source
first we're going to have an open source
UI in the future but a part of our
commercial offering will have a UI but I
don't I don't want to talk about it
because it's paid okay
what are you guys using no map for at
Hoshi core right now so we have more
than guys we have female employees too
so we are we're running our entire
production infrastructure on nomad so we
we believe in dogfooding for those of
you haven't earned that term it's like
eating your own products and so
internally we use all of our own
products we use console for service
discovery Nomad runs our production and
staging environments it also runs our
on-premise tool so if you buy on Prem we
actually install nomad on your
infrastructure and that's what runs the
commercial product cool thank you that's
all the questions coming hand
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>