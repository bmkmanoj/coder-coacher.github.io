<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hannes Payer: Speed is awesome, but low latency is sublime — JSConf EU 2013 | Coder Coacher - Coaching Coders</title><meta content="Hannes Payer: Speed is awesome, but low latency is sublime — JSConf EU 2013 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Hannes Payer: Speed is awesome, but low latency is sublime — JSConf EU 2013</b></h2><h5 class="post__date">2013-10-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3vPOlGRH6zk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you
but come to my talk about
is awesome but low latency is sublime my
name is hannah's i'm working for google
in munich and I member of TV a team how
many people in here know what v8 is yeah
quite some you know right soviet is for
people who don't know the JavaScript
engine which is for example used in
chrome and is the thingy that
understands or at least tries to
understand your JavaScript program and
execute it right so what is latency and
why is it important what we understand
by latency is the time DVM in our case
v8 interrupts your JavaScript program
and has to do some work right and this
work is mainly garbage collector work so
the DVM has to when it runs out of
memory it has to go through the
allocated memory and figure out which
memory is life and which one is dead and
free the dead memories so that the
JavaScript application can allocate more
memory and this process can also come
from the optimizing compiler so once in
a while we decide that one function
could run waste faster this is the time
when the optimizing compiler kicks in
optimizes the code and hopefully the
code runs faster and better after that
so these are all pauses which stop the
chava script application where vm stuff
happens and after that the travel script
applications is resumed and we call it
high latency right when these pauses
alone and low latency when this boss is
a short right and why do we want low
latency because it really matters shank
is something which is really visible to
the user right probably people
experience that in here right if you
play your games or watch a video or an
audio and they are like these chunks
this chitters in between it's really not
a nice user experience right and its
really annoying right nobody likes that
and Chang's gets worse and mobile
platforms right mobile platforms are
slower like the a3 of factor of three or
four slower than a beefy lab
top or a beefy desktop machine right so
in mobile the change is even worse right
so as a vm engineer we like performance
right we'd like to tune performance but
latency is something which is sometimes
really on trade off with Rupert right
you can get better latency but you pay
in throughput for that so what we are
trying to do is to keep this interrupt
short but short we mean few milliseconds
whatever that means rights basically
around would be great to have pauses
which are not longer than let's say five
milliseconds of below one millisecond
the shorter the better right the better
user experience obviously and it would
be nice to have predictable pauses right
so it's like a real kind of moving in
this soft real-time ballgame right where
the developer may rely on some soft
guarantees we give him right and that
that would be nice it's not the case
these days right but that would be the
optimum outcome right that the vm has
some promises for the developer and for
example it deposit ends are never larger
them and milliseconds or whatever so
short overview of my talk so latency is
an issue and we recently like start
focusing in it and improving it and I'll
give a shorter with you about our
garbage collector and now optimizing
compiler few basic how this stuff worked
and the efforts we are taking their to
keep Layton's low then I will show to
latency benchmarks we're using to
evaluate our optimizations show them in
experiments how we improved over the
last year and draw some conclusions and
there should be time for Q&amp;amp;A maybe
otherwise we talk offline after the talk
so in v8 we have a generational garbage
collector this project Lou is used by
many bm's we have a relatively small new
generation and they have use a semi
space heap in a scavenger semi space
means that we just use half of the
memory so when we allocate stuff there
that's okay the few objects and we run
out of memory there now
semi space the scavenger looks for the
object is what the scavenger is doing
it's basically walking throughout the
life objects and as soon as it
encounters the life objects it copies it
over to the other half of the SME space
let's say two objects survive here in
our example there is a dead right so the
scavenger is pretty fast then there is a
no generation and this is typically the
memory where objects end up dead
survived for quite some time right and
they ever use a mark and sweep garbage
collect the mark and sweep means we
calculate the transitive closure of our
live objects go through this graph and
mark all the objects we find that their
life and after that when we computed the
whole transitive closure we scan over
the whole heap and free not marked stuff
and for marked objects we just clear the
markup it and go on so you see
scavengers can be fast right the heap is
smaller and they are fast when a lot of
objects died in the young generation
immediately and the old generation is
big can be really big and garbage
collections can take longer there right
so let's fill up the semi space again
and then let's say object survived well
they eventually end up in the old
generation okay so the main assumption
between the generational garbage
collector is that most of the objects
that get allocated die young right they
are l research out there that shows like
for many many application that is true
right and this already gives us some
nice latency properties right this
results in many short young generation
collections which are shown you by s or
I just saw this lights are not really IC
shown all right now and once in a while
when when many objects a wife they end
up in the old generation and we have
this long are marking three times one
roofer should fix the slides here okay
now that's gone I apologize for that it
looks nice on the green screen here and
there
sorry yeah so we have once in a while
this long atomic markings report well
let's try to get again a full screen
mode maybe it gets better
no not really
step better oh okay that looks better
sorry for the interval that was high
latency by the way so to get rid of
these longer mark and sweet pauses what
we implemented there is a technique
called incremental marking that means
when we do mark and sweep we don't do it
all the work at once before we run out
of memory we already do some marking
work in advance to smaller marking steps
until a point we decide that oh we
probably have marked now enough and then
do a final mark and sweep operation so
it looks like that we smear over the
whole JavaScript program execution
smaller markings that which are really
really short right they are shorter than
a millisecond and then we do a final
mark and sweep operation this improved
latency significantly and user
experience right this long marking face
is now split up into many short ones and
a big final marking face so how we can
how can we do better there how we can we
shrink the size of these final mark and
sweep face well we can use multiple
frets rights to for example the speed up
the sweeping and what we implement that
and editor we ate about half a year ago
is a technique called parallel and
concurrent sweeping parallel sweeping
means that there are multiple threats
helping out to free memory so each fret
gets like one megabyte of the old
generation heap and go through these
megabyte and finds all the free objects
and gives it back to the main
applications so you can have multiple
threats doing that to get a speed up by
the number of frets and concurrent
sweeping means that we can also run this
sweeping phrase concurrently to the
JavaScript application which means that
this whipping phase can be completely
transparent and invisible to the to the
application so sweeping phase kind of
goes completely away and the execution
time of this final mark and sweep phase
decreased even more so there is a
trade-off right we have multiple threats
which means higher system utilization
and but we get wet latency out of it and
one really as today
earlier that right if you had too many
frets and increased performance too much
it can be an issue again on mobile which
means system utilization typically
translate into battery life right so
then let's tackle next the scavenging
times right as I said scavenging
typically so the new generation
collection doesn't take much time
because of the main assumption that most
of the objects die young but what if all
the object survive or not all of most of
the objects survive right what then well
then scavenging is really really slow
because as you remember when you a
locate object and all alive you have to
copy them all over in the semi space and
thats get gets even worse right let's
say they survive even longer well then
you have to copy them over to the old
generation so we had to do two copies in
that case right and this is really the
the worst case for for the generational
garbage collector so how can we do
better there and what we did is we
implemented a technique called global
preteen ring which sets kind of a global
flag in the vm when we encounter that
more than ninety percent of the objects
survive and go from the new generation
to the old generation we flip a switch
and currently what we do is we have to
dip demise all the generated code and
when we generate fast code again we
generated with a location code that
allocates directly into the old
generation and when we so what's going
on is we allocate directly there we have
white the copying and when you find out
that ok most of the objects now die in
the old generation for whatever reason
right the application behavior may
change well then we have to do p again
and generate code again where everything
gets allocated in a new generation it's
kind of a global thing right now but we
are trying to get this Pretender ring on
a allocation side basis so on a local
basis so that we can decide for each
allocation call in JavaScript well
should you
yeah located in the new generation or
would you better be in the old
generation right alright so what happens
there is for like let's say we have this
worst-case application where a lot of
stuff survives and we turn on the skill
while pretending we have wet the copying
and we can avoid these cabbages at all
right because stuff immediately gets
allocated in the old generation right
that helped to improve latency
significantly for this worst-case
behavior applications all right so much
for the garbage collector let's have a
quick look at the our compiler as I
mention in the beginning the compiler
may also introduce some chanc right what
we have in v8 is a we call the full code
generator which is really really fast it
is able to generate a good code i would
say really fast and then there is the
optimizing compiler and the optimizing
compiler is used when we encounter that
a function is executed many times then
we say it's a hot function and we should
generate faster code for this function
right and when we find that out the
travel sleep Fred next time and it
executes the fred is doing the actual
compilation right so it goes into vm is
doing all the optimizations and this may
take some time this actually depends on
the code size right that gets compiled
so you will have these vm passes but we
saw before for the garbage collector
also for a compiler and what we are
doing there well quite similar to the
garbage collector before we use
mechanism called concurrent compilation
so we use an extra fret that helps us
out with doing compilation work right
and we're trying to make more and more
phases of the compiler concurrent which
is quite tricky because the new
compilation time you can allocate object
or them the application itself may
mutate objects which are kind of hooked
up with the function we are now
compiling so we try to get more and more
phases
of the optimizing compiler to make them
concurrent and run in parallel with the
JavaScript application the result there
is of course we use compilation time
right if you can stuff to do concurrent
and a nice benefit of that is when we do
compile stuff concurrently to the
mutator we can actually do more
aggressive optimizations right if
there's time left and in the experiments
show that there's always enough time
left to do to do the compilation the
extra fret we can do really heavy
optimizations and generate even faster
code which was not possible before
because the post times would just be too
long if we would do it all on the same
JavaScript fret so all right now we
covered quite some stuff in the vm that
improved latency of the garbage collect
and the optimizing compiler now let's
see how we can measure that stuff so i'm
now going to explain how we measure the
stuff within the JavaScript application
of course we could look at the vm output
right we have in v8 we have several
flags where we can ask DVM how long it
took but we do we don't trust our output
right so we really want to have the
application tell us whether it was a
good chunk experience or not right and
the way we do that is we take a
JavaScript program and add time probes
to the application right so let's say we
have a function that's allocating stuff
and doing some calculations before and
after the function we measure the time
and we execute this function many many
times right so in the best case this
function should be always fast and it
should we always take the same amount of
time right except when DVM has to do
some stuff like garbage collection or
compilation then you may have outliers
right so the samples which we measure
may have different time spans and then
when we run this function many many
times we calculate the root mean square
of this sample times and this function
the root mean square
heavily penalized outliers there right
so this is what the square the formula
is taking care of when you have a huge
sample then you get penalized for that
right which translates into a bad user
experience right the first you can
execute the samples and the more
predictable they are the better the
score will be so the benchmarks I'm
using my experience here experiments
here to benchmarks from our octane
benchmark suit this is our JavaScript
benchmark suit where we measure
performance of v8 the first one is
called splay this benchmarks creates a
large splay tree and then modifies it
and in this benchmark like almost all
objects survive so this is kind of
divorce case benchmark for a
generational garbage collector right so
when we want to prove this worst-case
scenario dislike the worst-case post
times we can observe and we're trying to
make the better and for the compiler we
look at them and real benchmark this one
is also in the contain benchmarks field
it runs the 3d bullet entry imported
from C++ to JavaScript using module and
the thing there is there like huge code
chunks it's like thousands of lines of
JavaScript code and when the optimizing
compiler or the full kochan looks at the
functions compilation times may be
really really long so all right so this
is what we did then let's look at the
results the results both experiments run
on a nexus 10 on a mobile device it's
quite a beefy mobile device but I was
lying on my desk and with Android 403
let's look it's play first so this is
the garbage collection benchmark on the
x-axis you see the timeline starting in
September last year until today and on
the y-axis is the score so higher is
better so we take the root-mean-square
and just reverse to be higher this
better and all these features i
mentioned before except for the
incremental marking where
ship within the last year and one can
see that we we doubled our our score on
this benchmark which is quite nice if we
look at the specific samples what we do
here is like we do if you split three
modifications and then we measure a time
then go on do again like a predefined
amount of splay tree modifications on
average it takes 20 to 1 milliseconds
but they're like some maximum outliers
at the very beginning of the benchmark
they go up to 46 milliseconds which is
bad right so we can do better there we
investigate and what we can do were
there because 46 milliseconds means that
there will be lost frames right and this
is what you can also see when you
started it with a visualization see a
few chunks at the very beginning but
then it runs smooth right is like three
outliers we get in the beginning there
the minimum sample time is below 1
millisecond it's quite nice so form
unreal we hook the measurement up with
the frame rendering time so there's like
this time there is like the rendering it
sells going on and also the vm time on
average it takes 90 milliseconds to
render such a frame which includes like
everything right there's also the
computation work is also quite heavy
there um but there's a huge outlier in
the beginning right 249 milliseconds
just like one outlier and then they are
few around 40 milliseconds if I remember
correctly and after that it runs smooth
this is a time the first outlier is
related to when the full coach and
compiles the whole javascript file right
so again you may get some chunk at the
beginning when you execute the
application but after that you define
but again there's there stuff where we
can do better all right that already
brings me to the end of the talk low
latency of the vm i guess is key to keep
the user happy and make web application
run well on all devices right not just
on a macbook oh and a heavy desktop
machine but also on on mobile mobile
yeah it would be nice if javascript gems
could provide predictable performance
and give a promise to the application
developers we're not quite there yet but
I think that this is the direction we
should head because many programmers try
to work around to vm right like okay did
you see make a kin so let's build our
own memory management system within our
JavaScript application and this is
actually hard to do right the object
pooling and stuff like that and the
probability is high that one gets just
wrong right and you end up with day with
the same performance problems or it
could be even worse right we already
have some algorithms in v8 that
eliminate allocations right so we
recently landed an algorithm called
which is doing escape analysis right it
finds out whether an allocation escapes
a function and if it's not then it's not
actually allocated on the heap or other
allocation folding which takes multiple
allocations and folds them together into
enter one right so we're trying to to a
whitening get better there and the
latency benchmarks are showed while
they're quite important for us to keep
track of performance improvement and
regressions and they really help us to
show where where we can do better and
help us to evaluate new features all
right that's the end of my talk thank
you very much maybe there is time for
questions I don't know how much time is
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>