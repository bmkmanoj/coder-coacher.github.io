<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Simon Swain: Data-flow Processing for Node.JS with Straw - JSConf.Asia 2013 | Coder Coacher - Coaching Coders</title><meta content="Simon Swain: Data-flow Processing for Node.JS with Straw - JSConf.Asia 2013 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Simon Swain: Data-flow Processing for Node.JS with Straw - JSConf.Asia 2013</b></h2><h5 class="post__date">2013-12-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Q0iBoqhUVck" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">check check check check 12 JavaScript
brothers and sisters can you hear me out
there awesome straw stream processing
for nodejs so in the concept of stream
processing we have a topology of nodes
each node receive some input does some
processing passes on the result as it
turns out this is a really fundamental
concept in computing the original
definition of computer was one who
computes typically a semi skilled
individual who could only perform some
basic arithmetic so when a scientist had
a problem they wanted to solve that
would set up a room full of these people
they break their problem down to the
small pieces give each person a small
step of the problem they do their work
pass the result on to the next person in
the next desk so this is used to solve
some fairly significant for the time
problems they catalog tens of thousands
of stars within the 1700s they
successfully predicted the return of
Halley's Comet classes in modern times
during the Great Depression 450
out-of-work clerks and mathematicians
were put to use on the mathematical
tables project this stupendous project
produced 28 volumes of trig table trig
log tables mathematical formulas all
sorts of computational resource this
provided a really powerful springboard
to the United States after the end of
the Second World War meanwhile during
the war human computers were
instrumental in the ally defeat over
Nazi Germany at Bletchley Park in the
United Kingdom the codebreakers there
used human computing to work on encoded
German messages they set it up as a
series of huts each Hut would be
populated by highly skilled
mathematicians fresh German intercepts
will be passed into the first Hut they'd
work on a small part of the problem pass
it on to the next Hut the advantages
were twofold first the mathematicians in
each Hut could specialize on their task
secondly the compartmentalization of
knowledge was restricted this is a
central
preserving security in the war effort
meanwhile across the Atlantic dr.
Richard Feynman one of the greatest
physicists who ever lived was working on
the atom bomb it's a Manhattan Project
he also used compu Minh computers to
process the enormous amount of
mathematics that was required to develop
these weapons around this time ENIAC the
first computer was coming online now
Feynman did not believe that any act
could surpass humans so he squared off
his computing team against any ack for
two days they battled it out the humans
were winning at that point the humans
had to sleep from that point on humans
would never again surpass computers so
during the dark ages the baby boomers
solved most of the hard problems in this
space there was a huge amount of
research done at IBM and this champ John
Paul Morrison effectively wrote the book
on the topic he produced a system that
was employed at a bank in Canada in the
mid-70s this system is still in
production today gun meditate on that 40
years continuous production so right now
we have a veritable Cambrian explosion
of systems using this type of
computation Kestrel provides the
backbone for Twitter cacica does similar
work at LinkedIn the accra framework
built on top of scholar uses these
concepts and the early the Erlang
language itself is really built around
these ideas so to recap stream
processing you have a topology of nodes
your nodes are connected together those
pipes each node works in a small part of
the problem passes its results on to the
what next one in the chain this is how
you solve your problem if we have a look
at any act again any act the first
computer any echo ENIAC is actually a
stream processing system you can see
here it's made up of a number of panels
so these vertical panels are each nodes
in any egg they perform one function
each it might be an addition unit an
accumulator a square root module there
were about 30 different modules ahead to
program ENIAC you connect it together
with wise so if you see this woman here
they are actually programming ENIAC
these are some of the first programmers
ever over her arms she has a set of
wires she is using these to connect the
modules together
the way ENIAC would work is it would
have a punch card reader at one end that
would read data in and feed it through
the processing topology at the other end
there would be a punch card writer that
will punch out the results so when here
is developing his nuclear weapons
finally would put runs through this of
about a million punch cards in one
session this is max MSP it's a modern
flow based programming environment it's
our audio audio-visual media programming
environment so in max you have a set of
objects each object has one or more
inputs and one or more outputs to
program X you connect these objects
together otherwise this system can
handle MIDI data full quality audio and
full frame video is a really rich and
deep environment for programming AV
installations it uses exactly the same
paradigm is any acted so why do we do
this why do we want to program in this
way first of all it makes it really easy
to reason about your problem you can
break it into small discrete steps each
of those steps becomes easier to program
it's also great for real-time data if
your nodes operated synchronously they
just sit there waiting for some input
before they do anything this is a
natural fit for real-time processing the
other benefit is it scalable so if you
have a node in your topology that's a
hot spot it's working harder than the
other ones you can simply clone it
spread the same data across the two
copies of the same node with some kind
of load balancing and you've reduced the
heat of that area in your code after
that you can recombine the data and send
it further down the topology so I want
to do this myself meat straw straw is a
real-time processing framework for
nodejs it's on NPM you can install it in
fact you should install it it works just
like any other NPM module I find a
really good way to introduce straw is
through a hello world application this
app is made up of three nodes these
nodes are connected together with 2 y's
our data flows from less derive from
this picture you can't really see the
errors here very well so the ping node
that periodically generates an output
count it simply counts how many inputs
are received and every time it receives
the
input it sends that total through its
output print just prints out whatever
measures you'll receive to your standard
out so if I was going to write hello
world I set up a file structure
something like this topology j/s is your
entry point there's a file for each of
your nodes and there's the node modules
folder where n PM is going to put it
stuff can everyone see that okay yes no
yep so in topology J is you're going to
have something like this this is how we
define a new topology we require the
module we instantiate I'll topology and
we passed it in an object that describes
all the nodes we want for each node we
say which file it uses and what its
inputs and outputs are having a closer
look at one of these this is the ping
note it's the first one in our chain so
it's in the file called ping jeaious and
it sends it stuffs through an output
called P me out in the pink file itself
we have a structure something like this
if you use backbone this might be
familiar it is roughly building the same
principles we extend a prototype node we
override some functions we write some of
our own the key one and ping is the run
function when the topology is
instantiate 'add run gets called that
starts our operation off for this one it
just sets an interval timer that
periodically cause our ping function
that outputs the current date time the
count module is a whole lot simpler so
it receives input from ping and sends
its outputs for a channel called count
out inside it we have something like
this whenever it receives a message it's
going to process it all it does is bump
up a total and output that total and
finally we have print it receives its
output from count oh it's much simpler
even it just received some input and
prints it out to the console so if we're
going to run our hello world it looks
something like this no topology jeaious
you see some booting up action happen
every node starts it tells us it's
finished starting up when we get to the
bottom you see ping printed our print
putting out some results so if a process
going on pin count print pink count
print pink count prince that's what I'll
topology is doing
so if you want to start playing with
straw the best thing to do is have a
look in the examples folder in the in
the code base there are a bunch of
different ways you can configure your
topologies there are lots of different
models for doing it and there are a
whole lot of things I'm not going to
talk about today that it can do once
you're familiar with that you design a
topology to solve your particular
problem right some nodes straw takes
care of the rest when you write your
notes you're only really two things you
need to be worrying about at its core
one is you have to process incoming
messages so you override a base function
do some processing and there's a call
back to call when you're finished if you
want to output some data it's just this
dot output straw takes care of where
these messages go and where they come
from so this is a bit more of a
substantial application built using
straw who knows about the Twitter fire
hose hands up everyone heard of that
like three people okay so Twitter
provides what's called the Twitter fire
hose you can access this just through
HTTP requests and that will stream down
a percentage of all the tweets that go
through the system so you get basically
a flood of tweets coming down your
connection what you get is about 1% of
the actual tweets so these these this
data that comes down is it like a
JavaScript object for each tweet in that
it has a whole bunch of metadata about
the tweet who it's from the message of
the tweet the timer was sent the
geolocation the language the tweet was
in and a whole bunch of other stuff what
this app does is subscribed to that and
try and display that information on the
screen in real time so you can see in
the middle it's a bit hard to see but
that's a map of the earth with
geo-located tweets showing up on it to
the right we have the top 20 trending
hashtags and the count of those hashtags
that we've seen down the left you have a
histogram of the languages that have
been seen in the tweets if I was going
to build this I'd constructed something
like this at the very front we have a
consumer for the hearthfire house so we
connect to it we start receiving the
tweets for each tweet we get in we're
going to break it apart we're going to
route bits of that tweet to different
sub processing units one of them is
going to take the geolocation all we
need
with a descend up straight onto our
front end for the language we're going
to take counts of the languages we see
now what we don't want to do is every
time we see a tweet update our kind of
languages on the front end because if
we're proteins than ten thousand tweets
a second it's just going to swamp all
our web browsers out there so what we do
is maintain a running count on the
server and ever and periodically once
the seconds or so push that actual count
to the front end with the hashtags it's
a little bit more involved but not much
we take the text of the tweet we strip
out the hashtags and we pass this on to
another node that node then does account
very similar to how we do languages it
maintains a running total and outputs
the top 20 to the front end our front
end itself is served up through Express
so we have a topology running we have an
express server running they're actually
completely independent Express provides
a socket i/o connection to the browser
does everyone know what socket i/o is it
basically provides your real-time
communication between your web server
and your client so you can push data to
your to your browser the problem we have
is the topology is running completely
independent from the web server what we
need is a way of communicating our data
from that topology to the client is
assessing the web server so luckily
straw provides us a method for doing
that what we can do is use what's called
a tab so in our topology we provide an
output from a node that is going nowhere
and independently we tap into that from
our web server that receives all the
messages so that nodes outputting so we
do this for the geolocation for the
languages I answer the hashtags in our
server and the Express server we have
something like this we create a tap we
say we want to receive messages from
client geo which is the name of the pipe
and every time a message comes then we
have an event handler for that all that
event handler does is push it to the
client / socket thought I oh I know web
browser so this is on the client side
JavaScript code you're doing something
like this we simply bind to socket i/o
look for a message that's been tagged to
do and when we get the data in we just
kind of painted onto the screen onto our
map
so there you have haystack looking
something like this you have three
sections to it there's three separate
branches of messaging that are coming
from straw through your Express server
to the client there's a map with your
located tweets now if you see this in
real time you actually see them pinging
in as the tweets happened that looks
really cool and if you leave it for a
day you end up with a map of the world
and world and read where the tweets have
been down the bottom left you have a
histogram of the languages and are
trending tweets all of these update in
real time so if you want to play with
this it's on my github repo clone it
install some dependencies you will have
to go to Twitter and register it as an
application you need to get our keys
from Twitter to actually access the fire
hose once it's done you need to separate
terminals you run the topology and one
you're on the web server and the other
and then you can just serve to it
locally so we're going to take a bit of
a deep dive into how straw actually gets
this job done straw only really has one
hard dependency that is Redis who knows
about Redis yeah good more people should
learn about red us because it is super
awesome regice regice is a key value
database so what that means is that on
my server side if I want to store some
data I say for this key safe near value
Redis will take care of it but it's
lightning fast there are pretty much
source at all in memory so you're going
to have to install Redis first once you
done that clone the repo and install the
dependencies inside throw itself we have
four files that really matter these are
in the lib folder there is indexed jas
there but that's just provided as an end
point for our nodes require system
topology j/s is where the main work
happens you have a node prototype that
you can extend and then run and runner
or where the magic happens for actually
operating the nodes so if we have a look
at a topology and I make this bigger no
it looks something like this sorry I
don't have a copy of this on my screen
so i have to give referring to my notes
you created a topology into that
topology you pass a JSON object
describing the nodes you want straw
iterates through that increase our
creates a runner for each node
netrunner Forks the child process so a
child process is a completely
independent unix environment for that
node it has its own memory space its own
everything inside that you have a
separate instance of nodejs running we
run a piece of code there in called run
jas that sets up communication with red
us that loads in your node file and
instantiate your object now this own
process business we run our loads in
their own process there are few reasons
for this first of all it makes it really
easy to restart nodes so if something
crashes or we want to change our code we
can just kill a process and start it up
again if you've tried hot reloading code
with knows require system it's really
hard and I think you have to get a bit
happy to do that so when you're
developing the straw you can work on a
node save the file it will auto reload
it for you we implement that using child
process the other advantage is a crash
doesn't take everything down if your if
your node crashes that process is
crashed we receive a message and we can
just restart it it means you're not
gonna bring the whole system down by one
bad bit of code but I think the biggest
advantage is that JavaScript runs in one
thread that means you are bound to one
cpu core now most of us I think all of
us now have more than one core on our
machines that means we're wasting most
of our computing resource so by running
these nodes in their own process you
distribute this load across all of
course in your machine it also means
hypothetically you could be running your
notes on separate machines using Redis
as a communication infrastructure
between them so we dive into the some
code we're going to walk through our
creating a topology and see how it
actually does it so we're just going to
look at one node normally you'd be
passing in a whole set of them we're
going to look at our count node it's the
file called count DOJ's that has an
input and an output when the topology
first starts up it iterates through this
object it creates a runner for each node
into that runner it passes the
definition which is a bit of object we
saw on the previous slide and some ops
some options these options contain
global
was like where is my reticence and
swimming to communicate with this is
shared by all in all all the nodes
inside that runner days function we 4k
child process so this effectively runs
run jeaious in the command line in a
separate process so what you're seeing
here is the absolute posture that far
because it's not inheriting your
environment variables it doesn't know
where the file is relative to your code
you tell it exactly where it is then in
the array down the bottom you see the
command line parameters that get passed
into that file when you run it once it's
been forked we've only got one way of
communicating with this process we
combined them an event an event handler
to it to capture messages or send us or
we can send that messages directly we
use this for some very minimal
communication we say have you crashed
send us a message you need to terminate
yourself that's about the only level of
communication we have with the process
once it's running inside this child
process we pass the command line
arguments we require your file in we
tell your files are start itself up and
when it has its ends in then it triggers
the callback once that callbacks fired
to our parent process we send a message
to say we've initialized that means that
node in the typologies up and running so
the key bit of magic here though is read
us the nodes can't communicate with each
other through the regular JavaScript
environment the only way they talk is
through Redis this run process handles
all the communication with read us the
node itself knows nothing of it all it
knows is I receive a message I got to
process it or I can send some output
them you're going to deal with it
somehow it doesn't even know where it's
going to go so run sets up communication
with red us we use a feature of Redis
called Redis lists so as its base data
type Redis basically store strength but
it has another bunch of other data types
one really awesome one it has as lists
so with a list you can store a list of
values you can push values into one end
pull them off the other end or the other
way around so you can use a list as a
snack or a queue we use Redis lists as a
cute this is
an analog of the pipes we have passing
data along we push messages in we pull
them off the other end so Regis's base
command for dealing with list is
actually not too much use for us because
if I want to get a piece of data off the
other end of the list I ask Redis and
it's going to say here's some data if
there's nothing waiting for me there
it's going to say sorry no doubt I'll
see you later I'm going to have to keep
polling Redis to get that data but
there's a feature there that's really
useful for us called blocking pops the
way this works is I ask criticism data
and if it doesn't have any it won't come
back to me until it does so I say ready
see got some data and finally needle to
say here's your data the know can go and
process it we get something for free out
of this that's really useful it's
basically how we do the load balancing
on the nodes so if you have two
individual nodes they both queue up to
get data from that pipe when the message
comes for the first node is going to run
away and process it the second node will
be front of the queue this guy'll
processes data come back and join the
back of the queue so we get the free
load balancing by just using basic
readers commands if we have a look at
how this is implemented so the self dot
be our pop function is really the main
event loop of the node it connects to
rid us we tell rid us to be our pop the
self that L keys are the list of pipes
we want to subscribe to essentially a
node can listen to multiple pipes it can
have multiple inputs then there's a
callback function that gets triggered
when a message arrives we process out
with a node and we go back to BR pop
again so we have an infinite loop here
that's just pulling messages off the end
of the pipe so conversely when we want
to push the message on to the pipe we
have an event handler that listens to
our node where the node outputs a
message this handler gets called it
pushes a message on to the left hand
side of our pipe or our Redis list redis
has a great little utility go read or
CLI that lets us interact directly with
Redis when useful little command and
there is monitor it shows you all the
commands that go through reticent real
time where you can see here is a brief
trace of the hell
were there and what goes on inside read
us pushes some data onto the list of the
pipe and then the count node is popping
it off the other end so that's what it
looks like there's one other thing you
see it goes be our pop straw hang out
one these block and pops actually do
have a timeout do you say I'm going to
block 41 seconds if you have nothing for
me give me my control back because they
do actually block your JavaScript
process
so I'm using straw in the real world one
of my clients is the market sponsor of
Australian electricity futures and
options data we get a data feed from a
provider in the States effectively this
is just a raw socket that we connect to
and receive a flood of messages down
these messages come in a proprietary
format so we have a node that connects
to the socket takes these messages
changes them into a format we can use
it's the parser no there then they go
into a sanitizer note that dumps
everything we don't need we actually
receive the complete market picture of
all the futures and options we only
really need one percent of this so
everything we don't need we throw our
way after the sanitizer step it goes
into what I call catcher which basically
puts it into a database there's actually
a whole bunch of post-processing that
happens after that but it's not really
relevant to what I'm showing you here so
the big problem with this data provider
is that we can only have one connection
to the socket which is not useful to me
because I've got to have a staging in a
production environment I cannot be
developing on my production environment
the way around this is I have two
identical topologies running run in
production when and staging after the
sanitizer I just dumped a log of all
adult where I got down to a text file in
on my staging environment I tail that
file over ssh feed it into the identical
topology so after the sanitizer stage
everything in that top is the same I can
work on it I can change it I going to
break it do whatever I want without
affecting my production environment so
this is a couple of screenshots of the
system actually running down the left
you see a log of straws output this
ticks along at about one message a
second sometimes we get floods of the
other coming through and there might be
thousands of them there but generally
during during random market as a message
of seconds or so this is a little app i
wrote to actually monitor what goes on
in the pipes so the number you can see
in the column in the middle is the
build-up of messages in any given pipe
the way this data works is it's kind of
peaky we get a flutter data through and
then it'll trickle for a while then we
get another flood of data through so
this is fairly normal operation but
using this I can see maybe where the
codes inefficient where I've got hot
spot
where things are broken if messages back
up to 30,000 or so I know I've got a
problem with one of my notes this is
actually made debug the system I mr.
call back somewhere and I was getting
exactly that I get a mess of amount
piling up on one pipe and those that
just crash because it ran out of memory
actually registered crashed because it
ran out of memory down the bottom I have
some straight Redis counts that's a
account of the messages through the day
so over the day is this is nearly the
end of the day it's a couple of million
messages there straw also gives you out
of the box support for stats DSO stats t
is a fairly useful bit of monitoring
software what you can do with it is to
send that counts of events that happen
so in straw every time a message goes
through a pipe that sends account to
stats deep what you see here is the
counts of the electricity futures so the
purple background is the tip as the data
coming direct from the feed the other
one the message is coming on the
intraday cycle down the bottom is
actually the useful information so you
see it's thinning out as it gets further
down the chain this is a bit more of a
detailed breakdown to the same thing I
hope you can see that up to the top left
we have what's called a heartbeat so the
data providers gives us a the data
provider gives us a heartbeat message
once a second we feed that through the
topology and use it in our monitoring as
well these other charts you see this
tiki action going on every 15 minutes or
so they send us a complete market
picture so we get a price of every
commodity on the market what looks like
the noise from floor on these charts is
actually the useful data this is the
bids the offers the actual trades going
through the system what I found really
interesting is the blue one here at kind
of in the middle right you can see a big
hole from about midday to about 2 30
this is when the brokers have lunch at
the pub
so to sum it up stream processing we
have a topology of nodes they are
connected together with pipes each node
processes are small part of the problem
kind of to wrap it up please jump in and
state using straw it's on github I
contribute some code there are 50 things
it can be done to it but what I really
want you to do is I think this is a
really emerging idea in the technology
industry at the moment straw is a toy it
will help you get started it might help
you solve some real-world problems but
there are some really
industrial-strength systems out there
that are great at doing this stuff after
you've had a play with straw I'd
encourage you to explore and see what
suits your particular problem add straw
stream processing for an oj s thank you
hi this is really cool I I think that
you said the master process is just a
single process and the other like each
node can be distributed great yeah have
you given any thought to how you would
distribute the master the master node
and have like multiple masters so if one
goes down it doesn't take down the whole
thing I have but not deep thought it's a
tough problem that one in fact be honest
straw is my version 1 and I'm working on
some other stuff at the moment that'll
hopefully replace it and and that is one
of the problems to solve thank you
really interesting I was reading about
no flow Jess yeah which is I think I
have slow processing but probably is
under doing a single process and my
question is how do you manage like
multiple if you like multiple workers
sorry say again please how do you manage
if you like like multiple workers for
probably issued or is issued just a
worker there's a process how do I manage
the child processes yes do you manage
like Dean the number of processors for
the news yeah there's one process per
node so it doesn't do any kind of it
doesn't stop you doing anything silly
it's up to you to design the topology so
that it's going to work right but I
think know Jace itself will choose which
cpu those processes end up on you can't
really say run this one on that core or
this one on that core does that answer
the question this and you know in in
certain situations where probably is
more data intensive and it requires
probably you would like to because it's
processed on New Genesis 22 runs on a
single track so if you like to have more
more workers that more processors to the
process particular process studies
capability to do so I'm not sure I'm
right each if to each of those knowledge
has its own thread that's running in an
independent process so it's up to the
underlying operating system to manage
their okay and this has what pretty well
for you sorry this has worked with
absolutely or works it works well okay
yeah with a multi-core machine I mean
this is like you can see it in the web
browser if you had some JavaScript
that's too busy it's going to block your
UI interaction because it's got one
thread that handles everything it's the
same thing on the server so if one of
those nodes is doing some heavy
processing because it needs to if they
were running in the same process it's
going to starve out the other ones for
CPU time or for time on that thread
whereas that their own independent
threads the CPU is going to schedule so
that they're not blocking each other so
this is what about a hot court reloading
yeah so do you just like the stick now
the processor do exactly well they each
in an in the node prototype there are a
couple of methods is one good start and
one called stop you're runnin stop so
when I detect a change in the file with
what you can watch the fire for changes
it'll tell that process it will tell the
node to stop the node will give a call
back when it's successfully stopped
it'll take down the child process and
then it'll spin another one up but by
doing that it's reloading the file off
disc to get the fresh code so yes to be
done crew to master the master takes
care of that yeah if the crash us and if
it crashes it's the same thing it's
worth having a look at the the child
process docs and OJ's but they're a
bunch of messages you can bind to from
that one of them will tell you if the
process successes and what exit coda
gave you when it did it so you use it to
say well my process is just terminated
itself I better start up another copy I
think there is a chance of message loss
with this is it's not in any way perfect
if that nose was processing a message
and it crashes that message is gone
Thanks hello I in your cases have you
met a condition where when you have to
scale the stro not yet I've got some
ideas for how to do it and that your
first thing is that if you have to scale
it how do you did the two approaches
that the first is the cheap one where
you're using the same computer
and in your topology you have a node
that's hot you make a copy of that note
so I might have a process one and
process too but nobody's listening to
the same source the Redis will take care
of load balancing the messages between
them hopefully they'll end up on
different CPUs so they're not contending
for resource you can add as many as you
want you could have 10 of them and I
think the examples actually have one of
us in there the loader gets balanced
across them so that's the first way the
second ways you'd actually run a
different computer with a copy or part
of the topology but connecting to the
same Redis so you output a message here
and just by virtue of naming the pipes
correctly this one receives the messages
from the other computer but again it's
not going to you you're not going to run
Twitter on this thing right it's good
for small stuff and it's good for
learning yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>