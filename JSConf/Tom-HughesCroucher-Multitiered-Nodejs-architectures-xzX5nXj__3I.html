<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tom Hughes-Croucher - Multi-tiered Node.js architectures | Coder Coacher - Coaching Coders</title><meta content="Tom Hughes-Croucher - Multi-tiered Node.js architectures - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tom Hughes-Croucher - Multi-tiered Node.js architectures</b></h2><h5 class="post__date">2013-01-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xzX5nXj__3I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Tommy scratch I work for a
company called joint which you may be
heard of we do stuff with node which
you've also may be heard of and yeah i'm
writing this book i don't recommend
writing a book it's a stupid idea if
anybody ever tells you to write a book
you know just ignore them or hurt them
or both but this is this is the book i'm
writing in case wondering that's a
common tree true cuz alrighty clearly
let me pick my animal you know so I the
talk is multi Ted node architectures but
you know I wanted to kind of recap some
stuff I think the the people at Jay's
convoy always really smart but I think
often kind of looking at the core
principles of what we're trying to
achieve really helps so you know one of
the things that we want to do when we
write servers is minimize the client
response time and this is actually like
a big draw of why people use node right
is we want really fast service because
you know sometimes people just can't
wait to do stuff and you know that that
can be an issue in case we running I
also used to work for Yahoo so that's
the Yahoo that's a yahoo bathroom so the
goal number two is well sure we won't
really fast response times but you know
heck I only have so many servers you
know I mean I guess we could buy like a
server for every user that we have and
then all of their response times would
be really fast but nobody can obviously
nobody can afford to do that that's
stupid so it's you know we have we have
these two conflicting goals it's like
how do we how do we balance these things
because obviously you know at some point
we hit the capacity of our servers and
sometimes you know even if you're
planning capacity there's there's there
are issues there so I like to think of
this in in kind of the critical path
like one of the things I think people
people think of like these really
generalize models are computing and what
it comes down to for me is we have a
critical path this is what an Internet
application looks like a modern Internet
application looks like this you have
this big red blob of like network
latency at the front and this is
obviously this is really simplified but
you know the the talking back and
with a web client of the internet is
really slow compared to the actual
amount of computation it takes to render
a web page talking to the database it's
really slow compared to the actual time
it takes to do a query in general not
all queries but in general this is this
is generally true if you mean think
about Google if every time you typed in
a search query Google went out and
spidered the entire internet compiled
you know a search index and then gave
you back your result would be like a
month the point is that we take all the
computation as far away from the user as
possible so when the user is actually
interacting with the website the web
application whatever system it is that
we're doing the the maximum the biggest
pieces of this time is some kind of
rooting some kind of network transfer
and the actual amount of computation is
as small as possible and these are the
models of applications we build now so
the traditional way that we do this is
we pre allocate resources to the client
for the for the entire duration of the
request we go here you go here's a bunch
of resources and that gives you
efficiency within each individual
connection you give a bunch of resources
away and you get you know you get a lot
of efficiency it's like his here's a big
size of resources and that means that
the response time for this individual
connection is going to be really fast
because they've already given them all
the resources that they need to do this
so as soon as anything happens I'm
instantly ready to go so they kind of
makes sense you know I think of it like
this you know if the the you know if the
pie is my server I have like a certain
amount of resources it's just dedicated
to running you know whether it's node or
Apache or you know tom cat or whatever
and then every request you start to
share out the pie and you give each one
of these two requests and you know
that's great but we basically heavily
waste memory in order to serve the
fastest most optimal requests and when
you run out of you know memory the
Soviets overwhelmed and that's you know
that's kind of bad it's like fighting
off lobsters with a stick I guess to
stretch your metaphor I don't know um
actually I tried this talk somewhere
else and somebody had called me and said
those aren't lobs
those are crabs you know it's kind of
weird I mean to be honest I don't know
this gentleman well enough to know if
those are crabs or not that was awful
wasn't it but seriously I mean I think
the it's it's it's it's a model that
kind of it makes sense in principle but
we have these real-world constraints we
can't have as many services we want and
this is where the event loop comes in so
if you don't pre allocate memory what
happens we create a placeholder for so
test so this is kind of I mean this is
still kind of node 101 stuff but we have
all these placeholders and then we have
this shade work resource like now I have
some work to do I'm going to do it the
servers going to use memory in order to
deal with this one specific task we have
the first-in first-out model so we have
a bunch of events you can see that for
each individual event whether it's like
an event created by the node application
or from the OS it goes in and it comes
up but you know that makes the server
more efficient but it then creates this
thing well we can't block the event loop
so people start to you no wonder well
i'm using this model how do i actually
make the most of my service what are the
architectures that really helped me so
if i do big work in the event loop then
we can only serve after that after that
request is finished so you know this is
kind of this is like a different
programming model people like what I get
the other thing you know I just have
like one thread per user and I don't
have to think about it but now I'm in
the shared memory you know I'm sharing
all of this stuff how do i how do I do
that I have to worry about these big
tasks so blocking the event loop is just
bad nobody wants to do that you know um
don't worry we're going to run out of
I'm never that bad metaphor soon but
nobody wants to block the event loop so
it's like how do I do that how do i
create an architecture with node that
makes it work efficiently so this is the
obvious architecture and everybody uses
this in you know it's not rocket science
but it is a tiered architecture I have
my client and then I have some kind of
like node front end and then I have
database and this is I mean this is like
a really simple architecture but it's
it's clearly multi-tiered the database
is doing stuff that isn't in the main
node process we're not blocking we're
using eight
Kronus I oh and this this already is a
multi-tiered architecture you're pushing
the database work out so it's not
blocking the rest of the node process
right i mean we could just you know we
could have all of that together what
about something like this this is
something that I think a lot of people
were starting to play with will you know
if big things are no too bad how about I
do something like this so I want to do
clients maybe I want to use Jess domin
order to render all of my results people
people really like the idea of doing
stuff with jess Tom and that's cool I
think that's really interesting but it's
like well what if you know if if all of
the j estan stuff is really big I should
move that like a separate thing outside
of my main node process right and then
you end up with an architecture like
this well why would you do that one of
the things people want more paralyzation
like I'm spreading out my load I'm
creating more task synchronization but
if we look back at this diagram when
you're using something like Jess DOM and
you're just increasing the size of these
computational units you're just
increasing the size the critical path is
still the same the path between the user
asking for a request and getting on back
is the same you've just added more
computation so maybe the actual balance
on the critical path is different maybe
now computation is thirty percent of the
critical path instead of 10 because
you're doing more work on the server
because you're using more JavaScript I
think that's okay and this is the thing
is people people think that that doing
that helps them if you use non-blocking
i/o you're removing all of the resource
allocation during latency so all of
those latent periods you can forget
about and the computation is still the
same so pushing all of that
client-facing processing off the main
process so if we look at the
architecture again you know pushing that
back to another thing that's in the same
critical path isn't helping you in fact
it's actually just making it worse
you're pushing that processing so you
have to have another jump you have to
have more HTTP requests it turns out
that it's okay if you have a bunch of
like node front ends or running express
and
you know oh you know your individual
response time is now 15 milliseconds
instead of one millisecond well that's
okay because you're doing more work just
get more service you're going to need
more servers anyway so I think this is
something that people people you know
conceptually understand this idea if I
don't want to block the event loop but
if you're just doing more work then
that's okay so let's look at some other
stuff what other kind of architectures
might might we look at anything this is
where it starts to get more interesting
so we've got a client I've got a front
end farm which is talking to the
database maybe this is express now the
logging is this is something that isn't
on that critical path this isn't
involved in getting the response to the
user as fast as possible instead now
we've got something where it's you know
this is something that's out of band
this is something that and if you are
Matt is matt raney around somewhere hey
Matt there is he's really tall but he's
sitting down so now he's just putting
his hand up like an average height
person Matt Matt is doing a bunch of
stuff in this kind of model so i
recommend talking to matt if you if you
want to talk to somebody that's running
at scale a bunch of stuff where he's
pushing the work which isn't user-facing
out of band and this is this is where
it's useful if you have this process
that that you want to work as fast as
possible and you're sharing memory
things like logging just throw it at
Redis throw it at some node process
that's going to take it to read us a
real couch and get it out of the way the
user if you if you have and this is I
mean a really good example of this
actually is a standard error if you try
and write to process that standard error
on you'll note your main node process
that actually blocks it if the colonel
write buffer is full that will block
your whole main node process so the
better thing to do is if you want to log
to standard error which you don't really
need to but if you really did throw it
out to a separate note process that's
going to write to the that file
descriptor on your behalf you know you
can share the file descriptor you can
pass it over and it will write to it on
your behalf but you've got to get the
things that are blocking the ant to do
with actually serving the request to the
user out of the main band and
this is where it makes sense to start
splitting stuff up so move work which
isn't user-facing out of the event loop
don't worry about the stuff that is that
can stay there that's fine it's just
regular work and then of course you know
i mean i'm i've started displaying some
of these things now as stacks use pre
forking so just you know create things
into farms you want to use all of your
you know all of your cause on your
machine do people know about cluster
hands up people that use in cluster a
few people you can do you can do this by
hand I strongly recommend cluster I've
heard like TJ and Guillermo a good at
writing no taps apparently that's true
so this is this is a really this is how
you use cluster so I create a file
called children Jess I create a server
in this case obviously it's really
simple in your case is going to be a lot
more complicated create the server you
have a function which does stuff and all
I'm doing is instead of actually
listening myself I just export the
server it's a the people you spark to
anybody using spark no okay it's like
that if you've seen that and then to run
it instead of just running it as normal
I just have a rapper script so the
rapper script includes cluster cluster
children so I'm giving it the name of my
my JavaScript application and then I
tell it to listen on port 80 now what's
going to happen is in this scenario
instead of just starting a single node
process which is what typically happens
when you just type node my script it's
going to start 20 and what it's going to
do is it's going to create our a main
process which is going to create file
descriptors it's going to listen on port
80 and it's going to pass the file
descriptors it's going to pass the
socket that one socket on port 80 to all
of the other child processes but it's
gonna the way that no does that is going
to pass them at a kernel level so
basically we're going to tell the
colonel I have these 20 processes and I
want you to give anything that happens
on port 80 to one of them and one of
them is going to pick it up so now you
know if you have a server with you know
24 logical cores whatever so say it's
you know say it's just like a regular 8
CPU machine and each of them has you
44 cause you know you can you can use a
bunch of those cause in order to do your
node work that's that's great that you
now have a lot more capacity within your
node and this is by far the easiest way
to do it you can obviously I mean like
go and read the code it's you know I
think it's like a thousand lines or less
you know you can go and see what they're
doing but cluster has a bunch of really
nice tools for management and you know
keeping them the the notes heart and
restarting them and stuff and then if is
that's great but you can also use load
balancers to just distribute this is all
like regular stuff no proxy I don't know
if you saw Charlie's talk you know those
guys right node proxy squared varnish
hard proxy that you know there's a bunch
of stuff I'm not gonna really talk about
that and then the final thing I wanted
to talk about a little bit let me just
check my time all right so we've got
five minutes so shouting shouting is
kind of hard I mean you know there are
definitely strategies for charting I
think one of the earliest kind of good
references that I like on shouting is
cow henningsens book on flickr i think
it's like scaling or something like that
it's called so no variety book with a
fish on it and you know node node
process can handle lots of clients so
you can do really big shards this is
actually one of the things that's really
nice about node it's like typically when
you're doing shouting strategies it's
like well you know if each of my
machines only gets like a couple
thousand clients you know the logical
like separation so you've got like a
chat room if we were building a chat
room and you want to distribute across a
bunch of things you know and you have 30
people in chat room well you know it
gets difficult or like a hundreds of
people because node will happily handle
hundreds of people on a cat room you
know you could just have like one
process / chat room that's a really
common approach that people take there's
all kinds of things but because node
processes can handle lots of clients you
can do really big shards and that's kind
of nice and the other thing that's kind
of nice is that's a single process when
you start actually doing like multi-core
node on a single machine is you can use
unix sockets so you can you can use
those same file descriptors to start
passing a lot faster communication so
file descriptor sharing is actually
really nice because you can start to
have the same the same processes
listening to the same file descriptor so
you can basically create a bunch of file
descriptors pass them to a bunch of a
child processes on the same machine and
say you guys now have a really efficient
way to do multicast on a single machine
and the colonel is facilitating all that
so you're getting a really fast inter
process communication now I'm not saying
that that's easy i'm not saying that you
don't have to think about your
architecture but one of the things
that's great is because notice is
scaling so much on a single machine is
you can start to glue you know 8012
fairs and whatever it is 30,000 people
to a single machine and say you guys are
a shard and that you know depending i
mean you know i guess there's a few guys
probably from facebook or some like
other really big companies but for most
applications something even that simple
means that if you can just glue you know
however many people is to one machine
you've just achieved the amount of scale
that you wanted so let's look something
like this client load balancers glue
people to a machine and then the machine
then shards out are a bunch of processes
which can share some file descriptors or
unix sockets in order to do that really
you know that much faster than over the
network multicast so that's that's nice
so this is you know these I mean and if
you want to talk about like more
detailed things these get really
application specific but this is a
really nice way to kind of distribute
some of that load so you know UNIX file
descriptors have a much lower unbanded
latency than that work you know there's
still there's still a buffer there's
still the potential that you may not be
able to write to the file descriptor
when you want but it's a lot lower than
sending stuff over the network so
summary don't sweat client side facing
CPU time it's not a big deal move non
client-facing stuff away and then you
know use cluster if you if you're not if
you're using node right now in
production and you're not using cluster
I really strongly recommend taking a
look at that and it's just it's cluster
by learn boost really simple and that's
it so you can follow me on shimmer on
Twitter with one because I guess I'm
kind of old I've been on the internet
long time
but and any questions I think they have
a couple minutes yes possibly I mean I
think we're working at the roadmap we're
really interested in taking suggestions
on what you guys want for energy so
please you know let us know you know we
are working on some of that stuff but
right now we haven't published the
roadmap oh okay yeah yet so a bunch of
people I think use memcached you know I
mean is there's like a bunch of
strategies to kind of do that I mean you
think it depends what you want to
achieve and what architectures you want
to achieve I think things like memcache
they're kind of obvious i think the the
ability to use file descriptors in node
is maybe less obvious to people but i
agree i mean like using something like
memcached is a completely viable
strategy to ya how would you do like
sticky user sessions so you want one
user session to always be passed to the
same note in front Joe so the question
was like how do you do sticky user
sessions I think it depends what you're
building an app in so I mean I mean it
whether you do that has like the load
balancer or whether you do that you know
based on something with like Express and
actually like set sessions and then use
the the session handle and express in
order to like redirect let's assume the
Express so I would say you could either
use a lightweight express in front in
order to like redirect so just like an
express app that's just reading the
session handle and then passing it off
to the right place so using Express as a
proxy or you could you could just read
the cookie in a node proxy and then use
that Rick give any metrics are now
I don't you should ask Charlie you
should see you should ask the ninjutsu
goes I mean in terms of kind of I mean
in terms of like node proxy I don't know
in terms of a proxy written in node I
guess it depends on like the profile of
your traffic but we've we've done tests
were reasonable reasonable sized servers
are doing you know very high hundreds of
thousands of requests a second without
any trouble single not a single process
like Rocky is rocky by a few drunken
know by it's it's it's chartered it's
it's split across so it's it's sharing a
file descriptor in order to do learning
it's a solaris colonel cuz i work for
join um cool i do you have another talk
or is it I don't know at the end of it
anymore any more questions okay thank
you very much oh um and I couldn't find
a way to put in the side but I am I just
think it's really awesome I mean it's
just you know it's it's it's it's a
really bad movie that Sean Connery made
in like nineteen seventy-four called I
think it's ass off yes go and watch it
because it's terrible so that's it thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>