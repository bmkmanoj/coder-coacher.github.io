<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lian Li: Applying The Magic Of Neural Networks - JSConf.Asia 2016 | Coder Coacher - Coaching Coders</title><meta content="Lian Li: Applying The Magic Of Neural Networks - JSConf.Asia 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lian Li: Applying The Magic Of Neural Networks - JSConf.Asia 2016</b></h2><h5 class="post__date">2016-12-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rZ_8xTX1LU4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">gosh James talk was so funny it's like
following up Adele and the crew could
borrow something all right um so thanks
for your interest and i'll be talking
about neural networks today how they
work how you can implement in your
network yourself and hopefully also how
you can improve your algorithm so this
is me you can follow me on twitter and
github on google+ although i'm like
rarely on there but it's always nice if
someone still on google+ so yeah i'm a
web developer by day and I basically
turned machine learning enthusiast after
i attended the AI course by and room
that was the one that actually started
Kucera and afterwards i wanted to like
use my new skills for a hands-on project
and at this time I was in a betting pool
in the office for the football league in
Germany and like every week you have to
come up with new predictions and usually
if you're interested in football you
don't you're not very objective about
like teams and so it's you're not using
the scientific method basically so what
I was thinking was that would be a
perfect use case to implement something
like a soccer match tendency predictor I
will show you what i was thinking and
what I meant by that so um let's start
with a funny quote and this one's by
paul gascoigne and he's also known as
gaza he's a famous English football
player and he famously said that I never
predict anything and I never will so
following his lead i will also predict
something i will predict the tendencies
of the mattress for the Premier League
this weekend I think they will start at
like 9pm and then you can just check for
yourself how well the algorithm actually
performs and whether you want to bet
money on it
so um first of all we have to ask
yourself like what is machine learning
like why would we use machine learning
so like consider we were like the
programmers of an AI in like an RPG game
and this blue sword would represent a
player's action so the player would
attack us and then we will maybe react
with the defense action with a blocking
action so you could have some kind of
behavior tree where based on your leg
remaining health points you would also
have the option to launch a
counter-attack or maybe you want to
drink a potion so the problem is and
when you have to cite the appropriate
actions and that's you will decided when
you program this tree so the the inputs
and the weights so how important is the
health in proportion through the play
action for example its deterministic
because you decide on it beforehand and
this also means that the decision making
process is very subjective because it's
implemented by a program or a game
designer and the machine cannot adapt to
individual players does it doesn't it
doesn't learn and let's say if we wanted
to implement this in a deterministic way
might look something like this so if the
player attacks we look at our health bar
and then we say at the health bar like
above 75 percent then we do a
counter-attack if it's between 25 and
seventy-five percent we do a defense
action and we have less than twenty-five
percent of our house left and we will
drink or health potion so these
thresholds they have to be chosen
beforehand and they might seem arbitrary
or like it's like a personal feeling
that what makes sense but in complex
games and it might not be possible to
manually implement all the possibilities
and all the things you want to look at
so we would not want to have this like
the sturdy monistic logic instead we
want to have something that will change
or logic depending on something else
maybe so this entire thing should be
very dynamic
so um the way that you can do this is
with calculus and I'm trying to not go
too deep into math because I'm like
don't have the time for it but let's
just take the scenario that we have the
what the user did input and then we add
the how healthy we are input and then we
will get something that pertains to the
three possible outputs of course like
the two inputs are not equally important
for decision-making so what we need here
is some weights and in this case I just
called them a and B and basically those
weights determine how important those
inputs are in relation to each other and
then in the end we don't want to
directly have like the action that we
want to take instead we want to have
like a probability distribution this
could for example just be like what's
the most probable action that a human
player would take for example and this
way it's called classification because
we try to classify the correct output
for the given inputs and once we have
the correct weights like if we can
calculate the best a and the best be
then we have a model that represents the
logic or the pattern that we want to
make predictions on and the thing about
models is like Georgie keybox said that
all models are wrong but some are useful
and what he means by that is basically
that our model will not give us the
perfect or the correct result but they
will give us one that is the best
approximation to reality basically or to
the data that we already collected and
so this is basically the magic inside a
node but it's not called neural node
it's called neural networks so we have
to like understand what neural networks
are and so the important question to
answer is how does anything learn and
some type of people and took a look at
it like how learning is done in nature
and as you already know probably it's
done by neurons and this would be one
neuron
neuron receives input through its
dendrites that's a fuzzy purple
attentively stuff and then there it has
the soma which is the middle part with
the green coordinate and inside the soma
there's magic happening and the input
gets changed in a deterministic way
inside the soma and then the result of
it is sent through the axon terminals
and in nature of course a decision in
your brain is not made by one single
neuron but by a vast network of neurons
so we can have multiple layers of
neurons in this case for example we have
two layers with two and three neurons so
in all output from the previous layer
will be sent to all nodes in the next
layer so in the end every node will
output a value pertaining to one singer
like one single possibility that you can
take and the thing to take away from the
slide basically is that instead of one
neuron doing the magic ones you have
multiple neurons that do it in parallel
and also sequentially so even though
each neuron changes values
deterministically the whole thing is not
deterministic okay so this is basically
the same representation that we saw
before except now i just changed in the
natural neurons with artificial neurons
and so for the for the math to work its
magic we need numerical representations
of our inputs and outputs so for example
we can represent a health bar in a
percentage which would be ninety-five
percent in this case but so I personally
think it's better to have like the most
information possible so instead of
saying this we have ninety-five percent
over health left we could say that our
maximum health or your current health is
114 and the maximum help is 120 and by
that or algorithm can deduce that it's
ninety-five percent and have left and
then we have the attack so the attack
isn't it could be a string or label but
in this case our neural network
needs a numerical value so it can
calculate on it and we could just give
it like an ID and then just map it
somewhere and but I didn't really like
that idea because I was afraid M that I
would introduce some numerical
implications that I don't want to for
example if attack is one and defense is
to then somehow my neural network could
think that the blocking action is like
twice the attack action so this doesn't
sound very smart one way to counter that
is to instead use a vector or an area in
the JavaScript context so by using this
M airing I basically say that there are
three possible actions that the user
could take and he took the first one so
basically that's like a like a bull
array and so in this case the first
value would be attack and the second
would be defense and the third would be
potion so how do we get our predictions
and the first thing is I'm talking about
is going to call is called forward
propagation and it's called that because
we propagate data for word from our
input nodes to the output nodes so we
start with our inputs that's the one
you've seen before and then we send each
input to each node in the first layer so
let's focus on upper node so it's not so
complicated and in this node we do our
math magic so we have our weights and we
multiply it with our inputs and then we
get a new value which is X 11 because
it's the value we get from the first
node of the first layer and then we send
that new value to the next nodes and
then the lower note is the same thing
and in the next layer all the nodes do
exactly the same thing again of course
the weights are different here I just
call them D and E and then we get a
value in the end so I just I just said
that as 0.67 and because this would be
like the probability of the
action that is like directly linked to
this node because like each output node
is referring to one possible output okay
so now we have basically made
predictions from a training data but the
prediction is not that good because in
the first iteration which shoes are
weights randomly and now we have to like
the machine learns by trying to get
better at predicting stuff and so my
first instinct would be you could just
reverse engineer right because you have
the data and you know what's supposed to
be the output and then you can just like
reverse engineer and this is more or
less basically what the machine also
does this is called back propagation and
might have guessed already it's because
we propagate data back from the output
nodes to the input nodes and let's say
like these are our predictions this is
the probability that like for example a
human player would take this action with
the given inputs and then this is the
actual result this is like the truth
that we know from our data set and then
we need to figure out what the costs are
and the costs are basically just the
difference between the actual result and
what we predicted so in this case it
would be 0.33 0.14 and 0.19 respectively
and now inside the nodes we do not try
to get the right inputs instead we try
to calculate how we would have to change
the wave so the cost is lower next time
like in the next iteration and I'm not
going to go too deeply into that right
now because like it's very mathematical
and but after we've done that we will
back propagate the values the first
layer and we will do the same thing
there okay and so one way that we
calculate how to change the weights it's
called like gradient descent there are
other algorithms that you can use but in
this case I want to show you I'm the way
in descent because it's it's easy to
explain in a graphical representation so
um let's say our data is will be
represented in this 3d am representation
and we'll take a very simple
example would like only two numerical
features like only the max health and
the current health and then we have our
weights would so we do not plot the
actual input data we plot the weights so
that's a and B and then on the z axis we
plot the cost and so the idea is that in
the front you have high cost so the
predictions that you make are very far
from the actual reality and in the back
we have low cost and the way we plot
data in this three-dimensional
representation is we have those circles
the circle means that all the data
points that are on the circle so all the
combinations of a and B that are on the
circle have the same cost and then you
have different circles like this one and
it's the same here like all the
different data points on this circle
have the same cost although this circle
is a little bit darker and I made a
darker because I wanted to represent
that the cost is lower and so they all
have the same cost on the circle but of
course the differs from the from the
previous circle and then we you know
like further plot the data and then we
get like this kind of funnel where you
can just basically you look inside the
funnel and in the middle that's our
global minimum that's the point where
the cost is the lowest that we can ever
get with our with our algorithm so the
way that gradient descent now tries to
get there mathematically to the red dot
is that we have start somewhere randomly
like I said before we choose n be
randomly and then basically great into
the centers like walks steps towards the
global minimum and just like one was
around a little bit and just tries to
try to approximate the global minimum
and at some point hopefully it will get
there doesn't really happen that often
real life because we don't actually know
what the global minimum is we can see it
here because its graphical but if you
have a mathematical represent you don't
actually know and so the idea of this
whole thing is like we iteratively
approximate our model to the truth
okay so i hope you still with me because
this is gonna be a I'm not gonna go
further into it because it's interesting
to know how neural networks work but
when you implement something you want to
use a framework and not be bothered by
all the mathematical stuff and so I'm
gonna like introduce it another smart
guy who said this longer quote thing and
he says an approximate answers the right
problems were the good deal more than an
exact answer to approximate problem so
basically the better you describe your
problem the better your approximated
answer will fill it even though it's not
perfect so okay and let's implement
something I will not do a live demo
because I'm really scared and that's not
gonna work and I have used a node.js
library which is called synaptic and
like I said it implements all the fun
math stuff already and we just have to
like build the application so these are
22 lines of code and it's basically all
you need to predict a tendency with
their artificial network so we're not
doing like AI anymore for RPGs now we're
doing the prediction of soccer mattress
so first we have this historic data with
inputs in this case it's the market
value of the two teams home team and
away team and then the output represents
the result so in this case the first
element would be that the home team won
the second element would be I'm
representing a draw and the third
element would be representing that the
away team won and then we have like
mattress that are that haven't happened
yet they have of course also inputs but
they don't have an output because of
course we don't have a result yet so
that makes two datasets basically and
then we have to build our network and we
use the number of input nodes that is
the same number of inputs we have and
then India we have three output nodes
because we have three different classes
that are possible and inside we have
hidden layers and I just like edges
chose two layers with six nodes
it's not the this is the law or
something and then we have to get a
trainer for a network and then we train
on it we tell them what the learning
rate is the learning rate is basically
the size of the steps that gradient
descent takes and this is also i just
chose that value and because it just
felt like it and also i have to define
the number of iterations which is how
often you go through the entire training
set to train your network and then when
we're done with training we actually
make predictions and if i would run the
script it would look something like this
so um what we can see like if we look at
the first prediction and the the home
team is worth less than half the of the
away team and the probability of winning
the match is also about half of the
probability to lose the match that seems
pretty straightforward and probably
whatever I would have predicted to if it
just only had those given data but when
we look at like the third prediction
although the home team is also worth
about half compared to the way team our
machine predicts that is likely more
likely for the home team to win the game
and this could have a lot of like this
could have a lot of meanings it could
mean that there's something like a home
team advantage or there's like a
different like implication that we don't
know about and the problem with machine
learning and prediction is that we don't
really know how our machine gets there
because it's non deterministic and those
model is purely mathematical and the
weights don't translate into something
that you can understand intuitively so
we can reverse engineer something like
home team advantage unless we actually
implemented it okay so I hope you're
still with me and this is how we
implemented it now we have got like our
predictions and then now how do we know
how well we're actually doing with our
algorithm and if we know how we're doing
how can we implement how can we improve
it so um when we want to figure figure
out how our algorithm performs there's
something called the error like the
errors like I said the difference
between the actual real result and what
we predicted
and so it's an epic already gives
something and has something like that
it's called the data error and all we
have to do is add a schedule to our
trainer and we say basically that every
10,000 iteration do the stuff that I
give you in the function and basically
we just lock the data error rate and if
I run this it will look like this so we
can see that there is an error rate of
0.19 this does not mean that like
nineteen percent of our predictions are
false this is a mathematical error rate
it's the mean squared error and it's
basically just a distance the errors
distant distance between our predictions
and the truth so this doesn't really
translate into anything like if people
ask me how well does it I wouldn't
perform if I same I mean squared error
is like 19 0.19 nobody knows what I'm
talking about right so I wanted to like
try to come up with a metric that is
more straightforward and that would tell
me an error rate that I can actually
understand so I was coming up with
something that's called the
classification error it's basically the
same thing where we in this case we have
an error counter and then we do a
classification actually because now
before you saw that we have a
probability distribution and now we want
to make an actual prediction from the
distribution so basically we say this is
the most likely so I'm going to predict
this and then we just count the numbers
the times that we are wrong and then bye
I'm sorry from that we want to calculate
our error rate and in this case if I run
my algorithm it would love doing
something like that you can see that the
error went up compared to the mean
squared error which is logical because
we actually make predictions and we can
either be wrong or right we don't have
like a distance anymore so we have this
data but it's we use the classification
data we use the same data that we train
or network on which is not very smart
because our
should actually predict something that
it doesn't know about yet so to figure
out how our algorithm performs with data
that it doesn't know about you basically
take our data sets and then split it in
two thirds basically we take two thirds
of the trainings of the data set to
train and then the last third we use to
classify or to Bella date or a measure
of performance so I have to speed up a
little bit and basically we do the same
thing that we did before but in this
case we just use the cross-validation
set to make our predictions instead of
training set and if we run the script it
looks something like this again our
error I'd jumped up a little bit and
also it's just like all over the place
is like super weird because it doesn't
seem to like go down silly but it just
like jumps around and so okay we know
those error rates and I'm going to skip
the slide real quick because it's just
like a summary and and the next question
would be how do we interpret this and
how can we actually improve on it and I
have to skip the sled to I'm sorry ok so
there are like a few things that I want
to try to improve the improve the
algorithm first thing is adjusting the
learning rate so we saw that the error
rate was jumping like up and down and up
and down and this could mean that we
already like reach the global minimum
and are just like wandering around it
and I just wanted to like try what would
happen if I just changed the learning
rate so this is the error rate that we
had before with the am learning rate of
0.003 and then I just changed it to 0 0
to 1 and this is what happened what you
can see is that the error it is a little
bit higher but now it like slowly goes
down and doesn't like walk around
anymore and this could mean that if I do
more iterations I will get a better like
error wait I will be closer to the
global minimum even though my steps are
smaller and in this case I just decided
to leave it at that to just show you
what what can happen
when you change the learning rate and I
wanted to try some other stuff too so
the next thing I wanted to try was to
just simply get more data and i get i
just added in to new seasons which was
100% more data and again this is the
learning rate from before the error
rates from before and these are the
error rates after i added more data and
what you can see is that between the
10,000 iteration in the 100,000th
iteration there is like a bigger
difference with more data then with less
data which means like if you train your
algorithm like a hundred thousand four
hundred thousand integrations but the
error rate doesn't change that much you
don't really have to write so this could
mean that it actually would be worth
your time to get more data and then the
last thing that I tried was to simply
add more features to give the algorithm
or inputs which would also just mean
that I'm describing the problem better
or more detailed so before I had two
features those were the market values of
the two teams and now i just added like
three more features and the positions of
the teams in the table before the match
starts and also the MH day so I'm saying
probably it does have an influence how
far into the season we are and how well
they are compared to other teams and now
if we look at the error rate right now
looks pretty good except that the
cross-validation error seems to go up at
some point but it still is performing
better than having only two features so
we could say from that that it might
make sense to put more effort into
getting more features so yeah those are
the things that I actually tried and
this is basically just prototyping stuff
just trying stuff out and then decide
what would be like worth my time the
most and there's also this thing called
regularization that like real data
scientists use but I unfortunate don't
have the time to really go that deep
into it but we can talk about it later
after the talk and okay so I'm gonna
pull up another quote so you still with
me
so this Court is most famously of
tributed niels bohr but I'm not quite
sure if he actually said that but the
chorus prediction is very difficult
especially about the future and now as
they have seen it's not even that easy
about like predicting stuff in the past
or the present and so what I want to
like talk about now that I'm coming to
the end and is how do you actually work
with machines like how do you actually
work when you're doing machine learning
and what I've learned is that you have
to work incrementally you will get there
eventually but neural networks are just
basically like your brain and practices
what makes perfect so you can't expect
to have the perfect solution like right
out of the box have to slowly get there
and you have to think about the problem
not the solution the reason to work with
neural networks is precisely so you
don't have to come up with a solution
yourself you get them giving this to a
machine and you can put your time and
effort into thinking about how to best
describe your problem and you should try
out different configuration parameters
if you start with a new and problem or
new thing then no one can tell you what
your perfect learning rate will should
be or how many layers you will need or
how to choose like what what kind of
configuration camera parameters you
should use you should just come up with
a metric that is important to you that
you can understand and just like change
stuff and try it out and then basically
see what it does here metric and then
decide which way you want to go and the
most important thing about machine
learning is it's about the data and if
you have data that is biased you will
get biased results I don't know if you
remember this one an incident where like
this face recognition software would
flag like African Americans as like Apes
and that's not because the machine is
like evil or several races it's because
the data was biased and and so you have
to be really careful what you give your
machine because it's going to learn from
this data and not anything else okay and
so I have those recommended reading
section in case you want to read up more
on machine learning
and just dive into it I hope I've like
made it clear that it's not really that
complicated to actually build something
with it even though you're not like
totally into the math part and the first
link is a link to my repository and you
can find the whole thing there like the
whole data retrieval included and how
implemented the whole like error error
recognition of error rate and pop my I'm
sorry so you can just check it out there
and then this is the little link to the
library i use synaptic and i have the i
have also linked to the course I'm think
it's not that interesting now that I'm
going to tell you about like each and
every single link I'm going to put up
the slides later I'm gonna edit to get
her I think yeah get her on Twitter and
so you can check it out there but also
there's this amazing page called coding
game so it's basically just coding for
fun and coding with your friends and
everything and they have an awesome new
machine learning section that uses
tensorflow tender flows the machine
learning library for python by google if
you don't know that and it's like really
easy to use and and you should totally
check it out okay so like I said before
I've done some predictions for the
Premier League those are the predictions
you can like take a picture right now
and then check and then you just bet
money on it and then we can just see who
made the most money tomorrow by the end
of the day or like tomorrow and then who
made the most money you should just buy
us all drinks I guess alright so that
was it thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>