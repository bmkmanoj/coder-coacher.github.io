<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>[JSConfUS 2013] Selena Deckelmann: Schema liberation with JSON and PLV8 | Coder Coacher - Coaching Coders</title><meta content="[JSConfUS 2013] Selena Deckelmann: Schema liberation with JSON and PLV8 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>[JSConfUS 2013] Selena Deckelmann: Schema liberation with JSON and PLV8</b></h2><h5 class="post__date">2013-08-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wn2SpeHhWVc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right yeah so I'm here today to talk
to you about JSON and P L v8 inside of
Postgres and I also want to mention I
like birds and so I've been touring
around Amelia Island with my husband and
so I included a lot of pictures of the
birds that we've seen in the past few
days that we've been here so pardon
pardon my my birding but I I ended up
titling this schema liberation and the
reason why I use that phrase is there's
a lot of tension between managing your
data with schema design and what DBA is
typically do and the flexibility of JSON
objects and documents and there's a
misconception I think that once you
enter the world of database schemas that
it's just all twisty maze mazes of
sequel and slow tests and really sad sad
developers doing database migrations but
Posterous actually has for a long time
supported key value stores inside the
database this thing's called H store is
something that some of you here may may
have seen or used before but even with H
store there's kind of a mismatch between
what developers actually want to use and
how the database demanded that you store
and manipulate that so there really is
this gap between what DBAs think that
you should do and what developers really
actually want to do and I I want to talk
about this idea of schema liberation
from and I base this on conversations
I've had with people about MongoDB and
no one ever said those words to me
exactly but what they really did talk a
lot about was freedom this is from the
MongoDB documentation it's just a sort
excerpt that I think sums up a lot of
what data developers really love about
document store databases I think it's
it's this flexibility it's making
database objects that very closely
resemble what it is that you're
in the application and I hear this from
developers a lot that they wish changing
their schemas wasn't so hard that they
had better working relationships with
their DBAs and that deploying these
features into the datastore was just way
easier than it is so I just want to show
you an example of what this you know
what what this typically looks like and
what it what it really would be better
as so let's say you were recording your
bird sightings just a bunch of sequel
here this is what it might look like if
you were trying to store these sightings
in a sequel database you'd create a
bunch of tables a bird table allocations
table sightings table put this data into
the database a bunch of inserts and then
you'd have a joint able to put that in
there or you know we could just store
the JSON and looking at this it's really
obvious to me which one is more friendly
to the developer and I think for a long
time
DBAs have been pushing this problem of
early optimization on to developers and
I'm hoping that having JSON in Postgres
is going to make this a lot easier and
we can you know we can just help
developers enjoy working with relational
databases a lot better so could Postgres
be as flexible as want going to be could
we make developers as happy using a
relational database as they are using
document store databases well the
ingredients to do this right now
9.3 Postgres which is out in beta
currently you can download this and try
it out today or Heroku is actually
loaded all of these tools that I'm going
to show you today in their in their
development databases so you could fire
up a Heroku database and test it out and
they have PL VA ready to use there you
can also use it with 9.2 Postgres but
you have to load an extension called
JSON enhancements in order to get all
this functionality so v8 I mean I have
this slide in here it's mostly for the
benefit of the people who aren't here
today but after Brendan's talk of course
you all know what v8 is and PL v8 is
embedding v8 inside of Postgres and you
can see a lot more about PL v8
in that link there so I'm sure many of
you deal with normalized tables in the
database so let's say we have a table
called reports let's look at what this
might what this might be like and this
is an actual table that I have to deal
with there's like 38 columns here you
see all these different data types and
things like that and it's just not very
fun to deal with so let's just convert
this to JSON so if you're gonna take an
existing scheme you today and you wanted
to pull all the data out of it very
simple thing you can do is just a select
star from reports right and that would
give you all the data but I want to
convert it to JSON so there's actually a
function in Postgres that enables you to
do this it's called row to JSON but this
this query won't actually work you would
have to use a sub select in order to
pass it to it so because wrote a JSON
expects a record type so with this right
here I would pull all of the data out of
that reports table convert it to JSON
and spit it out but what I really want
is a table
I just want to shove this all into a
table and Postgres supports this really
great syntax for taking the output from
a query and dumping it directly into a
table you don't have to specify any of
the column types because post Chris
already knows what it is
so you can just put your sequel query
right in there generate a table so
magically there we go this is what it
looks like in Postgres you end up with
this table I called it liberated feeling
liberated and then my my column name is
wrote a JSON you could change that if
you wanted but that's how it turned out
and so I did this on a development
database that I have that has a copy of
our production data it took about two
minutes it was an older dev system this
entire table fit and memory is about a
two gig table and there were 2.7 million
rows in it so you know it was it was
pretty fast it didn't take me very long
and you know there's there's a penalty
for denormalization right in terms of
the amount of disk that we're going to
consume so you can see here the first
table my original table was two gigs
the JSON table was about four so you
know right well we pay a little bit of a
price there but honestly I don't really
care I got just to burn let's just
liberate our entire database right so I
wrote a function using PL v8 to liberate
your entire database and some friends of
mine have tried this out so it actually
does work
it's terrible JavaScript I apologize not
a JavaScript developer I think my
JavaScript looks like sequel well
there's a lot of sequel in there but
anyway so you could use this today to
convert some existing database that you
had in Postgres and I was just gonna go
through this really quickly to show you
kind of how simple this really is so a
lot of this is is this ended up being
about 25 lines with all my pluses and
whatever's in there but it's really only
about four lines of JavaScript there's
quite a bit of boilerplate so the first
thing here is this create schema
liberated and all this is is a namespace
the terminology and Postgres is a little
different than like MySQL we call a
cluster a group of databases and that's
like an instance of Postgres a database
contains schemas which and then these
schemas are these namespaces that
contain tables because I wanted to shove
those all in there and be able to you
know drop them when I was done easily so
then the boilerplate around here is
again it's pretty simple to create a
replace function
I gave it a function name and I'm not
passing any arguments because I'm just
going to convert everything it's gonna
return a boolean and then the language
is POV 8 obviously and then we've got an
ask function that signals ok now we're
going to show some JavaScript in there
and then the end and I had to sequel
queries in here that were a little bit
you know a little bit complex not too
bad and this right here shows you
pulling out all of the table names from
Postgres so when you're looking at
objects inside inside there
Postgres comes with a internal schema
called PG catalog and you can query all
of that data in there just as though you
were looking at an ordinary table so
that's kind of convenient so now I've
got a list of all of my tables that I
want to convert and then I'm just going
to Ryu
this query that I wrote earlier to
create all the tables that I want
without having to specify anything about
the the the actual structure of those
other than the fact that there's there's
a report query so you go so that's it
pretty simple VAR tables I've got a for
loop I'm doing a little elog in to let
me know that you know as each table is
being processed and then I execute it my
friend Craig tried it out so yes it does
work
he was already using the JSON data type
so it was like JSON within JSON which
wasn't very good test but anyway he
tried it out and it definitely did work
on our first try so if you're thinking
about using this I want to let you know
that the JSON data type is a first-class
data type you can index it you can join
against it you can compare it there are
operators that are defined for it you
can write functions for it as I as I
just showed and you can return it the
same way you would return an integer or
a text type or any other typical
database type so you might have already
inferred that this would be the syntax
for this but you know you can just
create table give a name to the column
and then you call the column JSON and
that'll create a table for you and then
inserting data in there is very simple
just insert into table name values and
then a JSON object straight into there
and then down below I show a select when
you select from it it just outputs
straight up JSON there's quite a bit of
predefined functions that we've put in
there for for helping you look at this
stuff so they're not always the best
names
maybe it's hard for us but they're very
well documented so I've got three here
they're pretty common commonly use JSON
object field text return field of text
JSON object field which will return a
quoted value in the event that you're
looking at things that aren't text and
then JSON object keys will return all
the keys in there
and like I said you can use these
because Postgres supports functions as
indexes you can create any kind of index
you want against this compare it join it
and use it just like any other column v8
is also trusted so you don't have to be
a super user to create and run functions
using Peele via inside the database this
is not this is not true of something
like Python for example pythons and
untrusted language and same goes for a
lot of the other languages that we
support we actually support Lua
as well Louis trusted anyway so you can
you can create and and run these just as
a normal user peel v8 automatically
parses JSON input so if you're doing
something like this where you input JSON
column and you have key text you can see
right there that I just immediately was
able to use that object without running
json.parse on it
POV eight of course also supports
running raw sequel and prepared
statements you can so yeah here's just
some examples execute prepare execute we
also support cursors which is an
advantage if you've got a lot of data
and you just want to transfer you know
one one row at a time and your return
function and I just threw this up here
because it makes me laugh you know we
could not only have sequel engine CH
injection but we can also have
JavaScript injection directly in the
database now because of support for PL
v8
I don't know that you'd want to actually
run this in production I do know some
people that have written things like
this and then stored their JavaScript
directly in the database to avoid having
to load lots of functions
time after time but yeah I don't know if
this is really advisable it's kind of
hilarious though so all that stuff
that's cute you know I'm telling you the
theory of you know how you might use
this but you know what what about in
production what does this actually look
like so I to work at Mozilla although
not on anything really JavaScript
related and I helped run this thing
called crash
that's at mozilla.com and this is every
time firefox crashes which I'm sure is
very rare I we look at those and and so
we have this this tool this is basically
a data warehouse tool analyzing this and
here's a picture this was made by a
contributor it looks it shows you what
our our internal structure is like you
know all the systems that are running to
make this thing go and you can see
there's two main data stores here we've
got HBase up there in the corner and
then Postgres you know very looming
large in the middle as you can kind of
see the contributors preference for
Postgres here it's a little misleading
though because our HBase cluster is
actually about 150 terabytes and the
Postgres cluster is only about two
there's a lot more interaction you know
a lot more tools that are interacting
with Postgres than HBase but you know
anyway so I thought I'd point that out
so we've got all of this data in each
base that's about you know 70 to 75
machines there and then there's only you
know three or four systems running the
post rest database so if we go back to
this reports table which is sometimes
the bane of my existence it's you know
very large it's complex it's not very
well documented exactly and it becomes a
real pain to you know I it's it's not
too difficult for me to add columns to
this when we add new information to our
crashes but you know it's it's not it's
not the greatest either so we started
talking about this when 92 came out and
we ended up upgrading our systems last
fall to 92 and then we just created this
table has the crash UUID
which is how we identify uniquely
identify all our crashes it has the date
processed and then it just has this JSON
blob of everything in it and we put this
into production at the beginning of May
and here's a look at what this looks
like it's very similar to the depth
system that I just showed you actually
so our production reports table is about
2 gigs in size
the production raw crashes table
containing all the JSON is about five
gigs so you know we I actually had
forecasted this out that it potentially
could be quite a bit larger we don't
know what size the crash object is going
to be ahead of time there's no way to
tell but you know I had suspected it
would be somewhere you know some
somewhere around here and so anyway so
we had to upgrade all our just to
support this but now we're we're pushing
somewhere around five gigs of raw JSON
into the database and we're going to
probably double that very shortly
because we actually have in addition to
the Rock crashes we have a processed
crash that has a little more metadata in
it that we're going to shove in there so
if you look at this the the comparison
here is you know JSON at 2k per crash
and then a normalized table would be
somewhere about around 0.8 K so there's
there's a couple lessons from that like
one of course you know right we're we're
gonna burn some desk when we do this
it's also that you know good DBAs do you
actually save you some desk there's a
reason why we normalize the data but
when you're in a situation like we are
where we don't really know what kinds of
reports we want to run ahead of time
it's very useful to have immediate
access to that to that data which brings
me to what really is the killer
advantage of doing something like this
in Postgres as opposed to say HBase and
it really is this so I needed to look at
you know a single value we were we were
interested in the garbage collection and
you know is garbage collection running
when Firefox crashes and so I first
wrote a MapReduce you know we use a pig
and then there's another tool called
j-tube which is actually you can write
some Python and then run a MapReduce in
sage space so I so I wrote my initial
query and I was looking at about you
know a week to two weeks worth of data
and it literally took 30 minutes for me
to get all of that data transfer it you
know to the host system and then
process it into a report they would be
useful to somebody on Postgres took 24
seconds so that saves me you know my
time you know as a developer's finite
and if that's gonna save me 25 minutes
for every time I need to run an ad-hoc
report it's totally worth it to chew up
all the disk so that that's really been
my motivation to moving this into
Postgres we're still going to use HBase
for storing the we've got these these
dumps these binary dumps of data that
are a little bit large you know they can
be they can be quite large so we're not
going to store those and Postgres but
all the metadata all of the JSON that
we're storing we're just gonna pull pull
it all in in the in the coming weeks so
why would you use Postgres just to sum
up this stuff why would you use Postgres
to store your JSON and as opposed to a
new sequel system well I think the the
most compelling thing especially for
those of you who are running reporting
systems and doing visualizations of data
and I think there's you know there's
quite a bit of that going on right now I
like to call him I started calling this
bulk bag schema design it's something I
got from will lean Weber who got it from
some comedy podcasts but I think it it
really describes what you're trying to
do you just want to throw everything
into an unstructured data store
initially and then have the option to
optimize it over time initially
especially in the prototyping stage you
don't really know what your reports are
going to look like you don't really know
how you want the data to be stored and
that premature optimization I think is
is a silly and unnecessary hurdle to
develop it at this point in time
if you're using Postgres you're really
it's gonna be easy for you to scale to
multi terabyte database instances and
particularly for these these write and
read heavy loads having to do with
reporting like our workload is is mostly
right and Postgres does a really great
job of handling that my one caveat on
this is that if you're if you're scaling
to multi terabytes non cloud storage is
probably a better option particularly
for the reporting workloads there are
some great examples of people using the
cloud for with Postgres and I think
Instagram is probably the best example
of that and they've they've published a
few different talks about how they're
managing that but it is it is pretty
easy to scale these these individual
post rest instances to multiple
terabytes and finally I think
post-arrest is a really great option
because you can manage your data with
the language that you already know and
love which has not really always been an
option right previously people would you
know you'd either have to write raw
sequel or you would have to use a
language like peel PG SQL and I think
that being able to write JavaScript
inside the database makes it far more
maintainable I think it makes it a lot
more likely that the developers that I'm
working with will actually write data
processing code as opposed to leaving
that to the the database experts and
then also being able to put that code
right next to the data inside the
database makes it a lot more efficient
because you're not having to transfer it
over the network to process it
so like I said 9.3 beta it's out today
you can download it and play with it all
these tools that I was talking about
they're also available if if you want to
like create a developer instance in
Heroku you can play around with it and
it works really awesome and if you want
to know a little more about the types of
features I just listed like a bunch of
things here I'm not gonna like go into
you know all the cool things about
Postgres but I just ripped most of this
from a slide from from Craig there are
so many things about Postgres like I've
been talking with quite a few people
here at the conference about the JSON
data type and I've had more than one
person say why didn't you tell me about
this last year I have had to I've had to
I've had to build things in the last
year that really would have this this
really would have helped me out and I
think that Postgres actually has a lot
of those types of features where you're
like oh wow like that would be really
great but anyway I made a list here
definitely definitely check it out and
you know feel free to ask me questions
and so liberate your data let me let me
help you liberate your schema today
Thanks
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>