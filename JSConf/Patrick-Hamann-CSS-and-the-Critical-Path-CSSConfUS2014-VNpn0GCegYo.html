<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Patrick Hamann: CSS and the Critical Path [CSSConfUS2014] | Coder Coacher - Coaching Coders</title><meta content="Patrick Hamann: CSS and the Critical Path [CSSConfUS2014] - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Patrick Hamann: CSS and the Critical Path [CSSConfUS2014]</b></h2><h5 class="post__date">2014-07-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VNpn0GCegYo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so fuzzy thank you for having me
pictures you inviting me Nicole Leah
Chris this has been an amazing
conference I've been amazed by the
location and all of the guests and all
the talks have been having so thank you
so I'm Patrick Hammond as Jen mentioned
you can get me on twitter there and i'm
here to talk to you today about CSS and
the critical path now what does that
mean i don't really know I hope you'll
know what that means by the end of this
talk so as done mentioned it's a talker
it's really a bit of a ruse I'm here CSS
confer I'm actually here not really to
talk to you about CSS but more about
performance and importance but
importantly how CSS is intertwined and
tightly coupled with our critical path
the path from which a browser takes to
render to our screen so have you haven't
guessed already a work for the Guardian
it's a national UK newspaper which is
now a international media organization
we have offices in New York Australia
and London and and I work on the cool
web team there which were helping to
make the next generation of the Guardian
website we've spent the last 18 months
completely re-engineering from scratch
the Guardian website used to be before
horrible large Java and monolithic
application and we're now moving to a
mobile-first responsive website as Brad
force one scrape coined the phrase
planting the responsive seed it's very
hard for large organizations to shift to
responsive so actually we kind of took
tiptoed along the way we first replace
our end sub domain with a responsive
site which it's going to become the
platform on the basis to take eventually
hopefully this summer we're going to
completely remove the old site and have
one single platform and so today I'm
going to talk to you about some of the
lessons I've learnt building this last
month we broke a record at the Guardian
we had 100 million unique users within
the first month that makes us now one of
the largest new yours I license in the
world but on the flip side of that those
100 million are being made up of
accessing our site from over 6,000
different devices and our current
platform
the old site is there for 10 years
wasn't designed they can't cater for
this array of devices and it's just not
acceptable what we are serving to these
devices and we had to do something about
that but more importantly the site is
extremely slow so here you'll see on the
left-hand side our old site and the
right-hand side our new site and the
difference in the rendering speed
between the two of them just hit see how
how what you think yourself so within
three seconds we've finished painting
and rendering and freely complete
loading of the whole site whilst we're
still chugging along on the outside and
that that's a median average it takes 12
seconds to paint and finish loading of
our website to the user now I just don't
think that's acceptable and I hope you
guys don't too but you know it's not
just us at the Guardian see this problem
use the low time expectations us as uses
our expectation of how fast a web site
takes a render is reducing you know from
2,000 it was nine seconds all the way
until 2012 it's just two seconds that
your users expect your website to load
in and again the problems not just ours
this is for taken from mates be archived
or great Google initiative that they
index data from the top two hundred
thousand websites and I ran this query
on big queries last month to ask ask it
hey what is the speed index I eat the
speed index is a metric that is in
milliseconds how fast your site takes
the fully render and the top ten
thousand websites the median average is
nine thousand milliseconds right nine
seconds whereas we know that our users
want it to load in under two can you see
where I'm going here with this so but
the Guardian you know how we have to
relate this back to our business so when
we are starting out on the new projects
we surveyed 3,000 of our users and ask
them about 17 core KPIs product
indicators of which one of them was
speed and unsurprisingly to me those
users rated speed as a feature the
second most important thing only after
easy to find content you'd hope on a
news website that the content is easy to
find although that did surprise the
business though but it was great for me
because
allowed as a leverage to really drill
into them that we need to make this new
platform fast and it's not just that as
well there's have been a lot of HCI
research recently that has gone into the
fact that the cognitive levels of us as
users how how we perceive speed on
machines this is taken from Illig agaric
high-performance browser networking he
works on the Google make web faster team
it's a strongly advise you read that
book but here it says that for a mission
for us as uses for field that a machine
is actually functioning and working any
action you perform must respond within a
thousand milliseconds 1000 milliseconds
and that's not just you can't just
relate that to websites you can relate
that at home I'm sitting on the couch on
watching TV I change the TV channel I
want that TV that TV needs to respond
within a thousand milliseconds for me to
deem the machine working and after 10
seconds but my mental subconscious just
moves on and that task is completely
abandoned and so Tim cowlicks been
talking about this a lot recent so you
talked so now we know that a thousand
milliseconds is what we're trying to
reach our website should render within a
thousand milliseconds what if we were to
set a budget to enforce that in our
development workflow every new feature
every new page we make has to hit that
budget so that's exactly what we did at
the Guardian we've set ourselves a
budget of ensuring that all pages render
within a thousand milliseconds no longer
we can think in seconds as web
developers we really need to start
thinking in milliseconds so let's dive a
little bit deeper into this for us to be
able to understand how we can paint
within a thousand milliseconds we need
to understand what's going on in behind
the scenes how the browser goes from
that and you clicking on that link to
being able to paint on the screen but
also knowing how the browser works makes
us as web developers make better
decisions when we build our web
applications so hopefully you will see
one of these before this is a waterfall
chart taken from web page test as a
segment explaining the network each bit
of the network's made up the DNS lookup
so converting the nice the Guardian com
string into an IP address we then can go
the computer can use that IP address
again make a tee
TP socket connection a freeway handshake
and then finally I can make the actual
Haiti request i can say hey give me
index dot HTML and when the server deals
of that it responds and finally we can
download the content and so all of that
has to happen before we can even start
creating a Dom but our physical tavera
files a millisecond barrier 600
milliseconds of that on average is
already taken up this is on a free
average 3g mobile connection 600 of that
is already taken up by latency we've
heard this before latency is the biggest
killer to performance and especially on
mobile so every network you request you
make you're incurring the 600
milliseconds and so that leaves us with
only 400 milliseconds to really paint to
the screen and now you start to think of
how important it is to rendering so this
is being very simplified but this is
what the critical rendering path is this
is what your browser has to go through
to be able to paint for to the screen so
let's look a bit deeper than that so I
asked for my index page I perform the
network on it incur the 600 milliseconds
once I've got the HTML fortunately i can
start parsing the HTML and creating a
Dom structure now one of the greatest
things about the HTML spec is that a
dumb construction and hates them are
pausing can happen incrementally so we
don't have to wait for the whole file to
be downloaded we can actually start
creating the Dom as the packets come in
and this is amazing feature so I'm
sitting there I packets come in I'm
constructing a DOM and boom my browser
hits a script or a link element it has
to stop it can't construct the Dom
anymore had to stop blocking it has to
go back and perform the networking to go
and get that CSS file so I download the
CSS file to create the CSS object model
now the CSS object model and the Dom
together create a render tree said
another tree like structure that has all
of the information about the nodes on
your page from the top left to bottom
right and but the due to the cascading
nature of CSS we can't parse the CSS
incrementally like we did with the HTML
file because of the Cascade so styles at
the top of the page might be overwritten
further down in a media query or
something like that so and that would
actually constantly stop painting and
flashing to the screen and causing
reflows so you know actually be a
negative user experience so we have
to wait for the whole CSS file to be
downloaded before we can construct the
CSS object model and then our lovely
friend and foe JavaScript now if we find
a script element JavaScript blocks Dom
construction because of that that great
feature document dot write JavaScript
can alter the dong and so because of
that the Dom can't continue to pass but
javascript is tightly coupled with CSS
because this year it has to block and
wait for the CSS object model to be
created because what is javascript was
the query is style on a node and this is
why the age-old best practice of putting
JavaScript to the bottom or even more
importantly declaring it as a sink your
you're making a promise with the browser
saying I know this javascript is not
going to do need anything dodgy you you
can just take it off the critical path
and but it's really interesting this
relationship it's into tying tightly
coupled and I ask this in an interview
quite a lot to people I'm interviewing
so yeah so yeah yeah I put job descrip
to the bottom of page why deeper
JavaScript oh because it looks why does
it block and they never know it's
surprising how many people don't
actually know what is the actual reason
why javascript blocked our browser
rendering so we've done the good thing
we've taken javascript off the only
thing we're left with is the CSS object
model we need that and so finally we can
then perform layout and paint and you
can see here how this is all traditional
websites look like this we've got our
HTML within download our CSS and a
transcript if we make the JavaScript
asynchronous take it off the critical
path now we've already shaved off a lot
of milliseconds there to be able to
paint so we know we have to get CSS down
as soon as possible it should be the
only thing on your critical path and in
my opinion it is the critical path but
how can we optimize our CSS for this
start to answer ask that question what
actually is your critical CSS and
especially on a responsive website you
know that concept of above the fold no
longer exists something that is above
the fold and the mobile might not be
above the fold on your desktop so here's
which guard in article page is my
critical CSS the sharing widget or the
popular content or the comments none of
this is only the article the user came
there to read the news that is the only
critical CSS that you require to paint
to that screen and it should be the only
thing so we take that one step further
at the Guardian that you know we
actually ajax in and progressively
hearts with the rest of the initial
payload actually only has the HDH ml for
the article in it this is our non
critical CSS verses are non critical and
so Angelina just up before me and
started to allude to this fact of so we
now know what our critical CSS is but
what if we were to inline that CSS into
the head of the document knowing what we
just we just learnt from browser
rendering then the browser has
everything it needs to be able to create
the render tree within one HTTP request
everything we don't need to wait that
incur that 600 milliseconds again to go
and download the rest of the CSS file
all of our styles the critical ones were
above the full content are delivered in
the first payload now we know it's very
controversial but that's exactly what we
started to do at the Guardian so we've
got the styling them at the top this
goes against everything we've been
taught as good web developers you know
separation of concerns styles and CSS
behavior in JavaScript files and
completely separate but if you want your
page to paint within a thousand
milliseconds this is what you have to do
so if you go to the Guardian be decide
right now you'll find this disgustingly
an ugly blob of JavaScript to the top
compressed into 14 K so if you've got a
full TCP connection window open you'll
have enough information within the first
packet for the browser to render to the
screen so that's enough about what is
that actually like to the user so the
top doesn't have any in line the bottom
does we've rendered in 600 milliseconds
there and we basically nearly finished
painting and the one without hasn't even
started
but for their though that observant ones
of you in the in the audience would
notice that I still have to load the
rest of them right I've got my critical
styles but I need to be able to load the
rest of them in and so we now at the
bottom of the page we have to a sink in
the JavaScript and then create a style
element and inject it into the head
therefore completely taking the rest of
those styles off the critical part even
more observant in the audience with we
start to thinking about that ever hang
on a second if you do a sink in the rest
of your global styles isn't that going
to cause a massive ugly reflow or
repaint once those new styles come in
and you're in fact correct that's
exactly what happens and once we first
implemented this a couple a week later
I'm a great director of The Guardian
came walking in and said what the hell
is this ugly flash on my website and he
was right you know this it wasn't
acceptable that the users was yes we
managed to if we optimize it well enough
you know they wouldn't see it but if
they started scrolling soon enough
they'd see that horrible paint instantly
so the only way to get around this is
that we obviously have to manage that
somehow we have to cash it and so we
looked to local storage to act as a
cache that what if we could once we've a
synced all of that CSS in we could then
cash it in the users local storage and
the next time round we have all of the
information within the first request to
be able to paint to the screen avoiding
any paint any flash of unstyled content
so first we we asked if the browser
modern enough the BBC's cut the mustard
test you know does it have XHTML OHV
object does it have event listener if
not no just user normal non word link
and but is there already stars in local
storage yes great there's paint if not
do they have local space storage now
this is an interesting one we found that
a large percent is an especially iphone
users have a quota exceeded already on
there on the local storage if so
requested with Ajax cash it in the local
storage and then paint again and then on
the next time round we can instantly
just go straight to instant painting
with all of our Styles not making a
single request to to a stylesheet so
here we've just a on to our top back in
the head
now check to see if this the storage
items are there if it is inject it into
a style node there instantly all of your
styles within the first hey tbo press
and enough to deliver the news to the
user within a thousand milliseconds
again now down the bottom so we've now
we've done our normal age acting but we
now have to store it so here we're
actually doing a bit of cache
invalidation for free at the same time
here if they've managed to get into this
block it means that they're either this
the style is outdated or they don't have
any so we first loop through all of the
local storage items to check if they
match against our pattern if they do we
clear it and then load in the new one
and store it again so we're using an md5
hash here of the file so every time we
practice continued continuous delivery
with deploying about four or five times
a day now and so instantly we can
invalidate that CSS the old global to
assessing get the new ones straight away
but what does this mean since actually
again to our users so here we're using
web page test here this is a UK cable
connection and so before we've got we're
still blocking rendering waiting for all
of our CSS and I Josh get to download we
async the JavaScript and then we in line
the CSS and cash it in local storage
instantly look at the difference to the
waterfall here we've gotta start render
event right there even though we're a
sinking in the rest the bounders doesn't
need it because it's got its inline
Styles here we've managed to shave over
500 milliseconds off I start render
event just by inlining the CSS from
cashing in local storage breaking our
1000 millisecond so again if you go and
look at the site now you'll notice that
in the style sheet tab in any inspector
you won't see any physical files
downloaded but if you go and check the
local storage you've got your CSS right
there interestingly we also do the same
with our fonts to avoid any flash of
unstyled text with the front loading now
as well as having fast papers this also
creates a much more resilient system a
system in which I'm traveling on the
train a friend of mine shares
82 a cool article on twitter i click on
it but boom or go into a tunnel now
fortunately my my mobile had enough
networking to perform the network
requests that first HTML press but but
normally I'd go into the tunnel and then
it would find the CSS file and try and
make the downloading for that but you
can't because you're in the tunnel and
that's that big white blank screen that
we're all so familiar about with this
technique we've created a Sicilian to
failure that I only need to make that
one request and I have all of the
information Google did to do this google
and bing have also do this you know it's
why i repeat one of Google's search
results pages the fastest paid in the
world what they do is actually more
interestingly send down all of the
styles in the first request and then
they put it in local storage but they
set a cookie so that request it again
any other knows all the subsequent
requests after that have already got all
of the slides we can't actually do that
at garden because we keep a completely
stateless domain so we don't use any
cookie so that we can patch our HTML on
so with all the performance techniques
you have must monitor these and optimize
and repeat so to do this I built a grunt
task that in the CI now I'm monitoring
the size of those of those hedge files
and so whenever a developer updates the
size of those files I can get alerted to
see if it goes too big because obviously
we don't want to go in the negative
opposite way that the file gets too big
and then we're actually bloating our
pages and slowing them down so in the CI
we monitor these and I now know the raw
and the gzip file size and it gives me
and I can trend this over time and set
and a threshold alert knowing when our
assets get too big again on in pool
repair is this is a big code smell to me
when someone's adding something to the
head file you have to ask them is that
your critical CSS does that feature
really deemed to be in the head and so
monitoring assets like that is all well
and good but actually what a real users
seeing and especially across the world
so today is we use a tool put speed
curve and this this uses clusters or web
page test instances around the world and
we can
it tracks and trends analysis for us but
I'm like I can filter this by our start
render event or our speed index and I
instantly know how our page is
performing over the last two weeks but
more importantly again I can set
thresholds on this and compare my sense
against our own competitor which is our
old site or even our competitors and
business competitors in the world if you
have tools like this is really really
important that you then facility spread
this around the department so now every
week all of our everyone in the
department from our project managers to
our design is get this report telling us
how fast the website is and if we've
broken anything and this is really
really important making everyone in the
team accountable for performance is the
only way that you can ensure that you
maintain that and it's been really
interesting having product managers come
up to us and say hey what happened last
week why have we slowed down by 100
milliseconds and it's great to see them
actually caring about that getting
everyone involved now I've talked a lot
about our technique but what are some of
the alternatives out there Google
PageSpeed module for Apache or genetics
has a feature to be able to in line on
the fly so they actually render the page
work out what the inline styles are and
inject them into your page so this is at
a proxy level so you can sit sit this
engine X proxy in front of your systems
now there's reasons why i love PageSpeed
it does a lot of good optimizations but
the specific inlining CSS feature
there's some reasons why I don't like it
i like have to have a lot more fine
granular control level being in the head
or not uncie SS and grunt uncie SS task
by addy Osmani this you can pass it a
multiple files on your website and it
will actually work out what is the CSS
required for this nicolette nicole
earlier on mentioned that she did all
the great work to optimize the CSS file
but it was actually bootstrap in the end
that was weighing up fifty percent with
this as a as a post optimization her
worries where she doesn't want to delete
some of bootstrap because it would make
upgrading in the future if you have this
as a post optimization you can get it to
strip out your see
test far and you just have a new
minified one that only has the bits of
bootstrap in it that you are using
across your core templates basket Jas
again by our owes money this uses
exactly the same pattern of using local
storage as a cash management in a
validation layer but all of this seems
like a hack we shouldn't be having to do
this as their developers take these
extreme things breaking everything that
we've learnt to be able to to get that
last couple of milliseconds out of our
performance fortunately there's been a
lot of thinking around this area
recently so HTTP two which is the HP the
underlying transfer protocol layer of
the web we're actually carrying 1.1 that
respect in 1997 it's now 2014 and we
finally got a new point a major version
this is actually near to finishing of
the standard it will be finished by the
end of this year but how does this
relate back to what I've mentioned is
there's a great feature in it called
server push now hcp to the biggest
benefit to us and as we know latency is
the biggest killer how do we reduce
those round trips and the latency is
that you can have multiple
bi-directional messages on the same TTP
connection so for my HTML file my CSS
file on my javascript file I don't have
to open up three different TCP
connections I can send them all down and
currently down the same socket but we've
got this traditional flow right we ask
for the HTML file the server gives it
back your browser then parses the HTML
finds the CSS file and then goes and
request it again but as web developers
we know that the browser is about to
then request the CSS file or your core
javascript file so knowing those
decisions what if we could push them
down with the initial HTTP request so
this is a new feature put server push
that completely forgets the need of any
of our techniques here because I can say
hey browser when it when you ask for the
initial HTML file you can say hey and
here's all of my other core assets I
know you're going to need them and let
the browser do the cache invalidation or
work out of it Nathan or not service
workers now this to me hb2 and service
worker among one of
the most important features coming to
the standards that we're going to have
within these next five to ten years
service work it's being worked on by
alice russell and from the chrome team
and Jake Archibald and they actually
interestingly really the ping the whole
spec out in the open and github I
strongly advise you to go and check it
out but so service worker is a request
handling with a persistent cash so you
can create a worker level that handles
all of the requests was in your browser
and deem whether or not you want to go
back to the network or return something
from the cash so basically it's app
cache on steroids or the better app
cache and so here's the traditional
model again I request my CSS file and
then we go off to the server to go and
get it but what did the serviceworker
already had one in its cache or you know
we were using local storage abusing
local storage for this pattern hit now
we actually have a new cache object in
CSS and service workers that have the
native new promises spec built in it
said right if I've got that in my cache
object return it and so now we don't
have to ever even talked to the server
again having all of the information the
CSS to be able to paint within our
thousand milliseconds so just to
summarize that you can really really
fast and here are some of the things
that I've learned along the way and that
I hope you do take away from this please
have a think about in lining your
critical CSS and deferring all
non-critical assets the your critical
rendering path is the most important
thing to getting your content in front
of your users and anything you put in
front of it it's going to block them and
as we know that 600 round millisecond
round trip for every request it's just
bolting on to your millisecond budget
where possible always in cash
aggressively and maybe in local storage
or hopefully in the future and service
worker and set a performance budget on
your sites but once you've got that you
need to always measure optimize and
repeat
you know our websites gets low on a
day-by-day basis as we add features to
them you need to constantly be measuring
that or thinking about when I add a new
feature do we actually need this feature
or how can I better optimize this
feature make it fast before we include
it or is this feature going to detriment
I'll are loading or the performance of
other features but most importantly
feature performance is a requirement
it's not a feature you can't retrofit it
you can't part oh don't worry the ops
team are going to handle it we all as
web developers must be worrying about
this and as I said before if the more
you make this information available to
the rest of your business the more the
rest of your business will start caring
thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>