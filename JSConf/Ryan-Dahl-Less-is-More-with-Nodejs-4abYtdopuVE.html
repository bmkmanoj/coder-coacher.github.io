<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ryan Dahl: Less is More with Node.js | Coder Coacher - Coaching Coders</title><meta content="Ryan Dahl: Less is More with Node.js - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ryan Dahl: Less is More with Node.js</b></h2><h5 class="post__date">2013-01-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4abYtdopuVE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">ah
I'm going to talk about node
unfortunately I'm not going to give so
much of an introduction to it because I
kind of give a rather long one a while
ago and there's this video on the
internet so I kind of figured like if
you want to know about it you could you
could watch that that video and I don't
really want to waste your time so much
so here's my introduction slide node is
a server-side JavaScript platform it's
built on v8 and it's really good at
handling lots of i/o at the same time
and not necessarily like lots of server
connections it does that fine but also
lots of different types of i/o like
standard input and standard output and a
UNIX socket and the TCP socket and vials
and just kind of all of this different
IO all at the same time and it does this
in a way that's not at all new it's what
twisted and what event machine do which
is making the network i/o non-blocking
and using an event loop yeah and the
file i/o is is asynchronous which is a
little bit new so it has a built in
thread pool so that whenever you call
like some blocking system call on on
some file which system calls usually are
on actual hard drive files it actually
goes to a thread pool so you you kind of
get this complete non blocking
environment and so just a very simple
demonstration this is a little program
that listens starts a web server and
starts a TCP server at the same time and
the web server is listening on port 8000
and the the TCP server is listening on
port 8000 and 1 and when you connect to
the web server you get hello world and
when you connect to the TCP server it
tells you how many people have connected
to the web server
just just an example that you can throw
multiple lots of different types of io
into the same program rather easily
right so here's a here's a reference to
a the other video so I want to talk
about speed actually in node because
it's it's pretty important like
everything is speed actually if the
program is not fast then it's it's
basically worthless and I think it's
important to make an environment like
this that is supposed to be for writing
servers and stuff as fast as it possibly
can be so what I want to do is is try to
describe to you how node is fast and
maybe some things that are not fast
about it just kind of put a realistic
perspective on you know what is this
there's no thing and really how fast is
it and so to do that I'll give some a
couple of benchmarks and I'll benchmark
against engine X which is a web server
written in C and is really really quite
fast and maybe there's the possibility
of a faster web server but I think we
can take engine X as kind of like the
baseline this is what is possible and
you probably can't get much better than
that and then we'll we'll benchmark node
and then we'll benchmark to web servers
for dynamic languages which is tornado
and then tornados Python thing and thin
is a ruby thing and I just did it on
this on this laptop these these
benchmarks aren't really meant to be
real necessary
it's not just trying to simulate like
actual what would happen on a website
because basically we're going to hit it
with more load than is achievable on a
single like Network device all four of
these web servers can saturate your
bandwidth right there they're all quite
fast what we're interested in is is more
about how they perform under very high
load levels and just see if we can bring
out any problems with with them so let's
let's just start with the standard like
hello world web server and what we'll do
is is we'll just hit it we'll hit each
of these with Apache bench and look at
how they differ with different amounts
of concurrency so hopefully you can see
this I'm not sure if it okay looks
visible um so so here's here's the the
the three control servers right nginx is
on the bottom okay wait so so
concurrency is is the horizontal axis
that's how many people are connecting at
the same time and the vertical axis is
response time so you want to be fast you
want to be lower down on the graph so
smaller is better so so engine X looks
really good it's at the bottom as
expected since somewhere in the middle
and and tornadoes a bit slower again the
actual numbers here aren't really so
important because they're probably not
you probably can't achieve this sort of
load actually in real life but the point
is engine X is quite a bit faster than
than the other ones by a factor of two
or three
so if we add note into there then it
looks like this which is good I would
say it's not really comparable to engine
X but it's it's looking good when
compared to thin yep
so now let's let's let's do another
benchmark and instead of just sending a
hello world response what we can do is
is fix our concurrency and and vary the
response size so you know send a five
kilobyte response send a ten kilobyte
response and see how the web servers
deal with with these different amounts
of throughput basically and so so well
ignore that bottom line here's oh wait
okay yeah so so basically this is this
is what the node webserver would do this
is for 16 kilobytes so you you just
generate some some string of data and
then you you when somebody connects you
write that string to the socket so this
would be 4 for 16 kilobytes but we're
going to vary the amount of response
size okay so so again here's the control
servers and again response time is on
the vertical axis and nginx looks really
good and it's it's approximately the
same as what we saw before you know thin
is in the middle tornadoes a little bit
slower and engine X is at the bottom so
let's let's add node into this picture
remember here this is 2 to the 18 so
that's like 256 kilobytes or something
like that and this is you know your
basic hello world program so you know
they slow down as you as you as they
send larger responses so let's let's add
note into this picture and I'll let you
grasp this for a second this is not good
that's a bad spike that's not a good
spike down here they look okay we can't
really see but it's pretty similar but
as we start getting into large response
sizes note sucks right node really sucks
your your that was like three seconds
well over three seconds four for a 256
kilobyte response that it's terrible
it's not rails terrible right I mean
this this is at 300 concurrent users at
the same time okay rails would be off
the map here but it's not it's it's a
different order of magnitude than these
other servers they're doing something
right and nodes doing something wrong so
so what's what's actually happening here
well v8 the the the virtual machine that
node runs on has this is very
complicated and has this generational
garbage collector so it moves data
around inside of its own heap and so v8
won't actually give you a pointer to the
data like the actual data of a string
you have to copy the data out of v8 into
your own heap outside of the vehicle and
then you can write it to socket so every
time node writes something to to a
socket it copies it out of v8 and then
copies it to the socket so I think in
the other web servers they can take
strings and write them directly to the
socket so to address this problem I've
added a buffer object to node which is
very simple it's just it's a chunk of
memory it's a chunk of memory that's
outside of v8s heap that you can modify
and write directly to the socket
and so this is what what the example
would look like with a with a buffer you
know it's it's binary data so it doesn't
look very string like but you can fill
it with data so so here where we create
a 16 kilobyte buffer and then we write
it to the socket so let's look at the
benchmark with the buffer now things
look ok actually things look really ok
over here with with the buffer note is
basically matching engine x4 speed which
is which is pretty impressive
considering that you know engine X is
this highly optimized C program and node
is this completely dynamic you know
things sitting written and java script
written sitting on top of this virtual
machine so so that's good yeah so so so
these buffers are pretty fast like node
can push these these buffers directly to
the socket and and it can it can do it
very well so if we just go back here and
we just took like a cross-section right
here at like the 256 kilobyte case and
and just looked at a histogram of of
these response times here just to get an
idea of how this looks now response time
it's a histogram so the response time is
on the on the horizontal axis here so
you want to be over there that's fast
this is slow and Engine X has this this
great spike here meaning it's fast and
node has a larger variance which is not
so good but it's it's getting in very
fast responses it's also getting in some
slow responses to the slow end is is
equivalent to the fast end of thin and
tornadoes kind of out there
so but the fact remains I mean if you
just took the hello world program that's
on the node website and and you you know
adapted that to write out a large string
to the socket it's going to be slow in
and it's kind of unacceptably slow
so hopefully this this can be fixed in
the future you know what what might have
to happen is is we might have to modify
v8 so that you know if v8 doesn't want
to give us a pointer to the data inside
their heap which is completely
reasonable since they're moving around
at all that they're moving around these
objects you know maybe we can give the
right system call to v8 and say hey v8
here's this file descriptor can you
write that string to the socket and just
let it deal with with its own memory
issues so that that might be a way to
solve the problem anyway so the this
buffer it's pretty simple and it's one
of the new features in node it's what
you would expect it's just a very simple
C buffer you can you can allocate it and
you can adjust the values and you can
encode strings into it and then decode
strings out of it so if you actually
have like a big file that you want to
write to the socket it's probably good
to allocate a buffer for it write the
file into the buffer using this and then
write that buffer to the socket many
many times so like for a static file
server that would definitely be the way
to go importantly you can't resize
buffers though so they're not completely
string like you can't just kind of
append values to them yeah Chris chris
has been talking about having some like
higher abstraction on top of buffers
which would be totally possible where
you could you know just depends strings
and stuff but under the underneath it
would be it would be buffers that would
be possible we'll see how it turns out
right and so since since the 1.90
release of node which was a couple weeks
ago note is is mostly written in
JavaScript now it used to be back when I
when I gave the talk last November that
that most of node was was kind of C and
it just kind of had this this thin
binding layer and so most of the
algorithms that node used to push data
to the socket and and all of the stuff
that I was doing was basically opaque
because it was kind of inside this the C
realm that people don't really want to
touch and it's it's hard to modify I
mean C's sees a hard language to just
manipulate and so now now most the the
the binding level is much lower
it's basically at the POSIX layer where
you just kind of bind certain calls and
then the rest is written in JavaScript
and this is working out really well so
so part of the rewrite was was to get
this buffer thing that we needed because
getting strings out of v8 was so slow
and also just because you know strings
aren't really an appropriate data type
in JavaScript for binary data I mean you
can pack arbitrary binary data into
strings but it's just somehow it's it's
not right for that but the other part of
the rewrite in addition to just kind of
making things more JavaScript friendly
was was to unify this concept of
strength streams so node had all these
all these different objects which which
would stream data but they they they had
different interfaces so like you had a
HTTP request object and it emitted this
body event you know if somebody's
uploading a movie to you it's a body
body body and you'd get various chunks
of the body and for HTTP response object
you would have this send body function
so you could you could send stream data
back to the to the user and a TCP socket
had a receive method so when you got
when
you got data on the TCP sockets they
receive receive receipt and then it had
a send method to send data you know it's
like the POSIX calls that made sense at
the time
and and standard i/o had like this data
event when you read stuff from from
standard in and it had this this write
event to write to standard out and you
know kind of very slowly in my very like
sloth-like
mental capacity like I came to realize
like huh maybe maybe I should like not
be making up new names for all of these
different things every time we came up
with a new stream and the more I thought
about it again very slowly that like
unifying these streams would be very
cool because if we did that we could we
could actually do polymorphism like we
could we could write general-purpose
functions that dealt with streams that
could like do things in particular we
could write like a general-purpose
pumping function that would take data
from one stream and pump it into the
other one and we could actually do this
in a really good way with all the proper
throttling and stuff and it could be all
on one event loop and we could just get
a callback when we're done with it so
maybe you somebody's uploading a file
and you just pump it to the standardout
if you want to see what it is or you
pump it to a file or you pump a file
into a into a response pumping data
throughout a process is a very common
thing and it would be nice if we could
just put it into a single function
basically so the the stream interface
which I will describe to you is split
into two parts it's readable and
writeable streams some streams are both
readable and writeable they're duplex so
like a like a TCP socket so the readable
stream looks like this it has a data
event you receive data and it says data
data every time it's
it's continually pulling in data and
when you get an EO F or a fin packet on
TCP or whatever if maybe it's a fake
stream maybe it's maybe it's an HTTP
request in pipelines HTTP connections so
it's not actually the end of something
but maybe you have an end event when
this data terminates and then you have
you have some things for throttling so
you can pause that you know stop I can't
deal with more data right now I have to
do some other stuff and resume and then
a destroy method which would just kind
of terminate everything it would close
the underlying socket it would say okay
I can't I don't want any more events
right now and they're the writable
stream has a right method basically it
has a right method and an end method so
you write data and then you end it and
then for for the purposes of of you know
the the the kernel right buffer might
fill up and at that point the right
method is supposed to return false and
then you would listen for a drain event
so you write all this data it all fills
up and you say you know I can't the
kernel can't handle pushing out more
data to the interface right now and so
you just wait for that to drain down and
then you get this drain event and then
you can fill up the the buffer again so
I mean with with with this drain and
this read this pause in this resume you
can actually you know take data from
here and pump it into there and then
when this one returns false from right
no because the the the buffer is filled
up then you say pause okay I can't
handle any more over here and you know
wait till wait till this data flows out
here you know if you're pumping
something from a very fast connection
into a very slow connection this is
likely that will happen so you need to
be able to handle the these sort of
throttling issues mm-hmm yeah so just as
an example there's lots of these streams
inside notes so like standard input
would
readable stream and a server request
would be a readable stream like
somebody's uploading a movie to you that
would be something readable and and if
you launch a child process the pipe into
that child process to the child
processes standard out that would be a
readable stream right because the child
processes writing stuff and and then the
opposite for the writable streams yeah
the stream interface I mean it I think
it needs to grow a bit organically
there's probably more that needs to be
added so for example there probably
needs to be some sort of low watermark
for for writable streams to to say when
when they should when they should
actually drain so that so that they
don't push that it out to the socket
immediately okay so what else is
happening so for rather technical
reasons node had this you DNS library
that used to do asynchronous DNS lookups
DNS is extremely complicated like
there's no way I could possibly write my
own DNS library I I would like to but
it's it's infeasible so I was using this
you DNS and that's been replaced with
with a similar library called see Aires
and when we did this rewrite of these
streaming interfaces we used to use new
TLS for the for the in the C code but
but now we're going to switch it out
with open SSL which is a bit more
available and you know usually you have
open SSL installed on your computer
whereas you don't usually have new TLS
but the side effect is that these to
these new TLS and you DNS I've rent they
they were both LGPL libraries and with
this replacement
all of the libraries that know'd uses
and note itself is is rather simply
licensed this last sentence isn't
correct 100% MIT BSD I mean open SSL has
this Apache sort of license but it's
it's rather ok and open SSL is kind of
this big understood library so this is
only good if you're like some company
and you like can't touch GPL code at all
well now note is pretty free from that
another thing that that was done
recently was was a repo library which we
had already but you could only do it on
standard i/o now you can do the repo
library on arbitrary sockets so you
could start a TCP server and put the
repo on the TCP server so you could tell
that into your process and examine
what's doing but that's a bit dangerous
you probably rather want to open a UNIX
socket and telnet into that UNIX socket
but this is cool right you can have a
running web server and you can tell that
into it and inspect variables while it's
running and it can completely demonize
and everything and and be detached from
a TTY so yeah here's here's an example
of of starting the repo on a on a UNIX
socket it's pretty trivial just copy
those three lines of code into your
program and then there was this other
thing this promise dot wait so node had
this thing where when you did like a
file operation you it returned a promise
and then I had this this weird thing
where you could wait on that and it
would make the the the it would make the
operation seem synchronous in that
instead of having to do a callback after
that file operation because it was
asynchronous you could wait on it and we
just return the value and then right
under that you could do the next line of
code as if nothing had happened but it
wasn't really synchronous it was just
kind of faking it it was switching to a
different exit
you should stack and calling other
events and then when it was done it
would switch back to the original
execution stack this was a terrible idea
kuru teens suck like don't listen to
Brian Mitchell there they're terrible we
don't have these in the web browser
right you don't you don't have multiple
execution stacks in the web browser you
get events you call functions and then
you return to the event loop and then
another event happens it would be
terrible if you were in your web browser
environment and you called all these
functions and then kind of like in the
middle of your call to jQuery somebody
else called the button clicked a button
and then that event started while your
other stuff was like being paused and
then that other button event you know
screwed up all the state of of what was
happening in the original function call
in the original event and then when you
returned back to this original function
can you follow me at all probably not
enslaving my hands the state is all
messed up there you have to thinking
about co-routines
is hard because you at any function call
you have to be prepared for the fact
that IO can like somehow happen somebody
you know I'm gonna call this function
and then in some library down the line
somebody's gonna call wait and it's
going to jump to the event loop and then
IO is going to happen so if I just have
a parser and I'm like parsing a socket
happily and then I call a function and
then suddenly like you know now more
data comes in on the socket and and my
parser starts on that data and then when
I return to my original function like
the state of my parser is completely
different what would have to happen if
we had co-routines is we would have to
have locks around the parser we would
have to be really careful about what the
state of where of what we were doing and
we had to make sure that when you call
the function that you were aware that
there was a possibility that anything is
going to happen at that point it's not
as hard as as thread safety co-routine
safety is easier
and thread safety you have to worry
about these atomic issues I mean in
co-routine safety you can you can copy
an array to some place and it's not
going to cut right in the middle of that
array copy copying but it's still
difficult let's just make life easy okay
it's it's hard enough
no co-routines let's just have events
we'll call some functions and then it
will be done the co-routines don't add
anything if anything they make our
program perform worse the only thing
that they do is give you this kind of
you know pleasure of having like these
calls one after another which I agree is
is kind of nice but it's just not worth
the mental anguish so cooperative
threading of any sort is a terrible idea
the other thing that's going on is yeah
the project is growing and now we have a
build bot which does continuous builds
and and runs all the tests and it even
like does performance test just like in
the Chrome project so on every commit it
runs a set of benchmarks and displays it
on a website and so we can see if we
commit something if suddenly that makes
the rest of the project slow and we can
revert that commit yeah
so so yeah the project's growing there's
been a lot of interest in it there's
been 42 releases and there's 63
contributors
it hasn't grown that much in code size
mostly because of this wrote rewrite but
we've got over a thousand people on the
mailing list and thousand people on
github that are watching it so so it's
it's nice yeah are there any questions
was the threshold for the drain event
right now
if any it rights to the socket
immediately so so you're going to - as
soon as you you write something to a
socket it flushes it it tries to flush
it yes reloading dynamically reloading
modules like hot reloading of modules
like you know when you're in Ruby on
Rails and you update some code and then
you know you go back to the website and
it's changed this is hard this is really
hard to do like I thought this would be
fairly simple but it's not
and the problem is you you're just kind
of carrying around these JavaScript
references and just can't you can't
really switch out the implementation
from underneath them there's been
several attempts none of them are very
satisfactory to me there's some new
development going on in v8 which does
live editing kind of at a very low level
which i think is the proper way to go
for for doing this sort of thing so I'm
I'm waiting for v8 to figure it out
right so so I want to stabilize the API
very soon now I want to do this at a
zero point to release I mean I can't
completely like say I'll never change it
again but I want to like not change it
every day okay
and I want to like thank you
so so there's there's been a lot that
needs to be done that's why I change it
all the time I hope that that when I
reach this 0.2 which should be in a
couple weeks I've been saying that for
weeks and months but it will be in a
couple I wanted to have it done for for
this conference but it's not I hope that
people will start building modules on
top of note at that point huh more
modules please yes there are some of
them okay that's it thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>