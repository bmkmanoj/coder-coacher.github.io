<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Jeffrey Lembeck: Moneyball for Performance Metrics - CSSConf.Asia 2015 | Coder Coacher - Coaching Coders</title><meta content="Jeffrey Lembeck: Moneyball for Performance Metrics - CSSConf.Asia 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Jeffrey Lembeck: Moneyball for Performance Metrics - CSSConf.Asia 2015</b></h2><h5 class="post__date">2015-12-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/E7n7XThDewY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I
cool all right so my talk today is
Moneyball for performance metrics my
name is Jeff I a web developer at mpm i
right as you can imagine then a lot of
JavaScript but also CSS and HTML but
today is a free time topic or a hobby
topic and that is sports I'm actually a
really really big fan of baseball so I
don't know how much of the crowd is
really into baseball but I'll keep the
sports metaphors down as low as possible
but baseball is a super interesting
sport to me because I grew up near
Seattle I live in Seattle and which
makes my team the Seattle Mariners ah
and that isn't necessarily a great team
to be a fan of if you follow baseball
because they're historically one of the
worst teams of all time it's like
they've never it's not that they've
never won a World Series which is the US
baseball championship they've never
actually even been to it it's
heartbreaking year after year and while
I could spend like all of my time up
here discussing the lifetime of
disappointment that I've had being a fan
of this team I'm not going to do that
instead I'll discuss some of the better
baseball that I've ever seen in my life
which of course was played by a totally
different team so let's talk about a
division rival the Oakland A's so in
2002 there's a man named Billy Beane and
he was the general manager for the
Oakland Athletics and their professional
baseball team located in oakland
california now Oakland has a
disadvantage as far as teams go of being
what's called a small-market team and
this means that the team normally due to
location doesn't have as big of a fan
base as you know some of the bigger
teams would and that means that they
can't really generate the money that's
needed to bring in some of the bigger
name players now in baseball the general
manager of a team controls all the
contracts the hiring and the firing of
players and since he was the GM
of a small market team Billy Beane had
the difficult challenge of attracting
big-name players and hugely talented
players to his team and because he
couldn't pay them as much as one of the
big popular teams would be say the new
york yankees side note and whatever you
know about baseball as little or as much
as possible if you can take one thing
away from this talk let it be that the
yankees suck okay so fortunately for the
A's Billy came up with a plan he decided
that the traditional ways of measuring
the quality of a player did not paint
the entire picture and they weren't
helpful for building a winning baseball
team especially in the case of a team
that couldn't afford to pay the biggest
players the most amount of money Billy
instead used newer aggregated statistics
and formulas to put together a list of
players who then we're could be measured
against these new metrics and these
metrics became far more valuable for
these players because it could make it
so Billy could get cheaper players who
would win better games this is actually
one of those formulas you saw the like
early statistics this this is one of
those big formulas it's for one called
on base plus slugging percentage which I
will not explain here today so the
strategy ended up being very successful
and it brought the A's to the playoffs
multiple times in a row and it can be
made them compete on the same level as
the teams who spent more than doubled
the amount of money in this new strategy
it spread throughout the league and it
became really really famous and there
was a book named after it and there was
even a movie made after it and it
starred Brad Pitt and if Brad Pitt plays
you in a movie you've done a pretty good
job probably so we're here at Def
estasia which is really beautiful thank
you and uh I'm up here and I'm babbling
about baseball and why and it's probably
because I could talk at all of you about
baseball all day long but also I think
that Billy's ideas can be applied to all
sorts of other fields so traditional
tactics for measurement they need to be
reanalyzed
from time to time they need to be tested
against new metrics to get a better idea
of how things actually work I think this
is especially true for one of my other
great interests in life which is web
performance we've spent a very long time
focused on a few key indicators that
tell us how fast our sites are but it's
become pretty clear lately that that
only paints half the picture so picture
me for a minute like I'm the web version
of Billy Beane which should be super
easy for you especially if you're in the
really really far back okay so let's
talk about web sites to find out where
we can start we have to know what we're
up against we have to know and
understand the enemies that play here
and enemies in this case are the things
that make up a slow ass web site because
slow ass web sites lose so what are we
up against let's take a look at what
he'll means for a web developer so the
state of javascript in android is really
really poor and seriously android
devices get a lot of heat for lagging on
performance and they should but it's not
just android that is killing us out
there on performance it's kind of all of
those little pocket computers we have a
ton of them and they've taken over this
chart right here is the the orange is
the growth in data usage over the last
five years for mobile devices the
overall growth of mobile device used for
browsing isn't something new responsive
design has been the way for about five
years now and in 2013 actually a twenty
one percent of all cell phone owners use
their phone as the primary device for
internet access and this numbers only
been increasing as the years go on and
we don't just assume they'll do things
with their devices while they're like on
the go this mobile context thing turns
out to be mostly bullshit but we know
they'll do basically anything on them
dogsitting dating making terrible
comments on YouTube buying food buying a
car
are buying a house um we so we have
these devices that everybody uses and
we're we're kind of stuck with the fact
that they do that but we have the
knowledge that they're going to be used
everywhere consistently for some
generally weird stuff from time to time
but you know they're super convenient so
who cares if they're fast well it turns
out basically everybody people expect
mobile to be fast and will punish you
for not making it so example etsy
increased the kilobytes of images on
their page by 160 kilobytes that's not a
lot and eating it ended in a twelve
percent increase in bounce rate for
their site Edmonds on the other hand
lowered their load time by seventy-seven
percent and got twenty percent more page
views and four percent drop and bounce
rate and three percent drop and add
impression variance and as you might
like so those good examples there and as
you might have experienced getting your
site to be fast on mobile is really kind
of difficult mobile traffic it is by
default not very fast and latency on a
bad network can bite you really really
hard and it's rarely the case that
somebody has access to a network where
latency isn't an issue example given
more people access facebook over 2g than
4g this probably isn't that surprising
to a lot of the crowd and that's where
winning and losing comes into play what
do I mean by winning and losing let's
talk more numbers edem drop their load
time from 1.2 seconds to 500
milliseconds strap or this increased the
time people spent on the site by
21-percent it increased their
conversions by twenty percent and
increase the amount of pageviews visited
by person twenty-eight percent walmart
dropped their load time by one second
just one second and it increased two
percent of conversions and they found
that for every 100 millisecond load time
they drop after that they increase their
revenue by one percent and if you have
any concept of how much shit walmart
sells
one percent increase in revenue is
really really really high Obama for
America during his last campaign drop
load time by sixty percent and that
increased conversions by fourteen
percent conversions in this case being
donations in the u.s. if you have the
most amount of donations you tend to win
the presidency so that's probably pretty
important and I can do this all day no I
mean seriously we could be here for a
really long while if I keep this there
are a plethora of performance related
stories out there for you to convince
the people who have all the money in
your company that you need to work on
this stuff example again removing one
client side redirect from google's
doubleclick resulted in a twelve percent
improvement click-through rate amazon
season one percent decrease in revenue
for every time they get hit a hundred
millisecond increase in load time a
one-second delay for bing turns into a
two point eight percent drop in revenue
two-second delay four point three
percent drop Mozilla cut their load time
x 2.2 seconds and saw download
conversions increased by fifteen point
four percent and so we have all this
knowledge we know that sites need to be
faster and the benefits of that but at
the same time features frameworks new
designs etc their bloating up our sites
the average size of a website now is
around two point one four megabytes
which is a twelve percent or twelve
point seven percent growth over just
last year so we have an increasing use
of under powered devices on shaky
networks and those users are being
delivered bigger website web sites all
the time these same users are growing
less and less patient over time with how
slow our websites are how are we
supposed to make a good experience
happen well my favorite way to handle
problems is to find definitive ways to
measure those problems and then focus on
improving those measurements we need to
find out what we want and find different
ways of gathering quantitative values by
which we can solve this problem so
there's actually kind of a big war
this though just because something is
difficult to measure does not mean it
should be disregarded if you find
something nearly impossible to measure
keep it in mind at all times trying to
approach it from other angles it's good
you can make it part of other
measurements if you if it can't be
broken out just by itself yet and daniel
ganc Ã©levage which is who's this guy
right here he had a great quote about
this the first step is to measure
whatever can be easily measured this is
okay as far as it goes the second step
is to disregard that which can't be
easily measured or to give it an
arbitrary quantitative value this is
artificial and misleading the third step
is to presume that what can't be
measured easily really isn't important
this is blindness the fourth step is to
say that what can't be easily measured
really doesn't exist this is gross
negligence so keeping that in mind we've
been using easy measurements for a while
and what are those traditional
measurements we've been using dawn
complete so dumb complete is when a
document object model tree has been
completely built this is frequently
known in the point in time which you can
query for elements that's good you
should definitely know that part of your
page um or the load event on load is the
point in time in which every single
asset on the site has been loaded and
Paige wait page weight is the size of
everything the client ends up
downloading to make the site work and
soul some together the request to
response timing so the request response
timing is the amount of time from when
your server receives the HTTP request
until the time where it responds and
that's fully encapsulated in server no
latency taken into account there and
there are plenty of options available
for back-end measurement that I've had
good experience with these three in
particular so those measurements
combined they can paint a bit part of
the picture for us but if you only pay
attention then because they're easy to
track you're missing out on crucial
pieces of performance and this can
absolutely sink you don't get me wrong
these metrics are useful and I actually
pay attention to them but they're just
part of what we're looking for when
we're trying to measure speed so what's
the new way what's the new strategy how
do we fill in the blank spaces that are
traditional measurements leave behind
how do we find the best way to give our
users what they want in the way we want
to give it to them and as quickly as
possible well that answer is kind of
complicated but it sums up as we need to
focus on the first usable time if
instead of monitoring how long it takes
for the entire page to load we instead
measure how long it takes for the user
to use the page for what they want to
use it for we can get a more accurate
gauge on general usability because it's
incredibly frustrating to get to a page
that clearly has all the content
downloaded but the text is blank until
the font loads this is the New York
Times yesterday on Chrome note how I
can't read the headlines and it's
incredibly frustrated to get to a page
that looks visually complete but has so
many different scripts on it that you
can't even interact with it so what
kinds of things are people using now to
find out if their site is usable I'd say
the most popular measurement right now
newer one at least is speed index speed
index was invented by the fine folks who
bring you web page test which is a
fantastic tool that allows you to see
video strips of your site and how it
loads you can break it down to the tenth
of a second and for those of us who like
to nerd out about this kind of stuff you
can roll through and really see how the
browser puts your page together it's a
fantastic tool I strongly recommend
using it and maybe even buying the book
about using it anyway the speed index
metric is based upon visual completeness
and how quickly your site can get there
so let's talk about the formula it the
speed index is calculated as an integral
of 0 to end which is recorded in
milliseconds of 1 minus the visual
completion percentage / 100 so if
integrals and calculus were not your
strong so
let's talk about this in the form of a
chart with visual progress being on the
y-axis and time being on the X the
shaded area here is the part of your
page that is visually incomplete you can
tell right here that it eventually
approaches zero this gives you something
measurable and you can use web the web
page test API to run several tests
against your page and return median
results which is something you can use
as a benchmark to make sure you're not
having serious performance regressions
this example here is the NPM website it
loads its content right there so you can
see how it looks visually by tenth of a
second and speed index isn't brand new
but it's become accepted as another
reliable data point to track that gets
not just accepted it's suggested by
Google and it's a fan favorite amongst
the performance crowd so this is a great
data point to add to those ones I
mentioned before somebody else has made
it easy for you to measure which is good
it gives you a legitimate target to
optimize for but what about when it
doesn't capture quite what you need what
if it's a detect what if it's detector
for visual completeness is actually way
off that happens from time to time but
what else can we measure about the time
it take or how about the time you take
while blocking rendering like lowering
this is the first key to making sure
your users browsers are able to start as
soon as they can or rendering your page
so how do we do that you start by
finding the files which are blocking
rendering these include any CSS on your
page and also any JavaScript that
executes before the content does once
you've found these you can use your
network tab and your dev tools to read
the total time you spent downloading
these files but that's not necessarily
sustainable for automating this process
so let's have phantom j/s do it you know
you can use phantom js2 right higher
files okay so in case you don't know
what a har file is our file stands for
HTTP archive file they can be used to
demonstrate the network traffic and
assets loaded when visiting a page just
like what the network
we'll give you okay so back to Phantom
by timing each assets request response
cycle including start time and time and
the size of the files you can do exactly
what the network tab does in this case I
ran a script that created a har file
which is in JSON format and then I
opened it in Charles to inspect it so
you could get a good breakdown of how
all of that stuff works and how it all
looks and you can have it automatically
produced and that's fantastic and useful
what else how many round trips does it
take to view your content is it over one
let's talk about how that works so when
you have a new HTTP request here's a
thing out there use TCP to connect now
did you know that TCP connections cannot
use the full bandwidth available to them
in order to prevent dropped packets TCP
starts slow as it doesn't know the
quality of the network it's sending data
over and wants to avoid congestion of
that network therefore it's a standard
ascent at a maximum 10 TCP packets on a
new connection for its first round trip
at 1500 bytes per packet that's only
fourteen point six five kilobytes at
this point the client sends an
acknowledgement that it has received the
data and it sends it to the server so it
will send more the server will slowly
ramp up the amount that it sends with
each trip but this can take a bit if
you've got a huge first file you're
sending so in this example here you just
like jump left to right that client says
hey I'd like you know to visit this page
the server gives back that first 14.6 5k
the client says I've got it so the
server starts ramping up how much it can
send with the client acknowledging each
time how much that it received it so
what does this actually mean for you in
practice if you can keep all of what is
needed to use the Site out of the gates
in one request lower than or equal to 14
points 6 for 8 kilobytes you're cutting
the amount of round trips that need to
happen for your site to be usable and in
ibly fast speed even over high latency
and low bandwidth networks the this will
feel snappy what else can we measure
what about timing differences on every
event under the Sun have you used the
performance timing API before it's
awesome so let's do this we'll bring
phantom Jas back out I can automate
running this performance timing load
event and minus performance timing got
navigation start what this does is it
gives you the time in milliseconds from
the moment the browser starts the
process of navigation to your page to
the moment it's finished loading the
page this is far more exact than
anything on load could ever give you so
it's really useful this would this right
here performance not timing Dom
interactive minus performance not timing
gut response start this will give you
the time from the moment where your
server response comes back to your
browser until the timer the browser is
finished parsing all of the HTML and Dom
construction is now complete are those
not exact enough here are all the
options available for timing for almost
every measured point here you can record
and report back your data this should
push you along nicely to having your own
real time user monitoring but sometimes
things aren't so cut and dry and this is
where a big caveat comes in in all this
you can come up with all of your own
statistics and all of your own
monitoring but different websites need
different measurements and it's great to
line up your sites and compete over
medium speed indexes and page weights
and load times and seriously competing
or without stuff makes a better web for
all of us but what if your page cannot
possibly be considered complete until
the hero image is loaded what if you
couldn't even think of your page of
using your page until your menu can not
only be clicked on but it can be used as
well and this is where we end up
building something of our own we can
have all these well vetted formulas and
ways to approach performance out there
but to really approach our problems at
their source we need something that fits
our own personal sites
for that we're going to need real-time
user monitoring and we're also going to
need some custom metrics luckily we've
got those turd the user timing API these
are timing API is still in recommended
status by the w3c and it's not used by
Safari yet including iOS or opera mini
but there's a perfectly good poly fill
out there for this so let's get going
the user timing API provides a couple of
really good methods that can help us
better track what's going on on our page
they attach right to the performance
interface these methods include mark
which allows you to take a quick time
snapshot that is saved and measure which
will give you a measurement between two
marks with these you can very accurately
time what's happening it's just how long
it takes for these things to happen so
let's use an example have a page that
isn't considered ready until this image
itself is front and center now with the
regular performance timing API I can
have the ability to grab the file that
was requested and it can tell you how
long it takes to get the file with get
entries binding and then I can just
check the duration of that but that's
not the whole story to the file we need
to see when it actually shows up so for
that we can borrow a little trick that
Steve Souders came up with and combine a
few different methods for marketing we
can start with an inline load man I
always wanted to use the laser pointer
on this in line on load on the image
itself and then we can also put an
inline script right behind the image tag
so it will execute while the page is
being rendered then we can check the
start time with the start time is for
each of these marks the highest in this
case will give us the actual time that
the image has been rendered on the page
this is immensely useful for a hero
image or for app image that the page
actually relies upon to be considered
usable such as if you're you have a site
where people are buying things and they
need to see the picture to be able to
use the page that's pretty neat huh so
hopefully you have an idea of something
in your head that you can measure that
will dramatically increase the actual
visibility you have in
to your site's performance but never be
satisfied with just those measurements
new techniques will continue to be
developed and with them will come better
insight along the way so pay attention
to your statistics and tests across the
board and you should have a lot of
success and then you can dance so now
now that you have your own measurements
in order maybe we can focus on what we
need to do to speed things up a bit so
Layton sees a big one latency is the
amount of time it takes for your request
to make it from the client to the server
the transmission is limited by first the
speed of light but then the resistance
provided by the copper used in the wire
and the path taken from routing station
or routing station for these HTTP
requests since the path is such a factor
in this case using a CDN can greatly
limit the amount of latency your users
incur by shortening the distance of the
request testicle another way to avoid
latency issues is cater to your critical
path as I mentioned earlier the first
request makes to the client they are the
first request that the client makes to
the server will be limited by tcp slow
start this limit is roughly 14.6
kilobytes with this in mind if you can
inline your CSS that is critical for the
page to load and then asynchronously
load your full CSS file along with any
unnecessary knee you know necessary
JavaScript you can make sure little to
no render blocking that relies on a
network request occurs and your first
round trip will have everything a user
needs to use a site one of my favorite
examples of this is the filament group
website in this case I throttle the
connection down at to just a 2g and the
site was still usable in less than a
second while sending an empty body and
waiting for a script to load all of your
assets may feel cleaner and and
certainly nice for a lot of things it it
guarantees that there will be a minimum
of two requests before you can even
start building the content for your page
and once that happens if your user has
an underpowered device then it can take
even longer and that's why server-side
rendering is important rendering your
site on the server first and sending the
HTML on the first response will almost
always provide a faster first page load
in the past we've been able to achieve
this with progressive and hand
which I'm a huge advocate for but now
javascript frameworks libraries whatever
you want to call them in this case
they're catering to this performance
necessity by allowing your first request
to be served HTML ember does it with
fastboot angular 2 does it I know you
can build it in with backbone etc you
can also use best practices which I
never really liked that term it tends to
mean hacks that involve tribal knowledge
so we can work around limitations of our
technology and with HTTP 1.1 we have a
lot of those so let's talk about why
they're actually recommended instead of
hand waving around them for example due
to the amount of concurrent requests a
browser can make 6 which is a completely
arbitrary number that we all for some
reason need to memorize we suggest you
concatenate all of your CSS and
JavaScript files so as to limit the
number of requests that your browser can
make without stalling and since we're
sending this big file of CSS or
JavaScript we want to make sure that we
can make it is syntactically small as
possible we want to strip comments we
want to make variable names as small as
possible etc minification makes this
possible by parsing your file and then
recreating your coat in the smallest way
then there's gzip I'm a huge fan of G's
if I think it's really really great gzip
works like the video you see on the
screen it looks for repetition in the
text that's being sent and it writes to
file something that references said
repetition if you can see the red text
that's starting to pop up there that's
the part that's rep that's repeating so
this compression process actually is
really really fast and it makes for some
immensely smaller files for transfer so
you should always gzip where you can
you'll save money on bandwidth and
provide a better experience for your
users once again everything in the red
there that gets compressed out so
combining gzip and minification can be a
huge for dropping your file size for
example here's jquery dropping from
247,000 597 bytes 220 9607 bytes so as I
mentioned best practices are normally
artifacts that come with limitations of
your current ecosystem http/2 helps
rest these issues in a lot of ways and
best of all you can use it right now
delivering your site based on what your
client asks for so hopefully you have
some ways in your mind to measure
performance on your site with these
measurements you can concentrate on the
pain points in your site by focusing on
methods to speed everything up this is
great it's wonderful but let's bring it
down to the last part never settling set
a performance budget and stick to it
know what you want your users to
experience measure increases and
decreases in your time and see how that
affects your traffic your conversions
your sales and make sure the continuous
integration system tests if your budget
is being met here's how etsy handles
this they keep a video showing on a big
wall in there a building how their site
currently loads displayed front and
center developers of the site see where
their members or where their numbers
currently are so they're empowered to
act upon problems and what they're
building to see if their success their
successes firsthand so I've talked a lot
up here about how performance affects
the bottom line and I even named this
talk after a baseball method of
extracting the most you can out of your
team without spending more money than
necessary but performance web
performance at least it's about more
than that building a faster website
makes for more money sure but it also
increases the amount of people who can
visit your site faster sites tend to be
faster period all the way down and so
that makes it your site more accessible
for everybody including everybody who
has a lagging network behind him and and
that with the web's all about thanks
thank you Jeff were wonderful talk a
follow from my own experience about the
performers API by the WTC I find the
issue is whether it's really accurate
for the use of all time like you add the
marking your transcript but in fact it
may be other time the pages do some
harder unusable so that's a one issue we
encountered yeah yeah no I I totally see
that and that's why at least in the
image hero part right there I had two
different marks and that piece you'll
find that the performance timing API in
the user timing API occasionally need
you to figure out first what you're
trying to measure and so usability time
as kind of arbitrary based on what your
site is and so if you can pinpoint what
it is you're exactly looking for it it
tends to be able to help with your
accuracy because you mentioned a Pollock
on getting I just wonder if it's still
worth to make some things like the
Facebook speak pipe it's a legacy maybe
legacy issue that we just try to
optimize the doting with several bundles
in in all just clear implementation but
I don't know whether if we have the HTTP
two we still need to do that yeah okay
so this is actually where HTTP two comes
in tremendously and fantastically so HED
be too in case people McLeod don't know
has a thing called server push which
will basically deliver your assets
immediately upon visiting the site it's
pretty awesome and I'm so you can right
now and it requires a little bit of
finagling you can based on the header of
the request you can direct your code to
say load this stuff with HTTP two and if
they're approaching with HTTP one dot
one shoot them this way instead so with
tooling you're still concatenated is
part of your build process and you can
say I want this one file to be served in
the case of HTTP 1.1 but screw it load
them all if they're coming with two
because you can have a ton of parallel
requests and they'll actually come out a
lot faster that way hi I'm my name is
Mitch I was just wondering your idea on
how Facebook do it with a block ace like
block certain elements with kind of non
distinct distinguished shapes and
whether that to the user is something
that's feasible or not or does it does
it appear to be faster or from the users
perspective you think it works
or like your opinions on that basically
um so I'm I'm actually kind of a really
big fan of the idea of building things
that appear to be faster because that
can well trick people into having your
it it makes it so your site is still
usable in a faster fashion and in
Facebook's situation there that's what
you're talking about that face image
loading thing where it like gives you it
turns out that that picture that your
first saying there isn't necessarily
what everybody is interested in right
away it feels like that would be the
case but mostly when you're looking at
pictures you're thumbing through
people's shit sorry I've cursed up here
a lot today but yeah so I'm an advocate
for it I work for a news agency and
everything works fine till you inject
the ads because the Moodle based on Ezra
pending so how do you make the
performance for it Oh God so performance
on ads is yeah that's that's always
difficult third-party javascript is
something that it's difficult to wrap
around I the only way that I've ever
been found or I've ever found to make a
site vastly more usable despite the ads
on the page is to make sure they're lazy
loaded in the most way possible put a
sink put defer on those and shove them
at the bottom of the page and everything
else will be requested first and when
there's time those will be requested in
hopefully they don't cause a reflow
that's that's my advice for you on that
one it hurts every time a little bit</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>