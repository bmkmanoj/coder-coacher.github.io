<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Julien Genestoux: Streaming Algorithms in Javascript and Node.js -- JSConf EU 2013 | Coder Coacher - Coaching Coders</title><meta content="Julien Genestoux: Streaming Algorithms in Javascript and Node.js -- JSConf EU 2013 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Julien Genestoux: Streaming Algorithms in Javascript and Node.js -- JSConf EU 2013</b></h2><h5 class="post__date">2013-11-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/e9httij2RG0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you
so my name is Julian I am the CEO of a
company called superheater and I'm going
to come back to this later this is a
presentation about streaming algorithms
also as as a note I tend to speak very
very fast so please do not hesitate to
kill me with science and slow down I'll
try so this is a presentation about
streaming algorithms and JavaScript as I
told earlier I am Julian 51 on Twitter
and other silos and my website is super
calm
please also I wanna take one minute for
you guys to enjoy yourself to each other
maybe find someone close to you that you
don't know that you've never talked to
because every time I sit I usually don't
know my neighbor so you have one minute
please introduce yourself exchange maybe
your email or something and then come
back to the presentation
all right ready
ready ready
oh alright alright let's go
hello let's go okay so some of you might
know what that is this is kind of a
picture of the web and I'm gonna start
with a little a history of how the web
is and how it is turning into now so for
the longest time the web has been kind
of a geography world where you move from
a site to another site following links
you will have web addresses which is
very geographically even the icons of
our browser are very Geographic when you
look at obviously Firefox it's it's it's
a it's a map of the world with a fox on
it but if you look at suffering it's
it's a compass itself so it's a very
geographical approach of the web and I
think we're slowly turning into
something that is more like this where
we have more and more of a time based
approach of the web where it's it's just
too big to just even think about how big
it is and and more and more the
information comes to us directly if you
think that Twitter I usually don't go
very far in my timeline maybe not even
birthday or something like I would go
back maybe 1020 tweets and the rest of
the data just comes to me an RSS reader
has been the same forever Facebook is
the same and more and more the
information that we consume online these
devices have notifications and it's just
data coming through us so it's less and
less about me browsing sites and more
and more about me sitting and like
getting vast amounts of data in my face
and this is actually happening at all
stages of the web more and more our
website which is to be basically some
kind of application code and a database
which has a lot of states is more and
more of a system with a lot of cues and
system where the data goes in and move
to the next step and then to the next
step and then to next step and then is
eventually served back to the user or to
or even to another user and and and
that's super feature we exactly deal
with this for those we're not familiar
with super feat what super filter is
it's basically a distributed
architecture to push RSS feeds in real
time so people will come to us give us
to use lists of feeds and when you say
people it's mostly companies they come
to us and say hey I've got like half a
million houses feeds that I need to
fetch I just don't wanna do it please
push it to me so we try to get this data
build this massive pipe of data and push
it to them in real time as soon as we
can
doing this has a lot of a lot of
challenges that we need to learn we need
to solve the first one is obviously a
memory constraint if the pipe is huge
getting all this data in real time and
when when I'm talking about all this
data it's a couple of hundreds or
thousands of updates per second
it's it has a huge memory impact and you
need to store it and forget it as soon
as you can because if you even store it
for like a minute or so in memory then
everything explodes the other
consequence of this as well is like
there is no like well I'm gonna get all
the data and then when I'm done with it
then I'll start processing it like the
data comes to you and it never ends it's
actually even increasing in volume so
you start to have to do it right away
when it happens you need to process it
and then fire-and-forget again it's also
real time like you cannot again wait for
the data to be processed before you send
it to the next step
you cannot Twitter cannot wait for the
number of retweets or start on an item
before they actually show you on the
timeline like it arrives they show it to
you and then maybe later that will show
that there's been retweets and there's
been stars or favourites on your on your
on your content this actually changes
pretty much everything that we've known
and used when building applications
including and starting with algorithms
that you cannot use what we've used so
far to build applications algorithms
that we've been that we've been using
for years to build the next app the easy
solution that rubidium I'm sure a
starting thing about is like well why
can't we do just windowing or sampling
when doing it would basically be like
fine I'm gonna keep the data for like a
minute do all my competition then trash
it and then start again so buffering for
like wait a minute or a couple seconds
this works even though it's very it's
also very inaccurate in a way that if
you want to start growing the window you
will eventually come back to the memory
foam that we've had sampling is maybe
rather than keeping all the data is
keeping maybe one every ten or one every
100 and then you start seeing patterns
from this same problem you might be
missing what the important important
data is and you actually want to have a
global vision and compute on the whole
set so we're not going to talk about
windowing and sampling in this
presentation so let's start with Max and
min this is actually very easy if you
get a flow of data that comes to you you
want to know what is the max what is the
mean the most common use case for this
oops sorry
is obviously finding kind of the best
the winner in a set of data that coming
that comes to you that's easy that's
really really easy you just keep a track
of the maximum that you've seen if the
new data that you get is bigger than the
previous one then this is the new
maximum and then keep keep will keep
doing this it's it's pretty simple I'm
gonna build a little JavaScript code and
so that you can see with a visualization
how it's done we have an object called
stream F which is basically doing math
better than hopefully regular JavaScript
does it gets data through the feed
method method compares the maximum with
the new value the new values greater
than keep it as the max we to test this
and to show presentation let's do a
little thing that generates number
randomly so this is a function that
generates a number randomly and they're
arbitrarily low I mean there's no normal
distribution and this new input thing
just yields number every 10 milliseconds
between 0 and 1 using the distribution
determined here and we just put this
that we can put that into blue plot so
let's do this just gonna run this you
see that's basically what it does and if
I put that into a new plot and then plot
this hopefully you will see this which
makes sense like the Green Line is the
maximum and all the points are the new
points that are being added forever
looks like I'm missing the end of the
screen here so what we can see here very
easily is basically this algorithm as a
little problem here it knows too much
basically it keeps track of the old
maximum even though like maybe in a year
this all Maxima is not really worth
remembering we need to make sure that
the algorithm is smart enough to forget
and how do we do that without again
keeping you a window it's it's actually
pretty easy we just use what we call
amortize values so doing this we rather
than keeping track of I mean we keep top
of the row value but rather than
covering the row value we add some kind
of factor which maybe is the time at
which we've seen this value or the index
in the sequence and by incrementing this
we know that all the values eventually
are going
be smaller I mean for two values that
are the same the oldest one is going to
be smaller than the new one basically
and that actually works pretty well I'm
gonna go back here if we go back to the
visualization here we see that quite
quickly obviously the the highest one or
kept in track but if there's nothing
very high for a long time then it's
slowly the the next higher value is kind
of the new maximum does that make sense
so this is an easy way to compute
maximum and have some kind of memory and
like not just take the first highest
value that we have forever being the top
one all right it works pretty much the
same with minimums rather than adding
value if you see then you remove the
value at which you are you've seen them
okay next step is the average this is a
slightly more complex problem in the way
that usually you compute averages by
summing all the values that you've seen
in the dataset and then you count all
these values and you divide the first by
the second one problem that we have
obviously we do not try keep of the data
we don't keep track of the of the values
so we have no way of summing them and
counting them afterwards well the easy
way to do this is to keep running counts
so rather than just having the data and
comparing to the previous one that we
did before now we keep track of both the
sum and the count works pretty well as
well as we can see here
so as you can see I'm pretty sure you
may be them the function that we've used
here is actually a random number between
0 and 100 and quickly as we've seen here
as we can see here the pH converges
toward 50 that's the green eye in the
middle can everybody see it yeah
quickly converges with around the middle
of the curve this is great but the
problem is sometimes your data set might
have interesting properties so like like
a different distribution and the problem
that we see with different distribution
is is obviously that if you have a
distribution that does basically a
sinusoid quickly the average becomes
uninteresting because it just sits in
the middle and doesn't really give you
any day any insight on what the data is
I've started this 10,000 milliseconds
ago and it's already very close to zero
if I keep this running for like an hour
nobody will be able to see anything in
the India bridge it would be meaningless
the the the value the the solution to to
counter this is to kind of wait the
averages and rather than having the same
wait for all the values in the past that
the the new data has the new the new I
mean rather than having the same wait
for all the data points that you've had
just use the previous average to come to
compute the new average with the new
number so basically you take the average
at the t minus 1 multiplied by weight
add the new value and then divide by the
weight plus 1 what this does is
basically it just shrinks the the
important of the older values into the
into the into the current weight the
current average and so we can do this
here very easily
and we can see that in this context
basically the the average still is
obviously lower than the maximum and the
minimum because it's an average but at
the same time it's it slowly starts to
not decrease anymore not change much
based on what the input is so this is
obviously bending the math in a way that
is it's not the actual average over the
whole data stream but again the whole
data seam doesn't mean anything and when
you have this size of data you need to
be able to have a kind of a running
average without the windows and keeping
the data which is exactly what we do
here the next one is standard deviation
this really starts to be complex because
I mean the main goal of standard
deviation is to identify the outliers
the the number that are either way above
the average or way below the average you
compare basically how far they are from
the from the from the from the standard
deviation the variance is the average of
the distance to the average this is kind
of the the math lesson that everybody
learns basically you compute the average
and then the variance is you compute the
distance for between all the points and
the average and you get what we call the
variance the problem of this is as we've
seen here it use it uses the average and
if we change the average with every
point we have we need to recompute the
distance for all of the points that
we've seen in the past we've the new
average is that clear so that's kind of
a problem because obviously we don't
give the data and we don't really want
to waste our time we're computing the
average all the time well that's great
because some scientist mathematician
Koenig and ujin's have actually fine
changed I mean not changed but like
found that the the the equation as an
equivalent that is much easier to use in
our context which is rather than
computing the average the distance to
the average for all the values you keep
track of a running square I mean the sum
of the running square and you just
abstract a square of the average and
then so that's exactly what we're going
to do here we have
Bowlings we keep the basically the
the new rolling number which is the sum
of squares and for each new value that
we get we still increment the Sam
stealing from an account because we need
it for the average we also keep track of
the new sum of square and then when we
when we need the standard deviation we
just compute the square root of the sum
of squares divided by the count minus
the square of the average and we can do
that as well you know we'll see that the
oh this is the same distribution as
before a random number between 1 and 100
and the standard deviation stays around
30 I guess yeah below of this which is
expected I guess for this kind of
distribution makes sense anyone is a
question maybe not okay all right what
what do you don't understand
all right no it's it's my fault I should
be able to tell you but basically like
the regular Matt the point here is the
regular standard deviation calculation
cannot work because obviously we need to
keep track of all the data right so we
transform that equation and use another
one that allows us to do incremental
calculation of the standard deviation
that make sense better
okay next one medians and percentiles
this one is nearly I mean it is at this
point not possible without keeping some
kind of window or some kind of sampling
which is sad but it also makes sense
because the median is trying to find
which value divides the set of data into
two sets of the same size one of Greater
elements and the other one of smaller
elements there's a way to do that with
two heaps by keeping track of some
elements so it can still be pretty low
in terms of memory the algorithm is
relatedly simple the first one if start
with obviously 2m g hips and then get
going
when you get a new number if it's
greater than the previously calculated
median ID to the greater the heap of
greater numbers if you get a small one
then you hide it to the other one and
then basically if the length of either
EEP is greater than the other one you
rebalance so you take one element from
one and you put it in the other one and
the median is always the smallest value
of the greater hip that makes sense so
it's basically you always keep track of
the middle here the prime again if like
if you try to do this on a long time you
will keep the the two heaps will
obviously keep increasing so you have a
huge amount of memory that you need to
keep luckily you can still king the data
by removing data from the two heaps the
problem of this is that you need to make
sure that you don't remove too much
because obviously if all of a sudden
your your time series or your series is
slowly decreasing you will slowly move
all the data from one hip to the other
one and then you won't have enough data
to keep track of what is the what if the
what is the the median here which is a
problem
all right another one that is
interesting is in a huge data if you've
got a huge data set and of incoming data
and you want to keep track of the thing
that you might have already seen and
make sure that either you've already
seen because you don't want to look it
up or you don't you want to make sure
any context of say a distributed
algorithm that would run across a used
data set you want to identify unique
profiles on social networks you would
have to keep track of all the profile
that you've seen which can be too big so
a good way to do this would be to use to
use bloom filters basically bloom
filters work in a way that there are
very space efficient probability
probably probabilistic data structures
that uses hash functions and and
dimensions to identify items that you
might have already seen so you set bits
to one when you've seen some of the ITM
using the hashes the problem with bloom
filters is you can have false positive
which we'll see later which means that
they will tell you that you might have
already seen some piece of data even
though you haven't seen it already but
you can never have false negative which
would be saying that you've seen
something that you actually have you
haven't seen something that you actually
have seen before it's also very
efficient and in increasing the the I
mean decreasing the probability of
errors is kind of easy because you add a
little couple bits and it's and it's
still very small to reduce significantly
the amount of false positives that you
may see on the data set here's a little
example that I've stolen online and it
actually works pretty well so basically
for each data that you see you will add
it to the your vector so these are the
blue dots can you see the blue eyes
maybe they're too small but basically
for each word that you see you add them
something's wrong okay so basically you
see like the yellow ones or the one that
I've added previously the blue ones are
the one that I've just added so every
time I add a new word I'm gonna add
Billy in here it will just add new
rectangles of the data that I've seen
and if I look for some data it will tell
me if it's Darren or not so I've added
Berlin Paris is not there if I look for
building obviously no it doesn't work
something is wrong here obviously it's
there sorry oh ah yes good point okay
and basically Berlin now is in there so
bloom filters are a great way to
identify data at the problem that they
have though is if you input a large
enough chunk of data you will end up
increasing the amount of false positive
by a lot and you may eventually end up
to a point where the bloom filter will
always say that it seemed that it you
have already seen the data that you've
included because it basically filled the
whole data structure with once I'm gonna
switch the next like and you have
different ways to deal with this either
you can time limit the these bomb
features so basically you do a bloom
filter per hour per minute per year and
then you can do things like finding
trending trends basically so if you do
bloom filters per minute you fill it up
and then every time you get the data
that matches it you check if it was also
in the last minute in the previous
minute in the previous hour and you can
see that a given term has been showing
up pretty often in the past time very
easily with this you can also extend
them by adding more bytes to the vector
the problem of this is like an infinite
data stream obviously the number of
bytes is gonna increase as well so it
doesn't increase as fast but still you
might with thousands and thousands and
billions of items you may end up having
to keep extending which eventually might
be expensive in terms of memory you can
do what we call counting filters which
which are basically filters to which you
add in remove data and you can either
remove data by removing the data itself
or just decide that you'd agreement all
the elements in the bloom filter by one
every amount of time so every 5 minutes
every hour or so you just decide that
the bloom filter is going to lose
some of the data that it seems before
that it's seen before using this
technique and this is kind of the end
here this was really barely scratching
the surface of how you need to handle
these kind of problems of streams rather
than considering use datasets and state
that you have I would definitely
recommend that you look into this this
is something that I've completely
discovered a couple a couple months ago
and this has been fascinating to me this
is now the end I will happily take
questions if you have any this
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>