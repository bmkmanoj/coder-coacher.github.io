<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Razvan Caliman: Disconnected Networking | JSConf EU 2015 | Coder Coacher - Coaching Coders</title><meta content="Razvan Caliman: Disconnected Networking | JSConf EU 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Razvan Caliman: Disconnected Networking | JSConf EU 2015</b></h2><h5 class="post__date">2015-10-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zvG97aOfAJM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Hi everyone, and thank you for being here.
I am
Razvan, I work for Adobe with a bunch of awesome
people
on interesting things like CSS shapes, masking,
CSS
blend modes, CSS regions, some of you may
know me for
building the CSS shapes added to your Chrome
extension
which allows you to edit shapes on the browser.
In
general I work on practical things.
None of that
matters today, because today I want to show
you
something which I'm fascinated about, and
that is
unconventional uses of technology to admit
data and
input.
This isn't so much a talk, but actually just
a collection of demos other people have built,
and
stories I founding really interesting in the
sense that
they use technology in unexpected ways with
unexpected
results ... so today I'm going to go briefly
into sound,
how we can transmit data using sound, and
how we can
build our own cheap version of the litmotion
just using
sounds on your laptop.
Then I'm just going to talk
about a way of using light to communicate
to your
computer, and if there's time depending, two
short
stories about interesting ways to transmit
input to
devices.
So without further ado, let's begin.
The year is 2013, October.
I come across this
article on arstechnica, and it is about a
respected
security researcher who supposedly found this
sophisticated piece of malware.
This was infecting his
machines and spreading by ultrasound.
What is more
devious about it is that it couldn't be diagnosed
with
forensic stools, and it didn't leave any traces.
That
was odd.
This guy found out the ultrasound was the
means of infection by repeatedly removing
stuff from the
computers, the hard drive, network interface,
the
memory, he changed so many components, and
it wasn't
until he actually removed the soundcard, which
was
controlling the speakers and microphone, that
the
infection stopped.
Now, these claims are being contested by other
security researchers who haven't been able
to reproduce
the same effects using the logs.
But other security
researchers have proved that it is actually
possible to
infect the bias of another computer just using
ultrasound.
Quite scary, if you ask me.
Around the same time, Boris Smus is an engineer
for
Google, he was playing around with the WebAudio
API and
trying to send data between devices using
messages
encoded as ultrasound.
And he put together this library
called Sonicnet.JS, which I'm going to use
today, and
the way this works is that it takes a part
of the sound
spectrum, specifically the part where you
can't hear it,
the ultrasound spectrum, and it chunks that
into various
bits and pieces and each letter of the alphabet
gets
a portion of this ultrasound spectrum, so
when you want
to send a message, you essentially encode
this as
ultrasound and send it in a frequency which,
as adults,
we can't hear.
Some pets can hear it and small
children, but it is kind of invisible or inaudible,
but
rather than talk about it, I want to show
you a demo.
Full disclosure, this is a demo idea I took
from Boris's
original experiments.
I just adapted it for this
presentation.
So all credit goes to him on that.
What I'm going to do here, exit full screen
mode and
on the right-hand side of the screen you're
going to see
that I have this demo running on two devices:
one is my
laptop and the other is a phone I have connected
over
here via cable.
They're both running the same demo and
what I'm going to try to do is get them synchronised
just using ultrasound, and I really really
hope this
works because there's so many things to go
wrong with
the demos today.
Okay.
So just to get a sense of what is going on
here, I'm going to turn on the visualiser.
This is
bouncing around because it is hearing me speak,
but as
I'm going to click on the icons on each one
of the
devices, they are going to send out a message
in encoded
ultrasound, and you're going to see a distinct
peak
towards the right-hand side of the spectrum
over there.
Let's try this.
I'll start with my phone and start
sending emoticons to my laptop.
It works!
It works!
This is unexpected.
It works.
Most of the time it
works.
My laptop is picking up the ultrasound from
the
phone, the phone itself, sometimes it doesn't
actually
catch its own ultrasound back, so the speaker
is over
here and the microphone is down here.
And there is
actually a big caveat with microphones and
phones.
They're optimised for speech, not necessarily
for
ultrasound.
This is one of the few devices I have which
actually works with this.
But if you don't have
ultrasound capable devices, you can do the
other thing
and reverse, you can use audible sound spectrum,
so I'm
going to switch both devices to audible.
Okay.
Allow.
And this is going to blast out a very annoying
high
pitched sound, but doing essentially the same
thing, so
I'll try this with my laptop now.
You can probably pick
it up from my microphone.
Ah.
Okay, it worked.
[applause].
You can't believe how
happy I am.
I tried it many times, but all props to
Boris, who built the library.
His work is amazing, and
he should get the applause.
So use case for this, we
synchronise two devices.
Google used this library with
a few motivations to use Google tone.
This is a browser
extension which allows people who have the
same
extension to share links using audible sound.
It has
a nicer sound, but essentially if you're in
the room
with someone, you want to share a link, you
just press
a button and your laptop just speaks the link
towards
the other device, which is kind of cool.
It works over
handouts, over a lot of things.
Nobody has to install
everything more than Chrome, in this extension.
The big
problem, though, is that you have to -- your
laptop is
always listening to the environment, so it
can pick up
these links.
And I don't know about you, but that kind
of creeps me out, so I don't use it.
Nonetheless, very
cool extension.
Okay, switching gears to motion sensing using
ultrasound.
This one is very cool.
So the doppler
effect.
You've all experienced these.
The doppler
effect is this phenomenon of the apparent
frequency
shift of a wave to a static observer as the
wave is
being emitted by something in motion.
So imagine the
last time you heard an ambulance siren, as
the ambulance
is travelling towards you, you're hearing
a higher and
higher pitch and the reason is that the source
of the
sound, the ambulance, is travelling, and each
soundwave
has to travel less and less to get to your
ear,
therefore you're going to have more soundwaves
hitting
your ear as the ambulance travels towards
you.
The same
thing happens in reverse as it goes away from
you:
soundwaves have to travel longer to get to
you because
the source of the sound is going away, therefore
the
pitch sounds like it is going lower and lower.
This is one of the fundamental principles
in radar.
In radar, you have a base station which is
emitting
a radio frequency, and it is listening for
that same
frequency around the same spectrum.
Those radio
frequencies are going to bounce back everything
they
hit, so if you have an aeroplane in motion
coming
towards you, the radar is able to use the
doppler effect
in a lot more technology in maths than I'm
capable of,
to basically determine if the aeroplane is
coming
towards it or going away, then using an array
of radar
systems you actually know where that aeroplane
is in
space by triangulating the sound.
Of course we have
everything to build a radar system in the
computers,
first of all replace the base station with
our laptop
speakers, and we have the microphone over
there, and
then we can use our hand to basically understand
where
it is in relationship to the speakers and
what -- how
we're going to do that is use ultrasound.
Daniel Rapp
built this very small library called doppler.
It sends
out an ultrasound from my laptop speakers
and is also
back for the same frequency.
What you're seeing on
screen right now, once it settles, is actually
my laptop
emitting around 20-kilohertz sound.
You can't hear it,
I can't hear it, but it is here.
There's this graph on
the screen here showing the difference between
the
frequency emitted and what's coming back.
So as
I introduce something in the vicinity of the
speakers,
like my hand, and I'm wiggling around, I'm
able to
affect the way the echo is going back.
Remember, the
radar, and the doppler effect.
And so I can pull up and
essentially, that's changing the way the sound
is
reflecting back into the speakers.
Or I can push
down -- [applause].
I know, it's awesome.
What can we do with this?
We're getting quite a lot
of resolution here with my hands, but we can
only look
at the peaks and tie these to something.
So for
example, let's say we want to scale a balloon.
I can do
these ample gestures or hit it so it goes
right back
down.
There you go.
Very cool.
This like the poor
man's version of the leap motion.
Therefore we can use
it for something practical.
We have some interference
but I can use these gestures, scroll a page
up and down
so I don't actually have to use my mouse.
[applause].
This is absolutely interesting because you
clapping
are affecting my demo, but please do that.
That's fine.
Because this is actually changing the echo
in the room
that my laptop picks up a different sort of
echo as
people are moving across the room, which is
kind of
cool.
All right, if you can do these things, we
can turn
into Darth Vader and just turn on and adopt
a navigation
for the entire presentation, and say I want
to go
back -- you've seen this before -- or I want
to go
forward, do we want to plug in for reveal
with this?
Yeah we do.
Okay.
That was sound.
And I'm glad those
demos worked.
Let's move over to light.
So I want to
talk about light.
The man on the left-hand side of the
screen over there is using a device called
a heliograph
... now a heliograph is a simple system of
mirrors which
focuses sunlight towards a target which is
farther away.
Ending a simple system of levers, the operator
can
flashlight towards the target on and off,
you can encode
essentially Morse code, sending it to the
other person
listening to you.
At the other end, there is another
person either with a sighting scope or another
heliograph line, and what is cool is you can
get
10 miles of range for every inch of the mirror,
which is
kind of amazing.
The record for range held by a heliograph
team is
300 kilometres, yes, between two mountains
using mirrors
about eight inches in in diameter, and this
was
happening in 1895.
There's only one drawback to this:
if there's no sun, there's no messages.
There's the
limitation.
If it's raining, we stop.
So the design
evolved to this device called a signalling
lamp, and
still being used by navies today in exercises
where they
have to maintain radio silence.
So instead of the sun,
you have a bright light inside of the device,
and
a series of shutters in front of it.
The operator is
using that lever on the side to open and close
the
shutters, therefore encoding a message and
send Morse
code to a receiver at the other end.
Bear this in mind as I continue.
Ambient light
event.
The ambient light event API is something your
computer has to essentially detect the light
level in
the room, and adapt your screen intensity
to the light
intensity in your room.
So right now, we have about 42
locks, there's nothing changing over there.
If I put my
hand in front of the light sensor here, you
see that's
going down.
Or I can do something else, I can turn on
the flashlight and you see that's going to
go really,
really high.
There you go.
It's off the charts,
absolutely off the charts.
Okay.
The textbook example for ambient light events
is to adapt the page so you get better readability
in
low light conditions.
So if I put my hand in front of
the light sensor next to the camera, that's
going to
reduce the light, I can change the document
style sheet
so it is better to read in low light.
As I put my hand
away, I can read it, therefore the slide sheet
changes
and I can do this over and over again.
But notice my
hand doesn't really affect too much of the
light sensor
the value is pretty low imports.
What if we use ambient
light events and Morse code?
What if we can send
messages in Morse code to the computer using
that light
sensor?
Let's try my most brutal demo ever.
This is
optical Morse code, so I'm going to turn on
an
application on my phone which sends SOS, it
sends any
message in Morse code as flashes of light,
so I'm going
to try to send an SOS to my computer.
Okay.
So SOS is
essentially three dots, three dashes, three
dots.
That's really going off the chart over there,
which is
good.
I like that.
There you go.
It worked.
Cool.
applause].
Just in case you're thinking this is a very
engineered demo, I am going to type something
longer,
like JSconf, and this is going to take a while,
actually.
So let's try JSconf.
One thing to note about
the ambient light sensor is that this is not
a stream.
You're going to get events from the light
sensor going
up and down, but you're not going to get them
in a very
predictable fashion, so I had to build a very
simple
sampler.
It is basically sampling the light sensor
once
every frame, so really hitting it hard.
So I can decode
those messages.
Come on.
F is always difficult.
JSconf.
Yes.
It worked ... [applause].
So happy about
it.
I am particularly happy that this thing worked,
because it shouldn't.
The reason is so you can argue
this thing will only work in Firefox, and
it is only in
Chrome behind the flag, so you need to enable
it.
You
can argue: well, you should turn on the camera
and
listen to the ambient light event with a much
higher
frequency -- sorry, listen to the camera and
decode in
the images in the camera, but there's only
one small
problem: you need to ask the user for permission
to use
their camera.
You don't need to ask them for permission
to use the ambient light sensor.
And this is a major
privacy concern.
And the people writing the spec
thought about this.
That's why you're not going to get
very predictable and accurate results from
the light
sensor.
Essentially, because you can do a low file
alternative to bluetooth beacons and fingerprint
people
without their knowledge.
So here is a use-case.
You're
walking around the store, a phone in hand,
using an
application, and that application is looking
as
monitoring light change events.
You want to enter
a store, a light above your head at the entrance
is
flashing a very distinct pattern.
Your application
knows you went into the store.
You go to an aisle.
A different light is flashing a different
sort of
pattern.
The application knows you're in that aisle,
therefore it can push notification and do
analytics and
do anything.
You go to the checkout and have
a different light pulsating in a different
pattern, and
the application knows you have paid.
You understand what's going on over here.
It sounds
a bit dystopian, and if it sounds improbable
it
shouldn't, because this thing happened.
This is an
experiment that Phillips did with a major
retailer in
France, and they're using a technology called
VLC,
visible light communication, and the way this
works is
it attaches a light controller to normal LED
lightbulbs
and causes them to flash in a very distinct
pattern
which we, as humans, can't observe, but sensors
on our
phones can.
So essentially what they built, they meshed
out the entire store with different lights
so you can
build an indoor GPS navigation system for
that store,
you can ask for directions to the toilet paper
aisle and
it will give the directions.
It's cool, but you need to
offer permissions to do this, so this wasn't
necessarily
a privacy infringement, but a very cool experiment.
The
technology is called VLC, visible light communication
and I strongly urge you go look into it.
Some people
see this as the future beyond wi-fi, mainly
because it
has 10,000 times more spectrum than radio
wi-fi.
It is
kind of cool.
But this isn't anything new.
This is the Timex data
link watch, which could be called the grandfather
of
smartwatches nowadays.
This was first marketed in 1994,
and the special thing about it is you can
store phone
numbers or 'to do' lists on your watch.
But that's not
very interesting.
The interesting bit is how you got
those things to your watch.
And it doesn't use a cable,
it uses light.
So above the display over there at the
12th hour there's a small camera, and that
camera,
coupled with an application for your computer,
is going
to receive flight pulses from your monitor
and encode
them as messages.
But rather than explain how this
works, I would want to show you the commercial
for the
Timex datalink watch from 1994, and this is
where I'm
going to need help from the sound people because
I connected my sound and this is something
you need to
hear.
All good?
Good.
music].
The cat always wins.
[applause].
I find it funny.
The cat always wins, the dog has no chance.
This was 1994.
That was 21 years ago.
Some of the
people in this room today weren't even born
when people
were trying out technology like this.
So let's fast
forward 21 years, where are we today?
This thing is
called the Michigan micromould.
By all accounts, this
is the smallest computer in the world right
now.
That
is not a huge coin, that's a small coin, and
the
computer fits on the size of the coin.
It has
everything you need: processor, memory, storage,
and
a small battery.
It has radio, it has some sensors like
temperature and light sensors, and it all
fits on the
side of a coin.
So this size begs the question: how do
you actually -- there is no room to put a
connector, how
do you program this thing and charge it?
The answer is:
use light.
The micro mode is initially programmed with
light pulses, and by the way that's the tiny
black thing
underneath the lightbulb over there: the entire
computer.
So the initial programming phase is done by
light phases up until it boots up and turns
on the
radio, so it can use the network to get further
instructions.
This is absolutely incredible.
I am both
amazed and terrified at the size of computers.
Okay.
Since I have a bit more time left, I am going
to go into
the other section, and this my favourite one.
This
device is called the Misfit Shine, one of
the earliest
fitness trackers, and in the video they alleged
it would
synchronise by you placing it on the screen
of your
phone.
And people went crazy for it, myself included.
We didn't know how this was going to be done,
we were
very curious, myself included: is it rhythmically
tapping on the screen to get the message,
something like
Morse code tapping?
Is it generating electromagnetic
field?
Sadly, no.
The 
Misfit Shine eventually
launched, and the answer is it is just using
bluetooth
low energy, which is useful because it is
a high
bandwidth way of transmitting data, and you're
a lot
less prone to errors, but the important bit
is it is use
bluetooth in a very low frequency so you need
to keep it
close to your device, so you need to put it
on the
screen for it to synchronise, but it would
actually do
it if you just put it next to the phone and
tap that
circle there.
A bit disappointing, but it is not
totally impossible to do that.
This is the Google Cardboard, most people
in the
room probably know what this is.
One of the low-cost VR
headset.
It is basically a piece of cardboard you
put -- you open an application, you put your
phone in
there, you put it up to your head and you're
projecting
into a 3D virtual world, but that introduces
a problem
and the problem is how do you actually interact
with
your phone when it is in a box stuck to your
head?
You
could try using the camera or you could try
using the
microphone, but those need user permissions,
first of
all, an extra step.
They're going to drain the battery
very fast, and if you're in a noisy environment
or in an
environment with a lot of light, they may
not be that
precise to begin with.
So the engineers come up with a brilliant
solution:
on the side there you're going to see a circular
metal
disc, a magnet, and it travels up and down
in a groove.
On the other side, there's another magnet
which stays
fixed, and as you pull them, you're actually
changing
the electromagnetic field between those.
So the
engineers for the Google Cardboard app are
looking with
the magnetometer for the changes in the electromagnetic
field, this is the same sensor you're using
for the
compass, and therefore that's the input device.
You can
register clicks, long presses, just by measuring
the
changes detected by the magnetometer.
Absolutely
brilliant.
Unfortunately we can't do this in browsers
today.
I would love to show you a demo like this,
but the truth
is we don't have access to most of the APIs
in our
phones.
We know about the browsers, but not much about
the underlying sensors, and if there is anything
practical to take way from this presentation
today, is
if you want access to all of the sensors in
your devices
in your browser to build your own unconventional
use of
them, I encourage you to get involved in the
generic web
sensor API.
It is at URL, an ongoing specification to
make a uniform way of exposing sensors for
us and
browsers to use, because until we have access
to the
sensors on the device, native is fundamentally
going to
be ahead of the web.
So we need access to sensors.
And
that's about it for my presentation.
I hope you enjoyed
it, I hope you found some bits of insight,
and
I encourage you to look for your own unconventional
ways
of transmitting data and input in whatever
you do with
your devices.
Thank you for your attention.
applause].</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>