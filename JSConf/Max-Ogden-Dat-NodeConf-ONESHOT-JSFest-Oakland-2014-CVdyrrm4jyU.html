<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Max Ogden: Dat | NodeConf ONE-SHOT | JSFest Oakland 2014 | Coder Coacher - Coaching Coders</title><meta content="Max Ogden: Dat | NodeConf ONE-SHOT | JSFest Oakland 2014 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Max Ogden: Dat | NodeConf ONE-SHOT | JSFest Oakland 2014</b></h2><h5 class="post__date">2014-12-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CVdyrrm4jyU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey I found out I was going to give this
this morning so luckily I am I had some
slides that I was working on anyway so
so I work on an open source project
called debt and I just wanted to kind of
share our use case i'm really excited we
get to use node for something other than
building websites and i think it's a
it's a really fun project that i am
excited have a budget to work on also
and it's open source so I kind of just
want to share with you and tell you what
we're doing and this is kind of state of
the project as of this part of 2014 well
I'm Max and I organized a project called
note school and I just want to do a
shout out about node school we have we
came up with the idea of chapters a few
months ago we have 60 chapters around
the world now in like three months and I
think that makes us the largest node
community so if you haven't got involved
in those school i'm going to guilt-trip
you right here I'm looking at every one
of you in the eyes either join your
notes cool find the chapter or if you
know another language translate the web
page or translate a workshop and help it
spread even more globally shut starting
a chapter you just go to the website and
follow the guide and it's really easy
and if there's already a chapter you can
just join it I also really love cats so
dat dat and cat they have two out of
three of the same letters by design that
is for sharing data and also
collaborating on data there's two phases
to it it's about a year old and we are
grant funded we're not a start-up a lot
of people think we're started because we
have a logo we're just a grant funded
projects and with it basically in the US
there's all these dead rich people that
when they die they give all their money
to a trust or a foundation and then they
spend the money in the money lasts a
long time because people be really rich
here so we've been funded by like
newspaper empires by automotive empires
exploitative industries in the past that
are now trying to offset their anyway so
that's how we make money if anybody was
wondering 100% precedent open source and
we have a distributed team were working
on some really cool stuff and we wrote
we've been writing a lot of
documentation about kind of how we're
doing the project from things like I
have a repo that's just like how our
team is trying to work as a remote async
team we also have been writing a ton of
modules like I think just between me and
Mattias we have like 500 modules and so
we took the time to kind of like write
down how we built the thing so you can
go to our docs on tat and read those the
main goal of the project the current
funder came to us and said we see that
you're doing stuff with data I had come
from a government open government
background I said we're seniors you're
doing stuff with data have you
considered working with scientists
because there's this movement called
reproducible science and we want your
work on that if we don't pay you to work
on that you're going to go work on
something else so we want you to work on
this and I was like that sounds awesome
i love science i'm not a scientist I but
they're like that's cool scientists need
better data tools and better
reproducibility tools it reminds me a
lot basically have like Travis CI but
for science or something like that maybe
that's a bad metaphor but the idea is if
you're a scientist you publish a paper
how do you build a paper you wrote some
code that generated some la tech maybe
you generated some graphs based on some
are code or some Python code and you had
to feed some data into those pieces of
code and you had to scrape the data from
ftp servers there's this like this
incredible stack of dependencies and if
any other scientists get citron ian as a
miracle that's the state of science
today and there's no incentive to do it
you get tenure by writing papers you
don't get tenure by writing test cases
for your unit tests for your science
code so there's a lot of people with a
lot of money trying to fix this problem
and make science something where two
scientists can actually reproduce each
other's results because a lot of people
think if it's a it's not science until
two people can independently verify
otherwise it's just you're trusting
someone's things that they wrote done in
a paper so what our project is about is
kind of like get but
data so source control before get you
had did anybody do code before get or
like maybe even be for subversion it's
uh and uh we used to do that remember
they're like you know 4050 years of
computing resource control people did it
and you know if you had a cool project
you want to fix a bug how did you find
it you got a zip file or something you
unpacked it and you manually edited it
and notepad.exe or whoever and you would
email the file back to the maintainer
and maybe you never heard back it's like
a black hole and like the fort number
four is actually the most important one
like nobody knows what happens after the
email back usually the maintainer would
like maybe copy paste your code in and
make a new zip and there's hope the new
zip finds the users as totally just like
the most insane there's just like no
rhyme or reason and then get came along
and get added all these cool verbs that
tracks changes and you could actually
diff and stuff like that but you still
had to email that was kind of annoying
emailing patch files but hey it was
better than nothing and then github came
along because get like a few years later
and you didn't have to email anymore and
this is actually from a collaboration
standpoint make email inboxes aren't
public so it's really important that if
you're about to send a poor quest you
can be like wait I should check if
there's somebody else that's already
sent this for requests that's what
github does among other things emoji so
with get you can just you don't have to
do all those manual steps you can just
get pull so our claim our kind of
operating premise of the project is that
data sharing I mean anyway this worked
with data knows it's the same thing as
the zip file source control thing it's
like you find an ftp server you download
a CSV if to parse the CSV into your
database you end up writing like a stack
of code that only does one thing and
it's super monolithic and brittle and if
six months later you want to switch to a
different database you have to rewrite
all your code or if you want to download
the CSV again you have to like delete
all the data you imported last time and
then just re-import the whole thing
again or it's just like there's no
automation if there's no automation
there's no reproducibility for these
scientists so you know you have to do
things like emailing CSV files around
now this is a popular one if it fits in
the in get you can just put like you
know your sequel table or export into
get which is you know just like a crazy
hack and so we're just trying to do for
data what get did for source code and
don't take that to literally some some
people think I like we literally have
the same commands as good but it's a
different problem so we have a different
approach but we're inspired by the
automation that get brings so dad is a
module you can install that and we have
a lot of dependencies the only
dependencies and aren't JavaScript our
node itself and level to be and there's
a command-line API so the main idea what
that is you you know you can do data in
it create a new debt store in a folder
anything that produces data you can pipe
into a debt import and then that starts
tracking the data in debt and then once
it's in debt you can do things like that
listen which starts a server that other
people can debt pull from ORD a clone
you can that push to another server
we're just trying to make these
automated data sharing pipelines easy
and on the command line I forget I
always forget that I highlight those so
for instance let me just show a demo I
have my 23 Emmy data somewhere in one of
these so I have in the command line this
is just like an empty folder there's
nothing in here nothing at my sleeve and
if I do that in it and make a new debt
folder it kind of asked me for some
metadata I'll say my genetic markers
from 23 and me and I guess I am my own
publisher of my DNA and so if i run this
little pipeline the data comes from 23
with me as a tsv and so it kind of looks
like this it's basically CSV about these
are tabs and I can parse it using the
csb parser module that we wrote it with
a tab separator so if I basically just
run this little pipeline here then the
last one is that import so I'll run that
and it'll just sit there and start
importing my parsing the tsv and
importing it into debt and now I have
I don't actually know how much data is
and this thing's I think it's like 80
thousand base pairs or something like
that but this might take a little while
because genomes are big so I'll just
kill it I'll say dat listen and I can
open up the server and we have a little
admin UI and now it'll kind of oh I
think the first time it has to do some
indexing let me do a debt cat oh yeah
here we go so there's like I just
imported like 91,000 rows of my data
into debt and what it adds is the
version so if I go in say I'm a genetic
mutation and I say this is now a CC and
I update then now there's version 2 of
this row and that means that also if i
go into if i go to the changes feed kind
of like CouchDB style and i get the last
one then the last change to my genome
was this key change from one to two and
when you track this kind of metadata and
the metadata we use to do replication
and that means and we're also building
kind of branching and merging and so you
can kind of you pull requests on data
and everything is streaming so the whole
idea what this is you can track changes
to data have versions of data push and
pull data between remotes and we're
building a lot of cool plugins on top of
it and so you can clone a data set from
a server and oh and then once you have a
data set you just do dat pole and then
you get the newest version and you can
do that pole live and you this is where
we started diverging from get with get
you can pull like discreetly but with
that you can be continuously pulling
because you in such a database so you
can have the data be like live updating
my dropbox and we don't just do tabular
data we also do large attachments we
call them blobs binary large objects and
there's a blob API for storing blobs the
blobs are not stored in the database
they're stored on file system but we
store all the actual data like the key
value data in the leveldb because it's
good at that we saw blobs on file
systems because file systems are good at
files but we've abstracted the file
system using this module called the
abstract lobster
or and this is like a community thing
that we're trying to get going which is
say you want to upload files anywhere
don't write code that uploads to s3
directly write a s3 blobstore or
actually it turns out that we've already
written one that you can use for s3 but
if it's basically an API where you're
streaming in and streaming out and it's
files that's what the have struck web
store is meant for but what's cool is
you can use our test suite and then
you're compatible with all the other Bob
stores so your uploader service can
instantly be compatible with all the
blob stores that we support or that are
implemented by different people so we're
just trying to introduce nice little
conventions so that we can not reinvent
the wheel every time we want to do file
upload oh yeah we also have cool ones
like you can read a file from bittorrent
using the same API as you use to Rio
file from s3 or do you ftp or local FS
or in memory so check check out the
abstract blobstore module and we also do
some stuff just really quickly with that
when you import data we generate you a
JSON schema by default but you can go in
and we actually use protocol buffers
from google to do the actual encoding so
if you have a specific schema you want
you can do your own schema we give you a
free rest api on top of the database i
kind of showed you the changes feed
really quickly and everything a dad is
totally hundred percent streaming so we
can work with really large data sets
without crashing debt so some demos so
npm is a cool data set we actually have
been importing the npm metadata into
debt so we have up online NP mg org dot
org isn't a real thing it's just where
we host little deaths right now so we
have 117,000 modules it's more than npm
because we store the deleted ones also
and we store the full data so npm
actually has like a lot of data every
version of every module is in every row
so and then we have the read Me's so if
i take this URL and i go to the terminal
and do i already have a one here okay
cool so if I debt clone this what it'll
do is start cloning it and what it looks
for is the blobs oh yeah so we actually
store
links to the tar balls and by default
when you clone that will clone the tar
balls so you'll have them locally so as
you get a row it also fetches the blobs
so you're actually I mean this will be
140 gigs it'll eventually finish and you
will make a full copy of NPM if you have
a lot of bandwidth it doesn't mean it's
doable but it's a little bit much for
conference talks so let me start over
I'll delete the folder and this time
I'll do dash dash skim and what this
means skim mode is we just skip the
blobs we just stream the tar balls so
then I you know it only takes about 10
minutes to clone all of NPM just the
metadata including all the reviews and
so then you can actually run your own
little local registry and would you
actually want to get tarball it just
lazily fetches it because we still have
a link to the tar ball and two debt it's
just a matter of it has it fetched it
and cashed it yet or not so and you can
stop it and then go back into there and
do it that pole and it'll finish where I
left off and then you can do pull dash
dash alive and it'll keep in sync so
just like a cooking show I have the full
version already done and if I go to
where's my slides did that one so we
have a little demo where you can
calculate how big npm is all we do is
loop over every row using that the dad
JavaScript API and we just have a
rolling some of the all the blobs how
many tar balls are an NPM basically so
if i run this size of NPM basically just
streams through debt and just calculates
a big rolling some of the accumulated
tarball sighs and I think it's like it
gets around 150 gigs or something right
now but you know it takes a minute to
run and so that's examples like
streaming data out of that but we also
have doing something a little bit more
cool you can pipe that cat which is just
all the data and debt out into some
transform anything UNIX for super unix
fans and you can also send stalker is
unix see you can do really cool stuff
with docker so you can this is kind of
like a really important use case in
science because dependencies are hard so
we've been using docker for a lot of
these crazy science dependencies so this
is an example
if I wanted to generate a screenshot of
every read me on NPM as a stream we
wrote like Mateus wrote this cool thing
called both marked down to PNG and I can
just run that really quick so we have
this little pipeline that gets all the
modules greater than or equal to debt so
like the debt module and then pipes them
into Matias thing and that generates a
tar balls so let me actually grab this
and I'll not do 1 i'll be like 10
actually so oops the emoji cat in my ps1
kind of messes with stuff sometimes oh
yeah greater than debt limit 10 so I'll
generate 10 read Me's like the first 10
modules that have that in the name on
NPM so what it's doing is it's piping
data out of debt spawning a little
virtual linux container using docker
generator to read me in a headless
browser and then it generated me this
tar ball just now and if I extract the
tarball then I get a folder full of
PNG's and they're basically rendered in
phantom jas but you don't have to worry
about that you just use the docker
container and everything streaming so
you can do like these really nice
advanced data processing things on data
sets of infinite size with like relative
ease that's kind of the goal and we also
made this which we're really proud of
which is called where is it here I'll
just go to it directly it's called get
debt we also use docker for this it's an
in-browser debt environment we log you
in as root to a new linux machine on our
server it's a virtual machine and You
Know Who am I I am root your sandbox
though and you can type stuff like NPM
install the unique module from NPM and
it'll actually get a real and p.m. it'll
do real and Pam install on the file
system but in order to make it a little
bit easier to learn we sync the file
system to this little editor that we
wrote so you can actually go in there
and inspect it and you can say oh I want
to go edit the source code of this
module and say console.log hello Jess
fists and then if you go up here and you
go into the node ripple and require
unique then it'll type hello jazz fest
so we sync the filesystem back and forth
from the brow
to the server so the point of this is
you have this tutorial over here that
teaches you the fundamentals of debt you
can also do stuff like require an HTTP
server um and a live coding with latency
so if I write a little HTTP server in
here we wanted it to be real so it's
like you could actually do real stuff on
a real server and not do like a fake
environment like what like Codecademy
does or something like that so if I just
say res and high then i go to my welcome
text it has a when you load the page it
gives you a unique coast and anything
that listens on port 80 is a real server
on the real internet so I have high here
so what's cool is when you close the tab
we close the process so you can't it's
kind of like our anti abuse policy so
now I can't find it but we do store the
data so in the tutorial you actually use
a real debt you start a real debt server
and you actually actually clone your own
data from the server down to your laptop
so you install that locally on like step
5 so we want to do something real and
this is actually a preview of we're
working on adding this functionality to
all node schools so that anyone can get
notes cool in one click and not have to
install everything locally if they don't
want to so we're also working on a bunch
of DNA processing one of people on our
team is doing his PhD on this stuff and
we have a project called bio node and
what that has taught us is that we need
to set you handle these advanced data
pipelines with lots of crazy
dependencies and everything needs to be
streaming because the data sets are huge
and we're using a lot of docker for
automating these really difficult to
install bioinformatics tools we've been
playing around with this thing called
gasket if you were at node comp this
summer we had a very early version of
that which is like trying to build these
cross-platform data pipelines we're
basically it's a really cool use case
for note I think because it's note isn't
doing is the computation notice spawning
the processes and moving the data
between them so it's just I mean that's
kind of I mean it's io literally it's
what note is built for and not
everything is 11 min monolithic note app
it's an invented pipeline where data is
getting pipe
in and out of different places and
coming from different network services
so I think it's a really cool use case
and gasket looks like this it's JSON and
you can declare pipelines but it's
little bit too linear for us so we've
been experimenting with a single debt
script super experimental which you can
define pipelines that do stuff like you
know this is a reads pipeline it will
run a search that outputs JSON and then
fork for pipelines and pipe the search
output into all four and then aggregate
those for pipelines into a debt so you
can do really powerful stuff in a really
simple expressive syntax this is like
super alpha but where this is the kind
of stuff that were interested in stuff
that's the future of the project we want
to be able to check out a data set to a
specific point in time just like it so
you can say get me the data as it was
two weeks ago so that when you publish
your paper you can have the hash of the
data of the paper so people can
reproduce it we're doing a lot of really
cool stuff for multi-master replication
and merging that Matthias will talk
about and we want to sync to a bunch of
different databases so that you can have
stuff like I wrote a thing the other day
that takes any data from debt and puts
it into a full text index you can just
do that automatically and not have to
install a database it uses sequel light
under the hood but we can stopped up for
like elasticsearch doing these like data
automation tasks and then finally we're
building a registry which is like what
I'm super excited about the registry is
like a super sneak preview first time
we've shown it publicly at all it's not
even like Alpha yet we're still working
on the first version but you'll be able
to publish data sets to the registry and
find them and clone them and get
information about them and actually view
the data in line and this is super super
early like i said but we think that
putting all the data online and making
it easy to find is going to help a lot
of these build these ecosystems out so
with that this is like how to find us
dad on freenode we have a gator and or
also the debt repo and i'll hand it off
to Matthias now thank you
good</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>