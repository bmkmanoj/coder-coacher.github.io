<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Andy Wingo: Stranger in these parts. A hired gun in the js corral. | Coder Coacher - Coaching Coders</title><meta content="Andy Wingo: Stranger in these parts. A hired gun in the js corral. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Andy Wingo: Stranger in these parts. A hired gun in the js corral.</b></h2><h5 class="post__date">2013-01-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hHytsQkVlhA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hey y'all my name is Andy Wingo it's
real pleasure to be here thank you and
and I'm a compiler nerd I have to say
right yeah it's a technical title
actually I work for a gallium it's a
spanish software consulting company that
does mostly consulting around free
software projects and if you know our
work it's mostly it would probably be
because of our work on whether it right
where it's difficult to count these
things but we're something like the
third largest corporate contributor to
the WebKit project we work on ports for
customers integrations for device
manufacturers and things that and then I
have been working on JavaScript core for
the last spend some time with v8 but I'm
I'm mostly going to focus on JavaScript
core oh yes fun fact you Galia is a
worker owned cooperative if y'all are
interested about such a strange project
come to me afterwards and you know
asking about the thing but I'm not
really a JavaScript programmer right I'm
I'm a schemer at heart that's you know
but it's been just real interesting
getting in the thick of these JavaScript
implementations they're fascinating
things and I find them you know quite
quite interesting so this is gonna be a
talk about some things that I've learned
about the various implementations and
it's going to focus on JavaScript core
because they don't do a very good job
marketing and I don't think anyone
really understands how it works
right so I figure it's a a good you know
time to take out some of these things I
have a feeling like my perspective is
that my languages are essentially tribal
right that's that's the thing that
defines a language community that you
know you have a group and you have in
and outside and you have your respected
elders and your tribe and such and I
come from you know this Lisp tribe and
one of our old bards was Peter Norvig
you might know I'm he's through a
research now at Google 20 years ago he
wrote a book called paradigms and
artificial intelligence programming and
the book has not held up as far as
artificial intelligence goes but it's I
think it's quite interesting on a you
know the practice of programming and
some efficiency concerns
and he he did some retrospectives on
this book five years after in ten years
after he hasn't done a twenty or one yet
but I'm aware of but he extracted you
know these lessons and they're kind of
cute one of them though that really is
stuck with me and all my programming
stuff be it god forbid even in C++ would
be that they're this statement that
there are four general techniques which
you can use to speed up an algorithm
being caching compiling delaying
computation which sometimes we call
laziness any through in this thing
called indexing because the bad time
Lisp programmers like to use linked
lists for everything and Husing please
use a an array or some structure with
better Big O complexity but you know in
any case this is you know escaping from
as a way to interpret what I have found
in in the JavaScript tribe so one
extreme example of this would be the
inline cash phenomenon so lose your hand
if you own inline caches sorry get
through the room or so so this when
you're a language imitation and you see
like X plus y like you have to do
something right you have to add it at
some point right but the question is how
exactly are you going to go about doing
this well for example in v8 and more
even than that and in Dart
what you do is you don't do anything
right you don't implement it at all
until it's run for the first time and
then when it's first run you see what
types are flowing through that call site
you compile a version of this operation
specific to the types for that call site
and you cash that code in stub so
they're those three most important
elements of norfolk's Maxim they're sort
of worked them together and of course
this technique also applies to field
accesses you know x dot y and
interestingly in v8 the the caches work
as input the optimizing compiler later
I'll talk a little bit more about that
in a bit but like I say on JavaScript
core and specifically to the new tiered
compilation model I'm gonna do the hand
raising thing again so how many of you
have her
the term TFG TFG very few I guess lol
int probably fewer okay some though
awesome great
so javascriptcore works it's it it
compiles your code to bytecode and then
the first time it runs it on cold code
it will interpret that bytecode and then
if that code warms up it will compile
those byte codes into native code and
then if that code JavaScript core
determines that it's hot then it will
optimize that compiled code and produce
a version which is specific to the types
flowing through that call site and it's
a speculative optimization meaning it
doesn't have to be general like so that
the the highest then it can fail at some
point say you're having a bunch of
integers and integers and integers and
something you see like a float
javascript or floating point but there's
the common optimization that small
integers are represented differently at
that point you would fail the
speculation tier back down from tier 2
up to tier 1 and wait again and try some
more and I think I'm sure many of you
have heard Larry wall Perl creator haha
but right he has the cardinal virtues of
programmer being laziness impatience and
humorous and I think they correspond in
this order actually to the L and the the
baseline JIT and the DFG JIT so the ll
inch is the lowest level of JavaScript
implementation javascript cores
implementation and this is very very new
it's maybe it landed a month and a half
ago or so so I'm not surprised y'all
haven't heard of it and I'm not
surprised you haven't heard the DFG
either because that's also quite recent
it's the new interpreter it's the first
pass at your and javascriptcore I need
to give a bit of context though for you
to understand how it's different than
the old one so I'm gonna do a bit of a
deep dive into the bytecode the bike
compiler the old interpreter and then
I'll show the new one and we're going to
see a bit of C++ here and I'm sorry you
know I have to deal with it'll work I
figure y'all got yourself a little bit
of it you know some somehow for
justice I guess so anyway if you have
javascriptcore for example you have
downloaded a WebKit nightly and you find
the JSC binary which is tucked in there
somewhere for the Mac platform or you
build javascriptcore yourself you will
end up with this binary called JSC and
that's the interpreter when you run it
with the data GE argument it's going to
dump any time it creates bytecode so you
know main function and you put it in
prisoner right it's going to print out
some byte code that it emits and the
interesting thing is like where where is
the addition here here we're just ok the
result of defining this thing is the
undefined value which is kind of a whap
thing as far as language goes but you
know where is the addition so like we
said about laziness javascriptcore and
v8 and I assume spider monkey and God
knows what the other engines do don't
actually byte compile they don't even
par seer function until it's run for the
first time they simply check the syntax
right so the function is never called is
essentially free there are some you know
overheads for that but if if you have a
function is never called like you have a
big jQuery library or something and you
just don't call some of those things
they never get created like they never
your parsed I never get compiled either
to bytecode nor native code nor anything
else so once you call them for the first
time though JavaScript core is going to
go ahead and and byte compile it and
parse it by compile it and produce this
output here we see we have three opcodes
right we enter the function we are going
to add and it's a negative eight
negative nine that's gonna reference
arguments coming in and the result is
gonna be put in our zero and then you're
gonna return from that function you of
its I think it's it's fairly clear what
this is question is what is implementing
these op codes right that that is the
interpreter at the very base level and
the classic interpreter it's world bitch
how about that you raise hands for Go
Fish we got many but that's the name
it's sort of stuck with you I don't know
I'm not sure if the other thing to you
new javascriptcore squirrelfish all
these all these names got you know
jumbled together marketing-wise the name
of the interpreter is javascriptcore
it's been marketed under various names
and when in 2008 the bike compiler first
came out because before they had this
tree based interpreter they called it
squirrel so the classic interpreter now
corresponds more or less to did that
release in 2008 and here we go with some
c++ I don't I mean you don't need to you
know understand this but I just wanna
sure the shape right so if you find
interpreter at cpp when you download the
source and you should dig this sort of
thing and you'll find op add I your
search for it and we see at the
beginning we're parsing out the the
registers like we had the dump and then
there are two cases right one is a case
for small integers that don't overflow
and yeah there's a more general case
where we're calling out to a stub
finally we put the well both of those
handle putting the result in the
destination we advance the program
counter and we go to the next
instruction and this is kind of
interesting here the next instruction is
uh is a macro to find the top and it's a
computed goto you see the go to star
right the the opcode actually stores the
address of the label and the C++
function it's kind of you know makes
your head turn a little bit and it makes
the compilers head turn a little bit as
well because this isn't it's not
standard C it's not standard C++ and
it's a it's a form of higher-level flow
control and the compiler doesn't really
know the C compiler right GC c lv m
doesn't really know what to do with this
function right it doesn't know what
things need to be in registers what
things can be spilled on the stack what
things can be thrown away because all it
sees is all the labels could branch to
each other and it doesn't it can't do
very good analysis my so what happened
was is this fellow Philip is low which I
don't know how long he's been working in
Apple but he's been contributing our
JavaScript core for about a year so
rewrote the the interpreter in assembly
right I assume that's not important I've
never actually seen before and he ll int
is the new interpreter for JavaScript
core instead of being implemented in C++
it's amended in a specific
assembly language and it's actually
compiled to the native code with a
custom Ruby macro assembler so I think
it's fair to say that javascriptcore now
implemented Ruby
it happens beforehand it's ahead of time
it's part of the build process for
JavaScript core itself and again I'm
going to show you this this custom
language sort of for shape you know if
you're really into this the the slides
will be up later you can check it out
and obviously it's in the source as well
at first we see we're defining a
dispatch macro so this is a macro
assembler that allows you to define
macros in its own language so your dogs
right dispatch we're adding some number
to the program counter and then we're
doing the computed go to but this is an
assembly now we dereference the OP code
that is there and jump forward to the
next label these like hello end up and
it lazy reg that's the simplest opcode
it's in the interpreter basically and I
put up there just to show it there's a
trace execution which generally doesn't
execute we parse out the register that
we're going to fill with an empty value
for various internal purposes we store
the empty value in that slot and then we
dispatch on to the next opcode and
that's the entire imitation that opcode
I can't show add though not not as it's
not in its entirety because you get
adding small integers with small
integers adding small integers with
doubles adding doubles with small
integers you know and then strings right
because it's a string append as well and
then other value of type things and it
gets a bit complicated so what he did as
this complication it has to be repeated
for all these different opcodes like add
divide subtract and all these things he
made a macro called binary op that gets
passed to macros right macro being
passed to macros one which is the ratio
and you see at the bottom that we're
instantiating the binary op with two
macros which are defined there
anonymously like lambda expressions I
know you're all with me right but so
this is this is your new JavaScript core
and the question is why right
the the low-level interpreter has
of advantages over the old one not only
you know can we select what's on the
stack and what's in the register a bit
better
we can select what's in line then what's
not in line which is something that's
kind of difficult to do in in C++ to get
very fine control over you know the
ordering of your instructions that gets
you better locality which means you're
you're missing on your instruction cache
less since you're not spilling as much
stuff on the stack you know you can
throw away the garbage collector doesn't
hang on to as much garbage to it doesn't
think some things but you're dead or
actually alive and it's much easier to
transfer to tear up from the low-level
interpreter to the JIT because it's the
same calling convention because we
control the con convention like you
don't when you when you compile
something in C++ and yes
OSR is this process of tearing up or
down you're on the stack in the middle
of a computation and you replace the
code that's running the current
computation from you know one tier to
the other tier on stack replacements
also quite complicated if you google for
that you'll find a blog post in which it
blows my mind entirely so it's much
faster is the thing the initial
statement was 200% is faster I don't
know whether that means x 2 or x 3 or
what I don't really know in reality if
you have a JIT you're going to tear up
on anything it's hot and so it doesn't
make that that much of a performance
gain on some benchmarks but when you're
just slinging lots of JavaScript it's
good to have like a fast low-level thing
where you just don't do very much again
delay in common delaying computation
right and on the iOS you have this
restriction where you can't generate
native code in as an app right and this
sort of thing really helps out in that
context I would imagine you know I don't
have any inside apple knowledge right
but it seems pretty clear that's what
what the deal is the other thing you can
do is you can actually use the ll into
profile what values are flowing through
your interpreter and you can use this
information as input you're optimizing
compilers say oh actually I've never
seen a negative zero in this
of the computation and you would be
amazed like how much work goes into
proving that you'll never see a negative
zero because if you can there's many
more code cases right so if you can
prove that you can't or you can
speculate that you can't because you
haven't seen one then then you can do
much better right
so that's tier 0 the low-level
interpreter tier 1 would be what was
called squirrelfish extreme if anyone
remembers this one it's essentially you
take all the all those code segments I
showed you like those little labels and
instead of making making the the native
machine jump back and forth in between
those labels you you copy them out and
serialize them so the native machine
pointer is advancing you you you miss
fuer branches the the branch predictions
work better you can do some small
optimizations here as well and this JIT
until the element was amended was also
this sort of standard for execution of
all JavaScript it's generic it handles
all the cases and it's okay
the advantage of course is that you also
control the stack as opposed to the old
interpreter so it helps you when you can
tear up to the optimizing compiler after
that all right
no c++ right crazy macro macro LAN
again I just want to show you the shape
here we're parsing out the arguments in
the beginning just as we were doing and
any other interpreters its operating on
the bytecode it's turning the bytecode
into native code and we have a few more
you know if blocks this means we have a
few more cases that we can compile
specially meaning your code can go a bit
more a bit faster if we can tell that
you're adding a constant to a constant
small number to a number right then the
the it can predict a bit better or what
you're going to have while still having
to implement the slow path but general
it doesn't have flow control it doesn't
know what's really happening
so it can just guess it has to be fast
it just slings out the native code and
then if you actually need it the D F G
is there D F G stands for a data flow
graph and it refers to how the type
information propagates in the optimizing
compiler this is for JavaScript core
what the higher levels of hotspot or
what crankshaft is for for v8 like this
is that right this this is the
optimizing compiler that tries to you
know spend a bit more time on your on
your hot code and use the type back to
unbox your numbers to find your
functions to what else I say here native
arithmetic and objects in allocate
registers and and all the things in
order to be able to do this you need a
couple of things you need to be able to
tear down which is this OSR thing and
you couldn't tear down to the
interpreter before because you didn't
control the layout of the stack but now
with the element you can tear between
all these different levels
you also need profiling information
about you know is this thing that is a
plus in the JavaScript source code is it
out of the numbers and if it's further
they doubles or are they small and and
you need the ability to sort of abort
when you're when your speculation fails
I cool and and crankshaft is is cool in
the same way that you can just try
really hard to emit the correct code
like assume that things are going to
work out and if things don't if you see
that negative zero then you bail and and
things go on with with the older JIT and
it doesn't go as fast but maybe it's
your fault anyway I don't know
it's a DFG I this is even more abstract
but I want to point out a couple of
points here and it's this word should
write before in the baseline JIT we
didn't know whether we you know should
speculate for an integer or should
speculate for a double or should only go
the slow path of string concatenation or
what have you but in the DFG we've
actually done some flow analysis we've
eliminated
code we've propagated types around and
so we can you know have an answer for
this should what should I do
right and that's that's what the DFG is
able to do and as far as comment works
go in JavaScript core now there is the
classic interpreter it's still there
there is the low-level interpreter there
is the baseline JIT and there is the DFG
JIT and they can exist in various
combinations and depends on the port as
to what actually exists on the Mac on
x86 32 and 64 and on arm v7 you get the
combination ll in baseline JIT and DFG
this is really what you want a lot of
the other ports like I work on the GDK
portable it hasn't caught up yet and
only have like the baseline JIT gtk
right now and JIT is it a minute at all
on Windows for for examples so you just
get the classic interpreter but it's
kind of nice that you actually have a
portable interpreter to run things on
you know even if you're on some strange
platform like Windows 64 so again back
to nor bigs for ways to speed up the
algorithm caching compiling delaying
commutation and you know for charitable
I feel like we're missing something you
know because this is what Norvig was
writing about in 1992 on big lists
machines and some things are available
to us now that we're not available to
amend it specifically concurrency and
you know how can we you know bring
concurrency to bear on the problem of
making JavaScript implementations faster
and what JavaScript core has been doing
is to paralyze what's called the mark
phase of garbage collection so garbage
collection goes you allocate you
allocate you allocate and at some point
there's there's no more free memory
right so you stop you mark all the live
data all the data which gets free
and this marking phase stops your
application right so if you have
something that's trying to update at 30
frames a second or you know even more
what again
Philip is low I hope to meet him one day
because he sounds really awesome
terrible is this mark face so we can use
for cours upward force it can use more
this is a value in the garbage collector
and I think coming to the end here
finally so this is nor big in this book
as this chapter nothing called on
efficiency and if you substitute lips
list for yeah I just list if you
substitute list for a JavaScript when
you're reading this if you check it out
or what have you then you're it's it's
quite relevant you know to modern-day
programming and JavaScript and
JavaScript implementations and one of
the things he says is that the expert
Lisp programmer eventually develops a
good efficiency model you know what what
is the cost of what I'm doing there's a
quote by Alan Perlis that a lisp
programmer and there was the value of
everything but the cost of nothing but
the thing is is that this efficiency
model changes over time right the things
that are fast now in JavaScript I was
talking to a fella yesterday's like well
you know if you just do this for x
equals 1 X is less than and you know do
something on my array that's much faster
than array for each well why is this
it's because the benchmarks don't have
array for each all right JavaScript
invitations are completely driven by
benchmarks and and you as programmers
are in this loop right if something if
there's a way that you want to program
it's not working you can file bug
reports or grade benchmark sweets or
such and and get it to a point where it
where it is fast like this efficiency
model can change over time and don't let
you know what's currently fast or slow
crimper style
so to speak and I think I'd say we can
do some questions you can find me this
dress or that website so thank you very
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>