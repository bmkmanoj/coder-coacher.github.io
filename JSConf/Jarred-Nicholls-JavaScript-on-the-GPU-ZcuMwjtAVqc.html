<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Jarred Nicholls: JavaScript on the GPU | Coder Coacher - Coaching Coders</title><meta content="Jarred Nicholls: JavaScript on the GPU - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Jarred Nicholls: JavaScript on the GPU</b></h2><h5 class="post__date">2013-01-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZcuMwjtAVqc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I don't I don't know if many people in
this room know who I am if you don't get
this reference then that's not good and
you you gotta catch opening Jackie Chan
movies
so let's want to give a brief
introduction to myself Who I am what I
work on again my name is Jared Nichols
and I work at Sencha I work in the
center's web platform team which is like
a corporate name for the web kid team we
just do web Kitty things we work on
WebKit products and we can you should be
back to the WebKit project and I'm a
WebKit committer for those who don't
know what that means it just means I can
push to the repository of WebKit nothing
fancy and I'm also co work co-author of
the w3c web cryptography API this
technically doesn't exist yet it's it's
coming though the the working group is
knew it was recently chartered and it's
in the Advisory Committee stage so once
it gets out of that stage then this will
be will be a you know full steam ahead
on this and I'm working together with
David dolphin Mozilla on this so that's
just briefly who I am when I'm working
on but today I'm not gonna talk about
any of those things I'm talking about
something totally different JavaScript
on the GPU is there something that's
caught my fancy recently it's more or
less just a side secret pet project
experiment that I'm working on with a
couple of my own my friends today I'm
just going to talk about like why job's
getting the GPU why is that interesting
why would we want that if if any of you
were here for the first session on
Monday when you heard Stephan from Intel
talking he was he gave all the reasons
why we should care about JavaScript
running on the GPU doing data parallel
tasks so I will go over that briefly but
I think everybody who attended that has
the gist of it and then we're gonna talk
about getting jobs going to GPU how we
do that and the approach to that I took
it's a insane approach and we'll walk
over that and show you all the issues
that I ran to it's crazy so after you
talk about that then we talk about
what's to come
so why JavaScript the GPU again we
already kind of covered that
but they're really that there's two
answers one is why not and the other
answer is there's a better question and
why the GPU the GPU is fast okay it's
fast it's not faceted everything as fast
as certain types of things and these
things are what we need to target on
GPUs and they're fast because there are
totally different paradigm their
architectures totally different from
CPUs they're geared towards data
parallel tasks and stream processing so
taking a stream or set of data
performing operations on every element
of that that set of data and doing it
all in parallel and it takes a
divide-and-conquer approach it has many
small working stream processing units to
to do this and each of those processing
users you can say that it can run a
thread of general-purpose code and for
example on my on my laptop here my HCI
Radeon has four engines 480 stream
processing unit sources effectively 480
small little cores to do something for
us on my mac pro back to my office that
has like 1600 so you can imagine how
much you can get done at one time on
these on these GPUs in general they have
a high memory bandwidth a lot higher
than system memory
most of the advertised values are
theoretical so you have to take
advantage of the GPU 100% to get to
those values but in general the
bandwidth is much faster than you know
system memory to a CPU and they can
they're specialized you know they were
made for mathematical operations
particularly with you know games and
they had to specialize in processing
floating-point operations really fast
and a lot of them at same time but they
don't solve all problems again they you
can't just throw any task or any any
problem at a GPU and expect it to go
faster it's just a that's that's a false
statement so they have you have to throw
something a problem set that can be run
in parallel homogeneous if you will
side-effect free Stephan cover this well
so I won't go any further so I just
asked myself a question how well would
jobs could run on the GPU if we throw
JavaScript running GPU let's I just want
to know how that would happen all right
how well would that work so we're gonna
find out so a couple buddies of mine who
work Accenture as well we decided to
start a little experiment we codenamed a
lateral Jas and that experiment was to
get JavaScript as a first-class citizen
on GPUs to run 100% compliant and to
take advantage of the GPUs in certain
ways to do data parallelization and
accelerate operations like native native
floating point operations instructions
built right into the hardware so we had
two options to get a general-purpose
you know whatever to run JavaScript on
only GPU those two options were open CL
and Nvidia CUDA and it's it's
interesting how we how we chose what we
did we went over a list and Nvidia CUDA
has a lot of great abilities basically
you can write a lot of general-purpose
code and it will run on these GPUs but
it's only for NVIDIA hardware and we
want to kind of make this stuff run on
all hardware all variable GPUs OpenCL
accomplishes that and it has support and
drivers for eight AMD Harbor and video
hardware and even Intel like a
celebrator processors but everything
else about OpenGL sucks
it's a terrible version of C you can't
have dynamic memory you have no
recursion available all functions are in
line no function pointers there's no
tooling like really hardly any tolling
the Intel XDK is pretty good but I mean
the tolling is just far behind on videos
and it's a little bit mature it's a
newer newer open standard but we went
ahead and chose up and see all the one
that sounded like a complete nightmare
so we really wanted to target all those
different GPUs but really the truth of
it is our Mac books had AMD cards so we
had to work with that so we had a couple
of different routes were gonna take and
the first one was doing a static
compiler which is exactly where River
Trail from Intel does it statically
compiled JavaScript into up and CL code
but we chose not to go that route
for one we wanted full JavaScript
support we wanted objects and prototypes
closures recursion variable typing all
the things you get with JavaScript we
want you know compliant JavaScript to
run on this on the GPUs and to
accomplish a really good stack of holo
you have to do a lot of type inference
which is pretty limiting it can be
solved for some cases but the rabbit
hole gets pretty deep to figure it out
stuff out statically and you know if you
if you have a really good static
compiler can figure stuff out you're
basically at that point interpreting the
code so that compilers are tough to
solve all those problems and get 100%
applying JavaScript running so again
with the sac compiler you're kind of
limited to those kernel size functions
which are perfect for you know doing
these data breaking up these tasks to
homogeneous parts and and running
parallel tasks on this hardware so you
know writing a small kernel function in
JavaScript totally possible to
statically compiled that river turtles
doing it but we just didn't think it was
insane enough we wanted to do something
that is just totally crazy and see how
it would run so what we did was Roberto
J's interpreter and OpenCL seeing which
is really crazy and you know for the
reasons reasons why we did that it's
just the opposite of what I just said
you know we can run full JavaScript it's
it is insane we have no idea how it was
going to perform we know wasn't gonna be
great we just want to see how it work
right and you know it is a challenge it
would be a challenge to make that happen
especially with all the deficiencies in
OpenCL so we thought it'd be fun and
then if you can make it at least work
then we can make it better and actually
make it you know something that is
practical to use to solve real world
problems so first we talk about all the
headaches we ran into just diving into
OpenCL and getting an interpreter
written as we all know the best cure for
headache is decapitation another
insanity well if I love the insanity
wolf so we ran into some issues and
here's a list of them with these
general-purpose
GPU SDKs or specs if you will they they
allow you to really optimize your code
to different memory addresses there's
multiple memory address spaces on these
on this Hong from these devices that you
can target with your code and they kind
of tear down from really fast but
limited amount to you know your entire
available vram but super slow bandwidth
right so you have all these different
options and in dealing with those
different address spaces you have to
know that you know when you're dealing
with pointers to address to addresses in
those spaces they are totally different
they're just autonomous you know sets of
memory and so when you have all these
pointers you have to fully qualify what
the pointer is pointing to so it's tough
it's this really verbose code again no
recursion all inline functions so you
know writing interpreter without
recursion it's certainly possible but
it's hard so we'll get into what we have
to do with that and there's no standard
Lipsy libraries whatsoever I mean you're
stuck with nothing really you don't have
you know mem coffee or string age or and
you know any of that stuff it's not good
no dynamic memory again so what do you
do with no I mean you can't do you can
allocate memory okay so what do you do I
mean and then you know they don't have
any standard data structures they do
have like vector you know for numeric
values that you can do vector operations
which you know harbors tuned for that
but apart from that they don't have like
standard data structures like stacks and
queues and linkless and so like that so
and then in general OpenCL since it's so
fresh you know the compilers with the
drivers are pretty buggy they have
problems they have memory alignment
issues it was a nightmare trying to get
this thing get this thing working so in
general it's bananas there's totally
bananas so real quick the memory space
is you have private which is very fast
it's like cache on each individual
processing core in the in the hardware
effectively it's super fast but you only
get about 64k of the per working onion
then you have local memory which is also
fast it's available to a group but
limited again like 64 K on average
depending on the harbor and the drivers
then you have global memory which is
basically all of your available vrm it's
slow much much much much much lower than
private or local memory but it's
available to you know every available
work item that's running for that
particular kernel on the device so to
talk about let you know to go a little
more detail but the pointer problems we
had with these different address spaces
you have to fully qualify pointers to a
particular address space if you don't
qualify it then it's implied to be
living in the private very pointing to
address in private memory and so when
you write functions you have to fully
qualify the incoming pointer if you're
passing a pointer to a function and that
gets kind of nuts so that you can see
here like if you have an address for K
is pointing to different spots and
global local and private so you have to
keep that in mind you can't just turn a
global pointer into a local pointer and
expect it to work you have to copy the
data over so because you have to fully
qualify all these dress spaces it just
got really redundant so we create some
macros to help us out and keep the code
a little bit shorter now no recursion so
there's no call stack and I don't know
whether it's a chicken and egg omelet
there's no call checkers are in lined or
they're in lining it because there's no
call stack I don't know what it is but
you can't write recursive functions
again no standard Lib C libraries new
mem copy string copy string compare so
we just implemented that ourselves
now we had to do this really strangely
we had to actually write these functions
as macros and then create a whole bunch
of versions of the same function but
using different address spaces
qualifying different address spaces so
you can see that this macro will produce
12 different copies of this M copy
function which is kind of it kind of
sucks because I mean these functions are
all in line when they're used so I was
taking up a lot of space but that's what
you have to do to make this clean and be
able to just cleanly pass in a
destination of a global pointer and a
source of a private pointer and just
have it work I mean I don't at
development I'm you know which what your
source and destination is and so you can
say I want to do mem copy underscore GP
and it'll just work you want to do like
casting and whatnot so so we had to do
this for every single Lib C function
that we implemented which is all a
string age some of
etc so it's kind of nuts no dying
memories a new mouth no free what to do
do it ourselves so we had to create a
large buffle of buffer excuse me of
global memory which would just call our
heap it's our virtual heap if you will
and we inflated our own malloc and free
we had to create our own pointer uh
excuse me uh handle system is like a
smart pointer that kind of points like a
virtual memory right and we had to do
that because as soon as you free data
and we decide we win the kind of
compressed data or rearrange it much
like an OS would do pointers would be
immediately invalidated they wouldn't
point to the right data anymore so we
had to create a handle and every time
you wanted to do something with the
handle put data or read data from it you
had to get the pointer out of the handle
and that pointer always be the right you
know point to the right spot and then
you could do something with it so that's
what we had to do it's interesting but
um I'll tell you what
doing that I got a lot of these screens
I definitely had a lot of memory issues
and infinite loops and things like that
yeah so I saw this I can't remember how
many times out of restart my computer
it's fun so you go I get the point
there's a lot of headaches for doing
this so finally we solved these problems
OpenCL and now we're actually getting to
what we wanted to do so we create out of
a architecture of this host that runs in
a process and the host has an embedded
v8 engine that runs the Escrima parser
everybody here saw Paul Irish talk about
ass prima a little bit already out he
he's a colleague of mine he wrote a
stream into JavaScript parser it's super
fast so we're using that to generate the
ast of incoming JavaScript then we had
like a little module in the host that
does data serialization to and from the
device and then a device manager so you
can actually send JavaScript at the same
time to different available devices on
the system to get multiple GPUs they can
manage that on the GPU side then we have
our data heat that I've talked about a
slow terrible stack based interpreter
well we'll get into that and then a
garbage collector so like the whole
interpreters on the GPU and the host is
basically just sending commands and
things like that so what happens today's
we get some code we're going to evaluate
it it builds a json AST we convert that
to a friendly c structure so we can
easily read it over in the the open CL
sign and we shipped the command say
interpret this ast to the GPU and we get
the result back so AST generation this
is kind of how it works
Java goes in this premium we get the v8
object out we enumerate the v8 object
convert it to see structures and then we
send the C structures over as one big
memory blob to the GPU so we can work
with that we're we already put up with
enough we're not gonna write a JSON
parser and OpenCL see it's not gonna
happen
so so the first thing is embedding the
stream of us so we very like a quick you
know a resource generator all does it
take the file in spits out you know a
string of all the bytes and then we can
use it inside of our code compiling a
stream of running after the first time
which initializes it and now it's ready
to parse something so that's the agency
generator initializing itself then you
can throw anything out you can throw
JavaScript at it so here's an example of
a variable declaration XYZ and it's just
it's a new expression for array of a
size 10 this is the function experiment
parse and if you don't if you never work
the v8 then it all it's doing is calling
a spring MoDOT parse passing in the
JavaScript string getting a v8 object
out and the v8 object holds
json HT that looks just like this so
then we have to take that and convert to
the lateral ast Struck's
so these trucks are made to you know be
compiled and ran on both the host and
the OpenCL run time so they have
compatible types we had to do some
macros to get types working you know
cleanly on both both sides of the fence
but we have an ast node structure for
every single node in that JSON that you
saw if we were to write this by hand
what the v8 object and numerator does it
would look like this it's a bunch of
creation calls passing in different
parts different nodes AFC nodes and
creating blocks of
ASD knows here's an example of creating
identifiers so we had to get the
calculate the size of what this identify
will be in total including the structure
size and then the string of the identify
itself allocate all that memory and then
we put the string into the right spot
just tuck it right after the the
structure so it's all one can take it as
you know blob memory it would look like
this you know size 16 name is a field in
the identifiers
structure and that just officer from the
beginning of the header onion excuse me
the beginning at the beginning of the
shock and and so it's pointing to 12 and
that 12 that's beginning of our string
XYZ in the four bytes another example
new expression is doing the same thing
calculate the size to fit it's solved
nodes into the the new expression blob
and then it's copying all that in and so
this was it would look like this in
memory you know it's a total size of 50
tube lights you have offsets for qwali
20 arguments 40 and then at 20 and 40
you you have those structures there in
place so you can enumerate down if you
will so lateral ast structs there again
they're shared across both the hosts in
the observer one time the host right
stinkin creates them slowly and then the
runtime is strictly reading down
traversing it and it's really important
that we did this as continuous blobs as
you saw so that it was easy to send the
gpo you're basically sending from byte
you know start to bite end or a byte
start this many bytes and it's one
operation the Santa GPS we only have to
go through the drivers and sent it a
device one time the whole entire thing
as opposed to you know following a bunch
of pointers doing a bunch of writing so
and it's still simple a diverse on the
open CL side with a pointer arithmetic
so now we're gonna get the de sac base
interpreter this is where it gets fun
we had some building blocks this is all
stack base and it was the easiest thing
to do at the time because of the fact
that there is no recursion and we wanted
to get done fast we were lazy we didn't
do any compilation or anything so we
just did all stack base so there's a
stack for the ast traversal there's a
stack for call operations
do something with a particular part of
the ASD and then a return stack so after
you run an expression you have some sort
of result after that we push those
results on to the stack so they can be
popped up later there's other parts that
go symbol symbol table is important and
then there's just a loop that goes
through these these or fills up these
stacks of how things off these stacks to
interpret a code we're gonna see how
that works
here's the kernel function that that
does it's the looping we basically
Traverse as far down as we can until
there's nothing left in the diverse
stack and then we start popping things
off the call stack and running them that
things in the call stack could put more
things on Traverse stack like a for loop
for example but uh it once there once
both stacks are empty you're done your
your application knows ran to completion
so let's quickly interpret this far x
equals one plus two so here's the here's
the ast on the Left we start with empty
stacks right so first we're going at the
variable decoration it's going to push
itself onto the ast stack on the next
loop it's gonna be popped off and its
job is to push the variable decorator
ones with this ast stack so traversing
down at this point we're kind of going
down as deeply as we can and then we'll
kind of pull our way back up with the
results and stuff so so it pushes very
well decorator on pebble decorator pops
off and pushes out the identifier and
the binary expression onto the SD stack
and then puts itself in the call stack
to later retrieve results then binary
expression is is popped off puts its
both of its operands onto the ast stack
the left and the right which are just
literals so one literals popped off
putting the call stack the next ones put
in the call stack and then the
identifiers the last thing now our ast
traversals done we've hit every node
that we can we want to at this point
it's now we can actually start popping
off calls from the call stack and doing
something with them so identifier pops
this off off creates a JavaScript string
puts into the resolved stack literal
does the same thing pop self off puts a
JavaScript number and into the returned
stack is you know
and then same thing with two and then
binary pops off it pulls in the right
operand and then the left operand as you
could see right it was two and left was
one and then it does something with them
it wants to perform the binary
expression so it does the and it sees
the operators plus okay I'm gonna go
ahead and add them together the results
three and it pushes that result on to
the return and then the last thing in
the call stack is the variable decorator
so it pops off it pulls off the init
three and then it pulls off the ID X and
it says I'm gonna store into the symbol
table you know a symbol of X with it and
it has a value of three and now we're
done interpreting their code so that's
kind of how it works and anybody who is
a JavaScript interpreter guru maybe like
Andy is probably like this is the worst
thing I've ever seen in my life
because it is it really is but it was
this was all like a perfect concept
experiment to see what we can do with
this right so we wanted to see how it
would work how it would perform so we
just we did as quickly as kind of stack
base benchmark it it's actually to do so
we did a just a really small loop of
flops you know the harbor's really good
at flops we we mapped you know the power
operation to the native power
instruction on the GPU so it run as fast
as possible that's not the slowest part
of this thing though so we ran it the
lateral interpreter on both GPU and on
the CPU since OpenCL can do that it just
compiles with LLVM down to the x86
instructions or whatever process you're
running and and we just want to compare
the two and then we ran the same block
of code in v8 and this the results are
not that surprising GPU incredibly slow
116 milliseconds just to do that little
tiny thing this awful on the cpu is 0.2
2 milliseconds which is interesting
that's not not well it's kind of what it
is what we expected but the difference
is not what we expected is a huge
difference and v8
shocker is like no time at all it's in
the nanoseconds so you don't say
so surprising so like I mean this this
number the numbers in the lateral sigh
I'm giving too much credit I was gonna
say its inflated and a little bit
because it's doing some prep but it's
nothing has nothing to do with that
really the the interesting thing is when
I run this on my Mac Pro which is a much
better video card that numbers in half
so it's still terrible down
so overall the experiment failed but
we're not giving up and we just wanted
to get that done as fast as possible to
see can we do it and I feel free Kotori
us of conquering up and sale to actually
do something useful but it's not what
the harbor is meant to do at all but we
can take it further we can we can do
something with this but we had to recap
in like what we're wrong and the first
thing was everything went wrong I mean
we had to solve all those problems all
those headaches and that took more time
than doing that's that silly little
stack based interpreter so once we got
once we saw those problems we feel like
we can do anything at this point but you
know the sack to deter Birds just
following the yellow brick road of the
ast it was just no optimizations
whatsoever
it wasn't do any compilation or anything
like that so so that was not the right
way to do it or the best way to do it we
are accessing a lot of global memory
because of our dominant dynamic
allocation needs we had to suck that
teat a little bit or so we wanted it
really badly so we everything was done
in global memory and we had a lot of
access global memory that's the slowest
memory available so we we had no
optimizations there whatsoever we could
do a lot of caching and private memory
or whatnot
to speed things up and we never did that
and we never broke things up excuse me
never birth things up into any parallel
tasks for data or even just you know
standard instructions so let's reflect a
little bit stack-based slow it was a
memory halt good use a lot of memory
like it collapsed it I mean it's
terrible and just that just that one
statement hit the sack 30 times and then
in between that did a lot of dynamic
memory allocation and freeing you know
and it's our own software an
implementation of that so
it's really slow and you know again we
didn't know inline optimizations of the
actual code we just interpret it
directly from the AST so we were just
rate up lazy but we can do better I mean
we could do something like compiling
into byte code on the host and then
doing like a register based interpreter
on the on the device along with other
things that would make it worth you know
more a while so global memory access
again just too damn I we we have to
limit that I want to give it a we
conduct a little experiment to just see
what the difference really is I mean
there are theoretical bandwidth numbers
out there but so we we went ahead and
created like two little kernel functions
one that was going through a large loop
that acts as nothing but data or excuse
me actually stuff from a global memory
and then create another one that created
temp temporary private variables
incremented and touched them throughout
the loop when every loop iteration and
then just touch global memory one other
time at the end the difference is
astounding the first one ran in 11 point
1 2 seconds in the second one ran in
point 0 for 4 seconds so with the proper
optimizations on memory access I think
we can get pretty far with our speed but
that's not the only thing we need to do
we need to do some magic and that's try
to figure out a way to break up do here
your sticks on code and break things
into parallel tasks not even with the
developer explicitly asking for it there
are times where it's possible to do
detection on what's you know side-effect
free what is homogeneous what can be
homogeneous especially if you're doing
an interpreter you could actually find
things at runtime leakage it would and
then we can make it faster at that point
so things like this loop you know we
could've done heuristics and say well
you know what nothing's being changed
outside of each individual element of
this input array so we could break that
up into ten different operations
interpret them in at the same time and
tended from cores in the GPU and speed
things up in general
if the majority of your application
can't be sped up or accelerated in this
way then you're going to be limited by
indels wall eventually if you don't that
is you can look it up on Google but you
know the this can only go so far to be
practical but you know there's a lot of
experimentation to be done and we wanted
to see how far we can take it so we're
going to continue you know experimenting
we want to see if we can get the
performance of this interpreter all and
Cl devices to an acceptable state where
you can actually go ahead and send tasks
to a stateless you know no share memory
tasks but you can send remember you can
send parameter values to it they'll be
serialized over and then you can pull
that memory back and you know do high
level things like map map reduction and
whatnot these are things that River
Trail is doing very well
and you know there's there are
limitations there so we can get this
interpreter going then we can actually
have full compliant JavaScript on all
these you know fast GPU devices
accelerator processors and see if we can
get JavaScript up to the next level you
can even you know think further and set
up like a clustering system and actually
do these things on a bunch of different
systems and orchestrate them which are
pretty neat so so that's there's a those
are the things that we ran into and we
came out with and we're still working on
it these all this stuff's gonna be
open-source we solved like legal things
to take care of like copyrights and
whatnot but it's gonna be open source
just people to play around with and see
what it looks like it's a mess but we're
gonna clean it up it'll be it's fun so
if anybody wants to just play around
with it see if they can make it faster
you can I'll be posting and tweeting
about it and my Twitter's at your
Nichols so if you can follow me I'll
I'll be talking about this quite a bit
and you can stay up to date it's on my
github account I didn't put the link in
there I don't what the hell I was
thinking but it will be live soon it's
probably right now because they've
copyright stuff but it'll be live soon
but that's it that's my talk so
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>