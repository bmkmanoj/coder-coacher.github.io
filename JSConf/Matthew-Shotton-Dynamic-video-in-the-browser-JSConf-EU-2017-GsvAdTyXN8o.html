<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Matthew Shotton:  Dynamic video in the browser | JSConf EU 2017 | Coder Coacher - Coaching Coders</title><meta content="Matthew Shotton:  Dynamic video in the browser | JSConf EU 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Matthew Shotton:  Dynamic video in the browser | JSConf EU 2017</b></h2><h5 class="post__date">2017-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GsvAdTyXN8o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi so I'm Matthew Sean I'm I work for
the BBC and today I'm going to talk to
you about doing dynamic video in the
browser but the for all of that I want
to give you a brief history of
broadcasting in emoji so in the
beginning people transferred knowledge
News knowledge and stories by word of
mouth and this information would change
and adapt over time then the written
word came along and this allowed these
news information and stories to be
locked down to a single representation
and this was almost the first form of
broadcasting except it was very very
slow then radio came along and for the
first time information could be
delivered to millions of people live
from a single central authority that was
shortly followed by TV which was very
much the same except we added pictures
and then Along Came the Internet this
distributed network of communication
that had the potential to let anyone
speak to millions of people and it was
amazing and a bunch of really good
people worked really really hard and we
learned how to fit the radio and the
television over the same pipes of the
cats but the internet isn't broadcasting
it's something different with its own
exciting possibilities and that could be
a bit of an existential crisis for a
company with broadcasting in its name so
hello I'm Matthew chaton I work in
research and development in the UX team
at the British Broadcasting Corporation
I love making things as well this is
some stuff I made I just put it in
because I really love making stuff so I
like physical things so it is like 3d
printers and giant torches and stuff but
I'm going to talk to you today about a
software project I've been building at
work called the video context so the
video context is an open source library
developed by BBC R&amp;amp;D and its aim is to
make it really easy to make interactive
video on the web and can I get a quick
hands up who's use the Web Audio API at
all okay so a couple if that's cool so
like the video context is very similar
to the Web Audio API but the video it
allows you to sequence and play HTML
media elements like videos images and
canvases it's also pretty fast so all
image processing operations are done in
shaders using WebGL and it's built using
modern open web technologies so now dive
right in and show you some code so this
is a full example of a really simple
video context composition so this is a
HTML document the video context is
brought in at the top and then we have a
canvas and that's going to be we're
going to render too so I'm going to zoom
in a bit more on this code so you can
see what's going on
so we first get a reference to the
canvas and then we pass it into a new
video context instance and then we
create some image source nodes so source
nodes in the video context are is
anything that'll output some form of
media and so the video context is all
graph based as well so in order to see
the output of these image nodes need to
connect them to the destination so the
destination is a special case in the
video context and it represents the
final canvas that things are going to
get rendered to we then need to sequence
these source nodes for playback so the
first image will play from time 0 to
time for the second image will play from
time for to time 8 and then we're going
to tell the video context to play so I
talked about how it was a rendering
graph this is a really
simple representation of that graphic so
you have the two image notes one from
time zero it's time for one print time
for two time eight and we have the
destination we have the connections
between the two so this is thought that
looks like some pretty glorious space
caps so this has a bit of extra stuff
thrown in to visualize it a bit easier
we have a time line down the bottom that
shows the images being played back from
time 0.44 to time eight and then there's
that graph view the side that shows you
when things are rendering so this isn't
so interesting at the moment it's just
two two pictures rolling around so let's
make it a bit more interesting so to do
that we're going to add video so this is
a video source node and you create
exactly the same ways you create an
image source node we can pass in a link
to a video file you then connect it into
the rendering wrap and we're going to
set it playing again from time 0 to time
II and I should say all of these image
and video source knows a black back
underneath by HTML media elements so
buried inside the image node is a just a
regular image element buried inside the
video note is a html5 video element so
playing and this is what the new graph
looks like so it's a bit more
complicated we still got our same view
image node we have a video know down the
bottom and that's clicked as the
destination and this is what that looks
like
so this is a bit disappointing we've
lost our space cats and the reason that
happened is because the video context
will render the media in the order that
it gets connected so our two cat
pictures are still being rendered but
they're being rendered behind Shia
LaBeouf so you might have noticed that
child buff was conveniently against a
green screen so I'm going to take a
brief aside and talk about how green
screening is done so this is
traditionally quite an expensive thing
to compute because you need to iterate
out of every pixel inside an image tests
whether it's green this is greater than
a certain threshold and if it isn't make
it transparent
and this is one of the ways that weather
presenters are put in front of their
interactive weather maps so yeah this is
quite expensive in a 1920 by 1080 image
at about two million pixels at 25 frames
per second that's 15 million pixels per
second we need to process but web
fortunately web GL shaders pride a
really convenient way to do this
efficiently so a WebGL shader is a chunk
of code which runs on the GPU and
they're typically written in a c-lite
language called GLSL and they describe
operations which get run in parallel
across many processing units on the GPU
and this makes them perfect for doing
simple operations across many many
pixels so back to the code I'm going to
now create a color threshold node so
this is similar to the source nodes
except it's an effect and you pass into
it a definition and this describes what
that effect should do we then disconnect
the video node from destination and
connect to the color threshold node then
connect the call threshold node to the
destination so quickly I'll give you a
brief look into what one of those
definitions look like and this is super
complicated and don't worry about if you
don't understand any of it it's it's
mostly shader code and so the bit in the
white box is the fragment shader code
that does our green streaming so the
video context has a bunch of effects
built in but it also is quite a nice
environment for playing around and
experimenting with building your own
shaders so back to the code we've
created that graph which looks like this
r2 image nodes we have a video node
which is now connected the effect nodes
then connect to the destination and that
results in this which is much better we
can see our space caps again thank you
yeah so we're going to take you walk
through that route one more time we're
going to introduce transitions because
transitions are really important in
video editing so a transition is how you
might move from one video clip to the
next so rather than just doing a
straight cut you might have like a
crossfade we might have a star light you
might do something a bit more jazzy so
we're going to create a new video note
and this is created using the same
source the first video note but we're
having we have this dispaly at the end
in this 10.5 and that tells the video
context how far in for that source video
to seek before it starts playing back so
this is how you can sort of cut up pre
existing video files but using this
library so you can create a transition
note and this is really similar to get
effect mode except we pass in a
transition definition and then we call
transition on it so this is going to
transition from time three two times six
from the first input the second input of
the transition node we then connect our
to video notice the transition knows a
transition note of the color threshold
node and then we set up our second video
node start playing and this produces a
graph that looks like this is getting a
bit complicated now and might be a bit
hard to read but results in this so
we've got Shia LaBeouf the in green
screen for the two green screen videos
are being transitioned between
but wait there's more so as I said the
video context has a bunch of effects
built in and a bunch of transitions
built in and this is a demonstration I
put together to kind of try and push the
limits so this is a level 11
simultaneous effects and five transition
effects happening in real time on full
frame images and this was rendered live
in the browser and then each one was
scaled to fit into this this grid and
this really shows the power of shaders
that you're able to do this much
processing this quickly in the browser
it's crazy so are the features
everything I've shown you by the video
context so far has been very much set up
a processing graphs at the park playing
times and say running but it allows you
to do everything dynamically as well so
at any point I could have disconnected
or reconnected those nodes any point I
prefer craters new effects
I could have set of new media's replay
back this makes it makes it quite a rich
environment for building interactive
video experiences it also has a limited
mobile support which I'll I'll go into
in a bit and if you're into making sort
of 360 interactive video it integrates
really nicely with a frame as well so
I'm going to take a bit of time now to
go through some of the challenges we
came up across when we built the video
from text and these challenges are
pretty agnostic the video context work
and the likely come off if you're trying
to build interactive video experiences
yourself so most browsers are limited to
about six to eight simultaneous HTTP
connections for domain and if you have
many clips to play that you want to play
back to back a naive way of doing this
to make sure they all play properly is
to create HTML video elements for each
of those clips of front and set them pre
loading but this quickly saturates the
number of available requests to a single
domain and you can make your website
really slow and responsive so the
solution to this is the video context
will load clips just in time so
all HTML media elements they create
about four seconds before they're
actually needs needed which gives it
enough time to kind of preload things
and get it working so that's all it
needs to be a master clock to sync
everything to in the video context and
the natural fit for this would seem to
be the video current time attribute
unfortunately the rightest updates
varies between browsers so if you're
doing things that require frame accurate
cutting between Clips if your current
time attribute is only updating every
bit of a second it can break things so
the solution to this was to use a
requestanimationframe as a master time
source which gave us millisecond level
accuracy and frame accurate updates this
doesn't come without its own problems so
if you switch tabs the crest animation
frame callback gets halted and this
means so traditionally on the vid on the
web if you're playing back video and you
switch the tabs you'd expect the video
to keep playing in the background but
the video context won't both do that if
it only is a requestanimationframe so
the solution we came up for this is - as
soon as you switch tabs spin out a
webworker and inside the web worker run
a set timeout loop and this won't be as
accurate as a requestanimationframe but
it will give you enough timing to keep
the video context ticking over okay I'm
a little bit ashamed of this one so
they're with me so on mobile there's no
water playing video and this is to
prevent videos also playing and using
Foley on mobile data and the way so the
reason for this is all control calls to
a video element must originate from or
the first one must originate from a user
action in order for a video element to
be controlled programmatically on mobile
so the solution to this in video context
is we create a pool of elements with no
source and they're all there
with no source and when you call play
for the first time in the video context
play gets called on all of those video
elements but nothing
because there's no source attribute but
it puts them in this activated state
where we can then control them
programmatically so the video context
then manages this pool of activated
video elements when it needs one little
porn out of the pool it will use it to
play my video and I'll put it back into
the pool so this allows us to play back
video on mobile that doesn't start at
the very beginning foot sequences okay
so finally this is kind of a biggie
there's no low-level API for frame
accurate sync of multiple videos and
this is kind of a problem because we
have quite complex timing requirements
in the video context we have videos
playing on a timeline at any point we
also have offsets within those videos if
you're taking a clip out of them we need
to make sure they all stay in sync the
solution to this one yeah you kind of
you've got to set them playing and then
hope for the best basically you can do
some quite naive things so if a video
starts buffering then pause will do of
your videos and liquid start playing
again and it turns out perceptually this
isn't so bad like people have an
expectation of video on the web that it
might buffer now and again which if in
broadcasting world if that happens
it'd be a massive no-no but you can kind
of get away with it on the web a bit
more okay so this is the big question
like why okay why we do this is all well
and unlike as much as some others might
want to turn broadcasting into cat gifs
and Shia LaBeouf it's probably not the
future unfortunately so this is the
BBC's mission statement and of note it
doesn't actually contain any mention of
broadcasting which may be avert sites or
central crisis so in R&amp;amp;D where I work
our role is to use new technologies in a
way that lets the BBC inform educate and
entertain people so it's faster cheaper
and better for now into the future
and specifically the user experience
team where I work we use user led design
to create novel experiences to drive
technological change
and for this tool the choices generally
the web so I'm going to show you a quick
demo video and this is one of the
prototypes you put together building the
video context and we've built a whole
range of these but I'll let just show
you this one
forecaster and the BBC R&amp;amp;D labs
demonstrates how a new approach
delivering programs would open the door
to more personal flexible experiences
for our audiences in this demonstrator
we transmit each of the media elements
separately and because of this we have
the ability to control each of these
individual elements in isolation from
one another the timeline tracks you see
the bottom of the footage represent the
timelines for each media object in the
forecast
to illustrate this you can see how the
on-screen graphics can be toggled on or
off or the whole backdrop removed to
reveal the raw green screen footage by
delivering content in this way one of
the biggest potentials lies in the
enhancements of accessibility replacing
the standard presenter with a signer the
first-class presence from the video for
example or rearranging the on-screen
graphics when subtitle elements are
present to avoid overlap you could also
change the background map to a higher
contrast view to aid the visually
impaired if media were delivered in this
way we could also have the footage
adapting to suit your screen size rather
than forcing a single aspect ratio or
font size on all devices here we can see
how a mobile portrait view would look
with a larger font size and a rearranged
view to include more of the map in the
future we could also link content to
your personal calendar or or the
third-party data services and feature
relevant local information as you can
see the ability to send media elements
or objects separately rather than as a
single video stream gives much greater
flexibility for playback allowing
content automatically adapt to the
screen size and preferences of the
viewer the flexibility of an object
based approach won't just benefits
audiences though
BBC R&amp;amp;D is also testing how it could
make production more efficient giving
programme makers time to flex their
creative muscles in new ways
we hope you've enjoyed the sneak peek
into the potential of IP production in
the future this demonstrator does not
represent a new service and has used
non-broadcast maps and feeds only I love
that disclaimer again thank you so in
that you heard the phrase object based
media and this is what R&amp;amp;D are calling
this new approach to broadcasting and it
might feel quite familiar if you've been
on the web for a while but we like to
think of it as bringing responsive
design to broadcasting making content
that adapts to the user to the device
and to the environment are in and that
was one of a bunch of experiments we're
building to explore this area more so
finally as well as a new way to deliver
content the web is a new medium for
storytelling in and of itself and we're
really excited to be exploring this area
and we hope that by releasing this layer
we make it a bit easier for the people
to do so as well and super excited to
see what you might make with it thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>