<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Carina C. Zona:  Consequences of an Insightful Algorithm | JSConf EU 2015 | Coder Coacher - Coaching Coders</title><meta content="Carina C. Zona:  Consequences of an Insightful Algorithm | JSConf EU 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Carina C. Zona:  Consequences of an Insightful Algorithm | JSConf EU 2015</b></h2><h5 class="post__date">2015-10-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/znwWYR1mzzw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Wow! Okay, that was great. That's never happened
before. (Laughing) can you do me a favor and
bring me the water. I left that with you.
All right, this is called county queenses
of an insightful algorithm. The talk is for
empathetic coding. We're going to delve into
specific examples and and in that spirit I
want to start with raccoon tent warning, I'm
going to deal with a number of examples that
are sensitive top Inc.s Greek PTSD, fertility,
racial profiling, con receivation camps sexual
history concept and assault. While these are
not the major point of the talk, in about
ten minutes we'll get into examples so you
have a bit of time to decide if it's not right
moment for you. Algorithms impose consequences
on people all the time. We're able to extract
remarkably precise insights about an individual,
but do we have a right to know what they didn't
consent to share? Even when they're willingly
sharing the data that leads us there. How
do we even mitigate against unintended consequences
like these? Let's start by just thinking about
what is an algorithm, defined step‑by‑step
set of operations predictably arriving at
an outcome. Predictivefully is pivotal here.
We're talking ability algorithm of computer
science, patterns of instructions that are
articulate in the code or in formulas. But
you could also think of algorithms as being
something in just ordinary every day life,
patterns of instructions that can be articulate
in the all sorts of ways such as a map or
a recipe or even C ‑‑ you can Lasly define
it as Al governorrisms as fast trainable artificial
gnarl networks. A technology been around for
a while since '80s in theirretcal scale and
confined to academia. In the past few year
this is' been big advance in a variety of
ways that make deep learning for extracting
insights out of Big Data out of construction
deployment. Opening up a lot offed possibilities.
In particular it's an approach to building
and training article official neural networks
you can think of them as decision making black
boxes. What does that mean, essentially we
have some inputs, is this an array might be
representing words concepts, Octobers, any
number of things, execution running a series
of functions repeatedly and layers that get
more and more recease in their analysis, output
our predictions of properties that might be
useful for drawing intuition about future
data set as long as they're similar to the
original training data set. That allows the
us to do some incredible things behavioral
prediction, the facial identification, sentiment
analysis, and things as extreme as self driving
cars which Google is already using this stuff
for a number of other companies are as well.
So there's a lot of practical applications
if that's already intriguing you you can check
out C Convnetjs, it's a great opportunity
to explore and experiment. So deep learning
relies on and ANN's automated discovery. And
it applies those discoveries to intuitions
about future inputs. There's aquavit, every
flaw or assumption in that training data set
or original functions is going to have unrecognized
influence on the Gordon and Maureen and the
outcomes they generate. We're going to take
a closer look at that in a minute, I want
to give you a neat example of what an ANN
is like. This is Mario. It's an ANN that teaches
itself how to play super Mario world. It start
was no clue whatsoever, all it does is manipulate
numbers and notice that sometimes things happen.
Over 24 hour period it learns movement and
play via a purely self training session in
which it engages in those hours of experimentation
each time learn ago little bit more about
the patterns and identifiable them and using
them to make predictions for next layer. And
speaking of games, let's play one. It looks
a little bit like Bingo, it's called data
mining fail. Insightful algorithms are full
of pitfalls by looking at case studies it's
an opportunity to explore some of the pitfalls
on this particular board. So are you ready?
Yeah.
All right, here we go. In the retail sector
the second trimester of pregnancy is known
as the holy grail. The reason is that it is
one of the few times in life where consumers
spending habits product loyalties, brand loyalties
are all kind of thrown up in the air, everything
is subject for an opportunity to change. And
for retailers this is an incredible moment,
an opportunity to capture a consumer for potentially
the rest of their lives and family. Target
managed to come up several years ago with
a predictive algorithm that was good at identifying
customers that were in the second trimester.
They started sending out add seculars to the
targeted people full of stuff related pregnancy,
babies, a funny thing happened one day a man
came into the store and he was really angry.
He's yelling at the manager, how dare you
send this to my teenage daughter are you trying
to tell her to have sex. Manager, you know,
like he's not in charge of this, this is a
huge national change, he apologizes, the guy
goes home he comes back the following day
and says, I you an apology, I talked to my
daughter it turns out there were things I
did not know and she is in fact pregnant.
So, target was right. But they were also wrong.
What they found were that a lot of women were
not okay with having their privacy violate
in the this way. And the way they describe
od it is, some women React badly. Which is
an interesting moral judgment on that. So,
they came up with a change in plan. The new
one was, now adds still go out to the same
people, same adds except they're couched among
other adds that seem completely unrelated
so that their perception is that you know
it's completely random bunch of adds that
they just happen to get that have some things
that are relevant to their life. And the reason
they do this. This is a quote &quot;as long as
a pregnant women thinks she hasn't been spied
on, as long as we don't spook her, it works.&quot;.
(Laughing) so algorithms aren't just about
the outputs, it's about how we use them and
how we abuse them. This is one of the examples
of ways that we can have all the math right
and still be wrong. Shutterfully likewise
was trying to predict things related pregnancy,
in this case what they were predicting was
recent childbirth. Congratulations on your
new bundle of joy, time to write thank you
cards to all the people that came to your
lovely party. Some of the people who got these
said, well, I haven't really been pregnant
seeing as how I'm male. (Laughing) and others
had different responses. Thanks, shutterfly
for the congratulations on my new bundle of
joy, I'm horribly infertile, but hey, I'm
aadopting a kitten, so ... I lost a baby in
November, who would have been due this week,
it was like hitting a wall all over again.
Shutterfly's response was the intent of the
e‑mail was to target customers who recently
had a baby. Well, yes, that's true. That's
not an apology. That is a statement that that
was what they wanted to do. They failed at
it. False positives can be very meaningful.
Few months ago mark Zuckerberg excitedly announced
he's going to be a father soon. He wrote on
Facebook about a series of miscarriages that
he and his wife had felt with as a couple.
This is part of what he had to say. He said,
you feel so hopeful when you learn you're
going to have a child. You start imagining
who they'll become, and dreaming of hopes
for their future. You start making plans.
And then they're gone. It's a lonely experience.
Facebook in review has ‑‑ Facebook year
in review has been around for a while. This
past year what they did was much more automated
putting the post from the past year that they
felt were particularly big and important and
memorable for you and throwing them back to
you at the end of the year to enjoy all over
again. What they failed to take into account
is our lives oconstantly changing in the course
of a year many of us have job changes, relationship
change, our life circumstances. All sorts
of things and some of those mean that not
every memory stays the joyous one that it
once was.Er reck Meyer coined the term inadvertent
algorithmic cruelty. The result of code that
works in the overwhelming majority of cases
but doesn't take other use cases into account.
So why does he get to name it? Well because
he's one of the people it happened to. This
is a picture ofmy daughter who is dead. Who
died this year. The year in review add keeps
coming up in my feed, rotating through different
fun and fabulous backgrounds as if celebrating
her death. And there's no obvious way to stop
it. Eric calls on us to increase awareness
of and consideration of the failure modes,
the educations, the worst case scenarios,
and I hope that we can do some of that today
and that you're going to carry it forward
to others W that in mind hear's my first recommendation
for all of us to think about. Be humble. We
cannot actually Intuit inner state, emotions,
private subjectivity. Not yet. Any way. (Scenario)
when fitbit started out, it had a sex tracker.
You know, quantified self, let's quantified
everything, it counts as exercise. Right.
There was a wrinkle, the wrinkle was that
it defaulted to public. (Laughing) you all
want a fitbit now don't you?! All right, so
first of all I appreciate the vigorous effort.
(Laughing) secondly, I also am a certified
sex educator and I'm look oing at the four
hours and I just tonight know whether to congratulate
or be concerned. (Laughing) fitbit users were
unwitting libraries sharing details of their
sex lives with the whole world, it was on
Google. That's because it was set public by
default. And this is one of the things that
we have to be thinking about an algorithm
is not just about crunching numbers our patterns,
prepredictable reproducible actions, this
was really unthinking decision to not evaluate
how different data sets might be differently
treated, differently considered. Different
amounts of privacy in our lives just because
you want to share with all your friends your
competition over how many steps you've taken
or how many runs you've done doesn't mean
that everything in your life is meant to be
a public DOM petition as well. This was ‑‑
public competition as this was a algorithm
for UX it was really a fail here. Most of
us use internal opt tools, it's mandatory
right. Performance tuning, business metrics,
a lot of something, Uber is called God view.
If you're a gamer you're already suspecting
what this implies. Uber did not limit access
to admins and not restrict it to operational
use alone, workers could freely identify had
any passenger and monitor the person's movements
even drivers were welcome to Bruce through
Ubers customer trip records. Meanwhile managers
felt free to abuse God view for non‑operational
purposes such as stocking celebrity ride in
real‑time showing it as party entertainment.
To is show you how horrifying God view is,
here's code expert. This is so nobly inappropriate.
mean, seriously. Auto play true? Okay. And
then of course there's the other choice. Background
image, that's pretty telling as well as to
what their intent was for this. The research
group at dating site okay cupid used to Blog
all the time things they were learning about
aggregating their data and Blog showing insights
into simple ways that okay cupid users could
use that data site well to date better. Uber
used to Blog about its day to too. There's
a critical difference in that Uber's approach
to it was not about improving customers' experience
of a ride service, it was about invading people's
privacy for the sake of judging and shaming
and stalking them. These are not predictable
consequences of signing up for an account
to take a ride. Galling add words is an interesting
study done at Harvard a few years ago, what
they did was took two sets of names, one that
is strongly correlated with black people and
one that's correlated with white people. So
for instance first name like Latonya would
be something that's highly correlated lated
with black women and something like Jill would
be highly correlated with a white woman. And
then what they did was they matched up the
first names a with the real last names of
professors and did some searches on add words
for those names. And what they found is that
a black identifying name was 25 percent more
likely to result in the an add that implied
that that person had an arrest record. So
for example adds like these. And I think it's
important to note here, Ad word algorithm
is focused on predicting what we'll click
on. That's it. It's not interested in whether
anyone was arrested. That's not it's point,
the real world isn't important. It's whole
job is to figure out what motivates us to
click. Which Ad template we're going to respond
to. Based on what it knows about us individually
and collectively what it knows about other
users before us, what we're see inning these
Ads is our collective bias at work and being
reflected back to us and then being reinforced
every time it's presented again and we click
on it again. This is a feedback loop. Data
is generated by people. It's not objective.
It's constrained by our tunnel vision it replicates
our flaws, it echos our preconceptions. Image
recognition is hard. And you remember it wasn't
so long ago we had things like this. (Laughing)
I photo helpfully helpfully detecting faces
in baked goods. It's funny, sure. It's a harmless
mistake, it's a false positive that's easy
to chuckle at. Some are less funny, such as
in this next photo. Flicker classified it
as essentially children's playground equipment.
For those who are not familiar that's ash
wits. This was in May ‑‑ ‑‑ a month
later Google photos mistagged this person
as an animal. Sorry that was actually flicker
again, apologize, it also tagged him as an
ape. Google photos a month after that tagged
someone as a gorilla. You'll notice a common
theme here, black skin. How does that happen?
Well, maybe some of it is just like those
Ad words our bias being reflected back, there's
also other answers for one of those you're
going to have go all the way back to the 1950s
when color film stock was first being developed,
created it was optimized for white skin to
get as much detame out of white skin as possible.
And for decades labs were given these, they
were scall called Shirly cards they were used
to calibrate developers to make sure they
were accurately producing details and colors,
for decades every film stock was designed
to optimize for white skin and to ignore black
skin. And black skin, to this day still has
a very hard time getting a nice accurate well
exposed photo. When we started moving to digital
sensors the obvious thing to do is to replicate
the experience people were already having.
If we have sensors that are radically different
we would all complain about how terrible and
faulty our cameras were. What we have here
is repeatuation is racial an mouse from thed
50s, we have decades of the data sets that
are contaminated with noise. And so when we
have these kind of misclassifications it's
easy to look at it and say, Hmm mistake or
Hmm racism. It goes deeper than that, these
are hard problems to solve, Big Data if we
throw enough data at it any problem can be
corrected is what we think. Where are you
going to find the data that is an easy corrector
for this? A firm is a rather unusual credit
lending company. They focus on lending for
certain small consumer purchases and make
a rather interesting set of criteria, the
basis for lending. Essentially just provide
a few thing, name, e‑mail, mobile phone
number, birthday and last four digit of identification
number and then from there it starts evaluating
behavioral factors even were you given that
much. Things like how long you take to fill
in that little form. What time ‑‑ how
much time it takes you to remember stuff.
And then if it needs more information it goes
out and looks at your social accounts including
GitHub. Which is already starting to replicate
privilege in the real world because only 2 percent
of women ‑‑ sorry only 2 percent of
Open Source cent toes are women. So that means
GitHub is inevitably going to be biased towards
finding men, and if you're making the criteria
for lending participation on a site like GitHub
automatically it's always going to be biased
against a lot of women. Think about how much
time it takes to remember something. Who might
need more time? Someone with, say a cognitive
processing disorder. Someone who's older.
All sorts of biases built into the supposedly
objective algorithm. UK researchers my shopping
cart abandonment rate is high because my toddler
grabs my visa card and runs off screaming,
mine, mine, mine. All right, sometimes it's
not that we're inattentive because we somehow
are a bad credit risk, sometimes we're inattentive
because other things distract us. This is
a person who's clock is invisibly ticking
and losing opportunities that are financial
as a result of it. A firm analyzes those social
media accounts, they're not the only one,
there are a number of other companies using
similar models. In 2012, in fact Germany's
biggest credit rating agency considered evaluating
Facebook relationships. More recently Facebook
pushes further down that line making credit
decisions about you based on the unrelated
credit history of your Facebook friends. Okay.
What? So are they unaware that friends doesn't
equal Facebook friend? Like we got to tell
Facebook about this. Here's an algorithm with
potential to deeply intrude on and alter personal
relationships. To just to prevent that algorithm
from financially shaming and pun ishing them.
This is a huge consequence. Data is not objective.
It always has bias, it's inherent at minimum
from how it was collected and interpreted.
A firm says that it's algorithm uses 70,000
factors to reach it's conclusions. They say
they don't even know what all of them are.
All right, so how do we know how many have
potential for discriminatory outcomes then.
How would anyone of them now. If thaw don't
know how would the consumer do anything about
mistakes? Forget about bias, just simple error.
Rationals for the algorithm can only be seen
from inside that black box. So let's take
a look at it. I took a photo from inside a
really, really black box. (Laughing) that's
what we can see. Making lending desessions
inside a black box isn't a radical new business
model, it's a regression. What is disrupting
is oversight and regulation. Right now we're
in an arms race. Facebook, Google, happening,
Microsoft, Yahoo. Baidu, IBM, AT&amp;amp;T Twitter,
so many companies are making big bets on deep
learning, some are already deploying, for
the moment, quality varies but we have to
remember that deep learning is all about iteratively
drawing intuitions at extremely fine grained
levels. And what that means is that they're
continuously getting more precise in their
correctness but also more damaging in their
wrongness. And that's a dilemma that we have
to take seriously as developers. Because underlying
actions, influence, outcomes and influence
consequences. They have underlying assumptions
about meaning, about accuracy, about the world
in which data has been generated, about how
code should assign meaning to to data. We
care about getting this stuff right (To Data)
the question is how do we flip the paradigm?
Well, we can do a few things like taking some
lessons from professional ethicists because
it turns out that's a thing. It turns out
our profession has professional ethicists,
who knew. These are a few that I've adapted
from the association from computer machinery
and a few other sources. We need to consider
decisions impact, potential impacts on others.
For instance how might a false positive affect
someone, like those shutterfly customers,
how might a false negative affect someone,
have we built in resource for someone to easily
get our conclusions corrected when we're wrong
about them. We need to be able to project
the likelihood of consequences to others and
to minimize negative consequences to others,
and yes, I keep hammering on two others, we're
pretty good at taking care of ourselves. We
have to be honest and trustworthy. Not just
because those are the right things to do,
but because we need to be able to lean on
it when we make mistakes, we need to be able
to buyback trust because we've earn it, because
we can say maya cull pa without that destroying
us. We need to provide others with the full
disclosure of limitations and call attention
to signs of risk of harm to them. And here's
a big one, we have to be visionary about counterrerring
bias, we have to be visionaries about creating
more than one way to counteract it. To counteract
bias data, bias analysis, bias impacts. And
here's the really big one. We have to be able
to anticipate diverse ways to screw up. When
teams are charged with defining data collections
use and anal circumstance any time those are
less diverse than the intended user base,
we're going to keep on failing them, just
like this. We have to have decision making
authority in the hands of highly diverse teams,
highly. What does that mean? Culture fit is
the antithesis of diversity, it's superficial
variations being allowed to exist as long
as their unique perspective is sure pressed
the purpose of culture fit is to avoid disruption
of group think. UniI did mixal variety is
not diversity either. Diversity is widely
varied on as many areas as possible. Different
assumptions, different experiences, until
you get to the point where there's no such
thing as a majority you can't find it. We
need to cultivate, inform consent. What that
means is we ask for permission with the default
being no and explain the consequences of a
yes. Focus on the many people that eerily
want to share themselves and enthusiastically
give consent and want to be served better
by that. And we have the audit outcomes constantly,
the reason is going back to the black box,
if we can't be sure what's happening inside
of it. We need to be able to look at the outcomes.
This is used a lot in checking for housing
discrimination and job discrimination. So
you put in two inputs that are exactly the
same on every criteria but one. One that should
not have any bias introduced and if the outcome
on those divers at all, we know we have an
problem with the algorithm. So this is something
we constantly have to be looking for. For
instance when Google photos made that mistake
with classifying people as gorillas, as soon
as they saw that problem on flicker a month
earlier they should of been dog auditing,
do we have a the same problem? And why? Because
photo of a big black box. And that's why we
also have to commit to data transparency and
algorithmic transparency. And I do mean both
because I recognize that these are hard decisions
to have internally. They truly are. But I
also think that, you know, it wasn't that
long ago that we were fighting for legitimacy
of Open Source in our professional Toolkit.
We push back, we were right, we're profession
falls, we won because we know what we're doing
and we made a good argument. We know that
transparency is crucial for drawing insights
that are general when and useful. So argue
for increasing transparency because it's for
a better product. Cleaner features, fewer
bugs stronger tests happier users public trust.
That's the argument. Because we want to build
stuff that matters. We're hired for more than
just to write code, we're hired as professionals
that apply expertise and judgment about how
to solve problems. That's who we really are.
We're not code monkeys, we're people that
think about how to solve problems. Our role
is to be opinionuated about how to make code
serve the problemtion based well. We can advocate.
When we're asked to write code that presumed
to Intuit people as internal life and act
on those assumptions as professionals we have
to be people's proxies, we have to be their
advocates, say no on their behalf to using
their data in ways that they have not enthusiastically
and knowingly consented to. Saying no to uncritically
reproducing systems that were based to begin
with. Say no to writing code that imposes
on authorized consequences on to their lives.
In short, refuse to play along. Thank you.
(Applause)</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>