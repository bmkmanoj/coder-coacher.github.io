<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Michael Mifsud: Real World HTTP/2 - CSSConf.Asia 2016 | Coder Coacher - Coaching Coders</title><meta content="Michael Mifsud: Real World HTTP/2 - CSSConf.Asia 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Michael Mifsud: Real World HTTP/2 - CSSConf.Asia 2016</b></h2><h5 class="post__date">2016-12-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3WIDa5-bPDs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you
so in 1989 Tim berners-lee first
proposed the world wide web project and
with that he'd his team at CERN invented
HTTP and HTML the first implementation
had one HP method and it was gap and
every response returned a HTML page
years before the creation of JPEGs or
Windows 3.1 with just HTTP and HTML the
first-ever web page was deployed and was
responsive as the first version of HTTP
was documented in 1991 multiple versions
and revisions over the next eight years
in a period of rapid evolution until
until HTTP 1.1 was finalized in 1999 and
what internet was born they should be
1.1 is what most of us would know is HP
one what most websites run on at the
moment so I may use HP 11.1 it to change
ibly but they're the same thing for the
extent of this talk in the preceding 20
years generations of developers use
these simple building blocks to create
engaging experiences of communities for
journalists musicians artists designers
crafters gamers and everything in
between we went from 386 s the Pentiums
to watches it more computing power than
the Space Shuttle has said people to the
moon we added JavaScript CSS images and
most importantly gifts to our toolbox
and with them we built new ways to
connect to communicate share in our
lives and engage with the world around
us the hypertext Transfer Protocol of
the revolutionary for its time is
crumbling under the weight of our modern
ambitions the design of the protocols at
odds with what we as developers are
building and with what our customers
expect in 1999 google announced they're
working internally on a modern revision
of HTTP they named a speedy speedy
primarily focus on reducing latency
which as a result had larger performance
improvements of HTTP one significant
performance improvements to rapid
adoption of the new protocol and
browsers and large sites like Facebook
and Google in 2012 looking to revitalize
HTTP to
nuvo HP with new version working group
took speedy and made it the initial spec
for HTTP to nature to spec was finalized
last year and as of now all major
browsers and web server support in the
HTTP 20 Spidey speedy for better or for
worse the story of HTTP two has become
tired to notions of free performance and
help make everything that web
performance sprung unfortunately the
promise of free performance comes with
some fine print and that's what I'm here
to talk about today so uh hi I'm Michael
I'm from Australia and I'm a front-end
performance engineer at 99designs and as
was said earlier and one of the
organizers of CSS company has confessed
rally on and i work on suburban source
projects you might be familiar with in
nerd Sasson lips eyes i want to start
off by saying what this talk won't be
about this is not insured HTTP to a lot
of great material out there a lot of
great talks and blog posts that could do
much better than I could in this time
what I will be talking about is some of
the differences when HTTP one HTTP two
and what this can mean for performance i
will quickly cover some fundamentals
necessary to illustrate these
differences so if networking protocols
in HTTP aren't your thing don't worry i
got you covered so let's jump right in
all in all HTTP is fairly simple you
connect to a server and then with a
small set of commands you instruct the
server perform some action and it
responds these actions issued as HTTP
methods in requests and metadata the
most common method being the get method
we instruct HTTP server to respond with
the contents of a file at a certain path
this simple interaction is largely
unchanged in the 25 years since tim
berners-lee created HTML HTTP they're a
bunch of other methods that I won't
cover today but for the most part of
boils down to a client in our case a
browser connects to a server and
requests something with metadata and a
method and the server responds with
content something like CSS HTML
Javascript underlying all this is our
tcp/ip this is the primary networking
protocol for all HP communication and
for the Internet in general it's with
this it's with these two protocols TCP
and IP that the two remote servers are
able to communicate and exchange
information efficiently being that it is
the primary protocol underpinning most
to the internet it's integral part of
what network performance is like when
using it so we'll cover that a bit one
of the important things of tcp/ip is the
connection and that involves a three-way
handshake essentially a client say your
browser will set will check to a server
and say hey Here I am with a syn packet
and a browser will say Here I am I see
you with a syn ACK packet and their clan
will be like I see you at the next are
communicating the details this aren't
particularly important the important
thing to take away from this is it takes
a full networking round trip from you to
the server on server back to you before
you can exchange any information so we
look at the anatomy of a request for a
page say the index HTML page first
client say the browser connects to a
server I do a handshake at one round
trip and then the browser can say hey
give me index dot HTML that goes to
server server does some work generates
the page rather be WordPress or a static
file the returns response in this case
HTML and that connection is closed if in
that response HTML there was a link to a
CSS file that process starts again you
find the server you handshake you then
request the server for a thing the
server does some work and returns it to
you and that connection is closed so
this is for round trips in total to ask
for a HTML file and a CSS file and this
this is important because one of the
fundamental things of tcp and white is
so reliable is it is a FIFO queue so
first in first out so the first thing a
server sends to you must be the first
thing you receive and your receipt and
you read them in order
if for some reason along the way due to
congestion or happenstance one of the
one of the Packers of data has dropped
on the floor or sequential packets are
just held by this by the client and
buffered and not read until we're able
to tell the server that hey we missed
something please resend it and then it
receives that and then reads the rescue
services that are it's a bit more
complicated than that but that's really
what we need for this talk and this will
happens at a layer well below anything
we see this is a networking layer in
often in your OS or in your browser and
so we have no control over it in our
code and the end result is that things
feel slow another impact of TCP another
feature tease be its position control
this is necessary because over the
Internet there's many different
bandwidths and between you and the
server community came to is intermediate
Abraxas many different computers and
servers always carry different bandwidth
and amplify that across the entire world
you can get into a case where people are
communicating at different speeds and
packets start backing up and you start
roping packets and soon you start
dropping packets the service has to
retransmit those packets that causes
more congestion causing you to drop more
packets causing you more weeks more
retransmits and more congestion and it
can go on on to the points can cripple
entire networks and has in the past so
as a result congestion controls start
with sending a really small window
saying I'm going to send you a little
bit and then the longer the connection
lives on the mortal sin kind of a test
to see whether capabilities that
bandwidth are and that's interesting
because it end up in this property of
TCP where most internet connect
communication is latency bound an
experiment done at Google a couple years
ago probably while ago now showed that
with increases in bandwidth start show
starring diminishing returns once about
5 megabits per second but increased but
decreases in latency have a linear
increase over time so the more you can
shrink down latency the faster things
get and this is due to the fact
everything requires your back and forth
round trip and the handshake must happen
so one of the things brought into HP 1.1
was you I'd ever keep alive in that you
can connect to a server
and keep that connection open for
multiple requests so still got a request
in what I sort of say please give me the
HTML file now please give me the CSS
file but you save yourself that extra
round trip in every connection each time
a new file until eventually one of this
one of the other client of the service
says ok I'm done close connection an
HTTP 1.1 was tried to bring in this idea
of pipelining extending the keep alive
keeping a connection over but saying hey
I know I need these three files give me
these three files at once and this runs
into the heddle ahead of line problem in
that the server must respond with one of
the files generally the first of all you
send but ice pack that receives an order
so you can't intermix sending those
files they must be sent in order so you
kind of lose a lot of the benefits of
use a lot of benefits of what you expect
miss because you can't multiplex so this
actually did have a lot of benefits in a
lot of cases but it proved really hard
to do well and there are classic cases
in Safari where you would mix up the
filename you requested with the actual
body causing image to show in the wrong
areas and intermediary proxies in the
world of which there are many would
understand these properly and mix up
response and garble responses so
although it actually has a lot of
benefits it's hard to do well and
there's a lot of bad actors in the world
up there that get in the way of it so
most browsers and servers will support
this where divorce turned off and
considered an advanced feature and
keeping these with these things in mind
you can see where when our common best
performance factors come from the exact
concatenations friday in lining and
domain shouting are all ways to work
around opening more connections and
doing more handshakes and avoiding
latency this is critical path is a way
of working around condition control in
the idea being you put C inline CSS into
the into your page but only put enough
in so you give fit within that first
window while toothpaste or testing your
connection so this is where HD video
comes in h2b to like
keeper lives and pipelining go forward
which is under reducing latency ways you
long live connections but something it
does differently is this idea of streams
and that you have one connection to a
server within that connection you have
multiple streams so this is like an
analogy of a kind of lanes on a highway
you have a highway to serve multiple
lanes a communication independent from
each other but they're bi-directional so
I can ask I can ask for three files at
the same time on one stream and get
those responses back on that stream
completely separate from another stream
which is regressing JavaScript and what
it has over pipelining that their of
multiplexing so as a request three CSS
files the server can respond to those
files in any order and in bits of pieces
so I can get the headers for one file
get the headers for another file and
it's part of its content and then after
that the next pack could be the content
of different file and this is baked into
the protocol so the servers and clients
understand how to reassemble these
things and each one of these packets
here is what's called the frame we're
not going to dig into frames too much
for this so as you can see with
multiplexing and a long live connections
and streams we're reducing a lot of
problems with bandwidth and working
around under the issues gesture control
being lots of tiny requests I mean one
on the request we at logic just from
troy windows we go flow more data more
freely and this is why people are
telling us that I really when I
performance is wrong you know I'm going
to worry about latency anyway about
congestion windows or bad with is free
and latency is not a problem and I took
this the research on internet is all in
this behavior it was very little work
bad we said about HTTP so with this
confidence we started moving towards htp
to at 99designs and before switch for
flicking the switch on this I
confidently said to my see to my boss
that it couldn't possibly any slower
we're fine to my chagrin we rapidly
started seeing decreases are we we soon
saw decreases in sight performance so
this next
save 15 minutes of my talk is me
systematically eating my own hat so I
work for 99 designs where a crowdsourced
graphics design marketplace which is
purely to say that images are very
important we have lots and lots of
images and to measure our site
performance we pick some key metrics we
focus more on perceived performance from
the user so we use a couple of ways of
doing this these are dashboards the
float around our office so we look at
content Dom content loaded to determine
if synchronous groups are delaying our
page loads we look at the first paint to
see if delay if rendering is being
delayed by fonts or CSS look at time to
visually complete to look at non blonde
non render blocking resources like
images and asynchronous scripts and one
of our main proxies at a glance is speed
index which gives us a good idea of
visual completion over time our sites
being drawn faster are they finishing
faster and visual pollution is mostly
concerned with what's above the viewport
so the initial viewed as the site
thought look like it loaded fast and to
get a lot of this data we use an app
quad caliber subsidies calabar and we
fetch the data route to store it locally
so before I get to some of the problems
in HD too we did see the great
improvements pages like our designer
portfolios which a typical
representative typical page on our site
you know they're mostly latency bound
lots of really small files need to be
pulled down lots of really small images
and for these pages we saw like a five
percent improvement on speed index so
pages look like that were drawing faster
the time to first paint was comparable
so the first paint would happen around
the same time and HP one versus age to
be too but the page would finish drawing
faster interesting interestingly was the
initial render the first drawer on the
page was macht HDD to which you'd expect
as you'll be getting more data at the
same time one of the bad things and the
thing that really stuck out to us was
our designer galleries these are
extremely me have each pages so between
80 pages on average and a page can be
delay between 500 to 5 to 10 megs of
images and these pages are Bam Bam
whisperer
not latency bound so because there
weren't latency bound we didn't expect
the reduction latency would have much
impact on the performance these pages we
actually saw a five to ten percent
faster time the visual completion speed
index us are slower so these pages would
finish drawing much slower and HP to the
HP one but we did see faster page load
times which suggests that the reduced
latency was having an effect which was
very concentrated to us we also did some
high latency testing because mobile is
important to us and to everybody for
this we use web page tests and compared
to HP 1hp to continue to have more
complete first paints so more data was
getting there sooner and we'll drawing
more complete the very first in the user
saw but they'd happen noticeably later
so the first pen would happen much
before much later than the HP one paints
but they'd be way more complete and we
were continually seeing faster page
loads so all the data was getting to the
user far faster eventually but we were
seeing much slower time to draw the page
completely so to kind of sum that up for
a typical page full of images on our
site ones were latency bound lots of
small farms a lot of small images we
were seeing a five percent faster time
to visually complete the entire page but
for extremely me hedge pages pages that
were latency bound painters it did a lot
of work and transfer a lot of data who
actually saw much slower visual
completion some between five to ten
percent slower than previously and on
high latency connections with low speed
say mobile networks we saw greater
delays in reaching visual completion but
in all tests the initial paints were
more complete even though that happened
much later so to summarize bandwidth
bound pages sniffing it longer to reach
visual completion despite loading way
faster and we
couldn't figure out why initially this
was unexpected to us so our first
hypothesis was network saturation it
could it be that requesting so many
things at once and having a single
connection that could multiplex was that
draining the training resources away
from other things in the page like CSS
and JavaScript that would actually block
the rendering of the page in your
typical hp1 water flow you see things
are staged so things that happen earlier
in the page tend to be loaded earlier
and you get a good distribution of
bandwidth across your requests so
looking at these Network waterfalls we
couldn't actually see that happening z
says with loading when sister should
load just blowing JavaScript reload
image loading image should load and they
think the loading and finishing at the
same time so our next hypothesis with
loading priority in h2 be one you have
this limit of 60 / connections per host
and this creates that first in first out
q are talking about where the first
thing in your page tends to be the first
thing requested and the first thing
responded to then things happen in order
as a result the relation of things in
your document document tend to be the
relation of things will load so you have
some control over the loading order of
things with HTTP to it's a it allows
multiple requests and responses over the
one connection happened at the exact
same time as a result you don't really
get that priority anymore everything
sees you at Oxford at the same time I
said just give me what you got no figure
it out and you kind of lose control of
like what should we loaded first what
should be loaded last what things are
more important than others so the
browsers do you have built-in
prioritization for this but you lose
control any document in your document so
you could very well be that images the
bottom of the page or given the same
priorities images on top of the page
causing those longer-term visual
completion so part of this we'd consider
the putting the best practice of putting
scripts part of our pages was closing
those scripts to get like a higher
priority and actually pushing off image
loading resources but we were able to
figure out this was in the case because
don't complete loaded at the same time
and don't be blocked on these JavaScript
files so with that being in the same
place and ask monitoring that we could
tell that the priority of this group's
weren't being moved and weren't changing
the render properties of the page so I
later came down something happening at
the network layer things just weren't
coming to us the way we expected to as
faster picking to come just release the
idea of resource priority in practice
the browsers download queue is
prioritized so starting a team adding
image requests before final script
bottom of the page doesn't delay script
loading at all these a coding behavioral
resource is undocumented however it
constantly changes our pilot problem is
that browsers have their own heuristics
in how they load things but typically
images have very low priority things
like fonts and java better higher
priority and one of the interesting
about is browser heuristics it developed
by browser vendors over time to suit the
current trends so an interesting one now
is the hero image heuristic so images
get a very low priority browsers will
find the first image in the page and
make it a very high priority the idea
being a lot of sites have is big hero
image and that should load first and
given the same priority assists in
JavaScript so knowing these heuristics
are in play we want to look at the
stream without that limit of six
connections person we get HTTP one we
could see ad image requests all firing
ones to the server and the server would
then respond to them simultaneously
because they wouldn't know which was
more important than the other and the
browser would draw them as they came in
this had an interesting effect and we
see something like this comparing HTTP
one and HTTP two we'd see that the HTTP
one page was drawing this image on the
fire image much slower which meant that
it was getting the packets much slower
but we knew that these pages were
finishing to load generally much faster
which led us into the area that let us
down the track of thinking that there
was some sort of bandwidth contention in
that the browser was treating all images
equally so there were images much
further down the page that were being
rendered and parse problem comes down to
is if all images are the same size and
the bandwidth distributed equally they
should all load equally and this page
would have been faster but in this case
this image on the right is slightly
larger than a 200
left and as a result it has a different
requires more data and is then drawn
differently as different loading
characteristics and because the
bandwidth is being distributed to pages
images off the page we see this kind of
phased loading much slower than an HTTP
one which had a baked in priority saying
this image is more important than images
down the bottom of the page um so there
is some fine print with htp really not
many people talking about an allegra
goreck of the chrome team said this
really well I with HTTP to the browser
relies on the server I to deliver the
responses in optimal way it's not just a
number of bytes or requests per second
by the order in which the bites are
delivered near test rotation carefully
the these loading heuristics that exists
are undocumented and they're
undocumented on purpose the idea is that
browser vendors can choose what
heuristics work better for their
customers in their environments they can
analyze that I'm they know better and
they differ between versions a change in
a heuristic in a version can actually
improve certain sites but significant
affect other sites and they're not
documented and they're not told about
these things and have a general for all
sites so what is good for a big hero
image may not be good for a site that is
an app were single page app and these
are concerns gotta be balanced so HTTP
to took this shift and says as a
developer on your site you know it's
best for your site I change the
landscape of resource prioritization the
responsibilities now shared between the
browser and the server the browser gives
the server hints unlike this stream so
it knows the stream of serving image
user says this stream has a priority
much lower than the priority of the
stream serving CSS and that is low on
the stream serving JavaScript as an
option the browser has whether it works
that way is different and most streams
tend to be the same for the time being
it's a change between browsers but this
is the server can ignore that the server
can just say actually I know that on
this page we have lots of images so
these get a higher priority than the
JavaScript that affects a button at the
bottom of the page and you can build
that control into your own server
in practice is much harder because he
sent you to see the anodyne are in these
servers yourself but if you run her own
HTTP servers you can put in your own
heuristics and it's option you now have
and she didn't have with HTTP one but
this ends up being a double-edged sword
resource prioritization existing in both
the client and the server I can really
muddy the waters and open up way more
problems we weren't aware of and put but
it does give the ability put the
developer in charge and the developer
who knows their site can do really great
things I previously hinted at the idea
of weights and weight as a hint the
browser gives at priority this happens
in HTTP one where the browser uses the
idea of priority to then determine what
order to send requests in in HTTP two we
send all requests the same time but we
apply a weight signifying its priority
something with a lower weight has less
priority and should get less bandwidth
there's something of a higher weight so
images ver CSS for a sponsor JavaScript
the browser can assign these different
streams and different weights and saying
these are more important than others the
servers fries with more all that if you
don't control eserver it's on you to
test your service actually doing and
dependencies so dependencies are
interesting and one of the big things if
HTTP two streams in that we can say that
although something is important it's
only important if its parent has already
loaded this allows you to say that here
all the JavaScript files on my site but
don't wasn't loading anything else
you've loaded jQuery or here are the
images on my site but these three at the
top of the page are way more important
to 32 bottom of the page and you can
describe this with dependencies so some
of the takeaways my investigation was
simply that there's no such thing as
free performance and this is something
our browser vendors have known for a
very long time web performance is a
series of trade-offs and new ones any
heavy image-heavy pages tend to prefer
HTTP two connections only when the bat
total bandwidth is less than the latency
incurred if your server lab which
bandwidth if your bandwidth is less than
the latency would have incurred then you
gain a lot running single long live
connection if you're very bound with
heavy reducing latency
actually save you that much so it's the
right mix of high latency and low
bandwidth you can see really big against
and these are the site lots of small CSS
files a lot of small images another
thing to take away from this is HTTP two
is very new and the surface area for the
protocol is huge you have resource
weight prioritization you have resource
dependency prioritization you have
multiplicity heuristics and you have
streamer connection flows these things
although the implementation is
documented the heuristics art it's
really free to you as developers for
sites people who building servers CDN
owners as on all of you to figure out
what is best for your situation of your
audience I hope and there is work in
browsers coming through to hint at these
so for you in markup or in JavaScript be
able to say these resources are more
important than others and have some
control over that or that appears to be
a while off just yet and there are other
glaring issues with hd2 that make this
really hard one is a lack of visibility
HTTP to unlock a should be one is a
binary protocol it's no longer clear
text over the wire you can't look at
what's happening requires specialized
tools and because it all happens over a
ssl it requires either man in building
your connection or doing it manually via
some sort of CLI that gives you access
to emulate a browser and dev tools
simply haven't caught up here they have
the information but it's very hard to
show to show in the dev console our
minds are programmed our experiences
print program to waterfalls of HTTP one
let us really apply history between
Timor and as a while all the tools still
catching up for a long time tools
reporting collection times being wrong
because of the way then attempted SSL
and these issues are bound and as I
mentioned the binary protocol makes it
very hard to sniff what's happening you
can only have a good intuition of what
what's been what's being sent you can no
longer you can't easily inspect the
heuristic supplier you can't see the
server's prioritizing something over
something else what order their packets
are coming in and that's multiplexing
payback we really important and have
really subtle effects so there's some
great resources in this area
HTTP to 101 is a quick video by soma
hopeful most high-performance browser
networking is must read for anyone who
work in sac performance or you care a
sub performance earlier gorica the
chrome team covers loaders material and
way more detail and is responsible for
much material in the beginning of this
talk has to be to hear is less optimized
is a one hour long talk by Alico gorrik
it's definitely worth watching but is
very in-depth HD 1.5 HP to an 1.5 world
by peter wilson this is a great talk or
not chopping on HP to train just yet and
some of the trade-offs and keeping in
mind that many of our users are still on
HP one it will be for visible future and
this is a blog post i wrote that goes
into many of the things we ran into
secure the hd2 in more detail and some
more waterfalls at some of the quizzes
we came to and thank you that is the
wrong way
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>