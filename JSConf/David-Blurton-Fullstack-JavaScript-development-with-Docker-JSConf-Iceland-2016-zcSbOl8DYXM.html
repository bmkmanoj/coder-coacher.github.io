<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>David Blurton: Full-stack JavaScript development with Docker - JSConf Iceland 2016 | Coder Coacher - Coaching Coders</title><meta content="David Blurton: Full-stack JavaScript development with Docker - JSConf Iceland 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>David Blurton: Full-stack JavaScript development with Docker - JSConf Iceland 2016</b></h2><h5 class="post__date">2016-09-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zcSbOl8DYXM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you very much yeah my name is
David Flynn I am from the United Kingdom
and I moved to Iceland one and a half
years ago to live here and as a typical
developer the first thing I did was
thought right I'm gonna have to try to
learn Icelandic how am I gonna do this
should I a spend lots of time studying
with books in a normal way that you
learn things well should I instead build
a website spend hundreds of hours on
that avoiding the problem and try and
build a website to help people learn
Icelandic so obviously I chose option B
so I'm going to talk to you about a
project that I've been working on in my
free time to do with helping people
learn Icelandic but really this is just
an example project because I want to
talk about using docker how how can we
use docker to help us with our
development workflow and then all the
way through to production so how many
people are using docker for production
work at the moment I have companies
where they're running their Production
apps okay lots of people and how many
people are using docker everyday in
development yeah a lot less and this is
this is kind of why I want to talk about
is there until now it's been really
difficult to use docker for development
there's been a lot of problems with how
many people have used boot to docker the
kind of old way and how many people
enjoyed the experience of using booted
docker yeah I was there we'll talk about
some of the problems with that and it's
good news basically everything is fixed
so just to give you a quick overview
with the application I'm going to talk
about it's just an example but it's very
kind of standard I've got a node.js
back-end that's going to serve our data
in JSON format and then we have a react
app that just renders that data on the
screen so hopefully something that
you'll be familiar with and something
that you can translate to whatever your
application looks like so what's changed
what makes now the now the right time to
use docker in development so basically
they've produced some new apps docker
for Mac and dock for Windows
and these these work differently to the
old one so the old version was using
virtual machines you had docker running
in a virtual machine and it meant that
you had to have some special IP
addresses to connect to it you had a few
quirks because it was running inside
VirtualBox so these new apps change
everything these are built on they run
on hypervisors they're not running
inside virtual machines using something
called hype kit which is an abstraction
layer to create cross-platform
hypervisors which is probably the most
crazy thing I've ever heard
the docker command is now available
system-wide so you don't have to have
this special docker terminal which is
the old way of doing it just you can run
docker as a program on the command line
from anywhere like a normal application
and the containers are all bound to
localhost you don't have this special
docker machine IP that you connect to
anymore you can just access your
containers very similar to if you just
ran your node application with node
you'll be able to access it from
localhost same now if you're running in
a docker container it's available from
localhost
the other thing that I'm going to fix
which is what really what sorry the
other thing that I'm going to talk about
that they fixed which really makes the
development story work is that before
there was a big problem with fire
Watchers like if you if your files
changed it it didn't get the
notification that that file had changed
that's like the file system event that
the father changed because the files
have been like copied into this virtual
machine but now they're using a
hypervisor they can use the real files
and everything worse expected so what do
we need to do to run our application
inside docker
so the first thing I'm going to do is
write a docker file who has written the
docker file before okay great so about
half people have written doc forum
before so I'm just going to go through
like how to write docker file for a node
no GS app and I'm gonna go through line
by line to explain specifically why I do
it in this particular way and hopefully
that will reveal a little bit about how
docker works
so all docker images are based on a
starting image so this is what this
first line is
node 6.4 it's you can specify the exact
version that you want which is nice that
gives us a nice repeatable builds and
that means that when a new version of
know comes along we're still going to be
targeting the same build so it's really
nice for Pete ability means that if we
try and build this in a year's time when
we're on node 67 then well it will
hopefully still work you also noticed
that it just says from note so normally
your docker images would have a
namespace so it'd be like for my images
it would be from David Blanton / talent
API these ones don't have a namespace
and that means that they're official
images these are images that have been
maintained by someone from the node
Foundation and they've been kind of
supplied as official images so it's nice
to have node configured by someone who
works at node and then you can just use
it and hopefully everything works as
expected this is how you set an
environment variable in a docker file
and here I'm just changing the NPM
config log level to warn because the
default is HTTP it will print out every
HTTP request that it makes which is
probably more information than you need
we're going to create a working
directory where we're going to keep our
code basically what docker is doing is
it's wrapping up your application with a
file system and that file system
contains all the dependencies that you
need to run whether it's a runtime
environment dependents use binaries
anything that's on the path that you
need so we need to say where we're
running from this so this is the working
directory this next line I'm not really
not really a hundred percent on this but
it's been working really well for me
basically if you're using NPM
shrink-wrap something I discovered is
that normally when you run NPM install
it would look at the package.json file
read the dependencies from that resolve
them based on their constraints you've
put in there unless you have a shrink
wrap file and then the shrink wrap file
contains the exact version the
resolution of all the sub dependencies
that it should install and I've
discovered although the NPM
documentation says it doesn't work is
that you can install from just a pack
from just an NPM shrink wrap file
so why would you want to do this why am
I not just installing front package.json
the problem with the package.json is
that it's become kind of a dumping
ground for everything else that we do at
node so you have scripts in there and
then lots of packages put their own
configuration in there so maybe you have
your babel configuration maybe it's in a
separate file maybe if you have some
test runner it's got some configuration
in there and the reason why that's
important is that basically we're going
to be when we build this any previous
layer that's unchanged will be cached so
if we for example managed to get down to
line 7 here which we should hopefully
always be able to do say ok say say that
our NPM shrink-wrap file hasn't changed
from the last time that we did a build
and that means it can just skip this
step it will reuse the cash from that
version and when it comes to something
like NPM install that saves a lot of
time we don't have to npm install again
even though we're creating container
from scratch so the containers are
mutable but we can use cache versions of
old containers providing that we're
convinced that they should be the same
so basically I was having problems with
like with a package to Jason you would
go to change a script and then that
would mean that you had to run npm
install again which seems like obviously
you know that you don't have to chain
run in p.m. install when you change the
script like that doesn't make sense but
because this is just looking at has the
file changed like if i copy a file into
the into the container and that files
changed and that invalidates the cache
for that level so this is why you'll see
this is a common pattern and people will
always copy in their package jason
higher up in the file before they
copying the rest of the files so we run
npm install and then we copy in the rest
of the application we're going to expose
a port that application runs on port
8000 and then we define a command how to
start up the application so this is a
complete docker file and this is a
docker file which would be able to run
our application here on this machine and
we could also use it to run in
production
so let's go ahead and let's go ahead and
build it at great personal risk okay
super simple to build a a docker file
gonna just run docker build this - T is
a tag I'm gonna tag this image so I can
find it again you don't have to tag them
but then you just get like a hash that
refers to this build so it's difficult
to find it again and then thought just
means where should I build from what's
the context of the build and that's just
the current directory so first thing it
does is like figures out what files do I
need to send to the to the build and
you'll see that it skipped all the way
to step 6 straightaway and that's
because I've ran this before and the NPM
shrink-wrap dot jason hasn't changed
there aren't any new dependencies so we
can just reuse the last dependencies so
this is really nice like we can build we
can build this image and even if they've
been changes to our application code we
don't need to reinstall anything we
don't need to download node again none
of these things have changed okay so
let's let's run the image let's try
let's try out our application that we've
made so how do we do that
docker run this - IT means that we're
going to basically attach this terminal
sessions here into the terminal session
that's inside the container and reason
you have to do that is otherwise you
can't kill it which is frustrating for
demos then this next document - piece is
we're going to take the port 8000 that's
inside the container and map it to port
8000 outside the container next document
is the tag that's the image that we just
built and then I'm going to override the
command so I have basically this last
argument I'm going to run my dev command
and that's going to run instead of
running it with NPM start that's just
going to run load one so that we can
watch for changes
so application starts up watching for
changes great the problem is when I save
the file you know the application isn't
reloaded and the reason for that is
because this copy operation is a
one-time thing we copied the files into
the container but now the container is
done it's immutable it's not going to
pick up the changes from outside so this
is no good for development I mean we
could just stop it rebuild the image but
this is a really rubbish experience for
development so how can we fix that
so try doing it slightly different way
so exactly the same commanders before
but now we have this - V flag and what
that's going to do is create a volume
we're going to take the current working
directory and we're going to map that as
a file mount onto this code directory
which is where we're storing our code
that was the the working directory here
from before and so this is just like
taking the files the outside just like
in your in your on your regular file
system and they're going to act as like
a network drive or a file file share if
you like inside the container so that
now it has the real files and again
we're going to run node one so it can
pick up file changes and now when I hit
save node one rebuilds the files and if
we you can see if we have the new
message that I just typed in there so
we've got live reload working so this is
kind of the steps that we want so we
have our application running inside node
inside the docker container sorry
running node inside the docker container
and the live reload is working we can
just continue our development as we
always happen however it's going to be a
bit of a pain to just always be typing
these extra commands like the past in
these flags how can we how can we do
this nicer so the next step is to write
a compose file this is a file that's
interpreted by docker compose which is a
a tool that docker provides to run
multiple containers and what does that
look like
so dr. Campos file again is quite simple
it starts with version 2 because they
added some extra features which weren't
back backwards compatible so you can opt
into the new version of the file we're
going to define some services and the
service is just anything that you want
docker to start a container for so in
this case we're going to define a
service called API and then basically
each of these commands reflects the
parameters that we had to pass the
docker to configure it correctly to run
for development so we're going to build
from the current working directory dot
we're going to override the start
command this just runs node 1 we're
going to map the ports so that it's
visible outside outside of the container
and then we're going to do the same
trick with the volumes now there's one
problem with what the volume that I did
before and the reason the reason for
that is that the node modules which you
might have installed on your Mac for
example or on Windows they may have
binary dependencies which are built for
that platform and if you just copy your
node modules into this Linux environment
they're not going to work they're not
compiled for the right architecture so
what we do here and this is the biggest
leap of faith that you'll have to take
for this talk is we're going to we have
two volumes and we have the one that we
just used so that's the current
directory outside on the host that's
going to get mapped to this code
directory but then we're going to create
another volume this is a named volume
this is like a quite a new feature in in
docker which I'm defined down here and
there's no extra configuration for that
which just means it's going to create a
regular file volume using the default
driver and then we're going to map that
to a directory inside our other mount so
you have the docker container which has
its own file system that's isolated
we're going to mount inside of that our
files from our host and then inside of
that we're going to stub out like a safe
space for it to install the node modules
for its own
platform and that means that when we run
npm install inside the container
these node what these node modules will
end up inside this volume and they won't
get leaked outside back to your host and
so that allows us to if you want you can
even install run npm install on the host
have the node modules for mac there
which is sometimes useful for running
things like es lint or like your dev
dependencies and then inside the
container all have the same dependencies
but compiled for the linux environment
that it runs in so that's how you can
get that to work and that allows you to
kind of not be locked in to developer
sort of developing in docker sometimes
it's easier to have those actual node
modules if you need to debug something
or whatever it works equally well if
you're going to be kind of careful and
commit to this run err always running
inside docker you can there's no problem
with just having these files being on
your hard drive the problem is if you
then try and run npm that's not going to
work because there for the wrong
architecture you have the Linux files
which have kind of come back onto your
Mac as part of the file share so this is
the reason that I would recommend this
approach because you the errors that you
get a very strange when you have files
compiled the wrong architecture they're
not always obvious so this kind of gives
you the safety and allows you to switch
back and forth more easily if if you
want to do that so the only thing left
to do is to run an app and the way to do
that is using docker compose and we'll
just use the up command which says take
my docker compose file and start all the
services in it and of course it didn't
work because it's still running okay no
problem
all right so you see we have this
slightly different logging output and
it's just basically a pen it's just
prepending the name of the service and
then it has API one because there could
be multiple instance of instances of
this it's not so applicable for
development but obviously in production
you might want to run multiple copies of
a container and then low bounce across
them so now we have this working and
again we can connect to the container
from the outside and it gives us the
message from before so this is great you
can see there we had the lock output
from the HTTP request there as well this
is great
and night-light neatly wraps up all
those all those parameters that we had
to pass in there are other other options
as well obviously but I think these are
the most important ones for development
so how do you run a command that you
would typically have to run outside the
container like you want to install a new
dependency super easy dr. Campos comes
with the run command so if we were doing
NPM install cause before now we do
docker compose run the name of the
service that we want to run the command
in and then the command so what this
would do is it would fire up the
container it would run this command npm
install cause inside so this dependency
is going to end up inside that volume
that we defined this respects all the
settings you have in your docker compose
file and basically this is just a
straight-up replacement where you ran
npm install cause before now just do da
compose run api npm install' course and
if it's too much typing then you can
just make some aliases to make that a
bit shorter it's quite quite easy to get
used to
so the dr. Campos file is really nice
because there's not really any overhead
to creating extra services so one thing
I like to do is just define another
service that runs my tests so this just
creates another instance of the same
container here I'm just changing the
command so now I'm running my tests with
the watch flag the watch flag works it
will rerun my test because we're doing
the same trick with the volumes to just
mount the files inside and we don't even
have to install the dependencies twice
it's reusing the volume that has all the
node modules in and you can imagine
defining more services maybe you could
run your linter and all of these will
just show up in like a nice interleaved
output so you only have one command I
don't about you but before I did this I
used to have to open multiple terminals
like start up various things they had
watches because they're long-running
process is so now this allows you to
have them kind of all together and to
start them all up with just one one
script or composer there's one other
really nice feature worth mentioning for
the services and I don't use this on my
project but obviously lots of people do
use databases so it's worth mentioning
so here's an example of how how do we
connect to a Postgres database in
development so normally most people will
just install Postgres on the machine and
then get it configured correctly and
then maybe if you want to use it with
another another project you have to
reconfigure it and what this allows us
to do is we can run Postgres inside a
container which gives us an isolated
version of this database it's super easy
like it's way easier than installing
postcards we just put the name of it
here if I wanted to install my sequel I
just put my sequel Redis all of these
have official builds and they're all
configured out at the box so that's
super nice we can interact with this we
can configure this image or like any
additional configuration that we need
using environment variables and all of
them just list the kind of configuration
parameters gives you like a nice API
onto this image so here we're going to
define some username password and then
in order to persist
the data that's in the database we just
give it a volume and then this exposes
the port here you'll notice that says
exposed before we had ports and that
defined a port mapping and this time
we're saying expose a port but there's
no what is the port that 5 4 3 2 is
mapped to so this is where we come to
links so you see that the API has a
definition here links to the links to DB
what this means is that docker is going
to set up an internal network route
between the API service and the database
and the database is not publicly
accessible this doesn't have a port you
can connect to it from and that's why it
would be perfectly fine to have your
username password like this this is this
database is not accessible from anywhere
else on the host or beyond on the public
Internet how do you connect to it from
the API what it's going to do is it's
going to create an entry in the hosts
file of this container for DB and then
that's going to resolve to the database
so you can just do connect to server DB
and that will be filled in in the host
file with the link to wherever it's
decided to run this and on whatever port
I really like this and like if you're
using some abstraction over your say
your sequel IV fusions on like sequel
eyes or whatever and you want to try
swapping out your database for something
else or you want to just say like I what
would you be like if we ended memcache
or Redis or some other service like this
super easy to just create a new entry in
your services you can connect to it
really easily using the links and if you
don't like it just throw it away delete
it from your config and there's no no
damage done so this is really nice
but when we when we're running in
production we're not having to worry
about what ports things are running on
everything has a name and I really
wanted to find a way to solve this
problem of like I want to have all my
projects running on a local domain and
turn out to be real easy this is
actually like way easier than anything
else to do a docker this isn't to do a
docker but I just wanted to go through
this just then
straight how easy it is and I was really
surprised these instructions are for Mac
but I assume it's easy on Windows we're
going to use DNS mask so just brew
install DNS mask
we just need to config files the first
one is the DNS mask config file and that
says that we're going to resolve the
address anything that ends in the
top-level domain dev to one two seven
zero zero one to localhost that's where
our containers are bound to so this is
actually enough but at the moment that
means that DNS mask is intercepting
every DNS request so what we're going to
do is create another file that tells the
operating system only use this to
resolve requests that end in dev so this
is this bit here is OSX specific I
assume you can do this on Windows as
well we're basically just saying here if
the domain ends in dev then use the name
server one two seven zero zero one so
now when you use any other site when you
go on Facebook comm it's going to be
resolved using whatever DNS you had
before but if we go to a dev domain it's
going to be resolved to localhost so how
do we actually get those requests that
are coming with these names and get them
to the right containers so this again is
actually really not really easy there
are two images that I know that solve
this problem and they solve it in the
exact same way they're completely
interchangeable I used this one which is
based on high availability proxy because
I was already running it in development
and it was easy I just copied the
configure hand development works in
sorry in production works in development
as well but there's another one that's
based on nginx so if you're more
familiar with nginx then that might be
better you can just read the config
files understand what it does but we
basically just add another entry to our
services for the proxy we're going to
link it to all the services that we want
and then in each service we're going to
define just this virtual host
environment variable that says what what
request should I forward to this
container so basically if the
if the request looks like API teller dev
then this guy will forward it to it we
have some port mappings here
82 84 43 we can do use this to do a self
emanation if you want to test out that
your or your HTTP to set up correctly in
development very easy to just generate
some deficit if Achatz give it to this
proxy here and he'll do the SSL
termination for you fold it on to the
containers exactly like you probably
would do in production the only mystery
line here is maybe this bottom line
which you have a volume which is the
socket what's going on here and also
somehow the it's like from itself to
itself like what is happening here this
basically says that we're going to
connect the docker engine outside the
container to the doctor socket inside
the container and that's so that it can
watch for any changes in in the docker
containers containers been started and
stopped and that means that the these
both these images the high availability
one and the nginx one we reconfigure
themselves automatically when services
start and stop so that's really nice and
that's works exactly the same in
production like that as well there were
two changes that I had to make to my web
pack config to get this working with
this which is just worth pointing out so
that you don't get stuck on the same
thing the web pack dev server has like
this leaky like before it would have
said question mark localhost and then
the port that you're running on but now
we can just change that to our
development domain which for this image
is going to be Tala dev and the
development server doesn't bind to all
interfaces by default so we just need to
put host 0 0 0 0 so it's going to listen
on both our Wi-Fi and also the docker
Network so
here I have another another dr. Campos
file and I wanted to just point out a
couple more features which make it
really nice for development you can use
alternative dockerfile and development
why might you want to do that in the
case of a front-end app the situation
for running in development is normally
very different to running in production
so say you're running a typical web pack
build you're going to be running it one
live reloading
but in production you're going to
compile all the files down to static and
it's going to be served by something
completely different maybe just nginx or
you're going to put it on App Engine or
whatever I I've been thinking about this
like philosophically like a lot of the
ideas around docker is that it's the
same in development is in production
same all the way through your pipeline
but I don't think it makes any sense to
try and like force yourself to run the
same image I mean like you could do this
so you could create a node server that
serves your files statically in
production and then you have some if
development thing then run webpack
config but really you're just you're
just masking the problem you're not
really running the same code in
development in production so I prefer to
just have these separate configs and
then I have another entry down here
which takes my built files and copies
them into an image running nginx and
this is the same image that I'm running
in production this is just the built
JavaScript files with the hashes and
minification and everything just so I
can test out are there any problems if
you're developing as part of a team and
you're going to adopt this workflow one
super nice thing is that instead of
building the code locally you can just
pull down an image so how this normally
works is that you have a CI system
that's building your dot building images
and pushing them to a registry you can
tag these with maybe the branch name
latest maybe even the commit so that you
can find a specific one again and that
means so for example here I can just use
a pre-built image of the API I don't
have to have the code checked out for
the API it's really nice if you have
people working on different parts of the
stack
but then when I come to do a feature
which affects both the front end and the
back end I can just change this to point
to change this image to build maybe just
built pointer to build dot dot slash
wherever I've checked out my code and
then I can make my changes there so it's
really simple to switch between
pre-built images which means you don't
have to have anything installed to get
them running and then to switch back to
the regular development with the files
checked out and then the rest of this is
exactly the same just the proxy to
afford to these things so what's going
to happen I started up so you see we get
a whole load of output on the start
that's just all the containers starting
up together if you have some kind of
ordering thing you can define one
container depends on another so they
come up in the right order you see
everything has come up now we're Packers
said it's finished building its gonna
just try and save this because I left
the other one running so it's probably
not actually come up just to clarify
like the one I showed you first is like
if I'm working on API I'm normally like
developing using tests so I don't have
to run the front end but if you're
developing on the whole stack then I
have a different compose file which just
starts up the whole stack so depending
on how your workflow goes so
so it's not a local host anymore of
course should be able to connect to it
on API dr. dev which you can we should
have another service that's running web
pack dev server and that's service some
HTML and then if I show you that staging
one you can see that here this server
line says nginx so that's been served by
nginx and this is what this is what the
site looks like you can search for
Icelandic words it will show you how
these words change according to the
grammar you can can access the API not
with HTTPS that will return the JSON and
you see all of these have got nice names
oh sorry that's the real service let's
go to the development everything has a
nice name
everything is linked linked together
this guy's using an image to build and
histories Icelandic for horse so you
learned an Icelandic word as well there
we go if I could find my slides again I
would say that's the end of my talk the
slides are on slides comm slash david
glenn if you want to have a look at
those config files have a go at setting
up yourself the code for this project is
on github.com slash Taylor Eastern screw
maybe just goes to the slides first and
then you can find this link but if you
if you're using docker in development
and you have some improvements to this
workflow I'd love to hear about them so
come talk to me thanks very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>