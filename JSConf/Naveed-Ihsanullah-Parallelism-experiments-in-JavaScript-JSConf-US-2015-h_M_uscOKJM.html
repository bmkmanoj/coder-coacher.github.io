<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Naveed Ihsanullah: Parallelism experiments in JavaScript | JSConf US 2015 | Coder Coacher - Coaching Coders</title><meta content="Naveed Ihsanullah: Parallelism experiments in JavaScript | JSConf US 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Naveed Ihsanullah: Parallelism experiments in JavaScript | JSConf US 2015</b></h2><h5 class="post__date">2015-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/h_M_uscOKJM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I said I want to talk to you about our
concurrency and parallelism in
JavaScript maybe a brief introduction is
warranted I do lead the GS team and for
the last several years we've been
thinking about this on these sorts of
problems and JavaScript and how we can
address that like to share some of the
ways that we've come around for
improving concurrency and parallelism in
TS but before we streams a little tight
before we go down that right let's
actually talk about some definitions
concurrency and parallelism often come
up together but they're not the same
thing so the way I think about it is
concurrency is how do you make an
application more usable and here we have
two queues of people and a single
revolving-door you can come up with some
logic to have them take turns going
through there but it doesn't matter what
you do you have one a whole queue go
through first or have them take turns
like that some sort of multitasking to
utilize the resource and make sure
everything happens and examples inside
the you know running program or
application might be pre-emptive
multitasking Windows 95 or event loops
some and async programming that were
very familiar with parallelism is all
about how do you be faster by leveraging
or exploiting Hardware availability so
in this case we now have two revolving
doors and we can theoretically hopefully
go twice as fast because there's no
contention and there's no complexity in
my examples but the real world is not
like that the real world has complexity
and the reason why concurrency and
parallelism are often mixed together are
because in the real world it's really
hard to separate the different aspects
and you have to be very sensitive the
way you take care of your application to
avoid deadlocks and races so now that we
talked about those definitions I want to
explain why this is improving this is
actually important in JavaScript and to
do that let's talk about the evolution
of the web application back in the day
not quite 1980s I guess are the 80s but
a way way back I think we all had a
homepage
that looks something like this I'm sorry
mine's under construction at the moment
but uh you know maybe all evolved a
little static HTML and gifs can really
only take us so far
in 1995 of something really big happened
I think most of us know what this is
hey guesses I think I heard it yes I've
been like invented JavaScript and it was
good right the universe of I think it
came to the right venue to make that
comment right the universe of the modern
interactive web was really born and
homepages were never the same again so
in the next 20 years I think the home
pages and websites have evolved tasks
Windows Alert and now you have desktop
applications that have actually gone non
native here we have an example of excel
and most of Microsoft Office suite is
actually now available as a web
application JavaScript and the web
platform is actually capable of
supporting even virtual reality
technology here we have a frame or a
couple of frames from the polar seed
project at Moz VR is working on
so today's javascript is really fast in
on our we fafsa.com we track most of the
modern browsers their performance and
how they're doing over time and here you
can see that we're all getting a lot
faster and that's continued to be true
for other factors as well because we're
inventing other technologies like
awesome jails that allow us to bypass
some of the inherent bottlenecks in
regular JavaScript on this are we fast
yet slide the bottom line is native
clangs C++ compile code and as MDS
running on Firefox it's the only 2x of
that we have techniques that in certain
workloads can get much much closer to
the native within 1.5 X so if javascript
is so fast so why do we have a problem
well to illustrate that I have a
snapshot here of my proof mom from my
Windows desktop right now you can see
that our
the desktop doesn't seem to be doing a
whole lot right we're running at 55% CPU
utilization and I have a ton of
applications open including Firefox with
I believe at the moment 30 tabs so I
open a one more tab with a really
intensive JavaScript application and
you'll see that the perfmon seems to be
indicating something so those who are
not familiar with Windows and proof lon
we have our bunch of squares for on top
and for right below each one represents
a CPU core and how much work it's doing
in this illustration course 3 &amp;amp; 4
they're a virtual cores but we'll sort
of not worry about that detail on course
3 &amp;amp; 4 are doing a lot of work but the
rest of the CPU is mostly just sitting
there and overall utilization is now at
24% out of a possible hundred right so
the computer is still 75 percent idle
why does this matter well the game Hertz
race is effectively at an end
it's hard to make CPUs faster than they
are right now power and heat have really
stopped our ability to innovate there
and so for future Hardware the way we
are scaling is by adding additional
cores or execution units your modern
smartphone typically has two cores some
of them are even quad core enabled
laptops and desktops have four to eight
virtual cores and workstations and
servers can have 16 to 20 cores per CPU
and they may have multiple CPUs on their
motherboards
so right here so far I've really only
talked about the parallelism aspect on
how to do work at the same time but
concurrency has its issues too we're
always waiting on something in a
JavaScript application
we're waiting on events we're waiting on
the user we're waiting even on the cache
and of course we're always waiting on
the Internet to do something so
improving parallelism and improving
concurrency can really make a big
difference to a host of applications
some of the application types of course
games AI rendering cogeneration and each
of these specific application types have
their own
the main specific way of dealing with
these constructs so it's something that
if we do solve this problem we want to
preserve as possible while looking at
this problem in investigating different
ways to deal with it web workers are
around web brokers do promote
parallelism you can do something on a
thread
they had tend to be very heavy threads
but it works the real issue for
saturating a core isn't doesn't seem to
be spitting up another thread its
communication between that and
coordination is really about concurrency
but currency does exist from the web
today it would exist in JavaScript we
have or we've had at this point parallel
JavaScript postmessage will always be
around it's very useful and then of
course to the buffer transfer semantics
inside post message so this would have
died into each one of these and see
where they may have some limitations
parallel j/s for those who are not
familiar with that tellegio's was a
experimental effort to see if we can
bring some of the semantics of MapReduce
to JavaScript and enable a whole slew of
application types it was a casual API
but it really took some sophistication
to use it
so the average programmer couldn't make
that much out of it and it had some
really high implementation costs in the
engine so it's hard to support going
forward post messages obviously around
and incredibly useful but it's also has
its own set of issues it's dependent on
the event loop the performance which
we'll go into a little bit more later
just isn't there and you cannot share
state your transferring state over
really one at a time and if you care
about the response you have to transfer
a new state back it's possible with post
message to transfer a buffer entirely by
reference and that solves some set
subsets of problems but you still want
to can can't work on the same data at
the same time this really is a
bottleneck a coordination bottleneck and
something that we want to address so
while looking at different possible ways
of handling this sort of became apparent
that obviously native has handled this
already the operating system system
libraries have a
slew of constructs that allow us to
solve concurrent problems solve them
well and solve them in ways that in each
individual application can tailor for
its own needs so that helped us come up
with some design considerations for what
our ideal solution may look like of
course we want need of like performance
that's kind of the Holy Grail benchmark
right let's be as fast as we can be we
don't want to be dependent on the main
event loop if possible because everyone
uses the event loop and that as you know
Simon pointed out can lead to some janky
news implementation versatility what I
mean by that is every problem has
probably an ideal answer and so you want
to allow the developer to come up with
an answer that makes the most sense for
them instead of being constrained by a
really high-level set of utilities that
forced them to talk in a particular
vocabulary we want to support this this
one may seem a little arbitrary but
we'll explain more in the future we want
to support the algorithms and
applications are based on threads and P
threads for a long time
threads have been around and there's a
lot of good computer science research on
them how to use them really effectively
and all those algorithms have a lot of
value not every single application not
every single problem can benefit from
those algorithms but it is possible to
directly apply them with minimal changes
it doesn't seem like a win for the user
so it's something that we would like to
support and finally support the exam the
extensible web philosophy and then when
out there familiar with the extensible
web selasa fee or have read the
extensible web manifesto a couple of
people so it's worth explaining a little
here I'm sure every single person will
have their own take on it and I
certainly invite you to go to the
extensible web manifesto web site to
read about it in more detail but at a
high level it's this thought that the
best innovation is going to come from
developers come from you guys
sometimes standards bodies and browser
developers since
too much time iterating an idea trying
to come up with something perfect and by
the time it comes out to where the
developers can use it we didn't really
hit the mark so it seems like it's a
better model for us to provide low-level
primitives that you can then generate
and iterate on quickly in J us instead
of for us to provide rich high-level
implementations that are surfaced and
then you don't have a lot of physical or
a lot of flexibility to tailor that to
the way you want to do work so taking
all this together we came up with shared
memory for JavaScript and it looks
something like this so this mod font
might be a little bit small for some of
the people in the back to read I know
you guys in the front you want to be the
first line so I'd like to thank Lars
Hanson for actually putting this
document together obviously it's
incredibly detailed and there's a link
at the end so you can take a look at it
yourself
but I think this might be one of those
times where an example might be more
beneficial than going through all the
pages so what I like to do is build an
example the incrementing broker and what
I want to do here is implement that's an
example in a couple of different ways
first with post message and then with
using the shared memory stick things
that we just came up with and you can
get an idea of what that workflow will
look like and one of the trade-offs
might be so first we're going to do this
with post message and what I did here is
build a sequence diagram so you can
really understand exactly what I'm
trying to do it's you know create a
worker wave worker to tell us it's ready
and then we'll run we'll start working
and the goal of this incrementing worker
is really the only thing that's going to
do is it's going to talk and it's some
sort of mechanism channel back to the
the master or the main thread passing an
integer each time and incrementing it
when it gets an integer from the master
they're going to agree upon some count
that in this case I think I used 100,000
and when that sort of message passing is
done the master will display some
results so the simplest scenario here is
the master says hey start at sea
go 400,000 times at the end the worker
will and the master will have a variable
it says 100,000 not rocket science but
hopefully enough to illustrate some
points here's the implementation of the
master side and once again it's code and
it's small and I think it's worth just
calling out some lines here so we agree
on 100,000 iterations and the master
posts to the worker to start at zero the
worker and the master will talk inside
this callback and the workers data is
going to come back as event data it's
going to be unpackaged when some
condition is met the terminal condition
you know it will end but otherwise NASA
will pass the the integer right back to
the worker and the worker side is even
simpler the worker all it does is
unpacks the masters integer increments
it and sends it right back on its way so
how does that work how fast does that
work well for postmessage in this
implementation on this laptop i get
about fifty four thousand messages per
second and i think the example is pretty
simple i there's probably a variety of
this that you use all the time i'm just
for solving message passing techniques
between the worker and the master let's
sell that again using shared memory
specifically using shared into 32 array
so there's going to be some new
constructs here coming out of the shared
memory document that i like to call out
and this is actually too small for even
me to read on the same page with all of
you so the first one i'm highlighting
here is shared array buffer this is one
of the the core core pieces of the
concurrency techniques that we've come
up with which allows us to share memory
you basically you specify a side and
here is a syncretic in 32 object there's
actually a layer on top of what actually
exists in the spec but what's in the
spec is a little too low-level to
cleanly show it and a presentation
the purpose of this is basically it's a
way table object it's going to allow for
the master and the worker to coordinate
using post message the only use of post
message in this example we're passing
the shared buffer to the worker but
because it's the shared buffer the
master will be able to use it as well so
this is a shared in 32 array that's a
view on the shared buffer we're
basically saying at this address inside
the shared memory region it's an integer
treated like an integer give me energy
semantics on it and here inside the the
callback using the socratic the master
is saying after incrementing the buffer
master is saying all right I'm done and
so the worker will wake up and now in
this example unlike the example from
before the master is also incrementing
the count the real only that we real
reason for doing that is this loop the
nos would do nothing other than say I'm
done without having done any work so I
wanted him to do something to justify
the existence of that loop other than
the call to let the worker continue
otherwise the worker would just spend
would count to 1,000 instantly and be
done without the master in the worker
going back and forth so here in the
worker side we have once again a that
weight of an object is in Quebec and the
this year a 1032 view on the shared
array buffer and on the loop side we
wait for the master to be done increment
the buffer and then say we're done so it
looks nothing like the post procedure
version version it's incredibly specific
to integer only post message you could
pass on anything but it does function
and it functions incredibly fast where
before we had fifty four thousand
messages per second this time we have
six million over six million messages
per second I think we can all agree that
I cheated a little bit right so unlike
the post message model where I can pass
anything in and do anything I want with
it where it's synchronized for me and
where I have cons
I don't have to worry about locking all
that sort of nitty gritty low-level
stuff was exposed in the shared memory
version so it's not really a drop-in
replacement for postmessage I think I
can do better and I'm going to try to do
better in this second version and once
again because performance isn't
everything
sometimes organ omits of the
implementation matter so here what I do
is and I'm hiding a lot of the details
but what I do differently here is I'm
creating a new object that wraps a
sender and receiver a channel sender and
a channel receiver and all this is built
on top of the same primitives that you
saw before and if we look inside our
loop now it's a lot cleaner and it Maps
much much more directly one-to-one with
postmessage
we can basically treat the send like the
post to the worker and they receive like
the post from the worker there's no
locks exposed here there's no way table
objects that you could screw up by
signaling at the wrong time and the
worker side is just as clean and just as
simple and I think very very easy to
read arguably even easier to read than
the native post message implementation
with received increment and send so how
does this perform it still performs a
lot better in post message we're at 295
thousand messages per second on this and
it's a huge trade-off for using this at
these high-level constructs versus
shared in 32 array but the choice is
yours right the choice for having high
level constructs or low level
performance is something that this
implementation allows you to have so
going back to our design criteria let's
see how we did did we get native like
performance well I didn't directly
compare it against native native being
native clang peoples plus but we are 5x2
much much more than 5x faster than
postmessage so I think we did pretty
well there but will actually come back
for that one again a little bit later
how about not depending on the main
thread event loop aside from the post
message to pass the shared array buffer
we don't use post message at all so we
are not really dependent an event loop
at all implementation versatility I give
myself a check we have two
implementations one really fast and
really specific and one general purpose
that still reasonably fast do we support
this extensible web philosophy the fact
that you can have these multiple
implementations and our iterations were
done in JavaScript instead of rebuilding
the browser each time I think I can give
myself a check for that as well and the
final one is three support applications
at algorithms based on threads and P
threads but we don't really talk about
that at all here so I'm going to table
that again for a little bit later on in
the discussion so we agree at least on
three or five maybe three and a half of
five anyone okay hard graders out there
in the audience okay so I have a couple
of demos and they're live demos so you
know we'll see how that goes this is
against a version of nightly with one
private patch applied so I am one of
these demos will work asses and in a
nightly version of Firefox the other one
is not quite ready yet the first one I'd
like to show you is an implement
implementation of the Mandelbrot
algorithm visualized in JavaScript let's
go out to my nightly I don't know why
that's here ignore it all right so here
we have mental broth in j s and you need
to do the usual things zoom around by
the way this is the first demo that I
ever showed my wife who's in the
audience of something that I actually
made possible on the web so she was so
excited to see this thinking now she's
gonna understand what I do for work
after I was done showing her this she
has absolutely no idea what I do for
work so maybe hopefully this will go
over a little bit better for everybody
else right now the default on this is to
use one to four threads I'm going to
reduce it to one thread and just show
that our CPU utilization is that number
is not exactly right what it's showing
is out of 800 could have eight cores so
a hundred percent would be full
utilization on one core
so to show that a little bit more
clearly this actually go to each table
which I have running in the background I
don't know how familiar people are with
the H top application it's similar to
the perfmon that I was using on Windows
where you can see in the top left corner
there are eight effective CPU cores on
this machine and right now it looks like
four of them are kind of doing something
and overall CPU utilization is around
100% out of a possible 800 percent so
back to our demo let's change this to be
used for threads and increase the number
of times that this thing updates from 50
to 500 now my frame rates are dropping
because of the update interval but
increasing because I added more threads
what I really want to show you here is
that it's possible for us to really make
this computer come to a halt so it's not
very often that you can run a JavaScript
application that pegs the CPU on a
modern computer and if you're up here
and I don't we can pick this up it's the
fans on this laptop are now at 100% so
yay
I came up with the most inefficient heat
warm heater in the whole world hopefully
this thing will shut down politely I
really don't know what that is I'm just
gonna ignore it I can ask you to do the
same go back to our presentation on a
different machine with sixteen effective
cores I actually captured the
performance of that demo so I could
Ellis trait it here and in that capture
I actually enabled something called
Simbi which are the blue bars let's
ignore that for right now
if you guys are really interested in
that you can come find me afterwards and
we can talk more about Cindy but the
gray bars are the performance normalize
against one core performance on that
memorable example and what you can see
here is we have near-perfect CPU scaling
across all 16 cores adding a core added
about 70
more power more processing power and
that's processing that we could use for
really awesome game demos like we saw in
the presentation just prior to this one
so the next next demo I'd like to show
something I'm really really excited by
and in fact something we only just got
completed so hopefully this works like I
said I'm running a private build of
Firefox
so take a sip of water for good luck
this is the unity WebGL benchmark are
people out there familiar familiar with
unity technology as a company show of
hands
pretty good pretty good percentage of
the audience for those who are not Unity
Technologies makes a 3d game engine
called unity it is one of the most
popular licensed 3d game engines in the
world if not the most popular one and
it's a company that I think is does
really really exciting things so for us
to be able to work with them on enabling
their benchmark using our shared memory
was really exciting like I said it's
something we only got done Friday of
last week so we'll see how that goes
back to my nightly and let's start this
up use a default them 4 cores
so as you can see right away we have a
text rendering bug on the first screen
which will ignore I'm also going to
uncheck the first two demos because
they're graphically maybe not as
interesting as on the other ones
and let this go so this benchmark is
much much more than a WebGL benchmark
what it is actually is testing the
entire unity 3d engine and it's all
running in the browser basically
automatically ported from C++ very
minimal changes on the unity side all
the changes were in our shared memory
and in Firefox itself here we have a
bunch of dancing bears and really what's
more exciting than skinned the dancing
bears so like I said it's much more than
WebGL they have AI running
physics particles skinning and also
their their job system which allows for
execution of threads threaded content
yes I think that is more much prettier
than any demo I could ride I'll probably
let it go one more thing oh yes have to
wait for the Snowman in the middle of
the flurries so like I said the reason
why I'm really excited by this is the
unity 3d engine is a massive codebase
enabling that basically automatically
poured it over directly from C++ on top
of this is an incredible validation of
the technology that we put together and
that is working at all is amazing but I
think what I'll show here let me stop
this
oops into money um is the performance is
really quite good too
I'm gonna focus on just the top half of
the slide those are all the tests that
it was running on the left side and what
we had is the blue bar is native
performance we seem to have an anomaly
on the first test the blue bar is
shorter than anything running in
JavaScript so I'm sure it's something I
did and we'll look at that later but for
all the other tests Green is normalized
against one core performance it yellow
is four cores and then eight red is
eight reddish orange so you can see as
we added four and then eight cores we
came much much closer to native
performance on a bunch of these tests
we're still behind on some of the other
ones obviously there's always going to
be work to do and not everything is
going to scale as well in JSON cores as
other tests so I cover this a little bit
but yeah this is actually a design
criteria how we did so we talked about
need of like performance I think now we
can say yes check we really do have
native performance especially on certain
workloads we're almost where we are in
NATO's in the unity benchmark the last
one support applications and algorithms
based on P threads is a direct port from
the pthread implementation of unity and
we were able to make it work with almost
no changes to their codebase so I'd like
to give myself a pat on the back do you
think I deserve it
all right
thank you
so I talked about the some of these
reasons why the unity test is
significant not everyone is a game dev
not everyone really came that cares that
much about games I personally loved them
I think they are they're not just fun to
play they're fun to develop and they
really challenge software developers and
hardware developers and so I think the
fantastic test of any infrastructure and
we were able to make all this work the
benchmark is not just functional but we
were able to make it work fast so Yuka
you're lucky
did most of the implementation work on
making the unity benchmark functional he
Brooklyn team scripts inside
he's a mozillian who worked on him
scripts inside and he worked on the GS
side he I asked the team for why is this
particularly thing significant to them
and he offered up this quote which I'd
like to share with everyone with shared
memory the web lifts an important
limitation with parallel execution that
had it that it had compared to native
and unity is now able to reuse the same
code base for the web as well shared
memory is not comparable to a library
call that can be emulated or polyfilled
so I am sorry for all those people who
are asking the question can we polyfill
this we cannot but it's a fundamental
concept of parallel secure architectures
I'm not sure if I can get if they can
get any bigger than this he's a very
emphatic guy and he's very excited but
he really did I think capture the
significance of what we accomplished
here so what's next for shared memory
and for the work that we've done here
well shared memory is in nightly right
now Azzam jeaious for those who are
familiar with it could get another show
of hands
how are people familiar with albums yes
okay most of the room to albums yes is a
low-level subset of GS that we put
together that you can really optimize
for and it's fantastic for certain
workloads especially cross-compilation
ones
so as ingest works with shared memory
and pthreads
it's not going to move out of nightly
until we standardize it but we are in
talks to standardize it the API the set
of documents you saw that large
four together still subject to change
we're getting feedback from a lot of
sources inside Missoula and out but the
good news is Google has actually
announced very recently that they're
going to start implementing this too so
hopefully in a year from now we'll start
seeing those saw and applications built
against it everywhere I was going to
have links that nobody can read but I
think they'll be some opportunity to
share on the this presentation later so
hopefully you can catch up on any
details here that you may have and
you're always welcome to find me thank
you very much for your time
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>