<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Felix Geisendörfer: Faster than C? Parsing Node.js Streams! | Coder Coacher - Coaching Coders</title><meta content="Felix Geisendörfer: Faster than C? Parsing Node.js Streams! - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Felix Geisendörfer: Faster than C? Parsing Node.js Streams!</b></h2><h5 class="post__date">2012-11-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Kdwwvps4J9A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks everybody for coming so I'm Felix
guys and over you find me on Twitter on
github as Felix GE I have a startup here
in Berlin where called transloaded we do
file uploads and video encoding we were
one of the first users of nodejs
which also meant we were one of the
first contributors of nodejs because it
was not really doing everything we
wanted but I'm not talking about this
I'm talking about faster than C and
before I get into it I want to apologize
sorry about the title bait we all have a
reasonable discussion about performance
I'm not gonna make some claims that I
cannot back up but I think we're gonna
talk about high performance JavaScript
for sure and I think the results in some
of the stuff I was doing look very cool
even so C is probably still your tool of
choice for some problem domains so I
want to make it like a battle the battle
is JavaScript versus C or good versus
evil
wait it's a good part about this evil at
least we may even need the bad parts to
win this one
and so just little backstory here my
story starts in my suffering really
starts in early 2010 because in early
2010 we had no MySQL module for no chess
it did not exist many people in the
community wanted such a thing but it
just wasn't around however what we did
have was no SQL library go figure
no SQL was first in node and that was a
Noah scroll people doing SQL it's crazy
but so then in early 2010 this guy came
along my Siri Tryphon github and he
actually posted a message to the mailing
list a note mailing list and fix the
problem with promises which are long
gone and then he was like by the way I
started this node my sequel module and
link to it and everybody was like wow
somebody's finally doing it so I checked
out the code and I I was really curious
on how he did to look my sequel binding
because back then it wasn't quite clear
how we would do these blocking library
integrations into no chess that's not a
solve problem but Bexon I was curious on
how it would be done and when I looked
into it I found something weird he
actually didn't buy into binding to live
my sequel he wrote a pure JavaScript my
C
client and so I was like wow how did he
do that and so well if he thinks this is
impressive check this one out this was
before buffers became usable so he had
no buffer support a note he basically
did his parser using javascript strings
I was like what's the this guy was
crazy apparently what he did he is also
based on prior art credit where it's due
he tooks a ruby a Ruby implementations
that had done the same thing for Ruby
and my sequel module and he pulled at
that to know chess and so unfortunately
never finished it so he get is kind of
working but then gonna stop working on
it and it went away but while we're
talking about buffers and all times a
node here's a little piece of trivia
buffers and know it used to be called
blobs raise your hand if you knew that
holy crap you guys are good because it
was not a long time that buffers where I
called blob sonoda it was three minutes
and fifteen seconds which is really a
shame because blobs to me would have
been a much better name because I'm
sometimes talking about buffering data
but I may mean like concatenating
strings or pushing stuff into an array I
don't always mean like a buffer of
binary data so it's a shame
rest in peace blobs oh well anyway back
to my sequel so what what mastery drives
library really did for me it showed me
that my sequel can be done without lip
my sequel I had not considered this
before Ryan actually had not considered
it before he was really worried that my
sequel with the deal-breaker for note
and so I really wanted to use my sequel
this margin was not continued anymore it
was based on strings we now had buffers
so it was like okay maybe I can do this
so I started this module and I wasted
too much time on it instead of building
my startup product I built this module
for like three months but then it
started working and people started using
it so that was nice right but no good
deed goes unpunished and I actually
traced this back who came up with this I
think Sir Isaac Newton make two proof
for it and he called it the third law of
motion and it kinda goes like this when
the first body excerpts a force f1 on a
second buddy the second body
simultaneously exerts the force f2
equals minus f1 on the first buddy this
means that F 1 and F 2 are equal magnet
and opposite in direction so I'm not as
clever as Isaac Newton says for sure but
I can imagine if he was living today
maybe some of the stuff he would do I'm
sure I'm sure he would be on github I
guess we can all agree on that and and
so I don't know maybe he would study
like the community effect and get help
maybe that would be his saying I don't
know but let's imagine he came up with
some like the third law of
github and it could go like this when a
first person pushes a library l1 into
remote repository a second person
simultaneously starts working on a
second library l2 which will be equally
awesome but in a different way so that's
why we love github right so that's what
happened but it can also hurt sometimes
I'll show you why
so this guy came along son Ethan on
github and he wrote a library called
node my sequel live my sequel client
which was pretty much what you would
expect it to be it's a binding to look
my sequel what he also did he did some
benchmarks and I did some benchmarks of
my own and here's a recent one and this
is what happened so on the left side is
my library and on the right side is his
library that's what you would expect
right this is JavaScript or sassy of
course let my sequel is in Britain and
see my library's JavaScript and see you
so much faster than JavaScript right so
that's what I thought for a long time
but I felt I felt wrong because I mean
haven't we getting some marketing
message v8 is supposed to be really
really fast turning our code into
assembly and then they make crankshaft
and hold us now it's gonna be even
better and it's gonna analyze what our
programs are gonna do and I mean no
chess I think it's gonna solve world
hunger and everything so was i living a
lie and I've been lied to
kind of I mean the stuff you read on the
note home page is like crazy because the
ADA node are our tools their tools and
performance performance is not at all if
anybody's healthy performance as a tool
that you can just download and install I
don't know it's that crazy no what
actually turns out to be I think what it
is is performance is hard work and data
analysis so if you expect this talk
being me like telling you here's one
little trick to do to do in your
JavaScript and now
everything's gonna be faster I have to
disappoint but I will present some stuff
that's useful I promise
so anyway I set out and I tried this new
approach that I'm gonna show you and I
continued working on my library and so
this is what happened I I beat them a
little bit maybe at least I can say I'm
as fast as with my sequel client so - so
the colors have changed but - one the
first is to the right now and as you can
see it's alpha 3 which means I'm
spending time optimizing performance
problems nobody cares about instead of
finishing so saying I mean it's working
but I want to do connection pool and
stuff but I just geeked out on this all
the time anyway so I told you about the
third laugh github and it could be
 but I don't know I saw I'm
starting to believe in it because after
I submitted my proposal it was accepted
this guy came along Brian White and he
wrote a new library called Note Maria
SQL and Maria SQL is a fork of MySQL and
they're continuing zuv development it's
actually won us a you see original MySQL
guy and they also continue to improve
the client so the Maria SQL client is
actually non-blocking
so it fits much better with what notice
doing not only that this guy Brian white
he's really smart so he knows how to do
C's findings much better and these two
things combined led to this add stuff
stuff here I was thinking I was present
like the fastest my sequel library on
earth written in JavaScript it's not
probably not gonna happen so is this the
time to give up should I give up never
so I'm working on this new partner and
this is undone it's not finished
and you shouldn't listen to it but it's
interesting because once again this is
my old library against my new library
it's a new parser it's not integrated as
a module or anything yet it's just
running a very limited benchmark but it
seems like I can get a 2x performance
increase or almost 2x and so I have
reasonable hopes that I can at least get
on speed parity with summary SQL marshal
if I find the time to finish this up but
should I do it because wouldn't the cert
love github kick in and somebody just
comes along and does a faster one I mean
what's the point
so actually I think I should do it
because I think we're we're getting
close to the end game and because the
end game here is that we're gonna run
into one last bottleneck and that's just
creating the JavaScript objects we're
gonna be able in JavaScript now or we
are already able on JavaScript to parse
these protocols really really quickly
but supposed to see libraries or to see
bindings and my code has to do the same
thing it has to ask v8 to create a new
JavaScript object like a row and to give
it column values and if I want to use
the library set bind or sets available
in JavaScript those performance
penalties probably going to be the same
for me in JavaScript and C++ people so I
think we will just end up at a very fast
speed where only the eight can make
things go faster but we will all get
faster C bindings and my module also we
already won because my sequel server is
already saturated I don't know unless
you're serving data from memory and you
have really good hardware setup I think
a single MySQL client either C bindings
also Java Script can fully saturate and
my sequel server using one processor on
a client so this stuff is getting fast
enough so anyway it was kind of cool but
what's even cooler is that I found out
some stuff on on how to do this stuff so
I studied a lot about how to write fast
JavaScript and I found out a few things
that don't work or at least not for me
for my problems one thing unfortunately
profiling profiling is cool the v8
profiler it can do good things for you
but I found that profiling is mostly
good for spotting small functions with
stupid algorithms performing many
iterations so that's the kind of stuff
where the profiler will be like hey yes
this function and you
and all your time in there and then you
go you look at that function you fix
that function is really nice but if you
have a complex function with many
primitive operations it's not so nice so
for example in my first my sequel module
ahead sis my whole part was one method
and that sounds stupid I know but stay
with me I I was cargo holding that
I was looking at Ryan's HTTP parser and
that's what he didn't see so I was like
that's the way to go one you just which
statement that's that's what we need to
do and so unfortunately the profiler did
not tell me since were stupid because he
just saw one big function posited right
being really hot and I'm like okay I
already know that so I didn't get very
far with supporter even as I continued
to make my module actually nicer and
separate things out into different
functions I still find supposed to be
lacking sometimes because sometimes I
spend all my time and stuff like
assigning properties to an object sure I
know it's this function is hot but how
do I make it faster why is it not fast
the profile I only gets you so far
use it but for some stuff it's not good
also another thing this one is important
please listen to this one don't take
performance advice from strangers and
I'm a stranger because I don't know I
don't know what your performance
problems are and I don't know what
you're trying to solve so any advice I
give you is really in the context of my
library so only use it for that there's
one piece of advice that I think is kind
of generic but all the other stuff just
ignore ignore all the stuffs of the
eight people tell you for optimizing
your code and blah blah blah I mean it
makes sense but only when you have
proven for it to make sense in your
problem domain so use this as good ideas
and inspiration but don't be a cargo
cool T like me like I was taking the big
switch statement idea and thinking that
was great without ever testing it
actually so what does work
I think PDD works oh that makes no sense
right behavior driven development no I
mean benchmark driven development and
since what I do all the time
benchmark driven development looks like
this it's basically very similar to
test-driven development so I only
suggest to use it when performance is an
explicit design call and it generally
works best if you writes a benchmark
first and not after so how do you do
that so basically you create a function
benchmark
and that's where your code is gonna go
in and I mean it's really stupid and
simple and then you do this while loop
you take some time when you start you
run the benchmark and you measure how
long it took well when you run this
obviously it's gonna say no zero because
we're not doing any work yet and if I
just add a little bit of code I'm
probably gonna stay at zero for a very
long time because the JavaScript timer
resolution is not very good it's only
giving me milliseconds whereas some of
those operations happen much faster
don't go out and try to find a better
timer just make sure that benchmark
function does enough work so you get
several milliseconds as a result it
suggests like 3 400 milliseconds is how
long your benchmark function should run
and that's that's where it will produce
good results this way anyway so the next
step is you implement a tiny part of
your function and that can be something
really stale like for example for my my
sequel library what I generally always
start out with when I repeat this
process to write a faster parser is I
just parse the headers in my sequel
packages so I get like a header that
tells me how many bytes are gonna follow
and then I skip over all these bytes and
just parse millions of packages really
fast and this is good enough for my
benchmark to slow down from zero to
something I can measure and then I start
to experiment I'm looking at what can I
do what did the v8 people tell me so
this way I remember the tips they gave
me but a lot of times I find that some
of them don't apply at all and some make
a big difference but you will never know
unless you actually benchmark it so I
mix these three except play around I try
different things and I repeated and that
was a result I get are surprising so I
found out that for example in my context
from my library try caches okay I can do
try caches performance penalties not too
bad I can do it big switch statement bad
I already knew that told you guys
function calls I was surprised I the
reason one of the reasons I saw the
switch statement would perform well
because it's all one function so has to
be easier for the compiler turns out I
can actually make almost as many
function calls as I want in my very hot
loop it hardly has any impact because v8
is really smart at inlining or doing
whatever magic by the way these VMs are
magic that's why I need to do this
I have no reasonable hope of ever
understand what's going on under the
hood so I need to like do this trial and
error
alchemy star another thing I found by
experimentation buffering is ok one of
the reasons my
original switch statement was the huge
was because I designed the parser the
cargo coding again after R I'm trying to
make it part use as little memory as
possible so I was trying to never buffer
any data so if I was receiving in my
sequel package I would always try to
parse the amount of bytes are already
received without ever storing any up
beforehand so what I'm doing now I'm
hovering entire packages and this
results in much less housekeeping I need
to keep less track of sustain less track
of where I am in the buffer I can write
much easier code that's actually better
and I only found this out by
benchmarking I thought if I use more
memories has to be slower but actually
turns out you can use memory in many
situations to gain better performance
here's my favorite one I think you guys
gonna like this one so I hoped I had
this function called pause rose in my
posture it very much looks like this so
it takes two arguments the columns and
the parse itself so parse itself is
hooked up to the socket which pumps data
into it so it's a writable stream but so
whenever I have a row data a row
buffered up this message gets called and
it's basically loops over all the
columns which I get beforehand and then
I receive two column data so I do this
loop and then for every column I have I
read one parser that read column value
one column value and this is I mean this
as simple as get and I have to do this
because I won't know the rows i I mean I
get to his columns beforehand but
they're different for every query so I
cannot really optimize his code faster
at least that's what I thought for a
long time and the profiler was also like
you're creating an object or something
but I don't know what can you do so I
set out to make this faster and so this
is what I came up with I'll give you
guys a second to take a look does this
make sense okay so what this basically
does is it creates a new pass or it
creates a new pause row function and
uses evolve so what comes out of it is
basically aesthetic functions that
already knows what column values it's
going to parse so instead of dynamically
adding C's values to the object it gives
supporter or b8 this work to do and so
we've heard all these things about evil
being terrible and being slow and blah
blah blah
screw that evils awesome here I said it
this is this is actually the Z trick
that got me the most performance for my
recent parser for the new experimental
parts I'm working on this thing was you
should get me from parsing 350,000 rows
per second to 450,000 rows per second it
makes an amazing difference and again I
don't recommend this in all contexts I
did only when it's your problem and
you've proven evil to you be useful and
you still need to make sure nobody's
trolling you this is my sequel server
sending me back if a column name that is
escaping my well my code then crazy
things will happen so anyway if you want
to level up your game next the next
thing you can do is you can do data
analysis and the way to do good data
analysis is to produce data points as
tab-separated values and so each line in
your in your output should be one
measurement like the duration how long
your benchmark function was running and
then tap separated you add as much
matter information as you can
so ya add virtual machine' matrix so
heap used heap total you had operating
system metrics like process RSS usage
and everything you can you get your
hands on that could be relevant to what
you're doing or relevant because
sometimes it turns out these things also
impact and the last one is the most
important one do not mix data now data
and analysis it was my biggest criticism
for all the performance benchmarking
tools out there
I give me a framework to write
benchmarks and performance things in and
then they will run my benchmark and
they'll be like the median is this or
they'll be like this function is 2%
faster
well they'll draw me one graph I mean
it's nice it's convenient it's fast but
it's nice and convenient and fast like
something else and since something else
is running sequel queries from HTML
templates you can put these things
together it works you get your done
really quickly but it's an estimate aim
and it limits your options of what you
can do later on so don't mix those two
steps per two separate files
here's how to do it best run your
benchmark print your stuff on standard
out and then pipe it into T it's a UNIX
utility and write it in two types of
rated value file so you can watch the
output while status coming out now that
the benchmark has finished running you
get this pure data in a file and you can
analyze it and for the analyzing I
recommend to use a tool that's really
good for it so that
see our programming language I'm not
really an expert but ggplot2 is the
library you can use to make really
pretty crafts with it the previous ones
we're useful that and later on I'll have
a link where you get really good art
tutorials it gets you up to speed in a
few minutes another thing I recommend is
use make files imagemagick
to just automate certain things so for
example R will produce PDFs and then you
can use make to automatically convert
the CSV runs ER or the tab separated
values make it run co-op program I'll
put PDS and then use image magic to get
PNG images for the web part of it and
then there's a tool called Skitch for
always ten that you can use to make
annotations so so why do you need to do
this I'll show you a practical example
of what I found so I showed you this
benchmark before and actually not only
this benchmark but all the benchmarks I
showed you so far we're kind of
because I give you little crafts that
show this thing is faster than listening
and they don't analyze anything and this
is what's wrong with most of the
benchmarks you see out there they don't
give a very deep so when you go very
deep you sometimes sometimes it doesn't
matter but sometimes you find
interesting things and I did with this
one this is what I found so this is a
jitter plot similar to the bar graph but
it shows me each data points that I
recorded and see x-axis doesn't matter
just randomly distributed for their
library I'm currently testing and what I
can see is the what is going on why
do I have two data islands here it makes
no sense it seems like sometimes my code
is really fast and sometimes it's really
slow I was really confused when I saw
this and I questioned my code for a long
time so but here's the value of having
your data separated from your analysis
you can dig deeper so when you now plot
this same performance data points over
the time axis it's not actually time
it's a iteration number for the
benchmark you'll see that both libraries
make a huge performance drop at the same
point in time or the same iteration and
then they stay at terrible performance
there's something happens there again if
you have more data you can look what
what happened else at the same moment so
the next thing I found was here's the
heap usage for off v8 or the heap total
so that's a total amount of memory v8
kids for putting my objects in and at
the point where my performance goes to
 basically the eight decided I have
20 megabytes of heap but I would like I
don't know 35 and then my performance
just goes terrible and I find all the
benchmarks that Ruth is pretty well or
another craft this is a heap used so you
can see v8 is actually doing garbage
collection it doesn't look like I'm
leaking anything it just said v8 decided
hey this is where I should get a bigger
heap and it it seems like the wrong
decision could still be a problem in my
code I don't know but these are the
things you find when you do data
collection well and you analyze it well
so ya collect data analyze it find
problems tweak C code
repeat that process it takes a ton
of time there's no like easy fixes but
that's the process that helped me do
these my sequel things thank you
I think we can do questions I'm open
sourcing this right now so now it's open
source cool questions okay hey I just
remember some times where we had to like
doing this kind of performance
improvements for other browsers and we
did really crazy stuff to get it faster
and then it turns out that other like
mechanics behind was optimized by p8 or
other process and so it turns out that
the original idea was very faster than
the tweak we did Oh
how do you handle that okay you're the
history of all your results and see okay
I have to rework this for the new v8
yeah so that's a good point so sometimes
you do crazy optimizations for example
my evil sing falls in the crazy
optimization category and could be that
the v8 guys figure out how to optimize
this case and they produce a result
that's much better than what I'm doing
was evil and then my evil optimization
actually turns into a non optimal code
again hard work do it again look at the
data it so far I was really lucky I mean
I was bitching about some virtual
machines being matching and all but
they're getting really really fast so I
get huge performance increases from
notes here that 4206
it was insane and 0.6 to 0.8 was also
with huge improvements new v8 versions
have helped me but they could also hurt
me if I'm doing optimizations that are
really stupid but generally speaking
evil is the only crazy thing I did for
most part my code actually turned out to
be simpler and more modular and not
worse so I'm not I'm not going crazy to
just please a VM if I can avoid it
the evil thing is just fun ok that's it
thank you very much Felix
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>