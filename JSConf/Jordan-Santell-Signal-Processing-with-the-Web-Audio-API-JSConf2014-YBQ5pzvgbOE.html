<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Jordan Santell: Signal Processing with the Web Audio API [JSConf2014] | Coder Coacher - Coaching Coders</title><meta content="Jordan Santell: Signal Processing with the Web Audio API [JSConf2014] - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Jordan Santell: Signal Processing with the Web Audio API [JSConf2014]</b></h2><h5 class="post__date">2014-07-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YBQ5pzvgbOE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is single processing with live
audio
I'm Jordan Santo and so a quick
introduction the Web Audio it's a
relatively new API all major browsers
support this except ie but they just
announced this week that they will
support this in the next version which
is awesome so this Audio API is a
modular routing API use for manipulating
audio it's using games synthesizers and
audio production tools so what is the
signal processing component of this so
signal processing is the theory of
manipulating and analyzing a signal in
the case of Web Audio the signal is an
audio buffer so just a giant array of
data so quick about me I love this gif
so I work for Mozilla on the SDK team
for the SDK for add-ons and developer
tools and also the Web Audio developer
tools so the audio API it is a route
based audio API for processing audio
signals as constructed as a directed
acyclic graph so the audio flows from
source nodes to the destination node and
along the way it gets either transformed
or analyzed and manipulated and all the
audio nodes are created within a sandbox
called the audio context all nodes must
live within a context and the context
decides things like what's the sample
rate of this whole graph
so it can be a little confusing because
it's unlike most other new web api's so
therefore no types these aren't official
these are just ways to classify them to
better understand the API so there are
source nodes and so source nodes emit
sound for the origin of some kind of
signal we can make a source nodes out of
things like the WebRTC audio stream HTML
audio element oscillator and a last one
something else and so this is where the
origin of sound so the sound travels
through the directed graph i'm it could
possibly either transformer analyzed so
transformation audio node would be
something like delay filter compression
something they'll transform the audio
signal to so it sounds differently an
animal ization node it doesn't affect
the actual signal it just lets us better
interpret it and get information about
it so we'd do that for something like an
audio visualization and finally we have
a destination node and anything that we
want to hear must ultimately arrive at
the destination so in this example we
have three sources and they could be
anything really and it goes through some
filtering distortion panning reverb and
ultimately a compressor and the
destination node in the Web Audio API is
a abstraction for our sound cards so if
we want if we have a source node and we
want to hear it
whether it's a mp3 file or whatever it
only has to flow into the the
destination node so we can actually hear
it so if this still doesn't make sense
here's a Viking metal band so you go to
a concert the guitarist is setting up
and the new thing of the guitar is a
source node this is where the sound
originates the signal travels from the
guitar through the cable into a this is
a Viking metal band so they have a lot
of distortion so they to transform the
clean guitar signal it goes into a
distortion pedal makes it heavy much
louder you know things like that it
transforms the signal and then
ultimately from the
you distortion pedal go into an
amplifier so we can actually perceive it
and you know it'll be a good concert
another way to think about this flow
this audio context flow is similar to
middleware in routing so like expressjs
middleware you can think of a request as
a source so when someone requests some
file from a server that's the origin of
this process and then ultimately it
could go through several pieces of
middleware when this middleware can
transform or analyze the requests
throughout so if you want to you know
have a middleware check make sure the
users logged in or something for some
resource then the middleware would
analyze that and determine whether it
should proceed and ultimately you get a
response which is the final destination
for that so a really simple example
first we create the audio context again
all nodes and everything must live
within audio context and you have to
have the context to create these nodes
as well so we instantiate an audio
context and create an oscillator and a
gain node to use the node types I
explained before an oscillator is a
source node so it creates a for example
a sine wave so that's where the sound
originates and I can travel through a
gain node and gain is a fancy word for
how loud something is so it originally
it's the signal originates from an
oscillator travels through a gain node
since we connected our oscillator to our
gain node and then the gain node
connected to the destination so we can
actually hear it and then we have to
start the oscillator so here's an
example make sure we're quiet so it's a
really annoying sound so here's an
example of the audio tools in Firefox
which just draws the contacts graph so
we can see what node is connected to
which node we can also edit things so in
the last slide we solved an oscillator
node that's connected to the gain node
which is connected to the destination of
the far right the other two notes the
analyzer and script process are just for
the demos
to display the time domain data so it
that's obviously a sine wave very fast
so oscillators create these tones with
periodic cycles so assignment and their
prey by some kind of equation so a sine
wave is created by the mathematical sine
function and all these waves are
periodic waves so they're repeated
patterns so let me make sure there's
other type of oscillator than sine wave
we can have a triangle and it's named
that way because it looks like a
triangle square wave is that too loud
excellent and my favorite you can see
what how these waves are named because
they actually look like the object that
they were named after so if those
familiar with electronica music or
synthesizers or you know if you ever
played a game boy a lot of these sounds
are the foundation of any synthesizer
synthesizers use these basic waveforms
to construct more abstract and complex
shapes and we'll do a little bit that in
a bit so we can change the type so right
now it's a sawtooth and we also changed
the frequency so a frequency is measured
in hertz and it's a unit that represents
how many times per second in this
periodic wave cycles so at 440 Hertz
this shape is repeated forty times per
second and that's what we perceive as
pitch so 440 is uh you know concert
eight for let's so if I drop this in
octave to 220 we can see we actually
perceive a lower pitch and the it's a
much you can see the cycles are a little
bit slower it's still 220 times a second
but if we drop this to something
inaudible like below 20 Hertz 5 let's
say
we can actually perceive that because
we're not robots right yeah we're
scaring all the animals right now so I
mean that's a really terrible sound no
one wants to listen to pure sine waves
all the dogs are angry right now so we
can manipulate these signals in a bunch
of different ways so one technique is
amplitude modulation in which we want to
chain an amplitude is the so how high
the signal is and which we perceive as
volume so if we want a really loud sine
wave we would have really large cycles
one quiet
sine wave would be a really subtle wave
so what we can do is and the previous
example we connected one audio node to
another audio node but we can also
connect audio nodes which again is just
this pure signal to an audio parameter
so in amplitude modulation so we have a
carrier signal which this is our pitch
same thing we did in the last one so
let's assume this is a 440 sine wave we
also have a modulating sine wave signal
that affects some other property in the
audio context so it's modulating sine
wave signal is much lower frequency than
the carrier signal so we use this
modulating sine wave signal to between
between a loud sound and a soft sound so
if we combine these two signals or have
the modulating wave signal affect the
subsequent gain we can see in the last
image as the modulating sine wave signal
approaches negative one the carrier
signal gets much quieter so implement
something like this similar to the
previous one we have a an oscillator or
carrier signal connected to our gain
node RJ nodes connected to our contacts
destination just like we had in the last
example we also have another oscillator
our modulator that's connected to the
gain nodes gain Auto audio parameter so
as the modulator goes between 1 and
negative 1 it'll
the the volume of our signal so it's a
little bit more interesting so it'll
sound something like like a-- i'm like a
lot of weird sounds throughout this so
it'd be like well well well because
we're pulsating the volume so so so in
this in this graph we have our
oscillator again gain node connected to
our destination node but this oscillator
down here this is our modulating signal
so right now it's only a frequency of
five so this is only repeating five
times per second so this means it's
going the carrier signal is going to go
between full gain and no gain five times
per second so if we change the
modulating signal we can change how
often this pattern is repeated so so
changes to one so only once per second
is the scheme changing and we can see it
actually being visualized as the volume
decreases there's no activity and the
time domain data we do this with other
waves over on the sine wave so we got a
sawtooth wave as our carrier signal it
sounds like a broken Gameboy so yes so
we can use amplitude modulation to make
things a little bit more interesting
because no one wants to hear pure sine
tones another thing we do is a frequency
modulation is where instead where
instead of modifying the amplitude or
the volume we're modifying the actual
pitch or frequency of the carrier signal
so same idea as amplitude modulation so
our modulating sine wave signal is
changing the frequency or how often the
cycle of the carrier signals repeating
so we see in this example as the
modulating sine wave approaches negative
one the frequency of the carrier signal
increases so the distance between each
cycle is lessened which we perceive as
pitch
so if we're going between a lower pitch
and a higher pitch in a synthesizer
we'll get some like weird 80s sci-fi
like and we're gonna do that so I think
I did it better but you sounded pretty
good so this is pretty much the same
example except where I'm gonna code real
quick so the same thing except instead
of connecting our modulating oscillator
into our gain nodes audio parameter
we're connecting it to the carrier's
frequency so as the modulating signal
goes up and down so does the carrier's
frequency as we've seen the sweet
example of an album just of that and so
the only difference thing here is that
our modulating oscillator is also going
into a gain node just to increase the
change so if we change the modulating
oscillators gain node to one it's not
that much of a difference does not
change the frequency too much if we
crank it up to mmm 1000 we get some
weird stuff going on and we see like
again this is modulating the frequency
so we see the sine wave the carrier
signal crushing together when it's
higher if you can see in expanding when
it's a lower frequency so we see this
kind of like breathing effect so again
you're not gonna listen to you know
that's the doesn't sound good but again
this is a foundation of you know any
kind of like electronic music synthesis
and so notice that we're also using
really low frequencies for the
modulating signal this is 2 Hertz humans
can hear between 20-ish Hertz and like
20,000 Hertz so we can't really perceive
this this signal even though because
it's affecting it so if we crease the
the modulating signal to something that
we can perceive things get weird so
let's make this so the killer sting goes
for 40 and if we make the modulating
signal
so that's like a bass note on a bass
guitar so we get this weird modulation
effect so like if you want to pretend
you're a Dalek you'd do something like
this and so the reason why we use low
frequencies for a modulating is so we
can't perceive we just maybe we just
want a subtle effect like a use often
like tremolo phasing vibrato we don't
want to perceive that so this is called
LFO not the 90's boy band but a
low-frequency oscillator so we don't
perceive we just we have some kind of
pattern so we can like repeat amplitude
modulation or frequency modulation or
some other effect so and we can also
modulate other audio parameters not just
frequency and amplitude so we can plug
this in we can pipe an oscillator into
filter a delay node anything that we
want so we can get more interesting
things otherwise we just sound like you
know game boys so that sounds really
cool in here so for this example so
again so much of the other ones except
right now we have two oscillators two
carrier signals one at a 3/3 a sawtooth
and a square wave at 80 yards and those
are octaves apart so it doesn't sound
awful going into a filter node and a
filter you know think of like an array
filter function filtering is kind of
pretty much equalizing so right now this
filter node a bi quad filter is a
low-pass type so it has a frequency
value and right now it's a 500 so this
means that any frequency higher than 500
is significantly reduced so we only
really hear the notes below 500 and the
modulator
never performing here we have again an
oscillator node one one hurt one hurts
so this is the modulation will happen
once per second and that's being pumped
into the by quad filters of frequency
property so what is being you know
animated or tweened or modulated is the
low-pass filter frequency so as the sine
wave continues we're going between only
allowing just the low frequencies which
humans have a hard time perceiving so it
sounds quieter than higher frequencies
into allowing more and more frequencies
and then oscillating back so that's what
we hear here so it's just going cycling
over letting lower frequencies only
lower frequencies and letting more and
more frequencies in and then going back
so sorry so in this example same thing
slightly different properties for the
oscillator node except we also have two
modulating signals at this point one is
controlling the filter node just as in
the last one so 0.5 Hertz so this is
happening once every two seconds and
again no to increase it and then we have
an oscillator no that's controlling the
game so we're also doing the filter
modulation as well as amplitude
modulation
okay so so and we can also so again this
isn't you know this is just like a
simple example of some you know
synthesis that can be performed and we
can hook this up to a you know a touch
interface like a keyboard or something
and trigger whether or not these notes
stop and start and deal with sustain if
you a user holds in a node and things
like that
and we can also just automate everything
and just compose a song that requires no
human input so we can do things like so
and this again we're just dealing with
I'm a composer so we're just dealing
with we just started with two oscillator
nodes once a sawtooth one's a square
wave and so we came from this like
Gameboy sound into you know something
that could possibly be in a a song or an
album or a concert because we just made
dubstep so thanks everyone
so I'm Jason tow and github and Twitter
bug me if you have any questions about
Web Audio stuff or the Firefox dev tools
for audio which coming out in Firefox 32
and still have no idea what we're doing
with it completely but any feedback
would be awesome so we can make it
better so we have more you know game
developers using Web Audio API
synthesizer makers and you know audio
production engineers so thanks s</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>