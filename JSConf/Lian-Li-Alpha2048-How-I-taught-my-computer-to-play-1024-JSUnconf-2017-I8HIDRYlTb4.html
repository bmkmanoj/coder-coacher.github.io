<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lian Li: Alpha2048 - How I taught my computer to play 1024 | JSUnconf 2017 | Coder Coacher - Coaching Coders</title><meta content="Lian Li: Alpha2048 - How I taught my computer to play 1024 | JSUnconf 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lian Li: Alpha2048 - How I taught my computer to play 1024 | JSUnconf 2017</b></h2><h5 class="post__date">2017-06-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/I8HIDRYlTb4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm if you've seen me last year I'm
the machine learning person and so like
I said in my introduction and this is
going to be about this is a use it a
case study about how I went about to
ride a system that will learn stuff
that's the short the the float
explanation I would like to walk around
more but apparently I have to stay close
to the microphone so this is me and this
was the good old days there was last
year at the json conf you can follow me
on twitter on github those other the
platforms that i actually look at if you
want to catch up so during my day time
basically I'm a software developer at
dot and squander and we're startup doing
a bit of travel screw a bit of Titan
machine learning and we have an app so
if that is that interesting for you you
can talk to me about that too and but by
night I'm a machine learning enthusiast
so this is my hobby basically all right
um let's talk about the game that we
haven't googled yet even though I asked
you to this is crease that's the
original game basically and as you can
see basically the idea is to merge one
and two or multiples of three and as
soon as you merge them you get a higher
value get the sum of those values that
you've merged and I don't know what the
goal is actually is I guess it's to play
as long as you can and basically you
have two inputs left right up down and
then every time you swipe left you will
swap the entire board to the left that's
the basic idea
so threes came out and then suddenly for
some reason I think someone implemented
the simpler version of it of open source
and then suddenly like all kinds of
clones came up the originals 2048 and
there are all kinds of 2048 likes games
I think the most popular was 1024 or
1024 but there are also like like this
one
which is pretty hard to know which one
you have to merge and what it's going to
be afterwards but I guess it works so
but I'm going to be talking about the
simplest implementation that's 2048
that's just the basic you can check it
out on github and it's free and runs in
the browser yeah the I think the most
the hardest thing actually
while working on it was not to get
distracted by the game also often
because you're gonna just like I'm going
to try this out and suddenly like 10
minutes later you realize I've been
playing this game for 10 minutes now I
should really get to work all right
um so let's look at the rules click
overview just so we are all on the same
page we have a four by four grid I've
all seen that and each term basically
you take an action which I call alpha
here and alpha is zero one two or three
so it's one of of this set of numbers
and so you take this action and after
you've moved the new tire a new toy will
edit to an empty space and there is a
probability distribution so there's a
90% chance that the new tile is going to
be 2 and there's a 10% probability that
the new tile is gonna before so every
time you move you get a new tile if you
merge two tiles with the same value then
you will get one new tile with the value
of the sum of those two tiles I think
you you've seen that I just written it
up once more so remembered all right and
then so basically there are three
measurements and how how well you're
doing you lose when there are no more
moves possible because they were borders
full and you can move anywhere to merge
anything and you win when you have one
tile with 2048 present or let 1024 in
the other game or like the rainbow dog
with the dog game and then there's
additional a thing that's called the
score and the score is basically every
time you merge two tiles then the sum of
those two tiles the added to the score
so basically every time you merge
something
score gets updated at gets higher so
these are the basic set of rules and so
I basically just started as thinking
okay first thing the most important
thing is the name after 2048 of course
it's the pun it's a reference to alphago
they would stick the most important
thing and once I got the name I was like
okay mo
I'm all set now the second thing was
that it has to be JavaScript because I
really like JavaScript it's easy to
setup it just runs you can run it in the
browser you can run with notes so it was
just just to simplify things basically
and then the other thing was that it
needs to be supervised and so basically
supervised learning means that I'm
teaching my machine something where I
already know the correct answer I
already know what I want my machine to
predict for me so I'm gonna I'm telling
my machine ok if you get this set of
inputs I'm expecting this output the
other thing I'm supervised learning you
just tell the machine do you see a
pattern here but I don't know the
pattern so supervised because we know
when we get to 2048 we be win all right
so I was I started with a library those
of you who see me give the talk last
year you already know it it's called
synaptic and blue because it's the link
and kinetic is a neuro Network library
for JavaScript oh yeah so I don't know
neuro networks I think you've heard of
it I'm not going to explain it it's just
it's doing machine learning magic that's
why I don't know
and so the what synaptic does is does
classification which means it classifies
words in the end you get either either 0
or 1 for whatever you want to predict so
you can't predict numbers per se that
would be called regression so you can't
predict like if I make this move what
will my score be so it has to be
something the thing you want to predict
has to be something between 0 or 1 or
true or false respectively so yeah and
then of course I have prior experience
with synaptic that's also linked to the
product I showed last year it was
a soccer result predictor thing so that
was basically the reasons why child with
synaptic I already knew everything and
then yeah I had a pretty good idea what
I wanted to so um yeah first iteration
so first I'm basically trying to sign
out what is it that I want to do
actually what is it that I want to teach
my machine or what what is the first
step I need to do right now
so the first step I thought for gather
data we I need some kind of data and the
idea was that basically you I try to
predict the next the best possible move
for every turn basically so the way I
went to do that was to simulate all
actions so if I have a board say let's
say that's the first day that I've and
starting to play the game then I'm going
to simulate all possible actions so four
of them and then afterwards I'm going to
check dipper score increase so basically
when I swipe left with the score
increase when I swipe up to the score
increase blah blah blah and then after I
did that I actually just made a random
move for the game so first I simulate
and then I just make a random move I
could have of course just checked where
does the score increase and make that
move but I wanted to start random
because I wanted to see how well my
algorithm basically performed against
this randomly moving and you would be
surprised if it will random move you
will get to like 256 every tenth time so
like you can just play it randomly don't
look at it it's pretty good so um yeah
this is basically what my data set
looked like board state was my input
that's a serialized state of the board
so I just have like I just I count from
left upper left to down right basically
and then it's zero to zero I don't know
one and then just goes on as this is a
16 different element of course and the
actions those are basically bulls that
say if the score has increased after
making this value so basically I'm
saying I have this board state right now
and for each action I took this is what
happened to the score zero is the score
didn't increase one means that a score
increased so I guess that's pretty clear
what I mean yes okay all right so I
played I like I know I similar they're
like 100 games or so and then I was like
okay I have I have data now I think
that's going to going to give me like a
good indicator whether my idea is
working on so second iteration this time
the goal was to compare the trend
network with the unfriend or with a
random decision maker and so of course
the first thing I need to do is train a
neural network and basically the way you
go about training a neural network is
just give it the input give it the
output and then let it run a lot of time
so it will slowly hopefully recognize
the pattern behind the stuff you want to
predict and then so after it
strengthened your network I will use
that network to predict whether the
score increases are not for each of the
possible actions and then in the game
itself I'm going to move make the move
where the probability that the score
will actually increase is the highest
that's pretty straight forward so I'm
not gonna I'm not going to show you any
data on this because it was pretty
boring and wasn't like there's nothing
to see I just save them into couch and I
came up with some several metrics but
the conclusion basically to make it
short was that it didn't work all too
great I think it was a little bit better
than doing it randomly but there was no
I see that the game is actually being
played in a in a meaningful well I guys
liked my I don't think the algorithm
actually understood what this game was
about and I think the reason why I was
like that is that it it focused only on
the next state so it was basically only
predicting whether
we'll increase the score for the next
state but it didn't have like the
foresight to check whether this will be
good for the for the whole game so what
I needed was some kind of machine
learning method that had had more
foresight that would be able to see that
it's not only one move but it's like a
series of moves and then at the end
that's the most important state
basically did you win or did you lose
all right so I looked around a bit
really I just had to ask myself do I
want to keep using synaptic and the
specification network and try to come up
with better better inputs and stuff like
that or do I want to scrap it and do
something completely different and
thankfully my husband who wanted to do
the ghost day I think button he didn't
and he helped me a little bit with it
and then he introduced me basically to
nice methods that were perfect for my
use case and basically it's about
reinforcement learning reinforcement
learning as the name already says is
basically that you instead of giving
your system a value at the end you will
just tell it this is slightly better or
this is slightly worse so it's not like
like a full like a 1 or a 0 it's just
like oh this is pretty good or this is
not as good the reinforcement learning
makes use of like two or three different
strategies basically and the first one
is called Markov decision process and I
don't know maybe you've heard of the
Markov chain the Markov chain is
basically it is a defined chain of
states and actions so you have a state
and that state has one possible action
then it will lead you to another state
and that's it will have another possible
action it's a stochastic model so
there's like the one action that it's
going to take is like the one that is
probably probable if the higher has the
highest probability of being taken but
this is one defined chain and there's no
decision-making in it because the chain
has already existed from
from the beginning and the Markov
decision process is basically an
enhancement of that chain so it if you
take if we take a decision process you
have the rewards and the end and you
have possible directions and the
decision process basically decides which
action is going to take or if you say
the other way around if you take a mark
of the certain process and only give the
reward 0 and if you only have one action
then it's the same as a Markov chain
that's basically the the thing then the
other nice thing for reinforcement
learning is that it can deal with the
delayed reward so you can have a lot of
states and actions and then give a
reward at the end of the of the chain
basically that's nice because then you
can tell your system that this is like a
chain and then in the end the end of
that thing that actually matters so this
is actually perfect for games or for my
case and so the one thing that is
basically the the challenge of
reinforcement learning is exploration
versus exploitation and what that means
is that so on the one hand all the
experience that your machine gets
especially from the machine playing so
it plays and while it plays it learns so
you want to have a lot of different
kinds of datasets so it will learn new
stuff we learn different stuff learn
different ways to go but on the other
hand you also want your machine to do
the best possible moves that are knows
of so you got to balance those things
like just don't be too random but also
don't only stick to the stuff you
already know all right and then there's
some another machine learning method
that's more it's like more concrete and
also something that is basically perfect
for my use case it's called temporal
difference learning and the thing about
temporal distance learning is basically
that is it knows that there is a state
in the end and there are states and
actions that precede this end
and it will slowly learn how those
states before can influence the state in
the end and also get that maybe he liked
the the begins there's a pretty
important which moves you make but the
the the ones to what the end are not as
important anymore so those are things
that the temporal difference learning
deals with and there's also two
basically two strategies you need to
understand the first one is dynamic
programming so utilize dynamic
programming which basically just means
dividing your big problem into
subproblems so you have like this whole
chain and you're dividing it into
smaller units of problems to solve and
the other thing that it makes use of is
called the Monte Carlo method the Monte
Carlo method is is a heuristic search
tree more or less you have like a tree
where you say okay from this stage I
have four different actions and this
will lead me to this state and so on and
so on and at the end of the tree you
have values which are our rewards so
every time we tell our algorithm to
predict something it will look into the
tree and look basically do I have a
reward for this exact scenario if not
I'm going to choose a scenario that is
close to it and has give me a high value
so basically what temporal disinterring
does is it combines a heuristic like
search algorithm with machine learning
because it does if it doesn't find the
exact reward it will approximate it by
the other data it already has and it's
also ha alphago did a lot of stuff all
right
and so I googled like any good developer
would do and what I found was a library
called reinforced days yes there is
actually a reinforcement library for
JavaScript which is pretty awesome
because it also runs in the browser and
reinforced us was written by a guy I
forget his first name but his last name
at Kapaa team and he based it on a paper
on a scientific
for where people solve at hari games
with reinforcement learning a temporal
difference L&amp;amp;Q learning and that's
pretty cool so I was like yeah okay this
is perfect and it's also the only
reinforcement library for JavaScript I
didn't really have a choice
alright so the code then I'm going to
show you is that's basically the code so
you have the state but before it's a
civilized state then you ask the agent
the agent is from the library you ask it
to act on this state the agent will
return an action this action is zero one
two or three and then you do the action
it's like this is hidden in the
calculate what they expected you you
take the action and then you will give
it a reward and say learn from this
reward of course like I said this is one
state one action you can do a lot of
actions and then give a reward in the
end like i did i get it i gave my system
a reward at the end of the alpha game so
basically when I use this everything
that I have to do it boils down to what
reward should I get like the whole model
stuff is basically all in this reward
that's the only thing I need to take
care of so okay I'm going to work in
iteration because that's the easiest for
me so first iteration I decided that do
the simplest thing just give it the
score as a reward so I don't need to
implement anything myself and the score
like I said it's better the more you
merge so that's a pretty good indicator
all right um yes this is my graph honor
and as you can see there's really not
that much happening if we check the
average it's really not there's like
yeah it's higher here and average but
then it drops down a lot so there's not
really like learning intelligence that
that I could actually see there so like
okay I tried it doesn't work that great
let's just try to find another
reinforcement value or reward value
to see if there's like something better
okay um so second iteration was
basically the the task of finding a
better value because I was I believed in
the library I thought it was the correct
library just my reward wasn't that great
so I came up with the string tile rating
and the tile rating was the sum of all
values divided by the tile count and of
course everywhere where there's no tile
in my serialize area is zero but there's
actually no tile and those of course
don't count otherwise you always have to
divide by 16 so what's different between
the rating and the score if you just
doesn't I'm sorry okay okay well this
one working very well so you didn't see
any of this of course you don't know so
when we start let's say this is our
starting situation and then this is our
board we have a score of zero because we
just started the game the rating is two
point five because we have some the some
of the ties is 10 and you divided by 4
that's 2 point 5 when we now make a move
to the left I'd say this is the most of
that the two upper the twos have merged
into a 4 and the 4 that I that has a
green frame is the new 4 so this is a
tie that's what was added after the
action was taken to the single time what
we have now is we have a score of 4
because we merged those twos and we have
a rating of 3.5 so okay rating goes up
when you merge something and it also
goes up when when you ties are in it now
if you for example instead move down
what you have now is that the tools were
not merged they're like the truth
the bottom row and the fourth new of
course and now this time we have a score
of zero and the ratings two point eight
so the rating has changed the score
hasn't changed so that was basically my
idea was well it's a little bit
different and so maybe that might be
better for some reasons I wasn't
entirely sure I just want to try
something else also it's thinking if you
don't win because in the end it's about
do you win the game until you lose the
game and if like all the games you play
are basically all losers then there's
really no point in giving the reward
back because always the same reward as
thinking the next best thing is the
highest sum of tires but with the lowest
number of tires that's basically if you
just look at one single stage one that
has like a higher value but lowers low
lower number of tires would be better
than a board with a lot of numbers that
they're all like two or four okay so
this was the idea behind the rating I
tried it it wasn't that great it kind of
looked like the score before so I tried
other things and I came up with this
where the reward was basically the score
divided by the ever score times the
rating divided by the average rating and
the ever which was taken from the last
hundred game so it was not the complete
average it was just the last hundred two
if there is a tendency so that we can
use it all right and so then what I was
seeing there was basically that okay the
yellow line always looks like it doesn't
move but that's because the values are
pretty high up there it goes of five
point five almost so um but if we look
at the average okay I was shaking a
little bit because I increase the time
so it's like one value per hour but that
was just so I could see does that is
there anything happening and it looks
like actually there is some intelligence
that like the score does go up over time
which could be an indicator that we're
getting better but I'm not pretty sure
because as we've seen before
sometimes there is like a really high
peak but then it like drops down
immensely so I would have to run it even
longer I used to run during the night I
thought it would be an ass but I'm not
entirely sure now so gotta try for a
longer time maybe train for a few days
but there was something where I thought
okay this reward seems to be working
better than just the score so maybe I
leave the reward for now and then try
something else and then see how that
works and so I did a third iteration
okay okay I did a third iteration where
I played a lot around a little bit with
the meta parameters of my system and
what I did was the K the epsilon all the
time and the epsilon is the value for
the exploration value so if you have
high and high epsilon you do a lot of
random moves if you have a low epsilon
you don't you just do the moves that
you're comfortable with that you already
know so if you decayed over time it
means at the beginning you do a lot of
random moves but the more games you play
the more you stick to the ones that you
already know and I thought that this
might work pretty well maybe because
then I don't go all over the place I
already have the stuff that I know and
then just try to go that path also it
was recommended in documentation of the
framework okay um so how did that look
let's take a look here it looks more
jagged right as most thing has more
stuff happening so so yeah um I played
fine so maybe like it looks good right
yeah it's perfect I want so um yeah it
looks pretty nice but so maybe you
didn't notice it when you check those
the total scores you see that maybe
there is a tendency moving up but
overall the overall average is a bit
lower than if you just returned the
score as reward so there is learning but
it's starting from a worse point that if
you would just give it the score which
might be okay because over time you will
reach like the best
possible point but maybe you will never
reach it but I'm pretty confident that
this was a that was a pretty good run so
this is how far I've got me basically
yeah I didn't I didn't really need one
anything but I I learned something I
looked into it and I learned something
and I have came to a conclusion so what
I saw is that it seemed to be learning
in some way it seemed that there was
like a tendency of the score going up
over time but we have a few problems and
I think I'm not going to be able to fix
it with this particular framework with
this particular system the one thing is
that the board size is kept is there's
16 tile spaces and after a time it just
becomes very crowded on the board
because you always have to have you have
to have like a 512 I mean I have a 256
next to it and you need another 256 to
merge those two merge school so you need
a lot of tires laying around and it's
get really it gets really crowded and it
gets harder the higher you get and it
could be that it's just at some point
the system just does not I just can't
handle it anymore because it's just it
just doesn't know where where to put all
the tires basically and the other
problem is that my input is the sled so
it's a it's a one-dimensional array but
the board is not one dimension of two
dimensional but for this library there's
two snow there was no way to input
multi-dimensional inputs so I had to go
with it and I could imagine that my
system just didn't get that number like
element number zero and element number
four are like they can merge them when
you go up or down so this is something
that the system would have to learn in
implicitly and I'm not sure whether it
got there actually and so the other
thing about reinforced is that it has
one heart one hidden layer hard-coded
and I looked into the library and I was
thinking about like changing it but
like stuff broken once again I'm not
going to do that I'm just going to go
with what I have and just yeah hold on
tight basically but I heard the guy is
going to be and in Paris I think when
I'm going to be there too and then maybe
I can meet up with him and we can like
have program it or something it would be
cool right so what's next what will I do
next
when I'm got to get around to it I can
play around a little with the meta
parameters there are lots of meta
parameters that you have to give your
system those are the parameters so
epsilon I already explained that right
the low epsilon means not as random high
as long means very random there's this
gamma thing that's also some kind of
greedy so if you're if you have low
gamma and that means that you try to get
the best possible reward instantly and
if you have a higher gamma you'll be
fine with doing a lot of stuff to get a
higher reward in the end so that's
something that you can play around with
there's a learning rate which you can
just try an error and try to find the
best thing
there's experience that you can make it
bigger whatever there's all kinds of
parameters and you can if you want to
get more into it you can check out
reinforced yes and they're explained in
a in a nice mathematical way which I
didn't include because it's kind of
complicated
and what else so I was thinking to
switch to Python Python has a lot of
nice machine learning frameworks and I
would be able to just do more to have
introduced more layers to try around a
little bit so that could also be
possible but then I couldn't run in the
browser so I haven't really decided on
that yet but one thing I could do is
something called convolutional net work
and the convolutional network is
basically a way for me to to tackle the
sled input problem because in a
convolutional network you could like
have four different nodes and each node
gets like one row or we have eight
different notes and then you have four
four rows and four four columns and they
will give all their values that they
calculated and give it to the net
network so that you can actually have
those kinds of relationships in the
horizontal relationships and vertical
relationships and the system book will
learn that yeah that's basically it so
that's basically what I what I did and
there are some links if you're
interested so the first one is open
source 2048 that's the game then there's
this unlife I didn't notice
so this is a paper where someone already
tried to do debrie enforcement learning
with Python will for the 2048 game I
think he just rewarded wins with won and
lost was with zero and said it didn't
work so well so you can look into it
what he did and it's pretty interesting
actually and then there's this
introduction to Temple distance learning
that's that's more of a scientific paper
but it's pretty it's still you can still
understand it and just for if I didn't
explain anything to well then maybe you
can look it up there and then yeah I can
presentation review address great
so um live demo so I will show you stuff
that I so I will let it run in the
backend I'm not gonna promise anything I
don't think it will do anything actually
but it's very hypnotic and it's it's
nice so I don't know it won't start
ok so if you have any questions now's
the time Thanks
I know yeah it's really hard to actually
that's like I said it's the hardest
thing and then you you like you start
the bot and like prepare for bed and
then you sit in front of it and just
like oh yeah move it to the left
yes came after to the left does anyone
have any questions well yes
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>