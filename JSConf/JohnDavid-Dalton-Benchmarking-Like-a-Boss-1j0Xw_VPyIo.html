<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>John-David Dalton: Benchmarking Like a Boss | Coder Coacher - Coaching Coders</title><meta content="John-David Dalton: Benchmarking Like a Boss - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>John-David Dalton: Benchmarking Like a Boss</b></h2><h5 class="post__date">2013-01-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1j0Xw_VPyIo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is John David Dalton and I'm
doing a presentation over benchmarking
and so I'll get into that all right I
wanted to do a talk about benchmarking
because right after jay is comfy you I
got into trying to benchmark my own
framework and I noticed I noticed there
was a lack of good quality JavaScript
benchmarks for framework code or for
just snippets in general and so I
started looking at task speed slick
speed and the the popular benchmarks
like v8 sweet son spider and Kraken and
looking to see you know how they
measured JavaScript performance and what
I found along the way was that there's a
lot of inconsistencies and a lot of
problems with measuring JavaScript
performance and getting a good idea of
the the actual performance so let's get
into it the the main ones i looked at
where sunspider cracking and v8 I also
looked at Dromio which is I'll cover in
the next one these benchmarks are
running synthetic bench synthetic speed
tests so they're they're doing tasks
like looking at like a 3d matrix or
doing some string manipulation and
they're they're saying that that is
representative of what applications
might be doing and will will gauge the
speed of your your browsers in this case
but
Dromio yo on the other hand is is a
different one and it measures not only
the same benchmarks of sunspider and and
v8 sweet but also has dom tests dom
tests are important because for example
ie8 or ie9 performs very well in some of
the synthetic benchmarks like SunSpider
but is is one of the worst performance
wise on dom manipulation and so having
relevant meaningful benchmarks
accessible to those guys the developers
for IE would it would have helped them
maybe pinpoint to narrow down more
meaningful benchmarks in their own
browser tests so i'm a co-developer on
Jas perf calm how many people have used
Jas perf awesome great so I make the
thing that makes this work which is a
benchmark framework called benchmark Jas
so i'll write their benchmark j/s calm
what I as I was looking at those other
solutions I took it apart rewrote the
the underlying code from Jas perf from
scratch and I wanted to make like the
most thorough benchmarking framework for
JavaScript and this runs and almost
anything it should support Safari 2.00
to IE 10 preview to node norwall ringo
rhino all of your bases should be
covered so let's get to this there's
also a small project i started working
on called x stats j/s which is j s /
tests a little snippets of JavaScript
and usually it's you start the timer you
do the snippet then you you in the timer
but this is more of a passive
benchmarking tool this allows you to to
monitor the frames per second the memory
usage and certain browsers that support
monitoring the memory
and and a little millisecond timer that
too and this is a rewrite of something
that mr. du bid but he used a Candace
element minds using just raw HTML to
like their Li Oh is there to dren der
that so it's taking away less from the
actual application or trying to
benchmark in mobile devices especially
if you've got canvas on the page you're
eating away at the performance that
you're actually trying to measure so
that's a passive approach there and
there's there's basically there's a few
patterns when you when you benchmark
things the most common one is you have a
while loop you start a timer you have
let's say you're counting down 100
executions and then you you end your
timer and then you subtract the
difference and get get a millisecond
count and that's done in things like
slick speed test speed sunspider nodes
benchmarks basically it's the most
common and probably the worst kind of
benchmark to do because because if
they're fixed iterations it means as
hardware and as your environments are
getting faster eventually you're going
to get a lot of zero millisecond record
readings and zero milliseconds you can't
do anything with that I mean there's no
way to compare 0 to 0 so that is the
major flaw with with doing it this way
but there's another pattern and this is
the one that Dromio uses and this is
also what v8 sweet uses they basically
run a test for one second repeatedly for
one second and then calculate the
operations per second and do it that way
and that gives you a better read but
it's still not quite there because
they're forcing all browsers to run this
this test for one second and if you're
doing especially small tests that are
your testing something that executes a
lot of times you're going to end up
choking up the browser and law
you know when you're running this and so
what I one of the things I learned
whatever I was developing benchmark Jas
is that you actually don't need to run a
test for one second one second was was
something that they added for IES poor
timer resolution which I'll get into in
another slide but basically it can only
it can only measure 15 about 15
milliseconds for the older I ease and
the newer ones like what four
milliseconds the ie9 and so whenever you
put that into a formula to figure out
the crispy how long you need to run a
test to get a percentage uncertainty of
like one percent it ends up being about
750 milliseconds and so most most of
these things just run for one second but
if you if you apply that to working
browsers normal browsers that have a one
millisecond resolution you bet a you
only really need to run a test for 50
milliseconds and so what what that
allows you to do is that with with tests
like v8 sweet and even SunSpider they
run these tests multiple times and then
they perform statistical analysis on
them and what what this allows you to do
is because I run the test for a smaller
amount of time only 50 milliseconds I'm
able to run more samples of that test
and then that will help me reduce the
margin of error of the end result so
that's that's something that they could
make it actually tweak there so a big
problem I see with or something I've
seen with and these these are actual
URLs you can visit there to see this
this test is context a lot of times
those developers will see the the green
result or the red result and make like
an instant snap decision Wow one is
totally better than the other and I hope
that you guys take some
just step back and go well wait a minute
that's you know that's 538 million per
second I mean that's crazy fast and the
slowest is 12 million operations a
second that's still crazy fast you're
not going to be bad either way if you
use either one of them so just kind of
pay attention to the operations per
second and and not necessarily just
which one is being you're being told is
the fastest so there's another thing
that goes into context and that is
consistency and reproducible being able
to reproduce results consistently in
this case you'll notice it's the exact
same test done three times and the end
result they vary a little but the system
is able to detect that and say hey these
are all statistically indistinguishable
from each other and so it says hey
they're all the fastest and that's
another reason why you don't necessarily
need to get hung up on the actual
operations per second because they'll
bury about five percent from test to
retest and I that's been pretty
consistent even into like mobile
browsing on mobile safari it's and then
on IE i noticed that if i increased it
from three times two let's see two more
than that even like this is an empty
test so that's dead code removal
but you can see that the result still
says it's they're very close and that's
because it's it's got a larger sample
size a lower margin of error and you can
see the margin of error there's a point
for 4.7 3.06 or point 46 and so using a
formula to be able to detect the
statistical significance between results
you're able to get a consistent read
every time and that's important there's
one issue I'll back up one slide here
there's actually a Firefox trace monkey
bug that has an issue with repeating
code blocks in getting gradually slower
results so there was a nice work around
with that where I have to access the
window object manipulate a variable
between each test so far Firefox has
been the one that's had the most engine
problems trying to get a bead on
performance so why is reproducing
results important well this is this is
Sun spiders result here and you can see
this is what I took this in my mobile
phone and I ran it I ran the same test
twice for the browser and you can see
there it says my browser is there's a
significant difference between my
browser and my browser and that's that's
incorrect and this is what browser
vendors are using to to to tweak their
stuff I mean this is this is so bad if
we go in here you'll actually notice
there's code there's there's there's
bugs and their actual formula for
calculating significance of results
you'll see there there's a typo there
and they're actually using an incorrect
formula whenever I was investigating how
to do benchmarking the hardest part was
finding formulas and finding accurate
formulas and I don't know statistics
naturally I mean I'm came from a
multimedia background so I had to do a
lot of research and trying to figure out
how this is done and it was surprising
you get into
little things like for example when
you're comparing rates there's there's a
there's a thing called a geometric mean
which is something that VA sweet uses
for theirs and then there's there's a
there's a harmonic mean and when you're
dealing with rates in this case
operations per second the harmonic mean
is preferred but Dromio it deals with
rates but it uses the geometric mean so
you get these slightly slanted views of
performance because their formulas
aren't quite correct so getting down to
the issue back where it was I ease timer
resolution where it's 15 milliseconds
versus one millisecond vs 4 milliseconds
on different environments how do you get
a consistent resolution and what I found
was that you can you can actually expose
the Javas nanosecond timer so you can
get nanosecond ish resolution in the
browser just by doing this tiny little
java basically I'm just exposing the
method and so now all browsers that
support java and this is yjs perf you'll
see your little java icon power up there
if you're using JSP and it's because we
by default if this is in the page
benchmark j/s will use the nano second
timer and it's not quite nanosecond
resolution because there's there's a
cost of communicating between JavaScript
and Java but it's still better than the
15 millisecond issue of IE and it's
still better it's still better than a
millisecond resolution so it gives you a
better more consistent way to do that
and it keeps the tests fast in different
environments like for IE if without this
we would detect the default time and it
would say 750 milliseconds and that just
plugs along on your bench marks so I
mentioned formulas and this is gross and
really boring but there's there's all
kinds of formulas that go into figuring
out statistical significance and and
meaningful information to your
benchmarks and data and so for example
there's the mean there's the variance
there is the sample a DV standard
deviation there's the standard error of
the mean I mean all of this stuff is is
is useful and you can you end up getting
that that nice display on Jasper where
it shows you the operations per second
your margin of error and then it's able
based with all these formulas be able to
tell the statistical significance of one
result versus another and this is where
it goes here for the statistical
significance and what was interesting is
now I mentioned before the formula i'm
using is something called a two-sample
t-test and it's great on paper and it's
something that sunspider uses i'm sure
some other ones use but i noticed that
in real life whenever you're comparing
javascript performance that it was way
too sensitive and as you saw with the
the SunSpider result where it said i was
significantly faster than myself that's
because this this formula is is really
sensitive and so I threw in a five
percent wiggle room there and that that
seems to be very consistent across
browsers so that's that's something to
consider um like I said I don't learn I
I didn't learn a lot of this stuff like
naturally I mean I had to to do a lot of
research and I really dug the videos on
Khan Academy like if you need to learn
something and this is the standard error
of the mean
and he just spells it out really nice
and so that was I use that I recommend
that you guys could check that out so
what are the gotchas of benchmarking and
so there was a lot of drama with ie9 and
SunSpider there's accusations going back
and forth and are they cheating or not
it just turned out to be dead code
removal and that's something you have to
consider like I would say the majority
of the benchmark Sanjay Esper are
probably bad benchmarks and it's because
making good benchmarks is hard and I
took care of the the part of measuring
the actual performance but now it's up
to developers and everyone else to kind
of make really good benchmarks so some
of the issues you can have our dead code
removal so like before where I had that
that empty block here that's dead that
could be dead pogrom well I mean that's
basically just running nothing and so
that you have to be careful of
benchmarking stuff like that because
then if your if your engine is optimized
to remove that stuff and it's going to
perform faster than something that's not
optimized to remove that then there's
other issues where you have like
long-running tasks like crank crankshaft
can kick in and you'll see the the
performance results change there or for
example opera has query schlechter all
but it caches its result and so since
this benchmark is based on repeating
things to get operations for a second
it's you're getting a cached result so
there's there's considerations there and
you just need to be aware of your actual
environment there's also reasons why for
example drumeo isn't as preferred as
cracking is because it does it has all
of its tests in a closure and then it
accesses variables on the global and
when it does that there's a performance
hit with Firefox and so they don't
prefer that test they want
show it on local variables because it
makes them you know look better so this
you have to keep that in mind with this
this kind of testing two I provide API
so you can do local test but by default
you're you're going from a closure to
the local the global variables so keep
that mind to whatever your you're doing
that it affects your your stuff okay and
so questions anyone nothing you yeah
does that affect
I i missed the actual talk on that there
they this has to deal with the timer
resolution and basically if you run the
new date and the new date what's the
smallest amount of time in there that
you can that you can read and that's how
I find the minimum the minimum
resolution and in this case I do it in
the for loop until you get a difference
in time and with ie9 i'm told it's it's
four milliseconds now and i actually
don't have to worry about it because my
script detects that the minimum time so
that that's really what it would detect
on the run speed the the long running
tests i run on Jas / if I run them for
five seconds just so I can get more of a
sample size so it usually ends up being
about a hundred which is above and
beyond most benchmarks I think Dromio
does five and sunspider does 10 so
having that bigger size just reduces the
margin of error but i don't know if that
that would that should not affect the
the length the benchmark runs it'll
affect the overall performance though
cool yes
yeah that I I did i used it i went into
all of that nano and micro benchmarking
on should i use new date should i use
date now should i use get time all in it
doesn't matter it's it's so tiny that
it's they're using one versus the other
is statistically indistinguishable from
them in fact there is there some
benchmarks that you know try to
calibrate out the cost and the overhead
of the the iteration like the while loop
you're actually having to do in the
function clunk and getting and entering
the execution context but i found that
whenever you subtract that calibration
cost from your end result you actually
get a higher margin of error so it
swallows any of the accuracy you were
hoping to gain by calibrating out that
stuff so in the end it's it's better to
just leave it in it's not a noticeable
difference oh
the that's Matthias's is the the main
drive behind me maintaining that and
that's just his design choice that's
simple it's really just he'd want it to
be very very simple to do so cool all
right said it okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>