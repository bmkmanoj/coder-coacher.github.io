<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tim Slatcher: Interactive Visualisations at Scale | JSConf EU 2015 | Coder Coacher - Coaching Coders</title><meta content="Tim Slatcher: Interactive Visualisations at Scale | JSConf EU 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/JSConf/">JSConf</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tim Slatcher: Interactive Visualisations at Scale | JSConf EU 2015</b></h2><h5 class="post__date">2015-10-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6FRURtW5qXI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Thank you.
A recurring theme over the last
decade or so in the software industry is the
growth of
data.
Some estimates say we've been doubling the
total
amount of data in as little as every 2 years,
and to
start with, we didn't really know what to
do with all
the data we've been recording.
As people started to
figure out the questions they wanted to ask
of their
data the software industry reacted to this,
we develop
backends and databases to scale that was able
to process
this data in realtime.
Technology like Hadoop, Spark
and Cassandra are now doing it in the scale.
This
enables us, as front-end developers, to build
front ends
to visualise and explore this data in realtime.
In this
talk I'm going to show you a couple of talks
we have
built out, and but first I want to explore
what is
a good user experience.
In my eyes, it is two things,
and there's many different ways of cutting
it, but I'm
going to talk about intuitive and responsive
but
intuitive, I'm talking about a user should
quickly be
able to get a good understanding of what their
data is
and how they should obviously ask questions
of it.
This
is by no means an easy problem.
It requires a lot of
studying by designers and front end engineers,
but it
basically requires a good grasp of what your
users are,
and the kind of questions they ask.
It can be dependent
on the industry you're developing for as the
shape of
the data.
And responsive, I'm not talking about responsive
web
design but a responsive application when a
user asks
a question of the application, it should be
easy for
that user to get the answer they're looking
for.
It
should be very fast and responsive.
In the enterprise
software industry, we've seen a lot of software
where
these ask a straightforward question of the
data and it
can take minutes, hours or even days to get
a response.
We don't want our users to switch between
asking
questions and receiving an answer.
Let's talk about
earthquakes.
Around 30 to 40 years ago, the traditional
ways of measuring them was using seismological
data.
This pen would scribble on 
the page.
These days, pretty
much all of the data is digitised so there
are websites
like this one which have a tonne of data spanning
about
last week for thousands of sensor groups all
around the
world.
Here we're going to look at an earthquake
that
happened earlier in the month off the coast
of Chilli,
it was kind of a big one, 8.3 on the Richter
Scale.
So
here we can see it happened around 11 pm,
maybe we want
to see if there are any aftershocks, so we'll
plug in
the next day, click update, wait for the spinners
to go
away and we'll see that kind of information.
That isn't exactly unintuitive, but I can't
zoom in
and see more of the data, I can't pan around
and compare
the aftershocks with the initial quake.
I'm not able to
do that side by sight analysis and there are
free charts
on the screen all showing slightly different
data, but
at this level of detail it is not discernible,
you can't
make out what they're trying to show.
Now I want to show you something we've built.
This
is chronicle, an application we built for
analysing time
series data.
Seismological data is one example.
I'll
search for Alaska.
In early September there was a swarm
of earthquakes off the islands in Alaska,
this isn't
a particularly part of the interesting work
for
earthquakes, the US geological survey has
a bunch of
things for recording activity.
We can see a big spike.
Select this and zoom in.
A huge cluster of earthquakes.
Zoom in.
It gets blocky here, and we'll talk about
that
later on.
I have enough screen real estate, so I can
split
these up as well, make it easier to figure
out what is
going on if we don't have them clashing all
on top of
each other.
We 
can now pan and zoom around like you'd
expect.
This is a responsive intuitive application,
I can zoom in and see what's going on, hide
these axes,
and we can see that this sensor recording
the earthquake
a few minutes, a few seconds, maybe, after
the other
sensors.
And that's actually because this particularly
sensor is on a different island further away.
We can
use that to maybe triangulate the position
of the
epicentre of the earthquake.
If you remember your high
school university or geology classes, you'll
know it's
made up of two component waves and a bunch
of the
smaller ones too, the P1 at the start, the
S wave is the
higher frequency one that comes in probably
around here.
If we wanted to compare this particular quake
with the
others, we could offset it maybe and interact
with these
charts individually, but we have a common
scrub bar
across all three, so we can get a good idea
of what's
going on.
What I'm trying to show you here is not actually
anything to do with earthquakes or seismology,
they're
both interesting and not terribly relevant
to the
conference.
What I'm trying to show you a is a good way
of measuring a large act of data.
That's key as an
engineer, a lot of that is very interesting
to me,
although as a user you can't really tell.
There is no
indication of how much is on the screen.
Our users
don't really care how much they have, they
just care
about being access and asking questions.
For reference,
there are four different series each recorded
at
100-hertz, so over the entire week, that's
around
300 million data points and all running on
my laptop and
I'm able to interact with it intuitively and
responsibly.
This was developed for an industry customer,
they
didn't care about data quite as dense as this,
what they
cared about was -- they had data of across
tens of
thousands of different series, spanning multiple
years,
and it is used in critical safety of life
operations
where they need to be able to look at up to
40 series at
once of live, incoming realtime data.
So in total, with
the tens of thousands of series over many,
many years,
they have around 300 billion data points and
they're
able to interact with it as if it was one.
They don't
really care about the data skill.
And this is just one
way of visualising large amounts of data.
I'm now going
to show you another, and this is something
we built
during hack week this year at Palantir.
It is a week
Palantir has every year where we get to basically
work
on whatever we want for an entire week.
Anyone in the
company is allowed to work on any project
completely
unrelated to their usual work, maybe a feature
to add to
an app, maybe an entirely new prototype of
an
application, and that's what we built up.
It's my
favourite week because it gives anyone the
opportunity
to build something cool and form a team around
it.
Many
of our great products and features have come
out of this
week.
What we're looking at is a network graph.
I have US
presidential election data from campaign finance
data.
This was taken from the last two weeks running
up to the
US election, and this dataset is entirely
available
online and is one of the largest I know of
of these
relational datasets.
We can see in the middle we have
Art Robinson for congress, a political action
for
committee.
This one particularly supports Art Robinson,
but however these committees can support a
candidate or
particular interest group.
Around it we can see payments and the green
dots are
the donors.
So we can evaluate who is giving money to
who.
In this particular case, we can see that William
Brady here is donating $200 to Robinson for
congress.
What's interesting about this is that while
we only took
about two weeks of the data and we only took
it for
California, there is actually a whole tonne
of data.
These are all the transactions in the last
two weeks,
and maybe this is a comment on the US political
system.
I'm allowed to do that; I'm in Europe.
So as we zoom out, we see a good view of the
entire
data and we can see some visual artifacts
going on and
I'm going to explain why these are later on.
So here we have the entire set of data.
There's two
large blobs in the middle and then two to
the bottom
right.
It won't surprise you if we use this little
tool
to jump in and analyse what's going on in
a given area,
we can find Mr Obama's political action commits
and,
woah, that way too far.
Zoom out now.
It's thinking
hard.
Traditionally at Palantir we've been dealing
with
these large datasets for a while, and we didn't
think
there was too much value in a graph of this
scale.
It
is so much data that we didn't think you could
visualise
what was going.
However, when we built and took
advantage of these features like the loop
I was just
showing you, and hope to show you again in
a minute,
when this has decided it's going to behave,
and through
some clever node colouring, I suppose I can
fit it to
screen, we're able to analyse this large amount
of data
in a realistic and easy way.
We're basically allowed to
see what's going on here, zoom in for more
information,
we can use the loop tool to figure out this
one of
Romney's parks, it is kind of unsurprising
these large
groups are both related to Obama and Romney.
So this is
a whole tonne of data and that's kind of interesting
to
me.
It's probably not as interesting to our users
how
much data we have here, but we're able to
interact
with it in a realistic way, we're able to
interact with
it, move nodes around as we see fit, apply
layouts,
follow the lengths so here I can expand my
selection to
all of the related lengths, and we can select
all and we
can throw them in a stupid grid.
We can do it.
It is
a very interactive application and we can
easily get
a good visualisation over this data.
It uses a tool in
the right-hand corner, we have 168,000 objects
on this
graph.
And that's 999,000 donations for some reason
one
shy of 100,000 donations here.
So in total we have
168,000 nodes with 200,000 edges going between
them.
And this is the kind of commonality I see
between
this and chronicle, the one I showed you a
minute ago,
we're dealing with a scale of data that has
not
traditionally been easy to interact with on
the front
end.
The big difference is this is entirely front
end
driven.
Once I loaded the file from a flat JSON file
it
is entirely stored on the front end, there's
no back
end.
This is entirely client side JavaScript project.
I'll jump back to the presentation.
There's the demos we don't need to use.
At the heart of these applications is a visual
display of our data, in one case this was
a series of
line charts and then in the other a set of
nodes and
edges.
Both are rendered on to a 2D canvas.
Now for
those of you familiar with canvas programming,
you'll
know there's a tonn of drawbacks to using
canvas.
In
HTML we can position elements using CSS, we
can use
tables, alternatives, take advantage of all
the fancy
reactive components we've built to easily
build together
an application.
We can use mouse event listeners on
individual elements to figure out what the
user is
trying to do at a given point.
In canvas we get these pretty primitive APIs
and
have to use pixel math to work out where we're
trying to
render and when a user interacts with the
scene we
simply get an event callback saying it was
an X and Y
coordinate, and we have to work out what the
user was
trying to do at this point in time.
It is hard to do anything incremental, as
soon as
you paint a node on to it, it's there for
good.
The
only way to get rid of it is to paint something
on top
or close the whole canvas down.
If I'm trying to render
a graph that size and want to move a node
across the
screen, I have to do a lot of work to make
that happen.
The question is, why did we use canvas for
this?
Why didn't we use SVG, 3G?
I'm sure we've all seen this
demo online, a lot of tune, or why didn't
we use
ploddable for synchronised line charts?
This is the
open source charting library we announced
here last year
and are still actively developing.
They even have an
example of synchronised chart on the website
as if it
was trying to tempt me to talk about them.
It all comes back to performance at scale.
The DOM
simply wasn't designed to have tens of thousands,
hundreds of thousands of nodes in it all animating
either independently or in sync, it just doesn't
work.
So for earlier versions we did use this, we
wanted to
use this, however when you start to move nodes
individually independent from the rest of
the scene,
things start to slow down.
They get pretty sluggish.
One of the drawbacks that I was discussing
was
having to rerender everything every frame
and we can
find a work around for this.
We can stack multiple
canvases on top of each other, for example
in chronicle,
we had a different canvas for each of the
different
series, the line graphs, we had a canvas for
the axis
and a canvas for the controls and the hover.
This meant
that as a user was hovering over the chart
we only need
to re-render one of canvases, not the entire
scene every
time.
The graph was even more decomposed.
We have
a different layer for the selected edges,
nodes
selecting nodes, this drag rectangle box and
so on.
This meant that if a user clicks on a node
or if they do
a simple operation, we only needed to re-render
one or
two, not the entire scene.
This trick extends to DOM
elements too.
We can use absolute positioning to
position DOM elements on top of the canvas.
This was
how the loop tool I showed you earlier worked.
We
positioned it to the left of the cursor, and
then we
were able to program in however we wanted
the content,
so we had this object previews with a table
of
donations.
We can use normal HTML CSS for that.
This
makes things flexible.
We can easily add extra hints
and information about our scene.
In an earlier version
of the graph we used that for every node.
Every node
was a DOM element, positioned absolutely on
the scene,
then we used CSS translations to move the
scene around
as the user interacted and dragged around.
This worked
out well, we had a single canvas for edges
and all of
our nodes were easy to programme.
It was all very clean
code.
The downside was that as we started to move
nodes
independently as we started to push the node
scale up to
tens of thousands, things slowed down.
It isn't
designed for this behaviour.
This technique of layering
canvas and DOM is really powerful.
Really flexible, but
don't expect it to scale with the data.
The biggest problem we faced when we tried
to scale
our graph out to hundreds of thousands of
nodes was
simply the time it took to render a single
frame.
Using
tools like the Firefox performance tools and
the Chrome
timeline we were able to get a good understanding
of
what is happening each frame.
And I'm not actually
going to go into them too much here, there's
a load of
great resources online about to how to use
them.
There
is also a lot about how to do specific canvas
profiling
and optimisations.
However, do take them with a pinch
of salt.
You'll often find advice like this: avoid
shadow blur, which is great, it has a good
intent
behind.
It can be expensive.
Rendering drop shadows
turns out to be one of the more expensive
operations you
can do on a canvas.
However, if you want to drop shadow
on something it is bad advice, it doesn't
tell me what
to do instead.
We wanted to drop shadow on to the nodes
because it looks cool, so how do we do that?
How do we
go about doing this?
We came up with the jsperf test to
see if we can compare the standard rendering
of drop
shadows to some other technique of getting
the same
effect.
This is what we're trying to render.
There's
a subtle drop shadow.
You can just about see it.
We
write our test, start with the width and height
of the
node and the number to render.
We used to render a
bunch because of the drawback of drop shadow
is when you
render them on top of them the anti-aliasing
is what
causes the slowdown to happen.
Then we have this render
function, it takes a canvas context and renders
a node
to it takes an X and Y position.
First of all make
radius, set the style using this red, and
have a shadow
blur of depth one.
For reference, this is what we're
trying to render.
So we're making an arc and fill that
with firebrick red and then stroke this to
get the white
ring round, and add an orange stroke, this
represents
the selection, as you select them they get
this orange
halo.
What we're going to do is rendering directly
to
the canvas, a thousand of these nodes versus
first
rendering to an image essentially rasterising
the node
down to an image and using that everywhere.
This looks
like this, make a canvas, make it slightly
bigger to
allow for the drop shadow, and we never attach
this to
the DOM, it is purely held in memory, then
we call
render note passing in with the canvas with
the H width
and height in the middle.
Our two test cases look like this: we have
a canvas
and pull out the context.
We clear it, JS perf uses the
same HTML for the tests and it runs another
one to get
a good aggregate of the number of time spent,
then we
iterate through the thousand nodes calling
render note
on to the canvas.
Our second test case looks almost
identical.
The only line that changes is this, instead
of calling it the node, we call it the draw
image.
And
this is what it looks like.
You'd be hard pressed to
tell the difference between the two if I didn't
say
which was which, you'd probably have a guess,
but maybe
not.
However, the time it takes to render each
is
hugely different.
15.1 milliseconds, for reference if
you want a 60FPS frame rate you need to be
rendering
every frame in under 16.6, so at a thousand
nodes we can
just about do the old technique, with this
we can scale
it up more.
The drawback of this approach is that we need
a raster image for every node size, not quite
every one,
but we can choose a few sizes we care about
and then
always scale them down.
It doesn't look too bad, and
this way the memory impact isn't too high
and we can
compute this as a one-off at the start when
we
initialise our scene.
So the take away from this is not
to avoid shadow blur.
If you just add it to one or two
objects, the cost might not be too high.
What you
should do is profile every change you're planning
on
making, if you plan on making shadow blur,
profile it.
Does it make a meaningful impact?
If so, are there ways
to find round this?
Another technique to push the scale a little
bit
more was reducing the amount of detail rendered
as the
scene -- as we zoomed out.
In this example, we render
the -- we lower the opacity on our node labels
as we
zoom out, once you get beyond a certain zoom
level the
labels aren't useful, you can't read them.
So there is
no reason to go to all the trouble of rendering
the
text, which is expensive.
We do a similar thing with
the icons too.
After a certain point, is all that
matters is having a purple circle that's the
right size.
This allows you to pick up -- oh look, there's
a lot of
interesting things going on without having
to think too
much about each individual data point.
This gives us
the aggregation of the data.
Again, we profile each of these changes, does
it
actually make a meaningful difference to the
performance?
In this case it actually did.
Another trick was, as we moved even further
out once
each node was taking up less than a pixel,
we render
them as rectangles because they're quicker
and better
than circles.
We figured if the user can't tell it's
a circle, why bother spending all of our GPU
time making
these circles?
The final thing is debounce.
This is a function
that lets you split your rendering pipeline
into two.
The problem we faced as we kind of tried to
scale up our
node scale even higher and higher, was that
every time,
every frame, we were iterating through hundreds
of
thousands of edges and nodes and spending
the time to
iterate through each was pretty expensive,
basically it
meant that our application didn't feel responsive,
as
a user was dragging and planning across the
scene, we
had to iterate through all these nodes and
edges and do
some basic rendering, but this still took
up a lot of
time, we ended up with the frame rate way
too high --
sorry, way too low, and this was just -- it
didn't feel
responsive.
We used debounce.
We debounced the
interactions and only call render when the
user stops
interacting.
That sounds insane.
We pushed back the
expensive render on this debounce, and we
need to find
a way of updating our scene without incurring
the
expensive cost of a render.
How?
We can take advantage of the fact that we've
just rendered the scene, and the scene we
rendered looks
almost identical to the new scene, apart from
a few
pixels.
So every time we rendered the scene we take
a snapshot and save it to a an offscreen canvas.
It
holds the entire screen.
It's not too bad, the bigger
it was, the more expensive, but for our purposes
it
works.
Ah, the user zooms out, we take the image,
render it
on to the screen to render it down.
You get white boxes
round the edge where we don't know the stated
of the
world.
It doesn't really matter.
As you saw earlier,
you'll get the white boxes, but you don't
really notice
if you're going pretty slowly, and you're
going fast, as
soon as you stop, the full render kicks in,
you see the
whole state again.
As you zoom in, things get a bit
blurry.
Again, it doesn't really detract.
You know
where you're tying to zoom in to, you can
see where
you're going to, it doesn't matter, and as
soon as you
stop interacting, the full state of the graph
is
available.
We do the same for planning as well.
We simply
translate the gym.
This is even more noticeable.
Generally you pan quite fast, but it doesn't
really stop
the interaction, you're still able to figure
out what
you're doing with the scene.
And it is not long before
the full render kicks in, so you're able to
see the full
scene again.
This allowed us to push that node scale all
the way
up, so that each frame render was no longer
dependent on
the number of nodes and edges we were trying
to render.
Our limitation was now the amount of stuff
to store in
the JavaScript memory.
We actually do a similar trick in chronicle.
If you
remember, as we panned left and right occasionally
things would get blocky, as we zoomed in,
or as we
panned the B section was missing, and the
reason is we
have to request data from our back end every
time we
want to view data because we're talking about
millions
maybe billions of data points, we need to
use a back end
to aggregate this data for us.
We simply can't do it on
the front end.
So instead of debouncing the render call,
we
debounce our request data call.
The render is cheap
enough, actually.
This is not too much data by the time
it makes the front end, but the request data
call puts
a tonne of work on to the server.
It has to load the
stuff into the memory and do aggregations.
It is fast
but it's not the thing where you want to be
thrashing
the server making it do a load of work where
no one will
see this, so we debounce this.
In the meantime, we have
the data from the previous scene on the front
end, so we
don't do the canvas tricks instead, we do
it with the
raw data, we re-render the data we already
have for the
subset of the scene we're going to know about,
so as you
zoomed we can render the same data, just small
smaller
and in the middle.
You as zoom in, we can render the
data and that's why it gets blocky.
As I pan to the
left, there's some data missing and the same
with the
right.
It interacts the same with the graph and as
we
zoom in, it gets blocky.
It doesn't detract.
You still
have the same information as you had before,
but the
back end is snappy.
So in no time the real data kicks
in and you can continue the analysis.
This is very similar to how Google Maps works
with
vector tiles.
To quickly sum up, we talked about what makes
a good
user experience and this is the idea of making
a good
intuitive and responsive user experience.
We talked
a bit about decomposing the scene using layering
canvases or DOM on top, and to cover those
of those
limitations, although there is still a lot
to get over,
we can profile to figure out where time is
being spent,
and we covered a few of the more commonplace
and more
basic performance techniques, such as reusing
objects
wherever you can, reusing the canvas, and
pre-empting
the next state.
That's all I have.
So thanks a lot for listening
and please come through with any questions,
please find
me afterwards.
[applause].</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>