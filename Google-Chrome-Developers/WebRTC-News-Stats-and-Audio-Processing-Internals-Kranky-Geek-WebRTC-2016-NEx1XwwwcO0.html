<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>WebRTC: News, Stats, and Audio Processing Internals (Kranky Geek WebRTC 2016) | Coder Coacher - Coaching Coders</title><meta content="WebRTC: News, Stats, and Audio Processing Internals (Kranky Geek WebRTC 2016) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Chrome-Developers/">Google Chrome Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>WebRTC: News, Stats, and Audio Processing Internals (Kranky Geek WebRTC 2016)</b></h2><h5 class="post__date">2016-11-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NEx1XwwwcO0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so yeah my name is Niclas bloom leading
product management forever you see I'm
super happy to be here with my two
colleagues so I'm a PI and Peruvian and
non software engineer in WebRTC and I
work with front and I'll do processing
like echo cancellation most suppression
and other stuff and I'm just a new
birdie it's great to be here today
I think the fantastic quality of the
presentations is I just phenomenal for
this event and I particularly enjoyed
that most recent presentation by Philip
hanky I actually learned a few things
that's really good that was great and we
are here to present you basically a
little bit from very severe web RTC
assets a moment and these are the stats
are we actually not getting to start
talking about gets that but is that we
see in chrome mainly from users who
opted in to provide this data to us
you're bringing up some news on what's
coming up in the next quarter and then
pebble do basically a deep dive into
audio processing pipeline and in policy
so where are we now in in June we
celebrated basically the fifth year of
WebRTC so in June 1st 2011 teen our
colleague Hara published on an ITF list
an announcement that we releasing WebRTC
into open source world and when I start
defining a set of api's on top of those
two to grow an ecosystem and unlock
basically real-time communication for
apps and services and this is what is
you know this was just announced last
week at chrome dev summit that there are
about two billion Chrome browsers across
that and Mobile out there and those are
all enables WebRTC right though these
are endpoints that are out there that
can be used but additionally there are
hundreds of millions of endpoints from
from Firefox and four-match out there
and just recently a group of open source
people have enabled web kit with opened
up RTC and that thanks personally me
very happy because I see basically the
open source community taking up the
responsibility on enabling even
further endpoints of a party see but
those that points are not only those
endpoints are not only their being web
part is enabled they are being used
right and those are really active so
this is what we see from from the Chrome
usage only so there are about 1 billion
combined audio and video minutes per
week happening inside of Chrome and this
is around 2000 years basically worth of
all your video communication which is
just happening in Chrome and this is
happening by the services that you
create basically and by all the users
had made use of approaches II but it's
not only audio and video communications
as often may be forgotten there's all
the data channel and I think it's very
impressive to see that we have basically
also per week one petabyte of data which
is being transferred over the data
channels it's very seldom basically that
I talked about petabytes and network at
the same time so I think this is very
impressive
and it makes basically up 0.1 percent of
all HTTP traffic in Chrome fractions in
climbing so yeah which is continuously
growing and I think you showed this
after the i/o of you see basically a
very nice growth here
yeah and these are not being used by one
company or one service so sorry is
tracking those for us and there are more
than 1,200 projects and companies that
make use of WebRTC and I think this
really shows the success of offering the
technology out to open source and
enabling and making to really boil to
free from the codecs that every cut it
basically can build on top of the stack
and can be enabled by adding real-time
communications or any kind of
peer-to-peer traffic to their service
and this is happening globally it's not
happening in a few selected countries so
this is the Google Trends that we see
from search for the term web RTC and
you'll see basically that that the u.s.
is not even in the
in the top five or top seven here and
it's also conscious continuously growing
with some outliers in here but this is
basically how we observe our web RTC is
being grown and how the ecosystem around
us from a developer perspective is also
growing a lot it's interesting right
China on first place here South Korea
second Taiwan third and yeah Sweden on
fourth right that's great it's not the
bad policy team that's having something
running here so what are the reasons
improvement that I wanna just basically
outline here that we have invested in
what we have launched basically in the
in the last six month just in the last
presentation from people we have seen
something about the pwe bandwidth
estimation video codecs audio
performance that we have investing in
and some changes in additions in chrome
so the two seconds is what was needed
beginning of this year in chrome to ramp
up to a one megabit per second video
stream and we have switched the the
algorithm or the mechanism to uncensored
only bandwidth estimation that means the
whole logic for the bandwidth estimation
sits on only one side and depends on
feedbacks it gets on the other side and
this actually reside into a reduction to
650 milliseconds that we are now at four
ramping up to one megabyte per seconds
and yeah it's been used for various
services right additionally we are we
are working on including audio also in
the bandwidth estimation and although
the headers basically to make it more
robust and we have also improved the
competition for TCP streams so if you
happen to be in a meeting that you are
very bored and you start watching your
YouTube video maybe at the same time
this will not ruin your audio quality
anymore yeah and we did what we
committed to do right so we we added
h.264 in Chrome we make use of open
h.264 for on the encoding side we
continue to use the loop ffmpeg that
there is always been and chrome for for
decoding side but I've recently been
pinked by folks who observed basically
inside of chrome that the performance
for h.264
it is less or less computing
requirements basically and the reason
for this is that from certain platforms
we also have enabled basically hardware
codecs to make it more efficient to not
create much read do not switch on the
fan
if servers want to make use of this yeah
but we've not only enabled 8:06 for
before the neighbors vp9 and we do I
think video gave a great presentation
about this but what I want to show you
here is basically a comparison though
these are two streams that we have here
a vp8 stream with 900 kapitza encoded
and wp9
stream for 650 running at full HD and
i'm playing this video now and i have
the slider here basically where you can
slide from left to right so this is
Marco and Jacky Marco is vp8 at the
moment Jacky vp9 there's no difference
right and this is great this is the
greatness of one of the great things of
vp9 you have 30% less bits but you have
the same quality and yes it is more
expensive and we have an Ableton
software and chrome approach easy demo
app RTC I mean has it enabled as a
default codec right now and we're
looking on how to bring it basically
into mobile and it is more complex it is
more expensive on these platforms but on
low resolutions or on low bitrate is
actually it can be run on mobile and
this is where the advantage lies if you
want to bring basically vp9 or video to
mobile for low bit rates over where you
don't have the bandwidth available
you can run basically vp9 at smaller
resolutions it works their final mobile
and you can make use basically of this
additional efficiency that you have in
the encoding and the 30% list bits yeah
but it's not only video it's also audio
that we are investing in so opus is the
preferred audio codec yes I psycho still
there we work hard on making opus more
efficient and to improve the quality for
for speech but also for content beyond
voice basically but an additional focus
that we are having is basically to to
continue to work on bringing it to the
ultra low bitrate as we call it so at 12
kilobits and below that you can run an
audio call maybe on an ad network on
mobile or on a Wi-Fi very ever a lot of
the rind and that's happening and
they're ready to be able to ramp up the
quality what the russ has been launched
and an m54 is a new screen sharing
picker so we added tap sharing to Chrome
which servers can make use of and with
this basically we took the opportunity
to completely revamp the X of the picker
and we enabled all your sharing as well
so you will if you want to make use of
it you can basically share the audio
from the tap as well and the the picker
is now separate in these three different
apps so you have your entire screen you
select your application window or you
can select just a specific type that you
want to share and I think this is also
interesting if you don't wanna share
basically you complete a blister on the
top of your menu tab so you can test
this that over there you have to install
like a a custom extension for this and
if you visit this URL basically you can
test the tap sharing and your your
sharing and what has been mentioned
already before and the great
presentation is that we have added
screen capture to Android and this has
been just launched or is being launched
it's already being used so for those of
you who have a pixel
phone and don't know how to use it and
call the the help hotline you can
actually start sharing your screen so
it's part it's part of the of the pixel
launch yeah and not not all web RTC
traffic or not not all administrators
are always happy to open up all the
network right so especially in an
enterprise environment of UDP is blocked
or UDP is limited to a specific set of
ports and what has just been enabled is
a chrome policy basically in which you
can define this port range and can limit
it to a very specific range which the
administrator as opening the firewall
and the folks are working at the moment
on bringing this into cPanel to be able
to throw this out into a managed
corporate network and with this I would
like to hand it over to Justin talking
about so the upcoming work all right
thanks Nicholas so there's a bunch of
stuff that we could really spend a lot
of time getting into details on you can
talk all about all the things that
Philip wrought up in his presentation
unfortunately we're not going to cover
all that today because I want to make
sure you leave time for a pair to get
into all the details of how the web RTC
audio stack works but what I really want
to do is talk about some of the top pain
points things that you've come to us and
said you guys really need to fix this
please help in the status of these
issues so let's get into that so these
are some of the things that have been
identified as like the top pain points
of people who are deploying applications
today with WebRTC and what is really not
working I think for the most part WebRTC
is getting toward this sort of mission
accomplished'
1.0 is done standpoint but there's still
a few places where things are just not
quite there yet
and so one of the places in particular
is this case in corporate networks
nicholas talked about what we could do
where port ranges need to be basically
restricted because the firewall config
on the local network is only inline
WebRTC out of a certain number of ports
we need to make sure WebRTC honors that
so we have that in WebRTC today but one
of the things that we're still seeing
missing is cases where enterprises banks
corporations are not letting any UDP out
of the network they're forcing
everything to go through a proxy
and web urgency will always take some
quality hit in this case because they're
forcing media to go over TCP and
traverse the proxy but what we know
right now is that some of these cases
you even have to log into the proxy and
even though web traffic works the WebRTC
traffic doesn't work because right now
Chrome doesn't understand how to route
WebRTC traffic through proxies that
require authentication and this is like
a real problem we've heard from many
customers where I customers require this
to work on their network and we just
can't satisfy it there's a bug right now
that's open it has over 100 stars the
reasons for this are deep and complex I
won't get into all the detail but the
basic thing is that the web stack has
its own set of credentials and cache
these sort of things
WebRTC needs have access to that without
polluting the state of the actual web
network stack but we have like some work
going on here a collaboration between
the WebRTC team chrome networking team
and we expect we'll have something you
can start playing with by the end of
this quarter so we notice the top issue
we're making real progress on it
look for something that can actually
work very soon next media reliability
this is one of the other things we hear
time and time again you know we say web
receives done it works it just works for
the most part and we still hear cases
where someone says well my customer was
using this they were using a Mac and we
didn't get any audio from their mic we
told them to restart their browser and
when they restored their browser
everything work well it's good there's
really some solution but that's really
not what we want to have happen we want
this to you know just work all the time
and it turns out this is like really
complicated due to a way chrome is
designed where all the interaction with
the system media you know audio and
video subsystems is managed by the
Chrome browser process you know for
those not familiar with the architecture
of chrome every tab every website has
own renderer process that does all the
layout and drawing of the the actual
HTML but then all the interaction with
the system is done by this single
browser process the problem is that
browser process lives for the entire
time chrome is up and so like if
something gets wedged you know there's
some driver issue some bad interaction
with like something in you know audio
core or whatever core audio leaking
resources the only way to get to a good
is to take down the entire browser
that's kind of frustrating the other
thing is that the browser process does a
ton of other things not just all the
media interactions but anything where it
basically has to interact with the OS
has to go through this browser process
so there are cases where things can get
blocked and the browser process due to
some the other handling that's going on
and for something that's trying to do 30
FPS streaming video like that can cause
like these very small glitches that can
lead to render lags or even cases where
like it calls the echo cancellation
problems because the timing gets messed
up a little bit so we are going to fix
this because of a new architecture that
chrome has called mojo and mojo is
basically a way for us to take a lot of
stuff that we have for doing specific
tasks and moving it out into its own
process that chrome can then spin up on
demand so what we're going to do is take
all of our interactions with the audio
subsystem core audio and then like and
move that out to its own process also do
the same thing with video device and
numeration and capture move that out to
its own process that means these process
will only run when there's actually a
web RTC session going meaning that that
code only is loaded when necessary these
subsystems we can then bounce them if
you know someone says oh my tab didn't
work whatever you know you can just
close a tab and restart it and
everything should be in good state we
five should have many fewer cases where
this actually will happen because we
don't have this long long-running you
know interaction with the system which
is what we believe is the underlying
causes on these things every time
basically your closing tab you're
getting a whole fresh new start and
since this stuff is all happening in its
own process with zone like main thread
the other stuff happening for the
browser process will not interfere with
the timing of all these critical
real-time events that we have for audio
and video and and perhaps best of all a
bad driver with a web cam will not cause
the entire browser to explode so lots of
upside downside well this is a
significant significant reengineering
it's going to take some time to get
through all this but we're hoping that
this quarter we can have the video
capture stuff pulled out to its own
process and then next year you know
cheese the same for audio so we'll see
how quickly we can actually complete
this work but we think this will kind of
help us to get from like two nines to
like you know three or four nines in
terms of actual audio and video
ability which would make a huge huge
difference in us be able to say whoever
si just works um
screen sharing screen sharing works
reasonably well right now if you're
sharing static content a document a
spreadsheet etc however lots of times
now people are trying to share an
application a game or even share like a
YouTube video and basically you're given
two choices at this point in time you
either have a very slow kind of jerky
video which everyone kind of sees on the
screen and like gets the sad about or
you have your fans spin up because
you're trying to basically scrape the
screen in 30 times a second and the
attendant sort of CPU overhead so what
we've got now is a new thing based on
DirectX on Windows a corresponding one
on Mac that's a much more optimized it
takes out some of the things that we're
selling us down in the old version and
basically this will be engaged when you
try to set the frame rate for the screen
capture to 15 or 30 FPS that allow
things to be much much more efficient
and this will basically open the door
for us doing actual streaming of games
we know there's a lot of people using
WebRTC for that as well as for videos
there's still some work we have to do on
the actual encoding side for encoding of
screen share to kind of keep up with
this
you know scraping is one part of it
that's the first part we'll work on then
we'll have some stuff coming to allow us
to basically make sure that we have
really good image quality for these
streams but we expect to see some
significant improvements in the next
quarter or so and lastly we hear often
that people are trying to make WebRTC
work on IOT device or some other thing
where it says I just want to do a data
channel I don't want to have you know
all the video processing stuff because
my app doesn't need that and in order to
make WebRTC actually build for my
configuration I had to go there and I
slash a hack in order to get this thing
to actually even compile and part of the
way things are the way they are is kind
of WebRTC kind of grew up organically
and it was kind of moved into Chrome and
we had all these sort of things where
there are some stuff that's kind of
interval well we have these same needs -
we need to make web parts you work in a
lot of different places we're going to
kind of chop back some of these
dependencies eliminate the places where
we have things that are cyclical and
basically allow for a lot easier custom
of these
customization through the rgn built in
fig without having to go in hack and
actually change the source code so this
will be a multi quarter effort but the
things we can expect by the end of this
as you can have a build of Weber to see
that might be voice only it might be
data channel only or if you want to have
specific codecs removed or add your own
codecs through sort of a some of the
api's we have similar to our way we can
inject a video codec you'll be able to
do that so this sort of makes some of
the maintenance cost of kind of
integrating in customizing WebRTC much
lower so these are all things that we've
heard of like this like most of the
small stuff in WebRTC has been taken
care of now these just some of the few
remaining big things and we're making
really good progress in these areas and
we expect to have some really tangible
stuff to show in the next quarter - and
with that I'll turn it on to a deep dive
into WebRTC audio for pair thanks for
that and yeah so I'm I'm going to talk
about data processing development that
we do in web RTC at Google and it will
be maybe not super deep dive but the
deep dive part we can take at the Q&amp;amp;A
afterwards and so we write the audio
processing algorithms we maintain the
code handle incoming issues and maintain
the pipeline out the processing pipeline
and write a lot of tests and the work
that we do we do a lot in response to
issues that we see without the
processing both in software battle also
and the hardware the process that we
utilize but we do the work with the long
terms improvements in mind so so we we
are able to do that as well and I'm
going to go through the software audio
processing pipeline that we have in my
party see and I will go through how they
utilize hardware how the process import
support that we have available on mobile
devices and now we'll go through the
tuning process that is done
in order to to tune this hardware
processing on
on mobile devices and that seemed quite
important to to understand that in order
to see why we are seeing the issues that
we are seeing on with the hardware the
processing support on MOBAs which I will
go through after that and I will go
through the solutions that we applied to
handle this and the ongoing work that we
have on the software deep processing so
this is what the our audio processing
pipeline looks like and this is the
standard functionality that there is
also experimental functionality but I
won't had discussed that in this talk
and the audio processing pipeline
basically so delirious sites inside out
the processing module which is a module
inside WebRTC and this receives the
audio coming from the network from the
decoder and the analyzes that and pulses
that onto the loudspeaker
so the loudspeaker is the small box up
into the rightmost corner upper right
most corner and then it receives the
audio from the microphone which is the
small box in the lower right most corner
and process is that and passes their
processed out their own to the coder
which passes it on to the network and
the functionality that this in place
here is is functionality that is
required in order to be able to to have
a successful board call and also
functionality that improves the quality
of the audio beyond that and we have a
basically two type of components
processing component the inside of the
processing module
most of them operate in the sub band
domain in on frequency bands and then we
have some that operate in the full band
signal and in order to provide the
signals the sub band signals to the sub
band processing components we have these
blocks which do down mixing when when
that is needed resampling and bound
splitting into these frequency bands and
then merging of the bands
and up mixing whenever needed
and the first of the processing that is
being done on the microphone signal is
the high-pass filter and the purpose is
the oh that is to is to provide a decent
signal for out there of modules to
operate on so for instance an echo
canceller have quite big problems with
handling seeing us with a DC offset so
and the same is the case for the noise
suppression so these modules typically
need to take take care of that anyway
and that's but but in this case it's
taking out by the high-pass filter and
that also have have have the task of
removing electrical hum which is picked
up by the microphone and then we have
the level control where they gain
control which controls the level of the
the output of the audio processing
module so the task is to ensure that the
output has a decent level and there are
four variants of that and the the
rightmost box here is the analogue
adaptive gain control which adjusts
analog gain the analog microphone gain
and then we have two variants of the
detailed adaptive gain control which are
the residing in there and the two left
boxes and they adjust the digital level
of the holder signal and then there is
also another mode which which applies
the fixed gain in a controlled manner
and then we have the echo canceller so
the purpose of that is to remove any
echoes originated from the loudspeaker
signal and that is and that is being
picked up by the microphone and the echo
console analyzes the signal going out
for rendering by the microphone and the
predicts and removes and echoes I've
ever done or suppressor and the task of
the noise suppressor is to reduce the
stationary noise to in order to increase
the listener comfort and decrease listen
a fatigue
and then we have a module component
called transient suppression which have
the task of removing and the sounds
originating from keystrokes and finally
we have the output signal analysis
component which provides information
about the outgoing audio to other
modules inside web RTC and that
information could be things like signal
level and the presence of voice and
whenever on mobile platforms we try to
utilize whatever Hardware out the
processing functionality that is
available and to be able to see the
hardware oddly processing is seen as a
layer that is in between this the
loudspeaker and the microphone and and
audio processing module and what we do
is that if a certain functionality is
available in hardware we turn off the
corresponding functionality inside the
software audio processing module so for
instance if Hardware echo cancellation
is available we don't do software echo
cancellation and the reason why we do
this is that if the hardware audio
processing functionality is properly
tuned and optimized it should provide
better functionality since the software
audio processing functionality that we
have is generic so it should work on all
kind of hardware but if you tuned for a
specific hardware you typically can get
better results and also the
functionality can be customized to the
hardware for instance if you have a
multi microphone Hardware you can use a
multi microphone noise suppression and
potentially it should have a lower
battery and CPU usage and for the echo
canceller it's it's typically a really
big advantage of doing that in the whole
relay since there are no render effects
in the echo path as seen by the
loudspeaker and that is typically not
the case for the
software counselor and in order to up in
order to understand how the hardware are
the processing functionality behaves in
practice is quite important to to know
how it is typically being tuned so if I
have a device a hardware a mobile device
that this to be tuned that is typically
placed in a silent room and the software
client is installed into the device and
this software client communicates with
another client that is located in
another room a control room and that
tuning device also have the capability
of playing out audio in the silent room
and picking up and capturing an audio
that is present in the silent room and
the silent room is where the device to
be tuned is located and the tuning is
done in such a manner so that the number
of scenarios are are created where there
are different kind of noise noises being
played out in the silent room and there
are different kind of conversational
scenarios with double talk single talk
and for each of these scenarios the out
you that is received from the device
being tuned is analyzed together with
the audio that is captured in that room
and based on these analysis a set of
parameters a new set of parameters for
devices computed and those parameters
are then uploaded to device and the test
is the scenario is repeated and this is
done until sufficient quality is
achieved and this is a very
time-consuming process and it's done
typically done manually and it's
important to note that for the VoIP case
there is no standardized tuning client
but the a party C mobile can be used and
have been used for this and it's quite
important and for for these devices the
tuning is done including the network and
the software client so
any kind of active software processing
that is done in the client will affect
the tuning which means that the client
used is really really important to get a
good tuning so for instance if high-pass
filter is active in the client that will
affect the tuning and yeah also another
thing with with with the tuning of the
hardware parameters is that each feature
combination is stored as a separate
profile inside the device so if there
are several features for instance
typically there are the gain control and
you have the echo cancellation no
expression all combination of
combinations of these need to be stored
as a separate profile in the device and
this is quite error-prone so because for
instance if you update the echo
cancellation parameters based on Ansan
Union you need to make sure that you
update all profiles work where the echo
cancellation is active and that's quite
easy to miss and indeed we we we have we
are seeing issues with a harbor audio
processing support and it's important to
note that this is beyond the control of
WebRTC
so if we choose to so we can basically
choose to use the hardware audio
processing functionality or not but we
cannot make it work because that is in
order but we are still affected by by
any issues that that arise from the
hardware audio processing so the main
issues we are seeing are with tuning or
related to tuning of the hardware
processing we see poor noise suppression
poignant transparency echo leakage poor
bandwidth and low signal levels and then
we have also issues we broke and
hardware API support and this is also
here solely something which is in the
hardware Northam about to see so for
instance if we try to explicitly turn on
the gain control in hardware that breaks
the hardware echo canceller
if you try to turn on the game
controller the hardware counselor starts
leaking echoes and similarly we have
seen that if we timed to turn off the
hardware noise suppression the the
hardware echo canceller starts leaking
echoes and we are also seeing issues
with silently failing hardware so we
have we have on case by the echo
consulate permanent stops working
suddenly after quite a while have been
being being fully working and the only
way to get it to work again is to make a
software reset and similar we are seeing
cases where we sometimes get silent
microphone signals and with WebRTC don't
get any notice of this happening and I
have some examples and this in this
example they this is from a scenario
where there was a only echo coming from
the loudspeaker and known ear and signal
so everything though that was picked up
by the microphone was echo and the
hardware echo canceller was active which
meant that if it worked properly the
output of the Hardwicke counselor should
be silent or close to close to silent
and what is shown in these figures is
that is the to the left we see the
spectrogram of the microphone signal and
to the right we see the waveform and
this signal is not silent and in this
case this was because we tried to turn
off the harbour noise suppression and
that caused cause this device to sudden
start leaking echoes and what this year
here is the leaked echos but if we if we
have the table noise present on this
doesn't happen and we have a silenced
microphone signal a silent output of the
harbour echo canceller now one more
example where the capture level is low
this is also the spectrogram and the
waveform from there from the microphone
signal and in this scenario that was
only near and signal a present so no
there was no echo present
and we couldn't get the signal to be the
signal to be picked up signal being
picked up to be stronger to have a
higher level than this one and this is
16 bit integer range so then the figure
basically shows what range would be
possible but but we couldn't get more
and we couldn't get the higher level
than this and this has a severe impact
of the of a conversation because when
you send this to the other side the
other side will perceive this audio as
being very very low which it is
digitally and we try to address this of
course so one thing we do is to use
allow and deny lists so basically we
detect on which platforms the hardware
of the processing is is okay to use and
which platforms it is not okay and when
it's not to get used we revert to using
the software audio processing instead
but this is really hard to scale and
it's very these lists are very hard to
maintain basically we need to test all
devices where we do this and the attack
should give suboptimal performance
compared to the ideal case because if
the hardware would have been tuned
properly we would have gotten better
likely gotten better quality how do you
call it by using the harbour audio
processing compared to using the
software the processing which we need to
do in these cases so we also work with
vendors in order to ensure that the
devices are properly tuned one thing
where plunder is an objective evaluation
tool based on a per se party C Mobile
which they can use to simplify the
tuning process and we work on improving
the software audio processing to ensure
that when we need to that when we don't
use the hard route the process
functionality the difference in quality
should be should not be noticeable when
before back to using the software audio
processing
and we have some kind some ongoing
worked on improving the software audio
processing and the main work is being
country being down on the echo
cancellation we have them the figure
here shows shows the the major
components of an echo canceller there is
a delay estimator a linear adaptive
filter and a nonlinear processor and we
have been placed some refinements of the
adaptation of the adaptive filter and we
have also robust defied the delay
estimator and these two changes lead to
more robust echo cancellation behavior
and we have ongoing work of improving
the echo removal and the transparency of
the echo canceller but to do done is
that we change the adaptation scheme for
the linear adaptive filter and we also
change the way that we compute the
suppression gains that are applied in
the nonlinear processor I will also make
the interaction between the sub modules
inside they could cancel a in counselor
more we're more integrated so that they
speak more together with each other and
probably this will end up with a lower
critical solution that this has a lower
complexity and it should be more future
proof for upcoming changes yeah to the
pipeline and it should be modular and
more easily to easy to maintain and
related thing that we are working on is
gain control so one thing we have in
place is a new digital adaptive gain
control mode which is able to operate on
lower level signals but we're also
working on analog adapter gain control
improvements which will affect the echo
canceller performance and what we are
doing there is is to ensure that you
have a better that we better handle the
case when the echo is saturated in the
microphone signal and this has a way
that the analog adapter gain control
handles the
has an impact on on how well the echo
canceller performs and also we are
modifying the way that that we detect
soft saturation when we have soft wet
saturation in the microphone and will
also add more integration between how
the analog adapter gain control behaves
together with the echo canceller and
that is really important since the
analog adaptive gain controller
constitutes a big artificial equipment
source a big source of artificial EcoPOD
changes as for Dekker counselor so
anything that that does is affects what
what echo canceller does so we are
working on echo cancellation and gain
control but we are doing much other
stuff as well but we won't go through
that today
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>