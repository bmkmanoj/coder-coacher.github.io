<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Improving mobile WebRTC video using SVC (Kranky Geek WebRTC 2016) | Coder Coacher - Coaching Coders</title><meta content="Improving mobile WebRTC video using SVC (Kranky Geek WebRTC 2016) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Chrome-Developers/">Google Chrome Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Improving mobile WebRTC video using SVC (Kranky Geek WebRTC 2016)</b></h2><h5 class="post__date">2016-11-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Zwjjb9Cx3ZA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">ginger shinnok I pressed it from video
and video is one of the early companies
that dealt with videoconferencing and
with video technologies and here is here
to explain how these technologies can
assist test when you start and take what
we have today in the browser towards
mobile and what these things do in order
to improve the quality their floor is
yours Thank You sahi and thank you for
the organizers for being for us being
here my talk today is about achieving
the best video quality on mobile and
video has been building scalable
architectures for over 10 years now and
we've been using a scalable video coding
in order to achieve this on lossy
networks which are especially important
on mobile environments when people think
about a video quality solution thinking
about the WebRTC codec itself and the
transport is really not enough you got
to think of the whole thing holistically
the platform you need to think about
what kind of video routing you're going
to have in the background what kind of
video codec you're going to choose which
makes a huge impact of how your stream
is constructed you have to think about
your audio solution and device
management and rendering all those
things have a big part to play in the
actual quality of the delivered stream
it's rendering as well as transmission
over the network so with mobile when we
were building our video service we had
to solve all these problems on that
user's behalf this is the same problems
that WebRTC experiences and they're the
most of the issues are that the mobile
is the worst case scenario for most of
your endpoints Mobile has fluctuating
bandwidth it has packet loss and jitter
it has varying device capabilities you
have retina devices you have devices
that have very low CPU and performance
and you have to make decisions about
trade-off between bandwidth and battery
life most of these devices you don't you
know if you ever played with some of the
other video solutions you'll notice that
device might get too hot and it doesn't
provide a great experience if your
device only lasts for
20 minutes in the videoconference apart
from all that you also have to deliver
very low latency throughout your entire
network real-time communications is a
special challenge over any other type of
media because of the low latency problem
you have to deliver a packet end-to-end
within 250 milliseconds timeframe and
that's a huge burden and requires very
specific choices about how you construct
your stream and how you create your
video solution so to describe why we
need scalability I'm gonna talk about
some primers of video coding what is a
what is a video stream a video stream is
essentially a series of video frames
send one after the other in a very
simplistic way you can think about it is
if you were to transfer a series of
JPEGs at 30 times a second now while
that would deliver your great quality
you certainly wouldn't be able to do it
in the bandwidth that you have and in
order to make this process fit into the
bit stream what happens is we have to
create a dependency structure this
dependency structure starts off with a
mainframe that you encode and then you
send differentials to make it simple we
break this down into five different
frames in this example we start off with
a key frame one that's a very big frame
to send and transmitted over a wireless
network once that's sent we can start
sending little differentials piece by
piece over to the other endpoint but
this creates a huge problem for a basic
single area coding missing any of those
frames in the middle will break the
stream and once the stream is broken you
have to actually retransmitted regional
frame or ask for the missing frames back
which is again creates latency and if
you have to retransmit that original
frame it actually creates a lot of
bandwidth limitations because that
original iframe is it very large so just
to solve some of these quality issues
you also have to think about what kind
of routing architecture you have in the
back end
a traditional routing architecture uses
single layer coding like you're used to
and it's an MCU an MCU is very simple in
the way it funk
you send your video string to the
bestest capabilities the server decodes
everybody streams composites it and
rico's it back to the endpoint now while
that is simple to do the same problem
that I described earlier with having
this interdependency between frames
especially on the mobile device creates
a lot of issues when it comes to
bandwidth and bitrate and packet loss in
addition to that all this transcoding
induces delay having to decode a frame
ring coded back adds a lot of latency to
the network which really is really bad
for real-time communications not to
mention the fact that all this decoding
and ring coding takes a lot of CPU and
this is something that you wouldn't be
able to scale to a large conference now
moving to a much better architecture is
a simulcast architecture now this is
great because now you can send two
separate streams they both have this
still have this inter dependency but now
you can the router can pick which stream
to forward to the endpoint now this is
much better for air resiliency it also
allows you to create custom layouts on
the endpoints now that your router picks
which stream to send to each endpoints
you can choose how you want to composite
them and lay them out this is a very
common way of doing video conferencing
right now and it does not require any
server-side decoding the problem with
this is that you have to pay the penalty
the penalty is that you're encoding the
stream twice and that creates about a
50% overhead over a single layer now
having to send two streams itself also
creates a synchronization problem
because now that you're sending both
streams at the same time when you're
making a switch you have to choose a
better approach is to use a scaleable
router this is something that video has
been building for over 10 years and this
is something that we are trying to make
sure that the vp9 has in this future to
create the best coding and the best
routing available in a scaleable router
you only send one stream but that
earlier in the dependency that I showed
you gets broken up in such a way that
you can now choose framerate resolution
by just picking packets out of that
single stream and a scalable router can
do that delivering the benefits of both
an MCU with a single stream and the
simulcast in addition by adding air
concealment that now you can do because
the interdependencies and it utilizes
less bandwidth so how do we do that we
can start with a series of video frames
as I described earlier it's a set of
frames we're gonna make it simple and
call them frame 1 through frame 5 now in
a single string coding as I described
earlier that interdependencies linear
you have frame 1 Frame 2 and they all
depend on the previous frame if we were
to create and temporally scalable stream
what we would do is we would break this
dependency a way to do that is to make
sure that we pick key frames let's call
them T 0 and only make them dependent on
each other now once they are dependent
on each other and we have frame 1 and
frame 5 we have a seven and a half frame
per second stream which isn't bad but we
can now create a dependency again in a
different layer 0 that will give us a 15
frames per second stream as you can see
the big difference here and the big
innovation is that now the T 1 frame
only depends on the T 0 frame and if you
were to lose it on the network during
transmission on your mobile device on
your weifare or anywhere else you can
still continue decoding and you can
still get yourself in half frame for a
second stream and all you're gonna see
is a bit of a glitch expanding to a
third layer we can add two more packets
now the dependency tree grows and you
have an extra layer that allow you to
have 30 frames per second now the way
that the router would adapt to this is
that as it's receiving these packets
from the end point it can decide which
packets to drop on the fly so in this
example you can transition from having a
30 frames per second stream to a seven
and a half one just by choosing to drop
the T to into one packets it's a very
simple thing to do does not require
decoding and all you have to do is look
at the packet headers this allows you to
shape the stream into the bit rate that
you have available on your mobile
endpoints this also adds a huge layer of
air resiliency
now that you can lose the tutu frame
without impacting the stream within it
without impacting the decoder without
having to retransmit any of the
important frames and the resilience
itself moves on as you're losing packets
over the network
losing a t1 frame will lead you to
having a 7 1/2 frames per second stream
but losing a 2-0 frame will require will
require you to retransmit it however if
you look at the new layout now the
retransmission only affects a quarter of
the packets so we don't have to
retransmit a lot we only have to return
a little and there's lots of methods to
do that FPC for error correction or AR q
an automatic request repeat are two of
the ways that we can reach transmit
those packets without having to lose any
data and if you think about on the
mobile side having to retransmit that
iframe in the restarted decoding stream
is the worst time to do it is when
you're in the packet loss right so most
of the hardware decoders I'll put the
single air stream and what happens is
that they either produce a periodic
iframe which is very big and very fat
you have to squeeze it into a small pipe
or you have to have this inner
dependency problem we're losing any of
the frames requires you to request the
iframe again now building on top of that
a scalable stream can also have a
spatial component just like I described
you earlier when you can break the
dependency on a temporal packets now we
can actually create an interdependency
in spin space as well we can create two
layers an SD and HD layer where the s0
the HD layer derives from the t0 layer
in fact we use the benefit of the fact
that those frames are very similar to
each other to create a differential in
space as well so as you saw earlier a
video stream involves differentials from
one frame to the other but that is also
true when you scale the stream from
being small to being large we can use
that differential to save ourselves a
lot of bandwidth when we transmit over
SVC expanding this to a full picture of
interdependence see we now have both
spatial and temporal scalability this is
a specially encoded stream that has all
this information in it that allows the
intermediary router to pick out any
frame rate or
resolution that I chooses for example
you can send one stream up to the server
and the server can decide that it wants
to prioritize frame rate so by using
these highlighted packets or the T 0
frame you can deliver 30 frames per
second in SD but if I'd like to have
resolution instead I can create a packet
stream that has 720p and 15 and all of
this can happen a McLean in on-the-fly
now how do we solve most of our video
challenges well the bit stream itself
becomes shapeable right so fluctuating
network on your mobile device as you
walk away in your signal strength drops
can be shaped dynamically without having
to do any of the other extra
retransmissions there's not much cost
there the loss in jitter can be
concealed and there's a lot less to
retransmit the device capabilities
themselves this allows you to have a
priority of what you want to do in terms
of the the way you want to view the
video if you're on a retina device you
might want to have a high quality image
if you're in a small device with low
battery capability you want to hold down
the framerate and make sure you don't
receive anything none of this will
affect the original source so if you
have a lot of people in the same
conference you do not want to use the
lowest common denominator you in fact
want to send the best thing you can up
to the server and let the server decide
for each end point with dust to deliver
you can choose battery life trade-off
vs. bitrate as you determine that your
battery life is depleting you might want
to say hey don't send me the higher
spatial layer so don't send me any more
temporal layers and you can reserve the
battery and still have a conference just
at a lower frame rate and of course this
allows you to have very low latency
because the server no longer transcodes
any of the media data in fact all you're
dealing with is a penalty of a hop this
low delay also helps in your
transmissions because when you're
transmitting less packets the video if
you had to ever freeze the video it
would be for a very short amount of time
just to retransmit those packets and
that will significantly improve the
delay instead of having to wait for the
entire frame to arrive before you
continue decoding so even though we're
now adding a little bit more
to do the scalable coding we did gain a
lot we have about twenty percent fewer
bits over simulcast due to our adaptive
spatial scalability and we can tolerate
up to 20 percent packet loss with no
significant impact due to error
concealment now if you think about a
mobile device on a mobile network which
is the worst case scenario for you
having a 20 percent packet loss that you
can easily tolerate is a great thing for
your video quality now this is something
this kind of technology is something we
really want to have in WebRTC and we're
working with Google to make sure that
vp9 has all the scalable extensions some
of this is already in vp8 if you look at
the vp8 right now it has temporal only
scalability which is actually great it's
the first thing that I showed you in the
slides where you can pick out a temporal
stream from that vp8 stream but going
forward it's becoming more and more
defacto to have the scalability built
into all of our future codecs both on
vp9 and the open media lines a v1 where
the temporals mandatory and spatial is
under review the air resilience that I
have described earlier is also available
in vp8 h.264 SVC and all the future
codecs like issue 65 in vp9 so in
addition to that we video we love to
work with WebRTC this is something that
I think Google is doing a great job on
and the future would be with vp9
for us all we need is to have these
spell scalable extensions that we feel
will benefit everyone we're contributing
code to both whoever to see and with
them to make sure that all those spatial
extensions are there and for everybody
to use chrome now has a full vp9
scalability in the decoder and the
encoder can be configured through a URL
flags and going forward we're working
with the bar BC editors to provide ways
to configure the encoder preferences
themselves for different use cases
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>