<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using the Camera - Developer Diary #Day3 | Coder Coacher - Coaching Coders</title><meta content="Using the Camera - Developer Diary #Day3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Chrome-Developers/">Google Chrome Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using the Camera - Developer Diary #Day3</b></h2><h5 class="post__date">2017-11-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ah7z3E56YPQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Harel I'm back again to talk about
building my camera this time I want to
talk about actually taking control of
the device camera and taking pictures
with it I already talked about how you
get hold of an image when you don't have
access to the camera so here we don't
have to worry quite so much about
compatibility except to make sure that
we use feature detection to disable this
feature where it isn't available as it
happens though the release of Safari 11
the latest stable version of all the
major browsers now actually supports the
getusermedia API which is the core
feature that we need so let's have a
look at how we handle all of this as I
said one of the first things we need to
do is feature detection so I have a
constants file here which just has a
couple of things so I'm checking for the
image capture object in the window and
I'm checking for the media devices
object in navigator we'll use those to
determine that these two api's actually
exist so I've extracted all of the
camera related code for my app into this
helper class that I've just called
camera helper this is gonna handle all
of the interaction with the device so
that the UI layer can just concentrate
on how to present this information to
the user so one of the first things we
need to do is if the feature is
available in the browser it doesn't
actually mean we've got a camera so we
need a way to let the UI layer determine
what cameras are available so that's
what this method is for they get cameras
method if the media devices api is
available then we'll use that the
navigator
media devices enumerate devayne devices
method to get hold of all of the media
devices that are attached to our phone
or laptop or whatever but this will
include video cameras as well as
microphones so we then filter this to
get only devices that are video input
devices and then we return that and if
the if this API isn't available then get
cameras we'll just return an empty array
once the UI knows that there is a camera
available it can call us to say that it
wants to start a stream and it can pass
in a device ID now this device ID comes
from the objects that will return from
get cameras and this allows the UI to
say that it specifically wants one of
those cameras so this allow
for UI features like switching between
the front-facing and rear-facing cameras
or just to pick an own camera from a
drop-down for example then when we
actually want to get the data from the
camera we need to use the Navigator
media devices getusermedia call to get a
stream and as a bit of terminology here
which is a little bit confusing if you
over loaded words so start off with a
stream is just a bunch of media
information it will consist of tracks
one or more tracks and those tracks
could be video or audio its own theory a
stream can have multiple video for
multiple video tracks and multiple audio
tracks in this case we're expecting to
get just video and I'll say why in a
moment
and we should just have one so we have a
stream and we have a track and the track
is the first video track now for each of
these streams and tracks there are
things called constraints settings and
capabilities a stream has both
constraints and capabilities the browser
has capabilities and a track has
capabilities constraints and settings
and then when we want to actually take a
photo if we're using the image capture
API but again takes another settings
object so I apologize if you get
confused with constraints and
capabilities and settings there are
there are several things called that
that's just the way the API is so the
first place that we're going to use this
is that in our actual getusermedia core
we pass in constraints for the stream
this this is our way of telling the
browser what kind of camera we want so
most of our constraints are defined as a
constant here at the top of the file you
can see that they we say that we want
video this is why we're not going to get
any audio because the audio property is
by default false so we're only saying in
one video we're not setting the device
ID here but we set it in the function
I'll share that in a sec we say that we
want the facing mode
be either user that is the cameras
facing the user it's a selfie camera or
environment it's facing away from the
user into whatever you're looking at and
we say that the width and height should
ideally be this is 1080p HD resolution
now with these constraints I could have
said that the facing mode was just user
I could have said I only want selfie
mode or I could have said that I want
the height the width and height to be
exactly 1080p resolution but if these
constraints can't be satisfied then you
won't get any you won't get any stream
at all so if I ran this with exact here
or having the just user on my laptop
here this would actually fail because my
laptop will only give me 720p resolution
out of the camera and it only supports
an environment camera I mean technically
it's facing the user but it describes it
as an environment camera so we're using
whatever device ID was passed in from
the UI layer to change the constraints
for the video so that when we all get
user media we're getting a particular
camera so that's the initial constraints
for getusermedia we also have various
settings we can put on the stream once
it's going so once our stream is going
we pull out the first video track of
that stream and we store it away we can
now set options second strengths on the
track when we want to set an option on a
track we need to make sure that it's
actually supported so there are two
things that we need to look at one is at
the top of the file here we have this
supported constraints object which we're
setting with navigator dot media devices
get supported constraints assuming that
we support the media devices API that is
this will tell us which constraints the
browser supports
so these are the ones that the browser
knows about it's things like white
balance exposure brightness zoom focus
distance things either these are the
things that we can set to change how the
camera is physically operating you know
what kind of
that we're getting back from the camera
and this is real-time this is affecting
the video feed that is coming from the
camera so now we know which constraints
the browser supports we also need to
take into account which constraints the
track that we currently have supports
because it might be different for
example on my phone the the selfie
camera doesn't have a flash it doesn't
support zoom or anything like that
whereas the environment camera does have
a flash so we can on that one will have
a different set of constraints so a
different set of capabilities that we
can apply constraints to so if we want
to get the available capabilities for
the track
that's another API call if we are
currently dealing with the track because
we might not actually have started our
stream and if that track supports the
get capabilities method because this is
relatively new method so this is a
feature detection if it sports both of
those things then we call that function
and it gives us the list of capabilities
for the track this is the same kind of
object as the supported capabilities for
the browser only now we know that it's
that specifically this track also
supports them so when we want to change
a constraint on this video feed we need
to check that it is a support constraint
so we check our support constraints that
it exists and that it is true we then
get the capabilities for the current
track and if that's also true then we'll
make a note of that we'll set in our
track constraints object that's
recording what constraints we want that
this particular value is set to this
name and then we'll apply all the
constraints that we've got there the
constraints object is just a little bit
weird instead of being an object with
keys it's an object with a single key
which is called advanced and that's an
array and inside that you put in more
objects so because you can't directly
map things because you could have
multiple entries for the same the same
constraint I would just have to I'm just
building this array here and then we
actually apply those constraints to the
track with newer browsers we can
actually use an API called the image
capture API and this
lets's direct the control actually
taking a photo but we still need to
support older browsers that don't
support this so there is a fallback way
which involves using a canvas so if we
support image capture we do one thing
but let's just quickly have a look at
the the fallback case so we create a
canvas and we set its width to the same
as the video that we're getting and then
we draw the video into the canvas so
that will just draw the current frame
into the canvas and then you can use the
helper there I talked about last time to
turn that canvas into a blob which we
can then store or manipulate on whatever
else we want to do one of the problems
with this is that getusermedia itself
for the video streams only supports up
to HD resolution and obviously if you
have a modern mobile phone your phone is
probably much better than 4k resolution
when it actually takes photos so this is
why we want to use the image capture API
so if the image capture API is supported
then we take that stream and we're going
to get out of its track and we're going
to create this image capture object for
that track and then the the simple thing
we can do is we can say that captured
take photo that will return as a photo
from the from the camera and this
because we're specifically telling it to
take a photo we use the full resolution
of the camera to get that photo so this
is already a huge improvement however if
we have a modern browser that allows us
to get the capabilities of the camera we
can also set some additional features so
here you see I'm checking this photo
capabilities object and I skipped over
it initially but in the start stream if
image capture is supported I set the
photo capabilities object to be the
return result of get photo capabilities
on an image capture API for the track
now these photo capabilities it's a much
smaller list of things
it's just something called fill light
mode it's red eye reduction and then
image height and image width in my app I
don't actually want to change
image height and image width I want the
full resolution but the other two are
pretty interesting so if this track
supports fill light mode that's firing
the flash so the options for fill light
mode when we actually take a photo
assuming it's supported will be either
auto off or on so on meaning always fire
the flash auto meaning do it depending
on the light levels whatever the camera
would normally do similarly for redeye
reduction so the way we do that is the
intake photo if we have this photo
capabilities object and the fill light
mode includes whatever we're trying to
set the flash to then set the settings
for light mode to be that and similarly
if the photo capabilities redeye
reduction is controllable then we'll
allow whatever our redeye reduction
setting is to be set there and redeye
reduction and flash are just public
properties on the camera helper so
though they can be set from the UX layer
whenever so then we can pass that
settings object to take photo and it
will fire the flash if we arrest it to
use the red eye reduction you'll do
whatever one so that's settings
capabilities and constraints for streams
tracks and they're taking a photo
hopefully that's all clear now yeah
there we go
proper access to all of the features of
the camera thanks very much for watching
and until next time happy coding thanks
for watching if you like to see more of
our videos click here and see you again
Cheers</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>