<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>EdgeConf 3: Pointers and Interactions | Coder Coacher - Coaching Coders</title><meta content="EdgeConf 3: Pointers and Interactions - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Chrome-Developers/">Google Chrome Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>EdgeConf 3: Pointers and Interactions</b></h2><h5 class="post__date">2014-03-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lRY8jHmiIIw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text"> 
MARTIN BEEBY: But
not in a pervy way.
Chris will be disappointed.
Anyway, we're
going to be talking
about pointers and
interactions, a topic which
is very close to my heart.
It sounds like a dating
website, doesn't it?
Anyway, I'm going to start
this by introducing the panel.
We've got, over here,
the lovely Patrick Lauke.
I always get that name wrong.
Have I got that name wrong?
Patrick Lauke?
[INAUDIBLE]
Jorik Tangelder
at the end there.
Jorik is the author
of Hammer.js,
which is a library I
used very recently.
It's wonderful.
It's dealing with touch
events and now pointer events
as well, more recently.
He builds mobile
apps for a living
and loves to experiment
with new techniques.
Patrick-- I didn't
tell what you actually
did-- you used to work
for opera, didn't you?
Being a Dev Row guy?
But he's now an accessibility
consultant at Paciello
and member of the Pointer Events
Working Group and W3C Touch
Events Community Group.
We have Pete Smart over here.
Pete is very much
UX focused, and he's
done a lot of incredible
work with ViziCities
recently with his partner.
PETE SMART: Partner-in-crime.
MARTIN BEEBY: Yeah, I
was going to say partner.
I didn't know what to say.
Not partner.
Rob Hawkes, incredible 3D
visualization [INAUDIBLE].
You should check it out.
And he's also the author of
&quot;50 Problems in 50 Days,&quot;
which describes my
last 50 days organizing
this portion of the event.
And then finally we have
the wonderful Steve Workman.
I've known Steve for
an awful long time.
An incredible web developer,
and very insightful young man,
I find.
So next I'm going to
pass over to Rick Byers.
RICK BYERS: Do you want
to introduce me first?
MARTIN BEEBY: Rick
Byers is from Google.
He's a wonderful man.
And he'll be talking about
pointers and interactions.
Thank you very much, Rick.
RICK BYERS: Thanks.
[APPLAUSE] Let me get my
slides displaying here.
 
Sorry about this.
OK.
 
MARTIN BEEBY: Yeah, we
did it on purpose, sorry.
 
RICK BYERS: All right.
I'm here to talk pointers
and interactions.
To me, this is about how do
we involve input on the web?
First of all, even just
looking backwards and saying,
how do we catch up
to native mobile?
I think there's a lot
of ways that input
is better on native
today that we really
need to catch up on the web.
But also looking
forward and saying,
what are the things
that are coming?
And how can we prepare
the web to make
sure we're ready for all sorts
of new ways of interacting
with content and applications?
So I'm going to talk about
this from my biased view
on the Chrome team.
I'm going to talk about the
ways in which we classify
the problems and the priority
that we're thinking about,
so that you all can
tell me I'm stupid
and that I should re-prioritize.
But first and
foremost-- this probably
comes as no surprise-- our
top priority in our team
is performance.
If your site doesn't
perform well to your input,
it doesn't stick your
finger, if things
don't respond immediately
when you touch them.
Or even on a laptop,
if things are
janky when you're scrolling.
That's a real problem
for user engagement.
So this has got to
be the top priority.
But we've also got a big
problem with richness.
Today, it's possible to build
really rich interactive user
interfaces, and
frankly, it's easier
to do that on other platforms.
We've got some problems on the
web where certain types of UI
are really hard to do well
without reimplementing
all sorts of browser
features yourself.
And last but not
least, we've also
got a problem with rationality.
And this is just the idea
that [? the pit of ?] success,
the idea that the obvious
thing should really ideally
be the correct thing
most of the time.
And you don't want to have a
million different foot-guns,
where every time you try to do
something that seems obvious,
you shoot yourself in the
foot, something breaks.
And like I said, I
think we're not here
on the web for any
of these things yet.
I think we've got
to fix all of this.
But what's really
worse is that there's
these trade-offs between them,
and we keep dancing around.
For example, over
the last couple
of years with all of
the focus on mobile,
I think we've really put a lot
of emphasis on performance,
and I think we've lost some
richness and rationality
as a result, by not focusing
on that at the same time.
So I'm going to talk about
three-- a big problem for each
of these areas.
And just talk about how it's
affected performance, richness,
and rationality.
So the first one's
been talked to death.
I'm not going to
spend too long on it.
But obviously touch latency
is definitely too high.
On a touch device, we're trying
to present a physical illusion
that you're manipulating
a physical object.
And as soon as the
latency goes up
so it's not sticking
to your finger,
or if it's variable latency-- if
you've got latency jank-- then
it destroys that illusion.
And that's a real problem.
It really causes
engagement to collapse.
And there's been a
ton of improvements
here over the last
several years.
Probably the biggest
architectural change
to the platform, even those
not exposed to the apps,
is that most browsers now will
scroll on a completely separate
thread to try to insulate the
scrolling from what else is
happening in the browser.
There's been a ton of talk
about the 300 millisecond click
delay.
And we've talked that to death.
But the one thing I'm
most excited about here
is we've just got-- we now have
a standard way of turning off
the click delay on
individual parts of the page
without turning
off anything else.
You just turn off
double-tap zoom.
We've got a new CSS property
called Touch Action that's
shipping in Chrome 35.
Firefox is going
to have it soon.
Touch Action does
much more than this,
but this is one
of the things I'm
excited about that Touch
Action lets you do.
And we continuously have
been making tooling better.
I think we still have
a long way to go.
On my team we've been
working, for example,
on trying to accurately
correlate input to painting.
It's not good enough to just
look at your timeline view
and say, oh I'm getting
stuff at 60 frames a second,
if that painting is happening
a second behind when
the input came in.
And so we've done a lot of work
to plumb latency tracking data
down through Chrome, all the way
from the input events coming in
from a kernel, down to
when we tell the GPU, hey,
we want to display this.
And I'm hoping that
eventually we'll
get that exposed in a more
friendly way through DevTools.
But for now it's there
on Chrome Tracing
if anyone wants to play with it.
So then, in terms of
richness, the biggest-- this
is where I'm starting
to focus a lot more.
I think we've neglected
this space for a long time.
And as we've done
all these fast paths,
we've taken control
away from developers
in the name of performance.
And frankly, I think when we
look at it really closely,
it's not necessary.
There's ways that we can
give developers control
but still maintain high
performance, or at least
high performance by default.
And this comes back to the
Extensible Web Manifesto
that I think has been
mentioned before,
the idea that as a
community, we're never
going to evolve the web to
be the platform that we want
it to be.
To compete with all of
our other platforms,
if we keep spending
years focusing
on these high-level
features and trying
to agree between browsers
and the high-level features.
Instead, I think we
really need to focus on,
what are the core primitives?
What's the kernel
that all browsers are
going to expose so that
we can enable jQuery
in different frameworks?
And all of you to innovate
like crazy and try new things,
but they're having to wait for
the long, drawn-out Sanders
process and agreements between
all the browser vendors.
So one of the things that really
bothers me here is, scrolling
is so critical in
this experience.
But if you want to
customize it-- for example,
there's a great
library called iScroll
that customizes scrolling
in various ways-- you've
got to reimplement everything
yourself that the browser was
already doing, and
you're prevented
from doing what the
browser's doing.
You don't have access
to this separate thread
I was telling you
about, so everything's
going to be blocked
on your main thread.
It's going to make it jankier.
Depending on the
browser, you may not
have access to the precise
physical pixel locations
of where the user's
finger is, because
on most-- on mobile devices,
anyway-- one CSS pixels more
than one hardware pixel,
only the browser knows,
in many cases, where exactly
the input or the scroll offsets
are.
And you might also not have
as accurate information
about velocities
that the browser has.
I think we need to fix all
these low-level primitives so
that somebody like iScroll can
go and build scrolling that's
just as good as what
the browser does,
with some additional features.
Even more immediately
than that, there's
a whole bunch of UIs that
are becoming really popular.
Effects like Pull To
Refresh, where when
we've made these trade-offs in
the browsers and we've said,
scrolling performance
over all else.
Whoops, I guess that means I'm
talking too long on this slide.
Scrolling performance
over all else
means that we've taken
control away from you.
You can't say, well,
I want to scroll,
but now I want to switch to
drawing my own little custom
effect that has different
physics than what
the scroll normally has.
And I want to keep that
lockstep with the scroll that's
happening.
And I want you to tell me when
user lifts their finger so I
can change that effect.
Similarly, IE has
a great propos--
Microsoft has the Snap
Points feature in IE that
is a great feature.
But we should be
talking about, how
do we enable the web so that
people can innovate on features
like that without always having
to come to the browser-- always
having to add every
new UI feature
at the top of the browser.
And one of the ones that
really bothers me here
is scroll synchronization.
When the scrolling is
happening on another thread,
it's a problem for
rationality because you're not
used to having to reason
about multi-threaded behavior
in your UI.
But it's also problem
for richness and the UIs
you want to present.
Everything from--
on a mobile app,
you never see checkerboarding.
Why does a web page always
have to checkerboard?
Maybe for your app, you might
want to trade off and say,
I would rather have scroll
jank than checkerboarding.
Or I would rather
have the ability
to have perfect parallax,
where where things are updating
exactly with the
scroll position,
rather than doing the
scroll asynchronously
to those sort of things.
And I think it's been naive of
us as browser renderers to say,
we know the best default
for all scenarios.
We're going to trade off
performance and richness
for you and not give you
any say in the matter.
I think we really
got to start giving
control of that to developers.
And also, how exactly should
touch behave while scrolling?
We're making a big change
here to this in Chrome 35.
I won't go into the details now,
but you can read all about it.
But in Chrome, as soon
as you start scrolling,
we stop sending you
events entirely.
And that's terrible
for richness.
It means you can't build
UIs like Pull To Refresh.
OK, now in terms of
rationality, one of the topics
near and dear to my heart
that people bring up
a lot is, why should
we have to build--
why should we have to
write different UIs
for different types of input?
We should be able to
make it easy to have
a single set of APIs
that work for all sorts
of different types of input.
IE has an API called
Pointer Events
that you've probably
heard about.
We've been working
on standardizing it
for a long time.
And we should be
talking, is this
the API that we want the
entire web to move to?
And in doing things
like that, how
are we going to make it so
that when we're exposing
these low-level input
APIs to web developers,
how do we make sure
that they're thinking
at a high enough
level of abstraction
that their sites are
still accessible?
That they're not
just targeting, I've
got to worry about
mouse and touchscreen,
but we want people
to be thinking
about screen readers and all
sorts of systems technology
as well.
And then, looking
further out, there's
all these exciting types
of input on the horizon.
Even old school stuff like
directional pads on TVs,
the web doesn't
handle very well.
Microsoft has perceptive pixel,
these gigantic touchscreens
where you can have multiple
people touching them at once
and track 100
different touchpoints.
How should the web
interact with that?
Or what if you've got multiple
users, each with their own Wii
remote, interacting
with a web page?
Voice input, depth cameras,
Google Glass heads up displays,
touch screens like
the Galaxy S4 that
can tell when your
finger's above the screen.
Haptics-- what if
your touchscreens
can start to give you
physical feedback?
And what worries
me is if we block
all of these new
technologies on, let's
wait until we have consensus,
let's wait until we can all
agree in standards
bodies, how this
should be exposed
to the web, we've
got a chicken and egg
problem and we're never
going to innovate.
And so we need to be talking
about what else is next
and how do we prepare the
web so that we can innovate
without having to have
consensus first really.
And part of this
is just discussion.
That's what Edge is all about.
I'm looking forward
to this discussion.
But there's also
plenty of places online
that we should be
discussing this.
We've just spun up the
Touch Events community
group, which is a group
within the W3C for people that
are have questions or
issues with Touch Events
and want to see Touch
Events get improved.
And of course we're always
interested in specific bugs
on Chrome.
That's it.
MARTIN BEEBY: Wonderful.
Thank you very much.
 
I'm really sorry for
that introduction.
I should've explained some more.
The reason I didn't really
give a good introduction
is because I wrote it by hand
and then when I got up here,
I couldn't read it.
Rick is an engineer working on
touchscreen support and crime.
Remember, the point is Events
Working Group and the Touch
Events community group.
Just to clear that one up.
So first up, we have
someone like Andrew Betts
with a question.
Is Andrew there?
 
You
ANDREW BETTS: So, web apps
that use touch gestures can
have problems with iframes
swallowing Touch Events.
Is the web developer
hamstrung by their inability
to exert complete control
over user interaction in ways
that native app developers can?
MARTIN BEEBY: I
guess we do give over
a lot of power to third
party control, third party
website and iframes
to the browser-- what
do people think about that
kind of giving up control?
Should developers
have more control?
Start with maybe Patrick.
PATRICK LAUKE: It's
a difficult one.
Developers will always
want more control,
be it with CSS stuff--
I want to control
exactly how my users will
experience the console I'm
creating-- and the same way
with the inputs, as well,
so output and input.
There is an argument to be
made that at certain points,
we should limit what the
developer can do so they don't
completely mess up the
conventions of the platform,
for instance.
You don't want
every single website
to just behave in a
slightly different way
and the user has to relearn
it just because the developer
thought, hey, this
is really cool.
I can do scroll jacking
but now extending
it to touch gestures and
everything else as well.
There should be some
kind of convention.
But on the other
hand, the topic we've
touched on at the
start-- touched on,
haha-- is that-- there'll be
a lot of these cheap laughs,
I promise-- is that, yeah,
we do want to innovate.
So we can't at
the same time also
say, well, we have-- I'll
say &quot;we&quot; even though I'm not
in that fold anymore-- we as
browser developers know what's
best, what's the best
interaction model.
Developers will want to
experiment with new things.
So it's trying to
strike the balance,
but there will be
situations where,
probably for
security reasons, you
don't want to have websites
that completely take
over the interface
and start showing
things that, the user thinks
they're doing one thing
but it's actually
doing something else
in the background.
It's a tough
balancing act to make.
MARTIN BEEBY: Jorik,
you're developing Hammer.
Do you want to give more
control to developers?
How does that work?
JORIK TANGELDER: I would like
to see a lower level API.
When a new device comes up,
like the Mio or the WiiMotes,
I just want to get that
information in my browser
so I can build my own
gestures and then don't
have to wait for four
years or something.
That's really long.
But when it arrives in
the browser for everyone.
RICK BYERS: So I'm
obviously a huge fan
of giving developers
more control.
But to play devil's advocate,
the scenario they mention
in this question
was about iframes.
And we actually-- I
think it's fixed in
all our stable releases
now so I can mention,
we had security bug
in Chrome for a while.
Actually, quite a long while.
Where the touch of any
API will tell you what
other fingers are
currently down.
And so it was actually possible
for an ad or something running
in an iframe, if you touched
on it, to receive information
about where your
other fingers were.
And that was a potential leak of
privacy and a security concern.
And security is a
thing that-- I'm
saying we shouldn't focus
as much on performance,
and we should be trying to
maintain our performance,
but give the developer
more control-- security
is the thing that
trumps all of that.
The web fails if the users
don't have confidence in it.
It's the unique
strength of the web.
MARTIN BEEBY: Were
you of the opinion
that, with gestures, you
should be giving developers
more control?
You said that in
the talk as well.
You want to give low-light APIs.
What are your thoughts,
Pete, in terms
of a UX around web design
and web development?
Do you think we should--
is there a benefit
to be had for gestures
which universally,
across an operating system
or across a device-- is there
a reason why developers maybe
shouldn't be allowed to play
around and fiddle around
with interactions?
PETER SMART: Interesting.
The conversations that I
have with developers often
go like this.
Guys, it would be great
if we could come up
with this really
innovative new feature.
Why don't we try--
scrolljacking for example.
Why don't we go into
that kind of realm
and see what's possible with it
and how we can surprise these,
and get them to reconsider
the experience that they're
looking at?
I think there the
common response
that I get is that we
don't want to override
the natural default
of the browser.
We don't want to confuse people.
I actually, after
many arguments,
I probably side
with the developers
on this particular front.
I think that although I,
as the user experiences,
I want to innovate, I
want to create experiences
which people find themselves
immersed in, surprised
by, and excited by,
I think that there
must be conventions which exist
across different platforms,
across different browsers,
which are therefore expected
because not everyone
wants to be surprised.
And not everyone wants to have
this immersive experience.
Most importantly, people
don't want to be confused,
and I think that is
the bedrock, really.
RICK BYERS: I think
one of the reasons
that developers respond that
way is, all too often they can't
do the one little thing you want
to do without reimplementing
like, the fling
physics or something.
And to me, this represents
a fundamental lack
of layering in our platform.
And many other platforms-- for
example, I know on Android,
you can replace little classes
or hook into the process
and not have to reimplement
everything yourself.
So you can get the native
look and feel, but also
customize it slightly.
And I think we failed
at that in the web.
PETER SMART: Interesting.
I think one good example
where, potentially, there
has been some
improvements, just with
some little personal
experiments maybe.
And-- I don't know if you
guys experience this--
but you're scrolling
through a web page
and you suddenly get to
a fullscreen Google Map.
And you're on your phone.
And suddenly, you're no
longer scrolling the page
and you'll suddenly
inside the map.
And that's an area which
we've tried to innovate in.
And we've tried to come up
with a hack around that which
allows you to declare when
you would like to interact
with the map and actually
when you are still scrolling.
And I think there
should be space
to allow developers, designers
to innovate in those areas
and try to break away from
the iframe suction of doom
that you go into.
But yeah, there's
got to be a limit.
I think convention's
also really important.
MARTIN BEEBY: You're
quite a good mixed bag
of dev and design.
What are your thoughts?
STEVE WORKMAN: So,
with things coming
into-- with all the Touch
Events going into iframes,
one of my concerns is, if I
were to make a web component we
talked about today, if I was
that web component developer,
I would obviously
want all the touches
to be sucked into
what I was doing.
Because I've said,
OK, I really know
what this component needs to do.
So I should have
control at this point.
Why are you trying to do
the rest of the thing?
The more this goes on
with web components
and with other areas--
advertising, of course,
iframes-- Maps is
a great example--
the more this happens,
the more important
that actually having that kind
of override is going to be.
So that when you
do, as developer,
know better-- and quite often,
you do know better-- then you
can override that
and you can actually
make a difference to
the web application
and improve the user experience.
MARTIN BEEBY: Andrew,
do you want the mike?
What's your thoughts on this?
Has anyone got any
thoughts on this?
[INAUDIBLE] Because you look
like you're doing something
else there, Andrew.
Pay attention. [LAUGHTER]
AUDIENCE: So just
kind of touching
on your point,
[INAUDIBLE] the idea is,
actually a lot of times
with the interface,
the developers
building it know best.
For example, your tangible
example about the Google Maps.
We actually implemented
the exact same thing.
It was a responsive website,
there's a big old map.
But what we did is once you
went into the mobile layer
and snapped
outwards, we actually
changed the Google Maps so
it was no longer draggable,
the interface gets changed,
it's now double-tap to zoom.
But I think that you
can only really do
that at the developer level.
You can't really do
that automatically.
I think the browser has too
many assumptions, because there
could be other pointers
where I don't want
that to happen for
whatever reason.
Maybe we would want an
immersive experience.
I think there's a notice
about the developer making it,
to make decisions on that.
STEVE WORKMAN: Absolutely.
You have to design
around what you're doing.
A while back I did the Met
Office weather observation
website and the
mobile view for that.
And most of that is
a bit Google Map.
And the really important thing
was on the different browser
sizes actually giving you
space for your fingers
down the sides of the map
so that you could still
interact with the map but
you could also still get out.
Because otherwise you're just--
literally, you are stuck.
MARTIN BEEBY: Next question,
Andre [? Brovens? ?]
In the front here.
A nice, simple mike up there.
 
AUDIENCE: Just a
question about gestures.
Is there a case
for custom gestures
that users would be
unfamiliar with, rather
than standardizing,
a set of gestures
that are semantically
well understood?
MARTIN BEEBY: Jorik, your
library's a lot about gestures.
What do you think?
JORIK TANGELDER:
Yes, you should be
able to write custom gestures.
But it also makes sense to just
use the system default gestures
because the user is
expecting-- like, you can swipe
and it acts the same on that
page like the other page.
But if you want to write a
custom gesture like swipe
with a weird thing--
I don't know--
you should be still able
to handle those things.
MARTIN BEEBY: It's really hard.
Writing gestures is really hard.
I've not done it
so much for Touch,
but definitely with
things like Kinect.
Trying to understand
what someone's doing
and how it follows through.
Especially if there's
a complex thing,
it's really, really
complicated to do.
It's maybe not for
the uninitiated.
Is that your experience, Rick?
RICK BYERS: It can be complex.
What we often find is
that the UX is actually
harder than the engineering.
That coming up-- we
actually had a bunch
of fancy gestures,
touch gestures,
when we first introduced
the Chromebook Pixel.
And we put a bunch of
work into engineering,
but users didn't
know about them,
they were hard to discover,
it's hard to train
people to use them.
And ultimately, we
said we should just
stick with the simple things.
That said, I think it's
essential-- we can't innovate,
we're never going to come up
with what the new interaction
modes are unless we
give people the power.
There's going to be some--
the next viral app that's
going to have some cool thing
that you do in your game
to make it do
something different.
It's going to become a standard.
MARTIN BEEBY: That's
an interesting point,
but are developers
necessarily the best
people to make decisions
about gestures?
Generally, they're probably not.
And if you give
people really-- I
don't mean to be
offending anyone,
but I am a developer
myself-- I think
it's very difficult to write
gestures which are well
understood across the system.
What do you think?
PATRICK LAUKE: It's definitely
more of a UX question,
I would say, because from
the technology point of view.
Not in pointer events as
such, if we talk specifically
about the Microsoft
implementation.
But there is a separate
power that Microsoft
has in IE that isn't
specced to the W3C, which
is all about the gestures
and how to actually write
programmatically
what a gesture is.
Picking out, say, two fingers
that you've put on the screen
and then following any
changes between them,
the angle, et cetera.
So technically, I think
that is not an issue.
We can, and we do, with
libraries like Hammer,
write our own gesture code.
And it really is more a
case of, are they standards?
Luke W. Has documented a lot
of the standards, gestures
that you get on a
variety of platforms.
There are similarities,
but also quite
fundamental differences
in some cases.
If you go from iOS to
Android, for instance,
there are a lot of pinch-zoom,
that kind of stuff.
Nowadays, we know about it.
It's become common knowledge.
But when it was
first introduced,
nobody had that understanding
that that is there.
UX-wise, there is / the
argument of how do you
teach your users that
there are gestures?
How do you hint at gestures
without doing a big--
before you can use the app,
here's a 10 minute tutorial
on how you move, how you
shoot, how you go into
your inventory, kind of stuff.
It's more of a softer
issue, I would say.
So from the technical point
of view, gestures are here.
We can create gestures.
We can hijack pointers.
We can hijack finger movements.
But should we, is usually
more the fundamental question.
MODERATOR: Steven,
you're quite involved
in the London web standards,
meet-ups and groups.
Do you think we need
more standardization
in this whole
[INAUDIBLE] of gestures.
There's an awful lot of
proprietary technology in this.
Is there maybe room
for more collaboration?
STEVE WORKMAN: I
think so, definitely.
So there's a lot of work going
in the different standards
organizations and different
roots going through this.
Obviously, Microsoft's
implementation is one.
Another is one from Apple, who
we haven't talked about enough
yet today-- who are
doing this spec called,
I think it's Mondo UI?
We talked about it last night.
UNKNOWN: Indy UI.
STEVE WORKMAN: Indy UI.
That's it, which is not a
gesture thing specifically,
but relates to actually
system-wide commands.
Like if you were to
do Undo, it would also
trigger an undo kind of
action, or a named undo action
throughout the web, as well.
And that kind of thing
also then expands
into gestures at the same time.
So Apple's going a
completely different way
of this from Microsoft.
And at the end, these people
need to talk to each other,
and we need to standardize
something like this.
Otherwise, we are going to
get two completely different
implementations
that are probably
going to be incompatible
with each other.
PETER SMART: But is there
a case for custom gestures?
I would say definitely yes.
I would say that
especially when you
stop considering traditional
point and click interfaces,
and touch interfaces, and start
thinking about the new input
devices that are coming onto
market at the moment-- things
like the Myo,
which kind of reads
the electronic signals
in your arm, for example.
Things like the
Leap Motion, where
you're interacting in 3D space.
I think these give us
fantastic opportunities
to start exploring
custom gestures.
And some of the
work that I've been
doing that with the very
talented Rob [? Hawks, ?]
who should probably be
sat here rather than me.
Looking at how to do
gestures in 3D space--
you can't just simply
kind of recreate
touch gestures in those spaces.
What we're trying to
create are 3D cities,
Sim City for real life.
And what we're
trying to look at is
how you extrapolate
out a city, and how
you're going to break it
down, and see different parts.
So we're talking about
gestures like this,
where you're able to
pull apart things,
like being able to pull apart
a watch, that famous diagram
that we all see.
And I think there's a real case
for looking at-- especially
if we start to look
at new forms of input,
to look at gestures
which are more suited,
rather than kind
of replicating two-
dimensional gestures
that we have now.
MODERATOR: Give
developers power.
Standardize more.
Talk more.
So next topic.
We are, I've got
Washaba written down.
Don't have a second name.
Sorry.
 
AUDIENCE MEMBER: Yeah, hi.
So my question is about
browsing the web on devices
like Google Glass or
Photo 360, the watch,
is a very different
experience, as they do not
have the traditional
methods of input.
How do we develop or
adapt our web pages
to better support such devices.
Would device information
in PIB be a good idea?
MODERATOR: You were
talking about new devices.
PETER SMART: I was
talking about new devices.
Well, I think I would
come at that question
from a high level, I guess.
I'm sure these guys will talk
about the technicalities.
I would probably
start with for me
what's most important
thing is the user need,
or the task that that
particular person is trying
to achieve on the device
that they are using.
And that we might not even know
the task, for example, so we do
need to be agnostic.
And I think we also
need to consider things,
like with Glass, for example,
the amount of retail space
that we have to display
information is very, very
small.
And the input, as well.
We're looking at
non-traditional forms of input.
We're talking about voice,
in that particular case.
If we're talking about
browsing the web,
on something like
Google Glass, what
should we be looking to do?
I think those three things
come into play at that point.
UNKNOWN: You were talking in
the original talk about this.
STEVE WORKMAN: Yeah, I mean,
I don't know about the UX.
I need somebody like Pete to
tell me what the UX should be.
But then what worries me
is how do we make sure
that people can experiment
with these things?
So there's a chicken
and egg problem, right?
We can't wait for whatever Glass
events, with W3C specification.
And maybe we can do this
with low-level APIs.
Maybe, if we can address
the security issues,
maybe we can give some limited
raw USB access to pages.
Or maybe we need something
like Indy UI events, just
a higher-level semantic.
And just be able to
say, hey, someone's
manipulating this object by
rotating it and scaling it.
And how they do
that on a watch can
be completely different from how
they do it on a touch screen.
And the browser, and even the
page, doesn't need to know.
RICK BYERS: There's also
this chicken and egg
problem with people.
You've got to design for
compatibility, right?
Most websites out
there are not going
to be designed for
your new watch.
So you have to come
up with something,
so that you can do a really
good job for existing content
that will still allow some
incremental extension to do
a little bit better for sites
that are really designed
for your special type of input.
PATRICK LAUKE: I mean, I agree.
We obviously talked about
this over the last day
or so as well, on the
pointer and its mailing list
while we were working
on the standard.
And on the one hand, it's
really good for a developer,
that we're now working
on these low level APIs.
I want to get access to--
is it a finger or a stylus
or is it three fingers and what
are the x- and y-coordinates?
And that's really great that
I can do that as a developer.
But in most cases, if I'm
developing an application,
generally I don't care how
the user's touching the touch
screen-- with a nose, or with
a finger, or any other body
parts, or any other device,
or if they're using voice.
What I really want is
usually the intent.
The user intends to
activate this button.
The user intends to get more
information or manipulate.
So even though
pointer events are
a step in the right
direction, I think
they might be a step too late.
Because now we already
have touch, and stylus,
and mouse, and everything else.
There's all these
new interfaces.
And really, we should be looking
more at high-level stuff.
Indy UI is probably a good
step in the right direction.
It's been a bit
sidelined because it
falls under the main--
oh, its accessibility,
so that's just for
blind people, and stuff,
so we're not going
to care about it.
I would say have a
look at Indy UI stuff.
It actually abstracts a
lot of these things that,
in most cases, unless you are
trying create something that
has a custom gesture, or
that really takes advantage
of something that can only
be done with Leap Motion--
if you really just care about
the user wants to open this
document, or manipulate this
thing, or resort this table--
You want the intent.
You don't want the actual
raw bits and bobs involved
of which key number was pressed,
which key code was pressed.
Because you're just going to
end up in a situation later on,
as a new device comes
out, that you're
going to have to
reinvent or implement
a whole new model, which is what
we've seen with touch events.
It was a great idea, but
all of a sudden you've
got a separate model
purely for touch.
And then something
new comes along,
and instead of-- which was
a really wise decision,
I would say, from Microsoft--
instead of then saying,
OK, we're going to
have Stylist Touch
and whatever else
Touch, Kinect Touch,
they decided to abstract
at least anything
that's pointer-like.
But it should really
go a step further.
It should have included
keyboard in my opinion,
and just be more device
agnostic, in general.
And yeah, Indy UI is
probably not perfect,
but it does move towards
more the idealized goal
of just looking at intent
rather than raw bubbins.
MODERATOR: So we've
got a question.
I think Chris was
first with his hand up.
AUDIENCE MEMBER:
[INAUDIBLE] that simulation
makes a lot of sense as well.
For example, I've written a
few things for Leap Motion,
and instead of doing
my own handlers,
I just file a click
event after that.
So if somebody wants to
build a UI, that text is in.
All they have to listen
is for click events.
And click events are great,
because they are also
keyboard events.
So instead of inventing, and
putting more and more event
handlers in our interfaces for
all the possible things, just
firing or generating an event
that is already listened to
is a simpler way in than having
another library around that.
PATRICK LAUKE: Absolutely.
It's like, Focus, Blur, and
Click are probably the ones
that, if you want to
do something today
that will still
work in a few years
time with whatever
other device is there,
they're high-level enough
and they're abstracted enough
from the actual &quot;What
is the type of input?&quot;
that it will work.
The reason that things
like, say, touch events
and even point events have
to fire other-- the mouse
compatibility events,
on-mouse over, on-mouse out,
and everything else--
is mainly because I
think we shot ourselves
in the foot at the time
when we started inventing these
very device-specific event
handlers.
And that's why now--
just to make sure
that the web, as it
has been written,
and 99% of the web that's
already out there, to make it
actually work on any new
device, any new input,
needs to-- at the UA level--
fire these compatibility
events.
Which is a shame,
and we don't want
to end up perpetuating this.
That in five years
time, we're going
to have to end up, when
we're using our &quot;Minority
Report until your arms fall
off&quot; kind of interactions,
that all of a sudden they
have to emulate from that.
It needs to emulate
pointer events, which
then need to emulate mouse
events, just to make sure
that you keep cascading back
to the old technologies.
MODERATOR: So just a quick
question, or quick comment,
from Red.
Red.
AUDIENCE MEMBER: Well,
it's kind of a question
rather than a comment.
So I might, I may be a
caveman, but the pointer events
and touch events,
I get they apply
to PCs and smartphones
that have browsers in them,
quite often made by the
companies that put the browser
vendor on the device.
This watch can get a
connection to the internet,
but there's no kind of
standards on that device.
And the Leap Motion, to get
it to talk to a web page
requires something
in the middle.
And to get the Kinect
to anything on the web
requires something
in the middle.
And I might be missing this,
but what devices are actually
talking web language, like,
web-compatible languages?
If we're talking
about these pointer
events for these kind of-- or
any kind of standardized event
for these kind of big--
yeah, Firefox, fine.
We're talking about--
AUDIENCE MEMBER: Does
it have a touch screen?
AUDIENCE MEMBER: The Minority
Report type interactions.
I don't-- I might be missing
it, but where are the devices?
TVs aren't getting there.
They're not.
UNKNOWN: TVs are going
to fire pointer events.
Sorry, kick me if I'm
dominating things.
MODERATOR: Now, we'll get
onto this later, I think.
So we'll go to a new question.
We've got Matt Andrew.
There's is a very similar
question coming up,
but Matt Andrew.
UNKNOWN: Boring.
 
MODERATOR: Sorry.
I'm laughing at him, not you.
 
AUDIENCE MEMBER: This
is from Patrick Lough
MODERATOR: Yeah.
AUDIENCE MEMBER: Touch events
are a very simple mechanism
for touch, whereas
pointer events
are a far more
detailed abstraction.
Should a browser vendor ideally
implement one or the other,
offer both separately or
some, kind of combined model?
UNKNOWN: As it's
my own question,
I'm not going to answer it.
MODERATOR: What do you think?
I mean, this must be a
problem for browser vendors,
because you've got to
support-- you say well,
I'm going to do
these touch events,
because that's what
everyone knows.
And it's so easy
to tie into what's
already happened with clicks.
RICK BYERS: And we
can't stop supporting
what we've already supported.
And it's not even as easy as
saying, &quot;Well, on the page,
we'll decide one or
the other, because you
have to make sure there's a
transition path for libraries
that operate within
an existing document.
So Google Maps, for example--
if we said the page is either
sending pointer events
or touch events,
then Google Maps
could never change,
because as soon as they
switched using pointer events,
the whole page would be
broken, or-- so there's
this difficult transition path.
I think Firefox is planning
on supporting both,
touch events and pointer events.
I'm really optimistic
about that.
I'm hopeful that, at some
point, we'll do that in Chrome,
as well.
We really want to make
sure that we're only
going to support APIs that
really last, that are lasting
APIs that eventually all
browsers are going to support.
And as long as the jury's kind
of still out on whether or not
all browsers are
going to support it,
we want to be a
little bit careful
to make sure we're not
introducing something
new that's largely
redundant, that
ends up not standing
the test of time.
So we're really looking for
feedback from developers
to make sure it's
something that developers
really want, and intend
to use for a long time.
Then we'll support
both, but we can never
get rid of the stuff we
supported previously.
STEVE WORKMAN: So that probably
is actually my problem.
So [? I'm ?] the UI
lead on Yelp.com.
It's a big website.
If we break something,
it's going to break hard.
So if I were to try
and convince my boss,
say OK, I want to implement
pointer interactions.
He's going to say OK,
that sounds great.
What browser is it supported on?
And I'll go, 10 and 11.
And I'll probably stop there.
Because right now, that's
where the support is at.
Chrome will get
there, which is great.
Firefox will get
there, which is great.
Apple, Apple Safari, which is--
let's pick an amount, 20 to 25%
of our user base--
they probably aren't
going to implement this spec.
And so if I'm going to
spend what it will take me--
a couple of weeks to completely
tear out our interaction model
and put pointer events
in there, instead-- even
pointer events with,
then, all the back
poly fills and all to
make it work on Safari.
That's going to be a massive
amount of engineering effort
for something which probably
isn't going to give me
any business benefit whatsoever,
except that it might get rid
of a 300 millisecond click time.
And that's going away with
something else, anyway.
So as from an implementation
standpoint-- and a purely
business standpoint-- actually
selling pointer events
is quite difficult.
Unless you're making
Windows 8, huh?
In which case go for it.
MODERATOR: And if you
are making Windows 8,
please speak to me at
the end of the event.
You're doing hand gesture.
You have this problem.
You must be dealing with
it all the time, well,
when you implement
pointer events,
about having to deal with
both at the same time.
GRAY PANTS: Yeah.
I'm telling you,
it's really-- yeah.
It's like when you
use a touch event,
it sends touch and mouse
right next to each other.
And sometimes in a
different, in other browsers.
So you can't really find out
when it's touch or mouse.
And then you have
the pointer events,
but it only works in the latest
Internet Explorer versions.
And it's hard to--
MODERATOR: I was
looking at the triage,
of all the different
events that happen
when a touch
happens on a screen.
You must live this.
GRAY PANTS: Yeah, yeah.
MODERATOR: Stuff happens.
And you have to put it in there
because of legacy behaviors
and so forth.
It's extraordinarily complex.
 
How do you work around
that sort of complexity?
Who heads it?
RICK BYERS: So first
of all, I mean,
we've got to do a better
job of documenting it.
I think there's a question
coming up about this,
but I think the touch event
standard didn't really
document any of this stuff.
It was all done
retroactively to try
to take the majority of what
the existing implementations had
done.
 
When I realized two years
ago that touch event standard
didn't have anyone from
Google on it, that I realized
there was a problem there.
It was all retroactively
trying to document
what had been done
without trying
to-- the real point
of standards is
to get the rationality
in there from the start.
So hopefully, I think
we're doing a better job
pointer BOUNCE.
We defined pretty precisely.
And then Patrick came along and
said, I don't understand this.
Let me write an
example, and show here's
the exact things
you should expect.
I think that's the way forward.
We've got to do some of this
stuff for compatibility,
but hopefully we can
rely on layers on top
to make it simpler.
So your UI framework, hopefully,
will say, on this UI framework,
you've just got to deal with
the events it generates.
And you don't need to worry
about different browsers,
and their compatibility modes,
and what the differences are
between IE and Safari.
And hopefully, we can
smooth a lot of this
over with frameworks.
PATRICK LAUKE: Now I think
a lot of the complexity
they get nowadays is
coming from the fact
that we've made bad design
decisions in the past.
I mentioned already
on mouseover,
on mouse out, and
everything else,
which is the reason
why touch events need
to fire mouse
compatibility events.
Because we, as developers,
were told years
ago to do a hover effect,
you just do an on mouseover.
And we've all been taking it
the cargo-culting it around.
And now that content is there.
It's set in stone.
And these new devices
need to use that.
If we could start
fresh, we'd change it.
But the reality is we are living
with both that legacy content
and we're moving towards
a multi-import world.
I mean, already now, we've
got devices-- a few years ago,
I was unheard of
to think, oh, you
might have a laptop that
also has a touch screen.
Touch surely means it's
a mobile, or then it was,
or it's a tablet.
And now it means,
the fact that you
get a touchscreen event means
there is a touch screen.
It doesn't say anything further.
[BELL RINGS]
And That's the fire alarm test.
MARTIN BEEBY: Yeah.
Sorry There's a fire alarm test
which we couldn't avoid today.
PATRICK LAUKE: Tension.
FIRE ALARM: (AUTOMATED VOICE)
The public address and fire
alarm systems are
about to be tested.
FIRE ALARM: OK.
MARTIN BEEBY: Do we
have your attention?
FIRE ALARM: (AUTOMATED
VOICE) The search message
will sound first.
Followed by the
evacuation message.
MARTIN BEEBY: Right.
OK.
MARTIN BEEBY: Should we
just run around and panic?
FIRE ALARM: (AUTOMATED
VOICE) Please
do not take any further action.
MARTIN BEEBY: No action.
PATRICK LAUKE: What
if a fire breaks out
while there is a test?
MARTIN BEEBY: No
one start a fire.
PATRICK LAUKE: We
didn't start the fire.
It was always burning.
FIRE ALARM: (AUTOMATED
VOICE) Attention.
An incident has been
reported within the building.
MARTIN BEEBY: I
repeat, it hasn't been.
Right.
 
PATRICK LAUKE: It's
like Space Invaders.
What's going on?
MARTIN BEEBY: This is
good, though, right?
FIRE ALARM: (AUTOMATED
VOICE) Attention, please.
Attention, please.
MARTIN BEEBY: Yeah?
FIRE ALARM: (AUTOMATED
VOICE) We have an emergency.
MARTIN BEEBY: An emergency?
It's not just an
incident, any more.
PATRICK LAUKE:
This has escalated.
RICK BYERS: Maybe
something is going on.
FIRE ALARM: (AUTOMATED VOICE)
The test is now complete.
Please advise your fire warden.
 
That last bit at the
end was brilliant.
If you had problems hearing
this, please alert someone.
PATRICK LAUKE: Accessibility.
I want to talk about that next.
So I can't remember
where I was, but yeah.
We are in a multi-import world.
MARTIN BEEBY: Well if
you can't remember,
let's just go to a new topic.
PATRICK LAUKE: Thanks.
Subtle way--
MARTIN BEEBY: Danny Croft,
do you have a new topic?
Because we have no
idea where we were now.
AUDIENCE: Are pointer
events over-complicating
touch interactions?
PATRICK LAUKE: No.
 
MARTIN BEEBY: All right.
I'd like to echo no.
We've got seven
minutes of this, guys.
PATRICK LAUKE: No.
They-- oh, sorry.
AUDIENCE: I'm aware
that moderators
were supposed to
write questions that--
MARTIN BEEBY: Yeah.
I'm aware of that.
And it probably sounds
like quite a good idea.
Also really, really
tell the people
which got the fire alarms.
Please don't do it.
Anyway.
So, yeah, I mean, are
they over-complicating?
There is an argument,
though, from a developer
point of view-- playing
devil's advocate here,
obviously, not talking on
behalf of my employers.
PATRICK LAUKE: As
a real developer.
MARTIN BEEBY: From a
developer's point of view,
it is quite a complex thing.
But is it better to kind
of, with touch events, you
kind of papered over the
cracks of, we needed touch.
We kind of got it in there.
I think with pointer
events, we took a step back.
And we looked at, well what
are all the different types
of interactions that
could happen on a device?
Realistic interactions that
could happen on a device?
 
Are we making too
much of it, though?
Because it's all good on
paper as an abstraction,
but is it too much of a
complicated abstraction?
RICK BYERS: I think if you
think of devices strictly like,
you've got tablets,
then you've got laptops,
then maybe you could
make an argument saying
pointer events are
over-complicating it,
but Microsoft believe that
that's not the only two
kind of devices
we're going to have.
We believe that
at Google as well.
And that we need to be prepared
for devices that are a hybrid,
and like there's a
continuum between them.
And then, the complexity of
having touch events and mouse
events on these kind of
devices is really high.
You probably don't realize the
extent to which touch events
implicitly capture to
where you start touching,
and mouse events
don't capture at all.
And the implication
that has for when
you remove a DOM element that is
in your event chain somewhere.
There's very subtle
bugs that result kind
of from these differences, where
pointer events unifies it all
and says, there's one model.
If you can, you
don't need to worry
about any of these
little things.
Just target this new model.
STEVE WORKMAN: The complication
comes from when you try and add
things on top of pointer events.
So if you, say,
were to look at TVs,
and you were trying to
add the D-pad into that,
switch gig currently doesn't do.
Then you're trying
to fire two models.
And that's where you kind of
get into much more complicated
stuff.
If you're still thinking about
the poly-fills and everything,
hammer.js solves pretty
much all that for you.
So it's just when you're adding
more and more things on top.
And, as we've
touched on already,
if you're trying add voice
onto the top as well,
you're trying to have a UI
that reacts to probably far
too many things, and that's
when it gets pretty much overly
complicated.
PATRICK LAUKE: Well I
think with pointer events,
the nice thing is, no.
It doesn't over-complicate it.
It actually simplifies
it in most situations.
Because already,
with touch events,
if you wanted to
properly separate it,
you'd have to listen
for the normal mouse
events, which work
in a certain way.
And then you'd
have to also listen
to touch events, which work
in a slightly different way
because Apple couldn't be asked
to just do something that's
a bit standardized, and had to
invent their own little things,
and then not
standardize it, and then
threaten people with
patents about it.
And instead with--
[INAUDIBLE] OK.
Anybody from Apple, here?
Whereas with pointer events,
the sanity has kind of returned.
It extends mouse events,
so any of your code
that you wrote already for mouse
just works in 90% of the cases.
And, again, as a
developer, you don't really
care how the user pressed on
something, activated a button.
Whether it is with a finger,
or stylus, or a mouse,
or whatever, it's Xbox One,
whether they use the joy pads
or the Kinects kind of
touch thing, or even voice,
I believe, it would just fire
the same events that says,
this button was activated.
So it unifies it.
However, if you want
to know explicitly,
this was actually caused
by a finger, or a stylus,
or a mouse, there
is a way of reading
the attribute of that
event that was passed on.
So that's what's been extended.
So it gives you the
best of both worlds.
You can write completely,
to a certain extent,
completely agnostic
input agnostic code.
And just forget about it.
And hopefully, if
new types of devices
come along that also use pointer
events, they will just work.
Rather than oh, I've only
got mouse and touch now.
I need to have x.
And if you do want to do
something very specific,
it still allows you to do that.
So I think it is
a good compromise.
It's not perfect.
As I said, keyboard,
for instance.
You still have to
handle that separately.
Or just go for
high level events.
Again, focus, blur, and click.
But it's at least a step
in the right direction.
And it's more sane than
inventing something
similar to touch events
but now just for stylus.
MARTIN BEEBY: OK.
We have a question
from the floor?
 
AUDIENCE: So Apple's touch
API has a really nice property
that the DOM element
that's received touch start
will always see the
touch end event.
And pointer events seem to have
regressed to the previous mouse
down, mouse up
situation, where it's
very easy to miss the mouse
up if the pointer just
ends up being
outside the element.
Is there a plan to have an
easier, less complicated way
for pointer events for
catching the up event?
RICK BYERS: So first
of all, the property
you're talking
about touch events
isn't quite as simple
as you alluded to.
I think the way you worded
it is actually incorrect.
It's not true that the event
that received the touch start
will receive the touch end.
The element that you touched
on received the touch start.
You might have an
ancestor that's
actually listening for the event
handler and receives the event.
And then if the DOM tree
underneath that ancestor
gets moved, the ancestor
will never see the touch end.
And you'll be surprised.
I've seen this bug in practice,
and people are really surprised
by it.
It's one of the disadvantages
of this implicit capture model
is that the programmer
hasn't told the browser what
element it really wants
to capture the events to.
The browser just says, well,
you started touching here,
so clearly that's the
element that wants it.
Even though it's really your
handler further up that really
cares about the event stream.
So pointer events
has an explicit API
called set pointer capture, that
lets you say, if what you want
is to track the finger no
matter what's underneath it,
then you call set pointer
capture and tell it,
here's the element
that I want to receive
all the events for as long
as this finger's down.
So it is one more step,
but it's at least explicit.
I think it's a better model,
especially because it's
the exact same model-- it builds
on top of the exact same model
that we're used to with mouse.
You still get pointer,
enter pointer, leave events,
that you can still
say, well, the user
dragged their finger off.
You just have to
remember to watch
for leaving as well as ending.
MARTIN BEEBY: OK,
I've got one question.
Natasha Rooney?
 
AUDIENCE: Hey.
So you guys have mentioned
a couple of these things,
so it's a sort of overall
question about what
might become mainstream?
D-pads, TV screens,
and multiple users
interacting on a single
screen are all scenarios
we currently consider
nontraditional.
Which of these will become
mainstream most quickly?
And why do you think that?
MARTIN BEEBY: Which will
become-- this is kinds of--
JORIK TANGELDER: All of them.
MARTIN BEEBY: I thought it was
your same question you had.
We always talk about
these new devices,
and these minority
report devices,
but they haven't got
a great market share.
Which ones do you think
can become mainstream?
RICK BYERS: If I knew, I
think our standardization
job would be so much easier.
We'd say, here's the future.
Let's design an API for it.
PATRICK LAUKE: Let's patent it.
 
RICK BYERS: Awkward.
PATRICK LAUKE: I'm allowed now.
I don't work for
a browser anymore.
RICK BYERS: But it's
because we don't
know that we have to
rely on the community
to innovate and experiment.
Try new things.
And see what comes from them.
 
MARTIN BEEBY: I mean, I really
want massive touch screens
to be popular.
I think there's a
real possibility when
you have like 80
inch touch screens.
A hundred points of touch.
I think that's really cool.
The collaborative web
experience that could happen.
I mean, Leap Motion.
What do you think
about Leap Motion?
Do you think it'll
ever become mainstream?
I mean, the problem I
have with Leap Motion is
it doesn't really
solve an issue.
If I'm like 10 centimeters
away from a screen, right?
I can touch it.
But with a Kinects,
you're 10 meters away.
You're solving a problem.
PETER SMART: OK.
Well we've got some waves.
But I'm going to have to debate
the ability of the Leap Motion.
I think it's got a
lot of great things
that you can't do with
touch, like depth for example
and being able to kind of zoom
in an out of things, which
I think are unique to that
particular type of input.
And what else would you like
me to say on that subject?
MARTIN BEEBY: What do you think,
what other devices do you think
are going to come out which-- is
there anything which you think
which actually, genuinely is
going to become mainstream?
Maybe voice?
I mean, that's not really--
it's kind of mainstream
on native devices, but on
the web, it's demo work.
PETER SMART: Voice
is getting there,
but the way voice
is actually getting
somewhere is the
Google Now stuff.
So if you've seen the Google
Where stuff this week,
and if you haven't
played with Google Glass,
the voice commands are, by
far, the best part of it.
And now that's starting to make
its way onto native Android
hardware through the
whole OK Google thing.
That's starting to
work really well.
And with the Xbox One as
well, it's getting there.
It's not quite right yet.
But it will get there.
So actually, the voice input is
going to start being important.
On the web, you're trying to
use the same APIs underneath,
but you don't really want
to talk to your computer.
It's more of a user
experience thing.
MARTIN BEEBY: Do you
have a quick question?
AUDIENCE: [INAUDIBLE]
Actually, I was going to try
and interject with--
AUDIENCE: What Remy brought
up earlier with regards
to where these kind of
devices, with the [INAUDIBLE]
that require custom
gestures and things of that.
I'm actually quite
in a unique position,
because I actually know
exactly where they are,
and they're
mainstream right now.
And that's a thing called
Digital Out of Home.
For those of you
who aren't aware,
it's basically any
digital display
you see that's an advert,
out on the street,
in the underground,
things like that, that's
an industry,
Digital Out of Home.
I'm experienced with it.
And there's a huge amount
of money going into that.
And that's actually a symptom
of how capable Chrome is.
Chrome is actually, and as well,
as I said, the other browsers.
But for some reason, Chrome
has managed to capture it,
is the actual platform
that's driving a lot of that.
Basically a lot
of those displays,
if they're ever interactive, are
basically a web browser screen.
And there's actually
direct implications
there where you do need to
come up with new gestures.
So that's things like where
hammer.js kind of comes in,
because as a tangible
example, when
you have a device that you're
swiping left to right with,
if it's a display on your phone
or actually a small device,
you swipe left to
right with one finger.
If you actually have
a large, touch screen,
and you walk up to it, you
swipe with two fingers.
We've actually noticed
that people do it.
They don't realize
they're doing it.
But that actually right
there is somewhere
where now the format, because
it's out in the world,
and it's actually
a large screen,
you now have to change the
way you're capturing gestures.
And if you're ever doing things
with regards to installations
and things like kiosks
and you have them
a lot in kind of
trade shows where
you have the digital
displays, they have games.
They're capturing weird
interaction events.
They're all powered
by the browser.
In the advertising industry,
I mean, that's today.
That's not like a weird
thing that's coming.
I'm not kidding, where I
work, every single month, we
have things like that going on.
And we're kind of
building these things.
MARTIN BEEBY: We're talking
about kiosks, though, and kind
of installations,
that's not mainstream.
That's not consumer technology
in the web sense, really,
is it?
We're using it
definitely as consumers.
But not--
AUDIENCE: That's the thing.
The technology's moved
into this non-web scenario.
But if you take for example the
[? Crosswell ?] TV ad network,
which is like the
visual displays you
get in the underground,
some of those
now are actually
Wi-Fi mobile enabled.
With your phone,
you can actually
get out and, through
the web medium,
interact with the display.
You're using web technologies,
but you're not on the web.
But actually, you're now
passively interacting with it.
You're now using
gestures on your phone
to control a big display.
MARTIN BEEBY: So
with that, I think
there's an awful lot
learned in terms of,
we should perhaps
talk a lot more.
PATRICK LAUKE: We should.
Let's take over
the next session.
MARTIN BEEBY: Standardization.
I think it's really good
actually, at this point.
There's now a lot
of collaboration
that's happening in touch
events groups and so forth.
And there's a lot to learn.
I don't think anyone
really knows the answer
to a lot of the questions
that we're asking.
Some of these, we're
going to figure out
as new devices, which will
become mainstream one day, Rem.
Never.
So I want to just
thank my panel.
And thank all you for listening.
We're going to have a break now.
 </div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>