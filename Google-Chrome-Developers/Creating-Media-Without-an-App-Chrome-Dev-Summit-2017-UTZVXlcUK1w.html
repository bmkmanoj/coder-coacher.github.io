<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Creating Media Without an App (Chrome Dev Summit 2017) | Coder Coacher - Coaching Coders</title><meta content="Creating Media Without an App (Chrome Dev Summit 2017) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Chrome-Developers/">Google Chrome Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Creating Media Without an App (Chrome Dev Summit 2017)</b></h2><h5 class="post__date">2017-10-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UTZVXlcUK1w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hey everyone so as you heard my name is
matt scales i'm with the web developer
relations team at Google today I'm going
to talk to you about creating media on
the web so you heard earlier from Tao
about how there are billions of users
coming online and how the mobile web is
a great way to reach them one of the
things those users are going to want to
do is to create and share media now the
web has always been a great platform for
sharing sharing what we're doing things
we found things that we've made you just
post a link to whatever it is and anyone
can just click it and go straight to it
but until very recently creating photos
recording videos editing and filtering
these results was pretty tough if not
impossible now thanks some pretty good
progress over the last couple years you
can do all of this on the mobile web
right on the device new API is of a
landed that let you create rich media
content so I'm going to talk today a
little bit about the challenges that are
still being worked on and I'll look
ahead some even more exciting things
that are coming in the future but first
I'd like to introduce Jenny and pizza
from Instagram who are gonna tell us how
Instagram are using these features on
the mobile web today
in strands mission is to strengthen all
the relationships through shared
experiences as we continue to connect
more of the world our biggest
opportunities are emerging markets
countries where more and more people are
starting to use the internet through
mobile devices to the send Instagram is
investing in mobile web support where to
make it easier for people to use
Instagram where devices have limited
storage or connections are unreliable as
a media heavy application how do we
deliver the Instagram experience within
the limitations of web today we're going
to share some of the best practices most
of you probably recognize Instagram as a
native application what most of you
probably didn't realize until this
conference is that starting this year we
started building out our mobile web
experience this is Instagram calm thanks
the existence of new API s Chrome will
prompt you one certain criteria are met
to install progressive web application
like ours to the home screen in the
background now I'm going to show you a
demo of our progressive web application
here's a native app that you're familiar
with like the native app you can see
that the progressive web app gets a
stone icon when you click it it loads
the same experience as Instagram comm
without looking like it's in a browser
this allows users particularly in
emerging markets whose phones or
connections may limit them from
downloading using or wanting to use
native app to get a true app like
experience using our web product you
have stories and you have the content
you follow over the people in the feed
and you can take photos as well so I'd
like to take a photo of all you lovely
people can we get the house lights on
please
all right then you can see that we have
the filters so let's post this
not the best that sleep
and shared out
and there you go so back to slides
please
Thanks now we're gonna talk about the
technical details about how we
implemented several these features
specifically we're gonna deep dive into
how to add your progressive web app to
the home screen
video playback using adaptive streaming
image capture and filters on web as well
as offline support one thing to note is
that the majority of the features were
talked about today is Android only
however because our target audience are
emerging markets and emerging markets
are almost exclusively Android markets
this didn't really limit our reach so
let's talk about how to get the phone to
prompt this Add to Home screen there are
a few requirements to make this work
first you need a web app and manifest
like this one the required fields are
name which is used in the prompt short
name which is used on the home screen
start URL to load and icon that's used
on the home screen in addition you need
to have service workers registered on
your site
for this we use work box a set of
libraries and abstractions for service
workers that Jeff talked about yesterday
and ever wrench in as well and this is
registered through registered router
that registered route as a prerequisite
for using a service worker your site
must be loaded in HTTPS something we
were already doing and finally the
person coming to the site needs to
trigger an engagement metric right now
Google has to set to 20 or 30 seconds on
the site for the pump to trigger for
testing and development though you don't
want to be waiting 20 or 30 seconds
every time so you can get the prompt to
trigger without linkage metrics by going
to Chrome flags and then turning on
bypass engagement check mode this is
what the prompt looks like unfortunately
it doesn't allow for much customization
and doesn't display information about
what you're adding now Owen did mention
yesterday that there's a new add to home
modo flow coming but since that isn't
available yet and despite requiring an
additional click we implemented our own
modal to give more information to users
before having chrome prompt film
in order to accomplish this we added an
event listener before registering the
service worker this event listener
listens for a before install prompt
event and prevents the event from
triggering and then instead of in the
inside saving an off to be triggered
later so we deliver a modal and when the
user clicks add we then show the chrome
prompt which was previously deferred and
that's how you can give your mobile web
users the feeling of being a native app
now Peter will talk about how we made
this experience more engaging with
optimize video performance yeah thanks
Jenny
in areas where Network resilience and
reliability issues are commonplace video
playback is generally a poor experience
you have a choice between a low quality
video or a higher quality video that
buffers and stalls one solution for this
experience is adaptive bitrate streaming
with ABR the video is broken up into a
sequence of segments where each segment
is encoded in multiple bit rates here we
have high medium and low a manifest is
then created which contains detailed
information of each segment and how to
fetch it now using the segment the
clients are then able to decide whether
to switch a to a higher or lower bitrate
depending on current network conditions
now our first step of integration was to
choose a client-side player we were
looking for something that was open
source supported open standards and was
extensible we found shaaka player was a
great out of box experience great out of
box solution for our initial experiments
so in this video is a comparison between
our existing video player on the left
and a Shaka player on the right they're
actually playing at the same speed but
you can see the video on the left
buffering installing as we transition to
2g network conditions the adaptive video
on the other hand is able to continue
with smooth playback now let's look at
how we integrated so this is a standard
integration you'd create a new
soccer player instance and pass it a
video element you then load the manifest
file that I described earlier so with
this approach there's actually another
round trip just a fat fetched the
manifest file but we wanted to avoid
this in our case before even creating
the Shaka player we already had the
manifest content so we had to approach
this a little differently
fortunately Shaka has a plug-in system
available we create a custom networking
plugin in this case so we came up with
this scheme here it's igw where the
video ID is in the path we then get the
manifest content from an existing store
using the video ID we then create a
response with the manifest content and
return it through a promise
finally we register these are custom
scheme through the networking engine
going back to the integration we simply
load the player with the URI with the
custom scheme that we created earlier
and we're done so we obviously wanted to
measure the impact of ABR so we tested
with three different variants
first there's default Shaka settings
this is just vanilla Shaka and then we
added our custom settings so we added
some overrides and finally we added a
custom a be our manager so in the second
variant one of the properties we changed
was the switch interval the switch
interval is the minimum amount of time
before switching bit rates we wanted to
test how a more aggressive switching
strategy would impact user experience in
the third variant we added our custom
API manager so this gave us a lot more
flexibility with the custom manager we
had more control over how we measured
bandwidth and when we switch bit rates
in the feed page we actually have
multiple instances of Shaka player at
any given time now that we had control
over how we measure bandwidth we also
could keep track of the latest
measurement using a feedback loop we can
then pass that back in to new
we create Shaka instances ensuring we
have a accurate default bandwidth we try
to follow some best practices during
these experiments in our feed we made
sure to only instantiate players when
needed and to actively just use destroy
API when we didn't we understood that it
would take some time to find that sweet
spot for how frequently we'd switch or
how much we'd buffer either being too
aggressive or too passive would change
results and finally we remindful the
types of videos that should be supported
Instagram has videos as short as three
seconds so depending on the video it
might not make sense to support ABR
while we expect to continue to iterate
on our experiments in general we have
high hopes for ABR and its positive
impact on user experience now Jenny is
going to talk about our experience
adding image capture and filters when we
started working on the Instagram mobile
web initiative at the beginning of the
year it was important to us to bring the
warm web users into the ecosystem of
Instagram as you heard in a mission
statement earlier we're looking to
strengthen relationships through shared
experiences it's difficult to share
experiences simply by watching other
people's experiences you need to be able
to create an excel media that captures
your own experiences as well since this
was one of the first features we added
we're actually not using the newest
api's which Matt will be talking about
in a little bit we're simply using the
image capture tap the image input yes
I'm sorry the input tag this is what it
looks like now it's also possible to add
a capture field but we purposely left
this out so that the user can either
upload the photo or take their own we
originally launched the creation flow
without filters so that our mobile users
could start sharing their experiences as
soon as possible
but since filters is an important part
to the Instagram brand it was important
for us to implement it as well we use
WebGL to implement our filters since
their native app uses
ngo we're actually able to reuse the
same shaders which let us bring over the
same exact filters as app as you can see
though the filter previews are done
differently this is because it is too
slow to calculate and load all the
filters so we use a standard balloon
image no matter the photo at Instagram
like with Peters video experiment we a
be test everything before launching so
we we put out filters do some percentage
of users we actually found that several
of our key metrics drops significantly
when investigating why we considered a
few different UI flows including seeing
if there was a way to test what not the
blue and images themselves or the
problem but then we took a step back and
we decided to test an even more basic
assumption we took her control which was
the creation flow without filters at all
and then we tested a variation where we
did all the WebGL processing in the
background with no user facing changes
this test taught us a lot because this
variation took the scene hideth in their
metrics as a variation with the
user-facing filters UI the performance
hit of WebGL was what caused the metrics
to drop so our next step was improving
the performance we started with logging
the timing of everything
Instagram always crops photos into a
square and we learned that creating the
initial crop was a significant
bottleneck specifically we're doing it
with a computationally expensive data
URL and blob conversion since the WebGL
rendering contacts a text image 2d
accepts the HTML canvas element we're
able to return a canvas directly and
WebGL will read that canvas as a source
pixels of the texture like so when the
user is done selecting the image then we
can render a canvas - blob which we
found to be 2 times faster than canvas
to your data URL and then then jamberry
the data URL from the blob this reduced
the time to first WebGL draw by 35% and
reduced the time to transition to next
step of the flow by about 85% and this
was just one of our performance
improvements another performance
improvement we did was to lazy load all
of our shaders which are filters and
compiling
the instead of compiling them all up
front this was our original code and as
you can see we would create all the
filters on an it and then return them
when we needed a filter program we
refactor the code to create a helper
function and that would initialize the
filters that don't exist already and
only calling them when a filter was
needed this reduced slew time by about
68% these performance improvements
really improved our filters experience
bringing filters to our mobile web users
next we made sharing work even while
offline Peter will talk about why and
how we made this happen so I know this
is the second last talk but for a moment
let's imagine you just arrived at chrome
dev summit you're super excited to hear
all the talks and see all the demos and
you want to share this experience so you
take out your phone take a photo maybe a
selfie and you hit share
unfortunately the requests times out
because all the access points near you
are completely overloaded but it's okay
and turn off Wi-Fi and then you use your
4G and you're good to go but what if you
couldn't simply do that what if that
wasn't even an option what if every day
was like things stuck at a conference
with really bad Wi-Fi when we think of
offline support we don't think of
someone getting on the airplane we think
of someone who deals with poor network
conditions as a part of daily life in
those moments when they really want to
share their experience there is so much
friction right now so let's look at a
demo of how we're trying to solve this
so here we have the PWA you saw earlier
and let's let's go offline with the
airplane mode so we get a toast telling
us we're offline but we can still post
so let's just post a photo
you add some caption maybe I'll find
demo and we get a toast telling us that
when we connect again it'll be posted so
let's go back online again
let's go back to the slides and see how
we did this so first we wanted to notify
the user that they're offline so we
listen to the offline event the offline
event actually is a little bit
unreliable for some phones battery saver
mode is actually considered an offline
state so it'll actually get triggered to
guard against this we added a
lightweight get request to ensure that
we're truly offline and then on error we
we show the toast with work box as Jenny
mentioned earlier we then register or
post request route service workers can't
cache push request so you actually need
to store the requests in a client-side
store here we use an X DB where we break
down a request object and store it we
create an offline post helper function
that will reconstruct the requests that
we just stored earlier and send it we
then use the background sync API that
was mentioned earlier talks now earlier
in the demo if it worked we would we
were planning to send the sync events
manually through dev tools which would
have then triggered the sync event so in
practice depends on the device to
actually trigger this this event and
it's based on a number of conditions so
in the callback we then check if there's
any items in the queue and then we call
our flying post helper and short a
notification so as you can see recent
advances in the web have enabled us to
create a rich feature-rich experience
for mobile web users especially those in
emerging markets we're actively testing
the features that we cover today and are
continuing to iterate and learn along
the way it's been incredibly challenging
exciting and humbling to work on
features and handle cases that we might
not think about in our day-to-day we
mentioned shared experiences several
times during this talk so it was really
great to be able to share our experience
building out these features I wanted to
give a shout out to our teammates who
aren't on stage with us they've all done
amazing work to
get us to this point and we hope to
continue the momentum we've already
built and now back to Matt will describe
the latest AP is for media creation
Thank You Jenny and Peter so we've seen
some of the web what the web can do and
it looks pretty great but we can do more
with new api's so one of the things that
Jenny mentioned is that Instagram
delegating image capture to the EMPA
element you can actually do this inside
your app now it used to be a little bit
limited we've had access to the camera
for quite a long time through
getusermedia which is part of WebRTC it
allows you to get a stream from a camera
or the microphone or both mm and it's
pretty simple API but you couldn't
really do too much with it before you
could use it for WebRTC which is what it
was designed for but then that you could
present it back to the user using a
video tag or you could grab an image
from it and the way you do that is that
you take the video there you take the
stream you put it into a video element
as the source then you draw the video
one frame of the video into a canvas and
then you take the canvas and turn it
into a blob and then you get the blob
and you turn it into an image which is
pretty long-winded and was also just
limited by the API is because
getusermedia those streams are limited
to 1080p HD regardless of what your
camera can do but we now have a new API
called the image capture API which makes
this a little bit easier and much better
so it takes the stream you get from get
user media and gives you a new object
back an image capture object and it
gives you a take photo method and when
you call this it tells the physical
device the camera to take a full
resolution photo and just give you
straight back a blob so none of those
drawings canvas nonsense in the middle
you also get some extra options so you
can see here that in this example I'm
passing through a fill light mode
setting this is setting what the flash
should do so here I'm saying that the
flash should be set to auto you can also
set the automatic redeye reduction
through this method
similarly for audio and video there's
the media recorder API again you take a
getusermedia stream you pass it in and
you say what mine type about put you on
mmm
then you get addictive ailable event
every time that the this that the
recorder has buffered up enough data's
give to you and at the end you can
reassemble this or look into a blob for
either the video or audio file that you
were creating and as well as using
getusermedia to get these streams you
can also get them straight from a canvas
or from Web Audio this is how you do
things like live filters you could take
a video draw each frame into a canvas
apply your filters and then use a stream
from the canvas to create a new video
which is the output and you could if
your if your canvas was applying like
instagrams filters you could get a
filtered video aside no not everything
here is perfect there are still things
that we need to work on one of the
issues that you'll have trying to do
these things is simply device
performance I mean as I said you know
towel has been talking about how we have
to many of the users that are coming
online have devices that aren't the same
as the devices that we use there you
know many of them are not as powerful
and things like drawing instagrams
filters are extremely computationally
intensive they're also limitations in
the api's themselves so as an example
let's talk about something that I tried
to do so I wanted to create a boomerang
effect what I wanted to do is take a
record a video with media recorder
straight from the camera and I wanted to
then create an output video which
played the video forwards and then
played it backwards again and then it
would loop it's called a boomerang
effect so I tried to do something it was
pretty simple I'd play the video and on
each frame I would set where I in the
video I wanted to be in that frame and I
would have it I knew as soon as it got
so the end it would set the direction to
minus one and it would come back at him
and then the idea was to record this
with media recorder and it's awful it's
trash this is a video that I took and
this is this is the the full quality
that I got there it's extremely jerky
it's difficult to tell exactly when it's
going forwards and when it's going
backwards why why did this happen there
are a couple of reasons
one of them is that at the moment when
you use media recorder to record a video
the output V is in a WebM which is
optimized for streaming and this means
that it doesn't put in the index of
where in the file each frame appears it
just assumes that you're going to play
it through right from the beginning and
it will just iterate through the file so
if you want to seek then it has to go
right back to the beginning of a file
and work its way through until it finds
the correct location there are some
optimizations if you playing for words
then it can make a rough guess oh you
got to this far and it was this time so
it's somewhere after there we're going
backwards you have to start again from
the beginning
mmm it also means you can't do the
simple even simpler trick of just saying
set the play rate to minus one because
that also doesn't work you would have
the same issue you can fix this with a
library but which would take the the
video that you've created and actually
manipulate the bytes to put in that the
index informations that you can then
make it see Keable but it's pretty
low-level it's a pretty chunky library
it would be better if the web platform
did this for you another issue is that
recording with media recorder is always
real time so I thought I could fix this
by taking the video lining up exactly
where I wanted to be and then saying to
media recorder API hey I want one frame
just record one frame and then wait
until I'm ready for the next one and say
okay take another frame there doesn't
work like that
they always records exactly real time so
if it's gently when it goes into your
canvas and then it will be janky when
you record it it also means that if I
try to take a one minute video and do a
boomerang of it it would always take
exactly two minutes to create I can't do
it faster than real time either so it's
not a perfect solution right now for
some of the things you might want to do
but of course we won't see the web the
mobile web is a great platform for media
creation so working hard to address all
these things another new thing that's
coming that looking forward to is
webassembly you heard about this earlier
from alex one of the things that this
allows you to do is take native
libraries recompile them for the web and
then use them in your page and people
have been already been experimenting
with native video libraries like ffmpeg
to do media manipulation on the web
video doesn't it yeah well we're also
excited about the shape detection API
this lets you detect things like test
barcodes and faces inside an image this
is currently behind a flag but Francois
Botha demonstrated this on iOS and
actually had a demo out in the forum
area which you might have seen I see
people taking pitches so if you'd like
to know more about the things that we've
been talking about here I've been
creating a sample application that I
called snapshot the source code is
available on github and I've been
documenting my experiences in a video
diary which is available on YouTube
in summary I think that what companies
like Instagram are doing with media on
the web is incredibly cool I hope you're
all as excited about the future for this
as I am
and thank you very much for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>