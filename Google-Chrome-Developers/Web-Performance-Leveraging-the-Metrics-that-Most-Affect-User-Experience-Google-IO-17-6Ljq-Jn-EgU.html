<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Web Performance: Leveraging the Metrics that Most Affect User Experience (Google I/O '17) | Coder Coacher - Coaching Coders</title><meta content="Web Performance: Leveraging the Metrics that Most Affect User Experience (Google I/O '17) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Chrome-Developers/">Google Chrome Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Web Performance: Leveraging the Metrics that Most Affect User Experience (Google I/O '17)</b></h2><h5 class="post__date">2017-05-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6Ljq-Jn-EgU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone my name is Shubham panikker
I'm an engineer working on Chrome and
I'm Philipp Walden I'm engineer working
on the web platform team over the last
year we've been part of our metrics team
in the web platform developing a set of
new metrics and api's that are user
centric in that they capture user
perceived performance we have developed
a framework for thinking about user
perceived performance that we want to
share with you today and Phil and I are
really excited to be here sharing these
metrics and api's with you in our past
lives
we've been web developers and we
understand the pains from gaps in real
world measurement and before Google I
worked on bett frameworks for apps like
search photos G+ etc and before working
in the web platform team I worked on
Google Analytics so I know a lot about
and I've seen a lot of the challenges
around tracking performance in the
browser so this is the goal of our talk
today to help you answer this question
how fast is my web app you certainly ask
yourself this and this may seem like a
straightforward question but the problem
is that performance and fast
these are big words what does fast mean
in what context that means different
things on navigation or clicking
scrolling or animations so what is
performance and what is fast in these
contexts and fast for whom exactly right
the truth is performance is hard we kind
of all know disks and for web developers
it's harder than it should be that's one
of the reason we're talking about this
you know there's a lot of tips and
tricks that you might have heard and
would not implement it or understood in
the right context they can sometimes
make things worse so in this talk we
don't want to give you more of these
tips and tricks we want to talk about a
way to think about performance a
framework a mental model for
understanding performance measurement
and then the hope is that once you
understand this model you have a lot
more tools at your disposal to solve
performance problems yourself in your
own app
but before we do that let's talk about
some myths and misconceptions around
performance today so I would say this is
probably the most common myth that I
hear some variation of this sentence I
tested my app and it loads in X point X
X seconds so the reality is that your
apps load time is not a single number
it's the collection of all the load
times from every individual user and the
only way to fully represent that is with
a distribution like the histogram you
see here in this chart the numbers along
the x axis show load times and the
height of the bars on the y axis show
the relative number of users who
experience the load in that particular
bucket as you can see while the largest
buckets and the most users were you know
between maybe one and two seconds there
were many many users who experienced
much longer load times and it's
important to not forget about these
users so this pattern toward the right
is often called the long tail and
unfortunately it's very common in the
real world and this histogram actually
illustrates the difference between
measuring performance in two very
different contexts and these contexts
are measurement in the lab versus
measurement in the real world and by lab
I mean great tools like dev tools
lighthouse webpagetest
other continuous integration environment
you might have set up lab is important
it gives you a sense for how your
changes are going to behave in the real
world it helps you catch regressions
before they hit your logic reduction
site and they give you deep insight and
break down so you can track down and fix
problems so that is super important it
is necessary but lab is not sufficient
real world measurement on the other hand
is messy real devices various network
configurations cache conditions all of
these different conditions for real
users are impossible to simulate in the
lab real user measurement helps you
understand what really matters to your
users it helps capture their actual
behavior which may be different from
your assumptions or your lab settings so
to really answer the question of how
fair
if my apps it's important to measure
this in the real world so in our talk
today we will focus on real-world
measurement so coming back to this myth
for a second there's another reason why
the statement is problematic the
question when exactly is load isn't app
loaded when the window load event fires
does that event really actually
correspond to when the user thinks the
app is loaded so I'd argue that load is
not any one single metric it's an entire
experience and so it can't be rent to
say it's not one single moment it's an
entire experience and it can't be
represented by just one metric so to
better better understand and illustrate
what I mean by that I want to show you
an example I'm going to play a video of
the YouTube web app loading on a
simulated slow Network and I want you to
pay attention to how the video loads the
app loads and notice that you know
things are kind of coming in one by one
so can we play the video
okay so think about how that felt and
now I want to play the second video and
I want you to pay attention to how you
feel watching the second video think
about the experience can you play the
second video
so it feels different doesn't it I bet
some of you were not sure if the video
was even playing and that's kind of the
point when you don't give that feedback
to the user it it makes them feel
something so these two videos as I'm
sure you guessed load in the exact same
amount of time but the first one kind of
seems faster at least it it feels nicer
because you know things come out right
away it's like if you went to a
restaurant and you sat down at a table
waited for an hour and then they brought
you your drinks appetizers entree
dessert check and dinner mint all at the
same time like that would kind of feel
weird you would wonder why they waited
until the very end so again you might
look at this and then you might think
okay well we should optimize for the
first initial render get content there
as soon as possible that's what that's
what is proved right
and again that's sometimes that's true
but that's not always true sometimes
when you do that you can make things
worse in some cases and cause other
problems so I'm going to play another
example a real-life example from Airbnb
mobile website and so for context I know
personally that the Airbnb engineering
team cares deeply about performance and
user experience and they try to make
their pages as fast as possible so one
way they do this is use server-side
rendering to deliver all the content in
the initial request and it shows because
the page loads really fast even on a
slow connection the problem is that on
slower devices that take longer to
execute JavaScript the page is rendered
but it's not usable for a couple of
seconds and you can see that in the
video so can you play the third video
so as you can see the user here try to
click a few times in the search bar and
then nothing was happening and it wasn't
until you know maybe the six clicker so
that the the component pane from the top
scrolled down and so to be clear this
video is from a simulated slow device it
doesn't represent the majority of their
users but you know Airbnb is committed
to providing a good experience for all
of the users and they they wanted to fix
this in the care about this and so
they're currently working on a fix to
this problem and I kind of just want to
mention on a personal note that I'm I'm
really happy and glad that Airbnb was
willing to let us show this to you it
kind of I think it's cool that that they
want other developers to learn from
their experience so can we go back to
the slides
all of these examples that I just showed
the straight why you shouldn't measure
load with just one single metric load is
an experience and you need multiple
metrics to even begin to capture it so
this is another commonly held
misconception
you only need to care about performance
at load time the loading is super
important but it's certainly not
everything and historically we've all
fallen into this trap of narrowly
focusing on load and part of it is just
our own developer outreach our tools
focus pretty much exclusively on loading
the reality is that there's lots of
other interactions that happen long
after load all kinds of clicks tap swipe
scrolls think of all the time you spend
on new site in your emails on Twitter or
Amazon Lord is a really small fraction
of this overall user session and users
associate performance with their entire
experience and unfortunately the worst
experience stick with them the most so
this is a summary of the problems that
we've highlighted today so far
real-world metrics are a distribution
they should be seen on a histogram not
as an individual number Lourdes is an
experience it cannot be captured with a
single moment or a single metric third
interactivity is a crucial part of loads
but it's often neglected and finally
business is always important to users
maybe on load time so these are the
questions that we want you to ask us
today and these are the questions that
we hope we can answer for you
as part of this talk user perceived
performance is important what are the
metrics that accurately reflect this how
can we measure these metrics on real
users how can we interpret these
measurements to understand how well our
app is doing and finally how to optimize
and prevent regressions going forward so
in this segment of the talk we want to
talk about these new metrics and the
basic concepts underlying them so we've
all used a traditional metrics like Dom
content loaded and window.onload to
measure load time the problem is that
they don't really correspond to the
users experience of loads they have
almost nothing to do which when the user
saw pixels on the screen for example a
CSS style might be hiding the content
when Dom content loaded fires and even
if the content is rendered interaction
can be blocked so JavaScript might not
be there to hook up a critical handler
for example and these old metrics
completely ignore interaction even
though we know that interaction is super
important for modern web apps so what
are the key experiences that matter to
users and shape their perception I think
it's helpful to frame these as questions
that the user might be asking is it
happening so did the navigation start
successfully has the server responded is
there anything that indicates the user
that it's working and then is it useful
has enough content rendered that the
user can actually engage with the page
and once content has rendered is the
content usable like can they interact
with it is it locked
is something preventing that interaction
for happening and finally is it
delightful or the interaction smooth
natural free of lag or jank and is the
overall experience good so now let's
look at how these questions map to
measurable metrics here's an
illustration of a pages load progress so
the first frame over there it's just the
blank white screen
I mean before the browser has loaded
anything the second frame represents the
first paint metric it's the point at
which anything is painted at the screen
that the user can see anything different
from what the screen looked like before
the response the second frame shows
first content full paint the second
metric it's when any of the content is
painted and by content I mean kind of
something in the Dom it doesn't just
have to be text it could be images or
canvas or SVG something in the Dom
that's painted to the screen in the
third or i should say the fourth frame
you see some more stuff coming in but
it's not quite enough content to be
meaningful and then you get to first
meaningful paints in the fifth frame
where the user can actually engage with
the content enough stuff is rendered
that the user can you know what they
came for is here and they can start
consuming it and then finally the the
last metric time interactive is when the
page is both meaningfully rendered and
usable meaning it's capable of receiving
input and responding in a reasonable
amount of time so Saul says that the
first meaningful paint is when the page
is useful and the user can engage this
is when the primary content of the page
has rendered but what is primary content
which elements exactly now not all
elements on the page are equal there are
some elements that are important
we call them hero elements and when
these hero elements are rendered you
have arrived at the user moment of it is
useful and the user can meaningfully
engage with the page so here are some
examples to show you what I'm talking
about
these are hero elements with some
popular sites so for YouTube we think on
the YouTube watch page the hero element
is likely the thumbnail of the primary
video and the play button for Twitter it
is likely the notification sound and
that first tweet for the weather app is
probably the primary weather content
even though they might be tons of other
stuff on the page so when these hero
elements have rendered this corresponds
with first meaningful paint and the
edges useful user moment and you may
notice that some of these here elements
are content based and some of them are
or interactive components like in
YouTube for example
the humor Mellon hero element is
rendered when the thumbnail is loaded
and the play button is visible but it's
probably not actually usable until the
JavaScript that controls the play button
has run and enough of the video has
buffered to actually be able to start
playing if the hero element if your
elements are interactive then not only
does rendering them matter but also you
know when it's useful when its TV is
however there are times as we mentioned
when interactivity can be blocked so to
understand why important elements might
be blocked and not interactive think
about a time when you are in a long line
from where let's say it's you know the
grocery checkout of the bank you're
standing in line and there's one or two
customers who are confused or they're
angry and they hold up the line causing
a long delay this is what long past due
on the browsers main thread these are
tasks that run long they occupied the
main thread for a long time and they
basically block all the other tasks in
the queue behind them and scripts are
the most common cause of long tasks like
all the work that scripts do in terms of
parsing comes population evaluating etc
so if you've used step two you're
familiar with all the primary type of
work style layout paint script it turns
out all of this happens on the main
thread and it also so happens that most
interactions things like taps clicks and
even animations typically also need the
main thread so you can see how this can
be a problem a long strip is running
begging the main thread and the user is
trying to interact and these
interactions are basically waiting in
the queue and this manifests as jank to
users as delays and click jank and
scrolling or Jenkin animation so you
might wonder how long is long what is
long and so we define long to be 50
milliseconds scripts should be broken
into small enough junk so that even if
the browser is idle and a user happens
to interact the browser should be able
to finish what it's doing and service
those inputs that interaction and so 50
millisecond joints will ensure that the
ray'll guideline for responsiveness is
always met you might have heard a lot
about 66 and 16 milliseconds and some of
you might wonder why is in the 16
milliseconds as the reason is yes if you
are animating than 16 milliseconds is
important and but animation issues are a
small subset of responsiveness issues at
large on the web today
and you know you are animating then yes
you have to share the 16 milliseconds
budget with the browser now long casts
are the cause of most of the
responsiveness issues on the web today
and scripts are by far the most common
cause of long tasks
so just to recap this table shows how
each of these metrics map the user
question from before so the question
isn't happening maps of the metrics for
first paint first content full paint is
the useful map's - the first meaningful
paint and the hero element timings is it
usable map's to time to interactive and
then the last one is a delightful Maps -
what should we just mentioned long tasks
or maybe more accurately the absence of
long tasks so you want to be wondering
how metrics like first meaningful paint
or time to interactive can work for
every app and you're totally right one
size cannot fit all we actually spend a
lot of time in our metrics team trying
to develop these generic standardized
metrics that work for every app and what
we've learned is that it's incredibly
hard to do that and that also makes it
hard to standardize that said there is
value in these generic standardized
metrics and so these baseline metrics
that work for the majority case in let's
say 70 to 80 percent of apps out there
and we have made such metrics available
in our tools like you might see them in
lighthouse dev tools webpagetest and we
are working to kind of consolidate these
definitions down the road we expect
analytics to start surfacing variants of
these metrics the main thing to
understand for these out-of-the-box
generic metrics is that don't assume
that they accurately capture the use is
it useful and is it useable moment for
your apps try them out see how well they
work for you and when it comes to real
user measurement we encourage you to
supplement these metrics the
your own custom user metrics or
customize these metrics and make them
your own make sure that they work really
well for your app and we'll show you
specific tips for doing that later so
now that we understand have these
metrics the question is how do we get
these in JavaScript that's the most
important thing to measure on real users
historically we've used like we said
metrics like Dom content loaded and
window load primarily because they were
easy to get in JavaScript I assume every
web developer here knows how to find out
when window load happens or when Dom
content loaded happens but these other
metrics have traditionally been a lot
harder sometimes impossible to get in
JavaScript and and trying sometimes to
to find them can lead to problems this
code sample shows how you would detect
long tasks kind of before these new
metrics and this is kind of a hack so
what this code is doing is it's
effectively making a request animation
frame loop it's doing measuring frame
after frame after frame and it's
comparing the timestamps from the
current frame to the timestamp on the
previous frame and if this is longer
than 50 milliseconds then it's
considering it to be a long frame but
there's a lot of problems with this
method I mean it kind of works but it
adds a lot of overhead it prevents idle
blocks it's not great for battery life
and it doesn't even tell you the source
of the problem you don't know you might
know that there was a long frame so you
can assume there was a long task but you
don't know what script caused that long
task and this isn't just a hypothetical
example this pull requests on the amp
project is basically them taking that
code out because they realized that it
was more trouble than it was worth you
know the number one rule of performance
measurement code is that you shouldn't
be making your performance works worse
by trying to figure out how good the
performances so these hacks show the
need for real API is built into the
browser so the browser can tell us when
performance is bad so performance
AP is on the browser solution do really
small measurement you have standardized
API so they're available in multiple
browsers not just Chrome
and been available we definitely
recommend that you use these api's in
practice though you will use a
combination of these api's as well as
your own javascript polyfills and the
reason why polyfills are necessary is
because the implementation timeline on
browsers will vary and you're asking you
to customize and supplement these
metrics so these are some of the core
building blocks as we see it for web
performance we have high resolution time
which you might be familiar with from
your use of performance start now
performance observer is an important
piece it replaces the old performance
timeline and it overcomes widths
limitations such as no polling it's a
low overhead API and it should avoid
preconditions from a shared buffer so
this is kind of what the usage or
performance observer looks like and it
also happens to be the code that
replaces the hack that Phil showed you
just a little bit earlier so performance
observer usage is fairly straightforward
you create a performance observer and
make a callback and then you say observe
with expressing interest in certain
entry types and as entries of that type
become available the callback is invoked
asynchronously and there are many
different entry types long tasks is what
we show in this example but this could
just as well have been resource timing
or navigation timing or paint timing
which is a new metric we've introduced
this also serves as a really good
examples of long task usage you can
basically use this code to understand
responsiveness issues on your app the
callback is called asynchronously when
the main spirit thread is observed to be
busy for more than 50 milliseconds at a
time and long task is available in
chrome stable today so I encourage you
to try it out so this table shows what
our recommendation is for how you would
track these metrics in your applications
and just to reiterate having these
tracked in your applications is what
allows you to measure these metrics on
your real users not just running it in
the lab so first paint
first content can be measured with
performance observer with the paint
entry type this is available in Chrome
Canary today long tasks can be measured
with performance observer also since
chrome 58 that's a chrome stable right
now for hero elements it's a little bit
trickier because you have to identify
what your hero elements are and you
basically have to write some code to
figure out when there when that's
visible and I should mention that along
with this talk I'm GU publishing an
article on developers.google.com slash
web very soon it'll be up when this
video goes up that goes into more detail
on how to do all these things you don't
have to worry about you know if you're
taking notes or whatever also I should
mention that we're working on a native
API to make this easier where you could
annotate tell the browser what are the
hero elements and the browser would tell
you when they're loaded or when they're
when they're rendered four first
meaningful paints at this point before
we develop a standardized metric we
think that you should use hero element
timing as the substitute for first
meaningful paint the first meaningful
paint metric is very like we said
generic it will we try to be like
one-size-fits-all fewer elements is for
your site and so it will always be more
accurate than first meaningful paint and
finally TTI we released the polyfill
today actually for the TTI FileZilla
it's on github and you can go try it out
right now to give an example of what the
usage looks like you essentially import
the module in JavaScript and then you
call the get first consistently
interactive method and that returns the
promise and the promise results to the
TTI metric value in milliseconds and
then once you have that you can send it
to analytics so you get a sense for what
the polyfill does you know I should
mention that the the first gift first
consists the interactive method takes an
options object so you can figure it for
your site and what you can do is you can
pass it a lower bound the polyfill will
assume the lower bound by default this
Dom content loaded but you can give it a
better metric for your site so the way
this works is you have the main thread
with long tests and short tasks and you
have the network timeline and then you
have your lower bound which by default
is Dom content loaded but the polyfill
does is it uses these resource timing
and long test entries to search forward
in time for a quiet window of five
seconds at least five seconds where
there are no long tasks and no more than
two Network requests basically it's
saying once we get to that quiet window
we think that the app is most likely
interactive now and then it considers
the moment of interactivity to be where
the last long task was so that's a bit
of how this polyfill works again you can
pass it a custom lower bound for your
site and one example of what you might
want to use is the hero element timing
that would be a great example you also
might want to pass basically the moment
all of your event handlers are added
because if your event handlers have not
been added yet the site is probably not
interactive yet so Phil showed you how
long tasks and push out your time to
interactive but there's lots of other
interactions that we're asking you to
care about maybe on learning like flakes
and taps and delays and these can
basically got pretty bad user
experiences so you probably wanted to
know when these important events are
delayed and ideally there would be a
first-class platform API that would
answer this question and we actually are
working on such an API but today you can
actually use this code sample to
understand the gap you can basically use
the difference of event or timestamp and
the current time in your event handler
now even dot timestamp is our best guess
of when the event was created until this
can be the hardware timestamp or when
our best guess is when you know the taps
you wrap the screen and this difference
will tell you how long the event was
spending waiting around on the cue for
the main thread now here if that
difference is more than 100 milliseconds
we send it to analytics now we haven't
shown this here but you can also
correlate this back to your long task
observer but you can actually look at
what long tasks happened in this time
when my event was blocked waiting and
those are likely the culprit so once
you've measured these key metrics and
sent them to some analytic service you
want to report on them to see how you're
doing that will allow you to you know
better
answer the question is your app fast so
this is just a you know just one example
of a histogram that I threw together
from TTI data for an app that I maintain
using the polyfill that we just showed
you and the point is not to look at
these numbers or compare them but the
main the main point that I want to make
is when you're tracking your performance
metrics in your analytics tool then you
can drill down by any dimension that
your analytics tool provides so in this
case we can see the difference between
performance on desktop versus mobile you
might also want to consider the
difference between one country from
another country or geographic locations
where maybe network availability is not
as great or network speed - not as high
it's important to know how this
difference manifests across the row in
the real world on real users in cases
where you can't you know show a whole
histogram I recommend using percentile
data so you can show the 50% the median
number you can also show things like the
75th percentile the 90th percentile
these numbers give a much better
indication of what the distribution was
and they're much better than just
averages or just you know one one single
value so a really important question is
do performance metrics correlate with
business metrics and again if you're
tracking your business metrics in an
analytics tool and your performance
metrics in an analytics tool and this
you know shows the value of tracking the
stuff on real users then you can see and
you can answer this question all the
research that we've done at Google
suggests that good performance is good
for business but the really important
thing is does this true for your users
for your application so some example
questions you might want to know do
users who have faster interactivity
times buy more stuff do users who
experience more long tasks during the
checkout flow drop off at higher rates
like these are important questions and
once you know the answers to these
questions you can then make the business
case for for investing in performance I
hear a lot of developers saying they
want to invest in performance but you
know somebody as a company won't let
them or won't prioritize that this is
how you can make that a priority
and finally we haven't talked about this
yet but you may have been wondering all
of the data we've been showing is for
real users who made it to interactivity
and you probably know some users don't
make it there some users get frustrated
with a slow loading experience and they
leave and so it's important to also know
when that happens because if it happens
90% of the time the data that you have
will not be very accurate and so you
can't know where the TTI value would
have been for one of those users but you
can measure how often this happens and
perhaps more importantly you can measure
how long they stayed before they left so
we have discussed a lot of specific
metrics and api's and we've shown you go
samples and so now we kind of want to
back up a little bit and provide some
higher-level guidance on how to best
leverage these metrics and API so one
great thing about everything we've
introduced today is that all of these
are user centric metrics and ap is so by
definition improving these will improve
your users experience so the first piece
of wisdom is drive down for Spain and
for its content fill thing and all of
the traditional systems for fast loads
applies here you know remove those
render blocking scripts from head
identify the minimum set of styles you
need and inline them in head you might
have heard of the app shell pattern that
helps improve user Pursey perception
perception the idea there is like very
quickly render the header and any
sidebars
now first paint and first content will
paint are important but they are
certainly not sufficient it's really
important to improve your overall load
time so it's not just enough to be off
to a good start
in a race it's really important to make
it past that finish line and time to
interactive is the finish line for
loading or interactive apps so more
specifically minimize the time between
first meaningful paint and time to
interactive we saw in the Airbnb demo it
was important for users to interact with
that search box but to shorten your time
to interactive identifies what is the
primary interaction for your users don't
make assumptions here do they tend to
browse or that it
they tend to interact with a certain
element right away and then figure out
what is the critical JavaScript that's
needed to power that interaction and
make the JavaScript available right away
one common culprit we've seen are large
monolithic JavaScript bundles so
splitting ups jeaious like code
splitting will take you a long way this
here and you know the purple pattern
kind of fits in here so specifically the
first two P&amp;amp;R or purple
ideally shiftless JavaScript but if not
at least Stepford the JavaScript there's
terms of JavaScript that the user is
never going to need all those pages that
they're not going to visit all the
features that they're not going to
interact with if there's a bitch it in
the footer that's below the folder they
are unlikely to interact with therefore
all of that javascript the third thing
we have is reduced long task cracking
down on the long task will really help
responsiveness on your app overall
however if you really need to prioritize
at least think about long path in the
way of those really critical
interactions unloaded you know bits long
tests that are pushing out time to
interactive or long tether in the way of
the checkout flow and other important
interactions for your app scripts are by
far the biggest culprits here so
breaking up scripts certainly helps and
it's not just about breaking up scripts
on initial load scripts that load on
single page app navigation like going
from the Facebook home page to the
profile page or clicking around like on
the checkout or Amazon or the compose
button in Gmail all of this javascript
needs to be broken up so it doesn't
cause responsiveness issues and the
final thing we have for you today is
porting third parties accountable as a
social widgets are known to cause the
majority of long tasks and they can
undermine all of your hard work on
performance like you might have done a
ton of work to split out all your code
carefully but then you embed a social
plugin or an ad and they undo all of
that work they get in the way of
critical interactions so to get an idea
of this you're actually doing a
partnership with so after a major
analytics company
and so they're doing a bunch of case
studies in the sub preliminary data that
came in they picked couple of their
sites their customers who had
third-party content in the first site
they found that 93% of long test for
because of ads on the second site they
found 62% of long tests were about
evenly split between ads and social
widgets now long past API actually gives
you enough attribution to implicate
these third-party iframes so we
encourage you to use the long task API
find out what damage these third parties
are doing on your app and once you've
optimized your app you obviously want to
make sure that you don't regress and go
back to being slow you don't want to put
a bunch of work into this and then have
it all be for nothing if we know one new
release you know it turns everything bad
so it's critical that you have a plan
for preventing regression so this is a
you know a workflow that I promote you
you start off with writing code you
implement a feature fix a bug improve
user experience in some way and then
before you release it you test in the
lab I assume a lot of people do this you
run it through lighthouse you went
through DEP tools make sure that it's
not slower than your previous release
and then once you release it to your
users you also are going to want to
validate that it is fast for those users
that you released it to you can't just
test in one you should these things
complement each other you should be
testing both in the lab and in the real
world and so for some automation ideas
is the best way to prevent regression is
to automate this process you're probably
going to slack on it a little bit if you
don't have it built into the release and
automated so lighthouse runs on CI and
there's actually a talk tomorrow
afternoon by Eric bidelman and Brendan
Kenny that kind of goes into how to do
this and I recommend checking that out
if you want to learn how to run
lighthouse on CI if you're using Google
Analytics you can set up custom alerts
that you know trigger when some
condition is met so for example you
could get an alert if suddenly the
number of long tasks per user spikes
maybe a third party you're using change
their javascript file and things got
worse and you you know
didn't know is that this is a good way
of finding out that stuff so getting
back to the original question how fast
is your web app in this talk I hope
we've given you enough of a framework to
think about performance in the big
picture in a user centric way I also
hope we've given you enough specific
tools metrics and ap is that you need to
answer this question for yourself we
know the situation isn't perfect we know
we have more work to do and shooby is
working on this leading our efforts here
at Google on the on the standard slide
and so she can talk about some of the
things that are coming down the road and
so this is our final and last light and
I just want to say that we asked we know
that our gaps and there's a number of
api's that you're working on we'd love
to have a first-class API for hero
element timing the idea there is that
you guys can annotate the elements that
matter most for your site and then
browse the browser can put those times
on the performance timeline secondly
we're working on improving long tasks
mostly by improving attribution we
really want to tell you which scripts
are causing problems and more detailed
breakdown so you can actually take
action right away
and fix those issues secondly we want to
really have an API for input latency so
you don't have to do go to all those
workarounds that we showed you for event
and timestamp ideally for your important
interaction for your app you should be
able to know like how delayed they were
was it which long tasks were in the way
and when the next render happens and
then there's other inputs that we
haven't even touched on that are in our
backlog things like scrolling and
compositor animations and finally I just
want to leave this with saying you know
we want this we said a lot today
but we really want this to be a two-way
dialogue we want to hear from you we
want to hear about your frustrations
don't be quiet about you know those gaps
and measurement and those frustrations
with performance try out these api's and
polyfills and please file bugs on the
spec repose on github it's actually the
best way to report issues and make
feature requests and if you're working
with analytics like whether it's a
different team or a third party push on
your analyst
to adopt these new metrics ask them for
these histograms like Phil showed you
and we're pushing on analytics to on our
end
start the chromium bugs on performance
who's actually a signal we use for
prioritization internally and we need
these signals to make a case for working
on measurement and finally as bill said
we have all the links in the article
that he will publish shortly and they
will also be linked from the video so
thank you and this is how you can get a
hold of us</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>