<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Google I/O'17: Channel 6 | Coder Coacher - Coaching Coders</title><meta content="Google I/O'17: Channel 6 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Chrome-Developers/">Google Chrome Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Google I/O'17: Channel 6</b></h2><h5 class="post__date">2017-05-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/safQDjgDVAQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">D oubleClick, wolf

vision.  Type 
TypeAdapters, pipe converter.  
Type

T ypeConverter.  Poe

Pojo, 
Dow.
   &amp;gt;&amp;gt;
   &amp;gt;&amp;gt; Testing, write something, 
test test write something.  Test
 test write something.  Test 
test write something.  Test test
 write s omething.  Test test 
write something.  I pledge  a 
allegiance to the flag of the 
United States of America and to 
the republic for which it s 
tands, one nation, under God,

indivisible.






Welcome to day two of Google 
I/O.  My name is Zach KWAOBG and
 I'm a product manager on the 
Chrome team leading our Web 
payments effort.  A special 
thank you for coming out and 
joining me at 8:30 a.m., which 
is very early, and a special 
thank you and welcome to 
everyone on the Live Stream as 
well as anyone that's watching 
on YouTube later.  I'm really 
happy to be up here today 
talking to you today about how 
we think about the future of Web
 payments and how we are trying 
to really help users, both our 
users and your users, your 
customers have pain-free 
checkout experiences on the Web.

 by giving a refresher on why we
 care about this space at all.  
I think it stems from the fact 
that the Web is better than 
ever.  You have probably heard 
it over and over again from the 
keynote to the track sessions 
that the Web is really amazing 
these days. 
   You can build fast, rich, 
app-like experiences and they 
are reeling compelling and the 
line between what is Web and 
what is app is more blurry than 
it's ever been.  This is an 
incredible opportunity.  Things 
like Web GL, Polymer and service
 workers, these are all tools 
that can help you create 
incredible experiences.  I think
 personally my favorite example 
of how far the Web has come is 
the fact that you can literally 
circumnavigate the entirety of 
the earth's surface directly 
inside of a browser tab these 
days.  This is Google earthth 
running inside of Chrome and it 
blows my mind that you can do 
this on the Web. 
   If you don't recognize this, 
this is actually where we are 
right now at shore line 
amphitheater which is the 
beautiful white tent where the 
key motes are and we are in the 
back parking lot, but it's a 
very nice parking lot.
   &amp;gt;&amp;gt; So really incredible 
stuff.  When you look at this 
amazing progress,  there is one 
area that seems obviously 
behind.  That's the way we buy 
things online.  Despite all of 
this great progress, this is 
still what most checkout flows 
look like.  It's the same 
questions, the same form fields,
 the same mull ties step 
process.  It's like for all of 
this great innovation, we are 
still 
stuck here and I suspect this is
 similar to the way checkout 
forms first worked when they 
were launched.  In preparation 
for this talk, I did a little 
bit of research and I discovered
 that the first online 
transaction was made in 1984 in 
someone's home by a 72-year-old 
woman named Mrs. Snowflake which
 I think is as incredible piece 
of fair trivia.  I think the 
stats reflect that we need 
change here.  Long checkouts 
continue to be one of the 
leading causes of cart 
abandonment out there. 
   It makes sense.  Has anyone 
in here on a mobile device ever 
abandoned a transaction because 
the process was too long or 
cumbersome?  You can raise your 
hand.  That's way higher than 
the stat by the way.  Is so 
that's amazing.  Sometimes I 
tell myself I will go back and 
complete it on desktop later.  
Maybe I do, maybe I don't.  The 
point is that there is lost 
opportunity to give users a 
really great experience here and
 to convert out that moment. 
   So we are quite literally 
losing money.  Despite these 
challenges though, mobile 
commerce is huge.  I mean, 
mobile commerce in the U.S. 
alone this year is expected to 
exceed 150  $150 billion.  
That's all the more impressive 
when you consider the fact that 
mobile websites still convert 
about a third lower than their 
desktop or laptop counterparts. 
 So we have a lot of work and a 
huge opportunity to better the 
lives of our users.  So every 
good platform out there whether 
it's apps or the Web, needs a 
good payment solution to 
succeed. 
   So it became obvious to us 
that the Web needs a better 
answer for payments.  That's 
what I'm here to talk to you 
about today. 
   I have really good news.  It 
already exists.  Last year at 
Google I/O a couple of my 
colleagues were up on stage and 
they announced we were working 
on this new A P*EU called 
payment request.  I'm happy to 
report that not only did we say 
we were going to launch it, we 
launched PaymentRequest in 
September of last year.  What is
 PaymentRequest.  It's an open, 
built into the Web, designed to 
be fast and secure, ready to be 
used today API for transacting 
on the Web platform.
   We are going to be talking 
about all of these different 
components during our talk today
 and I'm excited to dive right 
into it.  But f irst, I should 
clarify something. 
   PaymentRequest is not a 
processor.  We are not trying to
 make the browser a processor or
 a gateway or another new entity
 in the system to move money 
from point A to point KPW*. B.  
We think the industry has done a
 great job of actually filling 
this service, great players like
 Stripe and Braintree.  We love 
the work that they are doing. 
   No, we are focused on users. 
 We are focusing on taking the 
user experience that seems stuck
 in legacy mode and move iting 
out into a much more faster, 
streamlined experience. 
   So when we thought about 
designing the PaymentRequest 
API, we had two goals in mind.  
the first is that transactions 
on the Web should be seamless, 
and we have to start with the 
status quo.  So we have to take 
the existing world, oftentimes 
the world of credit cards, CDCs,
 big addresses, all of those 
complicated things from a UX 
perspective and make them 
better. 
   The second thing is we have 
to think about security and how 
we can improve that.  And I 
don't mean just things like
 https, but I mean we should be 
forward looking and say how can 
we bring more secure forms of 
payment like tokennization 
directly into the Web platform. 
 How can we unlike opportunities
 for Samsung PA*EU and Android 
PA*EU and future tokennization 
to have a home in the platform. 
 So high quality secure payments
 become first class citizens of 
the Web platform.  Now, I could 
talk about this all day long, 
but I'm going to go ahead and 
jump to a demo and show you what
 this looks 
looks like. 
   It's going to jump over to 
the wolf vision
 here.  So as I mentioned, 
payment request is alive and 
ready today.  So you don't have 
to wait to start implementing.  
We actually have a number of 
really great m erchants 
implementing one of which which 
is Kogan.com.  Kogan is 
Australia's largest retail 
company, and they have 
implemented PaymentRequest, and 
it just so happens that Chrome 
also has an office in Sydney.  
Now, I have never been there, 
but I'm going to operate under 
the assumption that one day they
 will let me go to Sydney and I 
will give myself a gift on 
arrival that will be waiting at 
the Google office for me.  So 
I'm going to buy something that 
everyone I think needs, USB-C 
adapters.  Super common. 
   I will add this to my cart.  
It's no added to my cart as you 
can see and I will hit the 
checkout button.  This standard 
thing happens, I'm on Kogan.com.
  I have my list of items, I 
have delivery estimates and I 
hit the checkout button.  And 
here is PaymentRequest in 
action.  So instead of going to 
the traditional check out flow, 
a PaymentRequest sheet slides up
 TKWREBGTly from the bottom -- 
directly from the bottom, 
customer chant  merchant name, 
Chrome logo so users are aware 
of where the data is coming 
from.  We have a total amount of
 money being requested.  So you 
see right now it's Australian 
dollars for 5-dollar, and you 
see that I already have a cease 
sa credit card -- Visa credit 
card default selected.  The only
 thing I have to do is choose a 
shipping address. 
   It's a physical good so we 
have to ship it somewhere.  I 
will tap on and choose and you 
sigh here that payment request 
already knows my frequently used
 addresses.  So the first one 
there, my number one is my main 
office in our San Francisco 
headquarters and the second one 
is our 
Sydney office.  This is the not 
first time I have practiced this
 so Chrome has stored my 
address. 
   I tap on the address.  What 
is happening when I tap on that 
is we take the address and send 
it behind the scenes to the 
merchant so they can use the 
address to dynamically populate 
the set of available shipping 
options.  So you will see they 
default selected free shipping 
for me, but if I tap on that, I 
can also select express 
shipping. 
   Now, in this case, I have no 
idea when I'm going to go to 
Sydney and pick up my converter 
here so I think standard 
shipping will be fine for me.  
Then I hit the pay button.  You 
will see the only thing that I 
have to do is input a CVC.  
Because this is a live 
transaction and some things 
should be kept secret from the 
world I am not going to show my 
CVV number to everyone.  But you
 can trust that I just typed it 
in and now the transaction is 
running behind the scenes, we 
have taken the data, we have 
bundled it and sent it behind 
the scenes to Kogan.  The 
transaction is successful, yea, 
I love when demos go well!  So 
that's amazing.
   (Applause). 
   I think what's cool about 
this is I didn't have to type in
 anything except for my CVC.  We
 had my data stored.  It was 
ready to go.  All I had to do 
was confirm it, Auth it seasoned
 give it over.  That's the kind 
of seamlessness we are talking 
about bringing to -TS  the Web. 
 I know we are focused on the 
mobile Web track, but the 
reality is that -- we need to go
 back into demo mode because we 
still have -- there we go, 
perfect.  Again, we have been on
 the mobile Web track and that's
 the big focus at I/O.  But we 
all know users love buying 
things on desk t ops and laptops
 and we thought we should bring 
the same great experience to 
these platforms.  Here I have a 
shop loaded up on my  Chrome, 
and Chrome  running on my Mac, 
and I will tap the buy now 
button and a news payment sheet 
optimized for the desktop 
experience comes from the top.  
And what's great here is you 
will see that my same 
information is available.  So I 
choose an address.  You will see
 it's got my same  addresses 
that I saw on the mobile device.
   I'm signed in and all of the 
data is syncing across all of my
 instances of Chrome. 
   I will ship to our spear 
street office.  You see that 
methods are still dynamically 
calculated so we offer free 
shipping inside of California, 
but you can see this is a demo. 
 If I do decide to ship 
somewhere else, shipping 
dynamically changes to $10, and 
in this case, my 8605 Visa is 
present, the card I just used to
 make the last transaction, but 
in this case, since I can't hide
 the screen, I have a test card 
that I will use to facilitate 
payment.  So I tap one pay, 
insert my TKHRAOE digit CVV.  
Hit the confirm button and the 
same things happens.  So really 
great and I'm excited to say 
that we are bringing payment 
requests to all Chrome platforms
very 
soon.  So now you have SAOEFRPB 
payment request in action.  I 
want to help you understand how 
it works.  I won't go into too 
much depth because we have a lot
 of great resources, but I want 
to give you the Core idea of how
 the process works.  A payment 
request is a JavaScript API, of 
course, it's built and baked 
into the Web platform to create 
these experiences.  What you saw
 there was natively Chrome 
rendered UI and the first thing 
you have to do when you want to 
create a payment request is you 
have to tell us how you can get 
paid.  We call this supported 
methods and there are two types 
of supported methods that exist 
inside of PaymentRequest.  The 
first one are things baked into 
the standard.  In this case we 
have something called basic 
card.  Basic card is a great 
fall back mechanism.  It's your 
way of telling the browser, hey,
 if it looks and feels and 
functions like a piece of 
plastic in someone's wallet it 
probably maps to basic card so 
it's your way of saying I accept
 any of these forms of payment. 
 I don't show it here but there 
are ways to get more nuanced 
than this as well.  You can say 
I only accept debit cards or I 
only accept credit cards from 
these networks.  All of that is 
totally possible. 
   The second thing you can pass
 in are supported methods that 
we call proprietary methods.  
These are identified by URLs, 
and in this case, for example, 
if you wanted to leverage the 
great stuff that the payments 
team announced yesterday around 
Google payments this is how you 
tell the browser that, hey, I 
support any of those Google 
payment methods.  So there is a 
proprietary system built in so 
that every entity in the payment
 space can participate freely in
 the payment request ecosystem. 
 It's the job. 
   Browser to look at the
 ways emerchant can get paid and
 look at the way the merchant 
can get made and browse those  
automatically.  All I have to do
 is confirm and pay. 
   The second thing I have to 
passion in are details of the 
transaction i tself.  There are 
only two required things, you 
have to pass in a label, library
 purchase amount, for example, 
and you have to pass in an 
amount of money that you are 
looking to get paid on.  There 
is a currency code.  This could 
be U.S. dollars or Australian 
dollars and a value.  And we use
 that to render the appropriate 
amount on the screen.  You can 
pass in an optional set of 
display items.  These are things
 that show up that basically 
inform the user about how that 
dote amount of money is reached.
  These are totally optional 
because it helps explain how the
 total amount of money was 
reached.  Taxes, shipping costs,
 et cetera. 
   And the final thing you can 
pass into PaymentRequest is your
 completely optional set of 
additional information you might
 need to complete your 
transaction.  So when we saw the
 Kogan demo, you saw what they 
had done was requested shipping 
true.  This tells the browser if
 you have addresses stored, 
leverage those so the user can 
select them.  This is a fullly 
dynamic system.  So when I tab 
an address, we take that 
address, we bundle it, send it 
to the merchant so they can 
dynamically calculate shipping 
options and there is a to update
 us on the set of available 
options so if makes a robust 
rich experience.  You can 
request things like email, phone
 and
 name. 
   Email was in the Kogan demo 
because it's utilized for a new 
customer like me who never 
purchased to be able to send an 
email transaction receipt.  All 
of these are optional.  The only
 thing you have to have is an 
amount of money and a way that 
you can get paid, one way you 
can get paid.  And now you have 
to put it all together, of 
course.  So quite simple, you 
just construct a payment 
request, so you pass in the 
three components that we just 
talked about. 
   Then whenever you are ready 
to show that payment sheet, you 
call. call dot show.  And that 
is the function that says slide 
up the payment sheet.  That 
returns back a JavaScript 
promise and you will wait and at
 some point you will get back a 
payment importance.  And the 
payment response is just a JSON 
object that contains the data 
you requested and that you need 
to facilitate the transaction.  
So you will get card number, 
name, big dress, even CVV to run
 C. 
   If it's prop bright Terry 
form of payment, you will get 
the data for the payment.  
Android pay is supported on 
this.  If you got  an Android 
pay response, you would be able 
to pull out the tokennized form 
of payment and send it to the 
payment processor.  Once you are
 all done, you just call 
complete, and that's your way of
 telling the browser, h ey, I'm 
finished with this transaction. 
 Go ahead and close the payment 
sheet.  That then also returns 
back a promise that will resolve
 when that payment sheet is 
completely closed down.  That 
way you have great UI bips that 
you want to flip or whatever, 
you can do it s eamlessly with 
the closing of our animation.  
That's it.  That's as much as it
 takes to actually get that 
great experience we just showed 
on Kogan.com. 
   There is one other API I want
 to mention or method which is 
can make payment.  Can make 
payment is an API that allows 
you to ask payment requests if 
the user has a form of payment 
already active and ready to go 
before calling dot show.  So if 
the user doesn't have anything 
set up, you will get back false.
  If you have something set up, 
you will get true.  We think 
this API along with the set of 
optional information at the end 
there x that third component 
allow payment requests to be 
really flexible and they can 
work into all of your different 
flows.  So it's really nice, you
 can just use it as simply as a 
payment mechanism just for a 
credit card or it can facilitate
 the entire flow including 
shipping address, names and 
phone numbers and we have 
merchant shipping all different 
variations of this. 
   Now, as I mentioned, one 
thing we are happy to announce 
is we are coming to every single
 Chrome platform.  We announced 
Android last year and now we are
 happy to announce it's coming 
everywhere, Android, L inux, 
Windows, Chrome OS, Mac as well 
as Chrome for iOS.  For users 
that are synced, all of the data
 is stored and synced across all
 d evices.  So the minute I sign
 in from one to another, all of 
the data is there and ready to 
go.  There is a theme with 
payment request, which is let 
the browser help.
   &amp;gt;&amp;gt; You we have got this data 
stored.  Our users trust us to 
store it.  What we want to do is
 share it with you.  All we need
 is user consent to do so.  So 
think of payment request as a 
way for user to grant consent to
 share this data with you 
seamlessly and easily.  Now, 
something else I'm excited to 
announce is that we also have 
support for all Google forms of 
payment built in.  So if you 
went to the keynote yesterday 
with sley dare and Polly you are
 seeing we built this great new 
payment experience where all 
forms of payment in Google are 
grouped under a single 
application.  This will work 
inside of Chrome and 
PaymentRequest where most of the
 heavy lifting  is done by us.
   You are passing that Google.
com/pay overrer and getting all 
of the benefits of the Google 
pay system built in and access 
to the hundreds of millions of 
Fops. 
   Something else I think is 
also incredible is the fact that
 PaymentRequest now also works 
seamlessly with AMP.  So if you 
are not familiar, AMP our 
Accelerated Mobile Pages.  They 
are incredibly fast loading 
pages that load instantaneously.
  In this GIF on the right-hand 
side you will see a live example
 of this working.  It all 
started  from a search, you 
search, you tap a result and the
 page loads basically instantly.
  So you get, so a user can very
 quickly go from searching to 
seeing the entire product 
information.  They tap that buy 
button on the product page and 
the payment request sheet 
collides up. 
   All of the information  is 
there, you are ready to 
transact.  The entire experience
 from front to back can be done 
in less than a minute.  AMP has 
seen incredible success.  If you
 haven't yet looked into into 
AMP I would encourage you to 
check it out.  If you have AMPed
 your product pages, I would 
encourage you to look into 
leveraging payment requests to 
facilitate that transaction 
right there on the page. 
   Now, I know I'm talking about
 a lot of Google here, Google 
Chrome, AMP, which is an 
open standard, Google the 
payments, but one thing that is 
really great today to be able to
 announce is that this API is a 
cross browser API.  We talked 
about openness, and openness 
means it should work everywhere.
  This is really great.  Edge 
has launched Samsung Internet 
has launched Chrome has launched
 and FireFox is in development 
and launching soon.  We are 
seeing a lot of movement in the 
industry which we think is 
incredible.
   (Applause).

there is commitment in the 
industry to solve this problem 
and to really make Web payments 
compelling and to give users on 
every platform independent of 
their browser choice a great 
checkout experience so it's been
 great working with these 
players inside of the W3C.  As I
 mentioned, this is available 
now.  This isn't just vaporware,
 it's not something we are 
launching now.  PaymentRequest 
exists and we have a number of 
great merchants literally around
 the globe shipping 
integrations, we have got Wego. 
 We have K ogan in us trail yag.
  We are excited and thankful to
 these early partners and these 
are just some of the partners 
shipping payment requests right 
now. 
   We want to bring the benefits
 of PaymentRequest and seamless 
transactions to all mer chants 
independent of size.  One way to
 get the reach is to work with 
great channel partners, wom p 
mobile, woo commerce, mob by 
few, shopify.  By building right
 in, we can bring benefits to 
all merchants because everyone 
should be able to give their 
users stellar experiences.  So 
we are really happy to work with
 all of these great guys. 
   Now, I realize that this can 
be overwhelming, and that 
checkout flows are complicated. 
 You are like, Zach, this all 
sounds great, but you have not 
seen my check out flow.  I have 
guest check out flow, I have 
registration, sign in, coupon 
codes, all of this stuff, and I 
realize it's a big ask to go 
from not using this new thing to
 leveraging this new thing.  So 
I want to throw out a potential 
place to consider getting 
started. 
   It starts with the challenge.
  The question is to go back and
 ask y ourself, what percentage 
of your transactions  especially
 on mobile have only one item in
 the cart at checkout.  So go 
back, run your numbers and 
figure out what percent of 
transactions only, have only one
 item in the cart at checkout. 
   And, again, focus on mobile. 
 I think you will be surprised 
at how high it is.  We ask this 
question to our partners all of 
the time because we are curious,
 and it's always higher than 
what we expect, but we have been
 amazed to see that it's up to 
80% sometimes of checkouts that 
contain a single product.  But 
if that's the case, why are we 
still stuck to a legacy system? 
 It's like the way we walk into 
a physical store is I grab my 
cart, I like walk down the 
aisle, I add items into my cart 
and then I go to the front and I
 check it out and that's the 
whole process. 
   And then we brought eCommerce
 into the world and we thought, 
we will stick with the same 
model.  We will go ahead and 
have virtual aisles and the user
 will have a virtual shopping 
cart and they will walk the 
aisles and add items in and 
click on the shopping cart to 
review and check out.  And then 
form factors got smaller and we 
are like we will keep the same 
model.  Users will go on tiny 
devices, navigate our virtual 
aisles, add to the shopping 
cart.  The flow ends up like 
looking for up to 80%  of you're
 users are on a product base 
having to tap on the product, 
tut in in the cart.  And review 
the cart and hit the checkout 
process. 
   Here is my recommendation or 
something you should consider to
 get started, which is consider 
PaymentRequest as a buy now 
button that you can add directly
 to your product pages.  It's a 
way to simply get started.  You 
can leverage tools like can make
 payment and request shipping 
and request email address to 
facilitate that entire 
transaction right on the product
 pain.  So just as an experiment
 and see how it performed.  I 
think it's worth giving users, 
especially those that only want 
to make a single fast purchase 
the option to do s o. 
   With the tools like can make 
payment you can do so in a way 
where you have full control of 
the system so they don't have a 
seamless experience built in, 
that's fine, skip it and go to 
legacy flow, but we thinks there
 is a great opportunity here to 
have an impact and drive up 
conversions.  And then later if 
this proves successful, you can 
consider how to leverage payment
 requests in your default 
checkout flow as well.  Now, we 
have talked from the very 
beginning about openness, and 
when we first announced 
PaymentRequest, we said it was 
never about just making Google 
forms of payment easier, 
although, of course, we care 
about that.  But it's 
recognizing that for true 
openness means that everyone 
should be able to participate in
 the ecosystem. 
   So one of the things we are 
realize happy to announce today 
that we have been working on for
 a long time is we are 
officially opening up payment 
request so that third party 
payment application providers 
can participate directly in the 
ecosystem and show up in that 
exact payment request sheet.  
Because the reality is that 
payments, the Web is g lobal, 
and payments are global, and if 
we want to have global scope, we
 have to open it up so that 
everyone can participate freely.
   And we think this is really 
great.  But to talk a little bit
 more about this, I want to 
invite a couple of my colleagues
 from the W3C Web payments 
working group who we have been 
working with, let me welcome to 
the stage max and Jiajia to talk
 more about this.
     Snoorn hello, everyone, I'm
 max and I lead the 
international standards for the 
Alibaba group.
   &amp;gt;&amp;gt; Hi, everybody, I'm, Jiajia
 I'm seen jor Engineer.
   &amp;gt;&amp;gt; Before we get started, how
 many have heard of alley  
Alipay.  Whoa.  How many have 
used Alipay to make a 
transaction?  Not bad.  So 
mobile payments are very popular
 in China.  For example, Alipay 
is used for online shopping, 
taking taxis, the Supermarket 
and lots more.  We can use 
finger print based detection and
 other bio metric authentication
 methods.  It's very secure and 
convenient. 
   Alipay is not only a public 
channel it's a global brand.  We
 are considering to bring Alipay
 into the global market we found
 that mobile Web is the popular 
venue for people doing online 
shopping.  We wanted to bring 
the good user experience of 
Alipay directly into the Web 
ecosystem and we see that the 
payment request API standard is 
the best way to do this.  We 
joined W3C and the payment 
working group to work with 
Google to make sure that the 
payment request API standard can
 support payment app.

instead of PaymentRequest API 
it's quite simple.  You need to 
update your Android Manifest.x 
ml fine and add metadata to 
specifies your payment name.  
For pay it's ally Pay.com lsh 
Web pay.  Your payment Apple 
then receive the payment 
request, the user intent and 
should reply with the correct 
response for your particular 
application..  Working with 
payment app in browsers has 
challenges, if there is a  
phishing attack and Alipay is 
displayed on user form.  We 
don't want the browser to open 
up the fix
 app.

filters contained on the 
website.  These contain various 
right of useable information 
used to verify authenticity of 
the payment app.  The first is 
what we call the payment amount 
manifest file.  This is 
downloaded by the browser using 
the payment method and 
identifier information which is 
ally Pay.com/web p ay.  It 
issues an HTTP header request 
which points to the 
classification of Alipay.com. 
   Instead of this payment 
manifest we can then define the 
second location of our second 
manifest, the Web App manifest. 
 This is where we define the 
information about our 
application including the 
package name and the finger 
print information.  What is 
important is that only Alipay 
has control if this manifest 
fails and the browser tries to 
first download the manifest and 
verify that the signatures map. 
 Let's look at a demo and see 
how this works.
     So our demo is real.  The 
first demo the merchant selects 
multiple payment masters and the
 user click buy button, Alipay 
can show up as one of the 
available payment masters.  The 
Apple be opened up and the user 
can use their finger print to do
 the authentication.  It's very 
convenient and secure.
   (Applause). 
   So in the next demo, the 
merchant only support Alipay so 
the merchant can even use pay 
with Alipay with button and with
 one click and within several 
seconds the payment could be 
finished.  It's very convenient,
 and I think is that it will 
help you to spend your money 
more easier. 
   Cool!  So we are currently 
working hard with Chrome team to
 bring this feature into 
production.  If you are a 
developer, you can use this 
feature in Chrome in Alipay, you
 can use the Web and hopefully 
your mobile apps in the near 
future.
   &amp;gt;&amp;gt; Alipay is the world 
leading payment platform.  We 
have sponsored more than 18 
currencies, USD, Hong Kong 
dollar, Euro, pong, we are 
continually improving user 
experience and security so that 
technology innovation and 
implementing of standards.  
Thanks for being here.
   &amp;gt;&amp;gt; Thank you.
   (Applause).
   &amp;gt;&amp;gt; Zach Koch I think it's 
exciting to see an industry like
 this evolve and we are happy to
 be a part of it and pushing for
 openness on the W eb.  As they 
mentioned, I will quick lerecap,
 there are three actual steps to
 get started integrating your 
application today.  One, you 
will define your identifier, for
 Google.com/pay that's what it 
is.  For Alipay, it's Alipay. 
Alipay. Alipay.com slb Web pay. 
 You will make updates to your 
manifest file and your Android 
App, and then finally you will 
set up manifests on your Web 
server so you can be confident 
that Chrome is opening the right
 application. 
   So really simple, really 
great, and I'm happy to announce
 that we are already working 
with really great partners so 
you will expect to see Samsung 
Pay coming to Chrome, Alipay as 
you saw in the demo as well as 
Square Cash and I number of 
others.  So I think you will see
 a lot of evolution o in the 
space over the next three to six
 months. 
   Now, I recognize there is a 
bit of irony here which is that 
we are on the Web track talking 
about how great the Web is and 
how amazing it is, and then I'm 
up here saying our first 
integration is with native 
Android Apps.  This is for a 
couple of reasons.  One is 
because we do think that there 
are great experience that's can 
be built with native 
applications.  You could access 
things like fingerprints.  So 
you could have bio metric 
authentication and it makes 
great experiences.  The second 
thing it's just easier to get 
started.  I have great news 
which is that we are actively 
working on pure Web App support.
  So you get all of the benefits
 of the Web like no 
installation, immediate 
availability and global reach.  
So Web Apps can be full first 
class citizens inside of the 
payment request system. 
   We think this is actually 
really compelling.  We recognize
 that users love to pay with 
certain brands but don't 
necessarily have the apps 
installed.  That's okay.  We are
 fully working and committed to 
bringing the support and today 
I'm excited to announce that we 
are w orking with PayPal to 
build out this experience and 
bring PayPal's Web App directly 
into the payment request 
experience.  So look forward to 
sharing more about that over the
 next few months. 
   Now, we talked about a lot 
today.  There are a lot of code 
samples, a lot of overviews.  
Questioned a lot of great 
integration guides out there.  
We have PaymentRequest guides, 
we have how you can get started 
integrating your Android payment
 application, and we have a 
great code lab that you can run 
through which will help you get 
familiar with the  
PaymentRequest system.  I 
encourage you to check those 
out. 
   After this we do have a 
mobile Web tent which is just 
around the corner out this door 
to the left.  So I would 
encourage you to stop by, say 
hello, see some of these things 
in action and ask the hard 
questions.  So thank you so 
much, everyone.  It's been a 
pleasure to be up here today.  
Hope to talk to you afterward.
   (Concluded at 11:04 CT).
giving the keys to everyone.  We
 are giving the keys to everyone
 for everyone to our Web pages. 
 So as soon as we add a script 
from a third party to a 
Web page all bets are off.  If 
you can't trust the source, they
 can do whatever they want on 
your Web page.  It's like giving
 the keys to a
 stranger.  What we get is 
non-responsive content, content 
shifting around that is not 
really great to 
e xperience.  And we just had 
that so I'm going to skip this. 
   Now, the current situation is
 pretty bad.  We have over 200 
server requests per mobile Web 
page on average and 19 seconds 
is the average mobile launch 
page time 
over 3G out of the 19 seconds, 
77% of all mobile sites take 
longer than 10 seconds to load. 
 Don't worry, it gets worse.  In
 fact, if you look at the 10 
second number in particular, 
it's really important because at
 100 milliseconds clicking on a 
button feels instant.  One 
second it still feels natural, 
you have the context model, but 
then at 10 seconds, you lost the
 user's attention.  And, y es, 
that has been measured multiple 
times before.  The fact that 
after 10 seconds if you load a 
mobile page almost all users 
drop off. 
   Now, if you have been 
listening to the previous, yes, 
that means 77% of all pages are 
likely never seen on mobile.  So
 that's why we created AMP to 
combat this problem to bring 
back the beauty of the mobile 
Web.  The AMP HTML which is a 
super set and subset of HTML is 
adds new components but 
restricts things you can do.  We
 have AMP JS that powers those 
things and the AMP caches.  Just
 two quick examples what AMP d 
oes, a few of the things it 
does.  It can prioritize content
 leading so it knows where 
everything is rendered on a page
 before external assets are 
loaded so we can pre-render the 
above the fold and not render 
anything below the fold.  And 
the caches can then use that, 
the platform that uses the 
caches to deliver the AMP pages 
can use that to smartly 
pre-render quite a set of pages 
so you are arriving, for 
instance, on Google Search and 
you get the top storage 
carousel.  Some of those things 
are  pre-rendered when you pre 
click on them which is why they 
feel instant.  If we go back one
 step and look at this 1999 
versus 2017 how does this look 
like in AMP?  Well, let's have a
 look. 
   Yes, I think that looks way 
better.  In fact, that's what 
you do.  You create HTML markup.
  Because we have high level 
components like accordions and 
those things in the HTML marker,
 if you add some CSS, out of it 
you get a fairly responsive nice
 responsive,  interrive content 
page, and it can be on a cheap 
server because the caching 
server on the top caches your 
content to be displayed in the 
apps so you get back to a really
 cheap deployment model.  So why
 progressive Web Apps?  Well, 
they are engaging.  They give 
you personal notifications and 
homescreen stickiness and they 
are also reliable.  They give 
you offline access and response.
 are the things that you know 
usually from the native world of
 things.  And that's a common 
reason why developers built 
native apps but 80% of the time 
is spent in the top three native
 apps on a phone.  And this zero
 number of apps that get 
installed on average per month. 
 So that means that it's very, 
very expensive, for instance, 
through advertising to acquire 
new users through the app store 
model. 
   And so what we want he is 
enspailly is a -- essentially is
 a combination of those things. 
 We want the capabilities of the
 native app system but we want 
the reach of the mobile Web.  We
 get that with progressive Web 
Apps because they have many of 
those features built in and they
 are also basically a website. 
   So it's a pretty good way to 
get both of those things.  So 
why is the combination of those 
two a good idea?  They seemingly
 seem to attack different spaces
 on a different level.  Well, if
 you just bold a progressive Web
 App, then you have the 
challenge that your first load 
is still going to be slow and 
the reason is that progressive 
Web Apps depend on a technology 
called service worker a client 
side proxy that accelerates and 
caches the delivery of your app 
share of articles and et cetera.
  The service worker only kicks 
in after the second request. 
   So on the first load, you 
don't get those performance 
benefits and usually it's the 
first impression that counts if 
you want to get a new user.  But
 then on the AMP side of things 
you get no user authored 
JavaScript.  There is no custom 
service worker and no piewsh 
notifications no Web App 
manifest which served from the 
AMP cache.  When served from the
 AMP cache you don't get all of 
those benefits.  So that is in 
in order to be predictably fast.
  On the AMP site you get 
reliable instant delivery and 
optimized discovery but no 
JavaScript,  static content 
mostly, but on the Progressive 
Web  app website.  It's slower, 
but you get access to the latest
 Web APIs, you can do whatever 
you want and it supports much 
more dynamic content. 
   And to combine those two is 
what we call start fast stay 
fast.  And n ow, of course, you 
could forget all of that and 
focus on the fact that the combo
 makes for some mesmerizing  
animations and if you have been 
d oing Web development in the 
90s and early 2000s, I think 
this one might work even better 
for you.  Wait for 
it!  Yes, okay.  Let's not go 
there. 
   How do you actually do it?  
How do you actually get there?  
Well, we have three application 
patterns that make it happen.  
First, I call AMP as PWA, and 
the third AMP in PWA.  Now, if 
you have been listening and I 
have been comparing the two, you
 are like, wait, S PWA?  We have
 for site that's have static 
content and don't have a lot of 
interactivity, you can have an 
AMP that is both a progressive 
Web App, in fact mynet who is 
one of the leading publishers in
 Turkey has done that with their
 pages. 
   So what you see here is a 
fully compatible AMP page, all 
of this is AMP with a full 
navigation concept back and 
forth carousel, whatever, all of
 it is saim,  AMP and it has 
progressive Web App features so 
it has a Web App manifest and so
 on.  Just by doing that, they 
saw tremendous uptick many quite
 a few numbers.  They got over 
25% higher revenue per article 
page view.  That is really an 
important number because that 
means the bottom line is number 
and four times average parter 
page load speed, over 40% 
average longer time on site, and
 much lower bounce rates. 
   So in the end, really 
something worthwhile to do.  But
 you can actually go further 
than that with our pattern.  You
 could actually, because you 
have the service work in place 
on your origin, you can insert 
anything into the AMP page that 
you want, random stuff that AMP 
doesn't like because you are
 in control so soon as the 
service worker is running.  Now,
 I had some nostalgia so I 
thought why not insert 90's 
magic into into the website and 
I found this amazing mouse 
cursor and this is exactly what 
I did. 
   So here we go, I will enable 
the service worker, I will 
reload, and, yes, I have a fancy
 mouse cursor and animated 
background, all of the things 
that AMP doesn't like, and first
 of all, what have I done?  So 
you shouldn't do exactly that 
maybe, but this pattern still 
comes in useful if you want to 
insert ads that are for instance
 not supported in the ecosystem 
yet or other things that you 
need on your origin to run. 
   So next is AMP to Progressive
 Web app.  This is based on a 
component that we call AMP 
install service worker and the 
AMP install service worker if 
you have used serviceworker 
before, you register it in 
JavaScript on your page and 
because you don't have access to
 JavaScript we van equivalent 
which is AMP service worker.  
This component also works when 
your page is loaded on the AMP 
cache let's say in the stop 
service carousel on Google.  It 
can install the service worker 
from your site's origin on 
demand so that sub subsequent 
clicks are accelerated. 
   The way this looks is the 
pattern is always the same.  The
 user discovers content, the 
service worker installs in the 
background while the user is 
consuming the corn tent and the 
user is upyaided instantly to a 
PWA.  Let's have a look at how 
this looks in real world 
production examples. 
   So first, let's look at 
crossing Xing, which is a job 
search site similar to Linked In
 all over Europe, and so if the 
user discovers content, they 
check out a job that they like, 
they click on it, and then the 
service worker installs in the 
background and this they are 
actually interested in one of 
those jobs, they are instantly 
forwarded to the progressive Web
 App that push notifications can
 remind them to come back if 
their job is not available 
anymore, et cetera.  So the next
 click does going to be instant 
-- is going to be instant. 
   Now, next example with 
goibibo which is a leading 
travel search company in India. 
 You get the same pattern, user 
discovers content, service 
worker installs in the 
background, now, this page is a 
page that just shows you the 
hotel, but now there is a get 
availability button at the 
bottom. 
   If you click that button, you
 are instantly redirected to the
 progressive Web App that allows
 you to check availability.  We 
could have even checked in 
advance in the service worker if
 there is availability so that 
even the API call wouldn't be 
necessary anymore.  And then 
Rakuten, Japanese recipe website
 that they have created recently
 is called Rakuten recipe.  And 
in this case, same pattern, the 
user discovers the content, 
arrives on the recipe site.  You
 browse through the recipe.  You
 realize there is a couple of 
connected recipes I like more or
 I want to book mark this.  And 
then the user is instantly 
upgraded to a Peg app. 
 P r o g r e s s ive Web app. 
   If you look at the different 
combination of things they have 
done it's quite impressive.  
They have seen over 50% time 
spent peruser.  3.6 times higher
 CTR within the AMP page 
compared to other Rakuten 
recipe, two other sites, and 
then with add to homescreen with
 the Progressive Web app feature
 they are seeing
 over 70% per unique user and 
3.1 times more page views per  
unique user.  A lot more 
re-engagement, three times more 
weekly  sessions per user, and 
five times after first month.  
This is really important. 
   With push notifications, they
 get 75% lower
 bounces for those users coming 
in by push notifications an by 
social shares.  So they are s 
eeing much more re-engagement 
through that channel.  Okay.  So
 this sounds hopefully pretty 
good.  But there is still a 
problem.  Now, if you copy the U
RL in the Progressive Web app, 
you share or send it to a 
friend, then that friend will 
not go through the AMP install 
service worker flow, so they 
will open the Progressive Web 
app without a warm cache.  What 
do we do with this? 
   Well, usually you would have 
AMP pages deployed on one sub 
domain and you have the 
Progressive Web app deployed on 
another sub domain and a link 
from the Web page leads to the 
Progressive Web app.  Sounds 
straight forward.  However, you 
can change that and reuse the 
same domain, the same URL space 
for both of these, and you can 
do that because the service 
worker can intercept navigation 
requests.  So if you click a 
link, the service worker can 
say, okay, no, I'm not going to 
give you the next AMP page.  I'm
 just going to give you the 
progressive Web App instead even
 though you are on the same 
domain. 
   It's just a few lines of code
 to do that.  So you check for 
navigation requests, spooned 
with a Progressive Web app.  
Now, what happens if we do this?
  A couple of magical things 
because without the service w 
orker, we still just get AMP, so
 for browsers that don't support
 it, you always get a fast 
experience and then with service
 worker you get a combination of
 AMP in the background and the 
Progressive Web app because the 
service worker intercepts and 
delivers to the Progressive Web 
app instead. 
   And if you don't have a 
service worker and you still 
want people to lead to that 
richer experience, a Progressive
 Web app that might not work 
fully because there is no 
service worker in some browsers,
 you can still do that with a 
technology that we call 
rewriting that's part of AMP 
install service worker.  What is
 this does is it says defect if 
service worker is available in 
this browser.  If it's not, 
rewrite the URLs on this  site 
to go to a fall back domain. 
   Now, we have this complete, 
but now we have another problem.
  We still have this problem 
with our deploy targets.  We 
still have plenty of deploy 
targets and in this case we have
 two content back ends, we have 
AMP HTML on one site and 
probably some Jayson or   JSON 
or some other that powers the 
Progressive Web app.  Now, that 
is where the final pattern comes
 handy
 in Progressive Web app.  And in
 fact AMP pages aren't just Web 
pages.  They're ultra portable 
embeddable content units that 
can stand on their own. 
   If you think about it this 
way, don't think about it just 
as a website, some magic things 
can happen because we can get 
from this step to this step 
where AMP HTML powers all of my 
experiences, in this case the 
AMP experience and the 
progressive Web App experience. 
 Now, one way to do this is to 
build an application shell and 
use an iframe.  But i frames are
 slow. 
   And what we do instead 
because we trust the content, 
it's our own content, we use 
shadow DOM.  Without shadow DOM 
you get one window for every 
iframe that I initialize, 
infinite number of AMP library 
instances and an infinet number 
of documents so lots of overhead
 to initialize the AMP library 
cover and over again for every 
iframe, but then with shad dome 
 shadow DOM, we have one library
 instance that we call a shadow 
AMP in the top of the page and 
it connects to lots of 
documented and
 renders them out.  So it's a 
much cleaner flow. 
   And in practice the PWA high 
jakeses and a half gabbing 
clicks, puts the content into 
the shadow, and call it's shad I
 do DOM.  But this was a slide 
that showed it just very 
briefly.  I also want to show it
 in more detail because we have 
a little bit more time today.  I
 think you can do this in an 
hour.  This flow of inserting 
content via the shadow DOM if 
you already have AMP pages I 
think you can do it in an hour. 
   If you don't believe me, 
let's do it in five minutes, 
okay?  So the first thing we 
want to do is, and this is a 
caveat, we want to have a 
content source somewhere that 
serves as the navigation.  So in
 this case, we need to have an 
overview page from somewhere 
that has a bunch of links, a 
bunch of images maybe, in this 
case, I just used YQL to fetch 
an RSS feed from somewhere which
 is a nice hack to create a 
compelling demo and this gives 
me a nice list of articles that 
I can use. 
   I used the guardian as a 
background, thanks again the 
guardian for letting me use the 
RSS feeds.  And this is what I 
have so far.  So this is just 
using the RSS feeds through YQL 
and so far so good.  And just 
again, this step is not so hard 
to do so I shouldn't be doing 
this, but I started on Monday.  
And so next one we add the AMP 
shadow library to the head of 
the page in our progressive Web 
App. 
   Now, again, this is a special
 version of AMP, and we wait 
until AMP is loaded, which is 
using this technique that you 
find a lot of frameworks 
nowadays.  So this is just a 
wrapper that the code inside the
 wrapper runs assuming AMP is 
fully loaded.  And then when you
 have that, you fetch the AMP 
Doc via XML HTP requests. 
   Now, again, straight forward,
 you know the link from the 
previous navigation you know the
 link to the full article, you 
fetch it, and then you return 
something that we call response 
XML.  Now, response XML contains
 a ready to use document object,
 not that well known of 
XMLHTTP request.  This is why 
I'm using old school, XMLHTTP 
requests.  Now, we have the 
actual document, the AMP 
document.  It's not rendered 
anywhere yet, but now, we render
 it using shadow AMP.  So we 
create a shadow root.  What in  
 this means it's a fan say way 
of saying we create a Dif, a 
container and we give that Dif 
and call it shadow root. 
   Now, the AMP Doc does all of 
the magic, so we will give it 
the container we just created.  
Actually this should say 
shadowroot.  So that's just back
 a slide.  And then we give it a
 Doc that was just fetched by 
Ajax and we give it the original
 URL.  And in this case, we can 
also have a function that 
notifies us when the pages are 
ready and rendered out. 
   Now, that's the basic flow, 
but there are a few things that 
make it even better.  For 
instance, if you don't need 
certain things in the AMP page 
because the AMP page usually has
 its own side bar, own 
navigation so it can stand 
alone.  If you don't need that 
stuff while in embedded mode, 
you can actually remove those 
things before you give it to the
 AMP shadow library.  You remove
 the header, remove the side 
bar, et cetera.  So this is 
actual code from the demo. 
   But also cool if you don't 
want to do something as complex,
 that's cool too because there 
is a class that we add to the 
body of the shadow AMP of the 
embedded page so that you can in
 your CSS of your AMP page you 
can use the dot AMP shadow page 
to hide things you don't want in
 embedded mode.  So that  works 
as well. 
   And finally, if you don't 
want to remove things but you 
want to insert things back into 
the AMP page, features like 
JavaScript highlighting or 
something like that, into the 
AMP you can do that as well.  
They provide a path for 
progressive enhancement of AMP 
Docs when shown in the 
publisher's own context.  How 
does this look like in action.  
Let's have a look.  Here we go. 
   So the content experience is 
now all powered by AMP.  As soon
 as you click on one of those 
links in the overview, the AMP 
page renders and loads and 
because it's everything in our 
control, we have the whole 
document available, we can do 
some really nifty transitions to
 animate the Windows too instead
 of a full page
 load.
   (Applause). 
   Thank 
you.  Now, finally, let's wrap 
things up.  With the progressive
 Web AMP sometimes I also call 
it PWAMP because I like the 
found of is, it's fast no matter
 what.  Great distribution built
 in, it's progressively 
enhanced.  It's one back end to 
rule them all, and less client 
complexity to fewer deploy 
targets.  Remember how we had 
the plenty of deploy targets 
before in the previous slide, 
when in fact we are now down to 
just three if we want to.  We 
have that PWA shell, the native 
shell and iOS, and those are 
thin layers that serve as the 
application shell that provide 
navigation model.  And then AMP 
is the back end that runs it 
all, the content source.  All of
 those could use AMP as a 
content source. 
   And so you support the Web, 
you support Android and iOS at 
the same time with very thin 
layers of extra code.  So 
finally, a word of c aveat, when
 do you actually do this?  Well,
 you do this when you have a 
site that has lots of Stottic 
content.  You don't want to do 
it if you build the next Gmail 
because it's more like a really 
dynamic app that doesn't have 
landing pages, what we call leaf
 pages that are accessed by 
organic circumstance 67 offer 
social discovery.  To it has to 
have static content for if to 
make sense.  And you have a 
large corpus of AMP pages that 
gets you through the first step.
  This works extremely handy if 
your engineering resources are 
con  constrained or to reduce 
truck complexity and finally you
 need to test out, if you 
haven't done AMP you need to 
test out if your content 
monnizing works within the AMP 
ecosystem. 
   Before I leave you and I 
think I have time to open the 
room for questions, I will leave
 this up for a second so you can
 take some screen shots.  So we 
have a React sample app as well.
  That's not the one you saw 
today, but it uses React so it's
 great for millennials.  Use the
 React sample app to see how you
 do it in react.  It has similar
 nice transitions.  We just 
launched the AMP channel if you 
are not sick of my face yet, you
 can see it again there, and 
AMP, we just published a big new
 guide about these patterns we 
talked about if you want to 
recap those things and go 
through the tutorials, you can 
do that on A MP.org.  With that 
I will open for questions. 
   Thank you, everyone.
     We have two mics on each 
side, please go ahead if you 
have 
questions. 
   &amp;gt;&amp;gt; AUDIENCE:  You mentioned 
embedding AMP within your iOS 
and Android apps have you ever 
thought about embedding PWAs as 
well.  When I think of the I/O 
app it's identical other than 
the Web, iOS and Android. 
   &amp;gt;&amp;gt; PAUL BAKAUS:  That's a 
very good point.  I think there 
is potential to do that.  I 
mean, you could easily do that, 
however, I think as far as I 
know, there is still a fuse 
gotchas with push notifications 
and service workers in the 
context of a Web view.  So with 
a native Web view, with a native
 app what you want to do is you 
want to really create something 
like an iframe, right, as part 
of your app, and that only works
 if you use a Web view as 
opposed to, for instance, Chrome
 custom tabs which is really a 
big overlay.  So the Chrome 
custom tabs they support push 
notifications service worker but
 the Web view, I could be wrong 
on this, but I don't think it 
supports push notifications, I'm
 not sure if it supports service
 worker. 
   &amp;gt;&amp;gt; AUDIENCE:  You could use 
Android push notifications. 
   &amp;gt;&amp;gt; PAUL BAKAUS:  I think you 
can do it, but you have to go 
through some hoops to make it 
work. 
   &amp;gt;&amp;gt; AUDIENCE:  Hi, Paul, so if
 an AMP page can install a 
service worker and my PWA can 
also install a service worker so
 the people who are coming to my
 website, if I do not want them 
to have the similar 
functionality of pre-cache and 
my AMP page I would want the 
people to have the whole PWA 
pre-installed, can I have two 
different service w orkers 
installed one on AMP and one on 
my regular PWA? 
   &amp;gt;&amp;gt; PAUL BAKAUS:  Yes, you can
 do that.  Now, in this case, 
you probably still want -- okay,
 so you want a different service
 worker for AMP and PWA, right? 
   &amp;gt;&amp;gt; AUDIENCE:  Yes. 
   &amp;gt;&amp;gt; PAUL BAKAUS:  You can do 
that because you can look at 
what client is currently 
connected in the service worker 
and so it would still be the 
same service worker, but in the 
service worker, you check where 
the client is coming from, and 
then you basically decide what 
you want to do based on that. 
   &amp;gt;&amp;gt; AUDIENCE:  How would we do
 it inside the service worker? 
   &amp;gt;&amp;gt; PAUL BAKAUS:  There is a 
save.client declines, and look 
at the clines and the URL of the
 declines and you can decide 
what to do for that given URL.  
So that's one way to do it.  We 
o could try a little bit later 
if you want to. 
   &amp;gt;&amp;gt; AUDIENCE:  So my question 
was in the example that you 
showed basically you were like 
removing the header and extra 
information from what was pulled
 down from the PWA.  Or from 
AMP, I mean, is there a risk of 
overfetching in that case versus
 just grabbing exactly the data 
you need from like a rest end 
point or something like that? 
   &amp;gt;&amp;gt; PAUL BAKAUS:  Yes, so the 
example I had where I navigated 
and then I think what you mean 
is where I did the extra fetch, 
right? 
   &amp;gt;&amp;gt; AUDIENCE:  When you 
downloaded the page from AMP and
 you are like maybe you don't 
want the header and you don't 
want some of the other, with the
 AMP with the response that 
comes back it always has the 
extra information.  So does it 
send more data than you need?  
Is that a problem of 
overfetching extra data? 
   &amp;gt;&amp;gt; PAUL BAKAUS:  In theory, 
yes, in practice, I don't think 
so because bandwidth is usually 
not the constraining bit when it
 comes to mobile connections.  
It's usually latency.  So the 
extra amount of bandwidth which 
hopefully will be a kilobyte or 
so because it's just the page i 
tself is hopefully not large 
enough to make a difference, 
however, what you could also do 
so that you don't even have the 
JavaScript open, you could 
actually do it in the service 
worker.  So in the service w 
orker, when you get the fetch 
event and you proxy the outgoing
 cache result, you can actually 
do the sanitizing in the service
 worker before it even arrives  
on the client. 
   Now, that still won't solve 
the bandwidth questions but it 
will solve the JavaScript 
overhead of doing it on the 
client. 
   &amp;gt;&amp;gt; AUDIENCE:  That makes 
sense.  Thank you. 
   &amp;gt;&amp;gt; AUDIENCE:  What's the 
roadmap for supporting payment 
components and supply browsers 
such as Apple
 Pay or AMP? 
   &amp;gt;&amp;gt; PAUL BAKAUS:  So we 
actually have a few demos that 
use payment API that work that 
use an iframe technique and AMP 
by example.com just added one 
that seems to work. 
   &amp;gt;&amp;gt; AUDIENCE:  I think what we
 saw was a little bit of -- 
because an i frame is not really
 native to the AMP ecosystem so 
there might be some sort of 
performance sort of I wouldn't 
say conundrums but challenges, 
and it's made as a formal 
component.  I was asking mainly 
for safari browser because on 
the Chrome browser it looks like
 a good experience for sure and 
we actually are launching that 
today. 
   &amp;gt;&amp;gt; PAUL BAKAUS:  I haven't 
looked into the safari scenario 
whether you could do the same 
things with Apple Pay.  It would
 be great to create a demo for 
that.  I will take a look at 
that. 
   &amp;gt;&amp;gt; AUDIENCE:  We will talk 
afterward then. 
   &amp;gt;&amp;gt; AUDIENCE:  Hi, Paul, in 
the one slide you showed you had
 the example sort of stalking as
 AMP -- talking as AMP source 
content user for iOS, Android, 
Web.  That is still just a 
mobile only view of the world, 
right?  Like we are not, AMP 
still isn't really tackling 
responsive design or dealing 
with cross screen sort of 
snaresios. 
   &amp;gt;&amp;gt; PAUL BAKAUS:  I should 
have talked about this.  The 
answer is no.  It's not a moibl 
only view.  AMP has 
responsiveness built in from the
 bottom, and fully supports.  So
 the story I had on screen meant
 you are using it for 340EB8 and
 desktop, and in fact you can 
see that with some examples, 
like, for 
instance, AMP.org is fully 
responsive. 
   &amp;gt;&amp;gt; AUDIENCE:  What site? 
   &amp;gt;&amp;gt; PAUL BAKAUS:  AMP project.
org is fully responsive and is 
fullly AMP.  It works on 
desktop, everywhere.  So you can
 do it.  There is also a new 
website just launched in Germany
 with a partner that has done 
it.  So there are definitely 
ways to do it.  In fact the only
 things that you won't get if 
you do that is the, some of the 
platforms that serve AMP pages 
don't do anything special once 
they see it on desktop.  So, for
 instance, the Google top 
storage carousel will not do the
 pre-rendering on desktop, but 
you can totally use AMP on 
desktop just fine. 
   &amp;gt;&amp;gt; AUDIENCE:  Thanks. 
   &amp;gt;&amp;gt; AUDIENCE:  So a bit off 
topic, but the AMP are coming 
pretty good in the  Google 
Search engine.  So the AMP is 
coming in pretty front.  So is 
there any common practice to 
bring that into that place or if
 you just make it an AMP it 
becomes automatically -- 
   &amp;gt;&amp;gt; PAUL BAKAUS:  No.  So 
there are a few things -- first 
of all, I'm not working on that 
side of the project so I'm not 
going to comment too much, but 
there are a few components, a 
few experiences in Google Search
 that need AMP to run.  For 
instance, there is the top 
storage carousel that shows AMP 
documents if you search for a 
query that is in the news.  Now,
 however, it's a very
 narrow component.  So that 
means if you are not a 
publisher, if you are not 
talking about top news, you 
likely won't make it into the 
carousel.  That's because it's 
all about top news.  This is a 
content that is driven by AMP 
and that's why it triggers in 
the first view port.  AMP itself
 does not give you any explicit 
ranking benefits.  However, of 
course, the answer is a bit more
 nuanced because if you create 
an AMP page it's going to be 
fairly fast by default and 
performance as we said in the 
past at Google is a ranking 
signal.  So there are implicit 
reasons for why you would do A 
MP.  So usually we just say we 
can't comment on this, but 
because it's hard to navigate 
around this, but the answer, the
 short answer is no, you won't 
get an immediate benefit, but 
you are going to make a very 
fast page, and you get a lot of 
implicit benefits. 
   &amp;gt;&amp;gt; AUDIENCE:  So in my 
country we work with many  
publishers of top newspapers.  
So do you know where I can talk 
with whom to bring those AMP 
sites on the top of the search 
engine? 
   &amp;gt;&amp;gt; PAUL BAKAUS:  No, 
unfortunately not. 
   &amp;gt;&amp;gt; AUDIENCE:  The last thing,
 so we have a main site where 
all of the main contents which 
is not an AMP site, by the way, 
and there is an AMP version of 
it.  If we have the same content
 on both pages it is going to 
reduce the search engine ranking
 of the second one because it's 
like the copy paste of the same 
content in two or three place 
is? 
   &amp;gt;&amp;gt; PAUL BAKAUS:  Yes.  So 
there are two patterns of doing 
AMP pages one doing the paired 
approach.  One main page, one 
AMP page one is doing the one 
AMP page.  But in the paired 
approach there is content m 
ismatch, where the content with 
the canonical page doesn't match
 the AMP page.  If you want to 
see if that's a problem for your
 site, you can check that out in
 the search Console.  So you can
 go to the search Console and it
 would give you warnings and 
errors if your content 
mismatches which then sometimes 
means that AMP pages will not be
 properly indexed.  Look at the 
search Console and it will give 
you that information. 
   &amp;gt;&amp;gt; AUDIENCE:  Thank
 you. 
   &amp;gt;&amp;gt; AUDIENCE:  iframes tend to
 be slow.  If you have an 
internal page or something and 
you are Ajaxing a lot of 
otherwise static content, would 
it make sense to feed AMPed 
iframes into those instead? 
   &amp;gt;&amp;gt; PAUL BAKAUS:  What do you 
want to do? 
   &amp;gt;&amp;gt; AUDIENCE:  If we are 
Ajaxing relatively content in 
the UI, lazy l oading different 
parts of the page.  If we served
 static AMP pages instead of the
 Ajax calls is that going to be 
faster round trip? 
   &amp;gt;&amp;gt; PAUL BAKAUS:  I'm not sure
 I completely understand the use
 case.  You have a progressive 
Web App. 
   &amp;gt;&amp;gt; AUDIENCE:  No, just a 
page. 
   &amp;gt;&amp;gt; PAUL BAKAUS:  You have a 
page and you want to embed AMP 
into the page? 
   &amp;gt;&amp;gt; AUDIENCE:  If currently we
 are Ajaxing a lot of different 
content blocks into it and the 
content in those blocks are 
relatively static, could we 
instead embed iframes on the 
page and have those point to -- 
   &amp;gt;&amp;gt; PAUL BAKAUS:  You could, 
but, again, you will get the 
overhead of the AMP library
 initializing for the iframes so
 it would be slower for sure 
than just to use the shadow AMP 
technique.  I think we have to 
wrap up.  Thanks, everyone, 
again, I hope you enjoy the rest
 of the I/O .
     (Concluded at 12:10 CT).






&amp;gt;
&amp;gt;
   &amp;gt;&amp;gt;
   RAW FILE
   GOOGLE I/O 2017
   SAN JOSE, CALIFORNIA
   MAY 18, 2017
   10:30 AM CT
   STAGE 6
   THE FUTURE OF WEB PAYMENTS
   T37A77

           Caption First, Inc.
           P.O. Box 3066
           Monument, CO 80132
           800-825-5234
           Www.captionfirst.com
   ***
   This text is being provided 
in a realtime format.  
Communication Access
 Realtime Translation (CART) or 
captioning are provided in order
 to facilitate communication 
accessibility and may not be a 
totally verbatim record of the 
proceedings.
   ***
   &amp;gt;&amp;gt;
   &amp;gt;&amp;gt;
   &amp;gt;&amp;gt;
   &amp;gt;&amp;gt;
   &amp;gt;&amp;gt;







test test Python.






inearRegressor.


Tensors,.






   &amp;gt;&amp;gt;
   &amp;gt;&amp;gt;
   &amp;gt;&amp;gt; 
   &amp;gt;&amp;gt; MARTIN WICKE:  Hello, 
thank you for coming, I'm Martin
 Wicke.  Eye will speak about 
effective TensorFlow.  I will
 later introduce Francois who 
will take the second half of the
 talk.  So, TensorFlow, 
why?  So I have a son, you will 
see him later in the talk 
actually, and when I used to 
explain to him how image search 
works I would have to say 
something like we, there is a 
computer that looks at the 
metadata and looks where the 
images are, and now when I 
explain how this works I 
can just say, well, the computer
 looks at the  images and if you
 search for a cherry blossom it 
looks at all of them in the 
world and when it sees one with 
cherry blossoms it returns it. 
   You have seen a lot of this 
at this particular event, AI is 
now going to be everywhere and 
Machine Learning is everywhere 
and you, you know, it generates 
these products that were 
impossible to imagine before.  
What makes thee products work in
 reality is for us at least T
ensorFlow.  And this enables us 
to make the apps and make 
products that use Machine 
Learning and once you have 
written one of these models, one
 of these Machine Learning 
systems in TensorFlow you can 
deploy it anywhere in mobile.  
TensorFlow is truly what enables
 apps that tell hot dogs from 
not 
hot dogs. 
   So why don't we see that 
more?  Why railroad hot dog not 
hot dog apps so cutting edge?  
The main reason is complexity.  
And complexity is in various 
shapes.  First of all, 
complexity is computational 
complexity.  Computational 
complexity used to be a big 
thing and with Cloud and the 
data sets you can rent that's 
not an excuse anymore.  So we 
have solved this problem 
assuming your code can run in a 
data center. 
   But you have to contend with,
 you have to make your dowed, 
models, apps make on different 
platforms.  In order to train 
them in the data center you need
 to work on CPU, GPU.  We have 
had this thing called a TPU.  It
 should work there too because 
it's going to be fast and if you
 want to deploy it once you are 
done it needs to work on a 
mobile device.  If you are into 
IoT or embedded systems it has 
20 work there too. 
   to work there too. 
   So making that happen is 
hard.  And finally, Machine 
Learning itself is actually 
fairly complex.  So that's why 
we have 
TensorFlow.  And TensorFlow 
gives you distribution out of 
the box so that you can run it 
in the Cloud if you need to do 
that.  It works on all of the 
hardware you need it to work on 
and it's fast and  flexible.  
And what I will tell you today 
is that it's also super easy to 
get started.  That's why we are 
here. 
   So TensorFlow takes all of 
the details out of the 
distributed system and the 
hardware and hides them from 
you.  It takes care of them.  
You don't have to know them 
necessarily.  It's nice if you 
do, but if you don't, that's not
 a problem.  What you see is the
 front end.  And what I will 
talk abouted if  about it the 
Python front end. 
   The ghennerric thing that 
people used to say is TensorFlow
 is low l evel.  You are 
thinking about multiplying 
matrixes, adding vectors 
together, that kind of thing.  
Well -- what we built on top of 
that is libraries that help you 
do more complex things easier.  
We built a library of labors to 
help you build models.  We built
 training infrastructure that 
helps you to train a model and 
evaluate a model and put it in 
production.  And, again, you can
 do this with Keras, and Fran 
sway will talk about Keras and 
finally we built models in a 
box.  Those are full complete 
Machine Learning ago for rhythms
 that run and you have to 
instantiate one and go.  That's 
what I will talk about today. 
   So usually when you talk 
about oh, my first model in 
TensorFlow, it's usually 
something simple like let's 
have, let's fit the line to a 
bunch of points, but nobody is 
actually interested in fitting a
 line to a bunch of points 
distributed.  It doesn't happen 
all that much in reality.  So we
 are not going to do that. 
   I'm going to show you instead
 how to handle a variety of 
features, and then train and 
evaluate different types of 
models, possibly distribute it, 
and we do that on a data set of 
cars because we have that.  I 
have uploaded all of the code 
I'm going to show you to this 
address which has both an O and 
a zero so be careful, and what I
 have done there, it will work 
with TensorFlow 1.2 and I'm not 
sure whether it's  been 
announced previously but there 
is now TensorFlow 1.2 the first 
release candidate so my code 
will work with that and nothing 
earlier, so you have to watch 
out. 
   So the first model today will
 be about predicting the price 
of a car from a bunch of 
features about the car, 
information about the car.  So 
without anything more, let's 
just do code.  The rest of the 
talk will be code mostly.  This 
is it.  This is my model 
definition in T ensorFlow.  I'm 
exaggerating a little bit 
because this only takes into 
account three different things, 
so first we define the input and
 I'm defining three different 
what we call feature columns.  
I'm telling the model that if it
 should expect an input that is 
a categorical feature, a string 
that's called make in the  
input, and I'm going to 
transform this into something 
usable by my Machine Learning 
algorithm by hashing it.  It's a
 way of telling you what to do 
with the inputs.  The next thing
 is I will say there is 
something called horsepower and 
that's just a number.  I call it
 numerical number.  And then 
it's cylinders, um of  cylinders
 in the input..  That's a string
 but it has few values to I will
 give you the values directly 
here in the code. 
   So in this data set, it 
actually is the case that simple
 linters is encoded as the words
 2, 3, 4, 6 and 8.  So that's 
it.  So there are probably many 
more of these, but in principle,
 that's what you do, and you 
say, okay, this is what my input
 looks like, and then we specify
 what kind of Machine Learning 
algorithm we want to apply to 
this.  I will use first the 
LinearRegressor which is the 
simplest way to learn something.
  And all I have to do is tell 
them, hey, look, you are going 
to use these input features I 
have just declared.  That's it 
and I'm done. 
   Now, I still have to give it 
input data.  And TensorFlow has 
off the shelf info pipeline for 
many formats.  In particular, in
 this example, imusing input 
from pandas.  It's a Python 
library can read a bunch of 
stuff, process, it's nice.  So 
I'm going to read input from 
panda data frame and what I'm 
telling it here is I want to use
 the batches of 64, so each 
iteration of the algorithm will 
use 64 input data  pieces.  I 
will shuffle the input which is 
a good thing to do when you are 
training.  Please always shuffle
 the input, and  epocks means 
cycle through the data, if you 
are done with data do it again. 
 And then I can say, okay, train
 me my thing for 10,000 steps.  
And then what happens is that 
TensorFlow goes off and trains 
my thing. 
   This is what the log output 
looks like and the interesting 
information for it, what's more 
interesting is that TensorFlow 
will also integrate with other 
tools that we have.  In 
particular it will integrate 
with tenser board.  It's 
wonderful front end that the 
training system writes data for 
and you can visualize it and 
look into it. 
   And once of the things you 
can see what I show here is the 
loss curve.  The loss curve is 
the most important thing to look
 at if you are looking at, if 
you are trying to train a model.
  What you see here is that a 
loss that is kind of the error 
that the model makes when 
looking at data is decreasing 
over time.  And that means it's 
learning something.., plain and 
simple. 
   So, that's good.  The model 
is learning something, great.  
The next thing I could do with 
tenser Board is look at the 
model created and look at the 
lower levels of the model, look 
at the graph.  TensorFlow works 
by generating the graph and the 
graph is shipped to all of the 
distributed workers that it has 
and its execute  executed under.
  You don't have to worry about 
it too much but it's useful to 
be able to inspect the graph 
when you are doing debugging. 
   So here is the graph that was
 generated by the model 
declaration that I showed 
earlier, and I have highlighted 
in red here the part that's the 
actual model, the actual linear 
model.  I can look inside of it 
and you can see that there are 
these slightly yellow boxes and 
those are the input
 processing computations that 
happen in this model.  So 
creating all of this is taking 
care of you by the 
infrastructure, but it's really 
useful to be able to look at it 
if you are debugging something 
or if you want to know what 
happens. 
   So what's going on.  The 
LinearRegressor I defined is 
what we call an estimator.  We 
inherited that 
estimator from cycle learn.  It 
supports the basic operations 
you need for an ML model, you 
can train it, you can evaluate 
it, usually on separate data.  
If you steak away anything from 
this talk, it's that you should 
always evaluate your models on 
separate data while you are 
training or during your
 training. 
   You can query the model for 
predictions worn's you have 
trained -- once you have trained
 it and this is fancy you can 
export what we call a saved 
model and give it to TensorFlow 
serving.  There is a talk about 
TensorFlow serving tomorrow in 
the amphitheater at 1:30 and you
 should go to t these methods 
hide a lot of tricky detailsna 
you don't have to worry about.
   They are just done and you 
just call them and you can be 
reasonably certain that it 
actually works.  We also defined
 an input function earlier and 
that basically reads from data 
files and feeds data in an 
appropriate format into the 
estimator itself.  There is a 
trick here, the estimator 
actually saves its state to 
what's called a checkpoint.  The
 checkpoint contains all of the 
variables that the model 
contains, and every time we call
 one of these functions, it will
 synchronize with the 
checkpoint.  This is very 
important for the distributed 
setting where you have several 
machines and they all do their 
own thing, but they have to 
synchronize and they synchronize
 when something restarts or when
 something breaks they 
synchronize via checkpoint. 
   The checkpoint is also what 
we use to export the safe model.
  Okay.  So this is great.  So I
 have shown you how to train the
 linear model with pandas input.
  What if you want something 
else, just anything else really?
  Well, so all of these 
components are obviously 
swappable.  So, for instance, 
the pandas import function you 
can -- input function you can 
swap to other inputs.  You can 
read from numpy, anything you 
can spet out of a generator, you
 can use -- spit out of a 
generator, you can use.  And you
 can write your own input 
function too.  It's not that 
hard.  Also the models that we 
have are not limited to just 
linear models.  So we have 
linear, regressor, classifier we
 have deep neural network which 
the straight forward neural 
network, basic feed forward 
neural network and we have 
estimators that will do random 
forest, support vector machines,
 you name it.  So all of this is
 available directly for use. 
   So you don't have to 
implement it yourself.  Let's 
say we want to swap this out.  
So first of all, we have to 
obviously exchange the name of 
the class that we are using -- 
change the name of the class 
that we are using, and then we 
will also have to adapt the 
input to something this new 
model can use.  In this case a 
DNN model can't use the, these 
categorical features directly.  
We have to do something to it.  
The two things that you can do 
to it to make it work with a 
deep neural network is you 
either embed it or you transform
 it to what's called a one hot 
or an indicator. 
   -- one hop or indicator. 
   We say make me an em combing 
and out. 
   Sill enders make it an 
indicator column because there 
is not that many volumes there. 
 Usually this is fairly 
complicated stuff and you have 
to write a lot of code, but this
 is it.  All you have to do is 
declare I want  an embedding for
 these and you are done.  The 
DNN basically we tell it make me
 a three layer neural network 
with layer sizes 50, 30, 10.  
That's all you need to -- it's a
 high level
 interface.  For other models 
that can be more complicated or 
less depending.  And if none of 
that has worked for you, there 
is still also, you can still 
take care, take advantage of the
 training loops and all of the 
infrastructure that is in the 
estimator itself, and you can 
completely swap out the model 
and write your own.  The way you
 do this is we have this 
estimator base class and you can
 put in what's called a model 
function. 
   And this allows you maximum 
flexibility.  The model function
 takes the inputs and produces 
Tensors that contain the thing 
you have to do for training, the
 thing that you have to do for 
evaluation, it returns 
predictions and then it will 
also produce a safe model if you
 need one.  You can write all of
 this yourself if you want to if
 none of the pre-existing models
 fit for your use case.  And you
 can use any kind o f -- you can
 use regular TensorFlow or you 
can use Keras and Francois will 
say a little bit about that. 
   So the exciting thing about 
TensorFlow is actually the fact 
that it runs not only in a 
single machine but distributed 
in a data center.  So to make 
this work, we have a utility
 that uses several workers and 
basically trains the model on 
all of these workers at once.  
We call the distributed, a 
distributed training run, like a
 single run of training a model 
an experiment. 
   And to make one of those, we 
first make an estimator as you 
have seen before, and then we 
add the input function and 
estimator together and make an 
experiment.  The name experiment
 comes from hyperparameter 
tuning and I'm not sure how many
 are familiar with that.  It's 
an important concept in Machine 
Learning.  Instead of training a
 single model for your data, you
 train a whole class of models 
and you take, the pick the best 
one.  It's very powerful and I'm
 not going to go into detail on 
this but the infrastructure is 
all set up, so you can in the 
experiment function that makes 
the experiment, you can pass in 
hyperparameters and you can use 
those hyperparameters to create 
the estimator and you return the
 estimator with the experiment 
and you can implement hitcher  
hyperparameter tuning. 
   We pass to what we call the 
learn runner which runs it.  And
 we can pass the user options in
 the run config and what the run
 config also does, it contains 
information about the cluster 
that we run it on.  And so we 
can use that information, for 
instance, if we make the 
estimator, we want to tweak it 
depending how many machines we 
have available or something like
 that, we can do that.  So this 
contains information how many 
workers do we have, how many 
parameter servers do we have, so
 on. 
   But the run  config also 
takes information from the 
environment and so we don't have
 to pass it explicitly.  So what
 do we have to do in order to 
declare, to TensorFlow, hey, 
here is a cluster, please use 
the whole cluster is we write 
what's called the cluster spec. 
 And it is a very, very simple 
thing.  It's just simply a map 
with names of different machine 
types that we want to do 
different things and then a list
 of all of the machines that can
 do this thing.  And usually you
 have parameter servers or PS, 
and you have workers and the 
parameter servers store the 
variables, the workers do the 
actual work.  That's kind of 
traditional, and you just fill 
out the lists and you are done. 
   You dump that in the second 
thing here, you dump that in an 
environment variable and 
everything else will work from 
there.  Cie.  Setting up a 
cluster depends on the 
environment and I'm not going to
 talk in detail about it, but if
 you are interested, it's on 
GitHub in the ecosystem repo.  
There is a number of scripts, a 
number of examples that help you
 get started. 
   So one thing I would like to 
mention and I don't know how 
many of you have seen the TPU 
talks.  This is a pot of TPUs.  
If you stick to these concepts, 
your code will basically work on
 a TPU without modifications.  
So you will be able to use this 
once you can use it.  So what I 
have shown you is we have -- I 
have shown you, we have 
TensorFlow implementations of 
complete Machine Learning 
models.  You can get started 
extremely quickly.  They come 
with all of the integrations 
with tenser board, 
visualization, serving and 
production for different 
hardware, different use cases. 
   They obviously work in 
distributed settings.  We use 
them in data centers, you can 
use them on your home computer 
network if that's what you like.
  You can use them in flocks of 
mobile devices.  Everything is 
possible.  And they run all 
kinds of different hardware.  In
 particular they will run on 
TPU, which is nice.  They also 
always run on GPU on C PU 
obviously. 
   So before we move on, the 
full code, again, is at this 
URL.  If you are interested in 
more, do check out the tutorials
 on TensorFlow.org and 
particularly the estimator 
tutorial.  For people who want 
to write their own estimators, 
and then do go to the talk 
tomorrow at 1:30 in the 
amphitheater for TensorFlow 
serving with Noah.  And with 
that, thank you very much, and I
 will hand over to Francois 
Chollet and he will tell you 
more.
   (Applause). 
   &amp;gt;&amp;gt; FRANCOIS CHOLLET:  Thank 
you,
 Martin.  So measures are great 
for many use cases but what if 
you need something that is not 
available as a candidacy major. 
 What if you need to write your 
own model.  That's where the 
Keras CPI comes in.  It's this 
API you can use together with 
the estimator and experiment 
class that Martin  introduced.  
As you k now, TensorFlow 
features these fairly low level 
programming interface where you 
spends most of your time 
multiplying matrixes and 
vectors, and that's very 
powerful, but it's not ideal for
 building very advanced complex 
models. 
   So we really believe that in 
the future deep learning will be
 part of the tool box of every 
developer, not just Machine 
Learning experts because 
everyone needs applications, and
 to make this future possible, 
we need to make deep learning 
really easy to use.  We need to 
make available tools that are 
accessible to anyone because you
 shouldn't need to be an expert 
in order to start leveraging 
deep learning to solve big 
problems. 
   So if you could design a deep
 learning interface without any 
c onstraints, what would an 
ideal interface look like?  I 
already think that the core 
building blocks of deep learning
 are all fairly well understood,
 and rather than letting the 
user reimplement everything 
themselves, instead it should be
 really easy to just take 
existing building blocks and be 
able to quickly assemble them to
 be new deep learning data 
processing pipelines. 
   It should be basically like 
playing with blocks.  And I 
think actually if you think 
about Lego bricks it's the ideal
 metaphor.  Lego bricks are 
intuitive to use.  They are easy
 to use.  They provide very  
flexible expressive framework in
 which you can build almost 
anything.  They allow you to try
 different things very quickly 
and immediately g et, you know, 
visual tactile feedback about 
what works and what doesn't 
work, and, of course, Lego 
bricks are very accessible.  
They are accessible to any human
 being ages five and above. 
   And the 3D idea, the ideal I 
would say that we had in mind 
when we designed the Keras API, 
we wanted to be the Lego for 
deep learning.  So let's take a 
look at what Keras can do. 
   So first of all, I think it's
 better to think about Keras not
 as a base API framework or a 
library.  It's really just a 
high level API specification.  
And it's a spec that has several
 different implementations.  The
 main once is the tenser frame 
implementation.  This is an 
implementation for AmEx net, 
there is one for Java and there 
are more coming.  What makes 
Keras different from any other 
deep learning interface that's 
available is it's deeps focus on
 user experience.  all about 
making your life
  easier, simplifying your work 
flow especially in terms of 
providing easy to use building 
blocks.  Intuitive, in terms of 
providing good feedback when 
things go wrong and reducing 
complexity, reducing cognitive 
load.  And, of course, if you 
make deep learning easier to 
use, then you are also making it
 accessible to more people.  The
 end goal with Keras is to make 
deep learning accessible to as 
many people as possible. 
   Until now the TensorFlow 
implementation for the Keras API
 was available as part of an 
external open source repository,
 but now what's happening we are
 bringing the Keras API directly
 into TensorFlow project and we 
are doing that to make Keras 
work seamlessly with the 
existing TensorFlow work flow.  
Concretely what's happening is 
Keras becomes available as a new
 module inside TensorFlow.  The 
TF.Keras module.  It contains 
the entire Keras API.  So if you
 are a TensorFlow user, what 
that means for you is now you 
get access to this new set of 
easy to use deep learning 
building blocks that will work 
seamlessly with your work flow 
and if you are I Keras user you 
gain access to high level 
TensorFlow training features, 
things like distributed 
training, things like 
distributed high parameter 
optimization, training on Cloud 
ML so that's powerful. 
   So to give you a concrete 
sense of what your work flow 
will be like when using the 
crags API to define your models 
and using TensorFlow estimators 
and TensorFlow experiments to 
train your models in a 
distributed setting, I will walk
 you through a simple yet fairly
 evident example.  We look at 
the video questioning problem.  
So that's what the problem looks
 like.  We have this data set on
 the videos.  Each video is ten 
seconds long.  It shows some 
people doing some activities, 
and deep learning model will be 
looking at the frames of these 
videos, and it will try to make 
sense of it, and then you can 
ask the model, short natural 
language questions about the 
contents of the video. 
   So in this example, we have a
 short video of a man packing 
some boxes into a car, and you 
can ask what is the man doing, 
and the model will be looking at
 the video, looking at the 
question and will have to select
 one wanser word out of a set of
 possible answers.  So in this 
example you can ask what is the 
man doing?  He is packing.  And 
it's actually an interesting 
question because if you  were to
 just look at a single frame 
from this individualsio, you 
could not answer the q uestion. 
 The man could be unpacking as 
well.  So the reason you know he
 is packing is because in all 
other frames.  So we expect our 
model to be able to leverage not
 just the visual contents of the
 frames, but the order of the 
frames as well. 
   So needless to say, this is a
 tremendously difficult problem.
  Just three or four years ago, 
before Keras, before TensorFlow,
 this would have been only 
doable for a handful of research
 labs.  This would have been 
pretty much six month project 
for a team of expert engineers 
and what we are doing now is we 
are making this really advanced 
problem accessible to pretty 
much anyone with basic scripting
 activities so we are 
democratizing deep learning. 
   So that's what our solution 
looks like.  That's our network.
  It is structured in three 
parts.  So first you have one 
branch that tags the video input
 and turns it into a vector that
 encodes information about the 
video.  And you have one branch 
that tags the question and turns
 it into a vector.  So now you 
can concatenate the question 
vector and the video et cetera 
vector and you can add classify 
on top.  The classifier would be
 to select the correct answer 
out of a pool of answers.  So 
the first step is to turn the 
video input tenser into a 
vector.  a video is just a 
sequence of frames. 
   And a frame is an image.  So 
what you do with an image is you
 run it 
through a CNN.  And each CNN 
will extract one vector 
representation for each frame.  
So what you get out of that is a
 sequence of  vectors including 
the   frames.  And when you have
 a sequence what you do with i 
t, the natural thing to do is 
run it through the sequence 
processing model, and the CSLM 
willry Tuesday the sequence to a
 single vector and this vector 
is encoding information about 
all of the frames and their 
order.  So the entire visual 
contents are the video. 
   The next thing to do is a 
similar process applied to the 
question.  The question is a 
sequence of words and you will 
use a module to map each word 
into a vector, a word vector, so
 you get a sequence of word v 
ectors and reduce it using a 
different LSTM layer.  Once you 
have your vector for your video 
and your vector in position for 
your question, you connect them 
and you add this classify on top
 whose job is to select the 
right answer. 
   So that's really the magic of
 deep learning.  You tag the 
very complex inputs which could 
be videos, images, language, 
sound, and you turn them into 
vectors.  So you turn them into 
 points in some geometric space.
  You turn meaning into points 
in a gee mote trick space.  
That's the essence of deep 
learning.  What's very powerful 
about it is once you have done 
that, you can use linear algebra
 to make sense of these 
geometric spaces and you can 
learn interesting mappings 
between different geometric 
spaces so in our case we are 
learning a mapping between the 
initial space of videos and 
questions and a space of answer 
words and we are doing that just
 through exposure to trained 
data. 
   And the way we are doing this
 is really by assemblely 
together this specialized blocks
 for information processing.  
And it's a very natural thing to
 do.  If you have an image, you 
process it using an image 
processing model which is CNN.  
If you have a sequence, you 
process it using a sequence 
processing model, and if you 
want to select one element out 
of a pool of possible 
candidates, then you use a c 
lassifier.  It's the natural 
thing to do.  So what you are 
doing with deep learning is 
plugging together these 
information processing  bricks 
that are limb sar to Lego 
bricks. 
   So building deep learning 
models is conceptually similar 
to playing with Legos, and if 
the ideas beyond deep learning 
are so simple, the 
implementation should be simple 
as well.  So let's take a look 
at the implementation.  This 
figure is a very forward 
translation into a Keras 
implementation.  On the video 
encoding side, we have this
 inception V3 cog net and we use
 a distributed layer to apply 
this cog net to each frame 
alongside the time axis of an 
input video tenser and then we 
pipe the output through this 
LSTM layer which will reduce it 
to a single vector. 
   So one interesting thing to 
note here is that inception will
 come loaded with pre-trained   
weights and the reason that's 
important is because with the 
current video data set we don't 
have enough date it a to learn 
interesting visual features on 
our own so we need to leverage 
pre-existing features that are 
learned on a larger data set, so
 image net in this case.  That's
 a very common pattern in deep 
learning and it's a pattern that
 is made easy about Keras and we
 will see how in a second.
   On the question encoding 
side, it's even simpler.  You 
just run the sequence of words 
into an embedding layer to 
process sequence of v ectors and
 then you reduce the sequence of
 vectors to a single vector 
using an LSTM layer.  So once 
you have the video vector and 
question vector you concatenate 
them and you add classify on top
 which is would dense layers 
which will select the correct 
answer. 
   Let's look at the code.  So 
this is the entirety of the code
 for the video encoding part.  
It's just a few lines.  It's 
very readable, very simple.  You
 start by specifying your 
inputs.  So this is the video 
input.  It's sequence of viable 
number of frames so the non-here
 is number of frames it is 
undefined so it could change 
from batch to bank.  Each frame 
is a 150 by 150 image.  The next
 step in one line we are  
identifying an inception V3 
model which is a complex model 
defined in just one line, and it
 comes loaded with pre-trained 
weights from the image net data 
set. 
   All of this is built in.  
It's already into Keras, so you 
don't have to do anything more. 
 It's literally just one line.  
And we are not including the top
 layers because they are not 
relevant to us, and we are 
adding some pooling on top which
 allows us to extract one vector
 from each 
frame.  In the next set we are 
setting this to be n 
on-trainable which means its 
representations will not be 
updated driewrg training.  This 
comes with good interesting 
representations and you don't 
want to add to them. 
   Again, so that's a very 
common pattern in deep learning 
to take pre-trained model and 
freeze it and make it part of a 
new pipeline and it's a pattern 
made easy in Keras.  So once we 
have these frozen pre-trained 
com net we use a distributed 
layer to distribute the com net 
across the time access of the 
video input, and the result of 
that will be the tenser frames 
which we ran through an LSTM 
layer to get a single vector for
 the video. 
   On the question side, things 
are even simpler.  We define a 
question input as a sequence of 
integers, viable number of 
integers.  Why integers?  
Because every integer will be 
mapped to a vector in some 
vocabulary, and we run this 
sequence of integer into a layer
 which will happen every integer
 to one layer and it is just 
part of the model.  And then you
 run this sequence of vectors 
through an LSTM to reduce it to 
single vector. 
   One interesting thing to note
 here with the two LSTM layers 
that you instantiated so far is 
that we were not configuring the
 layers beyond just specifying 
the numbers of outputs into it. 
 That's interesting because 
usually when you are using LSTM 
layers there are lots of things 
you have to keep in mind, lots 
of best practices you should be 
following to make things work.  
For instance, you should 
remember that it should be 
initialized.  You need to 
remember that the focused bias 
should be initialized to one and
 many more.  And here we are not
 doing this because all of these
 best practices are lrdz part of
 the default con significant 
race of the Keras layers.  It's 
a very important principle in 
Keras that best practices come 
included as default and what 
this means for you is that your 
models will typically just work 
out of the box without you 
having to train every parameter 
to make it work.  So that's our 
goal with Keras to reduce 
cognitive load, to reduce 
complexity.  We don't want you 
to care about the technical 
details.  We just take care of 
them for you. 
   So once you have done 
encoding your video and then 
encoding your question, you use 
a con  CAT to put them in a 
single layer and you add the two
 lawyers on top which will 
select one answer word out of a 
vocabulary of a given size. 
   In the next step, you are 
using your inputs and outputs to
 instantiate a Keras mod the  
model which is a container for 
graph of layers then you are 
specifying the training 
configuration.  So you are 
specifying the optimizer you 
want to use, and you are 
specifying the loss function for
 which you are optimizing.  So 
that's very simple so far.  At 
this s tage, we have defined our
 model.  We have defined our 
training configuration and now 
we want to train this in a 
distributed setting, maybe on 
Cloud ML. 
   So let's see how that works. 
 This is where the magic 
happens.  You can use the 
TensorFlow estimator and 
experiment classes that Martin  
introduced to train this Keras 
model in a distributed setting 
in just a few lines of code.  
All you have to do is to ride 
this experiment function in 
which you define your model, you
 use your model to instantiate 
an estimator using this built in
 get estimator method.  And you 
use it to create an experiment. 
   That that's where you specify
 input data.  It's just a few 
lines of code.  It's like magic 
in just a few lines a very 
readable, straight forward path 
of code will define a state of 
the art model and we are 
training it in a distributed 
setting.  So to solve this 
really challenging problem of 
video question  a nswering, 
which should have been 
completely out of reach to 
anyone just a few years ago.  So
 these APIs, these new high 
level APIs are redemocratizing 
deep learning.  That's made 
possible by two things.  On the 
one hand you have the Keras API 
which is this high level easy to
 use and powerful way to define 
deep learning models in 
TensorFlow. 
   And besides just being  being
 easy to use, each layer comes 
with default configurations 
which allows your  model to run 
out of the box without much 
tuning. 
   The other piece of magic is 
this new high level TensorFlow 
training A PIs that Martin 
introduced, estimator and 
experiment.  And together this 
allows you to solve any dep 
learning problem with very 
little effort.  So we think 
these new APIs are a big step 
towards 2K340BG advertising deep
 learning and making TensorFlow 
and deep learning available to 
everyone.  We hope you will find
 them useful and we are very 
much looking forward to seeing 
all of the cool applications 
that we will be building with 
TensorFlow and Keras.  Thank you
 very much for listening.
   (Applause).
   May 18, 2017.
   1:00 p.m. PT
.
   Stage 6.
   Architecture Components-
Persistent and Off-line.
                        persist
ence and Off-line.
   Services Provided By:
        Caption First, Inc.
        P.O. Box 3066
        Monument, CO 80132
        800-825-5234
        www.captionfirst.com.
   ***                          
   This text is being provided 
in a rough draft format. 
   Communication Access Realtime
 Translation (CART) is provided 
in
   order to facilitate 
communication accessibility and 
may not be a
   totally verbatim record of 
the proceedings.
   ***
.
   Raw File.
   Google I/O 2017.
   May 18, 201 #       2017.
   South Carolina     
         1:30 p.m. PT.
   Stage 6
.
   Architecture Components-
Persistence and Off-line.
   Session T
8D828.
   SPEAKER:  Welcome.  Thank you
 for joining.  Our session will 
begin soon.
. 
   At this time please find your
 seat.  Our session will begin 
soon.
     (applause)
.
   SPEAKER:  Welcome, good 
afternoon, everybody.  I'm Yigit
, technical lead for 
architecture components, and I'm
 here joining with Kirill, part 
of the team.
   Today we are going to talk 
about persistence.  This is is 
loading screen, this is one of 
my favorite screens.  Said no 
one    one, no one ever said I 
like to wait.  Especially if 
there is some content that they 
have already seen, and if you 
are making the user wait to see 
the same content, it's horrible.
  Like you are a bad person.  
(chuckles).
   I don't mean that.  So now 
how do we fix this, we persist 
the data.  This is what we 
recommend, whatever information 
you fetch from your network 
different data services, save it
 to disk, so that if your 
replication is restarted when 
there is no network conditions, 
you can show something to the 
user.  You can make that 
experience seamless.
   We say persist under it, we 
know this is a crowded field, 
there is different options, 
different companies, and most of
 these solutions are really good
 solutions.  But especially if 
you come to Android, if you are 
new to the platform what you 
will do, you will check what is 
inside the framework already.
   There is three things that 
come with the standard library, 
and if you read about them you 
realize that if you want to put 
structure ed data, then you want
 to go with SQ lite.  It is 
something we have been shipping 
since Android one.  It is proven
 technology, works very well.  
You say okay, I want to use SQ 
lite.  You go to this page.  
This is the first page you see. 
 This is horrible!  This is like
 it's kind of trying to say you 
know what, you don't want to 
persist.  (chuckles).
   That is not what we meant.  
But this is what we have.  We 
said, you know what?  Let's look
 at this page, like we want to 
make this better, right?  Let's 
get this page, it's trying to 
say I want to select this 
columns, with this constraints, 
with this order.  If you look at
 this, this is a very simple SQL
 query.  But it takes a lot of 
stuff to write it, you need to 
define all the constants which 
are not even visible on this 
page     page.  So you say okay,
 there should be room for 
improvement.  I came up with 
room, which is an object mapping
 library for SQLite       
SQLite.  We said let's step back
.  We said writing the same 
thing with SQL is shorter, nicer
, so let's go back to our roots,
 which now there is a SQL query,
 we assign I a string           
        assign it a string.  It 
is standard SQ lite.  If it is 
like this we cannot understand 
it.  We love annotation.  We put
 it inside a annotation.  It is 
inside a annotation, now we want
 to get the response from the 
query.  We want to say, put it 
in to method so we can 
understand what the query wants 
to return.
   We know that this is a query.
  It wants to fetch these 
columns, from this table, and 
with this constraint.  But if 
you look at the constraint there
 is a biparameter this is SQ 
lite standard byte parameter 
syntax.  We didn't come up with 
this.  Where do we get this 
program tar?  How do you 
parameter?  From the function 
argument.  We put it this      
there, the most obvious place to
 get the argument from.  We want
 no know what           to      
           We want to know what 
it wants to run.  It is 
returning a list of feeds.  
There is a class feed, we need 
that somewhere.  We have that 
class.  I want to put query 
inside the data access object.  
You don't want to have your 
replication making database 
queries on code base, you want 
to put them into certain classes
 which we call data access 
objects.  We need to tell the 
room that this is a Dao, we need
 to tell the room that feed is a
 class it would like to persist 
in the database.  Last but not 
least, we need database to put 
these two things together.  The 
room database class, the feed Da
 ao we define, we say the 
database that is this Dao and 
the feed entity we put into the 
database.  You can have multiple
 entities, put     multiple Da 
organizations,               oo,
   as long as whatever schema 
you define works with the 
database and the entities.  Once
 you have the description, you 
can get implementation of the 
database through this builder.  
It is similar to how we use 
retrofit dagger, you provide, 
define the interfaces, we 
provide implement          
implementation.
   Of course, because we are 
doing the implementation, select
 is specific, we don't know how 
you will select but there are 
other queries.  You want to 
insert something into the 
database.  We can define 
annotation to make it easy for 
you.  Inside these annotations, 
these are very flexible, you can
 say we compress multiple 
arguments.  If you read this 
method, is that what you want 
both in the database and room 
understand this, you can send a 
list of items, you can send 
variable arguments, multiple 
parameters.  If it is readable, 
Room will make it work.
   You can even say if when you 
try, if there is a unique key 
conflict in the database, do 
this.  You can specify these 
constraints.
   We have the insert annotation
, we have similarly deletes or 
update annotations, just for the
 common tasks.  Now there is, we
 start on SQ lite but there are 
cases where writing SQ lite is 
harder.  I have this query where
 I want to query these feeds, 
but I try to return a list of 
feed which means I want to get 
multiple feed items.  Right?  
That means I want to get them by
 I.D. so I want to pass multiple
 I.D.s probably.
   In SQL you will do this in 
these I.D.s but you need to know
 how many I.D.s you have so you 
can put that number of byte 
parameters but this is some 
information we don't know while 
writing this query.  That is 
okay.  Room can understand it.  
If whatever the parameter you 
are passing to a function is a 
collection, room knows that, 
they want to have multiple byte 
parameters, at run time we will 
generate the right query for you
 and we will do that.  Send them
 as a list, doesn't matter.  If 
room can understand it, we will 
do it for you.
   The most important part about
 Room is that once we let you 
define all these things, Room 
understands these queries.  Is 
it not just take this SQL and 
compile time generate the code. 
 Room understands it, knows what
 you are trying to do.  This 
gives us a lot of power.  If 
Room looks at the query, says 
this is a select query, with 
this bit     booith        
boo     byte parameter passing 
as a string it knows it's from 
the feed table and wants to 
return items as feeds, room says
 can I validate this, yeah.  The
 table has the three columns, 
the feed item has these three 
columns.  These match.  Fine.  
You can have another class 
instead of the feed that has 
these three fields, room will 
still work.
   Now after this verification, 
you made a type, right, the 
reason we define constants or 
use java builders is to avoid 
this typing like mistyping 
queries.  Room can do the 
validation for you similar to 
Java code.  You don't use build
ers to write java code, you 
write java code compile er tells
 you if it is wrong.  I.D. helps
 with this.  The reason we don't
 have builders, we think helping
 you write SQL query is the job 
of Android studio which they are
 working on it.  Once you write 
the query room verifies it's 
correct.  If you do this mistake
, Room will not let you compile 
the application.  Similarly if 
you access some columns that 
doesn't exist in the database, 
it will be a compiled time error
.
   It can be any query.  You 
might be joining five tables, 
room will still understand.  You
 may have grouping, like almost 
anything in SQL.  Say we made a 
mistake like that, where we only
 fetch the subtitle column but 
we want to return this as a feed
.  In this case, the feed object
 also has a subtitle column.  
This might be intentional, maybe
 you are going to fill in those 
fields later on.  Or you made a 
mistake.  We don't know.  In 
this case, Room generates a 
warning.  It says, feed class 
has two other fields that you 
are not returning from the query
.  It will just give you all the
 details, what the query returns
, maybe you made a typo, what 
are the fields in the column, in
 the entity.  There are two ways
 you can get rid of this warning
.
   The first thing you can do is
, maybe you want to return them 
as feed instances, then you can 
tell room to ignore it.  I know 
what I'm doing.  Suppress this 
warning.  Alternatively, you can
 create any classes, you can say
 return these as a string, you 
are returning one column, as a 
string.  Room will do it for you
.
   Very similarly, what if you 
are returning I.D. and title.  
In that case again Room will say
 there is two columns, one 
string that doesn't match, it 
will give you an error.  One  
     When that happens we can 
create any class in your 
replication, PoJo as long as we 
can read what is inside it and 
it matches what the query 
returns it will general raitd
               generate the 
code.  It can rename the columns
 in SQ lite.  All those things 
works here.  Only thing is 
sometimes you have classes we 
don't know about.
   I want Kirill to talk about 
them, how we extend Room.
   KIRILL GROUCHNIKOV:  Let's 
talk about something more 
interesting.  Going back to the 
feed object that has one I.D. 
integer field and two strings, 
these are primitive types that 
are directly supported by the 
underlying SQL database.  What 
happens if we want to add a 
field of a type that is not 
directly supported by S scw    Q
 lite such as java dat     date.
  Room is going to tell us I 
can't figure out how to save 
this.  You need to help me out
    out.  The way you help out 
in this case is provide two 
methods to take your original 
data, convert it to something 
that SQ lite can store, so that 
is going into the database, and 
on the way out, when you are 
doing the queries, convert back 
from that representation in the 
database to your original data 
type.  You implement these two 
methods in this particular case,
 we are converting day too long 
and back from long to date, we 
annotate them, we have 
annotations, we use the type 
conversion annotations in these 
two methods and take my convert
ers class and use the type 
converters annotation pointing 
to these two methods, pointing 
to the class that has our 
conversion methods.
   You can put it directly on 
the field in your entity object.
  You can put it on the entity 
object itself.  You can put it 
on to your data access object 
class or this specific query in 
it or you can put it on your 
database class.  At compile time
 and run time, room will do the 
right thing to find those two 
methods.  With this, we can 
write a query that finds the 
list of all the feeds that were 
posted between two specific 
dates, from and to.  Here, since
 we are accessing the from and 
to fields, or columns in the 
database that are defined as 
long, when the database table is
 create ed, you don't really 
want to use long as the 
implementation detail, since you
 are using the date class for 
your entity data field, you want
 to use the date objects in the 
query itself.
   This is what you do here, and
 when the query runs, these two 
values from and to are going to 
be converted to longs, the query
 will run and on the way back, 
when room constructs and fills 
these feed objects it will 
convert back from long to dates.
   Now let's talk about even 
more complicated stuff.  What 
happens if you have a class 
hierarchy in your kind of data 
model universe, that has object 
graph that is not flat.  In this
 case, we are adding a location 
object that has two double 
fields, latitude, longitude, 
into our feed.
   Now what happens at compile 
time?  Once again at compile 
time you need to tell Room how 
to store this field, since 
especially here if not even it's
 not a primitive type but it's a
 type that has sub fields in it.
  One option is going back to 
our type adapters.  You can do 
something like take latitude and
 longitude and convert them into
 a single concatenate ed string 
with some kind of sem owe    
     semicolon as a separator on
 the way in and on the way back 
it is going to be some kind of 
split where the spring        
string goes back to two doubles 
or you can do beat masking and 
beat shifting to try and convert
 two dawbles,          doubles 
and code them in one long field.
   This is going to be hard to 
query, even for this simple case
, when we only have two doubles,
 that we are trying to encode, 
we are trying to compact it into
 one field.  The more 
complicated the data structure 
is, the harder it becomes to use
 type adapt      adapters for 
these cases.
   Another option is to say that
 I'm going to flatten having 
       everything           
everything, you are going to 
flatten everything for us.  
Instead of having the location 
data field in your data entity, 
you essentially flatten all the 
fields from the location class 
into the main feed class.  Then 
room is going to create this two
 latitude longitude columns 
which works, but first of all 
you lose encapsulation.  Now you
 just look at the definition of 
your data class, of this feed 
object or somebody else in your 
project looks at that, and it's 
not clear that these two objects
 represent one single entity.
   Another thing is just because
 you want to persist this object
       object, and later on 
retrieve it, why should you be 
changing the definition of your 
data classes?  That is not good.
  Instead, there is another 
option, which is also not ideal,
 you can say we have a two data 
classes, why don't we store 
these objects, each one in its 
own separate database table.
   We have our feed object with 
I.D. titles, subtitle and posted
 app the date and we have the 
location, latitude, longitude.  
Now in order to connect them, 
not when you say but when you 
retrieve them, in order to 
connect the location back to its
 feed, you need to have the 
primary key, the feed I.D., so 
that they are combined together.
   Once again it works.  First 
of all, yet another table is not
 very clean.  But also going 
back to what I mentioned before,
 why should you be forced to 
change and tweak the definition 
of your data model classes just 
because you want to persist them
.  What would be ideal is that 
you have your clean separation 
of how you define your data 
classes, but at run time, the 
way the feed object is persisted
 is in one flattened table.  
Essentially the latitude and 
longitude fields from the 
location of flat         are 
flattened into the same table.
   The way you do it is with 
this embedded annotation at 
compile time and run time, Room 
is going to figure out how to 
flatten your entire hierarchy 
into this one table.
   Now you can write a query 
like this in your data access 
object, we are going to select 
all the feeds, in the specific 
geographical rectangular area.  
As you can see we are reference 
         referencing latitude 
and longitude in the query 
directly, they are not in a 
separate table.  You are just 
using the same names for the 
database columns as the 
attributes in your location 
object.  You can write the query
 like this to select the 
location for the specific feed 
based on the feed I.D., and room
 is going to figure out that 
while it needs to fetch the 
entire row from the database, it
 only needs to create and fill 
the location object, because 
this is what this method wants 
to return.
   Or if you want to be more 
specific, you can say I only 
want you to fetch the latitude 
and longitude from that table.  
The last part about embedded 
objects is what happens when you
 have more than one field of the
 same kind of nested class.  In 
this case we want to store two 
location, what is going to 
happen at compile time, Room is 
going to say as I was flattening
 these fields into one database 
table definition, I see that the
 same latitude field is defined 
by two objects and since SQ lite
 doesn't support having two 
columns with the same name in 
the same database table, it's 
going to fail as an error, not 
as a warning.
   What you need to do is, in 
case you do want to have 
something like this, to have in 
this particular case two 
locations, you want to use the 
prefix attribute on the embedded
 annotation, at compile time and
 at run time it is going to be 
used as room is flattening the 
data graph definition, it is 
going to use the prefix to 
create latitude longitude for 
the original, for the first 
location, and see latitude, see 
longitude for the second 
location object.
   Now let's talk about observe 
ability for those who     of you
 who listened to yesterday's 
introduction session and who 
have been here earlier in the 
day, to listen to the lifecycle,
 live data view model.  We want 
to talk about observeability.  
This is a simple query that 
returns a list of feeds based on
 particular queries         
query.  This is kind of a 
snapshot, a point in time where 
you have run this query, and 
however many feeds it returns, 
it represents the state of your 
data universe at that particular
 moment.
   What happens if your data is 
dynamic, it can be manipulated 
directly by your users in the 
app or maybe it's pushed or 
pulled from the server.  In this
 case, if one of the feed 
changes in the database, when 
you add new feeds, when you 
delete existing feeds, that 
match this query, every single 
time you reflect these changes 
back on to the screen you would 
need to refresh this query 
explicitly, to run this query 
again and again.
   Instead,  of rurpg       
returning list of feeds, you can
 wrap this return object, it can
 be a single feed or list of 
feeds in a live data object.  
This instructs room not only to 
fetch it once, but also update 
the live data, the callback that
 you pass to the live data when 
you call the dot observe, we 
will see in a few slides, every 
single time any one of the 
objects that were returned by 
the query was changed, or new 
objects were added or existing 
objects were removed from the 
result of the query.  This is my
 favorite part by far of 
architecture components.
   Also, we provide support for 
using flow from our ex java --
     (applause).
   Let's see an example of how 
Room integrates with the rest of
 lifecycle aware parts of 
architecture components, we are 
going to start with a simple 
database interface, it has one 
method to load the data for a 
specific user.  and one method 
to save the data, the first time
 it's going to be a simple 
insert and then as the data 
changes, it's going to be a 
update or replace.
   As you can see here instead 
of rejoining the user object 
directly in our load method we 
are returning a live data that 
traps the information about this
 user.  Now we are going to set 
up the data binding that show 
the information on the user, 
perhaps in the details page or 
somewhere in a profile page.  We
 have our lifecycle activity.  
It can be lifecycle fragment, 
life      lifecycle service or 
custom lifecycle owner, in this 
case we are using lifecycle a 
can       activity.  On create 
we get access to our database 
and database access object.  We 
are going to call our load 
method that returns the live 
data that traps the user data.  
Now we are going to use the 
observe method on this live data
 object passing two program 
        parameters.  The first 
parameter is reference to the 
lifecycle owner.  This is our 
activity.  The second parameter 
is the callback that is going to
 be invoked by room at run time 
every time this user information
 changes in the database.  On 
the first load, or on the 
subsequent updates.
   Word of caution, because it's
 like you know we    we don't 
want to put too many lines of 
code here on one slide, you 
usually don't want to expose 
database details or details 
about loading this information 
from your web service or cache
 ing it locally in some form 
directly to the activity or 
directly to the fragment.  This 
code is highly recommended to be
 put in a Dao model.  Once again
 the most powerful part here is 
this connection between the 
lifecycle activity, the owner 
that says I'm active or I'm not 
active, and the live data, the 
room returns as a result of this
 particular query.  This call 
back that you provide as a 
second parameter to the method 
is going to be invoked every 
time Room detects that the 
result of that query that you 
wrote select from the user 
database table has changed.  
Once again, the flow of 
information is one part of your 
app it can be some service that 
pulls information from your back
 end, or maybe the information 
is pushed through some kind of 
network tickles, it gets updates
 on the data, it's using this 
save method to insert the 
information into the database or
 update the existing information
            information, as the 
data is updated in the database,
 Room is going to detect at run 
time all those active places in 
your app, activity, fragment, 
service or custom lifecycle 
owners that are active, and that
 are subscribed or observing a 
live data object that traps the 
information that has been 
updated.
   Then this callback is going 
to be invoked, letting you know 
that most probably you want to 
update whatever slice of data is
 right now showing in the 
application stream.  For 
example, it can be an app that 
shows live results for some 
sports games, or live updates 
for the weather, wherever you 
happen to be.  Or live updates 
to market stock prices.  One 
part of your app handles the 
data flow, get that information 
and display and insert it into 
the database or update in the 
database, and the other part 
gets notified by Room and the 
live data that the information 
is now, has new data, and you 
need to update whatever the 
presentation is, once again in 
your activity, in your fragment 
or in your service that post 
maybe notification or handles a 
widget
.
   YIGIT BOYAR:  Thank you.  
Let's focus on another important
 topic which is relations.  
Relational database, it can 
understand the relations between
 entities, , is SQL and if you 
are using Android or any other 
platform they usually try to 
handle these relations for you. 
 Let's look at how it works in 
Room.
   If we have a feed, we have a 
user object which has posted 
this feed active.  You want to 
have a user field inside that 
field, inside that entity.  If 
you do this in Room, it    it 
won't compile the application.  
The moral of the story, we don't
 allow entities to contain other
 entities.  You probably are 
asking why    why, everybody 
else does this and if we can 
understand all this SQ lite why 
can't we just enable this.  
There is a particular reason for
 us not to do this.  We have 
seen a lot of problems with 
applications caused by this kind
 of models.
   I want to go through an 
example.  First let's look at 
this query, select a feed with 
some I.D.  When we select this 
feed or when you look at this 
feed item, you cannot know 
whether it also fetches the user
 or not.  It is hard to define. 
 Most of the O Ms the way they 
solve this problem is, just say 
delay loading until the user 
wants to fetch the data, don't 
load it, which works well if you
 are working on the server side.
  But on the UI it is a little 
more tricky.
   It's implied lazy loading, we
 can generate this code for you,
 we can say keep a user I.D., 
keep a user instance, when we 
get user is called, we can say 
if the user is the first time, 
now fetch it from the database, 
otherwise just return existing 
row.  It is easy.
   We believe this is actually a
 mine planted in your code base.
  Let's see how we explode it.  
We are using a recycle where we 
show these feed items and I will
 get the feed and we will show 
the title and the subtitle of 
the feed item, so it looks fine.
  Two months later your product 
manager comes, you know what, 
let's show the user name in that
 feed item as well.  It makes 
sense.  Okay.  So easy, your 
developer goes, adds, feed get 
user     user, get user name, 
put it on text feed, you are 
done.  You send the code, it 
looks obvious.  It passes the 
code review.  You test it.  It 
looks fine as well.  But when 
users start using it, the 
application starts receiving 
these application not responding
, ANRs.  This happens because 
when you are testing the 
application, everything is fast 
but when the user is using the 
application there is 50 other 
applications that are also 
trying to run.  If you want to 
relate to this, just try to use 
your application, updating your 
applications, you will see how 
it feels.
   These are still mobile 
devices, slower, so you have 
only sixty milliseconds.  Even 
if the query takes five 
milliseconds like ignore all the
 locks and everything, five 
milliseconds is a lot of time.  
You can probably lay out 20 
recycle views at that time.
   How do you solve this problem
 with Room?  Previously we said 
SQ lite is a relational database
.  We should take advantage of 
it.  We just keep the user I.D. 
 We know the feed is a user I.D.
     I.D., just keep it.  Now we
 can write a query that says I 
want title subtitle and user 
name and join me these two 
tables on this constraint.  This
 is already a solved problem on 
the SQ lite, why don't we 
embrace it.  When you do this, 
it's faster, on the data you 
need, you know data is memory.  
What is this item, it can be 
like any java class, it may even
 have a   a constructed with 
public fields, on the final 
fields, room will set this 
without any reflection, will use
 that constructive.
   You can seen      even say 
return to live data because Room
 knows about the query, it knows
 it's querying these two tables.
  Think about the other example,
 if you are observe         
observe ing the feed, if the 
user changes how do we on    
know the data feed or not.  But 
when we wrote this, we know what
 you are returning, we know 
which tables you are queer      
 querying so we can be clever 
about this.  It's what we tell 
people, if you want to have 
relations and you cannot use 
embed      embedded or type 
adapters you want to use poajos 
for your relations which means 
you don't have hidden costs, you
 only fetch what you need and 
it's still observable and you 
don't need to do anything for 
that -- PoJO.  But we said SQ 
lite is relational.  Room 
supports foreign keys as well.  
Inside your entity, you can say 
this entity has a foreign key, 
to this other entity, where I 
want to match the I.D. code in 
the identity with this child 
column.  SQ lite gives support, 
you may have multiple fields 
matching multiple fields in the 
other tables.  It is complex.  
You should go ahead and read but
 once you declare this now SQ 
lite knows about this 
relationship.  If it knows, you 
can say things like, if someone 
tested        test      attempt 
to delete the user, don't let 
them delete.  Or if the user is 
updated, please update me as 
well.  So trying to embrace SQ 
lite rather than trying to hide 
details and create a bunch of 
pitfalls.
   SQ lite keeps data in a 
structured way, so you can query
 it back.  With regard to query 
like this where you want to 
select feed items which has a 
certain title, but if you just 
write this query and don't do 
anything else, that means SQ 
lite needs to go through every 
single row in the database to 
find the ones that are matching.
  If you are not making this 
query frequently, that is fine. 
 But if this is the query that 
you run frequently, you probably
 want to index it.  It's simple 
in Room.  Inside the entity 
annotation, you can say can you 
please index this column.
   If your query is like this 
where you have, you create a 
title and subtitle together, you
 can also create a composed 
index which means two things 
will be indexed together, so if 
you query together, that will be
 fast.
   You can have multiple indices
.  It doesn't matter.  Just 
because you are querying 
something doesn't mean you 
should index it, because every 
index you add has a cost on 
every insertion or data change. 
 You need to just measure and 
see what is the best way.  SQ 
lite's documentation on query is
 amazing.  If you read that you 
will know how to write query ies
 and how to automize it.  Test
ing, in the past we haven't been
 good in this area.  But when we
 design architecture components,
 testing was a important topic 
for us.
   We want, what we create for 
architecture components that can
 scale and that can be testable,
 so let's look at how you can 
test your queries.  By the way 
you should really test, just 
because we are verifying your 
SQL queries doesn't mean that is
 what you really intended.  
Right?  It's your java code 
compiles but you still test it, 
similarly if Room compiles you 
should still test it.
   But testing Room, increment 
testing on the device but there 
is no activity or UI so they run
 faster.  You can test on the J
V   JVM by changing the SQ lite 
bindings we use.  I'm going to a
 sample case, before each test 
is create ed, I create my 
database but I create an in 
memory database, which means the
 database will be create ed when
 the test starts, and I can 
clean it afterwards.  You want 
these test toss run        
       tests to run isolate ed 
from other tests.  You don't 
want to save the data into disk.
  I say after just close the 
database so memory can be 
released fast.
   The rest is, I create a item,
 insert it to Room and make some
 queries on it, and I verify 
that that is what I expect.  But
 the way room is designed helps 
with the overall application 
testing.  Remember we talked 
about those Dao classes we 
generate.  Because you defined 
how you access the database as a
 interface, which has nothing to
 do with SQL, you can easily 
mark it.  If you have a model 
that you want to test, you can 
test view models on your 
computer, you don't need to run 
them on the device.  You can 
create a mock of the Dao, use 
mockito to test it.  Return 
these feeds.  Because it's a 
java interface, there is not 
even any mention of database 
there.  It's all very well 
abstracted, without you doing 
anything.
   Migrations is another 
important topic.  As you use the
 application over time, your 
entities will change.  Let's say
 if you have the user object, 
now we start to show user photos
, so you want to add this field 
into that.  If you do that, when
 you run the application, as 
soon as Room, you first access 
the database, Room says oops, 
looks like you change the schema
 but didn't increase the version
.  Is it going to crash your 
application.  You need to handle
 this.  You can simply increase 
your database version in the 
annotation, which means you 
don't want any of the previous 
data.  You want to start from 
scratch.
   Or, you can write a migration
.  To write a migration in our 
database builder, you can pass a
 couple of migration implement
          implementations like 
this one says, I don't have to 
migrate from one to two.  You 
can have one from two to three, 
three to four and then room will
 run whatever necessary from the
 current version on the device 
to your latest version.  It will
 chain them.
   If anything in the middle is 
missing, it's going to recreate 
the database.  Another important
 to know in this migration is 
don't use any constants.  Always
 use full S sc   Q lite because 
over time applications will 
change, those constants will 
change.  You wrote this but we 
said testing is important, right
?  Room comes with a testing 
artifact.  We have separate 
artifacts so you can test 
migrations.
   A couple of steps.  First 
when we compile Room, we can ask
 Room to export the schema, how 
your database looks, into some 
JSON files, which you should be 
committing to your version 
control system.
   Then we want the SQL files to
 be visible through our test so 
we can access them.  Last but 
not least, for Android test, we 
add new dependency from Room 
which includes helper classes 
that will help us with our 
migration testing.
   Let's say we are trying to 
test the migration for version 1
 to 3, this is migration test 
helper class which comes from 
that artifact, we create an 
instance of the helper.  This 
can onI can             
canonical name is how Room 
exports files by default.  Don't
 need to think about it much.  
Create a open factory once we 
have that migrate from 1 to 3 we
 ask helper to create the 
database at version 1.  You may 
have entities that doesn't exist
 anymore.  It doesn't matter 
because it did export previously
 by reading that, it can 
recreate your database in that 
version.  You can change it 
directly.  You can do whatever 
you want.  You realize this 
doesn't return your Dao, it just
 returns your database instance 
because DAO may not be valid 
anymore.  You can say run these 
migrations and validate the 
schema.  We review the database 
name     name.  We give it the 
version we want.  You may have 
tables that you had before, and 
maybe you want to keep them but 
don't tell Room about it.  So 
you can decide whether we should
 check for these or not.
   Now you give a list of 
migrations.  These are all 
migrations we have, multiple 
Room this, migrate the database 
from the previous version to the
 new version and check the 
schema for you    you.
   Now you can also have, if you
 have constant things, changed 
entities in the previous 
versions to the new versions, 
you can manually assert on these
 things.
   That was Room.  We have very 
detailed documentation page, 
documentation on the Web Page.  
Like we do for Room, also for 
all our architecture components,
 we have a testing artifact 
which includes one rule for 
instrumentation test, dealing 
with the background tests we use
.  We want to make sure 
everything we have here is test
able, and if it is not, we will 
fix it.
   What is next for you?  Room 
is available today, please go 
check it, like play with it.  
See how it looks.  Check out the
 developer.Android.com/arch.
  These can work independently 
but these also work very well 
together, as you have seen in 
the live data example Kirill had
.  Check it out.  Also check out
 the code labs.  We have code 
labs for Room and life cycles 
and they are available in the 
code lab tabs.  Thank you.
     (applause)
.
     (end of session at 1:10 
p.m. PT)
   Services Provided By:
        Caption First, Inc.
        P.O. Box 3066
        Monument, CO 80132
        800-825-5234
        www.captionfirst.com.
   ***                          

in a rough draft format. 
   Communication Access Realtime
 Translation (CART) is provided 
in
   order to facilitate 
communication accessibility and 
may not be a
   totally verbatim record of 
the proceedings.
   ***


   Google I/O 2017.
   May 18, 2017.
   1:30 p.m. PT.
   Stage 6
.
   Super charged Live
.
   Session TC5F04.
   Services Provided By:
        Caption First, Inc.
        P.O. Box 3066
        Monument, CO 80132
        800-825-5234
        www.captionfirst.com.
   ***                          
   This text is being provided 
in a rough draft format. 
   Communication Access Realtime
 Translation (CART) is provided 
in
   order to facilitate 
communication accessibility and 
may not be a
   totally verbatim record of 
the proceedings.
   ***
.

   Google I/O 2017.
   May 18, 2017.
   1:30 p.m. PT.
   Stage 6.
   Supercharged Live.
   Session TC5F04.

  Thank you for joining.  Our 
session will begin soon.
   At this time, please find 
your seat.  Our session will 
begin soon.
   SPEAKER:  Hello!
     (applause).
   How is everybody doing?  You 
okay?  Yeah, that was really bad
    bad, how is have been doing,
 you okay?
     (cheering).
   Much better.  I'm Paul.
   SPEAKER:  I'm Surma.
   PAUL LEWIS:  We normally do a
 show called supercharged where 
we live code things for people, 
and last year, we did live, live
 live in front of people live, 
and we thought let's do it 
again!  That is exactly what we 
are going to do today.  But what
 we normally do is, we have one 
of us code and one of us talk.  
But today we thought we would 
have both of us code.
   SURMA:  Not at the same time.
  We could.  We shouldn't.
   PAUL LEWIS:  That would be 
stupid.  We shouldn't do that.  
Surma is going to code first.  
Tell them what we are going to 
do   do.
   SURMA:  I had the idea or I 
exported my trawd       archive 
because I wanted to look through
 it.  And it turns out the file 
it cons      contains, all my 
tweetle         tweets I've done
 is 60 megabytes, which is a big
 file.  I'm not going to open it
 because VS code doesn't like 
big files apparently.  But I 
have the first four tweets in a 
extra file.  That is what we are
 looking at.  You can see my 
first tweet was in 2008, where I
 say something fundamental, as 
got an account on Twitter, yay.
   PAUL LEWIS:  That is have 
been's             have been   
        everybody's first.  Or 
hello, world.
   SURMA:  Shutting into a empty
 echo chamber.  Nobody cares.  
How would you use this file in a
 web app.
   PAUL LEWIS:  Bring the file 
back.  This isn't an array
          an array of objects.
   SURMA:  It is concatenate ed 
JSON objects.  They end here.  
The next one starts.  There is 
no commas.
   PAUL LEWIS:  The JSON pause, 
if you get rid of the selection,
 it has a wiggly under there, it
 thinks it's not real JSON.
   SURMA:  There is more than 
one JSON object in there.  There
 is going to be difficulties.  
The first thing we have to do is
 type really fast and say that 
we have a copyright.
   PAUL LEWIS:  Yay!  We like 
snippets too.
   SURMA:  I'm going to work 
with scripts.  I'm going to work
 with my part of the program, 
and I'm going to use the new --
   PAUL LEWIS:  Dot JS.
   SURMA:  That is me.
   PAUL LEWIS:  Brilliant.
   SURMA:  Follow me.
   PAUL LEWIS:  Always branding.
  I love it.
   SURMA:  I'm going to use a 
module which is a new thing, 
which allows me to be defer by 
default which is something you 
should be doing but something we
 get to use the new fancy import
 which we are going to use later
.  Let's write a new file and 
look at the tweet and fetch the 
tweets.JSON file.  I'm not going
 to put anything else in here.  
I'm going to put this here and 
call it dot Surma dot JS and go 
in here into the networks tab.  
If I load this, I downloaded 
sixteen megabytes of, which is 
quite a bit.  This was really 
fast because I might have a 
server running locally.  The 
second you are on the Internet, 
this will take a while.  You 
would have to wait seconds if 
not minutes before you could do 
anything with the data.
   PAUL LEWIS:  A important 
point, the web is a streaming 
medium       medium.  We want to
 treat it like that, rather than
 say I'm going to hold 
everything back and then --
   SURMA:  The official I 
received a response sound, T da
      ta-da?  It's not actually 
a JSON object.  It's a bunch.  
This would fail     fail, there 
is trailing data, please take 
care of this.  Can't use it.  
The next option that most people
 aren't aware of -- not that -- 
dot text, which would give you a
 sixteen megabyte size 
JavaScript string, and I'm 
telling you, on mobile, you are 
not going to have a good time.
   We are going into the new 
territory and use something that
 probably is not as well-known, 
namely response.body is a stream
.
   PAUL LEWIS:  What kind of 
stream?
   SURMA:  Thank you.  I was 
waiting for your line.  It's a 
readable stream, meaning it's a 
stream that you can read.
   PAUL LEWIS:  Came in for the 
education.  You get it.
   SURMA:  Readable stream is a 
synchronous data
 structure, some people are 
going to, it's a little like a 
promise but it can be recalled 
multiple times.  Every time you 
get a new result back.
   PAUL LEWIS:  There is a pipe 
of data.  Assume data off it.
   SURMA:  Every time you get 
the next chunk of network 
delivered, because it's a stream
, once you get the chunk it's 
gone.  It has been consumed.  
Therefore, there can only be one
 reader at a time.  We have to 
get a lock for the reader which 
you get by get reader, which 
means you have reader object and
 you are the only person who can
 read from this stream.
   PAUL LEWIS:  What if you want
 to get rid of the log?  Get rid
 of reader.
   SURMA:  Release lock.  We 
don't need that.  We have a read
er.  A reader ask a asynchronous
 data structure.  I'm going to 
make use of async functions 
because otherwise this is going 
to get weird, because with async
 functions we can write code in 
a linear imperative way even 
though it's asynchronous so we 
can do stuff that is very old 
school and usually hated upon 
while true.
   PAUL LEWIS:  Ew, really?  
This is in              This 
isn't production code.
   SURMA:  It is not production 
code.  It will work, hopefully. 
 Once you want to read from a 
stream, you would call your read
er and say read.  Since it's 
asynchronous we have to wait.  
It is basically a promise but 
with the wait it inlines all the
 handling and it makes it nicer.
  What we get from there is a 
value and a down flag.
   PAUL LEWIS:  Destrokeing 
going on there.
   SURMA:  All the new features.
  If the stream is done, end of 
stream, then it's done.  Done is
 true.  We can just return, 
which is a well true loop is 
okay because that is our cancel 
condition.  This is how we break
 out of it.
   PAUL LEWIS:  I'm still iffy 
on it seriously
.
   SURMA:  What is its value, 
let's
 try out what the value is.  The
 value thing, I'm going back 
here, open the console.  We can 
see we get a bunch of U and 8 
arrays, this is literally the 
raw data from H   http response 
packaged in tight arrays in java
      JavaScript.  They are 32 
kilobytes big but that is more 
from coincidence than anything 
else.  The last one is smaller. 
 There are probably smaller ones
 in the middle.  I don't know.  
But it's a number you can't rely
 on.  This might be a implement
ation of Chrome or it doesn't 
matter, the number not really 
relevant.
   Now we are receiving these 
chunks.  They are 32K bit 
coincidentally.  But the problem
 is we have these series of JSON
 objects in my tweet file, and 
now a tweet could be, there 
could be multiple tweets in one 
chunk or they could be one tweet
 spanning multiple chunks.  That
 is not helpful.  We are going 
to write a so-called transform 
stream, which is going to 
transform our stream in a way 
that the chunks and the JSON 
objects align.  Every chunk 
after the transform we can be 
sure is exactly one JSON object.
   PAUL LEWIS:  You can pass it 
to Json.parse and you would get 
a Json object.
   SURMA:  In the end we should 
be able to do that.
   PAUL LEWIS:  We want to pair 
objects.
   SURMA:  Exactly.  We are 
going to use the reference 
transform streams, they are 
going to be in the browse er.  A
 transform stream is a readable 
stream, stream that you can read
, write able stream is stream 
that you can write, put together
 into one object where you put 
things at one end, do transform 
and get it out at the other end.
  But they haven't landed yet.  
I have to rely on the reference 
implementation.
   I'm going to import from 
transform stream.JS.  Now we 
have it and can use it.  Let's 
see how to actually use it.  
Response         Response.body 
is a stream.  If you have a 
transform stream you can use 
pipe through, which means here 
is a transform stream, pipe the 
data through the transform and 
give me the result.  Here we 
would create a new transform 
stream, and that transform 
stream takes a transformer.  Not
 to be confused with the cars.
   What we have to write is a 
JSON transformer.
   PAUL LEWIS:  You sure you 
don't want to know the new 
insight there as well, go three 
deep.
   SURMA:  I can gel     get you
 a variable.  This gives us the 
JSON stream.  As we return we 
get the transform stream which 
is now a stream of individual 
JSON objects.  Afterwards we are
 not going to get the lock on 
our body but on our JSON stream 
and that should work.  Let's 
look at the actual juice y bit 
which is the JSON transformer.  
So class, Json transformer, it 
has a constructor which we are 
going to need but not right now.
  The transformer has to have 
three methods, it's a start, 
called at the start, we don't 
need that, there is flush, which
 is called at the end, and there
 is attention transform, ooh.
   PAUL LEWIS:  That is what we 
need.
   SURMA:  You get two program 
terse,                parameters
, the chunk you have to 
transform and the controller.  
The controller is something that
 allows you to control your 
stream.  Let me explain.  Code 
is always better than trying to 
hand wave all the things.  It 
allows you to do stuff like find
 what is the output of the chunk
      chunk.  You can have 
different signals, like back 
pressure and your cache is full,
 and we don't need any of that.
   PAUL LEWIS:  I have a feeling
 if you Google for Jake 
Archibald streams you are going 
to get a good --
   SURMA:  It is going to appear
.
   PAUL LEWIS:  That is the 
worry, if you say streams too 
many times.
   SURMA:  I'll stop mentioning 
the word now.  We take the 
controller.  With MQ we can put 
something on to the output queue
      queue.  To show you it 
works I'm not going to do any 
transform.  Whatever comes in, 
I'm going to put it into the 
output.
   PAUL LEWIS:  Like a null 
transformer.  Do nothing.
   SURMA:  Ideally everything 
should work the same.  Yes.  
It's still doing nothing, 
downloading the file.  But now 
we have a point, we can inject 
our code to do all the transform
 a             transformation-y 
bits.
   How do you do that?  We have 
to write our own little Json 
parse er.
   PAUL LEWIS:  Really?
   SURMA:  Not as complicate
            complicated as it 
sounds.  We will collect all the
 chunks that belong to one JSON 
object and one we have them, we 
will squash them together, get a
 string.  Then we start oaf over
          over.  In the chunks, 
we collect all the chunks that 
belong to one JSON object.  We 
have to figure out how do we 
know when we have reached the 
end of a JSON object?  We do 
that by counting curly braces.  
Whenever we get a chunk, byte of
 chunk, we loop over all the 
bytes in our chunk.
   PAUL LEWIS:  That could be co
nst.
   SURMA:  It could be.
   PAUL LEWIS:  Sorry, 
inadvertent.  Review, sorry.  
Carry on.
   SURMA:  We have to convert it
 into a character.  JSON is 
always ASCII so we don't       
don't have to worry about 
shenanigans.  We are going to 
switch over whatever character 
that is.  We care about either a
 opening brace or closing brace.
  We are going to start and try 
to count how deep -- whenever we
 encounter a opening curly brace
 we increase it.  When we 
encounter a -- caps lock is in 
the way.
   PAUL LEWIS:  This does not 
feel brittle to me.
   SURMA:  Stop it!
   PAUL LEWIS:  I'm not cool 
with this.
   SURMA:  When we decrease the 
depth we will hopefully reach 
zero at some point, which means 
we have found the end of the 
JSON object.  We are going to 
emit.  We will talk about this 
later.  If we reached the end of
 the chunk without having 
reached 0, we have to continue 
into the next chunk and this 
current chunk -- yes, I do this 
dance y programming.
   PAUL LEWIS:  Do the chunk, 
okay, fine.
   SURMA:  It helps people 
understand.
   PAUL LEWIS:  I get it.  It's 
great sushz when           .
   SURMA:  When we get to the 
end of the chunk but not emitted
 it, we chunk, push, chunk, push
.
   PAUL LEWIS:  This is like 
when I type.  I'm the one that 
does the typos, he is really 
good at
 typing and really annoying to 
me   me.
   SURMA:  We are in the middle 
of some chunk.  We reached 0.  
We know the start of the chunk 
as part of the JSON object.  The
 next is part of the next JSON 
object.  We are going to split 
that apart.  We have the tail.  
Uint 8 away        8 array.  All
 typed arrays in JavaScript are 
a chunk of memory.  You can have
 different views.  We can look 
at the same memory as a series 
of floats or series of ints but 
also you can have them in 
different positions          
positions.  We are going to 
split it apart by create ing new
 views onto the same memory.
   PAUL LEWIS:  To be clear 
rather than say copying this 
array into two separate arrays 
you are create ing two views on 
to the same memory underneath.  
That one becomes the end of the 
object and the next one becomes 
the start of the next object.
   SURMA:  Exactly.  I realize 
for this we need to know the 
position the index where we are 
at.  I'm going to rewrite to do 
the old --
   PAUL LEWIS:  I was feeling 
R50E8       R50E8       really 
smart --
   SURMA:  I know, tail goes to 
I plus 1.  At I we have the 
closing brace.  The next one is 
where we want to cut.  We have 
the next object where we kind of
 do the same.  We start after it
   it, if we emit the last 
program fur             
parameter that means all the 
rest of it.
   PAUL LEWIS:  I plus 1 is the 
next character because I is the 
curly brace.
   SURMA:  After this we 
continue working on the 
recommend           remain 
       remainder.  We are going 
to overwrite because that is the
 next thing we can work on.
   PAUL LEWIS:  This stuff are 
youence my head             
        are you         ruins my
 head.
   SURMA:  Tail is the last bits
 of our JSON object, it's part 
of the chunks array.  We know 
the chunks array is our next 
JSON object, but potentially 
split into multiple chunks.  We 
need to convert this into a 
string.
   PAUL LEWIS:  Several 
potentially Uint data arrays 
that need flattening into a 
stream.
   SURMA:  Yes.  To convert a 
buffer or atyped array to a 
string we need to use a text 
decoder.
   PAUL LEWIS:  Sure we do.
   SURMA:  I wish it would be 
simpler.  But that is how it is.
  We can use this, dot chunks
.reduction         reduce.  
Start with ament     empty 
string.  Take the string and the
 chunk and concatenate the 
string with a decoded version of
 the chunk.  That is going to be
 our JSON string.
   PAUL LEWIS:  Sure.
   SURMA:  Is this correct?  I 
think it is.  Now we have a Json
 string which we can --
   PAUL LEWIS:  Is there a 
German word for the feeling of 
sad    sadness you get when 
something is really convoluted?
   SURMA:  You could say 
something like ...
     (speaking German).
   PAUL LEWIS:  Pardon?  I love 
it.  CSS.
   SURMA:  Wait ...
     (speaking German.) German 
always has a word.
   Json string which we can emit
.  We have turned into a string,
 we can push that out.  The only
 thing left to do is to reset 
our state.  We have to say, this
 chunks is now empty because we 
are done processing it.
   PAUL LEWIS:  That is true.  
You can set dot length equals 0.
   SURMA:  You can?  I can do 
this?
   PAUL LEWIS:  Yes, you can.
   SURMA:  I didn't know length 
was readable.  I'm trusting you.
   PAUL LEWIS:  Good, awkward.
   SURMA:  On the next loop we 
want to start at 0 but it's a 
four loop so I we have to set to
 minus 1.
   PAUL LEWIS:  That's gross!  
You are not shipping that, are 
you?, I am.
   PAUL LEWIS:  Prototype.
   SURMA:  I'm going to take it 
on 4 because otherwise it's 
going to take too long.
   PAUL LEWIS:  If this goes 
right we are going to see four 
tweets.  No, we are not.  Line 
13.
   SURMA:  People are paying 
attention.  I like it.  What is 
on line 13?  Bytes is our chunk 
i.
   PAUL LEWIS:  He is judging 
you because you changed my 4cons
t, is to that.
   SURMA:  I typo'd decode.  Yay
!  Bigger, so we can all see we 
have four individual logs which 
contain one Json object which 
means the alignment works.
   But, and there is always a 
&quot;but&quot; what if there is a opening
 brace.
   PAUL LEWIS:  See?
   SURMA:  You would see nothing
 because the parse er would see 
opening curly brace, a object 
started but it's in a string and
 we didn't know that.  It's not 
hard to fix.  We need to track, 
if we are in a string.
   PAUL LEWIS:  You are kidding 
me.  Come on!
     (laughter).
   SURMA:  Whenever we find a 
double quote, we are going to go
 in string, is this.
   PAUL LEWIS:  Sure, sure.
   SURMA:  In string.  And 
whenever we are in string, we 
don't actually care.
   PAUL LEWIS:  Now I'm getting 
that word, that feeling, the 
German one about the sadness.
   SURMA:  Feels less brittle 
now?  Isn't it nice?
   PAUL LEWIS:  Feels perfectly 
great.
   SURMA:  This should work now.
   PAUL LEWIS:  Yay!  Back size 
double quote.
   SURMA:  He is right.  If I do
 this
, it wouldn't work.
   PAUL LEWIS:  That's bad.
   SURMA:  Let's fix that.  If 
we find something that's escaped
 which we have to he is indicate
 because                        
escape because it's the escape 
--
     (laughter) we have to skip 
the next character.
   PAUL LEWIS:  Sure.
   SURMA:  And skip next.  That 
is false.
   PAUL LEWIS:  Can I revoke 
your programming license?
   SURMA:  No.  If you have to 
skip the next one, we have to 
set it to --
   PAUL LEWIS:  It's your fault,
 yeah, go on then.
   SURMA:  And continue.
   PAUL LEWIS:  Yay!
   SURMA:  Ultimate test is 
going back to the full file, and
 make tools grind to a halt.  We
 are not blocking the main 
thread too badly because this is
 kinda okay.
   PAUL LEWIS:  Kinda important.
  I've got a 16 mg file that is 
being handled on the fly, being 
streamed and transformed on the 
fly.  What is the most recent 
one there?  Must be, in German, 
yeah, okay.
   SURMA:  That is processing 
going on but it works pretty 
well.  Now --
   PAUL LEWIS:  Move it.  
Because no more people I don't 
think would sit there going, 
yes, I am enjoying your JSON 
coming down and watching it 
arrive in the console there.
   SURMA:  I certainly do.
   PAUL LEWIS:  Know your 
audience.  We thought the better
 thing to do might be to create 
some progress style.
   SURMA:  You do the visual 
stuff.
   PAUL LEWIS:  That's how I 
roll.  I'm going to make a SCdao
.  So my branding is okay.
   SURMA:  I can't believe you 
did that.  If you want to call 
it LewisJS.
   PAUL LEWIS:  I want to call 
it stick to the script.
  Aero twist.  Not what I 
expected from today.  I'm going 
to make aero twist.JS.  I have a
 custom elements.  I don't like 
writing these out.
   SURMA:  I can type by heart, 
I choose not to.
   PAUL LEWIS:  With lifecycle 
callbacks filled out, 1.0, what 
is the German for custom or 
lifecycle callback functions?
   SURMA: .
     (speaking germ      German)
.
   PAUL LEWIS:  Normally we have
, I'm going to call it SCDAO.  
Normally we go with a unnamed 
class.  Today I'm not going to. 
 That applies to SK dial.  I 
don't need attribute change call
back         callback.
   SURMA:  You want to refer to 
the name of the class later.
   PAUL LEWIS:  We need couldn't
 stands in here.                
 stants        stant    
            constants in here.  
I'm going to get rid of this, 
because --
   SURMA:  I like my log.
   PAUL LEWIS:  Tough.  I'm in 
control now.  Normally, you 
could do something like this 
with SVG but today I'm going to 
use canvas because I feel like a
 lot of people, I did it but I 
used the canvas.
   SURMA:  You are not going to 
bend over backwards and make dif
 be round and --
   PAUL LEWIS:  No.  I'm going 
to use canvas like this.
   SURMA:  Canvas is blazingly 
fast and underutilized on the 
web    web.
   PAUL LEWIS:  When you use 
canvas you get the context from 
the canvas like this.  Just 2D.
   SURMA:  Old school.  Think of
 the context as your pen that 
you move around on the canvas 
because you give commands like 
move to, align to and you draw.
   PAUL LEWIS:  See, I told you 
I was going to do -- there we 
go   go.
   SURMA:  Constants.  Makes you
 a good citizen because you 
avoid magic numbers.
   PAUL LEWIS:  I know, right?  
I feel really good.  This is a 
custom element I can call append
 child rather than something 
else, extending html element.
   SURMA:  But, it's a bug but 
works in Chrome which is 
hilarious, you are per spec not 
allowed to manipulate the Dom in
 the structure of the element.  
You have to do it -- it's a bug!
   PAUL LEWIS:  Works.
   SURMA:  Move it to the 
connector callback.  That is 
where you are supposed to do it.
  Technically you cannot rely on
 the fact that the dosm is    
     Dom is already available in
 the constructser.  It could 
float in the ether of custom 
elements.
   PAUL LEWIS:  It works.  I 
probably should remove it but 
whatever.
     (laughter)
.
   Sure.  The other thing we are
 going to do is, I want a API 
where we can set a percentage 
value for the dial, we get our 
16 mgs down as the file comes 
down, between 0 and 1 I'm going 
to say.
   SURMA:  We need a way to put 
that value in from the outside.
   PAUL LEWIS:  Exactly.  I'm 
going to set percentage.
   SURMA:  The reason you are 
not using a straight up property
 is because we want to tie logic
 to the fact when somebody 
changes the value.
   PAUL LEWIS:  Exactly right.  
Use like number.percentage.  I 
can never get ha right.  You 
                   get that 
right.  You could be like throw 
new error.  No.
   SURMA:  Helpful error message
s.  Well-done.
   PAUL LEWIS:  I'm all over 
this.  I'll teed      fidey it
          fide      tidy it up 
before it goes to the GitHub
 repo.
   SURMA:  All our previous code
 is on there.  This one will get
 up there at some point today 
when we are done.  Feel free to 
use it.  Play around.
   PAUL LEWIS:  When you set 
percentage values, we get a 
reference to the dial from 
outside, we say percentage 
equals 1 or 0.5, then draw, 
which we will draw on the canvas
.
   SURMA:  I wonder what the 
draw function does.
   PAUL LEWIS:  I don't know.  
We will begin, always operate 
against the context.  We will 
begin path.  Then if you are 
going to do something like begin
 pass or path, always use, or is
 it close path, put its 
corresponding.
   SURMA:  Because the opposite 
of begin is close.
   PAUL LEWIS:  Is enclose he 
is               close.
   SURMA:  That makes sense.  
Welcome to the web.  Pawlsz    
   PAUL LEWIS:  I'm going to do 
a circular dial.  I want a arc.
   SURMA:  Normal progress bar 
but went into a circle.  Yeah, w
h   whee!  Pawlsz I          
   PAUL LEWIS:  I love this.  XY
 radius.  I'm going to say mid, 
mid and mid.  I haven't defined 
mid, yet.  That is fine.  0.
   SURMA:  It's the middle.  Let
 me guess.
   PAUL LEWIS:  You are so good 
at this.  Fine, fine, I'm guess
      guessing an angle, which 
is in radius, because I want to 
check that it draws something 
because when you work with 
canvas sometimes it draws 
nothing.
   SURMA:  That is fun to debug.
   PAUL LEWIS:  Const mid equals
 sc dial, we go for the size.
   SURMA:  Good mile      
mileage out of that constant.  
It's all over the place.
   PAUL LEWIS:  It's going to be
 great, this.
   SURMA:  Don't you want to do 
a half?
   PAUL LEWIS:  Shh.  That 
didn't draw anything.  You know 
why, because after you close the
 path you have to say to the 
context to fill the path.
   SURMA:  Yeah because path 
defines the path.  You have to 
say whether you can stroke it or
 fill it or do both.
   PAUL LEWIS:  Need to actually
 call draw.  What a day!  There 
we go.  Got something drawing at
 least.
   SURMA:  You are lease       
easily excitable
.
   PAUL LEWIS:  That is not what
 I wanted.  I wanted to Pi    
pie kind of -- Go here, draw arc
, go back to the start, it's 
corrects, but as always you did 
the mistake, not the computer.
   PAUL LEWIS:  Thanks.  The fix
 for this is to move your pen to
 the middle middle, think of it 
the context is like a pen.  Move
 the pen as it were, say move to
, inside the begin path, say 
&amp;amp;%PM &amp;amp;%F0
                      say, 
mid    mid, mid.  It draws a 
kind of pie chart.
   SURMA:  That's good.
   PAUL LEWIS:  That is pretty 
good.  What else are we going to
 do?  Let's call this the outer 
arc.
   SURMA:  I find it weird that 
it starts to the right, because 
we learn to read the clocks, I 
feel like --
   PAUL LEWIS:  Fine.  He is 
right.  You wouldn't normally go
 to the side.  If you start 
progress, you start at top 
middle.  One option would be to 
take this start being 0 and do 
minus 90 degrees, but in the 
interest of showing something 
else to do with canvas, I'll do 
it like this.  I'll do a rotate,
 rotate the canvas, rotate the 
coordinate system.  Minus SC 
dial.  I'm such a grownup.  90 
degrees.  Look at that!  Like a 
proper --
   SURMA:  But half doesn't get 
its own constant?
   PAUL LEWIS:  No.  Return math
.pi --
   SURMA:  Didn't we say 
sometimes the canvas draws 
nothing?
   PAUL LEWIS:  Yes.  I know 
what this is.  I've seen it 
before.  It is because we are 
rotating around the top left 
hand corner.  We are rotating 
around the origin top left.
   SURMA:  Out of view, so 
drawing up there.  Cool.  Just 
to help people.  You are going 
to move the origin.
   PAUL LEWIS:  Yes.  If you 
move the origin with a translate
 and we move it to the middle 
middle, move it to the mid and 
rotate and move it back again, 
it sounds odd, but it will work.
   SURMA:  Makes sense but 
otherwise we would have to --
   PAUL LEWIS:  I don't know how
 else you would do it.  It is 
right.  Here is another thing.  
We will do a inner arc while we 
are here.  We are going to wire 
it up to your code, which is now
 legacy code.
   SURMA:  Ew.  We started out 
at bleeding edge and it's 
already obsolete.
   PAUL LEWIS:  No kidding.  
Math.Pi.
   SURMA:  Full circle.  Ooh.
   PAUL LEWIS:  This will be fun
, won't get anybody upset.  We 
will call it tau.
   SURMA:  You flame bait, 
apparently it has support in 
this room.  Yes, you should use 
tau, same as 2 times Pi.
   PAUL LEWIS:  CTX
.fillstyle equals, let's do -- 
fills style here, I'll send that
 to -- that doesn't look great, 
because it's the same radius.  
But if you do mid times by 0.8, 
look at that.
   SURMA:  Nice.  We get like a 
little thing.
   PAUL LEWIS:  We are good.  
Let's wire it up to your stuff.
   SURMA:  Actual the loading 
percentage pawltz       .
   PAUL LEWIS:  Percentage will 
be value between 0 and 1 times 
by SC dial.tau, which will 
disappear because the value is 
zero, which is fine.
   SURMA:  This time it's 
working as intended even though 
we don't see anything.
   PAUL LEWIS:  Absolutely.  In 
your thing I'm going to get 
reference dial equals document 
doc.
   SURMA:  This is where we can 
say this is our production code 
because we are miptioning 
           mixing concerns.  My 
module takes care of loading and
 now we patch U   UI stuff in 
there.  For the sake of how to 
do this it's valid, I think.
   PAUL LEWIS:  Sure.  If it's 
done, we know that dial     
dial.percentage is 1, that is 
fine.
   SURMA:  That's correct.
   PAUL LEWIS:  For everything 
else we want to know how many 
bytes have come through the wire
, we need total bytes.
   SURMA:  Http protocol can 
help with that.
   PAUL LEWIS:  Yes, it can, 
headers.get and ask for content 
length.  I'm going to pause it  
        parseint on that.  We 
know the total number of bytes, 
if header comes through which 
I'm relying on, let bytes count 
it, is zero.  Sure.
   Now if we get down here, we 
are going to --
   SURMA:  Increase by the 
number, by the string length 
because it's JSON, that is equal
 number of bytes, because that 
is what unicode things --
   PAUL LEWIS:  Yeah.  It's 
counted, over bytes total.  
Between 0 and 1.
   SURMA:  Looks good.
   PAUL LEWIS:  Yeah, that's --
     (applause).
   SURMA:  I mean kinda.  We 
could say this is intended and 
ship it.
   PAUL LEWIS:  Sure.  You know 
what is going on here?  The 
canvas is a state machine, 
effectively, so every time we 
call draw, it's rotating the 
canvas by 90 degrees.  Oops.
   SURMA:  We keep rotating and 
drawing.
   PAUL LEWIS:  Da da da da da, 
that is obviously the drawing.  
We can actually ask, before we 
do that, the rotation and so 
forth, we can ask the canvas to 
save its stuff, its state.  Like
 what is the fill style, 
everything like that, we can 
save it.  This is another one of
 the moments where if you call 
dot save you immediately figure 
out where you are going to call 
this dot ctx.restore.  You are 
going to push something on a 
stack and never pop it and that 
won't be fun for you.
   SURMA:  This will revert to 
the previous state except all 
the drawing that happened in the
 meantime will persist.
   PAUL LEWIS:  Yes.  Here now 
we should go round -- definitely
 better but it's actually, 
probably can't tell, I'm going 
to zoom in.
   SURMA:  This is pixel ated, 
isn't it?
   PAUL LEWIS:  It's jagged 
because you have to clear the 
canvas       canvas.
   SURMA:  Canvas by default 
draws smooth which is surprising
 but we keep drawing over the 
smooth edges so it adds up to 
not smooth.  Pawldz we will do 

size.
   SURMA:  One of the most 
efficient constants ever 
create ed.
   PAUL LEWIS:  While I'm here 
since it's not in the middle of 
the screen, I'm going to move it
 to the middle of the screen.  I
 do like doing this style.  
Style.  There you go.  Html.body
.
   SURMA:  I do that all the 
time.  Are you going to do the 
discipline, vertical centering.
   PAUL LEWIS:  Yay!  People are
 like you can't do vertical 
centering in CSS, you can say, 
sure you can, display flex, 
align items center, justify 
content center, smile a happy 
smile.
   SURMA:  Hurrah!
   PAUL LEWIS:  Next umm,      
up I'm going to change the color
 from black      black.  A 
little monoChrome.  There we go,
 that is a blue color.  Let's 
make it nicer.  This is a 
template string.
   SURMA:  Oh.
   PAUL LEWIS:  Stop it, Paul.  
Told you I made typos.  We will 
do this -- whoa.
   SURMA:  That would be 
interesting.
   PAUL LEWIS:  That would be 
weird.  Times by 255.
   SURMA:  We are going to go 
from blue and no red to blue and
 full red.
   PAUL LEWIS:  Except going 
from black to pink.  Because RGB
 values have to be rounded.
   SURMA:  Once you have decimal
 points --
   PAUL LEWIS:  We are doing all
 right here.  What else can we 
do?  We can put a actual number 
in the middle to tell you how 
far we have got through the 
downloading.  Let's do that here
.  Put label, label.  I'm doing 
comments, because I know when 
you come back to canvas code.
   SURMA:  Half hour after this 
and we try to upload this, we 
will have forgotten what it's 
about.
   PAUL LEWIS:  It's right after
 regular expression goes when  
                    expressions 
when you are like, really?
   SURMA:  On keyboard it still 
kind of works.
   PAUL LEWIS:  Fill text, we 
should set the color for that.  
Fill style equals, let's do it 
as a gray color.  Fill text.
   SURMA:  Preferring 333 
honestly.
   PAUL LEWIS:  Okay, fine.  You
 are normally not bothered by 
this stuff.  Percentage times by
 a hundred.  We will do it at 
mid, mid.  In the middle of the 
dial.
   SURMA:  Sounds good.  I mean 
... I would ship it.
   PAUL LEWIS:  Sure.  Do you 
know what, this is a great 
opportunity to talk about the 
restore, because if we do the 
restore before the label.
   SURMA:  It's still turned.
   PAUL LEWIS:  That will put it
 back where it needs to go.
   SURMA:  Probably round this 
too.
   PAUL LEWIS:  Math.round.  You
 are right.
   SURMA:  People don't care 
about the 15th decimal place.
   PAUL LEWIS:  I know, right?
   SURMA:  Look how tiny that is
!
   PAUL LEWIS:  Let's make it 
bigger.  CTX.font equals.  Let's
 have a look.  Make one of these
, sc dial.size.
   SURMA:  You make it dependent
 on the -- that is good.
   PAUL LEWIS:  Quarter size, px
, aerial.
   SURMA:  Symmetry, you don't 
handle negative feedback very 
well is the thing.  So I feel 
like the bottom left corner of 
the one is actually perfectly 
centered.
   PAUL LEWIS:  It is actually, 
it's because the text baseline 
and the text alignment will mean
 that it is for the bottom left 
happened corner which we can 
change by saying this --
   SURMA:  I thought we have to 
measure ourselves or something. 
 Here and now.
   PAUL LEWIS:  That would be 
awful.  Can you imagine?  Let's 
do this, text baseline.  One in 
the middle, the other one center
.
   SURMA:  Flex box is center 
center.  Let's not talk about 
it.
   PAUL LEWIS:  There you go!
   SURMA:  Nice.
     (applause).
   PAUL LEWIS:  But I think what
 we can do, another label, 
because we can.  777.
   SURMA:  I'll let you have it.
   PAUL LEWIS:  Let's do that at
, I don't know -- I love guess
      guessing numbers.  Number 
fishing is great, 0.6.
   SURMA:  Why do you stop using
 constants now, hu    huh?  
Percent.  Great stuff.
   PAUL LEWIS:  Minus 20
 plus 15.  Sure, close but not 
quite.  Minus 20 --
   SURMA:  Close but no cigar.  
Looking good.
   PAUL LEWIS:  Minus 15.  Minus
 14.  Plus 26.
   SURMA:  Why not?  It looks 
good.
   PAUL LEWIS:  I feel pretty 
good about this.
   SURMA:  Can we do this with 
network throttling
?
   PAUL LEWIS:  Absolutely.  At 
the moment this is no throttling
           throttling.  Imagine 
we were to click on that, and it
 was regular 4G.  This is 
important because we are now not
 blocking, as Surma's code 
showed you we are not blocking 
stuff coming down before we show
 anything.  We are able to give 
the user some information.  If 
you are able to do something 
with those tweets, you can be 
doing that work here.
   SURMA:  You can be rendering 
parts of the tweet because every
 chunk contains one tweet, we 
can start listing things while 
there is a progress bar in the 
top.  The main thread is still 
available because it can draw.
   PAUL LEWIS:  We have just 
about enough time to get away 
with this.  We call draw, I'm 
going to do a request animation 
frame to call this dot draw.  
I'm not going to call it any 
more.  It is going to go into 
this busy, it is always going to
 draw and it will probably break
.  I'm not going to spend ages 
explaining why    why, it's 
because the request an    
animation frame when it's called
, this stops referring to the 
class instance and starts 
referring to window.  The way to
 fix that is
 -- dot bind, it's weird.
   SURMA:  Lewis bind I call it.
   PAUL LEWIS:  Every single 
supercharged.  Since it's every 
frame, we should be able to do a
 recording.
   SURMA:  It's comfortably at 
60SPS we have lots of head space
, head room in our per frame 
budgets.
   PAUL LEWIS:  Giving something
 to our users fast.  We are out 
of time.  But transform streams,
 custom elements, canvas stuff, 
we have you something which is 
taking 16 mgs.
   SURMA:  We are under 200 
lines of code.
   PAUL LEWIS:  Let me check.  
This is a reminder how awesome 
the web platform can be because 
you have 110 from me and about 
70 from you.  So that is about 
180 lines of code and we have 
got something that I think is 
reasonable and quite 
interesting.  There you go.  
Thank you very much for joining 
us today!  We had a great time.
     (applause)
.
     (end of session at 2:12 
p.m. PT)
   Services Provided By:
        Caption First, Inc.
        P.O. Box 3066
        Monument, CO 80132
        800-825-5234

     www.captionfirst.com.
   ***                          

in a rough draft format. 
   Communication Access Realtime
 Translation (CART) is provided 
in
   order to facilitate 
communication accessibility and 
may not be a
   totally verbatim record of 
the proceedings.
   ***

   Raw file.
   Google I/O 2017.
   May 18, 2017.
   2:30 p.m. PT.
   Stage 6
.
   Web Performance:  Leveraging 
the metrics that most affect 
user experience.
   TBDA1D
.
   Services Provided By:
        Caption First, Inc.
        P.O. Box 3066
        Monument, CO 80132
        800-825-5234
        www.captionfirst.com.
   ***                          

in a rough draft format. 
   Communication Access Realtime
 Translation (CART) is provided 
in
   order to facilitate 
communication accessibility and 
may not be a
   totally verbatim record of 
the proceedings.
   ***.

   Google I/O 2017.
   May 18, 2017.
   2:30 p.m. PT
.
   Stage 6.
   Web Performance:  Leveraging 
the Metrics that Most Affect  
User Experience.
   Session 2B D.A. 1D

  TBDA1D.
. 
   SPEAKER:  Welcome.  Thank you
 for joining.  Our session will 
begin soon.
   At this time please find your
 seat.  Our session will
 begin soon.
   SPEAKER:  Hi, everyone.  My 
name is Shubhie Panicker, I'm a 
engineer tbork       working on 
Chrome.  Phil Walton 

 working on the web platform 
team.
   SHUBHIE PANICKER:  Over the 
last year our team in the web 
platform are developing new 
metrics and APIs that are user 
centric, in that they capture 
user perceived performance.  We 
have developed a framework for 
thinking about user perceived 
performance, that we want to 
share with you today.  And Phil 
and I are excited to be here 
sharing these metrics and APIs 
with you    you, in our past 
lives we have been web develop
ers and we understand the pains 
from gaps in real world 
measurement.  Before Google I 
worked on web frameworks for 
apps like search, photos, G, 
etcetera.
   PHIL WALTON:  I worked on 
Google Analytics so I know a lot
 about and seen the challenges 
around tracking performance in 
the browse er.
   SHUBHIE PANICKER:  This is 
the goal of our talk today, to 
help you answer this question.  
How fast is my web app?  You 
certainly asked yourself that 
     this and this may seem like
 a straightforward question, but
 the problem is that performance
 and fast, these are weighed 
words, what does fast mean n 
what context?  Fast means 
different things on navigation 
or clicking, scrolling or 
animations, so what is 
performance and what is fast in 
these contexts and fast for whom
 exactly.
   PHIL WALTON:  The truth is 
performance is hard.  We know 
this     this.  For web develop
ers, it's harder than it should 
be.  That is one of the reasons 
we are talking about this.  
There is a lot of tips and 
tricks that you might have heard
 and were not implemented or 
understood in the right context 
and can make things worse.  We 
don't want to give you more tips
 and tricks in this talk.  We 
want to talk about a way to 
think about performance, a 
framework, mental model for 
understanding performance 
measurement and the hope is that
 once you understand this model,
 you have a lot more tools at 
your disposal to solve 
performance problems yourself in
 your own app.
   Before we do that, let's talk
 about misconceptions and myths 
around performance today.  I 
would say this is the most 
common myth that I hear, some 
variation of this sentence, I 
tested my app, and it loads in X
.XX seconds.
   The reality is that your apps
 load time is not a single 
number.  It is the collection of
 all the load times from every 
individual user.  The only way 
to fully represent that is with 
the distribution, like the 
histogram you see here.  In this
 chart the numbers along the X 
axis show load times, and the 
height of the bars on the Y axis
 show the relative number of 
users who experience the load in
 that particular bucket.
   As you can see while the 
largest buckets and most users 
were between one and two seconds
, there were many users who 
experienced much longer load 
times.  It's important to not 
forget about these users.  This 
pattern toward the right is 
often called the long tail.  
Unfortunately it's very common 
in the real world.
   SHUBHIE PANICKER:  This 
histogram illustrates the 
difference between measuring 
performance in two very 
different contexts.  These 
contexts are measurement in the 
lab, versus measurement in the 
real world.  By lab I mean great
 tools like dev tools, 
lighthouse, Web Page test, other
 continuous integration 
environments you might have set 
up.  Lab is important.  It gives
 you a sense for how your 
changes are going       going to
 behave in the real world.  It 
helps you catch regressions 
before they hit your live 
production site.  They give you 
deep insight and breakdown so 
you can track down and fix 
problems.  Lab is super 
important.  It is necessary.  
But lab is not sufficient.  Real
 world measurement on the other 
hand, is messy.  Real devices, 
various network configurations, 
cache conditions, all of these 
different conditions for real 
users are impossible to simulate
 in the lab.
   Real user measurement helps 
you understand what really 
matters to your users.  It helps
 capture their actual behavior 
which may be different from your
 assumptions or lab settings.  
The answer the question of how 
fast is my app, it's important 
to measure this in the real 
world.  In our talk today, we 
will focus on real world 
measurement
.
   PHIL WALTON:  Coming back to 
the myth, there is another 
reason why the statement is 
problematic.  The question, when
 exactly is load, is a app 
loaded when the window load 
event fires?  Does that event 
really actually correspond to 
when the user thinks the app is 
loaded?  I'd argue that load is 
not any one single metric.
   It is an entire experience.  
It can't be, sorry, I meant to 
say it's not one single moment, 
it's an entire experience and 
can't be represented by one 
metric.  To better understand 
and illustrate what I mean by 
that, I want to show you an 
example.  I'm going to play a 
video of the YouTube web app 
loading on a simulated slow 
network.  I want you to pay 
attention to how the video loads
, the app loads, and notice that
 things are coming in one by one
.  So can we play the video?
     (video played).
   Think about how that felt.  
Now I want to play the second 
video, and I want you to pay 
attention to how you feel 
watching the second video.  
Think about the experience.  Can
 we play the second video?
     (individual know played
     (video played).
   It feels different, doesn't 
it?  I'm sure some of you didn't
 even know if the video was 
playing.  That is the point.  
When you give feedback to the 
user they feel something.  These
 two videos loaded in the same 
amount of time but the first one
 seems faster.  It feels nicer, 
because things come out right 
away.  If you went to a 
restaurant and you sat down at a
 table, waited for an hour and 
they brought you your drinks, 
appetizers, entree, dessert, 
chick and dinner mint all at the
 same time.  That would feel 
weird.  You would wonder why 
they waited until the very end
    end.
   Again, you might look at this
 and might think, we should 
optimize for the first initial 
render, get content there as 
soon as possible.  That is what 
this proved, right?  Sometimes 
that is true but not always true
.  Sometimes when you do that, 
you can make things worse in 
some cases and cause other 
problems.
   I'm going to play another 
example of, real life example 
from a website.  I know the 
Airbnb engineering team cares 
deeply about performance and 
user experience.  They try to 
make their pages as fast as 
possible.  They use server side 
rendering to deliver all the 
content in the initial request 
the             request.  It 
shows because the page loads 
fast even on a slow connection. 
 The problem is on slower 
devices that take longer to 
execute Java vice-president 
                    JavaScript 
the page is rendered but not 
usable for a   a couple seconds.
  You can see that in the video.
  Can we play the third video?
     (video played)
.
   As you can see the user here 
tried to click a few times and
     in the search bar and 
nothing was happening.  It 
wasn't until maybe the sixth 
click or so that the component 
pane from the top scrolled down.
   To be clear, this video was 
from a simulated slow device, it
 doesn't represent the majority 
of their users, but Airbnb is 
committed to providing a good 
experience for all their users. 
 They want to fix this and they 
care about this.  They are 
currently working on a fix to 
this problem.  I want to mention
 on a personal note that I'm 
really happy and glad that 
Airbnb was willing to let us 
show this to you.  It's cool 
that they want other developers 
to learn from their experience.
   Can we go boo     back to the
 slides?  All these examples 
that I just showed illustrate 
why you shouldn't measure load 
with just one single metric.  
Load is an experience and you 
need multiple metrics to even 
begin to capture it.
   SHUBHIE PANICKER:  This is 
another commonly held 
misconception, you only need to 
care about performance at load 
time.  Loading is super 
important.  But it is certainly 
not everything.  Historically we
 have all fallen into this trap 
of narrowly focusing on load.  
Part of it is our own developer 
outreach, our tools focus pretty
 much exclusively on loading.  
The reality is that there is 
lots of other interactions, that
 happen long after load, all 
kinds of clicks, taps, swipes, 
scrolls.  Think of all the time 
you spend on new sites in your 
E-mail, on Twitter, or Amazon.  
Load is a really small fraction 
of this overall user session.  
Users associate performance with
 their entire experience.  
Fortunately, the worst 
experience stick with them the 
most -- unfortunately.
   This is a summary of the 
problems that we have 
highlighted today so far.  Real 
world metrics are a distribution
, they should be seen on a 
histogram, not as an individual 
number.
   Load is an experience, it 
cannot be captured with a single
 moment or sing metric.  Third 
interactivity is a crucial part 
of load but it's often neglected
.  Finally, response iveness is 
always important to users, way 
beyond load time.
   These are the questions that 
we want you to ask us today.  
These are the questions that we 
hope we can answer for you as 
part of this talk.  User 
perceived performance is 
important, what are the metrics 
that accurately reflect this?  
How can we measure these metrics
 on real users?  How can we 
interpret these measurements to 
understand how well our app is 
doing?  Finally, how to optimize
 and prevent regressions going 
forward.
   In this segment of the talk, 
we want to talk about these new 
metrics and the basic concepts 
underlying them.  We have all 
used a traditional metrics like 
Dom content load and window on 
load to measure load time, the 
problem is that they don't 
correspond to the users' 
experience of load.  They have 
almost nothing to do with when 
the users saw pixels on the 
screen.  For example, a CSS 
style might be hiding the 
content, when Dom content load 
fires.  Even if the content is 
rendered, interaction can be 
blocked.  The JavaScript might 
not be there to hook up a 
critical handler, for example.  
These metrics completely ignore 
interaction even though we know 
that interaction is super 
important for modern web apps.
   PHIL WALTON:  What are the 
key experiences that matter to 
users and shape their perception
?  It's helpful to frame these 
as questions that the user might
 be asking.  Is it happening?  
Did the navigation start 
successfully?  Has the server 
responded          responded?  
Is there anything that indicates
 the     to the user that it's 
working?  Is it useful?  Has 
enough content rendered that the
 user can engage with the page? 
 Once content is rendered is the
 content usable?  Can they 
interact with it?  Is it blocked
?  Is something preventing that 
interaction from happening?  
Finally, is it delightful?  Are 
the interactions smooth, natural
        natural, free of lag or)
 ank and is the overall 
experience good.  Let's look at 
how these questions map to 
measurable metrics.  Here is a 
illustration of a pages load 
progress.  The first frame over 
there is the blank white screen,
 before the browse er has loaded
 anything.
   The second
 frame represents the first 
paint metric.  It is the point 
at which anything is painted to 
the screen that the user can see
, anything different from what 
the screen looked like before 
the response.  The second frame 
shows first content        
contentful            
content        contentful paint,
 second metric, when any of the 
content is painted.  By content 
I mean something in the Dom.  It
 doesn't just have to be text.  
It could be images or canvas or 
SVG, something in the Dom that 
is paymented to the screen.  In 
the third or the fourth frame, 
you see more stuff coming in but
 it's not quite enough content 
to be meaningful.  Then you get 
to first meaningful paints in 
the fifth frame, where the user 
can actually engage with the 
content, enough stuff is 
rendered that the user can, what
 they came for is here and they 
can start consuming it.
   Finally, the last metric 
timed interactive is when the 
page is both meaningfully 
rendered and usable, meaning 
it's capable of receiving input 
and responding in a reasonable 
amount of time     time.
   SHUBHIE PANICKER:  Phil said 
the first meaningful paint is 
when the page is useful and the 
user can engage.  This is when 
the primary content of the page 
has rendered.  But what is 
primary content?  Which elements
 exactly?  Not all elements on 
the page are equal.  There are 
some elements that are important
, we call them hero elements and
 when hee     hero elements are 
rendered you have arrived at the
 user moment of it is useful and
 the user can meaningfully 
engage with the page.
   Here are examples to show you
 what I'm talking about.  These 
are hero elements with popular 
sites.  For YouTube, we paint on
 the YouTube pawch page         
   watch page but here our old
ment is likely the thumbnail of 
the primary individual       
     video and play button.  For
 Twitter the notifications count
 and the first tweet.  For the 
weather app it's the primary 
weather content, even though 
there might be tons of other 
stuff on the page.  When these 
hero elements have rendered this
 corresponds with first meaning
ful pane and it is useful user 
moment.
   PHIL WALTON:  Some of the 
hero elements are content based 
and some are more interactive 
components.  In YouTube, for 
example, the hero element is 
rendered when the thumbnail is 
loaded and the play button is 
visible but probably not usable 
until the JavaScript that 
controls the play button has run
 and enough of the video has 
buffered to be able to start 
playing.
   If the hero elements are 
interactive, not only does 
rendering them matter, but also 
when it's usable, when it's CDI 
is, however there are times when
 interactivity can be blocked.
   SHUBHIE PANICKER:  To 
understand why important 
elements might be blocked and 
not interactive, think about the
 time when you were in a long 
line somewhere, say at the 
grocery checkout or bank, you 
are standing in line and there 
is one or two customers who are 
confused or they are angry, and 
they hold up the line causing a 
long delay.
   This is what long tasks do on
 the browse er's main thread.  
These are tasks that run long, 
they occupy the main thread for 
a long time.  They basically 
block all the other tasks in the
 queue behind them.  Scripts are
 the most common cause of long 
tasks, like all the work that 
scripts do in terms of parsing, 
population, evalling, etcetera. 
 If you use dev tools you are 
familiar with the primary type 
of work, style, layout, paint, 
script, it turns out all of this
 happens on the main thread.
   It also so happens that most 
interactions, things like taps, 
clicks, even animations, 
typically also need the main 
thread.  You can see how this 
can be a problem.  A long script
 is running        running, in 
the main thread, the user is 
trying to interact.  These 
interactions are waiting in the 
queue.  This manifests as delays
 in click, jang in scrolling or 
   or animation.  You wonder how
 long is long, what is long.  We
 define long to be 50 
milliseconds.  Scripts should be
 broken into small enough chunks
       chunks, so that even if 
the browse er is idle, and a 
user happens to interact, the 
browse er should be able to 
finish what it's doing, and 
service those inputs, that 
interaction.  50 milliseconds 
chunks will ensure that the real
 guidelines for response iveness
 is always met.  You might have 
heard a lot about 60 phipps and 
sixteen milliseconds and some of
 you might wonder why sixteen 
milliseconds.  If you are 
animate ing then sixteen 
milliseconds is important but 
thanmation            than      
animation issues are a small 
subset of responsiveness issues 
at large on the web today.  You 
know you are animate ing, yes, 
you have to share the six     60
 milliseconds budget with the 
browse er.  Long tasks are the 
cause of most of the responsive
ness issues on the web today.  
Scripts are by far the most 
common cause of long tasks.
   PHIL WALTON:  This table 
shows how each of the metrics 
map to the user question from 
before.  The question is it 
happening maps to the metrics 
for first paint, first content 
full paint, is it useful, maps 
to the first meaningful paint 
and the hero element time ings. 
 Is it usable maps to time to 
interactive and the last one is 
it delightful maps to long tasks
 or more accurately the absence 
of long tasks.
   SHUBHIE PANICKER:  You might 
be wondering how metrics like 
first meaningful paint or time
 ed interactive can work for 
every app.  You are totally 
right.  One size cannot fit all.
  We spent a lot of time in our 
metrics team trying to develop 
these generic standardized 
metrics that work for every app.
  We have learned it's 
incredibly hard to do that.  
That makes it hard to standard
ize.  That said, there is value 
in these generic standardized 
metrics, so these baseline 
metrics that work for the 
majority case, 70 to eighty 
percent of apps out there, we 
have made such metrics available
 in our tools, like you might 
see them in lighthouse, dev 
tools, Web Page test, and they 
are working to consolidate these
 definitions.  Down the road we 
expect Analytics to start 
surfacing variants of these 
metrics.  The main thing to 
understand for out of box 
generic metrics is that don't 
assume that they accurately 
capture, is it useful and is it 
usable moments for your apps.
   Try them out.  See how well 
they work for you.  When it 
comes to real user measurement, 
we encourage to you supplement 
these metrics with your own 
custom user metrics, or custom
ize these metrics and make them 
your own, make sure that they 
work well for your app.  We will
 show you specific tips do 
remember             for doing 
that later.
   PHIL WALTON:  Now that we 
understand how the metrics, the 
question is how do we get them 
in Java scriment.  That is the  
JavaScript, the most important 
thing to measure on real users. 
 Historically we used
 window load or Dom load.  But 
metrics have been harder, 
sometimes impossible to get in 
JavaScript, and in trying 
sometimes to define them can 
lead to problems.  This code 
sample shows how you would 
detect long tasks before these 
new metrics.  This is a hack.  
What this code is doing is 
effectively making a request to 
animation frame loop.  It is 
doing measuring frame after 
frame after frame and comparing 
the time stamps from the current
 frame to the time stamp on the 
previous frame, and if this is 
longer than 50 milliseconds, it 
is considering it to be a long 
frame.
   But there is problems with 
this method.  It kind of works, 
but it adds a lot of overhead.  
It prevents idle blocks, it is 
not great for battery life.  It 
doesn't even tell you the source
 of the problem.  You don't 
know, you might know if there 
was a long frame so you can 
assume there was a long task but
 you don't know what script 
caused the long task.
   This isn't just a 
hypothetical example.  This poll
 request on the amp project    
     project is them taking that
 code out, because they realize 
that it was more trouble than it
 was worth.  The number one rule
 of performance measurement code
 is that you shouldn't be making
 your performance worse by 
trying to figure out how good 
the performance is.
   These hacks show the need for
 real APIs built into the browse
       browser so the browse er 
can tell us when performance is 
bad.
   SHUBHIE PANICKER:  
Performance API os the       s 
on the browse er solution to 
yield measurement these are 
standardized APIs.  They are 
available in multiple browse ers
 not just Chrome.  And we 
recommend that you use these API
s, the fact is you will use a 
combination of these APIs as 
well as your own JavaScript 
Polyfills, and the reason why 
Polyfills are necessary is 
because the implementation time 
line on browse ers will vary.  
Asking you to customize and 
supplement these metrics, so 
these are the core build
ings           buildings 
          buildings blocks as we
 see it for web performance.  
High-resolution time, you are 
familiar with.  Performance 
observer replace ts old       s 
the old performance time line.  
It overcomes its limitations 
such as no pulling, low overhead
 API, it avoids race conditions 
from a shared buffer.
   This is what the usage or 
performance observe er looks 
like.  It also happens to be the
 code that replaces the hack 
that Phil showed you a little 
bit earlier.  Performance ob
serve er, usage is fairly 
straightforward.  You create a 
performance observe er and make 
a callback.  Then you say 
observe, with expressing 
interest in certain entry types,
 and     and as entries of that 
type become available, the call
back is invoked asynchronously. 
 There are many different entry 
types, long task is what we show
 in this example.  But this 
could just as well have been 
resource timing or navigation 
timing or paint timing, which is
 a new metric we have introduce
 ed.
   This also serves as a good 
example of long task usage.  You
 can basically use this code to 
understand response iveness 
issues on your app.  The call
back is called asynchronously 
when the main thread is observed
 to be busy for more than fifty 
milliseconds at a time.  Long 
task is available in Chrome 
table today, I encourage you to 
try it out.
   PHIL WALTON:  This table 
shows what our recommendation is
 for how you attract these 
metrics into your applications. 
 To reiterate, having those 
tracked in your plaqueses     
      plaques         
applications allows you to 
measure metrics on your real 
users, not just running it in 
the lab.
   First content of paint can be
 measured with performance 
observer          observer.  
Long task can be measured with 
performance observe er.  For 
hero elements it's tricky ier 
because you have to identify 
what hero elements are.  You 
have to write code to figure out
 when that is visible.  Along 
with this talk, I'll be 
publishing a article on develop
ers.Google.com/web, it will be 
up when the video goes up.  It 
goes into detail on how to do 
these things.  You don't have to
 worry if you are taking notes 
or whatever.  We are working on 
a native API to make this easier
, where you can annotate, tell 
the browse er what are the hero 
elements and the browse er will 
tell you when they are loaded or
 when they are rendered.
   For first meaningful paint, 
at this point before we develop 
a standardized metric, we think 
you should use hero element 
timing as a substitute for first
 meaningful            
meaningful paint.  The first 
meaning        meaningful paint 
metric
 is very generic.  We try to be 
one size fits all.  Hero 
elements is for your site.  It 
will always be more accurate 
than first meaningful paint.  TT
I, we released the Polyfill 
today, actually, for the TTI 
Polyfill it's on GitHub.  You 
can try it out right now.
   To give a   an example of 
what usage looks like, you 
import the module in JavaScript 
and call the get first 
consistently interactive method.
  That returns a promise and the
 promise resolves to the TTI 
metric value in milliseconds and
 once you have that, you can 
send it to Analytics.  To get a 
sense for what the Polyfill 
does, I should mention that the 
first, get first consistently 
interactive method takes a 
options object you configure it 
for your site.  You can pass it 
a lower bound.  The Polyfill 
will assume the lower bound by 
default Dom content loaded but 
you can give it a better metric 
for your site.  The way this 
works, you have the main thread 
with long tasks and short tasks.
  You have a network time line. 
 You have your lower bound which
 by default is Dom content 
loaded.
   It uses these resource timing
 and long tasks entries to 
search forward in time for a 
quiet window of five seconds, at
 least five seconds where there 
are no long tasks, and no more 
than two network requests.  It's
 saying once we get to that 
quiet window, we think the app 
is most likely interactive now. 
 It considers the moment of 
interactivity to be where the 
last long task was.
   That is a bit of how this 
Polyfill works.  You can pass it
 a custom lower bound for your 
site and one example of what you
 want to use is the hero element
 timing.  That is a great 
example        example.  You 
might want to pass the moment 
all of your event happened 
letters are added         
                   handlers are 
added because if they haven't 
been added yet the site is 
probably not interactive yet.
   SHUBHIE PANICKER:  Phil 
showed you how long task can 
push out time to interactive but
 there are lots of other 
interactions we are asking you 
to care about maybe on learning 
like clicks and taps.  Delays in
 these can cause pretty bad user
 experiences.
   You probably want to know 
when these important events are 
delayed.  Ideally there would be
 a first class platform API that
 would answer the question.  We 
are working on such an API.  But
 today you can actually use this
 code sample to understand the 
gap.  You can basically use the 
difference of event.time stamp 
and the current time in your 
event handler.  Voant         
Event.time stamp is our best 
guess of when the event was 
create ed.  This can be the 
hardware time stamp or when our 
best guess is when the tap, you 
tap the screen.
   This difference will tell you
 how long the nth event  
         event was spending wait
ing around on the queue for the 
main thread.  Here if that 
difference is more than a 
hundred milliseconds we send it 
to Analytics.
   We haven't shown this here 
but you can correlate this back 
to your long task observe er.  
You can look at what long task 
happened in this time and my 
event was blocked waiting.  
Those are likely the culprits.
   PHIL WALTON:  Once you have 
measured key metrics and sent 
them to Analytics services, you 
want a report to see how you are
 doing that will allow you to 
better answer the question, is 
your app fast.
   This is one example of a 
histogram that I threw together 
from TTI data for a app that I 
maintain, using the Polyfill 
that we just showed you.
   The point is not to look at 
the numbers or compare them, but
 the main point that I want to 
make, when you are tracking your
 performance metrics in your 
Analytics tool, you can drill 
down by any dimension that your 
Analytics tool provides.  In 
this case, we can see the 
difference between performance 
on desktop versus mobile.  You 
might want to consider the 
difference between one country 
from another country or 
geographic locations where 
network availability is not as 
great, or net work speed is not 
as high.  It's important to know
 how the difference manifests in
 the real world on real users.
   In cases where you can't show
 a whole histogram, I recommend 
using percentile data, so you 
can show the 50 percent, median 
number.  You can show the 75th 
percentile, 90th pest      
percent 50E8.       50E8        
      percentile.  It gives a 
better indication of what the 
distribution was and are better 
knowledge                  
better than averages or one 
single value.
   A important question is, do 
performance metrics correlate 
with business metrics?  If you 
are tracking your business 
metrics in a Analytics tool and 
your performance metrics in a 
Analytics tool and this shows 
the value of tracking the stuff 
on real users       users, you 
can see and answer this question
.  All the research we have done
 at Google suggests that good 
performance is good for business
, but the important thing is, is
 this true for your users, for 
your application.  Some example 
questions, do users who have 
faster interactivity times buy 
more stuff?  Do users who 
experience more long tasks 
during the checkout flow drop 
off at higher rates?  These are 
important questions.  Once you 
know the answers to the 
questions, you can make the 
business case for investing in 
performance.  I hear developers 
saying they want to invest in 
performance, but somebody at the
 company won't let them or won't
 prioritize it.  This is how you
 can make that a priority.
   Finally, we haven't talked 
about this yet, but you may have
 been one     wondering, all of 
the data we have been showing is
 for real users who made it to 
interactivity, and you know some
 users don't make it there.  
Some users get frustrated with 
the slow experience and they 
leave.  It's important to know 
when that happens, because if it
 happens 90 percent of the time 
the data that you have will not 
be very accurate.
   You can't know where the TTI 
value would have been for one of
 those users.  But you can 
measure how often this happens. 
 Perhaps more importantly, you 
can measure how long they stayed
 before they left.
   SHUBHIE PANICKER:  We have 
discussed a lot of specific 
metrics and APIs and we have 
shown you code samples.  Now we 
want to back up and provide 
higher level guidance on how to 
best leverage these metrics and 
APIs.
   One great thing about 
everything we introduce ed today
 is that all of these are user 
centric metrics and APIs.  By 
definition, improving these will
 improve your users' experience.
   The first piece of wisdom, 
all the traditional wisdom for 
fast loads applies here.  Remove
 those render blocking scripts 
from head.  Identify the minimum
 set of styles you need and 
in-line them in head.  You might
 have heard of the app shell 
pattern, that helps improve user
 perceived, perception, quickly 
render the header and any side 
bars.
   First paint and first content
 full paint are important but 
not sufficient.  It is important
 to improve your overall load 
time.  It's not just enough to 
be off to a good start in a race
.  It's important to make it 
past that finish line, and time 
to    to interactive is the 
finish line for loading, for 
interactive apps     apps.
   More specifically, minimize 
the time between first meaning
ful paint and time to 
interactive.  We saw in the 
Airbnb it was important for 
users to track to the search box
.  Identify what is the primary 
interaction for your users.  
Don't make assumptions.  Do they
 tend to browse or do they tend 
to interact with a certain 
element right away.  Figure out 
what is the critical JavaScript 
that is needed to empower that 
interaction and make the 
JavaScript available right away.
   One common culprit we have 
seen are large monolithic 
           monolithic JavaScript
 bundles.  Splitting up JS like 
code splitting would take you a 
long way here.  The purple 
pattern fix in here, 
specifically the first, the P 
and R of purple, ideally ship 
less JavaScript but if not at 
least defer the JavaScript.  
There is tons of JavaScript the 
user will never need, all the 
pages they will not visit, the 
features they are not going to 
interact with     with.  If 
there is a widgets         
widget in the folder they are 
unlikely to interact with, defer
 all of that JavaScript.
   The third thing we have is 
reduce long tasks.  Tracking 
down on the long tras,       
tasks will help responsiveness 
on your app overall.  If you 
need to prioritize, at least 
think about long task in the way
 of crate       critical 
interactions, long tasks that 
are pushing out time to 
interactive or long tasks that 
are in the way of check      
checkout          check      
checkout flow or other important
 interactions for your app.  
Scripts are by far the biggest 
culprits.  Breaking up scripts 
will certainly help.  It's not 
just about breaking up scripts 
on initial load.
   Scripts that load on single 
page app navigations, like going
 from the Facebook home page to 
the profile page, or clicking 
around like on the checkout for 
Amazon or the compose department
 in gMail       gMail all the 
JavaScripts needs to be broken 
up so it doesn't cause 
responsiveness issues.  The 
final thing we have for you 
today is holding third parties 
accountable.
   Ads and social widgets are 
known to cause the majority of 
long tasks.  They can undermine 
all your hard work on 
performance.  You might have 
done a ton of work to split out 
all your code carefully.  But 
you embed a social plug-in or an
 ad and they undo all of that 
work.  They get in the way of 
critical interaction            
 interactions.  To get an idea 
of this, we are doing a 
partnership with Zoasta, a major
 Analytics company.  They are 
doing case studies and there is 
preliminary data that came in.  
They picked a couple of their 
sites, their customers who had 
third party content, and the 
first site they found had 93 
percent of long tasks were 
because of ads.  On the second 
site, 62 percent of long tasks 
were about evenly split between 
ads and social widgets.
   Long tasks API actually gives
 you enough attribution to 
implicate these third party I 
frames.  We encourage you to use
 the long tasks API, find out 
what damage these third parties 
are doing on your apps.
   PHIL WALTON:  Once you have 
optimized your app, you want to 
make sure that you don't regress
 and go back to being slow.  You
 don't want to put work into 
this and have it all be for 
nothing, if one new release 
turns everything bad.  It is 
critical that you have a plan 
for preventing regression.
   This is a work flow that I 
promote.  You start off with 
writing code, implement a 
feature, fix a bug, improve the 
user experience.  Before you 
release it, you test in the lab.
  I assume lots of people do 
this.  You run it through 
lighthouse, through dev tools, 
make sure that it's not slower 
than your previous release.
   Once you release it to users,
 you are going to want to 
validate that it is fast for 
those users that you release it 
to.  You can't just test in one.
  These things complement each 
other.  You should be testing 
both in the lab and in the real 
world.
   For automation ideas, the 
best way to prevent regression 
is automate this process.  You 
will slack on it if you don't 
have it built into the release 
and automated.  Lighthouse runs 
on CI, and there is a talk 
tomorrow afternoon by Eric and 
Brendan that goes into how to do
 this.  I recommend checking 
that out if you want to learn to
 run lighthouse on CI.  Using 
Google Analytics you can set up 
custom alerts that trigger when 
some condition is met.  You can 
get an alert if suddenly the 
number of long tasks per user 
spikes, maybe a third party you 
are using changed their 
JavaScript file and things got 
worse and you didn't know.
   This is a good way of finding
 out that stuff.  Getting back 
to the original question, how 
fast is your web app, in this 
talk I hope we have given you a 
framework to think about 
performance and the big picture 
in a user centric way.  I hope 
we have given you enough 
specific tools, metrics and APIs
 that you need to answer this 
question for yourself.  We know 
the situation isn't perfects.  
We know we have more work to do.
  Shubhie is working on, leading
 efforts at Google on the 
standard side.  She can talk 
about things that are coming 
down the road.
   SHUBHIE PANICKER:  This is 
our final, last slide.  We know 
there are gaps and there are a 
number of APIs we are working on
.  We would love to have a first
 class API for hero element 
timing.  The idea is that you 
guys can annotate the elements 
that matter most for your sites,
 and the browse er can put those
 times on the performance time 
line.  Secondly we are working 
on improving long task mostly by
 improving attribution.  We want
 to tell you which scripts are 
causing problems and more 
detailed breakdown, so you can 
actually take action right away 
and fix those issues.
   Secondly, we want to have a 
API for input latency so you 
don't have to go through all 
those workarounds that we showed
 you for time stamp.  Ideally 
for your important interactions 
for your app, you should be able
 to know like how delayed they 
were, which long tasks were in 
the way, and when the next 
render happens.
   Then there are other inputs, 
that we haven't touched on, 
things like scrolling and 
composite an    animations.  I 
want to leave this, finally, 
with saying we said a lot today,
 but we wanted this to be a two-
way dialogue.  We want to hear 
from you.  We want to hear about
 your frustrations.  Don't be 
quiet about those gaps in 
measurement and your 
frustrations with performance   
         performance.  Try out 
these APIs and Polyfills.  
Please file bugs on the spec 
repos on GitHub.  This is 
actually the best way to report 
issues and make feature requests
.  If you are working with 
Analytics like whether it's a 
different team or third party
      party, push on your 
Analytics to adopt these new 
metrics.  Ask them for 
histograms, like Phil showed you
.
   We are pushing on Analytics 
too on our end, the chromium 
bugs on performance signal the 
use for priority          
prioritization internally and we
 need signals to make a case for
 working on measurement.  
Finally, as Phil said, we have 
all the links in the article 
that he will publish shortly.  
They will also be linked from 
the video.
   PHIL WALTON:  Thank you, and 
this is how you can get ahold of
 us.
     (applause)
.
   Far par these             
   These mailing lists are how 
you can submit feedback.
     (end of session at 3:10 
p.m. PT)                        

This text is being provided in a
 rough draft format. 
   Communication Access Realtime
 Translation (CART) is provided 
in
   order to facilitate 
communication accessibility and 
may not be a
   totally verbatim record of 
the proceedings.
   ***


.
   Gloos R       Google I/O 
2017.
   May 18, 2017.
   3:30 p.m. PT
.
   Stage 6.
   Building virtual reality on 
the web with WebVR.
   Session T254BE.
   Services Provided By:
        Caption First, Inc.
        P.O. Box 3066
        Monument, CO 80132
        800-825-5234
        www.captionfirst.com.
   ***                          

in a rough draft format. 
   Communication Access Realtime
 Translation (CART) is provided 
in
   order to facilitate 
communication accessibility and 
may not be a
   totally verbatim record of 
the proceedings.
   ***.


   May 18, 2017.
   3:30 p.m. PT.
   Stage 6.
   Building Virtual Reality on 
the Web with WebVR.
   Session T254BE.

   SPEAKER:  Welcome.  Please 
fill in the seats in the front 
of the room.  Thank you.
.

   Par Pa      
   Welcome.  Thank you for 
joining.  Our session will begin
 soon   
  soon.
   Welcome.  Thank you for 
joining.  Our session will begin
 soon 
    soon.
   SPEAKER:            
   MEGAN LINDSAY:  Welcome.  
Thanks for joining us this 
afternoon.  My name is Megan 
Lindsay, and I'm the product 
manager for WebVR at Google.  
Today, I'm here to talk to you 
about WebVR, I'll show you the 
opportunities that it opens up 
for web developers. , how this 
is going to benefit the VR 
ecosystem as a whole and what 
others are already doing with it
.
   Then, my colleague Brandon 
Jones will demonstrate how easy 
it is to build a cross device 
WebVR web by doing it right here
 on stage.
  WebVR enables web developers 
to build fantastic cross 
platform, cross device VR 
experiences.  Here is a quick 
overview of what WebVR is all 
about and a introduction to our 
recently released WebVR 
experiment site.
   SPEAKER:  VR should be 
accessible to everyone because 
it has potential to let everyone
 explore, play and create in 
amazing new ways.  But right now
 VR is complicated.  To make 
awesome         awesome VR stuff
 developers have to learn a new 
Lang weaj and               
language and spend time      
time to make that work on 
multiple headsets.  When we want
 to play with awesome VR stuff 
we have to have the right 
headset.  VR should be easier.  
Developers can make something 
quickly and share it with 
everyone, no matter what device 
they are on.  Like how easy it 
to       it is to share stuff on
 the web but with VR.  That is 
the idea behind WebVR it's
 VR on the web for everyone.  
Here is how it works.  You are 
in a browse er like Chrome.  You
 come across a WebVR experience.
  You tap the link.  Put on a 
headset and boom, you are in VR.
  Developers can build WebVR 
things the same way they build 
web things with JavaScript.  And
 since it    it all works in a 
browse er it's easy to make it 
work for all kinds of VR devices
, whether it's someone using 
their phone, their computer, or 
their entire room.
   Developers are already build
ing and sharing awesome stuff 
with WebVR.  We have started 
showcase ing their work on WebVR
 ex pair mtion               
experiments sites.  It gives you
 a glimpse into what is possible
         possible.  You can play
 games.  See the world in a new 
way.  Explore interactive 
stories.  Play with a friend or 
lots of friends.  Each 
experiment comes with open 
source code to help others make 
new experiments and developers 
can submit what they make.  All 
of this is an effort to make VR 
more accessible.  So anyone can 
build and everyone can play with
 awesome VR stuff.  Come and 
start playing at WebVR 
experiments.com.
   MEGAN LINDSAY:  I want to 
tell you why we at Google care 
about WebVR, and why we are 
investing in it.  As was said in
 the keynote yesterday, immerse
 ive computing is going to 
change how we play, work, live, 
and learn.  We are at the start 
of the next computing revolution
.  Many of you have seen the 
technology adoption curve before
.  2017 is shaping up to be a 
pivotal year for VR where it's 
moving beyond innovators to 
early adopters.
   This is a time of opportunity
.  However, one of the largest 
barriers to even broader VR 
adoption and more user 
engagement today is content.  
Content is absolutely critical 
to the success of any new 
ecosystem, giving users great, 
diverse and plentiful things to 
choose from will keep them 
coming back.
   I believe that the open web 
is exactly what virtual reality 
needs to take it to the next 
level and WebVR is the first 
step along that path.
   WebVR opens up VR to the 
largest developer platform in 
the world, you, web developers. 
 You can build for VR for the 
first time.  The web is a open 
ecosystem that we at Google 
strongly believe in and support.
  We are developers from around 
the world      world, work 
together to innovate in a Stan
      standardized interoperable
 way.  The web isn't controlled 
by any one company.  It's unique
 in providing access to contents
 from any device, through any 
web browse er.  There are no 
walled gardens here.  What this 
means is that WebVR 
simultaneously decreases the 
barrier to entry and extends the
 reach of your content.
   Using WebVR, you can start 
developing for VR with gradual 
investments, by progressively 
enhancing your existing websites
.  You can light up your site 
with VR when a immerse ive 
experience adds something 
special, from breaking 360 news 
on the ground to exploring your 
next home.
   With WebVR, you can build 
your experience just once, to 
reach all of your VR headsets 
and the mobile and desktop users
 giving you access to the broad
est audience possible.
   And you gain all the benefits
 of the web by making your 
content searchable, linkable, 
and low friction, with no 
installation required.  Sharing 
is as easy as a length.         
link.  The potential of VR goes 
well beyond gaming.  What kinds 
of VR content just makes sense 
to do with WebVR?
   Your imagination is the limit
, though I believe the first 
wave of WebVR content will be 
the use cases that are already 
first and best on the web.  
Ephemeral content found 
primarily through search and 
social media, short form media 
content and important but 
perhaps less frequent tasks 
where you may just not want to 
keep an app around.
   Let's take a look      look 
at some of the things that 
others are already doing with 
WebVR.  Matter port has create
 ed technology that allows 
capturing real world spaces in 
3D to view them virtually, for 
industries such as real estate, 
travel and hospitality, and 
architecture and engineering and
 construction.
   Matterport customers like 
Sotheby's, home visit and 
mansion global have stand nearly
                   have scanned 
half a million places and make 
these available to users with 
matter port's web player.  The 
web player let's       lets the 
user navigate through 3D virtual
 space on their phone or desktop
.  Before WebVR, users were 
required to download a separate 
app to view the full VR 
experience.  This create ed a 
lot of friction and resulted in 
significant user drop-off.  But 
now with WebVR, this friction 
has been limb nailted.         
      eliminated.  Users can 
step into the home they are look
ing add directly from the 
website.  When the user exits VR
, they are still on the original
 website, rather than in the 
separate app.
   Matter port supports WebVR 
for Daydream view and cardboard 
support is coming soon.  With 
over a million scenes create ed 
and posted by their community, 
Sketchfab is the world's largest
 platform to publish, share and 
discover 3D content on-line.
   With WebVR, any Sketchfab 
model can be viewed and 
manipulated in your VR headset. 
 Content create ors or 
enthusiasts can use Sketchfab to
 share or embed models anywhere 
on the web, enable ing them to 
be explored either on a 2D 
screen or in VR.
   Pow stir creates custom 
experiences for movies and music
 helping with discovery of major
 entertainment products.  With 
the rise of virtual reality, pow
ster used WebVR for the broadest
 audience reach and create ed 
experiences focused on movie 
websites         websites, show 
times and ticketing.  Here is a 
look at what pows     powste 
rhas done recently.
     (video played)
.
   Movie studios saw over five 
times more movie theaters 
selected inside VR than on the 
regular websites.  Audiences 
viewed the 3D trailer and the 3D
 gallery images an they 
converted to seeing the movies 
in 3D rather than in 2D in the 
actual these ears.             
theaters.  Finally from 
filmmaker Christopher Nolan 
comes the epic action thriller 
Dunkirk opening worldwide this 
July.  Here is a   a preview of 
what you will see in Dunkirk.
   SPEAKER:  On the beaches.
  Short fight on the landing 
grounds        grounds.  Fight 
in the fields.  and in the 
streets.  We shall never 
surrender.
  We shall never surrender.
  Dunkirk, rate ed PG   PG13, 
experience it in I-Max July 21.
   MEGAN LINDSAY:  Just as the 
film offers the first person 
perspective, Warner Brothers 
wanted technology to offer a 
deep immerse ive perspective on 
just what happened at Dunkirk.  
They brought this vision to life
 as one of the first collaborate
 ive VR experiences on the web, 
showing the depth of soldiers 
camaraderie through a 
cooperative experience between 
two people.  Working together to
 survive the evacuation, each 
player will become both the 
rescuer and the victim.
   Here is a taste of what's 
coming for experience dun     
Dunkirk.
     (individual know played 
                     video 
played)
.
   Experiences Dunkirk will be 
releasing in June and will be 
open to everyone, supporting 2D 
devices as well as VR headsets. 
 A tease er of the experience is
 live today.  Check it out.
   Other areas we are seeing 
particular interest in WebVR 
include news, e-commerce, 
interactive VR films, education,
 art, and custom business 
solutions.  We are eager to see 
what web developers use WebVR 
for next.
   WebVR content is arriving and
 WebVR browse er support is 
already here.  In Chrome for 
Android we have released WebVR 
support as a origin trial for 
Daydream view and Google 
Cardboard          Cardboard.  
Our friends at Mozilla, 
Microsoft, Oculus and Samsung 
have all released or announced 
coming support for WebVR, bring
ing it to Samsung gear VR 
windows mixed reality, Oculus 
rift and HTC Vive.
   In Chrome, we are continuing 
to improve and extend our WebVR 
support.  In our latest release 
of Chrome for Android currently 
in beta, we have significantly 
improved performance, making it 
more consistent and stable 
overall, and making it easier to
 reach target frame rates by 
adjusting rendering settings.
   We have also released WebVR 
support for Chrome custom tabs, 
enable ing you to enhance your 
native Android app with your 
WebVR content too.  Looking 
forward, we have support for 
desktop headsets in development.
   And we are bringing great 
WebVR content right to the 
Daydream home screen.  Stay 
tuned for more on this soon.
   We have talked about 
progressive enhancement of VR 
content, and how this is a super
 power unique to WebVR.  Let's 
dig deeper into what that 
actually means and how others 
have solved it.
   Weather.com recently released
 a interactive WebVR experience 
called the birth of a tornado.  
They applied progressive 
enhancement and responsive 
design principles to ensure 
their experience can be used on 
any device, by optimizing the 
interaction model for the device
 being used.
   On desktop, you drag with 
your mouse to change the 
viewpoint and click to interact.
  On tablet, you change the 
viewpoint by dragging with your 
finger, and tapping to interact.
  On mobile, the phone's 
accelerometer is used to provide
 a magic window into the VR 
experience. Cardboard, head 
movement is used to gaze and 
target and tapping the button to
 select.  And Daydream view uses
 the controller for interaction.
  Birth of a tornado also works 
with Samsung gear VR and the TTC
 Vive, the model used in birth 
of a tornado can be used with 
many WebVR experiences and we 
have a library to make this easy
, that Brandon will show you a 
bit later.
   This is just one way to 
support cross device experiences
.  Another example is dance 
tonight, some of you may have 
experienced the dance tonight 
project at IO yesterday evening,
 also built in WebVR.  Dance 
tonight is a ever changing VR 
experience made by LCD sound 
system and their fans.  It's 
made entirely from VR motion 
capture recordings of fans 
dancing to a new song by the 
band.  Another special thing 
about the project is that it 
works across devices, but 
playing to their individual 
strengths.
   On desktop and mobile, you 
get to be in the audience.  On
 Daydream, you are on stage.  In
 room scale, you are a performer
.  If you didn't catch this in 
person, it will be available 
on-line this summer.  Your input
 choices and supported devices 
may differ for your WebVR 
project, though we recommend 
starting with the goal of 
universal access as a best 
practice, and just see how far 
you can go.
   While WebVR content is still 
best experienced immerse ively 
in a VR headset, most people 
have still never tried immerse
 ive VR at all.  WebVR content 
will be their very first hint of
 what they are missing.  The 
great WebVR content that you 
create will be the reason a new 
user decides to pick up their 
very first VR head set.
   I hope you are as excited 
about WebVR's potential as I am.
  Now I'd like to introduce 
Brandon Jones, he started WebVR 
as his 20 percent project 
several years ago, and 
coauthored the spec.  Today, he 
is going to build a cross device
 WebVR app for you live on stage
.  Please welcome Brandon.
     (applause).
   BRANDON JONES:
  Thank you, Megan       Megan. 
 She talked about the principles
 of progressive enhancement, 
making pages that can be used on
 desktop and mobile devices as 
well as across multiple VR 
devices.  That can seem 
intimidating, but the right 
tools can make it simple.
   It starts with create ing 
great web GL content.  Web GL is
 a API for rendering 3D graphics
 to the browse er supported 
across all platforms today.  
There are many great web GL 
tools and frameworks to help you
 bring your ideas to life.  From
 there, turning your web GL page
 into a immerse ive WebVR 
experience can be as easy as add
ing a few lines of code.  To 
show you what we mean, we are 
going to build a quick WebVR 
experience on stage today.
   The app that we are going to 
be building will be a 360 photo 
viewer, which is great fit for 
WebVR.  These type of photos are
 easy to create with many 
cameras available to capture 
them and provide a fun 
experience that you don't get 
from traditional photos.
   Best of all they can easily 
be viewed in 2D and browse er 
with click and drag or magic 
window controls, while VR can 
optionally be used to provide a 
enhanced viewing experience for 
users with the right hardware.
   360-degree photos represent a
 class of content that is 
difficult to get users to 
install a native app for because
 of the content's simplicity, 
the overhead of a install is 
enough to discourage most users,
 given that they likely only 
expect to spend a few seconds 
looking at each image.  It's 
likely that most users would 
never get past the app store 
link.
   Ideally they can fluidly step
 into VR and out of VR with 
little overhead, view images 
quickly securely and move on 
without uninstalling anything 
afterwashedz.  This        
             afterwards.  This 
ephemeral experience WebVR 
excels at.  We are going to 
switch to the code.  and 
actually build the experience.
   Now, we are starting out here
 -- can we get the laptop up on 
screen?  Thank you.
   We are starting out with 
boilerplate code.  We are using 
3JS, not the only framework that
 you can use for create ing 
WebVR and web GL experiences but
 it is a common one.  There is a
 frameworks that are available 
that are expressly for WebVR 
content such as A-frame or react
 VR.  But because 3JS has wide 
developer acceptance already, we
 are using that today as our 
example.
   The boilerplate that we are 
starting with is fairly simple. 
 I'm not going to cover it in 
detail.  This is the type of 
thing that you would see in a 3J
S tutorial 0.
   What it produces for us is a 
black screen.  That is okay.  
That is a great starting point. 
 Thank you.  That was awesome.  
(chuckles).
   Because we are create ing a 
360 photo gallery viewer we need
 images.  Normally these would 
come from the database, CMS of 
some sort but we are going to 
hard code them in for the sake 
of example.  We need a way to 
view them.  Because it's a 360 
image, the buffer we are going 
to use is to create a gigantic 
sphere, in this case about 500 
meters in radius, invert it, 
that is what the viewer scale 
here is doing is inverting on 
the X axis, so all of the faces 
point inward.  Then keep the 
camera for a scene at the center
 of that.  That way when you 
look around, you are seeing this
 sphere all around you that is 
practically at infinity.
   Next we put our image on the 
inside of the sphere, using the 
3JS mesh basic material.  This 
loads up what is known as a 
texture in web GL.  We will 
apply it to the inside of that 
sphere.  It so happens that 3JS
's default coordinates systems 
work out well for rectangular 
images which is the default that
 most 360 cameras spit out.
   We need to combine the 
geometry and the material 
together into a 3JS mesh which 
is the basic primitive that it 
uses to render and add it to the
 scene.
   Once we have done that, we 
can switch back to the browse er
 and see that we now have a 
photo.  Unfortunately there is 
no interaction, so we can't tell
 that it's a 360 photo.  We will
 fix that by pulling in what is 
known as the WebVR Polyfill, a 
JavaScript implementation of the
 WebVR API that is targeted at 
mobile devices primarily.  It 
uses their accelerometers to 
provide basic head tracking used
 for a cardboard style 
experience.
   It happens to provide us also
 with a basic emulated click and
 drag mode on desktop that we 
can use to get basic 
functionality on the desktop 
computer.
   In order to make our 
application responsive to the 
WebVR Polyfill, we have to add a
 3JS extension called VR 
controls.  We will attach this 
to the camera and then this 
makes it so that any head motion
 that happens on your headset is
 automatically applied to the 
camera itself        itself, in 
order to make sure that it keeps
 updating with the head motion 
we also need to add a update 
function to the animation loop.
   Once those two elements are 
in place, we now have basic 
click and drag functionality, 
and we can see that we actually 
now have a complete photo viewer
 for a single photo.  But that 
is not tear      terribly 
interesting.  We want a gallery.
   The next thing we are going 
to do is provide a 2D version of
 the thumbnail gallery that 
allows us to switch between 
images.  We will loop through 
the image gallery that we had 
loaded previously, load a 
texture for each and pass them 
to this add to gallery 2D 
function that we are about to 
define.
   Here we are going to use a 
little basic html manipulation 
to create a container div, add a
 simple class to it.  I'll leave
 the CSS as a exercise for the 
viewer.  Append the image that 
is associated with our texture 
to that container element, and 
then add a click handler.  When 
we click on this thumbnail, we 
are going to swap out the 
texture on our gigantic viewer 
sphere with the texture that is 
associated with the thumbnail.  
This will give us the basics of 
iterate ing through the gallery.
  I should note, you can see 
that here, this is a terrible 
practice actually because 
normally you want to use smaller
 images for the thumbnail to not
 impact loading time.  But 
because we are doing everything 
locally here and for the sake of
 time, I'm skipping over that.
   You can see here that as we 
click on each of the images, we 
can cycle through the various 
items in the gallery, and they 
are all viewable.  At this point
 the 2D side is done.  We have 
done everything that we need to 
work both on desktop and mobile.
  But we are here for WebVR.  So
 let's figure out how we allow 
people to dive into VR from here
.  The next thing we are going 
to pull in is the utility called
 WebVR UI.  This is a library 
create ed by the Google create 
ive lab that provides a button 
that advertises WebVR support to
 your users.
   It will also communicate to 
them if they don't have WebVR 
support.  In order to add that 
to our application, we need to 
go in, create an instance of the
 WebVR UI button, append it to 
the Dom, and then in this case 
we are going to ask it, what VR 
device it's associated with and 
cache that off for later use.
   If we switch over here now, 
you can see that we now have the
 button that normally would tell
 us that we can go into VR but 
because we are on the desktop 
site, we can't actually go in 
yet.
   That is we don't have the 
hardware connected, so we can't 
go into VR here.  We would be 
able to on mobile.  I'll switch 
over there in a second.
   Even if we could go in 
because we have the correct 
hardware, we haven't actually 
wired up any of the VR rendering
 yet.  We will do that with 
another 3JS utility called VR 
effect.  This makes it so that 
the content that would normally 
go through the renderer and show
 up on screen will be rendered 
twice for stereo view, using the
 correct parameters that it's 
going to query from the WebVR 
API.  We need to update the 
animation loop, to make sure 
that we can properly handle when
 we are in VR mode versus nonVR 
mode.  We do this by asking the 
enter VRUI if the users click 
the butt own and if            
     button and if its present
ing and if so we enter the scene
 using VR effect.  Otherwise we 
use standard 3JS renderer.  The 
last thing is make sure our 
standard request animation frame
 is using a VR specific variant 
if it's available.
   This makes sure that if we 
are on a desktop device where 
the VR headset runs at a higher 
frame rate than your average 
monitor like 75 or 90-hertz, we 
are running at the same frame 
rate.  Otherwise the user will 
experience stuttering in VR and 
come away possibly sick.
   We will update that.  At this
 point, we will need to switch 
over to our Android device, to 
see the rest of the experience.
  All right.  Great.
   You can see on Android, we 
now have that nice magic window 
interaction mode where we are 
able to spin around and see the 
360 photo without going into VR 
at all.  This is great if you 
just want to showcase, one, that
 there is 360 content here, 
because the users' natural hand 
motion will give them a hint 
that there is more to see.  And 
two, it gives people a preview 
if they are sitting on a bus and
 don't necessarily want to 
blindfold themselves.  However, 
once we hit the enter VR button,
 we can now switch into VR mode 
and at the moment we are 
configured to use a cardboard 
device, and you can see that we 
would also get the nice stereo 
view.  These images aren't 
stereo       stereo, but you 
could render a stereo view of 
the scene.  And it would work 
correctly in any cardboard 
device.
   So, we have now create ed a 
basic WebVR enabled 360 image 
viewer.  But once again, in the 
VR side we have only done it for
 a single image.  That is not 
great especially when you are 
using mobile VR.  You don't want
 to force the user to put that 
phone into a headset and take it
 out repeatedly, in order to 
navigate between different 
elements in the scene.  Ideally 
we would like to take the 
gallery that we create ed here 
and pull it into 3D to allow the
 users to select between the 
different thumbnails there      
there.
  This gets more complicated 
than the Dom elements, because 
we are dealing with 3D.
   A lot more math is going to 
be involved.  But the basics are
 still simple.  We are going to 
be using spheres, like the 
larger viewer, to represent the 
individual thumbnails.  They are
 going to be much smaller this 
time.  Then down when we are 
looping through our gallery we 
are going to add items to the 3D
 gallery as well as the 2D.  
That looks like this, which now 
we start to get into a lot of 
math because it is 3D, trig is 
kind of par for the course.
   But skipping over the exact 
details of what we are doing 
here     here, once again we are
 create ing the texture to go 
along with the image.  We are 
associate ing it with each of 
our thumbnails spheres, and then
 the positioning code here puts 
them in a semicircle around the 
user's waist, somewhere that is 
nonobtrusive but easy to reach
.
   If we switch back over to the
 Android,
 we should now see, yeah, we 
have this nice semicircle of the
 same thumbnails, once again.  
This is cool, but that is 
probably not the experience that
 you want to leave your users 
with most of the time, because 
we are double ing up on the 
thumbnails in the nonVR version.
  So over in the code, we will 
just add one more line to say, 
if we are not in VR mode, let's 
hide the gallery.  It makes 
things a little cleaner.
   Next up, now we have got the 
number        thumbnails but 
don't have a way to interact 
with them.  We are going to add 
that by using a library called 
re input, the library that was 
used by the weather.com example 
that Megan was talking about 
earlier.  What it does is 
provide a single unified model 
for cardboard, Daydream or high
er end desktop experiences with 
controllers.  It gives you a 
cursor and a ray that are based 
off of whatever the user's 
capabilities are and uses that 
to cast into the scene, find a 
object and allow you to click on
 it.
   To start off with, went a 
little too far, we have to 
instantiate the ray input.  We 
provided the camera so it knows 
where we are looking.  We set 
size, that is a little 
bookkeeping            
bookkeeping.  Then add the 
meshes that are associated with 
that into the scene.  This is so
 that we can get the curse ers 
that are associated with it in 
the ray itself.  Like the VR 
controls we have to update this,
 every animation frame, to make 
sure it stays in sync with the 
controller movement in this case
.
   We want to make sure that we 
know when we are actually select
       selecting each of the 
thumbnails, so we are going to 
modify their opacity whenever 
the cursor is over the top of 
them.  We will start out with a 
lower opacity
.  Then we will have some event 
handling here that makes their 
opacity higher as the cursor 
hovers over them.  One bit that 
I skipped here, we do need to 
loop through all of the 
thumbnails in our gallery, and 
let ray input know that they are
 selectable.  Otherwise it will 
also try to select the larger 
sphere in the back and that 
doesn't do us any good.
   Finally, the last piece is 
that we say when we have clicked
 on whatever input, whatever the
 primary input is for our 
control mechanism, we are going 
to do the same thing that we did
 for the 2D gallery which is 
swap out the texture for that 
thumbnail for the larger sphere.
   Let's see how that looks on 
mobile.  Let's save it first.  
Then we will see how it looks on
 mobile.
   You can see, we no longer 
have the image spheres in 2D, 
but if we jump into the VR mode,
 and there we go, we now have a 
nice cursor that can swivel 
around and select the different 
spheres based on our gaze.  This
 is once again a cardboard mode.
  If we clicked on it, it 
switches the thumbnails.  Now we
 have a fully functioning 
gallery that the user does not 
have to leave VR for in order to
 switch through it    images.  
To demonstrate that this is 
actually, this works correctly 
with more complex input 
mechanisms as well, we are going
 to come out here -- that is not
 what I wanted.
  Sync this up with a Daydream 
device, and then
 normal part of the Daydream 
entry flow is that we have to 
sync up the controller, which 
takes just a moment, and then we
 should be back into our same 
experience, with now you can see
 a basic ray based selection 
cursor that does not depend on 
the movement of my head, but can
 still be used to do basic 
selection from the gallery.
     (applause).
   Thank you.
   That's it.  In 150 lines of 
code we create ed a experience 
that works on desktop, mobile, 
and VR, both cardboard VR, 
Daydream VR and if we were to 
put that on some of the larger 
desktop systems        systems, 
it would even work with a Oculus
 rift or a Vive.
   Let's switch back to the 
slides.
  To summarize some of the 
recommendations that we covered 
during the development, at this 
early stage we should be focus
ing on apps that can be used in 
2D with VR as a   a enhancement 
while the VR ecosystem is still 
growing        growing.  We 
should strive to allow users to 
stay in VR for as long as 
possible, as frequently 
switching between the 2D and VR 
modes can get tiring.
   Finally, there is a variety 
of input methods across all VR 
devices and using library like 
ray input helps normalize that 
into a single interaction model 
that is common between all modes
      modes.
   So now I'm going to turn it 
back over to Megan who is going 
to tell you more about the 
future of Chrome in VR.
     (applause)
.
   MEGAN LINDSAY:  Thank you, 
Brandon.  Everything up to this 
point has been about bringing VR
 to the web.  Now I want to talk
 about bringing the web to VR.  
Today, when you encounter a 
WebVR link, you drop your phone 
in the Daydream viewer and when 
you are done, you take the 
headset back off.  Soon, you 
won't have to take it back off 
to continue your browse        
browsing.  As we announced in 
the Daydream keynote this 
morning, we are bringing the 
full Chrome browse er and the 
entire web to VR for Daydream 
view first.
   You can use the Daydream 
controller to navigate regular 
web pages and follow links and 
for WebVR experiences, you get 
transported into fully immerse 
ive worlds.  You will be able to
 watch videos in a large screen 
theater-like experience.
   Plus, Chrome and VR is the 
same app that you use for 
browsing in 2D.  It sharys all 
of               shares all your
 tabs, bookmarks and history.  
You don't have to relog in to 
websites in VR.  Things just 
work.  VR browsing is coming to 
Chrome for Android later this 
year. what's next?  Take a look 
at our WebVR developer portal, 
with some great tutorials and 
case studies, and the helper 
libraries that Brandon showed us
 earlier.  Check out the full 
set of WebVR experiments on-line
, and consider submitting your 
own.  You can also fry     try 
out some of the WebVR 
experiments here in person at I/
O in the experiments area.
   Thank you so much for joining
 us today!  I can't wait to see 
what you come up with.
     (applause)
.
     (end of session at 4:09 
p.m. PT)
   Services Provided By:
        Caption First, Inc.
        P.O. Box 3066
        Monument, CO 80132
        800-825-5234
        www.captionfirst.com.
   ***                          

in a rough draft format. 
   Communication Access Realtime
 Translation (CART) is provided 
in
   order to facilitate 
communication accessibility and 
may not be a
   totally verbatim record of 
the proceedings.
***


   JUSTIN FAGNANI:  
So, it's really important that 
if you are going to measure in 
DevTools, here I have chosen the
 regular profile which gives us 
download speed, and then very, 
very importantly, I turned on CP
U, and this is important because
 your mobile devices can't do 
this near      nearly as fast as
 your laptop or desk top.  Once 
we have set up our test --
  are we okay?  Check, check.
  Yeah in thank you for fixing 
that.
   (Applause).
   JUSTIN FAGNANI:  Awesome.  
So, before we start diving into 
numbers, we need a comparison 
point here.  So I took the shop 
application and I de-optimize ed
 it a little bit and I made I 
had so it eagerly imports 
everything, this is a naive 
structure for building an app, 
it doesn't have any minute if I 
indication or service worker.  
So, I ran this new DevTools and 
I got numbers.  We have 5.9 
seconds on the initial visit and
 4 seconds on the reload.  So, 
we are going to apply some 
optimizations one by one and see
 how well they do givens the 
baseline.  The first one is 
minute any if I indication  
                          minute
 if I indication             
              M  MINIFICATION.  
It makes all files smaller.  And
 so the theme I'm going to go 
with here, we try to make all of
 these optimizations as ego I  
     easy as possible to apply. 
 You can turn this on for the 
different languages.  If we turn
 it on, we can see that we have 
brought our initial load time 
down to 4.4 seconds which is 25 
percent better, that's a big 
advantage, you should develop  
       definitely do that.  
Next, an optimization recall in 
pre-fetch links, this adds links
 to all the entry points and 
frag many times in your 
application.  And the benefit of
 this is that when you are about
 to load a view and it has a.  
About of depend        
dependency ies that need to be 
loaded, this tells it up front 
so it can download them all in 
parallel.  This reloses the 
trips                   
re           So, you can get 
some of the benefits of HTTP 
push, maybe your server doesn't 
pouter        support it   it.  
Again, this is easy to apply, 
you turn the insert free fetch 
links to true and we can measure
 the results and we have gone 
from 4.4 to 3.1 seconds, this is
 another 30 percent improvement.
  Easy to apply to your project.
   Next let's look at service 
worker pre-fetch.  So, service 
workers are a powerful tool, a 
background worker, a network 
proxy and a programmable cache 
     cache, with all that power 
comes little guidance and 
structure.  So, we auto generate
 one for you.  The one we 
generate does two things:  First
 it precautious all of the 
depend        dependency ies in 
the background, you get fast 
transitions, and second it makes
 it off line capable.  The way 
we do that, we do the same e
ntrypoint fall back on our Dev s
erve       server.  Instead of 
sending a 404 response, they 
were send the erp which allows 
the route er to boot up and 
handle that URL.  Again, we want
 to make this as easy as 
possible, so you just put add 
service worker true on your 
Polymer.json, if you do that, 
the first meaningful paint 
number didn't change, 3.1 to 
3.2, but look what happened to 
the paint, repaint visit.  .9 
seconds, which is a 65 percent 
improvement.  This is a benefit 
for some of your most important 
customers           customers.  
These are your repeat visits or 
signed on customer, people who 
like your app or installed it to
 the home screen and tapping on 
the icon.  Maybe faster than 
native app sometimes, actually.
   Okay.  Next let's take a look
 at lazy loading.  Lazy load 
     loading is not so much of a
 tool but a technique.  But I 
want to talk about it here 
because of how much work we put 
in our tools to support lazy 
loading.  You only inport what 
it needs to renter.  You        
      rent      render.  In a 
Polymer, we have a couple to do 
this, one of them is this 
function which adds a new HTML 
import to your document.  And 
lazy I am      imports, where 
you can declaratively describe 
the lazy structure of your 
application.  If we a ply       
apply this     this, we have 
brought down the meaningful 
paint number to 1.9 seconds, 
another 40 percent improvement 
over previous.  And the first 
meaningful paint on repeat is 
.9, lighted inning fast, that's 
good                            
          .9 lit inning fast, 
                    .9 lightning
 fast     fast, that's good.  We
 do smart bundling, it merges 
all of it is       it in one 
bunld and the more advanced 
tools like Webpack, well, ac
tually, there we go, I was 
promised a shout out.  Actually 
bunld things into multiple bun
ldzs if they                 
bundles.  Our buttoned is lazy 
-- depending on the lazy 
structure of the application.  
The way it works is by analyze 
ing the dependency graph here.  
We look for every file, what it 
is, the combination of entry 
points that require that file to
 load     load.  And then based 
on the unique set of erp 
entrypoints that we discovers, 
each one of those becomes a bu
nld.  Nowy woo can get         
           Now we can get a fine
 bunld       bundle        
bundle ing.  It's possible to 
create too many and have a 
negative impact on your 
performance.  What we have added
 on top of this is the idea of 
bunld strategy                
bundle strategies, the one we 
have included in the CLI has a 
-- where it says if any erp     
entrypoint is required by more 
than or any bundle is required 
by more than two entry points, i
t combines them     them, you  
   this ends up being a pretty 
good option.  We also make this 
incredibly easy to use by simply
 setting the bundle property to 
true in your Polymer.json.  We 
get our first meaningful paint 
number to 1.5 seconds which is r
eally fast on the 3G network.  
And we still keep that blazing 
fast, .9 repeat time.  All 
together we have had a 75 
reduction in the first meaning  
      meaningful paint time or 
four times faster all for 
setting a bunch of options to 
true.
   Next, I want to talk about 
explanation             
compliance            
compilation, wee this 
interesting situation where 
custom elements have to be ES6 
classes, on the one hand, they 
have to be on ES6, on the other 
hand, they can't be ES6.  Let me
 show you why this is true.  
When you write a custom element,
 you extend HT element        
element.  This is similar to a 
ray or p Ma.  And when         
        map.  You have to have a
 real construct torso the system
 can initialize the object 
properly.  When you compile a 
construct tore ES 5, you end up 
where you are calling the 
construct -- if you do this in a
 browse er that has this built 
in, it will throw in an error.  
Custom elements have to be ES6, 
but you can't run it on 11.  We 
try to    to take care of this 
in your tools, we recommend that
 everybody write their elements 
and distribute their elements as
 ES6, and then you only compile 
them with necessary at the 
application level.  And this is 
because the app is the place 
where you know what browse ers 
you need to support, what 
environment capabilities you are
 targeting.  So the app is the 
thing that knows what 
compilation needs to happen.  
The Polymer tools help you to do
 this because we are based on 
the -- graph of your project, we
 can compile all the JavaScript 
that is reach      reachable 
from your application.  So you 
can create two bills, and if you
 have a smart server, and ES5 to
 browse ers like 11.  But not 
everybody has a smart server.  
We made it possible to produce 
universal builds that you might 
deploy to a static file server, 
and we do this with something we
 call the ES5 adapter which 
patches up the environment on 
browse ers that natively support
 it and forces them to accept ES
5 and sub classes.  And again, 
we want to make this extremely 
easy to do, all you have to do 
in your build configuration is 
set compile to true for 
JavaScript           JavaScript.
  So, that's what bill does, it 
does all this stuff.  And even 
though we have tried to make 
each individual item here as 
easy as possible, to turn on or 
off    off, it's still a lot of 
things to understand, keeping 
your head into configure.  We 
want to make this easier.  So 
recently we introduced build 
presets.  We include what we 
think are the three most common 
and useful presets for you to 
use.  We have ES5 bundled for 
your older browse       browsers
, we have ES6 bundled for newer 
browse ers, but make your 
network or server doesn't 
support the push, and we have ES
6 unbuttoned willed for that 
full incremental serving.  So, 
this is what happens to your 
Polymer.json file, you can go 
from specifying each one of 
these individually to specify
        specifying the preset.  
Again, the theme here is we want
 to make this as easy as 
possible.  It shouldn't be 
difficult to build an incredibly
 fast app even for emerging 
markets.
   Okay.  So, that takes us 
through our whole development 
cycle from getting started all 
the way through building through
 production.
   Or does it is?  You can't 
just build for production, right
?  You have to actually put your
 app into production.  And so we
 are hadding a new              
 had     adding a new step which
 is targeting at deploying your 
app.  Now I'm happy to announce 
that we have a new initiative 
that we are calling purple in a 
Box.  Presm in a Box          
prsm in                 PRPL in 
a Box.  And use HTTP2 push 
depending on whether the b
rowse er supports it.  We are 
building out a configuration 
based on your CLI and we have 
initial versions for node that 
works well on app engine and 
also for Firebase.  The node 
version is the first one we 
releasing, it's in a preview 
state, but you can look at it 
now.  This works for node and 
app engine and you can look at 
it at the PRPL sever node. 
:  So   So, this server is cache
 and Edge cache friendly.  So 
they could be cache ed 
aggressively and it alsoing 
designed to work with HTTP/2 
push proxies.  It takes one 
server and if you specify 
certain 4EDers in       hetd
          header        headers 
in your response, it will 
automatically put it to push 
requests.  Now that brings us to
 covering the complete 
development cycle, all the way 
from getting started through 
editing and testing through 
production.
   Now, that's basically my talk
.  If there is anyone thing I 
want you to take away if this 
talk, Polymer tools make it easy
 to start, develop        
develop, and build with Web 
components APD Polymer so you 
end up with a fast off line 
capable progressive Web app.  
That does it for me, thank 
you    you, everyone, for coming
 out.
   (Applause).
   JUSTIN FAGNANI:  I'm going to
 be in the
 mobile Web sandbox, which I 
think is right over there, if 
anyone has any questions, I 
would lo to hear from you, you 
can find me on Twitter.  Thanks 
a lot.
   .
   .
   .
   .
   .
   .
   .
   .
     .
   .
   .
   .
   .
   .
   .
   .
   .
 .
   Minute if I indication



   .
   .
   .
   .
   .
   .
   .
     .
   .
   .
   .
   .
   .
   .
   .
   .

 .
   ,.
   .
   .
   .
   .
   .
   .
   .
   .
     .
   .
   .
   .
   .
   .
   .
   .
   .

 .
   5/18/17.
   5:30 p.m.,.
   Stage 6.
 level        
     V8, advanced JavaScript and
 the next performance frontier.
   Speaker:  Seth Thompson.
   Welcome, thank you for 
joining, our session will begin 
soon.

(music) .
   WebAssembly           

your seat.  Our session will 
begin soon.
   &amp;gt;&amp;gt; Hello, how is everybody, 
my name is Seth Thompson, I'm a 
product manager the V8 team.  
And Chrome.  So, V8 is an engine
, and it's the engine that runs 
JavaScript in Chrome       
Chrome.  And our mission is 
quiet simple.  We want to speed 
up real world performance for 
modern JavaScript.  And we want 
to enable developers to build a 
faster future Web.  There are 
two parts of this mission that 
are important:  The first is 
that the JavaScript that V8 is 
opened        optimize ed for is
 the JavaScript that you as 
developers are actually writing.
  And it's the JavaScript that 
includes new language features 
as they get introduced, new 
patterns of application 
development, no idioms.  And the
 second is that as we as an en
gine, you know, participate in 
the TC39 standards committee and
 develop toops       tools and 
give guidance, all of this goes 
towards a faster, future Web.  
So we will talk about more of 
all parts of that in a little 
bit.
   But first, I wanted to start 
with some financial statement  
                   fundamentals
es of a   JavaScript engine.  
Specifically V8 is a just in 
time compile er or a jit.  Now, 
what this means is that when 
JavaScript is sent to the browse
 er, the browse er has to 
execute this code as it runs it 
or immediately.  And in order to
 guaranty maximal performance, 
the engine wants to transform 
this code, this JavaScript, into
 machine code, native code.
   But because it's doing this 
all as soon as you load the Page
, it needs to do it just in time
 or at run time.  And there's 
some fundamental trade-offs  
          trade offs that play 
here.  One of the things I would
 like to do is provide more or 
should        shud some 
          she had         shed 
some more light on with we say 
run JavaScript fast.  There are 
a lot of different ways to run 
JavaScript.  In general, the 
more optimization an engine 
performance, the faster the 
machine code it generates.  So  
 So, the faster that code can 
potentially run, but the longer 
the initial delay.  Because 
remember, all of this 
compilation and optimization 
happens after you load the Page 
and the browse er sees the 
JavaScript for the first time.  
So there's a trade off there.  
The top peak speed, once the 
program starts running, versus 
the initial delay when it gets 
started or start up.
   And the second trade off is 
that in general, in a jit, the 
more optimizations              
 optimizations an engine 
performance, the more memory 
that the engine consumes.
   So, any time someone says 
that their engine is five times 
faster or five percent faster, 
you should think about what -- 
faster in what dimension or how 
does that number get translated 
into sort of a position in this 
problem space or trade off space
?  So
, let's examine this in a little
 more depth.
   Here are these constraints as
 I have laid them Outlook.  
Generally                     
out.  Generally an engine can 
have a fast start up or high 
peak performance.  And 
specifically an engine can make 
decisions about execute ing a 
particular function, with this 
granular         granularity.  
So, can immediately run it or it
 can make optimizations and run 
it faster, but pay the cost of 
making those oppedzs         
optimizations up front.  The 
second is that an engine can 
have a low memory footprint, you
 can think of an interpreter, 
which has a very low mep     
memory footprint, but that comes
 at a cost of the max speed as 
well.  So, memory and speed also
 are a trade off to make.
   So, let's say that I wrote a 
Web Page and all it did was run 
one line of JavaScript.  F  FOO.
  Now, we don't know exactly 
what foo is doing here     here.
  But I would be willing to bet 
that a JavaScript interpreter, 
which you might have a visceral 
sense of something slow, but I 
would be willing to bet that a 
JavaScript interpreter can 
execute one function much fast  
   faster than an optimizing 
JavaScript compiling           
compile er which takes the foo 
function, looks at it, turns it 
into native code, and then 
performance multiple aps path  
        optimizations paths over
 that before it can even execute
 it.  All of this is happening 
when you load a Page or start a 
JavaScript execute        
executing a JavaScript file.
   So, to put that into context 
on our little chart here, if you
 were only, if you knew that you
 were just execute ing one 
function, you would probably 
want to optimize for a fast 
start up, not peak performance, 
because you are only running 
this function once.  So the time
 that it takes to make it fast 
to run multiple times, you would
 have already paid the cost by 
running it once.  So, what if 
this exact same function, though
, is run 10,000 times?  Does the
 trade off change at all?  So, 
in other words, if we know that 
we have to make foo fast and 
it's going to be used 10,000 
times, then in this case, it's 
worth taking that initial start 
up delay to optimize our native 
code for foo, because we will 
amortize that cost of start up 
over the next 10,000 sexual 
abuses               sexual 
abuse                     10,000
 executions.  So in this case, 
for a code pattern like this, 
you want to optimize your foo 
for peak formulas, if this is a 
desk top browse er, you can rest
 assured that there's enough 
memory to compute lots of these 
optimization passes.
   But what if this exact same 
code is run on a low memory 
mobile device?  Let's say an 
android de-voice with       
        de-     device with low 
RAM.  Thean, taking          
       Then, taking the memory 
to compute multiple passes and 
generate a lot of machine code 
is going to be the difference 
between your device being under 
memory pressure and closing a 
bunch of background tabs.  In 
that case you multi-want to ak
                  might want to 
sacrifice the peak formulas 
performance for       
                   performance 
for a low memory JavaScript.  So
 you can keep more tabs open on 
a browse er.  I would argue on a
 mobile device, although you 
would like peak performance, you
 have this other con     
constraint which is you prefer 
low memory usage.  And finally, 
what if that same line of code 
is on a server? , in a file run 
by node JS?  In this case, your 
server only starts up once, and 
then it keeps running on 
whatever machine is receiving 
the request from the users.  So,
 in this case, you don't really 
care about the start up cost at 
all.  You would like the engine 
to take as long as it can to 
optimize this function.  Because
 once the engine or the node app
 is up and running, you want 
each of those requests to be 
served as fast as possible.  But
 again, if this is an IoT device
, maybe you are running node on 
something that's also memory 
constrained.  Well, here you 
might have to sacrifice some of 
that peak performance for a low 
memory footprint.
   So the reason I go through 
all of these exams is to say  
               examples is to 
say that the same function, 
single function of JavaScript 
requires many different types of
 optimizations or many optimal 
ways of execute ing this 
function, depending on the 
context, depending on the device
 it's running on, whether it's 
running on a  aserver or client 
side, how much memory there is.
   So, as an engine, or as 
Chrome developers, as we are 
developing V8, we want to put 
together an engine that spans 
that entire trade off space, and
 is able to use -- to know 
whether it is should be tuned 
for fast start up or peak 
performance or low memory.  So  
 So, over the last year, and 
actually even beyond that, for 
the past two to three years, V  
V8 has been working on an 
entirely new sexual abuse 
pipeline                       
execution pipeline.  The 
compilingers that V8 previously 
used had been completely 
replaced by a new execution 
pipeline.  Let me quickly walk 
you through the history to good 
give you a sense of how much 
machinery there is, how many 
moving parts there are.  In 
2008, V8 started with a simple 
code generate or, and generated 
semi optimize ed machine code.  
In 2010, we added an optimizing 
compile er.  Remember, that's 
the one that takes more time to 
start up because it's computing 
optimization passes, but then 
generates code that when run 
multiple times, is very fast.  
In 2015, we realized that our 
first optimizing compile er 
wasn't citizensable enough, it  
                       citizens 
extensible, and we knew we 
needed something for new p
atterns of JavaScript, like JS. 
 So we created a second 
optimizing one, and then we 
added an interpreter, I will 
talk about all of these things 
in a second.  And finally, we 
get to the present day where we 
have all from that first yellow 
compile er, and crank shaft, we 
just removed those from V8.  So 
today we have two parts of our 
engine, which are completely new
.  So, what are those parts     
 parts?
   Well, the first part of the 
all new V8 is turbo fan.  And 
TurboFan is an optimizing 
compile er:  I mentioned that it
 is designed to be able to 
squeeze out the most possible 
performance from the machine 
code that it generates.
   Now, it's also designed to be
 extensible from the beginning. 
 We were able to implement all 
of ES2015, which are the newest 
JavaScript features, and 
TurboFan, as well as follow on 
features, from ES2016 and ES
2017.  In TurboFan supports the 
entire language, so like try, 
catch, finally, can be optimize 
ed, to peak performance her 
historically they weren't.  What
 all of this means for you as a 
developer is that the new  V8 
has fewer performance cliffs.  
You are less likely to run a 
single function, have it be fast
, make a change, and suddenly 
wonder why it's slow.
   And that's because it 
supports the engine now it 
supports a more diverse settle 
of workloads                    
 set of workloads.  To recap, 
TurboFan is optimize ed for peak
 performance and multiple 
optimization passes even if it 
takes memory.
   But we have also added 
ignition because we know we need
 to serve these cases where the 
start up of the code     code, 
of the initial execution
 is fast, and there's a lower 
memory foot print.  So, the 
mission is an interpreter, and 
contrary to popular belief, it's
 not necessary ily slow if it's 
used at the right time.  
Ignition generates a bytecode, 
which it then runs and we notice
 that it's particularly 
beneficial for loading heavy 
pages fast.  And it's integrated
 with TurboFan to make it 
simpler.  This means that if we 
start execute ing a function 
with ignition, we can watch and 
see whether it's used often and 
use this to say that we should 
probably send that function to 
TurboFan and optimize it for max
 performance.
   So, the mission is optimize 
at this end of the spectrum.  
When we put these two things 
together, you end up with V8 
and, an all new V8 which can 
target the multiple places along
 the spectrum of low memory, hi 
memory, fast start U7    U7 up, 
peak performance, depending on 
the workload, depending on the, 
that weee      we see as we ex 
cute your code, depending on the
 disee advice, depending on 
whether it's embedded in node or
 Chrome.
   So this means that real world
 JavaScript is faster.  The 
engine can run in a much lower 
memory footprint.  Fewer 
performance cliffs, it's a more 
well-rounded engine and better 
tuned for node JS than our 
previous configuration.  
Finally, there is a third new 
part of V8, that's what we are 
call Orinoco, Orinoco as new, 
mostly parallel and concurrent 
compacting garbage collector.  A
 previous bar gaj collection was
                        garbage 
collection,            
collection -- to make for fast
     faster pauses when we are 
cleaning up the memory of an 
application.
   So, all of these things come 
together into this new package, 
but I think, and I have sort of 
mentioned many of the different 
dimensions on which you can 
compare the performance of 
JavaScript.  But I want to talk 
a bit about how we benchmark 
JavaScript or how we tell 
whether we are getting faster or
 not at real code.
   So, V8 has started measure
        measuring the 
performance of real Page loads. 
 We have a system which can 
record user actions.  So we can 
set up a benchmark that loads a 
Page, scrolls through the Page, 
potentially watches a video or 
reads a news article.  And all 
of these simulations we can run 
benchmarks against.  And we are 
happy that after optimizing for 
these real world Web pages, we 
saw 20 to 35 percent improvement
 on the speedometer benchmark 
depend       depending on the 
platform, over the course       
 course of the last year.  So 
what this means is that by 
optimizing real Web pages, we 
were able to deliver 
improvements on a benchmark 
speedometer.  But not all 
benchmarks are good.  In fact, 
if we could choose, we would 
always just run against real Web
 pages.  The reason that we use 
something like speedometer is 
because it runs in multiple 
browse ers so you can compare 
between engines.
   So here is the performance 
over the last year for 
speedometer.
   And one of the down sides of 
benchmarks is that, cues      
excuse me, the traditionerral 
benchmarks, Mott real world 
simulations but what you would 
run into a browse er tab to 
compare engines, is that they 
are not always -- of the types 
of JavaScript you are rieg      
writing.  In the beginning we 
talked about the four different 
ways you could write code, and 
the octane benchmark was tuned 
only to exercise the peak 
performance of a compiling   
          a compile er.  So we 
believe that chasing octane or 
optimizing in particular for 
octane, led engines down a path 
where they over optimize for 
peak performance and under 
optimize for things like low 
memory usage and start up.  So  
 So, this year we announced that
 we retired octane bawt we felt
              because we felt it
 wasn't yielding the right 
decisions for engine observes.  
And I mentioned              
              observe         
optimizations.  And it's 
something that better 
approximated real world websites
.  The reason it better a 
approximates them, it includes 
applications to do -- that 
implement the same Todo 
application across many 
frameworks, speedometer includes
 angular and react.  And what we
 have done is, we have worked 
with Web kit, who have 
originally implemented 
speedometer, to add even more 
frameworks.  So, I'm excited to 
announce that speedometer two 
has just been committed to the 
Web kit code base.  It expand 
the frameworks that it tests, 
now it tests angular two, and 
you         angular one, it adds
 Preact, inferno, 2015 code, it 
uses code with bundle ers, Web 
package, and it's update ed all 
of these frameworks to the 
latest version.  So, while no 
benchmark is a prefer 
approximation of real world code
, we hope that speedometer two 
will be a better way to compare 
engines across browse       
browsers.
   And those are the frameworks 
and the tools that are included 
in the bundle ers that are incl
uded.
   So, speedometer two is coming
 soon to Web kit, you will be 
able to find it on browse er 
bench dork.  So          .org.  
One of the things that I 
mentioned in that past section 
when I was talking about 
important parts of a performance
 storage is ES   ES2015.  ES2015
 is, and the newer features are 
the latest version of JavaScript
.  ES2015 features things like 
promises, rest, and spread 
operators.  Array iteration 
becomes a lot easier with ES
2015.  And when ES2015 was 
initially implemented, there was
 a slow down on the ES2015 code.
  This is because engines take a
 long time to optimize 
particular code patterns to make
 them fast.  So, ES2015, when it
 was first introduced, it was 
actually a lot slower than ES5 
code.  Over the last year, we 
have been
 using a tool called 6 speed, 
which compares ES2015 code to 
the trans       trans piled 
version or the equivalence of 
the same action.  So it's 
compared to an anonymous version
.  We have been using this tool 
to optimize the biggest 
performance difference          
 differences between ES2015 and 
the trans piled equivalent.  So 
we worked on optimizing
 for of, we worked on improving
 objects assign.  It shows up 
everywhere in react and -- code 
especially.  We worked on 
improving iteration and -- we 
also improved the performance of
 spread calls.  And by doing 
this, we decreased drastically 
the slow down of ES2015 trans pi
led ES5 compared to ES5.  And 
you can see here that over the 
past roughly six months, we went
 from the average ES   ES2015 
code being almost three times 
slower than ES5 code, to the 
present, Cher we have almost 
                    where we 
almost reached parity.  There 
are fewer and fewer reasons not 
to use E.  S2015 code.  When 
your user supported node on the 
server.
   And we also, I wanted to 
highlight a couple language 
features in particular, which 
got special attention because 
they are so useful but there was
 a lot of performance to be on 
the table, to be add     had.  
So, generators are now two and a
 half times faster, and aink and
          a       async and a  
await, for turns       turning 
the code into something that 
looks more sin cron          
synchronous, four and a half 
times faster than it was in the 
previous six months.
   (Applause).
   &amp;gt;&amp;gt; SETH THOMPSON:  It is a 
big deal.  Yeah.  Now, 
underlying all of this is our 
promised implementation.  For a 
while native promises were 
actually slower than promises 
that came from a library like 
blue bird.  While I'm also happy
 to announce over the past year 
we have improved          
improved the speed by four times
.  So, native promises
 are now something that are able
 to be included in real world 
code without worrying about the 
performance impact.
   So, I have talk a lot about 
language features and the 
different places that you can 
run JavaScript.  And one of 
those environments is node J: 
        Node.js.  It's a server 
side language and it node embeds
 the V8 engine.  So, over the 
past year, we have actually 
invested a lot more in the node 
community than we ever had 
previously.  V8 is now 
represented on the node core 
technical committee and you can 
find us on get had you been  
            hub on the node 
repository working through 
issues that come up under the V8
 engine label.  These are things
 like regression, somebody 
notices that their node code 
slowed down for some reason.  We
 are working with the node team 
on releases and making sure that
 as soon as the new version of V
8 is available, it can get up 
streamed into it as fast as 
possible and tested for release.
   And we have also worked on 
performance optimizations 
specifically for node.  So, in 
addition to exposing JavaScript 
features, node also has a rather
 large standard library.  And 
some of the, those some of the A
PI's, things like buforts, we 
have had                      bu
ffers, we have to do specific 
observe         optimizations 
for.  We worked a on supporting 
long -- and in general, we made 
sure that it is as fast as their
 bar equivalent.  We also notice
 ed that certain libraries are 
used in node throughout the 
ecosystem.  And the through 
library, which is used for 
creating streams primitive.  We 
also spent time optimizing to 
make sure streams and node are 
fast.  All of this is summarized
 by what we call an Acme air 
benchmark.  It starts up a node 
server and tests sending 
thousands of requests, it's a 
big app.  That showed a so 
percent improvement when we 
around ched TurboFan and 
ignition.  Goes to show that the
 improvements that we made 
toward making our engine more 
well-rounded did in fact yield 
faster node performance as well.
  We are really excited about 
that.
   And the V8 is part of Chrome.
  When you are de-    de-bugging
 JavaScript in Chrome, you can 
use DevTools, so over the past 
year and a half or two, we have 
been working on making DevTools 
support node     .JS.  I'm going
 to                  node.JS.  
I'm going to
 show a demo of where we are 
right now.
   So, traditionally when you 
are writing a node application
            application, it's 
difficult to de-bug things, not 
quite as simple as it is -- side
 when you can pop open the 
inspector          inspector.  
So, if for        for this demo,
 I would like to go through a 
node command line interface 
called emerge.  It's a cool 
program, an open source program,
 I just found it, where you run 
it add type something, any 
sentence, say hello, it comes 
back with a bunch ofee moaj  
         of,ee moaj I          ,
 EMOJI's.  I want to find out 
exactly how it's implemented.  
To de-bug any node application 
with DevTools         DevTools, 
all you have to do is pass a 
flag to node -- inspect.  And 
what this does is, opens up a 
port, a de-bug port that can 
communicate with the DevTools, 
and you can de-    de-bug in Dev
 tool.  Now, previously you had 
to paste this relatively long 
URL into Chrome in order for 
this to work.  But now if you go
 to Chrome inspect or about:  
     about:inspect, you can see 
that Chrome will automatically 
detect any running node 
instances on your computer.
   Yes!
   (Applause).
   &amp;gt;&amp;gt; So, let's go ahead right 
there and click inspect.  We get
 a dedicated window that opens 
up and here we can see our CLI. 
 If I look into my files into my
 sources, I feel      see that 
file that I just executed and 
it's right here.  What's really 
powerful about this is, I can 
come in and use any of the de-
bugging features that are, that 
have been introduced in the last
 six or so months.  One of those
 that's really powerful is in
line break points.  So, here, 
what I'm going to do is, I'm 
going to set a break point on 
this line of code.  This is the 
code that fetches those EMOJI's 
that the server that it's using 
to --   .  -- and what I really 
want to do is I want to know 
what the server is returning.  I
 think it's returning an array 
right around here in the middle 
of the line.  If I normally 
would just break on this line, 
it would break at the beginning 
of the line, and if I advanced, 
it would have completed the 
fetch already.
   What I really want to do is 
break right here.  So inline 
break points, which is a really 
cool features          feature o
f DevTools, I can do just that. 
 Now, I can also use this 
feature
 to de-bug node code.  With that
 break point in place      
place, let me try this again.  I
 will run hello, and I get 
something back, I get pause and 
a break point.  I want to show a
 couple new DevTools features.  
One of them is that this cost --
 is -- supports a sink cron nows
                sink Ron nis 
code execution.  It traces 
process through a variety of 
functions that can trace promise
s being resolved.  I want to 
come down to the scope here and 
I can see that this array 
variable, the array that it 
returns is actually 10EMOJI's 
long, now I can see what this is
 doing is slicing that array and
 only returning the top 7 
results.  Relatively easily, 
just bypassing the inspect flag,
 and then opening up Chrome and 
clicking connect to node, I can 
jump into the execution of a 
node program and use all of the 
DevTools features to de-bug it. 
 I was interested in the de-bug
ger today, but the JavaScript CP
U profile er is available for 
node     node.  The memory 
profile er is available, and the
 Console is available.  Yes, 
this is node, it's running this 
version of V  V8.  So, all of 
this is immensely useful.  It's 
just one of the new features 
that we have for de-bugging node
 with Dev tool.  I will briefly 
say a we also have a new 
dedicated won for node.  You can
 actually close roam, open this 
node de-bug irrelevant, add in 
the                       ger 
and it will always stay 
connected to whatever node 
instance, even if you are 
running multiple scripts at the 
same time.  Thea        Yes    
Yes, that's an exciting step.
   (Applause).
   &amp;gt;&amp;gt; SETH THOMPSON:  Let's go 
back on the slides briefly.  In 
addition to these inline break 
points that I just talked about,
 and this integration of node 
and DevTools, the V8 team has 
work with the DevTools team to 
support a number of really use 
   useful features for writing 
JavaScript applications.  So, 
one of the themes of Google I/  
 I/O this year on the Web track 
has been optimizing the 
performance of progressive Web 
app in JavaScript by simply 
shipping less code.  If you are 
using a buttoned          bundle
 er and you require many module 
from MPM, it's very easy to end 
up in a situation where your app
 bundles ships across the 
network to the client, parses 
and starts up way more 
JavaScript than you actually 
need, it's just simply how the 
bundle er included an entire 
library, even if you only used 
one function from it.
   So, I would like to do 
another demo here and show how 
DevTools can help you find the 
situation and fix it, if you are
 working on an app with lots of 
depends sis             
dependency ies.  I have got a 
brief application here and I'm 
going to serve it on a little 
server that watches my code, 
sends it to browse er, re   
recompiles it, basic stuff.  But
 I can show you that I'm 
requiring something, I'm require
 Lodash as a library and     and
 a bunch of other dependency ies
.  Here is the app    app, a
 GitHub repo for malter.  If  
           malt               re
formatter.  Let's say I want to 
look at the performance of this.
  Now, browse er, if I turned my
 JavaScript into a single file, 
a bundle here, and it's actually
 quite large, it's about 6,000 
lines.  Now, I could go and 
manually figure out exactly 
which part of the 6,000 lines I 
actually used.  But instead, I'm
 going to use a feature called 
DevTools coverage.
  Now, you might be familiar 
with coverage from test coverage
.  You run a coverage tool to 
figure out whether you have test
ed all the parts of your code.
   This type of coverage tool is
 a little bit different.  
Instead it tests, of the 
JavaScript that the browse  
      browse er saw, how much of
 it was actually executed?  How 
much of it was dead weight or 
dependency ies that weren't used
 in if we go under, you can 
press escape to bring up the 
drawer, and we go under the 
coverage tab here, this is new 
   new, you have to do that in 
Chrome canary.  If it's not down
 here, it will be under more 
tools.,  then coverage.  This 
panel allows us to load our Page
 and check which JavaScript is 
actually executed.  We have to 
hit record and refresh, you can 
type in something, and now you 
can see here that we have, if we
 isolate just the bundle file, 
you can see that we have only 
used about a third of the 
JavaScript that we are shipping.
  What more, this tool allows 
you to look at the source and 
see which functions were called 
and which weren't.  So although 
there is some green here, which 
is code that we ran, there's 
also a lot of red.  And actually
 know exactly what it is.  All 
of this red code is, are 
function         functions from 
the Lodash library that I didn't
 use.  Because actually I'm only
 using one function.  This is 
the function that's actually 
turning the -- hello, Google I  
I/O into what they call ca Bob 
case.  I'm only using one under 
score function here.  I happen 
to know that actually I can, 
rather than loading the entire 
Lodash library, I can load just 
KABOB case, I can trim my 
dependency ies down to exactly 
what I need.  And in this case, 
when I save it, my process will 
recompile my bundle.  If we re
record and re   reload, we can 
see that
 now our bundle is only 1,000 
lines rather than 6,000.  And 
the percentage of bites loaded, 
actually, I think I need to 
clear this first, okay.  The 
percentage is now much, much 
smaller of code that's not 
actually being executed.
   So this was a trivial toy 
example, but you can imagine 
running this on a very big code 
base where it's not easy to know
 which dependency ies are used 
not and which parts are used.  
This will help you make sure 
that anything you are shipping 
to a client for a particular 
Page or route, is just what's 
needed to execute the 
functionality that you need.
   So, that is code coverage in 
DevTools.
   Let's go back to the slides 
      slides.
   If you are familiar with the 
performance panel in DevTools, 
the performance panel is another
 way to instrument and 
investigate the performance of 
your app.  You can go to the 
DevTools documentation and find 
a number of really good 
tutorials for using the DevTools
 performance panel.  A couple 
other things we have added or 
include line by line profiling. 
 I think that was the first
 of these features.  And if you,
 in addition             in 
addition to seeing the coverage,
 you can record a performance 
profile, look back at sources, 
and in the gutter see the time 
that each function took to 
execute.  You can use that to 
identify a particular bottle 
neck in your source.
   We have got code coverage, 
which was just launched and a  
asink de-bugging, which has 
gotten a lot simpler recently.  
So, V8 and DevTools are 
constantly working to make sure 
that de-bugging and making fast 
applications in the fares place 
is easier and easier.  There's 
one last thing that I wanted to 
touch on.  This is mainly a talk
 about JavaScript.  But 
WebAssembly is a really exciting
 new technology that V  V8 has 
recently added support for.
   So WebAssembly, if you are 
not familiar with it, is a new 
language for the Web.  And 
specifically it's a low level 
language designed to execute 
near native code.  So, 
WebAssembly is ideally suited 
for a library that you might 
otherwise write in C, if you 
want a different environment 
than the Web.  For the first 
time you can use WebAssembly to 
compile C and C plus programs, 
and run applications like desk 
top swiewt applications   
                  swiewt like 
graphic intensive game or an 
editor that previously was 
constrained by the performance 
of a dynamic language lieb 
JavaScript.  So, we are excited 
to announce that V8 is 
supportive in Chrome, and I 
think one of the most amaze ing 
parts about WebAssembly is that 
it's not just a Chrome 
technology.  In fact, when I see
 this slide, I think the thing 
I'm most excited about are the 
other browse ers here.  So 
WebAssembly is also launched in 
FireFox and currently in preview
 in technical builds of Edge and
 Web kit.  So, WebAssembly is 
poised to become a cross browse
 er solution for running native 
code.  It's the first new 
language that we have used on a 
Web that has this sort of cross 
browse er support.
   It doesn't require any plug  
   plug-ins, and it uses the 
regular platform API's that you 
are all familiar with.  We 
launched it in Chrome 57.  As I 
mentioned, you can compile C and
 crus plus plus to Web a sem pli
.                WebAssembly.  
Already we are seeing incredible
 demos.  This is the demo of the
 engine running in a Web browse 
er.  We also have in the 
WebAssembly website a unity game
 as well as a bunch of community
 projects that have already 
started being create ed.  Just 
the other day I saw a video 
editor running with realtime 
effects, 30 frames per second.  
So if you are interested in 
learn more about WebAssembly, I 
encourage you to check out the I
O recording of a talk, I believe
 happened yesterday by Alex.
   And soon for WebAssembly, we 
will have more start up 
performance oh, my goodnesses
                   oh, my 
goodness                     
           optimizations and 
more advanced native code 
features.
   So, to summarize, or to pull 
this all together, if there's 
one thing to take away from this
 talk, it's that the V8 engine, 
and really JavaScript engines in
 general, need to be 
well-rounded engines.  They need
 to be able to run lots of 
different types of code fast, 
and lots of different 
environments.  And the 
constraints of those different 
virnlts might               
environments might change the 
types of code needed to run.  So
 V8 today with the ignition 
interpreter and the TurboFan 
optimizing compile er is well 
equipped to run code at bodes 
ends of these                 
    both ends of these spectrums
.  I think with WebAssembly, 
this diagram can be expanded
 a bit.  WebAssembly works along
 side JavaScript and will allow 
developers to push that peak 
performance angle even 
farther        farther.  So, 
WebAssembly just expand the 
possibilities that a JavaScript 
or an engine can run code in a 
browse er.
   So, that's my talk for today.
  If you are interested in
 some of the things you heard 
today, and care about the types 
of optimizations we are doing or
 even the language features that
 JavaScript offers, I encourage 
you to go take this survey.  We 
put it up at
 this website.  This gives you a
 bunch of up coming features or 
proposals and asks you to rate 
them, how exciting they are to 
you.  This will help us decide 
what      when we come on stage 
next year to talk about.  Thank 
you very much, my name is Seth 
Thompson.
   (Applause).
   .
   .
   .
   .
   .
   .
   .
   .
     .
   .
   .
   .
   .
   .
   .
   .
   .

 5/18/17
.
   Google I/O
.
   Women Techmakers.com-a 
progressive Web app my brakes. 
           may graix           
migration.
   Speakers:  Jeffery Posnick 
and Ewa Gasperowicz.
  V8     
 cautious          cawsh       
caching 

 payload

 PRPL     
.
   (music) .
   (Applause).
   &amp;gt;&amp;gt; Hello, everyone.  My name 
is Ewa Gasperowicz, I'm a 
program year engineer here at 
Google.
   JEFFREY POSNICK:  I am also 
on the team at Google.
   EWA GASPEROWICZ:  Jeff is one
 of the contribute ors to the 
library.  Today we are going to
 share with you our story about 
transforming
 Techmakers           
Techmakers.com into a 
progressive website.  Before I 
get to that it, let's meet what 
brought the two of us together 
on the project.  Some time ago I
 was talking with a friend of 
mine at work, I knew she was 
involved with the women 
Techmakers many pra, I asked her
 how it was going.  She told me 
with a lot of enthusiasm in her 
voice, Eva    ee Ewa    Ewa, 
it's going great.  It's growing 
and it's amaze ing because so 
it's community based, so much 
going on, but actually sometimes
 it's also challenge ing, it's 
hard to keep everyone in the 
loop.  Given that everyone is --
 these days     days, we are 
thinking whether we should have 
an app for it.  And it may     
made me think if an app is the 
way to go, how about a 
progressive Web app.  It also 
made my personally curious.  I 
knew quite well by then how to 
write a progressive Web app from
 scratch.  But transform  
        transforming an existing
 life site is an entirely 
different story.  I know some of
 you are bothered about it as 
well, you approached me during 
the conference and asked about 
it, so I'm very happy that I 
will be able today to share this
 story with you.  Lucky for me, 
this is also where Jeff comes 
into the fict.  He was working 
on a word books library, when he
 learned about my migration 
plans he approached me and he 
said listen, Ewa, me and my team
 are working on a set of tools 
that would make my -- of course,
 I said yes.  And that's how me 
and women Techmakers became a --
 for the Workbox project.  So, 
that's what we settled on.  We 
settled that women Techmakers 
will get the progressive Web up,
 that Jeff would provide us with
 tools and I will get my 
Discovery process about how it 
feels to migrate a website that 
I can share with you today.  Our
 intention is to give you some 
insight and in   in -- so that 
you can attempt migrations in 
your own projects in future with
 confidence.  Okay.  Let's start
.
   First I would like us to look
 into the decision process 
behind how we decide ed to go 
for a progressive Web app.  But 
in order for you to understand 
our decisions, you need to know 
more about women Techmakers.  
Women Techmakers is a Google 
program for women in technology.
  It is actually a great 
pleasure to talk about it here 
at I/O, because this is where 
all of this started.  Six years 
ago here at I/O the first tech 
maker took place, an event to 
bring support in the community 
to women who tended        
attended the conference.  I'm 
very happy to say that the 
support, results, and our number
 grew, in 2013, there were only 
8 percent women at the 
conference and this year, we aim
 to have 25 percent of us attend
ing.  And I'm happy to say that 
we made it.
   (Applause).
   EWA GASPEROWICZ:  Since then,
 women tic makers have grown 
from a humble once per year 
event to hosting hundreds of 
event for over 70,000 women 
world wide.  There is a program 
for college students and 
membership to help women get the
 support they need and many more
 initiatives.  It's now part of 
the global movement for women in
 technology.  It's not focused 
in silicone valley or only in 
the US, it's for women in over 
160 countries, that's a lot of 
diverse environments.  Because 
of that, women tech      
Techmakers get creative at how 
they can support such growth and
 scale.
   When women Techmakers was 
entering new markets, it also 
started targeting new audience  
       audiences.  And this 
audience is often meant more 
users and mobile devices and 
internet connections.  The -- 
they wanted to make sure they 
had the infrastructure that 
supports this diverse 
environment.  And it seemed that
 a progressive Web app might be 
just what they need.
   A progressive Web app is a 
Web application that uses more 
than network capabilities to 
deliver up like experience to 
the users.  It should be in 
particular ready able,        
     reliable, which means load 
instantly even under certain cbs
, it should be fast      
                  conditions, it
 should be fast, on all kinds of
 devices and engage ing, which 
means it should feel like a -- 
providing a user experience.  
You can see how those features 
corresponded to the women 
Techmakers goals.  By I can Ma 
can the                       by
 making the site fast,  -- and 
on slow internet connections and
 by making the app always just 
one click away for the user on 
the mobile phones, it would help
 to keep users engaged and 
involved even more in the 
community.  It would allow us to
 improve the experience and stay
 frugal at the same time, we 
could keep the current 
infrastructure, have a single 
code pace base and support only 
one plom going forward.  That
platform going forward.  That 
was the decision.
   So how did we do it?  How did
 we approach our my brakes   
        migration?  Well, this 
is the process we followed if.  
First of all, we needed to 
understand the current state of 
our site.  In order to do that, 
we used lighthouse, how many of 
you know what lighthouse is?  Or
 use it?  Yeah, you have been to
 the mobile Web tank, hopefully.
  Well, lighthouse as Chrome 
extension that all of you can 
install in your browse er that 
allows you to measure how close 
your Web app is to a progressive
 Web app.  When you run your 
website through lighthouse, it 
gives you back a report and a 
score that summarizes the State 
of the app for you.  When we 
started the process, it was 
about 45, the score, and our 
goal was to get to as close to a
 hundred as possible without 
changing how the site looked or 
worked in general.  That's why 
we called it migration, we 
didn't want to alter the site 
too much, we just wanted to make
 it smoother in different 
environments.  If you are 
curious about your own scores, 
you can try out the lighthouse 
in the Web tent.  If it goes 
green, it means you have an a  
amaze ing, amaze ing website.
   So, here is how our 
lighthouse report loobd like.  
You see the skull there.  You 
also see a list of features or 
traits that the website should 
have in order to be considered 
progressive.
   The green ones means we are 
doing well on those and the red 
ones means there's area for 
improvement.  One cool thing 
about this list is that actually
 each of those is an ix panted  
         expandable section so 
you can click on it and see more
 information, and also find 
links that give you resources 
that tell you how to implement 
that feature or get better on it
.
   And this makes it super cool 
tool for people that are less 
experienced with progressive Web
 Apps.  Because as a matter of 
fact, you don't even need to 
know what features you should 
implement in order to get 
progressive.  Lighthouse will 
just simply tell you.
   As you can see, our 
lighthouse was 45 and 
represented with a list of 
options.  So what do we do next?
  Well, we needed to priority 
advertise                       
pry or advertise.  We didn't 
have                   
            prioritize.  We 
needed to make sure we focus on 
the most important things first.
  This is why I put the women 
Techmakers logo there.  You 
bring it back to an organization
 or to business people.  Because
 those are folks that know best 
what's good for the users.
   And important thing to 
remember about the progressive 
Web app is that it's not -- 
technology that you just drop on
 your site.  It's actually a set
 of modular solutions that you 
can usually implement 
independent from each other.  So
 you can take that list and just
 pick and choose and decide what
 would make the biggest 
difference first, and then add 
on to it.  Don't need to be 
scared, you can start with 
smaller changes and take it from
 there.
   In our case, we decide ed to 
focus on two areas, off line and
 on-line experience, off line 
obviously would improve the 
performance on low internet 
connections, and that would 
improve how the app works not 
only for the users in the 
countries where the connection 
is scarce, but also the 
environment like some 
beneficiary you probably 
encountered this problem where 
you can't asks the website and 
women tech maker is even a 
driven initiative, so it was 
important to add that.  
Secondly, we decide ed to add an
 up like experience, which means
 you can now install the home 
screen of your mobile device.  
We hope this will drive engage 
the            engagement with 
the user     user, easier to 
keep everyone in the loop.
   Once we have our prioritize  
          priorities           
priorities, we needed to get to 
know our tools.  In just a 
moment, Jeff will work -- will 
take you through all the ins and
 outs of the Workbox library and
 show you how easy the library 
our migration became.  When Jeff
 is done, I will tell you how we
 actually implement it, the 
whole process.
   I'm going to share some 
challenges with that during the 
implementation phase, and share 
some lessons learned.  So I will
 hope you will find that part 
really interesting.
   Finally, at the end of the 
process, it is important to 
measure what you achieved.  Of 
course, first of all, we wanted 
to know our lighthouse score, if
 it's a hundred.  This is a 
measurement you do for yourself 
to know how well you did under 
the migration, but it is also 
important for your stakeholder, 
because it gives a nice concise 
way of presenting the way your 
progress you made on your app.
   While this numb bring a lot 
of satisfaction, especially ily 
if it turns green and it lights 
the lighthouse up there      
there, the second type of 
measurement I put out there, the
 Google analytics is even more 
important to me.  Because I 
think it's very important to 
check afterwards how your users 
reacted to the changes you 
introduced.  After all, it's the
 users that are the ultimate 
judge of your changes   
     changes.
   And this also gives you a 
nice base to start this process 
again.  Once more, progressive 
Web app can be doubled up 
gradually and -- over time.  So 
you can just start the process 
again, follow this path, and it 
rate as many times as needed to 
reach the optimal experience.
   Now, you know more about the 
business case, we wanted to 
solve, and about the process we 
followed.  So how about we get a
 little bit more technical?  
Jeff?  Are you ready?
   JEFFREY POSNICK:  Thanks, Ewa
.  So, before we detail this 
specific tools that we used, I 
wanted to provide some 
background on a Web platform 
technology called service 
workers.  Service workers are a 
code that you write in 
JavaScript and they act like a 
proxy server sitting in between 
your Web app and the network.  
They could inter       intercept
 requests and return a response 
from cache or from the network 
and implement custom logic that 
mixes and matches where they get
 the response from.  With the 
right service worker in place, 
we can create a Web app that lo
ads almost instantly and works 
even when you are off line.
   But if you are not familiar 
with service workers, it could 
be kind of hard to picture 
exactly what is going on. , you 
can imagine that a service 
worker is like an air traffic 
controller.  Think about your 
Web Apps requests in response as
 your airplanes that are taking 
off and landing.
   So let's see how that analogy
 plays out when someone makes a 
request.  So we have this 
airplane representing a request,
 it's taking off, we are making 
a get request for this image, 
and now the service worker is in
 control.  It gets to decide the
 route while your request is in 
polite        flight, and going 
against the network, receiver 
response just like it normally 
does.  That response will go 
back to your page, but the 
really cool thing is, the 
service worker can decide hey, I
 want to save a copy of that 
respond for use later in a cache
.  That's great.  So, Web app 
gets a response and we have a 
cache copy that's good to go.
   So, next time a request takes
 off, the service worker is like
, hey, I know this URL.  I could
 go straight to the cache, I 
could give the Page response 
from that cache, and bypass the 
network completely and 
everything looks the same from 
the perspective of the Page.  
That's really what the service 
worker is doing in a nutshell, 
routing where your requests will
 go and having some intelligent 
logic there.
   But, you know, many people 
would be reluctant to write code
 for an air traffic control 
system from scratch.  I don't 
blame you.  But while the 
service worker API isn't quite 
as come plek as that, there are 
some subtle issues that you can 
easily run into.  And 
problematic service code could 
lead to things like delayed 
updates or missing content in 
irrelevant Web                
your Web app, just like it could
 lead to flight delays or even 
worse.  The women tech maker 
service worker implementation 
uses a brand new sets of tools 
that
 we are happy to announce 
today      today.  And it's 
called Workbox.  And you can 
find out all about it at work JS
.org      S          Workbox,S
.org.  So, I would like         
                S            
Workbox,JS.org               
Workbox,JS.org.  I will show you
 how Workbox helps you avoid 
subtle problems if you build 
everything from scratch.
   So, first up, most service 
worker implementation start by 
adding all the critical assets 
they need to cache making sure 
that there       they will be 
available for later reuse.  This
 is referred to as pre-cache ing
.
   So, here are some basic 
worker code that waits for a 
service worker to be installed 
and pre-caches a list of URL's  
    URL's.  This is the sort of 
thing you might find a sample on
 the internet and copy and paste
 and maybe even deploy it right 
away.  But there are some 
pitfalls that become apparent
          apparent as you 
release newer version of your 
Web app or add additional assets
.  One thing you need to 
remember is manually bump that 
version variable each time you 
change everything or all the new
 assets might not be cache ed.  
Update your URL's, that reflect 
your current names.  That's 
particularly tricky when it 
conveyance            contains 
version, like you see there with
 the JavaScript and the C
SS files.
   And if your site could be 
accessed via -- HTML, you need 
to have cache entries for both 
    both.  For the ing     
        For the           
Gorgt         forgetting to do 
any of that can lead to -- or 
just not a fully implemented 
cache.  So, you could side step 
those pitfalls by using Workbox.
  It integrates into your 
existing build process, whether 
using Webpack, MPM, or the women
 Techmakers, grunt.  Let's look
     lookee pre-cashing.  
There's a single method, that 
takes in a list of files and all
 of the information.  We call 
this information the pre-cache 
manifest.  You can see our 
source service worker here, 
which is pretty empty precast 
manifest.  The nice thing, 
though, in this source file, we 
don't have a hard CODA list of 
URL's or anything like, that we 
could keep it empty.  The goal 
of the process is to figure out 
what should go into that 
manifest.  Our final service 
worker would have that empty 
manifest to replace with a list 
of URL's along with information 
about each URL.  The Workbox 
ensures that all of your -- are 
available off line using your 
revision info in that manifest 
to keep them all up to dated.  
date            date.
   Let's take a closer look at 
the build process.
  And here we are using a method
 called inject manifest which is
 part of the Workbox build 
module.  The service worker file
, that is the one with the empty
 manifest and we tell you where 
to write the destination file, 
to have the fully populated 
manifest ready for use.
   Next thing is, we can use 
wild card patterns to tell them 
which files we want to be 
pre-cached.  Whenever we add or 
rename a file, we don't have to 
update a list of your URL's.  
Also no longer a need to 
implement a version variable, 
Workbox handle it, the details 
and the manifest.
   We also tell Workbox that our
 site responds to request for 
the contents of index study HTML
, so we don't have two separate 
entries.
   Okay.  So, that's only one 
part of the picture.  In 
addition to pre-caching, it's 
common to use one time
 caching strategies too large to
 cache.  You might need one time
 caching that handle something 
that is not needed tore every 
section of your site.  Here is 
some boiler plate code that I am
 plementsz the strategic  
         strategy, first it 
saves a copy of the response in 
the cash      cache.  This is 
something you might want to copy
 and paste.  It's simple to the 
illustrate or earlier with the 
air traffic control        
controller.  The

add entries to the cache, there 
is actually no code that's going
 to clean up the entries when 
they are no longer needed.  
Think back to the planes dlifl
ing the files                   
 delivering the niels       
files to the cache.  And our air
 traffic controller isn't going 
to do anything from stopping 
them to pileup.  This will lead 
to cawsh response which waist  
     wastes storage space in 
your devices.
   And that's only one half of 
the picture that -- cache ing.  
Once you defined your caching 
strategy, you need to tell your 
service worker when to use the 
strategy.  Here is some boiler 
plate code, a request ending in 
PL NG and using the process we 
just described.  For very basic 
Web app, that might be fine.  
But things get out of hand 
pretty quickly, if you need to 
implement different run time 
strategies for different type of
 resources, end up change       
 changing          change chain
       chaining them all 
together and that doesn't scale 
very well.
   So, let's see how we are 
using       using
 Workbox to handle one time 
caching.  There are a number of 
features that lead to clear more
 concise one time caching.  
First we have a built-in route
 er.  And here we are using a 
regular expression as a criteria
 for what triggers our route.  
Workbox has built in support for
 common caching strategies so 
you don't have to write or copy 
our own response logic.  It's 
ready to iewts right out of the
                        use 
right out of the Box.  But 
Workbox goes beyond the basics 
with powerful options like 
specify        specifying new 
expiration policy for a given 
cache.  Work Board of County 
Commissioners will      
      Workbox will take care of 
them carefully.  Getting back to
 our analogy, our air traffic 
controller knows how to clear 
out planes to make room for new 
ones.
   And so leath just            
let's just take a look' adding 
work      Workbox to a site     
site.  As you can see in the 
DevTools network panel, even if 
a user's device is off line     
line, all the responses we need 
come from the service worker 
giving us a progressive Web app 
that loads in under seconds.  
You no loaninger have         
       loaning         longer 
have to build a native app for 
the speed, reliability and 
network independence.
   So, that's great.  But we 
don't have to stop there.  
Workbox also offers a um in of 
built in features that go beyond
 caching.  I would like to 
highlight a couple of them that 
the we can Techmakers are using.
   Your first Workbox make it 
easy to add in support for off 
line Google analytics, it takes 
a single line of code.  One 
enabled, Google analytics is 
cued up or replayed when the 
network comes back.  This means 
that the women Techmakers team 
won't lose out on valuable 
insights
 like PWA off line.  Workbox 
also helps you follow user 
experience best practices.  So, 
using a cache first strategy 
means that your PB WA converts 
instantly but you userrers will 
see previously cache ed content 
even if you deployed          
deployed an update to your site.
  A common UX pattern is playing
 a message at the bottom of the 
screen there, when your users 
know if they refresh the Page, 
they will see the latest content
.
   Workbox makes it easy to 
follow this pattern by 
broadcasting a message to Page 
when there is an update made to 
one of the caches that it 
maintains.  This message 
includes important context like 
the URL of the resource that was
 update ed.  This gives you the 
flexibility of ignoring updates 
that are less important assets, 
like some random CSS file while 
prompting the user to refresh 
when something critical is 
update ed, like the site's main 
HTML.  So, that's just a small 
owe view of what Workbox can do.
  We hope that you find Workbox 
equally useful when building 
your own apps, it's available 
for use today, and examples can 
be find at Workbox -- I want to 
offer a thanks for being an 
early adopter of library and 
offering tons of feedback along 
the way, thank you for that.
   (Applause).
   EWA GASPEROWICZ:  All 
right      right.  Back to 
implementation               
implementation.  But first of 
all, thank you, Jeff, for whack
       walking us through the 
library and thanks for I am     
 implementing it in the first 
place, it saved me time.
   Implementation:  The first 
take away I wanted to share with
 you from the implementation 
process is that going for a 
progressive Web app as great 
audit opportunity            
opportunity.  And not only 
because, lighthouse will list 
all of your -- anyway, but 
because when you are planning to
 implement off line, especially 
if it involves caching some part
 of the website on the user's 
device, you need to be respect
ful of users resources, like 
storage space, and you don't 
want to   to -- cache, a bloated
 website        website, and 
necessary assets, because it's a
 waste.  Making the site clean 
and resource friendly should be 
a priority and it makes it more 
usable to all users, not only to
 the ones using the off line 
mode.  Often it's really not 
very hard, and usually you can 
find some easy fixes and pick 
the    the -- for you to start 
with.  That's what we did with 
the women Techmakers.  Let's 
look at the example Sm this is 
the network panel of the site.  
What I did here, I just sorted 
all the assets, the Page was 
using by size.  And only by 
looking at the very top of that 
list, you can easily support 
easy targets for optimization, 
the higher chance you can 
optimize.  Here you can see the 
two biggest files by far, and 
the -- which is part of -- UPI, 
JavaScript file, and this nice 
big image on the home Page.  So,
 can we optimize those?
   Let's start with the image.  
It covers the full header of the
 Page, so it should be as big as
 the view part at least, but it 
doesn't need to be bigger than 
that.  So if the view part is 
smaller, you can make the image 
smaller.  What I did here was, I
 created two more vergz of that
               versions of that,
 a medium and small.  I added a 
few break points so it uses the 
appropriate image, and look at 
the stats, it a loud me to save 
21 percent on overall image load
 on that Page, with how many 
lines of code, right?
   So image,        imagine what
 would happen if you do it for 
more of your images on the Page?
  It's a really easy fix.
   Now back to PPI.  Over the 
life span of your Page, the l
ibraries and resources might 
change.  New Apps come out and 
some become obsolete, and so on.
  Now it's possible to go with 
an I frame when it's configured 
properly.  By embedding YouTube 
images on the video, we could 
just delete that file and 
suddenly we gained 400 kilobytes
 of the overall Page rate fsm we
 didn't make this decision to go
 for a progressive Web app, 
probably wouldn't even support 
that there is this opportunity
            opportunity.  It's a
 good moment when you go for a 
Web app to think again about 
resources you are using.
   Similar thing, below the 
library, we were using some 
basic functionality of the 
library, so we were able to 
replace this with Lodash. 
 To 4 kilobytes.  You might say 
that -- is not that big, but 
this really adds up in your 
whole Page.  Again, here I 
changed only five -- from my 
code and that's the game, right?
  It's easy to get that.
   Now, once we are sure we are 
not pushing too big resources to
 the user, it would be good to 
also make sure we don't push 
things twice if we don't need to
.  For that we need to leverage 
the caching, this is different 
caching from service worker 
caching, it's regular browse er 
cache, every browse er has it 
these days.  In the old side, 
what was happening, we would
      would -- if you will, by 
diversion, like --
 number or time stap p in some 
cases                      stamp
 in some cases, every time we 
make a -- of the site, the file 
would get a new name, even 
though the file itself might not
 have changed.  That would 
prevent the browse er from 
caching it because the browse er
 doesn't know that under a 
different name, it's still the 
same content.  So instead we 
switched to content base ed 
caching.  Now we make a  a -- of
 the content and embed it in the
 file name, the file will get in
 your address only if the 
contents would change.  This 
allows us to push less resources
 to the users.  Remember, this 
is a community.  A lot of team 
are coming over and over to the 
Page over the life span of the 
context         contacts and 
content makers.
   Okay.  So, a lot of this type
 of gains are really find     
findable in your lighthouse 
report.  If you dig deep, you 
will find a lot of good practice
s and good hints and better than
 that report, so just by 
following, you know, the red 
color, you can find a lot of 
optimizations that would make 
this site better for all of your
 users.
   Now the second tyke      take
 away, it's going to be quite a 
journey, bear with me on that 
one, is about rethinking your 
site's resources, but not from 
like a -- perspective.  Let's 
say your site is nice and clean 
and it's time to think what to 
cache and when so user can use 
your site off line.  We call it 
a caches strategy.  For coming 
up with the right creark 
strategy, you really        need
 to         really need to 
understand your resources.  
Let's look at the example:  This
 is the women Techmakers site 
loaded on-line        on-line.  
It's a rich, visual speerps, 
lots of images, lots of 
graphical effects, it's really a
 beautiful site.
   What would happen if we saved
 all of the images from all of 
the site to use as cache.  It 
would just become really crowded
.  Image a user browses a lot of
 websites, if each of the 
websites had possible images 
from the whole app, that would 
make it inefficient.  So the 
question is, are they really 
necessary, the images?  Maybe 
they are not at the core of the 
site.
   Well, that's how it looks 
like without images.  It's kind 
of ugly.  But apart from being 
ugly, the site is also unusable,
 like you can't even click to go
 to different Page, which means 
it's entirely broken.  This 
means no images is a no go, and 
all images is a no go.  You need
 to find a middle ground 
somewhere.  So how do you find 
the middle ground?
   Well, I started to think 
about images by the function 
they have on the website.  Let's
 go color by color.  Yellow ones
 are images that are for 
navigation and the images that 
allow user to perform some 
action.  And this means they are
 super important          
important.  If they are not 
there, the user cannot really 
use the website, they cannot 
perform the action, they have 
priority.  Now the red ones are 
the ones in our case that are 
related to branding but in your 
case, it might be different.  
Those are images that for you 
for some reason put priority on.
  This could be images that 
create the connection, perhaps 
we want the user to understand 
what type of app they are using.
  These are your priority images
       images.  To the contrary,
 the blue ones are priewr       
 purely decorative.  They are 
cool, make the website look nice
, but if they are missing, it's 
not the end of the world.
  Oh   Oh -- we are here.  We 
were here.  Okay.
   Now the green ones are kind 
of funky, because I -- for the 
images.  What I mean by this is,
 apart from adding to the visual
 side of the story, they also 
convey some message.  In this 
case, they
 tell you which of the companies
 featured content, right?  So 
apart from being visual, they 
also convey a message.
   Now, what would happen if we 
applied different caching 
strategy to different type of 
image?  The inline ones, the 
navigation and -- ones are super
 important.  What I did, I just 
E lined them.  Because they are 
small, they are icons      
icons, and I put them directly 
in my HTML.  This means I don't 
even need to think about caching
 strategy, eventually the images
 are there, and I'm done.  Easy 
fix.
   Now, branding and priority, I
 pre-cache.  You saw that before
, it's pretty straight forward. 
 Those are images I always put 
in user's cache, because they 
are important for my user's 
experience with the ach.  The 
blue ones, I cache it one time. 
 This means that when user 
enters the particular part of 
the site, I cache them on a best
 -- basis.  So there's no 
guaranties            guaranty 
there will be cache -- reenter 
with that site.
   I also put a limit so that 
they don't build up forever, 
right?  And this means that they
 might be in the cache and serve
 the purpose, or they might be 
missing.  What happens if they 
are miss something well,
 less visually attractive, but 
not the end of world.  It's 
usable, the content is there, 
you can check your events, and 
oh on       so on.  Now, they 
show you the full power of 
service worker.  What I wanted 
to do here, when the image is 
not in the cache, I wanted to 
serve some kind of a place 
holder, you know, that tells you
 very well that it was supposed 
to be an image but it's not able
.  But I also wanted to convey 
the message.  So I was using the
 -- of the images to read like 
what company it referred to and 
tried to render this to the 
service worker.  And that's the 
cool thing that service worker 
is just a JavaScript file, so 
you can do all kinds of crazy 
rendering here and creating a --
 kind of off line icon and I 
attached the name of the company
 so that I don't really store 
anything in those images were 
never in my cache, I rendered 
them in on on the fly and they 
served the purpose of the 
website.  So those are the four 
caching strategies I used with 
the help of the Workbox library 
of women tech maker, the truth 
is   is, your sites, your 
projects might be different and 
you might need different caching
 stredges, it should show  

 of service worker, you could 
fine the strategy that would be 
best for your users.
   Now, let's say we implemented
 all this off line and 
progressiveness on our app    
app.  What I wanted to tell you 
from my experience with this 
migration is that this thing 
really influences other parts 
your app as well and you need to
 remember that.  If you forget, 
you will get in trouble in the 
other parts of your app.  
Remember the little message that
 showed you, a new version of 
that, this is the influence on 
the type of UX I have in mind.  
You need to think how going off 
line influences your users UX 
and they respond according to 
the                  accordingly
.  For example, there might be a
 form that you cannot submit 
when you are in that mode, we 
need to remember that it's not 
enough to dump everything in the
 user's cache, I also need to 
implement to tell you something,
 sorry, you can't do this in off
 line mode.
   The second thing is, to 
measure your impact.  Some of i
nteraction with the user will 
happen in the off line mode.  So
 you need to add a line of   of 
-- (?).  You need to learn one 
line of code, that the -- you 
need to remember to put that 
line in your service worker and 
not for getting             
forget about it   it.  Finally, 
this can change your work flow 
and here are some things that I 
found use    useful during 
development.  First of all, it's
 useful to have two different 
service workers for different 
environments.  We separated our 
development service worker from 
the production service worker, 
because in development  
          development, we didn't
 want anything cached, we want 
to be able to refresh your pain 
and see              Page and 
see the changes.  So we got an 
empty file.  Because it's a 
progressive end     enhancement,
 it it is transparent for all 
the requests.  And in production
, we have the fully fledge ed 
service workers that does all 
the caching.  That's the 
solution we used for separating 
environments.
   Now, when developing off line
 and actually working with the 
service worker, you often want 
to see how your patient would 
look like for a user that enters
 the site first time.  If you 
keep refreshing, that's not 
easily achieve able.  For that, 
the best solution is to use the 
incognito mode.  Often when you 
do mistakes and mess up and you 
don't know what is happening 
again, just go incould go neat 
toe                     in    
incognito, that's your friend.
   And finally, I just wanted to
 reiterate that you should 
really use build tools to 
version your files, trying to do
 it by hand is just asking for 
trouble and because the Workbox 
build process is a module, you 
can usually integrate it with 
whatever work flow you have.  
So, I don't use tools to avoid 
manual -- with the cache entries
.
   What happens if there is no 
service worker?
  There is no problem.  All of 
stuff we discussed here is 
progressive enhancement which 
means users that do have service
 worker are able will get some 
more features, some more robust 
behaviors like off line 
experience, but even the users 
that don't have it will get a 
lot of gain from your 
progressive Web app because of 
those other fixes you did on the
 way, because of the -- but 
their performance on the website
 and so on.  So, I urge you to 
consider progressive Web app, in
 the end, it will increase 
satisfaction for all of your 
user base.
   Did we implement everything 
we wanted?  Well, no, there's 
always more.  And our 
improvement that did not make 
the bar, here is an example of a
 few of those that we plan to 
implement in the future.  But as
 I told you, this process can go
 on and go on and go on, and -- 
we will get them implemented in 
the next situation.
   All right.  So, let's recap a
 bit.  I told you today a little
 bit about women Techmakers, I 
told you about the my brakes we 
attempted, I told you how       
 may graix we attempted 
migration, Jeff worked you 
through the tools, the Workbox 
library afn we        and we 
showed lessons learned.  
Prioritization and the 
development work flow.  IP you 
enjoyed that.
   Now it's your turn.  Go and 
start your own progressive my   
 migrations.
   JEFFREY POSNICK:  So what is 
next?  What are good places to 
go?  We put together a lot of 
great PWA guidance, hopefully 
everybody is inspired to use 
lighthouse either in person or 
in the sandbox or go and get the
 extension yourself.  You can 
find out more about that, and IP
 that folks try Workbox, give us
 feedback, we are really looking
 forward to talk to developers 
as well.  You can find us in the
 framework session, some of    
              section, and 
Workbox JS.org is the site for 
that.
   EWA GASPEROWICZ:  And 
remember to join the women 
Techmakers movement and remember
 that all genders is welcome in 
the movement.  So, join us.
   (Applause).
   JEFFREY POSNICK:  Thank you  
  you.
   EWA GASPEROWICZ:  Thanks a 
lot.  I will see you at the 
concert.
   (music) .</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>