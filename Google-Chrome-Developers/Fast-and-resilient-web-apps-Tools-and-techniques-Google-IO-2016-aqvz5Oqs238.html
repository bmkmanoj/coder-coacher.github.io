<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fast and resilient web apps: Tools and techniques - Google I/O 2016 | Coder Coacher - Coaching Coders</title><meta content="Fast and resilient web apps: Tools and techniques - Google I/O 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Chrome-Developers/">Google Chrome Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fast and resilient web apps: Tools and techniques - Google I/O 2016</b></h2><h5 class="post__date">2016-05-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aqvz5Oqs238" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right hi everyone thank you for
coming
Congrats on getting in so my name is
ilya grigorik I work on the Google
Chrome team we're in my day to day job
we focused on making the platform faster
and also we're working on the api's and
infrastructure enable developers like
yourself build nice and fast user
experiences although I guess I should
say fast and resilient because as it
turns out building a fast path is not
enough and a good example of this is a
pattern that I very frequently see
across many different teams go something
like this the team really buys in into
the vision of performance matters they
understand it they internalize that they
have the metrics and they do all the
best practices they optimize their app
they release it they put processes in
place they have audits they even gather
some real user measurement data to
understand how the app is performing and
then lo and behold they look at the data
and they say hey we made everything fast
we don't we did all the things you told
us but despite that we still have these
reports when we look at our data that
there's this really fast device in fact
maybe even the device that I've been
testing on locally and it's not meeting
my frame target our 60 FPS limit or
maybe the app is on a fast network but
it's still loading really slowly what is
going wrong right like I did everything
right is there a bug somewhere in Chrome
maybe it's the carrier maybe there's the
phones at fault do you know of anything
that could have caused this and my
answer most of the time was like yeah
probably like let me tell you there's a
million things that could go wrong but I
think you may be asking the wrong
question and fundamentally the problem
here is that yes you've made the app
fast which is great congratulations
you're doing all the things right but
you forgot to make it resilient and this
is a subtle shift and an important shift
that I think we need to internalize and
understand because our intuition fails
us our intuition as developers often
fails us in this regard so what is
resilience right if you look at the
dictionary definition it says its
ability to maintain an acceptable level
of service in the face of faults and
challenges to normal operation okay
what's an example of a fault or a
challenge to normal operation
during millions of them right for
example it could be that the capacity
within the carrier is limited maybe
there's just a lot of people at one
event and there's just only so much
spectrum that you have maybe you have an
unresponsive server so maybe your
provider is overloaded maybe there's a
network outage maybe the devices in the
low-power mode and as you will see later
when the device in low-power mode it
runs slower maybe there's a background
process on the user's app or on the
user's phone rather that just kicked in
and all of a sudden your app is not
performing as well maybe it's under
memory pressure the point is there is a
million things that can go wrong and
they span from right on the device that
you're holding in your hand to somewhere
in the in the network maybe in the carry
network maybe out in the public intranet
to maybe on your server or maybe even
like in your database
there's million things that can go wrong
and there is no special carve out here
for hey but the user is on a 4G network
or the user has the latest flagship
phone right it doesn't matter what
device you have or what network you're
on these things can happen and they do
happen and that's the thing that we need
to understand so the message here is
that fast is not enough and we'll I'll
try to I guess motivate why that's the
case fast devices are often slow fast
networks are often slow variability in
performance and this is the critical
aspect makes things feel slow and
despite our best intense and best
practices and all the things that we do
to engineer great great experiences
variability is really what kills the
user experience because they don't get a
reliable performance out of the
application but despite all these things
it is still possible to build great user
experiences and we'll see some examples
of that so there are some simple
patterns that we can apply once you
understand what these problems are that
will help you build much much better
applications so let's actually take a
step back and kind of walk through from
beginning to an end how do we evolve to
this point and what kind of practices we
have in place in my day-to-day work I
actually get get to work with many
different teams both within Google and
outside of Google and I kind of over
time develop those three stages of
performance enlightenment as I call it
where when I go and talk to the team I
kind of see them in one of these stages
first one is performance as a bug as in
I get to talk to them or they reach out
to me because they previously did not
care about it but all of a sudden
something has happened
caused it to be such a big issue that
it's like okay let's fix it right like
let's add performance to our app okay
that's damage control the second one is
the internalized that performance is a
feature just like anything else so they
start work on robustness are adding
robustness to the app and then finally
the third stage which I'm hoping to
convince you today is this thinking
about resilience and what you do have to
do to get to that stage so let's step
through it one by one performance as a
bug alright so as I said most of the
time this is notable our performance is
notable by its absence you've accrued
enough technical debt or something has
gone wrong
at which point things just fell on the
floor the users are complaining the
product manager is angry and all of a
sudden like there's a fire drill to fix
performance and performance is fixed
after the fact and as much as you can
actually fix a faulty foundation right
and I intentionally put the Tower of
Pisa as a metaphor here because if
you're looking for some fun and reading
I encourage you to actually go and read
about some of the engineering efforts
that have gone in to keep this tower at
just that angle they can't actually like
make it straight they probably don't
want it to you know to be honest right
because it's a landmark but they
understand that despite all of their
engineering they've literally poured
tens of millions of dollars not hundreds
of millions of dollars to try and keep
the building upright and despite all
that work they know that the best
solutions that they come up with today
will only last fifty years right so it's
like they're just paying that technical
debt and they know that this solution
will still fail and that's kind of like
that's similar to performance it's very
often the case that you can't add
performance after the fact just like you
can't add security after the fact or
accessibility into your application you
really have to think of it from the very
early stages of your development and
that's where you get kind of that second
stage its performance as a future the
the team really understands this from
beginning to an end so if you're
building an application say for a market
where you expect lots of users with
limited connectivity or poor
connectivity or just very sensitive to
data usage for example your UX designer
despite the fact that they really want
to put that like really pretty and
nice-looking cover image on the top of
your page because it's just like it
appeals it it's good conforms with your
brand and all the rest they know they
can do that all right
because it'll hurt performance it'll
hurt the actual experience of the user
and then once you get to the next stage
developers understand that they need to
pay attention to performance and finally
when you release the app you have
processes in place to kind of complete
that cycle monitor the performance
detect the fact that you may have
regressed and change that or or fix that
so it becomes a learning curve right or
just a learning cycle where you're
you're making changes you monitoring and
you adapt over time and this kind of
sets us up on a path to what I call a
robustness where you develop a
collection of best practices that will
help you build great applications and
continue delivering great applications
and there's a long list of examples that
we can talk about of like what goes into
this sort of processing process for
example code reviews like the very basic
form of just preventing failure right
like we're smart developers but the even
the best of us make mistakes silly
mistakes performance mistakes and it's
just good to have your friend or
coworker check your code just to sign it
or check it that's an example of
building and robustness we then also
have test Suites which detect when we
regress some other component of the app
that we didn't even expect to regress
when we make a change in point a and
something fails and B we add redundancy
into the system so we do things like
duplicate data across data centers we
have backup servers we develop all these
things such that if one component fails
the system as a whole continues to
function we add processes to actually
mitigate failure so for example you have
your spike in traffic you don't just
collapse because you've overloaded all
your servers you actually start doing
things like shedding traffic where you
say yes I'm going to intentionally drop
10% of my traffic and they will not get
a response but the other 90% will still
get a response that is still
significantly better than failing all
100% it's a bad outcome but it's
significantly better right so you build
these things into your process and then
finally you even simulate failures so at
Google we take down entire data centers
intentionally to make sure that the
applications that we run are not built
to just run in one data center and your
application should be able to continue
working if we just take out entire data
center and this is part this is a
regular fire drill that all teams have
to go through and of course some of you
are sitting here and you're just like
mentally checking all the boxes and
you're like yeah I'm so awesome we weave
all of this like okay I need to pay I
can pack up and I can just head out the
door right like as engineers this is
this is all the best practices that
we've been taught to do except let's
consider this case so let's say you are
building an awesome app right and your
awesome app is in fact pretty awesome
you have really good uptime which you're
measuring continuously and if you look
at say your response times for for your
service
you see 250 ml second response time for
75th percentile and two seconds for 99
which by all means and measures is
pretty good these are pretty tight
bounce you understand kind of the
performance curve
and all the rest now the app is also
fairly complicated there's lots of
functionality so you have about 16
critical resources why 16 according to
each the archive the number the median
number of critical resources on the web
today for median webpage is 16 so I'm
just using that as an example so let's
just go with it
say we have 16 resources I have a
question for you what fraction of peach
loads will take longer than two seconds
it's kind of a tricky question let's do
some math I know you didn't sign up for
math but we're gonna do some math so we
know that one percent of the requests
will take greater than two seconds right
that's our 99th percentile so let's
assume that the requests are actually
independent and we're making 16 of them
right what is the probability that all
16 requests finish in less than 2
seconds and then in order to get our
actual answer the page will take more
than 2 seconds if any one of those
requests takes more than 2 seconds we
can crunch the numbers you can verify
the math and the answer is 15 percent
this is where I shouldn't start like a
really really long 1 minute long
dramatic pause because 15 percent should
be pretty surprising isn't it your 99th
percentile is 2 seconds you have 16
requests and roughly one in 6 or 1 in 7
P glows will take more than 2 seconds I
find that very surprising it does not
fit Mayans
and I have to do this math every single
time just to proof myself that this is
actually right so what can we do to fix
this I have three suggestions you should
reduce the number of critical requests
and then reduce them again and then
reduce them again all right
which makes sense the fewer dependencies
you have like this the better the odds
that you will not hit this case but
let's say you actually do take your page
from 16 requests to 5 to 5 protocol
requests we can repeat the same math and
you'll find that the fail ratio for this
exam test is about 5 percent which means
that we've taken it from 1 than 6 page
loss to roughly 1 in 20 definitely an
improvement but I think you'll still
agree that that's pretty surprising and
not very good we should be able to do
better so one interesting takeaway here
is robustness right like we built a
system which has very high uptime very
good latency tight bounds but it's not
sufficient we still have this experience
where 1 in 20 or 1 in 6 some significant
portion of our traffic is seeing this 2
second load so turns out we actually
have a lot of experience with this and a
Jeff Dean whose works on Google search
has given a number of really good
presentations and he published a paper
that I really encourage you guys to
check out after this talk you can find
that there's recordings and there's also
the presentation that he's given so he
basically sets up this very problem and
describes the case of a search query
comes in and through Google search in
order for us to search the web and give
you back the results we actually fan out
that query into hundreds if not
thousands of servers so you can think of
like verticals you have images and local
we kind of dispatched those requests in
order to compose the page and give you
back that set of results we wait for all
the responses to come back and then rank
them and give you back the top 10 blue
links it seems simple enough except as
you can imagine this fan-out is pretty
large so it's not just 16 requests in
the case of search it's hundreds if not
thousands of requests and he basically
demonstrates in that in that paper in
his presentation that say you have a
hundred requests 63% he actually used
sets of the same numbers of two second
99th percentile if you make a hundred
requests more than half of your search
queries will take more than two seconds
which is unacceptable and he walks
through I think in a very compelling
manner for why you cannot fundamentally
fix this variability you can certainly
make your latency details even tighter
you can spend more money on better
hardware but fundamentally you cannot
eliminate it there's virtualization
there's a million reasons why the
request may take longer than what you'd
expect but he then goes on to say but we
can still fix this we can build
processes or build additional
functionality into search that will fix
this problem as an example he gives a
long list I'm gonna highlight three he
says well you know what if we only
search 99% of the search index in 200
milliseconds we don't have to wait we'll
just return the results from the 99% and
in this case that like the user probably
won't even notice it's not the best
outcome but it is still better
objectively better than waiting multiple
seconds to give you back 100% accuracy
so that's one example right just
tolerate in exact results if you're
composing a home page with many
different components and many different
widgets perhaps you don't want to wait
for every widget to come back before you
display the page you can set a cutoff
and say every widget must come back in
this amount of time and if it doesn't I
will deal with it and I'll be smart
about it
so for example Amazon has this policy
when they're composing their homepage
hundreds of components there's a there's
a bound on every request and if one of
the components is not responding it'll
just won't show up in a page and that's
fine so they deal with it explicitly you
can abandon these slow subsystems you
can say you know what the spelling
correction is taking really long time
for whatever reason maybe you introduced
a bug maybe there's something wrong
we're gonna drop that functionality so
it's dropped at runtime and search knows
and that the search app and the search
infrastructure knows to disable this and
further it actually is able to adjust
with this dynamically so it's not that
someone jr. on your team gets page and
says hey the subsystem is kind of really
really slow at the moment you want to do
something about it instead the search
system says we you told me what your
latency target us I'm going
and force it this component does not
conform to it so I'm just gonna take it
out and we've already before we put it
into place talked about what that means
we talked about the failed case and what
it means to disable your part so this is
kind of interesting because
fundamentally with the message here is
you have these unreliable components
some of them will fail some fraction of
the time we cannot fix that but despite
that you can still build a great user
experience so this is great right but if
you actually think about it our problem
or our problem space as what developers
is even more complex because what I just
described kind of looks at that green
box on the Left where Jeff fixated on a
search query comes in into the Google
infrastructure but as developers as what
developers we also have to deal with the
other components which is the device
where application is running and then
there's all this stuff in between which
is the public network so let's talk
about that because that's another huge
source of variability and performance so
today you can walk in and pick up a
couple of different phones in different
stores around the world right perhaps
around this area you can walk in and get
yourself a nexus 6p which has lots of
great functionality good great
performance lots of storage lots of
memory and all the rest just as likely
somewhere else in the world somebody
will walk into a store and pick up a
device on the left which has
significantly fewer resources right and
of course some of us are sitting here
thinking like yeah you know it's kind of
it's kind of crazy that we have such a
disparity but like honestly really more
users are buying towards kind of the
high end right or like this stuff is an
outlier the $33.00 phone and maybe you
were sitting in this very room in a
previous talk where a town was talking
about building for the next billion and
I think she very compellingly
illustrates the problem so she has a
slide where she shows where are the new
users coming from so for example in the
past year the color coding here shows
the the darker hues represent the
density of new users coming online all
right anything stand out there what we
have India in China right like as one
example for context right let's dive a
little bit deeper the number of new
entry
users that came online in India and last
year is a third of the US population
it's kind of crazy right like that's a
lot of users and if you drill into the
data a little bit more you'll find that
in terms of where the users are still
that are not online and will be coming
online it's India China Indonesia
Pakistan it's all these countries so
it's not just the next billion it's the
next several billion that's where the
growth is coming from that's where the
new users are picking up their phones
and then she basically goes on and tells
you about the constraints of that
environment and I think one of the
constraints that you'll discover is
people there are much more cost
conscious so they are in fact picking up
these phones towards the other end
they're not picking up the Nexus 6
they're picking up the other phone and
that should scare you a little bit right
so fundamentally we have kind of two
competing trends the way I think about
it we have a trend for features and
performance that's what we hear about as
developers most of the time like every
six months there is a new shiny thing
with more cores more cameras more finger
sensors whatever right smell testers I
don't know they were gonna come up with
something crazy and on the other hand
there is a trend for price and they're
fundamentally they're trying to optimize
price they're not yes they're upgrading
the components yes they're getting a
little bit faster but first and foremost
the question is is can I make this phone
keeper so the way I think about it is
there's an expanding range of
performance the tail or the head of the
curve bill is accelerating because we
keep adding these new things new
features and new capabilities and speed
and performance but the other end is
only slowly moving forward all right so
the actual range of performance is
getting wider and wider and wider which
creates some problems for us as
developers so I call this the
performance inequality gap right the
flagships will continue to accelerate
cost is critical and this this range
will only get further apart so ok fine
let's say we actually do have the nexus
right we have this great phone it can do
all kinds of amazing things it can
decode high-resolution video it can
allocate lots of
Emory for editing video or doing
anything else it can connect to all the
latest 4G infrastructure and great
performance it's all great but I have a
question for you will it because even
though you have this device there's a
bunch of constraints that may conspire
against you so for example on the
previous slide for a nexus SP it's
advertised as a octa-core so there's
eight cores in the device
she actually drill in a little bit
further you'll find that the
architecture in these new phones is
what's known as big little so there's
actually two different sets of course
there is the big course and a small
course and it's not about like eight
cores is better than four because you
get eight times or two times the
performance of the four course it's more
that we've added these big and small
cores because we need to optimize for
power efficiency so it's not the CPU
cycles it's the power efficiency it
turns out that it's significantly better
to run a workload on the slower clock
rate because it's just so much cheaper
on your own your energy then it is on a
on a fast course so these devices
actively migrate your workloads between
the high cores or the big core story and
the small course and that's the big
source of variability and performance
right there so in fact the way I kind of
think about and I think the way it's
more helpful to think about this is
think of the device like say you have a
dual-core device right those dual cores
one of those cores can be shut off say
the phone needs to preserve energy can
just say look I'm gonna run at one core
and we'll look at this in a second when
you have a say a quad-core device it
will probably have to fast course and to
slower course and the way I think about
it is almost two separate subsystems
it's not that you're going to be running
all four cores simultaneously that is a
very effective way to turn your phone
into a space heater and then into a
brick right yes it can do that and look
like it'll do that when you run a
benchmark but that's not a sustainable
long-term performance so the device will
can run fast but more likely than not
you're gonna be seeing your workload on
the fast course and that will migrate to
the slow course and yeah if you actually
compare just a slow course just for a
second pretend that let device is in a
energy-saving mode or is overheating
it'll turn off the the big course and
all of a sudden this quad-core is within
spitting range of the
the one above it right it's like it's
just slightly faster so once again you
have this very wide dynamic range just
because the device can go fast does not
mean that will always go fast say I
mention this a few times if the user is
on a running low on energy
Android for example will kick in the
power savings mode guess what guess
what's the first thing that goes one
that turns on its your big course right
and if you actually run your benchmarks
or run some sort of metrics I think
you'll find a dramatic difference in
performance in my own studies I see as
much as 40 to 50 percent depending on
the device right so that's a pretty big
range if the device is overheating it's
a sunny day you have your phone on a
dashboard guess what you're not running
on a big course kind of silly but that's
what it's all right
maybe there's a background process maybe
there's something else to do it
consuming the CPU cycle so you there's
so many variables you can control so
expect variability right so some of
these reports that we started at the
beginning it's like hey I have this fast
device but sometimes it's not getting me
in the FPS that I expect it's like yeah
probably because there's just things in
the world that you have to anticipate
that cannot guarantee consistent
performance that you get when you put
your phone on a desk and you just
benchmark it right there with like AC on
right so that's CPU that's just one
example the same argument applies to
memory and of course networks right I
has anybody had a fast or sorry slow 4G
connection right is 4G always passed for
you no no right it's like just because
you have a fast network does not mean
that you're gonna go fast all the time
there's not nothing that guarantees in
in 4G that you'll go fast yes there's
much higher peak performance and if
you're lucky in your good area and with
good coverage and there's not enough
people and the network weather is nice
and if you're looking right at the tower
you're gonna get really good performance
but otherwise expect failures and
further the other kind of interesting
and exciting surprising fact is how
often users are actually offline like
sometimes not permanently offline or in
the sense that that you it actually says
I'm offline but it says I have signal
but effectively I'm offline it's it's
the life I
example right I dug up some data in
Chrome so in chrome we have telemetry to
track how often does the page fail to
load and this number or these numbers
the first time I saw on them were like
shocking to me right because effectively
what I'm showing you here is stats for
failed navigations this is main
navigations this is user typing in the
URL or clicking on a link trying to open
say your website and that navigation
fails this is interesting data because
this is the type of request that never
even makes to your servers you don't
even know about it right which kind of
begs the question of like if no server
was there to hear the request the
request really happened right it's like
yeah it did right because we've all
experienced this and I am showing you
ranges here so what does it mean to be
say 2 to 2 to 5% failures
I don't percentile and it turns out like
the the rates do vary between different
countries and different continents so
there's just better connectivity in some
places at which point you know the first
time I saw this graph I'm like aha you
know like this this doesn't really line
up with my experience like maybe it's
the problem in those other countries
like to put the bad connectivity
countries sort of thing so I started
digging a little bit further so sometime
later I was actually reading a report so
this is coming from open signal so they
published these quarterly reports on
like performance of LTE in different
regions of the world and there's this
really interesting jamming there have
look at the bottom in Germany Italy
France and UK the chances that a 4G
subscriber will connect to an LTE
network are a little better than a
coin-flip why all right like that should
be surprising it'll certainly surprising
to me all right
you expect that like North America
Europe should have this all covered
turns out that's not the case so I
started digging a little bit deeper you
can actually go into the open signal
data I'm like ok fine so let me let me
compare say Vodafone I'm not picking on
Vodafone it's just it was a convenient
example that I used so I look at the
stats for Vodafone India and Vodafone
UK right so let's let's compare them
roughly the same you can see that for
example the 4G performance in terms of
the download the peak download and
upload speeds is definitely better in
the UK interesting the latencies are
about the same but then the thing that
really stood out to me is this section
at the bottom so the way open signal
defines this is they run an app on your
phone which periodically just kind of
pings data to their servers it tries to
connect the server just to see like am i
connected and if I'm connected maybe
I'll run a test just to see how fast I
can upload or download data and this
number the availability number is
showing how frequently they were
actually able to make the connection
so the phone thinks it's connected it's
showing you that bar in the top right of
your phone but the connection fails very
frequently so for example look at suji
in Vodafone say India 75% reliability
which means that there is one in four
chance that even though you have the 2g
symbol on your phone the request will
just fail and this is the experience
where the user clicks and it's just like
it's just sitting there right it's like
it literally even takes a couple of
minutes to get to the chrome dinosaur
which is not a very happy ending so it's
like okay this is this is not looking
very good so I started digging a little
bit deeper it turns out open signal also
has kind of a deep dive into UK in
particular and they share some of the
numbers in terms of the coverage
specifically for Vodafone users and how
much time they spend on each network so
for example if your Vodafone user in UK
how much time do you spend on 4G how
much time do you end up spending on 3G
and then we have the reliability data so
you do you multiply those out and you
say okay so if I'm on 4G which is
roughly 40% of the time and I have 97%
reliability about 1% of time even though
I have the 4G signal signal I can
connect if I went through if I'm on a
mix of 3G 4G switching reduced between
those networks it's about 4% and then
finally on 3G is about 15% so
effectively I'm off that that's my
effective offline time and then there's
the actual offline time like the phone
knows that has no signal and you just
can't connect which is not
five percent and if you look at these
numbers they'll line up pretty well with
the failed navigations that I showed you
earlier right which i think is shocking
it was certainly shocking to me when I
first saw this data and I was able to
line them up and say like okay this that
this is a much bigger problem than I
thought like this is you can repeat this
test for North America and you'll find a
very similar results there's definitely
a range of performance across different
countries but they're roughly in the
same ballpark so there you have it it's
not just those markets not those
emerging markets where this problem is
happening so one takeaway here is
offline is not an exception right we
keep thinking about it and treating it
as like this an exceptional state that
the user is in San Francisco they should
never be outlined as somebody that lives
in San Francisco I can tell you that's
definitely not true because I spent a
lot of time offline it's surprising the
places where I'm offline when I go to
Gold Gynt Park I'm offline it's kind of
remarkable and scary so we do have I
think a better analogy that we can use
here we develop you experts onus right
like when you develop product you try to
understand how the user is interacting
with the application what their needs
are and I think you can actually use
that persona so you also derive some
performance insights so as an example
right you have two different people two
different needs one is an urbanite or
lives in a big urban city they have
pretty good 4G coverage as you would
expect right high density good 4G but it
is high density so they actually find
themselves quite frequently on 3G
because there's just not not enough
capacity surprisingly they spend a lot
of time offline and why often times one
of the most common culprits is public
transit you go on the subway you go
somewhere else and I think every and
each one of us has experienced this it
sometimes it just does not work so they
actually spend a significant time
offline effectively offline
compare that to somebody who's living in
say more rural area
okay the 4G coverage is not there yet
the density is not high enough but they
spend a lot of time on 3G and then the
worst case they fall back to 2g and
occasionally they're actually offline so
they may be slow but they're not
permanently offline right there's an
interesting message here which is there
is no such thing as a 4G
right you got your 4G plan somebody sold
you the 4G plan but you're not spending
all of your time on 4G you're migrating
between different networks you're
actively migrating between different
cells and hence the performance
variability and I rule I think we really
need to think about that and understand
that not only is offline just like not
an exception it's a normal state for
these networks but there's a range of
performance and we need to design for it
so we can't just say oh yeah you know my
app is for urbanites they're typically
have 4G they live in North America so
everything's gonna be nice and fast nope
definitely not the case so coming back
to what is resilience right I claim that
resilient applications must account for
the growing dynamic range of performance
they must account for the increase in
the actual performance and equality
between these devices and also just the
range and we saw a couple of examples
there's the latency and the variability
there's the CPU there is a memory there
is the network right between those all
of those combined that's why you're
seeing reports like why is this user on
a 4G network on a fast device having a
really bad time trying to load my
applications it's like well I don't know
maybe maybe there are low energy and
they enable data saving and there
happens to be roaming like that's a very
valid scenario which many of our
applications fail today so performance I
claim is a combination of multiple
things right you have to be a robust
performance is a future you build a
whole lot of processes to make sure that
you don't regress that you're able to
catch these sorts of things and you're
able to deal with failure but then also
it's the responsiveness it's being able
to adapt to the current situation
understanding what the capabilities of
device are and that is what gives you
the resilience were you able to say yes
this device smells like a fast thing the
network sounds like it should be fast
but it's not right and that's not a
problem with the network like it is a
problem with the network but it was my
problem and I should be doing something
about it I can't just say like the
network is bad so hence it's not on my
control so I claim that there are some
axioms of what it means to be a
resilient application right performance
is not a static characteristic it's not
something that you just say like I
curved out the fast path in my
application I eliminated all the bad
things that's not sufficient right
because of all the variability fast
devices actually are very slow very
frequently you'd be surprised if you
actually look at your telemetry that is
probably why you're seeing those long
tails in all of your real user
measurement data and this range is only
going to increase so I think this
problem will only get worse you'll find
yourself sitting a year later and saying
like look these we're doing all this
work to optimize the fast path but which
like the tails are getting even longer
what's what is happening here devices
migrate between different networks and
specifically for mobile connectivity and
offline
I keep drilling this is not an exception
right and I think you've heard many
other speakers in the mobile track talk
about this we need to build mobile or
offline first applications because it is
a normal state in that sense so how do
we put this into practice I think once
you understand these constraints it's
actually not that hard to change how you
architect your applications there's
subtle things that you can do first of
all once again offline all right it's
norm we should be able to provide
consistent user experience regardless of
the type of connectivity so today we
make the request we cross the fingers
and we hope that the thing comes back
quickly that's not good enough right
like we should be able to control that
and that means that we need to be able
to render something regardless of
whether you're in a fast and slow or an
offline state and if you accept that and
you're able to render something and then
say ok I'm now making the request it's
it's the app shell model right like you
can load the Chrome and say waiting like
I'm doing something but it's already
significantly better than the user
staring at a blank screen or worse
ending up looking at the dinosaur so
that's you're like that's your baseline
because with that in place you can do
other things how do you get there with
serviceworker that's a whole topic on
its own we don't have time but
thankfully jake is covering this in
detail he's talking later today at 4
p.m. so really really encourage you to
go check out talk and he goes into all
kinds of detail for what it means to
actually build this and then the best
practices and the gotchas and everything
else but say you do get the
serviceworker installed right the first
thing that you will do or I suggest that
you do is
once you have this thing installed you
should take control of every request
that is flowing through your application
every request should have a bound there
they should not be unbounded I am not
sure when this will come back so as an
example we intercept and in this example
we're using service worker and we're
gonna intercept each request and we're
gonna pass it along to the network just
as I did before but at the same time
we're gonna set up a race with a timer
that just says I'm gonna wait for up to
one second and if the timer fires before
the response comes back I'm gonna deal
with it I will fail right this is the
example of like hey the service is slow
and I need to change how my application
behaves maybe it's simply providing
feedback to the user saying that hey I'm
still waiting do you want me to continue
or something else or maybe you just take
it out entirely in this example I'm just
returning a request timeout with a
four-way status code and maybe you're
upstream application knows how to deal
with that that's one example and note
that once you actually have this in
place once you have this race off line
is just an optimization to this race
because if you know that the user is
offline there's no reason to run the
race right you just say well I know
their customer is not going to succeed
so the offline is no longer an exception
it's just a natural state just like you
take into our data centers of line and
it's assumed that you'll have to deal
with it here it's assumed that requests
will fail and you have to deal with it
once you've done that right like now
we've controlled some of the latency
tails tails on the network and we're
able to provide consistent experience we
need to actually adapt how the
application runs when we bootstrap it or
start running it on the device for
example how many cores does the device
have you can access that their API is
for that what is the battery status of
the device you can query that too you
can actually know if the device is being
currently charged if it's in a low-power
state or if it's being discharged you
can access that you can figure out if
the user has enabled a data save or
optioned in the browser you can check
what type of network the device is
currently on he can also find out on all
kinds of other properties at runtime
like what's the resolution of the screen
what's the device pixel ratio and maybe
you can adjust how your app behaves as
an example coming back to our request
flow right you
augmented with all kinds of interesting
logic where he can say look I'm gonna
intercept the request but the user is
telling me that they know they want to
save data so maybe instead of fetching a
really high-resolution image for this
product photo or something else I'll
fetch a lower resolution because that
that's what the user is requesting if
the if the user is on a slow connection
like something like a 2g or 3G you know
you're not gonna get the high download
performance so you similarly you can
trigger the same logic and say like you
know if I'm a mapping application email
download fewer places or lower
resolution tiles because it's important
that I get a consistent experience back
not that I get a high fidelity
experience back in ten seconds that's a
bad experience same thing for low
battery and all the rest you can imagine
using the same sort of logic to adjust
your runtime right where you can
actually say I'm trying to drive this
thing at 60 frames per second but for
whatever reason the device is in the
low-power mode or something else I can't
do that so I'll need to change that and
this is I think the crucial part you
have to monitor this so it's not just
like you bootstrap your application you
detect these things and you somehow
initialize the app and then you just
cross your fingers hope for the best you
monitor this you've made certain
assumptions you know that what the
network is theoretically capable of but
then you start observing every request
that goes through we expose data about
how long it took how much data did you
download you can use that to augment
your previous assumptions and say you
know what I'm on 4G but this 4G is just
not what I thought
4G should be so I'm gonna downgrade the
experience I'm gonna disable this
feature or maybe I can't drive this
thing at 60 frames per second so I will
disable some other features this is not
new to many game developers right they
continuously monitor their raph loop and
say okay well I can't render these
high-resolution sprites I'm just gonna
drop some of them right because that's
fundamentally better experience than
delivering a janky experience to the
user which will meat may actually make
them nauseous it's not that far out
right and it's pretty simple so you have
to use these signals continuously and
adjust a runtime and that way when the
user transitions from 16% and they're
running on their big cores on their
necks
or some other device and all the sudden
they stopped over the 15% line and the
power saver kicks in your application is
able to detect that fairly quickly and
adapt to it right and the experience is
much better same thing for the network
so as I said these things are not
fundamentally hard right
they're just subtle shifts and these are
the four things that I hope you will
take away to help you build faster more
resilient applications because just
carving out the fast path is not enough
so that thank you I'm gonna be hanging
out at the des mobile web booth for the
rest of the day and tomorrow so if you
guys have any questions please come on
by and ask me and also Twitter or
Google+ thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>