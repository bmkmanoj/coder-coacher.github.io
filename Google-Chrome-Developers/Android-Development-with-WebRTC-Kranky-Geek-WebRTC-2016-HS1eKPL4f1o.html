<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Android Development with WebRTC (Kranky Geek WebRTC 2016) | Coder Coacher - Coaching Coders</title><meta content="Android Development with WebRTC (Kranky Geek WebRTC 2016) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Chrome-Developers/">Google Chrome Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Android Development with WebRTC (Kranky Geek WebRTC 2016)</b></h2><h5 class="post__date">2016-11-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HS1eKPL4f1o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">
CESAR GUIRAO: Today, I'm going
to talk about the Android
development with WebRTC.
The summary of the
talk actually is,
we are going to see
the options that we
have for integrating WebRTC
into an Android application.
And we are going to
have a live demo on how
to use WebRTC using the Java
APIs provided by WebRTC,
and the bindings that they have.
So, let's start with options.
The options here are from the
point of view of using WebRTC.
So, at the end, if you are
using another language,
or other framework,
whatever, they
are going to use
these three options.
Maybe you are not
doing some of them,
but the framework
will use one of these.
So, we have the Android WebView,
we are going to see that,
the Native Java API for
WebRTC, and the C++ API.
Let's start with
the Android WebView.
The WebView that
is based on Chrome
was added to Android
some time ago.
It replaced the old WebView
that was already available
in Android using WebKit.
And this new
WebView uses Chrome,
and it has WebRTC support.
It was introduced on Android
4.4, but the problem with that
is that it was introduced using
a very old version from Chrome,
and it was not
supporting well WebRTC.
So, at the end, it is only
usable in Lollipop and higher.
So the good thing
now, on new devices,
is that it is updated
very frequently
with every version of Chrome,
so it is one of the options.
But it has some limitations.
The problem with working
only on Lollipop, at the end,
there are a lot of devices
that you are not targeting
if you are using the WebView.
And in Lollipop and newer,
versions are only 50%
of the market share of Android.
The other thing is
that the WebView
is a standard
component on Android,
so it's updated outside
your application.
That means that it's
a good thing, maybe,
but also it can break your app.
You don't control when
the WebView is updated.
That means that sometimes
{} people can be running
your application in {} our
WebView that you have not
tested, or something, or maybe
you need to update the WebView.
And people are just not doing
that, so you cannot force that.
And the other thing is
that all the video views
are inside the WebView.
So there would be
applications where everything
is contained on the WebView.
So if you want to mix the video
views with other components, UI
components, from
Android, it is something
that is not easy to do.

As alternative for
the WebView, this
is the most popular
one, that is, Crosswalk.
It is an open source project
that they compile on Chrome,
and they provide the
Chromium build for you
to use as their WebView.
The good things
about that, the pros
are that they have 4.0 support,
so it works on more devices,
and you can embed the binary
inside your application,
so you can decide
when to update.
That is a good thing,
depending on your use case.
And you have always the
latest version of Chrome.
That is also a good thing.
As cons, as you embed the
binary inside your application,
the binary size is
going to increase a lot.
The Chrome [? units ?]
are a very big project.
And also, that is a good
thing, another thing,
if you manually upgrade
the version of Chrome,
the thing is, Google can force
you to update your application
at some point if there
is some security issue
or something like that.
Maybe sometimes this can
happen, and you have to update,
or your application can be
removed from Google Play.
So, let's go to the second
option, that is the Java APIs.
And WebRTC already provides
some bindings for Java,
so it is something that
you already can use.
The good thing with
that is that you
have access-- all the video
is rendered using Native view,
so you can integrate all the
UI with your video views.
So, you can mix them, and create
your application as you want.
And also it is manually updated.
The problem here is that
the WebRTC is a big project.
So, there are
prebuilts available,
Pristine.io was one of
the most popular ones,
but it's outdated now, and they
are not maintaining anymore.
And the other option is to
compile it from the source.
That is not an easy task.
I think that other people is
going to talk about that today.
For example, for
Android, it doesn't
compile on Mac or Windows,
so you need a dedicated box
using a [INAUDIBLE]
to compile it.
So, it's not an easy
thing to maintain.

Other things about
the Native Java API,
that the problem
is that the API is
more complex than Java Script.
Mainly because Java, the
language, is different,
but there are other things
from Android that make
that a bit more complicated.
Also, using that approximation,
for example, the binary size
is so big, it is like
integrating the WebView.
For example, when you create
something with Pristine.io,
you have 20 megabytes
of APK [? sites. ?]
Here, in the Java option,
you have other alternatives,
like TokBox, and other
platform providers.
They provide the Java
API that you can use,
and they instruct all the
PeerConnection APIs, so you
don't have to care about that.
And also, they don't
do all the work
of doing all that's in
[INAUDIBLE] everything there.
And the last option
is the C++ API.
WebRTC is done in C++, so you
can access all the APIs in C++.
That make sense if your
codebase is already in C++,
that is not maybe our common
theme, but for portability,
it is a good option.
You still need Java access
for the capturing/rendering,
and in Java, most of the
APIs are in Java and Android,
sorry-- most of the
APIs are in Java.
So, you would need JNI to
have access to the camera,
or there are new APIs, but
it depends on the person.
So, at the end,
it is complicated.
But the good thing
with that is you
have the maximum portability.
The same code in C++ can run
in iOS, Android, Desktop,
whatever, in other platforms.
But it is very
complex to maintain.
The C++ API is not as
stable as the others,
so they are changing the API.
And sometimes if
you want to upgrade
to the new version of WebRTC,
you have to modify your code,
so it is hard to maintain.
Let's start with using the Java
API to create an application.
This is the fun part, I think.
Well, at least for me.
The set up.
We create a single
activity application
in using Android Studio.
The thing is, to start
with using WebRTC,
we need to decide something
about the signaling mechanism.
You can use Websocket,
PubNub, this is up to you.
You can use SMS if you
want, this doesn't matter.

In the example, I'm
going to use Socket.IO.
That is very easy to set
up a server for an App.
To get WebRTC, for the
example, the easiest way
is to get a prebuild,
like Pristine.io.
You add that to
your gradle file,
and you already have all the
APIs needed to use WebRTC,
is the only line
that you have to add.
The problem with that
is it's not updated.
The last version is from
December of last year,
so that's an issue, maybe.
And don't forget to
add the permissions
needed to access the camera, to
access the internet, and access
to the microphone.
And we can start with the
WebRTC initialization.
WebRTC is a C++
code, so at the end,
this is the thing that
I was commenting before,
that you need to access
all the APIs in Java.
So, there is this initial
method, a starting method,
that you pass the context to
access all the hardware, APIs,
something like that.
And also to decide if
you want to use all
your own video, if you want to
use hardware acceleration here.
This is something that is
needed from the C++ to access
to the Java APIs.
With this you create a
PeerConnection factory.
That is the object that is used
to create the PeerConnection.
So, the next step is
the video capturer.
The good thing is, with
WebRTC, they already
provide all the setup
to start capturing.
So, you don't have to know how
to use the camera, the APIs,
or something like that.
So, [INAUDIBLE] has
these two lines.
You can get the name of
the front-facing device,
and you can create the video
capturer using the name.
They already provide
implementations
for the camera APIs, so you
don't have to deal with that.
Also, they added,
some weeks ago,
the possibility to create
a video capturer using
screensharing.
So, you can screenshare the
view from the application.
So, there are some interesting
use cases using that.
That is a good
thing, but it's not
available in the
Pristine.io compilation yet.
I mean they are
not maintaining it.
Here is the line to
create the video capturer,
and the next step is to add this
video capturer to something.
In WebRTC, we have the
concept of the media stream.
That is what we use to send
the video to the other peer,
so we need to add in the stream
two tracks, the audio track
and the video track.
When we create the
tracks, we set up
the video source using
the video capturer
that we have created before.
So after that, we
create the audio source
and create the audio track, and
we have the local media stream.
With this local
media stream, we can
start showing the preview of the
local video in the application.
For the rendering.
Here, to see the preview
of the local video render,
there are several
options available.
In WebRTC, they already provide
these two, GLSurfaceview
and SurfaceViewRenderer.
The difference between
them is the GLSurfaceview
is a common
GLSurfaceview, that is
being used by all the renderers
in the same conference.
So you can overlap
the video there,
but you have to add the
renderers in the order
that you want to put
them in the screen.
And all of them share
the same SurfaceView.
Maybe it's OK for
some applications,
but there is the other
SurfaceViewRenderer,
that you use a different
view for every video.
So you can place the views
in the layout in any way.
The problem with this last
one is it's more flexible,
but the SurfaceView views in
Android have layout issues.
At the end, the implementation
of SurfaceView in Android,
they are not really
views, they are
like Windows over
the real window,
so maybe you can have
some layout issues.
And WebRTC provides all the APIs
to create your own renderer.
So, at the end, if you have
issues with one of them,
you can create your own renderer
using [? text tool ?] view,
or maybe, for example,
if you have a game,
you can integrate the
texture or the video frame
inside your game.
You have the
possibility to do it.
So, in the sample
code, in this example,
we are going to use
the GLSurfaceview.
We get the SurfaceView
from the layout.
And at the end, we
create the renderers.
This is the other peer renderer
that we are creating first
to cover all the GLSurfaceView.
And we are creating
our preview renderer,
covering only one part,
one square, in the view.
And if we add the renderer
to the local video track,
we start seeing our preview.
So, with that, we have
half of the [INAUDIBLE].

The next things is to
create the PeerConnection.
It's not tricky, but the
thing is, you usually
see the PeerConnection.
When you create
the PeerConnection,
people add some STUN server
from Google, but at the end,
you need to provide your
TURN and STUN servers.
This is something that you
need for your deployment.
If you don't have that,
the PeerConnection maybe
works in the local network,
but this is very probable
that it's not going to work
outside your local network.
Because, at the end, sometimes
there are firewalls, or knots,
that makes you to have issues
connecting from one's peer
to the other.
So, this is very important.
So, this is where all the
three parties are important,
because, for example, TokBox, we
provide all these things so you
don't have to deploy your own.
So, we create the PeerConnection
using the PeerConnection
factory that we
have created before,
and we add our local stream
to the PeerConnection.
That way, when
the PeerConnection
is connected to the other
peer, they will see our video.
And here start the
SDP negotiations.
The SDP negotiation is the
same that is on Java Script.
Maybe it is a bit more
verbose because of the way
that you have to
[? lead on ?] Java,
but you have to implement
the peerConnectionObserver
and SDPObserver to
get notifications.
They are the listeners
to get the notifications
from PeerConnection
when something happens.
We are sending, in our sample
code, the SDP over socket.IO.
That is our signaling protocol.
And this is the last step
to have media flowing.
You have to remember
also, when you
get the stream from the other
peer, to add the render there
to see the video
from the other API.
This is the diagram of how
the SDP negotiation works.
It seems very complicated,
but it's not so much.
At the end, both clients connect
to the server using socket.IO,
and the server decides
to send the createOffer.
That is the start
message to one of them.
So, one of them, using
the PeerConnection API,
creates the offer and
sets the local description
in this PeerConnection object.
Then that generates the
offer SDP that you'll
send to the other peer.
The server is just
relaying the message.
And the client 2, when
it receives the message,
it sets the remote description,
and creates the answer that
travels to the other way, and
they have the information of,
the codex of, what size.
So, in this process, there
are also other things
that are called candidates.
That whenever you start
doing the SDP negotiation,
PeerConnection
automatically starts trying
to guess which IPs you have.
So, all the
candidates are options
to connect to your host.
So, if you have several
[INAUDIBLE] servers,
maybe you would see
more candidates here,
because you have more options
to connect to you or something
like that.
And at the end of
this process, we
have the media
established, after this.
And the server is
something that maybe,
if you are doing
Android development,
is something that maybe
you are not used to doing.
But the server is
something as easy as that.
This socket.IO server,
only 30 lines of code.
It doesn't have
any kind of logic.
When there is a message,
an offer message,
it sends the message
to the other peers.
Answer, the same, and
the candidates, the same.
So, for the first one,
it's sends the createOffer,
but to start the process,
the server is very easy.
At the end, also the
application is also very easy.
It is very small.
260 lines of code for
the Android application.
That is the minimum thing
needed to have video working.
There is no [? other ?]
handling and other things,
but it is something that
is easy to understand.
And the server is very
small, as you have seen.
Here, you can find
all the source code
that I uploaded here to pick
up, so feel free to use it
and to test it.
And some Android
tips, to finalize.

The binary size of the
application, using WebRTC,
is very big.
I recommend to use
this split mechanism,
creating different APKs
for every architecture.
At the end, the
size is something
important for the
final application
if you're creating something
for a commercial application,
or something like that.
Remember to stop the
camera and the microphone.
This is something that
WebRTC is not doing for you.
WebRTC doesn't have access to
the events of the application.
This is usually handled
by the activity,
so you have to take care of just
stopping the camera when you
go to the background, or
stopping the microphone when
you receive a phone call.
These are important
things to remember.
Audio routing, this is
something that it seems easy
when you connect the headset,
or Bluetooth headset,
something like that.
It's not as easy if you
look at the implementation
of the AppRTC application.
That is the example code
that is provided with WebRTC.
It's not as easy.
There are a lot of edge
cases, and it is better
to look at an implementation
and do something similar.
And at the end,
if you want to try
new codex, like the VP9 or H264,
by default it is using VP8.
But there is no easy
way to select them,
so you have to then
modify the SDP,
and then reorder the codex
there to use one or the other.
Maybe in the future is
something that is a bit easier,
but now is a bit complicated.
And that's all for today.
Thank you.
Thank you.
[APPLAUSE]
[MUSIC]
</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>