<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>V8, Advanced JavaScript, &amp; the Next Performance Frontier (Google I/O '17) | Coder Coacher - Coaching Coders</title><meta content="V8, Advanced JavaScript, &amp; the Next Performance Frontier (Google I/O '17) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Google-Chrome-Developers/">Google Chrome Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>V8, Advanced JavaScript, &amp; the Next Performance Frontier (Google I/O '17)</b></h2><h5 class="post__date">2017-05-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EdFDJANJJLs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello how is everyone
hi my name is Seth Thompson and I am a
product manager on the v8 team and
Chrome so v8 is an engine and it's the
engine that runs JavaScript in Chrome
and our mission is quite simple that we
want to speed up real-world performance
for modern JavaScript and we want to
enable developers to build a faster
future web and there's two parts of this
mission that are important the first is
that the JavaScript that v8 is optimized
for is the JavaScript that you as
developers are actually writing and it's
the JavaScript that includes new
language features as they get introduced
new patterns of application development
new idioms and then the second is that
as we as an engine you know participate
in the tc39 Standards Committee and
develop tools and give guidance that all
of this goes towards a faster future web
so we'll talk about more of all parts of
that in a little bit but first I wanted
to start with some fundamentals of a
JavaScript engine specifically v8 is a
just-in-time compiler or a gyp now what
this means is that when JavaScript is
sent to the browser the browser has to
execute this code as it runs it or
immediately and in order to guarantee
maximal performance the engine wants to
transform this code this JavaScript into
machine code native code but because
it's doing this all as soon as you load
the page it needs to do it
Just In Time or in at runtime and
there's some fundamental trade-offs at
play here I think one of the things I'd
like to do is provide more or shed some
more light on what we mean when we say
that an engine runs JavaScript fast
because there's a lot of different ways
to run JavaScript so the first
fundamental trade-off is that in general
the more optimization in engine performs
the faster the machine code it generates
so the faster that code can potentially
run but the longer the initial delay
because remember all of this compilation
and this optimization happens after you
load the page and the browser sees the
JavaScript for the first time so there's
a trade-off there the top peak speed
once the program starts running versus
the initial delay when it gets started
or startup and the second trade-off is
that in general in a JIT the more
optimizations an engine performs the
more memory that the engine consumes so
anytime someone says that their engine
is five times faster or five percent
faster you should think about what it
faster in what dimension or or how does
that number get translated into a sort
of a position in this problem space or
this trade-off space so let's examine
this in a little more depth here are
these constraints as I've laid them out
generally in an engine can have fast
startup or a high peak performance and
specifically an engine can make
decisions about executing a particular
function with this granularity so it can
immediately run it or it can make
optimizations and run it faster but pay
the cost of making those optimizations
upfront and the second is that an engine
can have
a low memory footprint you could think
of an interpreter which has a very low
memory footprint but that comes at a
cost of max of the max speed as well so
memory and speed also are a trade-off
Tumut so let's say that I wrote a
webpage and all it did was run one line
of JavaScript foo now we don't know
exactly what foo is doing here but I
would be willing to bet that a
JavaScript interpreter which you might
have a visceral sense of something slow
but I would be willing to bet that a
JavaScript interpreter can execute one
function much faster than an optimizing
JavaScript compiler which takes the foo
function looks at it turns it into
native code and then performs multiple
optimization paths over that native code
before it can even execute it so all of
this is happening when you load a page
or start a JavaScript executing a
JavaScript file so to put that into
context on our little chart here if you
were only if you knew that you were just
executing one function you would
probably want to optimize for fast
startup not peak performance because
you're only running this function once
so the time that takes to make it fast
to run multiple times you would have
already paid the cost by running at once
so what if this exact same function
though is run 10,000 times does the
trade-off change at all so in other
words if we know that we have to make
foof a stand it's going to be used
10,000 times then in this case it's
worth taking that initial startup delay
to optimize our native code for foo
because we'll amortize that cost of
startup
over the next 10,000 executions so in
this case for some code pattern like
this you want to optimize your
compilation of the food function for
peak performance and if this is a
desktop browser you can rest assured
that there's enough memory to compute
lots of these optimization passes but
what if this exact same code is run on a
low memory mobile device let's say an
Android device with low low RAM well
then if you know taking the memory to
compute multiple optimization passes and
generate a lot of machine code is going
to be the difference between your device
being under memory pressure and closing
a bunch of background tabs in that case
you might want to actually sacrifice the
peak performance of executing this
JavaScript for a low memory footprint so
you can keep multiple or more tabs open
in your browser so I would argue that on
a mobile device although you ideally
would like peak performance here you
have a set of constraint which is that
you'd prefer low memory usage and
finally what if that same line of code
is on a server in a file run by nodejs
well in this case your server only
starts up once and then it keeps running
on whatever machine is receiving
requests from your users so in this case
you don't really care about the startup
cost at all you'd like the engine to
take as long as it can to optimize this
function because once the engine or the
node app is up and running you want each
of those requests to be served as fast
as possible but again if this is an IOT
device maybe you're running node on
something that's also memory constrained
well here you might have to sacrifice
some of that peak performance for a low
memory footprint so the reason I go
through all of these examples is to say
that the same three lines of JavaScript
or the same function of a single
function of JavaScript requires many
different types of optimizations or many
optimal ways of executing this function
depending on the context depending on
the device that's running on whether
it's running on the server or client
side how much memory there is so as an
engine or as Chrome developers you know
as we're developing v8 we want to put
together an engine that spans that
entire trade-off space and it's able to
use heuristics to know whether it should
be tuned for fast startup or peak
performance or low memory so over the
last year and actually even beyond that
for the past two to three years v8 has
been working on an entirely new
execution pipeline so what this means is
that the compilers that v8 previously
used have been completely replaced by a
new execution pipeline so I quickly walk
you through the history just to give you
a sense of how much machinery there is
behind a JavaScript engine
how many moving parts there are in 2008
v8 started with a simple code generator
and it generated semi optimized machine
code in 2010 we added an optimizing
compiler now remember an optimizing
compiler is the one that takes more time
to start up because it's computing
optimization passes but then generates
code that when run multiple times is
very fast in 2015 we realized that our
first optimizing compiler wasn't
extensible enough it didn't support the
full JavaScript language and we knew we
needed something for new patterns of
JavaScript things like as MJS eventually
webassembly so we created a second
optimizing compiler then we added an
interpreter I'll talk about all of these
things in a second and finally we get to
the present day where we evolved from
that first yellow compiler and
crankshaft we just remove those from v8
so today we have two parts of our engine
which are completely new so what are
those parts well the first part of the
all-new v8 is turbofan an turbo fans an
optimizing compiler we've been working
on it for over three years as an
optimizing compiler I mentioned that it
is designed to be able to squeeze out
the most possible performance from the
machine code that it generates now it's
also designed to be extensible from the
beginning so we were able to implement
all of es 2015 which are the newest
JavaScript features in turbofan and as
well as a follow-on features from es
2016 and ES 2017 and turbo span supports
the entire language so javascript
primitives like try/catch and finally
can be optimized for peak performance
were historically they weren't what all
of this means for you as developer is
that the new v8 has fewer performance
cliffs you're less likely to run a
single function have it be fast make a
change and suddenly wonder why it's slow
and that's because it supports the
engine now supports a more diverse set
of workloads so just to recap turbofan
is optimized for peak performance and
multiple optimization passes even if it
takes memory
but we've also added ignition because we
we know we need to serve these use cases
where the startup of the code or the
initial execution is fast and there's a
lower memory footprint so ignition is an
interpreter and contrary to popular
belief it's not necessarily slow if it's
used at the right time an ignition is a
generate a bytecode which then runs and
we've noticed that it's particularly
beneficial for loading heavy pages fast
and it's integrated with turbofan to
make adaptive optimization simpler so
this means that if we start executing a
function with ignition we can watch and
see whether it's used often and use a
heuristic to say that we should probably
send that function to turbofan and
optimize it for max performance so
ignition is optimized at this end of the
spectrum and we put these two things
together you end up with v8 an all-new
v8 which can target multiple places
along the spectrum of low memory my
memory fast startup peak performance
depending on the workload depending on
the heuristics that we we see as we
execute your code depending on the
device that your code runs on depending
on whether it's embedded in node or
embedded in Chrome so this means that
real world JavaScript is faster the
engine can run and a much lower memory
footprint there's fewer performance
cliffs it's a more well-rounded engine
and it's better tuned for nodejs
than our previous configuration finally
there's a third new part of v8 and
that's what we're calling or no code
Orinoco is a new mostly parallel and
concurrent compacting garbage collector
our previous garbage collection was not
always parallel and Orinoco expands our
ability to perform garbage collection
across multiple threads
to make for faster pauses when we're
cleaning up the memory of an application
so all of these things come together
into this new package but I think and
I've sort of mentioned many of the
different dimensions on which you can
compare the performance of JavaScript
but I want to talk a bit about how we
benchmark JavaScript or how we how we
tell whether we're getting faster or not
at real code so v8 has started measuring
the performance of real page loads we
have a system which can record user
actions so we can we can set up a
benchmark that loads a page scrolls
through the page potentially watches a
video or reads a news article and all of
these simulations we can then run
benchmarks against against and we're
happy that after optimizing for these
real-world web pages we saw a twenty
five a twenty thirty five percent
improvement on the speedometer benchmark
depending on the platform over the
course of the last year so what this
means is that by optimizing real web
pages we were able to deliver
improvements on a benchmark like
speedometer but not all benchmarks are
good and in fact if we could choose we
would always just run against real web
pages the reason that we use something
like speedometer is because it runs in
multiple browsers so you can compare
between engines so here's the
performance of the last year for
speedometer
and one of the downsides of benchmarks
is that of scuse me these these
traditional benchmarks not real-world
simulations but the benchmarks that you
would run and a browser tab to compare
engines is that they're not always
emblematic of the types of JavaScript
that you're writing so at the very
beginning we talked about these four
different ways that you could write code
on a server on a low memory device and
the octane benchmark was tuned only to
exercise the peak performance of a
compiler so we believed that chasing
octane or optimizing in particular for
octane lead engines down a path where
they over optimized for peak performance
and under optimized for things like low
memory usage and fast startup so we this
year we announced that we retired octane
because we felt that it wasn't yielding
the right decisions for engine
optimizations and I mentioned
speedometer earlier is something that
better approximated real-world web
websites and the reason it better
approximates them is because it includes
applications to do MVC application to be
specific that implement the same to do
application across many frameworks so
speedometer includes angular and
includes react and what we've done is
we've worked with WebKit who have
originally implemented speedometer to
add even more frameworks so I'm excited
to announce that speedometer too has
just been committed to the WebKit code
base and it expands the frameworks that
it tests so now it tests angular 2
rather than angular 1
it adds pre-act bouge s inferno
it adds es2015 code it uses code with
bundlers webpack and it's updated all of
these frameworks to the latest version
so while no benchmark
is a perfect approximation of real-world
code we hope that speedometer 2 will be
a better way to compare engines across
browsers and those are the frameworks
and the tools that are included in the
bundle errs that are included so
speedometer 2 is coming soon to WebKit
you can find it or you will be able to
find it on browser bench org so one of
the things that I mentioned in that past
section when I was talking about
important parts of a performance story
is es2015 so es2015
is and and newer features are the the
latest version of JavaScript so es 2015
features are things like promises rest
and spread operators array iteration it
becomes a lot easier with es6 is 2015
excuse me and when s 2015 was initially
implemented there was a slow down on es
2015 code and this is because engines
take a long time to optimize particular
code patterns to make them fast
so when es 2015 was first introduced it
was actually a lot slower than es5 code
well over the last year we've been using
a tool called 6 speed which compares es
2015 code to the transpiled version or
the es 5 equivalents of accomplishing
the same action so an error function is
compared to an anonymous function and
we've been using this tool to optimize
the biggest the biggest performance
differences between es 2015 and there
transpiled equivalent so we worked on
optimizing for of so now using the four
of keyword is as fast as writing a
simpler JavaScript loop with a var we
worked on improving object assign object
out a sign shows up F
everywhere in react and redux code
especially we worked on improving
iteration and destructuring and we also
improved the performance of spread calls
and by doing this we decreased
drastically the slowdown of es2015
transpile ds5 compared to es 5 and you
can see here that over percent over the
past roughly six months we went from the
average es2015 code being almost three
times slower than es5 code to the
present where we've almost reached
parity so what this means is that there
are fewer and fewer reasons not to use
es 2015 code natively when you can when
your users support it or on node on the
server and we also I wanted to highlight
a couple language features in particular
which got special attention because
they're so useful but there was a lot of
performance to be on the table that to
be had so generators are now two and a
half times faster and async and a weight
which is a very useful idiom for
returning a promised based then style
code into something that looks a little
more synchronous I think in a wait is
four and a half times faster than it was
in the previous six months that's it is
big deal yeah
now underlying all of this is our
promise implementation and for a while
native promises were actually slower
than promises that came from a library
like Bluebird well I'm also happy to
announce that over the past year we've
improved promise speed by four times so
native promises are now something that
are able to be included within
real-world code without worrying about
their performance impact so I've talked
a lot about language features and the
different places that you can run
JavaScript and one of those environments
is no js' so nodejs is obviously a
server-side language and it node embed
the v8 engine so over the past year
we've actually invested a lot more in
the node community than we ever had
previously v8 is now represented on the
node core technical committee and you
can find us on github on the node
repository working through issues that
come up under the v8 engine label and
these are things like regressions
somebody notices that their node code
slowed down for some reason we're
working with the node team on releases
and making sure that as soon as a new
version of v8 is available it can get
upstream didn't notice as fast as
possible and tested for release and
we've also worked on performance
optimizations specifically for node so
in addition to exposing JavaScript
features node also has a rather large
standard library and some of the those
some of the API is in the standard
library things like buffers we've had to
do specific performance optimizations
for so we worked on faster instance of
we worked in a buffer dot length
regression we've worked on supporting
long argument lists and node and in
general we've made sure that let and
Const are as fast as their bar
equivalent we also know that you know
certain libraries are used in node
throughout throughout the ecosystem and
the through library which is used for
creating streams primitive we also spent
time optimizing to make sure that
streams and node were fast and all of
this is summarized by what we call the
Acme air benchmark which is a benchmark
that starts up a node server and tests
sending thousands of requests to that
server involves a database it's a big
app and that benchmark showed a 10%
improvement when we launched turbofan
and ignition so those to show that the
the improvements that we made to towards
making our engine more well rounded did
in fact yield faster node performance as
well so we're really excited about that
and v8 is part of chrome when you're
debugging JavaScript in chrome you can
use dev tools so over the past year and
a half - we've been working on making
dev tools support node.js and I'm happy
to announce that it's now easier than
ever I'm actually going to show a little
demo right now of where we are right now
so you know traditionally when you're
writing a node application it's
difficult to debug things so it's not
quite as simple as it is client-side
where you can pop open the inspector and
navigate around your your webpage so for
this demo I'd like to go through a new
command-line interface called a mode and
it's a really cool program it's an open
source program I just found it when you
run it and type something any sentence
so say hello it comes back with a bunch
of emojis that correspond to the
sentence so it's really cool it's an
open source project and in this case or
for this demo I want to figure out
exactly how it's implemented so to debug
any node application with dev tools all
you have to do is pass a flag to node -
- inspect
and what this does is opens up a port a
debug port that can communicate with the
dev tools instance and you could debug
in dev tools now previously you had to
paste this relatively long URL into
Chrome in order for this to work but now
if you go to chrome inspect or about
colon inspect you can see that chrome
will automatically detect any running
node instances on your computer yes so
let's go ahead right there and click
inspect we get a dedicated window that
opens up and here we can see our CLI if
I look into my files under my sources I
see that file that I just executed and
it's right here so what's really
powerful about this is I can come in and
use any of the debugging features that
are have been introduced in the last you
know six or so or months one of those
that's really powerful is inline
breakpoints so here what I'm going to do
is I'm going to set a breakpoint on this
line of code and this is the code that
fetches those emojis from the server
that it's using to perform I think some
machine learning and I what I really
want to do is I want to know what the
server is returning and I think it's
it's returning an array right around
here in the middle of the line but if I
normally would just break on this line
it would break at the beginning of the
line and if I advanced it would have
completed the fetch already what I
really want to do is break right here so
with inline breakpoints which is a
really cool feature of dev tools I can
do just that and now I can also use this
feature to debug node code so with that
breakpoint in place let me try this
again a run hello and I get something
back I get pause on a breakpoint
I'll just show a couple quick new dev
tools features one of them is that this
call stack is
supports asynchronous code execution so
you can see here that it actually traces
processes through a variety of async
functions it can trace promises being
resolved but I want to come down to the
scope here and I can see that this array
variable the array that it returns is
actually ten emojis long and so now I
can see okay what this is doing is
slicing that array and only returning
the top seven results so relatively easy
easily just by passing the inspect flag
and then opening up Chrome and clicking
connect to node I can jump into the
execution of a node program and use all
of the dev tools features to debug it so
I was interested in the debugger today
but the JavaScript CPU profiler is
available for node the memory profiler
is available and the console is
available I can see if I can see process
dot versions v8 yes this is node it's
running this version of v8 so all of
this is immensely useful and it's just
one of the new features that we have for
debugging dev node with dev tools
I won't demo it now but I'll briefly say
that we also have a new dedicated window
for node so you can actually close
Chrome you can open this node debugger
you can add in the port and it will
always stay connected to whatever node
instance you're running even if you're
running multiple node scripts at the
same time so yeah that's a exciting
stuff
okay let's go back to the slides briefly
in addition to these inline breakpoints
that I just talked about and this
integration of node and dev tools the v8
team has worked with the dev tools team
to support a number of really useful
features for writing JavaScript
applications so one of the themes of
Google i/o this year on the web track
has been optimizing the performance of
progressive web apps and javascript by
simply shipping less code if you're
using a bundler like browserify or web
pack and you're requiring many modules
from npm it's very easy to end up in a
situation where your app bundles ships
across the network to the client parses
and starts up way more JavaScript than
you actually need it's just simply how
the bundler included an entire library
let's say even if you only used one
function from it so I'd like to do
another demo here and show how dev tools
can help you find this situation and fix
it if you're working on an app with lots
of dependencies so I'll briefly show you
that I've got an application here and
I'm going to serve it on a little server
that watches my code sends it to browser
if I recompile is it fairly basic stuff
but I can show you that I'm requiring
something I'm requiring lodash as a
library and a bunch of other
dependencies so here's the app it's a
github repo formatter so if I type in
hello IO 2017 it'll give me a little
slug that I can add into github it's
kind of useful let's say I want to do
look at the performance of this now
browserify turned my JavaScript into a
single file a bundle here and it's
actually quite large it's it's about
6,000 lines
now I could go and manually figure out
exactly which part of those 6,000 lines
I actually used but instead I'm going to
use a feature called dev tools coverage
now you might be familiar with coverage
from test coverage you know you you run
a coverage tool to figure out whether
you've tested all the parts of your code
well this type of coverage tool is a
little bit different instead it tests of
the JavaScript that the browser saw how
much of it was actually executed how
much of it was just dead weight or
dependencies that weren't used so if we
go under
you can press escape to bring up the
drawer on dev tools and we go under the
coverage tab right here this is new so
you'll have to do this in Chrome Canary
and if it's not down here it'll be under
more tools than coverage this panel
allows us to load our page and check
which JavaScript is actually executed so
what we have to do is hit record and
then refresh we can type in something
and now you can see here that we've if
we isolate just the bundle file you can
see that we've only used about a third
of the JavaScript that we're shipping
what's more this tool allows you to
actually look at the source and see
which functions were called and which
weren't so although there's some green
here which is code that we ran there's
also a lot of red and I actually know
exactly what it is all of this red code
is function our functions from the
lodash library that I didn't use because
actually I'm only using one function
this is the function that's actually
turning the string hello Google IO into
what they call kebab case but anyway so
I'm only using one underscore function
here and I happen to know that actually
I can rather than loading the entire
lodash library
I can load just kebab case in other
words I can trim my dependencies down to
exactly what I need and in this case
when I save it my process will recompile
my bundle if we record and reload we can
see that now our bundle is only 1,000
lines rather than 6000 and the
percentage of bytes loaded actually I
think I need to clear this first ok the
percentage is now much much smaller of
code that's not actually being executed
so this was a trivial toy example but
you can imagine running this on a very
big codebase where it's not easy to know
which dependencies are used and which
aren't
or which parts of dependencies are used
and this should help you make sure that
anything that you're shipping to a
client for a particular page or
particular route is just what's needed
to execute the functionality that you
need so that is code coverage and dev
tools let's go and go back to the slides
if you are familiar with the performance
panel in dev tools the performance panel
is another way to instrument and
investigate the performance of your app
and you can go to the dev tools
documentation and find a number of
really good tutorials for using the dev
tools performance panel a couple other
things we've added are include line by
line profiling I think that was the
first of these features and if you an
addition to seeing the coverage of which
code was executed you can record a
performance profile look back at sources
and in the gutter see the time that each
function took to execute you can use
that to identify a particular bottleneck
in your source we've got code coverage
which just was launched and async
debugging I showed a little bit of this
in the node demo async bugging has
gotten a lots more
simple a lot simpler more recently so v8
dev tools are constantly working to make
sure that debugging and making fast
applications in the first place is
easier and easier there's one last thing
that I wanted to touch on and this is
mainly a talk about JavaScript but
webassembly
is a really exciting new technology that
v8 has recently added support for so web
assembly if you're not familiar with it
is a new language for the web and
specifically it's a low-level language
designed to execute near native code so
web assembly is ideally suited for a
library that you might otherwise write
in C if you're in a different
environment in the web so for the first
time you can use web assembly to compile
C and C++ programmer programs and run
applications like desktop suite
applications like a graphics intensive
game or a video editor that previously
would have been constrained by the
performance of a dynamic language like
JavaScript so we're excited to announce
that v8 is supported in Chrome and I
think one of the most amazing parts
about web assembly is that it's not just
a chrome technology in fact when I see
this slide I think the thing I'm most
excited about are the other browsers
here so web assembly is also sloshed in
Firefox and it's currently in preview in
technical builds of edge and WebKit so
web assembly is poised to become a
cross-browser solution for running
native code and it's the first new
language that we've introduced on the
web that has this sort of cross browser
support it doesn't require any plugins
and it uses the regular web platform API
s that you're all familiar with and we
launched it in chrome 57 and as I
mentioned you can compile C and C++ to
web assembly within scriptum already
we're beginning to see some very
incredible demos
this is a demo of the Unreal Engine
running in a web browser and we also
have on the web assembly website a Unity
game as well as a bunch of community
projects that have already started being
created just the other day I saw a video
editor running with real-time video
effects and it was very 30 30 frames per
second so if you're interested to learn
more about web assembly I encourage you
to check out the IO recording of a talk
that I believe happened yesterday by
Alex Danilo and soon for web assembly we
will have more start at performance of
optimizations and some more features
that enable things like multi-threading
more advanced native code features so to
summarize or to pull this all together
if there's one thing to take away from
this talk it's that the v8 engine and
really JavaScript engines in general
need to be well rounded engines they
need to be able to run lots of different
types of code fast in lots of different
environments and the constraints of
those different environments might
change the types of code that needs to
run so v8 today with the ignition
interpreter and the turbofan optimising
compiler is well equipped to run code at
both ends of these spectrums of both of
these spectrums and I think with web
assembly this this diagram can be
expanded a bit web assembly works
alongside JavaScript and will allow
developers to push that peak performance
angle even farther
so web assembly just expands the
possibilities that a JavaScript or an
engine can run a run code in the browser
so that's a that's my talk for today if
you are interested in some of the things
you heard today and care about the types
of optimizations we're doing or even the
language features that JavaScript offers
I encourage you to go take this survey
we put it up at bitly slash v8 lang
survey and this gives you a bunch of
upcoming features or proposals and asks
you to to rate them how exciting they
are to you and this will help us decide
what we come on stage next year and talk
about so that's all for today thank you
very much and my name is Seth Thompson</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>