<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Pushing the Limits of Kubernetes with Game of Thrones | Coder Coacher - Coaching Coders</title><meta content="Pushing the Limits of Kubernetes with Game of Thrones - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/freeCodeCamp/">freeCodeCamp</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Pushing the Limits of Kubernetes with Game of Thrones</b></h2><h5 class="post__date">2018-04-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9MOZ7-CvJew" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let me say it's a huge owner and let me
first thank the Academy hey wrong stage
all right my name is Ilya this is Z and
today we'll talk to you about HBO
journey to kubernetes the journey which
began not that long ago from not having
a single service running inside the
container to hosting gaming from season
seven on kubernetes Z and I will split
this talk I'll tell you about why
reasons and Z will cover how we get it
done okay it's downhill from there it's
boys come to the shows that everyone is
talking about from the groundbreaking
series to the documentary sports to the
biggest blockbuster movies available
anywhere for over 40 years people who
love entertainment have recognized HBO
as the original the first and the best
place to find world most innovative
programming HBO digital products is
represented by HBO GO
which is part of your TV programming
subscription through cable satellite or
other providers and HBO now subscription
directly with HBO both provide unlimited
access to HBO programming just about on
any device
it brought sense digital products is
everything and anything to do with the
content streaming and user products
that's where Z and I work and platform
team so if we zoom if you look at the
HBO streaming services this is how they
could look like from the mile-high view
this is not the actual image but
resemblance is pretty close and if we
zoom in they could be best described as
a matter of API services written mostly
in OGS also we added more immersive
written and go and all sources were
deployed into ec2 into single service
instance single ec2 instance paradigm
all wrapped without a scaling group
which handled both deployment and scale
all front it with the internal or
external load balancers and route route
he handled a DNS needs overall it was
and still is tried and true set up
running services and a double years it
works for general case however if you'll
see next that HBO case is anything but
general HBO traffic pattern can be best
described as the wall and this is just a
random example of how fast Lee
Bax I started on Sunday night during the
game of Thrones season premiere a season
around 6:00 p.m. known as the price as a
prime time and I think this point in
time can be best represented by this
image in terms of what our API source is
faced with as well as the emotional
state of our engineers and so looking at
Game of Thrones a traffic pattern
episode after episode season after
season left us with variants only
feelings about a future can we hold on
during the next episode
what about next season and the answers
were less than optimistic because we
were running into multiple problems
chief of them would be under utilization
Rani Nagi implies that you can utilize a
single CPU core at most now since we're
deploying in this ec2 instances easy to
Asus with single core typically almost
always do not have good Network so to
find good balance between network and
CPU we had to select instances which run
at least two cores of CPUs so right
there would not be utilizing 50% of ICP
use across all deployments under been
scaling is good however it's slow or
slower in comparison and sometimes it is
inadequate or slow to react to spike
traffic thus we have to over-provision
our deployments doubling tripling
sometimes more to accommodate for
unpredicted traffic patterns so you take
initial 50% under utilization buffer up
with over scaling and multiply by a
number of regions that will be a lot of
unused CPUs
so as 4lb goes that every service
communication requires alb and again
it's tried-and-true lb+
hg approach however even for internal
communications within say me pc who
required stand-up ELB for services again
that resulted in lot of ill bees which
led us to problems limits or otherwise
known things that we run and out of it's
ironic because we were under utilized in
more than 50% of CPUs and yet were
running out of all other resources so to
keep up with the resource usage we have
dedicated alerts which will fire up
every time we cross 80 percent threshold
utilization on given resources lbs SGC
purity groups and when we get modified
will contact AWS support to increase our
quota limits of course things got more
interesting when we begin to run out of
instances for given instance types or IP
addresses for given subnets well even
AWS could not help us with our problems
then of course there are external
resources that directly tied to his
account like a telemetry provider who
would calculate the license usage based
on the instances so this brings us to
kubernetes and rather than going through
this bullet point they're all true I
will tell you my personal story in fact
this is my second time being on the
stage at the cube con first time was in
San Francisco in 2015 when I was
summoned to stage just like today by
Kelsey Hightower but my great surprise
in horror because I was not supposed to
present what happened that walking
through the hallways of Keep Calm 2015 I
dropped my wallet some way and someone
have found it and returned it to event
organizers and he called me to stage to
return my wallet so I cannot simplify in
a better example of kubernetes saving
the day maintaining stories up and
running and prevented and outages
otherwise would be very interesting
flight home so we I was selling
communities from the server but we did
due diligence will look at messes with
this years we look at swarm we look at
the ECR and for us police wasn't still
is a clear winner we were just begin of
a journey and we had the very tight
schedule given that we had to consider
Izola services first and continues ation
and mass scale is a huge undertaking on
its own so once we began country in that
hill so we start playing with Publius
and NWS and again this is the end of
2015 a lot of change since then so what
we started with we would most basic set
up available around the cube up to
deploy kubernetes into existing pcs we
have to tweak and make some
configuration changes and what we needed
to do we need to show to appear store
bosses yourselves that companies know
this paperwork but more importantly that
kubernetes can be operated in AWS
cluster to host production great
services and once we get
we started cutting achieve with jenkins
infrastructure cluster and once we got
him successful that we began to
provision a home class that your
streaming services and that's when we
realized that basic setup simply one
carrot and we had more work to do and z
will tell you how we got it done okay
I'm gonna tell you something we learned
from our kubernetes journey so we create
our own Tower from templates for our
provisioning our clusters we started
before some of the community projects
started for example or Cuba wsk ops or
cube spray this allowed us to do
something really cool for example we can
deploy our clusters into our existing
AWS infrastructure by providing our VP
CID subnet IDs and security group IDs we
also had high availability in mind so
from the very beginning our minions and
master aSG's are multi easy the purpose
of the SDS are different though so for
masters we want to maintain a fixed
number of nodes so if one fails a the
base will automatically launch a new
master for minions we want to scale up
and down very fast so we use ASG to
launch a determinate nodes master is
also running in a chain mode meaning
that API servers are low balanced and
schedulers and cube controller managers
are running as leader the followers
despite be homegrown we keep
incorporating best practice from the
community we turned our o IDC or open a
d connect authentication for a coupie
API server so as long as our developers
github accounts are in HP organization
they will get a token for their cube
cut' or authentication tariff or modules
is a great way to promote reusability
and modularity
we create we created self-contained
tariffs or modules for both communities
masters and minions when we want the
launch of the cluster what we do is we
compose a third form template like this
and we will run terraform apply and bam
we have a cluster up and running several
weeks later we have had some experience
of how to operate a cluster and then we
notice stuff
problems so first we run Prometheus II
in the cluster to squirt metrics with a
provision I ops EBS volume as data
storage because our cluster skill up and
down all the time sometimes the minion
that hosts the prettiest part get
terminated we have to wait a long time
for premises part to come back because
kubernetes has to detach and reattach
the EBS volume the process could be very
slow and during that time we were losing
metrics the second problem is for big
events like Game of Thrones premiere or
finale or simply our regular load
testing we have to scale pre scale our
cluster up significantly to overcome the
wall effect Ilya just mentioned and
sometimes a de Bresse cannot provide it
sufficient capacity of our desired
instance type for the minions so these
issues led us to an improved version of
our telephone modules first we added
instance type arrival to our minion
module so that all the minutes launched
from this s you will get this particular
instance type we also added a tenth part
to the minion module and passed that as
a couplet startup flag so if this VAR is
defined then all the other minions
launch from this this SG will register
with that particular taint again we
benefit from the modularity of our
telephone code so for regular minions we
pass our maintenance type to the 2d
module we added another module to our
cluster we call a backup minion these
minions are exactly the same as our
regular minions except that they run on
our backup instance type which is C for
the 8x another module we added with what
we call reserved the minions so reserved
the minions are again exactly the same
as regular minions except that they are
tainted by this taint to reserve equals
true at the same time update our cluster
autoscaler so that when cluttered skills
down the mean our reserved dominions are
excluded so to summarize we add a 2 new
mini aSG's to our cluster to address the
issues we had earlier first if a device
runs out of
the maintenance type we want with care
about backup minion to bring more
capacity a second for Prometheus we
update the premises deployment to have
affinity to reserve the minions and also
tolerate the reserved taint so in this
way primitives part is not interrupted
even when cluster skills their flannel
was the networking layer we chose at the
beginning compared to other solutions it
was simple to setup especially before
the scene I came and it is included in
every caress distro the distro that we
use however when we were doing our
regular load test we discovered that on
the heavy load there were some problems
first it was increased latency and
timeouts both between parts and going
out of the cluster a second there was
UDP packet drop which impacted our cube
DNS lookups and custom metric collection
both of which are UDP traffic these are
just two issues that we saw very
frequently during the season to github
links are on a slide now let's talk
about different types of services that
we tried and the different ways we used
to get traffic into our cluster first we
provision earpiece for every node port
type of service and associated is the
obvious with the minion aSG's so in this
way all the minions launched from the
SGS will be registered with the obvious
automatically however there's a bit hard
limit of 50 yo be spurious tree and also
since we are provisionally Ogas
ourselves you have to keep track of them
manually
next is ingress which i think is the
most common set up out there so we put a
shared ear be in front of ingress
controllers and they'll be forward
traffic to ingress controllers which
then proxies traffic to upstream
services however there were some
problems with that too first when we
looked at the cloud watch metrics for
shared ELB and we noticed some 500
errors but which back-end services are
serviced exactly these 500 comes from
it's pretty hard to tell without
scrunching the ingress controller logs
or EOB
locks and second we notice that ingress
controller seems to be since you
struggle against a very burst or spiky
traffic that we saw and this this test
the set up produced more connection
timeout errors in our load test versus
the no port setup on the previous slide
and finally the publicity of your shared
ingress gob will determine all the
publicity of all the services a last but
not least we tried no balancer service
type which is cloud provider specific
dienes in this scenario
criminalists actually handles the
provisioning of the yellow bees and
registered a minions with the Yogi's
this method is not affected by 50lb
limit at first we noticed there was a
API interface API throttling problem a
second there was some ELB security group
customization issue that didn't get
resolved until recent 1.8 release so and
this is these are our choices for
services and ingress for production
services we use no port plus individual
Yogi's for non production services we
use ingress controllers and share the
Yogi's in both scenarios we use the
built-in service discovery for making
calls between our micro services pube
dns is always an interesting topic have
you ever looked at the result account
file in your part basically this is how
it looks like first you've got a bunch
of internal domain names to search for
and order and a depressed domain
internal domain names so this code is
actually from a part in default
namespace so you see the default or
service to cross your logo there I'm
second you got your service IP of crip
DNS the finally there's and those five
options so this option basically means
that there will be many invalid and
unnecessary
DNS lookups basically this is what
happens so for example we if you want to
resolve a DNS called PG s go back and
calm because it has less than five
dollars in it we append all search
domains and try them first before an
actual DNS query happens
so why didn't does fire was chosen was
explained in detail in this ticket by
Tim Harkin for reasons like same
namespace lookups cross namespace
lookups and of course crush the
Federation and the next foreign slide we
will share some of the tunings we have
done to reduce those embed in lookups
okay these are some of the tunings that
we found very important to us
first is the cache size of DNS mask
container I think a default is somewhere
around 100 or 200 but you should set it
to max which 10,000 and less memory is a
really constraint in your system well
set in to 10,000 ordering cost you
additional a couple megabytes of memory
- - address flag it is a really big
performance booster so this flag tells
DNS mask to return an IP address for
specified to my name
however we use it slightly differently
and we specify a whole bunch of invalid
domain names that were created by in
those five and we do not specify a IP
address so effectively for these invalid
domain name lookups could DNS mask will
return not found immediately instead of
doing an actual look up this way we
speed up stings a lot so if you haven't
taken a look at your Kube dns deployment
I recommend this flag and finally if you
have some internal name name servers you
want to hook up the - - server flag is
for you all your parts will be able to
resolve internal domain names without
additional changes and with that I'll
hand it back to Ilya thanks
quick word about telemetry it's not
surprising that we didn't have any
containerized services before we
couldn't take almost anything from a
telemetry stack two companies except
Splunk and it was a Splunk telemetry
team did some heavy customization and
tuning for Splunk forward to get
reliable logs everything else on this
slide is was new technology to us and i
think it was a great thing
zero dimension special case for reserved
instances for stable service like
Prometheus and we also love Prometheus
how
run in EBS with the availability zones
and not affinity for Prometheus juggling
of it's not fun at all
and speaking of PBS it can have some
interesting mountain and the mountain
times so we related route with a great
success and we just didn't risk to put
in production before Game of Thrones
season however we excited to see you
become an ciencia project it's been
submitted to toc okay so for seed
advisor is one thing to consume metrics
from the Advisory run a new
infrastructure cluster with few notes to
CPU cores each and couple Jake is pod
deployed it's totally in order to run on
the 300 node cluster with 40 cores each
and more than twenty thousand containers
deployed consuming metrics at that scale
felt like drinking out of the firehose
and we had to do some extensive metric
tuning filtering and permit his memory
adjustments to get metrics in reliable
state so when do you know you're already
ready for Game of Thrones season premier
for us it ball down by seven up the bar
and in terms of threshold viewership and
ramped up speed and beating it with a
low tests so for about two or three
months leading to Game of Thrones
premiere season premiere we ran a weekly
mega load tests and first attempts were
just beautiful it loves us in ruins and
that's when the real work began it began
on both fronts on services side a
service engineer did some heroic job
investigating issues and fine-tuning
services to accommodate for new
environments and a granade aside when we
begin to look for issues reporting if
none were found and fixing what we could
slowly we began to purge in better shape
gaining more confidence in kubernetes
and sources running on it if there is
any moral to a story after successful
Game of Thrones Season seven and
kubernetes it feels it feels good it
felt good to be right perhaps for the
first time by making the right choice
and if there is any advice we can give
is trust yourself trust your team
succeed the small things and you'll be
well positioned to succeed at big
initiatives and it won't always be a
smooth ride but you and your systems
will emerge in better shape than you
will in for us many problems we found in
our services
were not caused by kubernetes that were
there all along kubernetes made them
more visible so as mentioned earlier we
looked in alternatives however the
biggest and in undeniably most important
reason why we chose kubernetes was
vibrant in active companies community
without all the github issues and fixes
without C groups and slack channels
without meetups and coop cons just like
this there's high chance that the
journey would not end well and will most
likely end up like this to guys but
likely didn't happen and here we are at
cube Cantillon a success story of Game
of Thrones in seven and kubernetes thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>