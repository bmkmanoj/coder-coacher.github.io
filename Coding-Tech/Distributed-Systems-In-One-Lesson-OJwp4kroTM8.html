<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Distributed Systems In One Lesson | Coder Coacher - Coaching Coders</title><meta content="Distributed Systems In One Lesson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Distributed Systems In One Lesson</b></h2><h5 class="post__date">2017-12-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OJwp4kroTM8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everybody time to get
started I am Tim Berglund this is
distributed systems in one lesson these
guys realize that's not what they wanted
to be see you guys if you're expecting
to be in distributed systems in one
lesson then you are in the right place a
little bit about me I already said my
name I'm Tim as of a couple of months
ago I work for a company called
confluent and a lot of my attention is
shifting to kafka and that that whole
ecosystem that's what confluent does I'm
the director of developer experience
there so my team is in charge of
documentation and evangelism and making
sure meetups are staffed and training
and all those things so it's not a big
team yet but we're hiring and the
exciting time to be at confluent also an
exciting time to be here I realized just
so you know I picked up a bottle of
water and I thought it was still water
and it turns out it is light sparkling
water and do you guys know when you have
a microphone if you have a chance
between still water and sparkling water
always go with the still water so I
might not drink very much of this anyway
we got about 50 minutes to spend
together and I want to talk about what a
distributed system is and why they are
terrible now this might seem strange
coming from a guy who makes his living
in distributed systems and has for some
years but that's kind of a theme of this
talk is you really if there's anything I
can do to talk you out of building a
distributed system and choosing a
simpler life I want to now I know I'm
gonna fail I know you're gonna do it
anyway but we'll talk about the various
things that go wrong when we try to do
simple things like store data compute
data move data from one place to another
all of those things are relatively easy
all at small scale on a single computer
when we get multiple computers involved
they become terrible so I'm gonna show
you the things that go wrong and for
each category of things we'll talk about
the storage and
mutation and messaging for each one of
those things I'll try to dive into an
open-source technology that addresses
that problem that's really what we're
doing today kind of an introduction to
some topics and introduction to some
open source technologies that solve the
problems that come up when we build
distributed systems a definition might
be in order
Andrew Tannenbaum defines a distributed
system as a collection of independent
computers that appear to its users as
one computer so you might say something
like amazon.com I guess it looks like
one computer so that would be a
distributed system a Cassandra cluster a
Kafka cluster all of these things at one
level of abstraction or another they act
like a single system but in fact they're
composed of many computers now
distributed systems have three
characteristics for something to qualify
as a distributed system it has to be
made up of lots of computers they have
to operate concurrently that's kind of
obvious right there are these computers
are all running at the same time they
might fail independently and the
possibility of one of our machines
failing is a specter that is always
looming over us we just have to deal
with that and here is those are frankly
are too trivial things this last one is
a little subtle and this is what makes
things interesting
the computers don't share a global clock
so if you ever have a collection of
processors that are in some sense
synchronized there that their clocks are
synchronized that technically is not
going to function as a distributed
system and all of the various terrible
things that happen when we build
conventional distributed systems some of
those terrible things won't happen if
you have a global clock but that's just
not practical that's not how we build
real systems in the real world so we
have independent clocks independent
failure and of course computer's
operating concurrently and that's what I
mean when I say distributed system so as
I said we will dive into three topics
here storage computation and messaging
now in the single server single process
or even single thread version of these
three things life is great
and these are more or less solved
problems and everybody knows how to do
them like everybody knows how to use a
single master a relational database
that's a that's a fairly commodity skill
everybody knows how to do computation in
a single thread if you're a programmer
at all you know how to do that but when
we try to distribute these problems more
broadly life gets real hard and like I
said I'll look at each one of those and
it's it's my it might my goal here again
is to convince you don't do this
get an easier job build a smaller system
live in the country
slower pace of life I'm just looking at
you I know you're not gonna do it I wish
you would I'm trying I got I got a few
minutes left to convince you but let's
let's see where I get also by way of a
shameless plug
there is also an O'Reilly video
available on Safari
with the same title distributed systems
in one lesson that's like three or four
hours long I go into more detail and yes
by me so that's the shamelessness of the
plug if you're interested in one more
all right let's talk about storage now
to remind you single master storage the
the pleasant life you used to lead the
salad days the simpler times
when you have a database that exists on
one server it's very simple but life
gets hard life gets complicated and if
you could imagine the behavior of a
database and if you had like a slider
where you could turn the scale of the
system up and down certain things happen
to us as we crank that scale slider up
and the system gets busier and busier
now typically in in a lot of web systems
there are more reads than writes right
there's more read traffic than write
traffic now in a relational database
reads are usually cheaper than writes so
that's okay
but imagine as we're turning this scale
slider up at some point this single
server runs out of gas we've got all
these reads happening not as many writes
happening and we need to scale reads
what do we do we all know the answer we
do
read replication okay that's easy enough
here we'll take rights that go to the
master and will propagate them to the
two follower servers and you're able to
do reads against those keep the rights
going to the master and you buy yourself
some time right as this scale slider is
turning up now you've broken something
already you've created a distributed
system a distributed database it's just
not a particularly intentional one yet
and what is the thing that we have
broken we have broken consistency when
we were back in the simpler days can't
say I didn't warn you
anytime I write to this database and
then I read from it I am always going to
read the thing that I wrote that is a
guarantee and you know for any for any
database were ever likely to use we're
gonna read the thing I write now when I
do to go to read replication that's not
necessarily true I'm gonna write here
and when I read here that read may not
have propagated yet so I now have an
eventually consistent database
accidentally and the the bummer here is
that I may be telling myself well wait
this is a relational database I've got
strong consistency guarantees it's okay
but I have broken that so a little bit
of life has gotten bad also this only
works for so long as I continue to turn
up that slider what happens well I can
keep adding follower servers for reads
and scale my reads as much as I want but
all of the writes still go here and so
I'm gonna run out of gas on the the
master on the the leader server so then
I do an even more terrible thing and
that is sharding now again this is not a
new technique people have been charting
relational databases for approximately
as long as there have been relational
databases but what I do here is I will
find some key I'm using name here and I
basically just break up the database
into three pieces based on that key so
if I'm storing people's settings based
on their user name let's just say I will
pick username as that key and put a
through F here F there in here and n
through Z as my people say on the last
server I suppose I should say Zed
now I'm gonna keep saying Z alright and
each one of these is its own little read
replicated cluster so I can sort of make
the database as big as I want there but
I've broken something else I already
broke consistency and now kind of I've
broken my data model if this is a
relational database and win one scales
like this it almost certainly is a
relational database now for example I
can't join across shards I've got
completely independent databases from
shard to shard and that's a little bit
of a bummer that works sometimes I have
a talk later this afternoon where we're
going to talk about sharding as a way of
scaling an entire system and in that
case charting a whole system sometimes I
get away with that real well sometimes I
don't and can be a bit of a bummer
anyway briefly that's just what you do
with a database that's not built as a
cancer as a distributed database that's
how you can make it get a little bit
bigger again the bad thing is certain
behaviors and guarantees and claims of a
relational database that I start off
with little by little I give those up in
fact it's even a little worse than that
there there are other things if you can
imagine that scale slider going up and
up I have to do things with the topology
to address that I have to add computers
and make a distributed system I'll also
have to take things away from my data
model as I turn that slider up right so
suppose for example the the your
database comes under pressure latency is
going up you're having higher
transactional volume and suddenly reads
are not performing properly what are you
going to do and what's the one of the
first things you'll do in response to
that most people will say at this point
add an index kind of an okay answer
right maybe that'll help maybe you're
missing an index that's good
suppose you've already done all the
index optimizing you can that's trivial
after that what you do is you you
realize oh wait the reason reads are
slow is because of this wonderful
relational model
I'm doing all this joining because my my
date my schema is properly normalized I
have to take joins away in order to make
reads perform and how do you take joins
away
well you denormalize which we all know
is immoral right that's what we're
taught in school so you actually become
what you think is a terrible person and
you do normalize because you your scale
is forcing you you know your you have no
choice also if your first answer was to
add indexes that is another pressure
that you'll come under on the right side
right as as right right traffic
increases before you shard or even after
you shard you're likely to give up on
indexes which will then have further
pressure on you denormalizing your data
model down the line so all of the
wonderful things that you thought were
true about your database when you start
it they're just gone they're taken away
from you by the time you have to scale
it and you end up with something that
doesn't really look all that relational
by the time you're done so why not just
do it right and here is an approach that
is taken by really I'll be talking in
terms of Cassandra cuz it's a database
that I know fairly well but this is sort
of if you look at the the scalable
non-relational databases that family of
products they all kind of work like this
they use a technique called consistent
hashing in fact consistent hashing is a
nice thing to know when you're looking
at other systems and other open-source
projects in the distributed systems
world this method is often come pokes up
his head is like winking at you from
various systems its front and center in
Cassandra but it shows up in other
places so just be advised now here's an
example of how you build a database from
the ground up to be distributed now it's
still doing bad things to you all right
there are still lots of features
cassandra is not gonna give you that you
wish you had but it's very upfront about
it right right from the beginning it's
saying here are the limitations of the
data model here's how scale works period
and and sort of just deal with it and
move forward from there so suppose I've
got this database here I've got eight
computers and they're all the same
they're all peers it's a pretty simple
architecture the
just identified with a numeric token and
in the case of Cassandra we always draw
this in a circle now that's not because
they're actually connected that way
you know these are regular computers on
a net network just like you're a normal
person but they all have this unique
token this unique numerical ID and it
helps to draw them in order in clockwise
in a circle and I'll show you why
right now I'm storing favorites and per
the coffee picture I forgot to say this
at the opening slide as many of these
examples as possible I will draw from a
coffee shop just to be cute so suppose
this web-scale coffee shop is trying to
remember people's favorite coffee
basically I'm an Americano guy sometimes
if I'm feeling a little crazy almond
milk latte
that's about as fancy as it gets what we
want to store my my favorite coffee
americano what we do is we take that key
value pair Kassandra is not a key value
database but this is a an easy
abstraction to kind of get on board and
this is as far as we'll go here in this
summary take the key and run it through
a hash function it doesn't matter what
the hash function is as long as it's
always the same hash function we hash it
we say okay where is that number gonna
fit in this ring we want to see okay
it's bigger than 8,000 it's less than a
thousand so just based on the way those
arrows are going I'm gonna write that
key into that node simple as that
now the cluster can grow the hash values
can change and as long as internally the
database moves keys around as those
things as those things change all I ever
need to do when I'm writing to the
database is look at it and see what are
your tokens right now how many nodes do
you have water your tokens and hash my
key and I know where to write and I know
where to read from later on if I say hey
what's Tim's favorite coffee
there's a new barista he doesn't know
what my favorite coffee is it's fine
this is web-scale he can't remember all
those things will hash that will say oh
nine f72 fine that's gonna come from a
thousand I'll go there get that key and
pull it out
so that's basically how you read and
write with a consistently hashed
database of course we would never want
to have just one copy of a piece of data
that would be crazy so we can do things
like replicate if I say well you know
the the first copy would have gone to
this node I'll just go to the next two
nodes down the ring in the cluster and
put a copy on those nodes as well right
that fixes the problem of now that I'm
in a distributed system what was my
second characteristic computers fail
independently we know this we have no
way to predict when a nodes gonna die I
would never want to key in just one
place so now I've got three copies of it
I've solved that problem I'm now able to
tolerate hardware failure and be up
sounds great I told you distributed
systems are terrible I created a problem
by solving one and what is that problem
consistency I have three copies on three
computers of something that can change
I'm a fickle coffee drinker it's
americano today tomorrow maybe it's
triple skinny two-thirds decaf half
chocolate no whip grande mocha with
extra foam you've no idea what I might
come up with tomorrow I am the very
definition of mutable state so now that
I have three copies of something that
can change I have created a consistency
problem and two to be clear one of these
if I if I write tomorrow my new that
crazy thing I just said maybe one of
these nodes is down while I'm doing the
right and it doesn't succeed and it has
an old copy of the favorite coffee and
the other two nodes have the new copy
entirely possible and then when I go to
read how do I know who's telling you the
truth
all of these things become problems now
basically I want to give you a rule for
thinking about consistency in a database
like this when I write to the database
I'm gonna require that a certain number
of nodes take the right so as a client
client application I'm gonna say you two
at least two of you need to succeed if
only one of you succeeds I'm gonna
consider it to be a failure or I'll say
only one of you needs to succeed I know
I have three replicas but I'm in a hurry
I'll take one those are options that I
have I could be I could be particularly
fastidious and say I need all three of
you to succeed nobody would do that
really just one and two are the
practical options likewise when I read I
need to say well I'm gonna hear from two
of you or one of you on a simple formula
here for knowing whether you are using
your database in a strongly consistent
way if the number of nodes I write plus
the number of nodes I read is greater
than the number of replicas then I have
strong consistency so if I if I write
one and read one and there's three
replicas that's eventually consistent if
I write two and read two and I have
three replicas that's strongly
consistent because andrew is a pretty
cool database if it's if you want a big
database it's kind of the way to go and
it lets you it lets you tune this on a
request by request basis you can you can
change your consistency semantics
on-the-fly pretty cool so even though
everything's a problem in a distributed
system and solving one problem creates
another one there are some things that
have elegant answers and and cassandra
is one such alright the cap theorem
should we talk about the cap theorem
just a tiny bit the it back when no
sequel was new it was like 2010 reading
hacker news in the morning which is the
thing I do it's like my newspaper it was
painful because about once a week there
would be a new and newly misinformed
blog post that would make it to the
front page of hacker news that was
something about the cap theorem
everybody cared all of a sudden because
everybody was making scalable databases
and so people had to blog about it and
say things that weren't true it's really
not hard to grasp and it comes up
sometimes still in distributed systems
discussions so I want to give you a
simple way of understanding it the cap
theorem says if you're building let's
just
make it simple a distributed database
you you want your database to have three
properties you want it to be consistent
that is that's the see when I read
something I want that to be the most
recent thing I've written so I write and
then I read and I see what I read I want
it to be available and that simply means
when I try to write it works and want to
try to read it works I also want it to
be partitioned tolerant that means it's
a cluster I've got lots of nodes being a
database together and if some of those
nodes go away for a little bit and then
come back together I don't want the
database to completely fall apart I want
it to be able to deal with the fact that
sometimes parts of the system can't talk
to the write the rest of the system
that's partition tolerance so imagine if
you would you're in a coffee shop like I
said we're oh wait I forgot this slide
consistency availability partition
tolerance you guys got this all right
imagine you're in a coffee shop
forget about databases for a minute and
you are writing something together
you're working on your screenplay
with a friend your co-writer and your
old schools you're doing it on a yellow
legal pad with a pen it's you it's not
me I don't know why you're doing this
this way but you are and both of you
have a copy of the screenplay
one of you makes a change here the other
one makes a change on their copy that
person makes a change you delete
something you keep your copies
consistent because it's how you want to
do it it's a little strange but it's
what you're doing the coffee shop closes
I'm not gonna judge you I don't know why
you're doing it this way but you just
our coffee shop closes but you're in the
zone so you go home and you keep going
over the phone and you keep in sync over
the phone you say well I added this line
of dialogue I removed this scene heading
and you're keeping things up to date but
then your battery dies and you can't
talk anymore you lost your charger you
now have a network partition you keep
writing because you're in the zone
presumably your partner keeps writing
your spouse comes in the room and says
hey how's the screenplay going read me
back your most recent page which is
great that your spouse is so interested
I think here we have a decision so back
to the Venn diagram here you can't have
all three of these things that's the
fundamental inside of the cap theorem
in the presence of a partition in this
situation you can either choose to be
unavailable that is you say to your
spouse I can't tell you because I might
tell you the wrong thing and I would
hate to do that that would be unfair to
my writing partner or you could say well
I'll just give you an answer it might be
wrong so I could I could give up on
availability and keep consistency by not
answering or I could keep availability
and give up on consistency that's the
fundamental insight of the cap theorem
is I can only have two of these three
good things I always want all three but
sometimes I can't have all three it's
again there's a there's a lot of writing
on the internet about what this means
and what's implications are and when it
applies and so forth but just think of
distributed databases it's a simple
trade-off between those three things
that you want all right
enough of storage let's talk about
computation again computation is super
easy if you have just one processor if
you go to multiple threads on one
processor and you try to do computation
in a threaded way that's usually pretty
terrible and there are talks at this
conference about ways to avoid that
terribleness and you say but you got
multiple computers and you're trying to
distribute a computation task among
multiple computers life gets really hard
and we've created some truly terrible
tools for solving this problem the good
news is life is getting a lot better
trying to do this ten years ago our 12
years ago was bad trying to do it now is
starting to not feel like somebody is
hitting you with a 2x4 so I think
there's a lot of a lot of reason for for
optimism and MapReduce is not one of
those reasons but let's talk about it
just to make sure we're on the same page
and you know the badness that we've come
from so the typical way to explain
MapReduce is
with word count and this is a union rule
people I have to do this so I'm gonna
use a word count example just to show
you basically what's going on here so
I've got this big chunk of text now if
I'm actually oh this works really well
in the US cuz everybody knows this
it's an American poet he's from
Baltimore his name is Edgar Allen Poe
the American football team from
Baltimore's anybody know the name of the
team
the Ravens because this poet is from
Baltimore this is his most favorite poem
it's an amazing poem strongly
recommended and I'm not even a football
fan but I kind of love the Ravens for
this reason anyway if you actually had
the text of the Raven I think it's about
1800 words the right way to count that
is to use the WC command
don't screw around that's a small data
problem but this is helpful just to see
how Map Reduce works Map Reduce funnels
all of your computation through two
functions one called map and one called
reduce that is a terrible thing to do to
a person but here's how it works
I would dump the text into my map
function and what map would do is it
would it would tokenize it it would
split it up into words and then it would
turn those words into key value pairs
this is I'd like to go through this
because this is kind of a neat insight
into the way you would solve a problem
like this in a distributed way what we
begin by doing is counting words that is
what I have just done here that's the
first step in the MapReduce canonical
MapReduce word count example is you
count the words now what people say is I
tokenize the text and I turn the words
into key value pairs the word is the key
and the value is the number one really
what I have done though is I've just
counted each word and I've seen each
word one time it's like very naive
stateless counting I don't remember
whether I've seen the word a or K more
rapping before I just count each one one
time all right
then there is a step in between that
always gets ignored it's called shuffle
so after I do that mapping I count all
my words once then potentially I have to
move these words or
the network two different computers to
make sure similar words are near by each
other so at or rather a rapping tapping
I want to get those in order then I take
these collection of collections of
similar words and I dump them into a
function called reduce and what reduce
is going to do is basically add up the
numbers so it's a less naive counting it
says well I counted the word a three
thousand times
let me just add up all those numbers now
I am skipping a bunch of steps here in
the spirit of summary and a bunch of
complexity here in the spirit of summary
but this and I end up with with counted
words and I'm able to do this I'm able
to split up the work of mapping and the
work of reducing and distribute it among
lots and lots of computers this is a
really good idea if your data is already
say in a distributed file system if
you've got so much data that you need
three hundred computers to store it all
and replicate it all then doing your
compute this way see it seemed like a
good idea at the time and it was a
moderately good idea a lot of people
built I shouldn't say a lot some people
built systems using MapReduce that were
worthwhile all right and it by itself
MapReduce is just a pattern it's not a
product but it's the pattern the compute
pattern used by Hadoop Hadoop is still
widely deployed rapidly becoming in my
view in the view of many others a legacy
technology it does not is not the cool
new thing anymore at all but you should
know that whole MapReduce thing yep
that's the fundamental layer the bottom
of Hadoop and the other the other I
think more long-lived
part of Hadoop is the distributed file
system HDFS that is in the words of one
of its co-creators a cockroach nobody is
really building any new MapReduce
systems anymore that's not you don't see
that in lots of new designs HDFS is
going to be with us until our
grandchildren
writing software I think it's it's
probably going to be a very long-lived
piece of infrastructure and for good
reason it works just fine
Hadoop spouted an enormous ecosystem and
not necessarily in a good way in in my
opinion at least it became very very
complicated because the underlying
program auto programming model was so
terrible that we needed this big
ecosystem of more pleasant layers on top
of it and that's kind of what happened
things like hive grew up and that became
a relatively pleasant way to use Hadoop
actually programming MapReduce not many
people did that which is why a tool when
we can we keep going through the world
of distributed computation a tool like
spark has really taken over the the mind
share of MapReduce in the last three
years the way people have been talking
about spark at least you know when it
was new they would start off by saying
hey this is a way better thing than
Hadoop just believe me and you know
MapReduce is dumb and then you start
looking at spark internally you can
learn the API you're like yeah this is
way better than MapReduce instead of map
I have transform instead of reduce I
have action I like it so that the
underlying paradigm is still very
similar we still assume we've got a
bunch of data sitting out on a bunch of
computers and then we need to go to that
data and do stuff to it and if that's
what you're gonna do then that basic
scatter gather paradigm is always going
to happen and it's probably always going
to follow that basic Map Reduce
sort of duality now sparks insight was
to say let's create an abstraction on
top of the data that's officially a part
of our API and originally the the in
early versions of spark that abstraction
was the RDD now kind of the front facing
abstraction and spark is a thing called
a data set it's very similar but in
contrast to Hadoop spark gave you an
object that you could program you felt
when you're writing MapReduce
you feel like like someone is hitting
you with a stick when you're writing
spark code you feel like a programmer
you've got an object that represents
some big collection of data and what do
you do you call methods on that object
and it does stuff to it now maybe
they're essentially map and reduce that
are that are just have a little bit of
syntactic sugar on them but the spark
programming model is so much more
developer-friendly than Hadoop that
MapReduce has even in in the eyes of the
Hadoop vendors at this point essentially
been completely taken over by spark and
it deserves it spark is a much much more
pleasant way to write code it also
doesn't have storage opinions Hadoop's
MapReduce really said you could put data
anywhere you want as long as the HDFS
spark says of course I will operate on
HDFS data I'll operate on Cassandra data
I will operate on dang park' files and
s3 I don't care it just wants to make
rdd's and datasets and let you transform
them and do actions on them and do that
distributed computation now the
assumption is still in the world of
spark again I've got a bunch of
computers I have a distributed storage
problem that's why we started with
distributed storage I have I have a
distributed storage problem and now I
have to do compute over all that data
that's sitting out there so if I have a
distributed file system or a distributed
database that's that's data at rest out
there then spark the spark paradigm
still makes sense Hadoop is the
MapReduce paradigm really you could
think of that as yesterday's news and
spark is more of a today thing now kafka
it's not quite time for Kafka's day in
the sun but because we're talking about
computation and in the last year
Kafka has entered this world I should
mention it is an approach to distributed
computation in which everything is a
stream so as long as you've got
streaming data you can do computation on
that streaming data as the data is in
flight you don't have the assumption
that you have put the data out somewhere
and you're gonna go to that data and do
compute on it this is a stream
processing framework
we're streams our first-class citizens
and you don't even need a cluster to do
your computing spark importantly also
has a stream processing API and it's not
just API there's actual infrastructure
in a spark cluster that helps with
stream processing there are other
solutions to the stream processing
problem but Kafka needs mention here
because it is not just a messaging
messaging framework but is also a
computation framework now all right we
are at about the t-minus 15 minute mark
oh you know what I didn't say at first I
didn't say if you have a question ask it
did you assume that it's a little late
now because I won't add will never leave
time for questions at the end I always
prefer that those just happen when they
come too late now you know if you got a
question to ask it the worst thing I'll
say is I don't know all right let's talk
about my third promise and that is
messaging messaging is great because it
lets us loosely couple systems and that
lets us build little chunks of
functionality and really release them
and version them independently and still
have them communicate and does that
sound like anything we do these days if
I say little chunks of the system that
are released and versioned independently
that sounds kind of microservice ii and
that's amazing so messaging is because
of microservices in the last three years
messaging and streaming I'm seeing come
way more to the foreground in the last
year or so people are realizing that
micro services are a super good idea for
a number of reasons and people have been
struggling with how to get them to talk
you know early Mike maybe this is your
micro services deployment and I should
be respectful but kind of an early
approach to micro services is well let's
just have a bunch of little pieces of
code and one big database and they talk
to the database people realize that was
a terrible idea it didn't really work
and so getting those micro services to
talk
suddenly messaging is coming back into
the foreground so it's a way of loosely
coupling systems we usually use the term
subscribe
burrs or producers sometimes subscribers
we might call them consumers all these
kind of words we almost always a
messaging system organize messaging
systems organize them into named topics
which are just namespaces for similar
messages and there's usually this this
this term broker which is just a
computer in the messaging framework we
typically think of messages as being
persistent over the short term now it's
up to you how you define short term the
New York Times is a Kafka open source
Kafka user and they keep all of their
newspaper content going back to the
1860s in Kafka so you get to set the
data retention yourself and believe me
1860s in America is a long time ago so
that's that's a lot of newspaper data
for us now yeah the basic idea here is
you've got a producer it sends a meta
sends messages into a topic and then the
consumer gets to read those out at its
own rate the problem is when that
producer gets big what if a topic gets
too big for one computer now how can it
be too big well you can retain too much
data suppose you want to retain data
into the 19th century hey again I'm not
going to judge it's your system so
that's big or your messages are big or
you are transacting messages too fast
for one computer to keep up maybe data
retention isn't a big thing but you're
reading and writing too fast for one
machine given the fastest machine you
can reasonably deploy right now what if
one computer is not reliable enough
remember they break and you would like
your system not to go down what if you
really want to be able to deliver to
guarantee delivery even if a computer is
down
well enter Apache Kafka I should say
when you have a messaging system and it
is a single server messaging system it
can do all kinds of
tastic things like it can trivially
guarantee that messages are delivered
exactly one time what it can't do is be
particularly resilient to failure or
particularly scalable oh I should say
also it can always guarantee that
messages are ordered this is a message
queue after all we would expect messages
to be ordered and when I have a single
server message queue or message broker
then everything's great my messages are
always going to be in order I'm going to
have to give up on some of those things
when I scale but I'll get things in
return and it's as it turns out a lot of
awesome things so Kafka now just a few
quick definitions a case I use these
words of course message that's the basic
thing topic I've already said that
producer this is a client process
outside the Kafka cluster that puts
stuff in consumer this is a client
process outside the Kafka cluster that
takes messages out and broker is simply
one of the computers that the Kafka
cluster is made up of so a a broker will
have many topics for example I'm not
really deep diving on Kafka here just
like I was in deep diving on Cassandra
so it does things like replication also
and it answers the consistency question
also in various ways so all of those
problems come up and all of those
problems are solved by this particular
distributive message broker but the
money in Kafka the really interesting
thing is when we get past just being a
message queue so I want to show you how
it works as a message queue and where
the distributed system ugliness creeps
in
like what Kafka does to you that's mean
because it's distributed and of course
what we get in return for that then I
want to talk real quickly in closing
about what happens when we have made
everything into a message like how how
thoroughly can you live that life and
what sorts of things happen when you do
so Kafka all right well again
producer-consumer topic this is the
trivial account of Kafka Kafka as a pipe
and this is currently how most people
think of Kafka because it's is it's
value-added features that are more than
just pipe are really only a year old so
let's just go through the pipe version
when that topic gets big what do we do
of course we partition it we'll just
split that up onto several computers so
let's say instead of one computer I've
got that single topic partitioned over
three computers now each of these
partitions of course I would also
replicate just put that to the side for
a moment we're not going to talk about
that we've got these three partitions so
these three rows here are those three
brokers each of which has one partition
of the topics these are independent
computers the gears over there are
producers they're going to produce
messages and these diligent people
working at like 2010 IMAX those are the
consumers so when I create a message as
a producer what's gonna happen is we're
gonna look at some part of that message
now maybe I'll just hash the whole
message maybe I'll look into the Fiat
look into a field of the message like
user name or IP address or something
that means something to me as a producer
but I'm gonna take some part of that
message up to all of it and I'm going to
hash it now this is like consistent
hashing over there poking its head up
and winking at you like I warned you it
would I'm gonna hash that message mod
the number of partitions and that's
gonna tell me what partition to write to
within each partition I will be able to
keep things in order each partition is
ordered so I'm gonna keep writing in
here and I will kind of randomly really
or uniformly have these messages
assigned to my partitions oops I
consumed one let's not consume that yet
now already you can't remember what
order these happened in system-wide our
topic wide ordering is lost because this
is a distributed message queue
I can only have ordering within a
partition that's a limitation I have to
accept I don't get global topic wide
ordering that's a bummer I would love to
have it there is no day I will wake up
in the morning roll out of bed have a
cup of coffee and say I literally do not
want global ordering on my topics darn
it I wish I didn't have it shake my fist
at the heavens so it's not gonna happen
I would always want it but I can't have
it it's just not a possibility but I do
have it within partitions and so I
that's why I said that at the at the
producer level you can be smart about
what part of the message gets hashed and
so I can guarantee within a topic that
messages from the same user or from the
same host or from the same whatever will
always end up in the same partition and
then I can consume them in order from
each consumer and those consumers are
independent computers operating
independently scaling my system like a
boss so cool I have a message bus I have
solved the problem of distributed
messaging I already came up with a way
to solve distributed storage we talked
about one I came up with there are a few
different ways to solve the server D
computation now I've got this
distributed messaging system so all my
services can communicate with one
another through topics in this
elastically scalable essentially
infinitely scalable message bus that's
great and that's what I did yesterday
I would generate events into this bus
this is kind of how people start out and
then I would I would I would consume
service would consume one of those
messages and what do I do I write it to
a database so I've got this event and I
turn that event into a row and that row
sits in a place the data is sitting in a
place somewhere maybe tomorrow instead
of doing that instead of saying here's
an event I'll put it here and I'll go
back here when I need it
maybe tomorrow I could just do
computation on the events after all I've
got the giant message bus sitting there
and everybody has an API everybody is is
coupled to that bus through that API
they're producing they're consuming
maybe I could do that so without
that this this what is now really an
anti-pattern emerges about seven years
ago this was considered pretty cool and
it's it's the so called lamda
architecture that means I've got all
these events out here and I have to
build two parallel systems to process
them this is what happens if you don't
if you don't use your message bus
thoroughly I have one system that all my
batch processing takes place in that's
my distributed database those are slow
but they're very thorough those jobs up
there they can take minutes or hours and
then I've got this other system down
here that really is just my temporary
message bus thing and I might do little
bits of stream processing for quick
summaries all right this lamda
architecture emerges this is bad because
essentially what lambda did to us and if
you come to my talk this afternoon I'll
talk about this in more detail is it
forced people to write the same code
twice he had to write the stream code in
the batch code and yeah they say the
best code is the code you never write
well the worst code would be the code
that you write two or more times that's
a terrible idea so the the basic idea
once I've got a message bus and I say
hey look how about I stop obsessing over
taking events and writing them to
someplace in a database and why not just
let them be events and process them in
place through the you know through a
stream processing API at the very top
level I could do something like this
I've got a Kafka cluster and I've got
various services attached to it I've got
some service maybe that's doing some
analytics or some monitoring a service
that's doing some analytics I've got a
couple of little databases that's okay
for a microt service to maintain a local
database but all of these are services I
might even have a dupe of a dupe cluster
because it's still there we spent a lot
of money on it the CIO doesn't want to
get in trouble for not using it so you
have to keep using a Hadoop cluster I
might have an elasticsearch cluster down
here all doing different kinds of things
that my system needs based on this
messaging bus that they all talk to
pardon me so I already did this and I
just wasn't looking at the screen so I
have events out in the world that go
into that cluster and get propagated to
the services that need them and the
nifty thing here is each service doesn't
really need to worry about what other
services might be doing with it an event
is just an event it happens it's there
in a topic and a new service can grow up
and say hey I'm interested in that event
without having to have any coupling to
the way that event is produced or who
else might consume it so once I start
putting things into a message queue I
can be it could cause me to think very
differently about the architecture of
the system this can be an impactful
thing this distributed messaging thing
it's possible to go from my single
server relational paradigm to a
distributed Cassandra paradigm I have to
change my mind about some things about
the way consistency works about the way
data modeling works right like that's a
big deal when I go from single threaded
programming to distributed computation
that's kind of hard that's a tough thing
to do when I when I get streaming into
my blood suddenly the world is very very
different and I start building I start
building systems in a very different way
indeed now with that just about out of
time and we have covered storage
computation and messaging a brief
introduction to a few things that go
wrong and right when you build
distributed systems and how a few open
source projects solve them
shameless plug videos by the same name
on a Reilly if you care to learn more
absolutely not shameless plug for Kafka
meetups because this is what I do if
you're interested in more check that out
take a picture of that slide if you want
to go to a Kafka meetup I would love to
see you there sometimes I am even seen
in Europe not often enough but thanks
for having me here today have a good day</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>