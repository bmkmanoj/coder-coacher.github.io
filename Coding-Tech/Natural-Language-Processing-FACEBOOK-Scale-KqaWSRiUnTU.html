<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Natural Language Processing @ FACEBOOK Scale | Coder Coacher - Coaching Coders</title><meta content="Natural Language Processing @ FACEBOOK Scale - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Natural Language Processing @ FACEBOOK Scale</b></h2><h5 class="post__date">2018-01-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KqaWSRiUnTU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Roshan I lead the National
anguish understanding group at face book
before this I was leading the National
on your understanding group at City at
Apple so today I'm going to be talking a
little bit about why language
understanding is key to enabling really
compelling user experiences for people
on Facebook what are some of the
specific problems that we're trying to
solve some farad techniques for solving
those and give some examples of you know
really nice product use cases that would
not have been possible without really
sophisticated language understanding so
our kind of mission statement is we want
to be able to understand all forms of
textual and language content with newer
human level accuracy at face book scale
so text is everywhere at face book you
know even though people are posting more
videos and images over time text is
still looking in terms of you know your
Facebook posts your comments your
interactions with people on messenger
and you know being able to kind of
understand this text is really critical
to make sure that we can surface the
kind of content that is actually
relevant to people so let me give just
one example of that so when I log on to
Facebook you know of course I want to
know what my friends are doing but I
also want to know what's happening with
some of the topics that I care about
well so I care about machine learning
and I want to know about you know
machine learning conferences that are
happening nearby such as this one these
are actually my boss's screenshots he
cares about hockey I don't so I don't
really want to know about hockey but
substitute basketball there and you get
the idea so so yeah this is and this is
just one example we want to be able to
you know surface your content for the
topics that you actually really care
about and I'm going to be talking about
a lot more examples going forward as
well so you know when we say natural
language understanding what do we mean
there's a wide variety of tasks in that
bucket I will talk just about a small
subset that are really important and
interesting to us at Facebook and press
there's there's a lot more that we're
just scratching the surface off so let's
start out with you know text
classification and some of you might be
wondering you know isn't this a pretty
standard problem is in text
classification solved you know and yeah
you could kind of say that in certain
situations but it turns out that you
know people keep inventing ways of
messing up our models so here's here's
an example you know this actually
happened so in the right way and it's
very clear that the topic is actually
cooking in the left pane
you know two alphas is very clear that
topic is sports it turns out that our
models thought that the topic was
cooking and so when Joel made that post
he saw a bunch of cooking related posts
after that which is probably not the
behavior he had in mind and and so
there's all of these ways where you know
it's it's not as obvious as it seems at
first glance the other interesting thing
is that usually in literature people
talk about text classification into
twenty or thirty or hundred labels but
when you're talking about all of the
content that people can post on Facebook
we are now talking about tens of
thousands of labels and you know being
able to you know achieve 90% on a 10,000
label problem is much harder than doing
it for 100 labels and so that's another
challenge that we're constantly dealing
with similarly of course we also want to
be able to actually do word
classification and label you know every
single word in a post as well not just
kind of the overall post and I'll come
some more interesting examples of this
later on another really important use
case for us is actually content
similarity so given two pieces of
content you know how semantically
similar are these and again this is one
of those examples where if you just do
superficial stuff like looking at how
many words there are in common you know
it just it breaks down really quickly
and finally parsing named entities from
text is again is a huge problem for us
so you know if I if I post about you to
a bunch of times
you know if all I have is a topic model
I know that I care about music but maybe
all I really care about is u2 and not
all of music and so I really want to be
able to extract that entity as well and
this again is kind of interesting
because you know just to stick with this
example band names are really noisy
sometimes they're words that don't exist
in the English language sometimes
they're really common phrases and again
sometimes people might have typos
misspellings abbreviations but you want
to be able to capture all of these just
the same so you know this is kind of a
sampling of the kinds of problems that
we really care about in order to do a
good job of surfacing relevant content
folks and you know how do we how do we
actually go about solving these so it
turns out that you know we do something
pretty shocking we use deep learning
yeah so the the the the the really
interesting story over here is that a
few years ago when we were starting to
get serious about some of these problems
you know we were starting to get worried
about how to actually scale the amount
of you know feature engineering that we
would had to write for different
problems across different languages and
how we would make that work and you know
somewhat serendipitously around the same
time there were a lot of these seminal
papers including one from facebook that
showed that essentially being able to
run distributed representations of words
and sentences as inputs to a neural
network allow you to achieve stated we
have state of the art performance on a
variety of different NLP tasks without
having to do any feature engineering
sometimes even using the same model just
with multi task learning and so that
kind of you know that theoretical
insight came at just the right time for
us and so that's what led us to start
investing pretty massively in what we
call deep texture so this is our
internal machine learning platform for
language understanding the there's three
main problems that we really want to
solve the deep decks that motivate a lot
of our work even now so we want to be
able to solve a variety of these NLP
asks as I talked about before we want to
be able to support multiple languages
and we want to have a variety of
different you know neural architectures
in the same place so what we want is for
you know an ash language engineer to be
able to not have to worry about this
task versus that or training a model for
one language versus the other and
essentially be able to kind of attack
all of these different dimensions pretty
much at the same time the basic this is
this is the this is the only slide with
a block diagram I promise
the basic idea is actually pretty
straightforward you know we consolidate
all of our features in one place these
features could be as simple as you know
embedding representations of words they
could be you know scores coming in from
models elsewhere and Facebook the idea
is that you should be able to use
whatever feature you think is necessary
to solve the problem even if it's coming
from you know an entirely different
product or vertical you put all these in
a central feature store you allow people
then to pick their model and their
learning algorithm of choice and then
finally you provide a consistent way to
publish these to production either by an
a/b test or directly and so this is kind
of the platform that you know we've kind
of invested in to make sure that the
entire company can essentially operate
at the scale I'll talk a little bit
about some of the specific tasks tasks
and some of the insights that we've kind
of gotten from the work that we've done
so in particular I'll focus on
classification and sequence labeling so
you know I kind of alluded to what we
mean by text classification before you
know given a document where a document
is either a post or a message or a
comment we want to figure out you know
what's the versus the topic that we're
talking about here and actually this is
the least interesting slide you can just
throw it to an lsdm you know take the
run the entire document through the lsdm
take class reinstate put softmax on top
and you know this this is not
interesting in 2017 the thing that I
actually ended up being interesting is
the
when we use Khan nets for document
classification because these can be
fully parallelizable and like the
computation in an RNN we can make we can
actually make these work an order of
magnitude faster and not only that but
the fact that you're using correlations
and that you're processing information
hierarchically actually allows us to do
significantly better even on long posts
which is kind of surprising because you
might expect
LST ants to do better there and so you
know one of the interesting insights for
us was that it's actually better to go
with Khan nets for a document
classification pretty much anytime you
have that problem at Facebook there's
another really interesting new
architecture that Facebook research
published just a few months ago which we
call fast text so the idea here is that
your features now are not the words
themselves but they're engrams of
features and you don't learn embeddings
of words but you learn embeddings of
engrams and you kind of use a neat
hashing trick to make sure that your
feature space doesn't explode and then
your model itself is really simple you
just have one hidden layer and so kind
of the trade-off that you're making is
that the model is simpler but now your
features are richer and it turns out
that this actually achieves very
competitive performance to LST M's to
con Nets but is 30 to 40 times faster to
Train even than cnn's
and so what this gives you is that you
know you can train a model for a new use
case provided you have the data within a
few minutes and get a really good sense
of what accuracies you can expect and
what increments you need to make in
order to actually ship it and again kind
of putting this in kind of the same
place as the other lsdm of the konglish
models is again really kind of it
accelerates the pace at which people can
kind of seamlessly switch between these
different models talking about sequence
labeling a little bit again you know I'm
sure this is very familiar to most of
the people in the audience the main
difference over here is that you're now
labeling every token instead of the
entire sentence and so you don't really
have to do kind of an aggregation across
the sequence so for example if you're
using CNN you don't
need the pooling layer one thing that's
actually not in this slide but again
that we noticed and there's a bunch of
research from folks including at Google
on this as well is that instead of
predicting labels per token and
optimizing for each token prediction if
you stick something like CRF on top of
your LST mm and kind of jointly
optimized for your entire sequence you
actually get significant improvements in
recognizing named entities and again
that's it's hard to kind of shift that
at production because that function is
actually a lot slower than vanilla l
STM's or commnets so one of the things
that we spent a fair bit of time on is
actually making it such that you could
do this you know CN n plus CRF for LS TM
plus CR of combination work at Facebook
scale for every single post and message
and comment in real time so this is
actually one of my favorite models so
this is what we call kind of the two
tower models so I talked about content
similarity before and it turns out that
something that works really well is you
know if you have two pieces of text you
know run bonus run both of them through
a CNN or an LS TM take the hidden
representations put them into a single
softmax and then add a ranking loss on
top of that and this actually ends up
giving a state-of-the-art performance in
terms of figuring out semantic
similarity between two pieces of text as
opposed to just you know syntactic or
world level similarity I'm not going to
go too much into detail on this but
another problem that we have been
looking at a lot is what we call the
entity linking problem so you know let's
say my models can figure out that in
this case Barcelona is a named entity
and real is a named entity I still need
to be able to map them back to the
actual football clubs themselves and you
know not map Barcelona to like the
partial airport for example and so this
is another area where you know we're
kind of investing in quite a bit because
it's kind of key to giving a really
compelling user experience
so I'm gonna switch gears a little bit
at this point and talk a little bit
about some use cases that we were only
really able to ship leveraging some of
this technology so one example some of
you might have seen it is kind of the
marketplace tab and Facebook where you
can actually list stuff to buy or to
sell and one thing that we noticed which
is kind of really interesting and drives
usage of this function a lot is you know
for people who don't actually know that
this tab exists can we actually detect
the intent to sell just from a regular
Facebook post and not only that can we
actually parse out what they're trying
to sell what price they're trying to
sell it at and kind of automatically
create a marketplace listing from that
and without that you know it's just
another Craigslist right so that was
actually something where you know when
we were able to deploy that using some
of these deep text models engagement for
this feature just just took off so this
is one that I am actually really proud
of so this is what we call the social
recommendations feature so the idea is
that you know you want to be able to ask
your social graph for recommendations
and the way the feature works is that
first of all we detect that there's kind
of the soliciting recommendations intent
we also detect what it is that you're
actually looking to be recommended about
is it a local business is it travel
recommendations for a new city and then
once people actually start commenting we
parse all the comments and we pull out
all of the relevant recommendations
themselves and not just dimensions but
also linking them to like actual places
in the Facebook knowledge graph places
or businesses or what-have-you and then
kind of overlaying all of these on the
map so this is something where again you
know it is actually really easy to do
once you have all the AI building blocks
in place and it's just not doable at all
without it so this was one of our first
features that was really kind of powered
by deep language understanding from the
get-go I don't have slides over here but
another feature that a lot of you might
have seen which is again powered using a
lot of the technology that I talked
about before is what we call em
suggestions so when you are talking in
message
and you know if you're if you're trying
to make plans to watch a movie or to go
to dinner with you know three or more
people it says it's like herding cats
right but but if we can detect that that
is in fact what you're trying to do and
if we can actually detect you know what
places people are suggesting maybe make
a poll out of those or maybe see if you
know we can get you a new bird the way
we see it is you know we're taking this
social coordination exercise that's kind
of inherently quite difficult but we are
lubricating it as much as we can based
on our deep understanding of what people
are talking about so and then switching
gears again a little bit one of the
things that we've been thinking about a
lot is you know how do we kind of get
maximum leverage from all of these
pieces that we have talked about so for
example you know how do we get it to a
point where all of these models can be
trained not just by you know machine
learning or national language engineers
but by an order of magnitude for more
people right how do we reuse models that
might have originally been developed
with one use case in mind for a variety
of different use cases and how do we
actually get the most bang for our buck
just in terms of annotated data right so
it's annotated data is costly to come by
to begin with and how do we make sure
that you know we're only really
annotating high-value pieces of examples
and you know not things like hi or call
mom so with that in mind one of the
things that we built is what we're
calling clue or that's the content and
language understanding engine and so the
way this works is you know it's very
much a self-service model so if you're a
developer whether that's a second or
third party developer coming in you can
collect some data by searching some of
our unlabeled corpus you can you know
label a handful of examples yourself
bootstrap an initial model which may not
be perfect but hopefully it should at
least be sensible and good enough and
then you can really you can review and
you can see what are the examples where
the model is the least confident or
we're labeling those would move your
numbers of the most in terms of your
test sets and so what we've been able to
do
with this glue platform now is that at
Facebook every week you know more than
200 models are trained in a purely
self-service manner without any
involvement from my team you know across
a bunch of different products in
facebook whether it's feed or ads or you
know messenger so I think that's
actually my last slide so you know this
is just a kind of a broad strokes
high-level overview of all of the
different places where you know we are
you know heavily dependent on language
understanding in order to actually make
our products work the way they're
supposed to and you know we're not
hiring I'm just kidding
we're hiring as well so yeah please come
see me later if you have any questions
Thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>