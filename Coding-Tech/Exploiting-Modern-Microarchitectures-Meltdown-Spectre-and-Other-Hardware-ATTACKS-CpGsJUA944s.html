<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Exploiting Modern Microarchitectures: Meltdown, Spectre, and Other Hardware ATTACKS | Coder Coacher - Coaching Coders</title><meta content="Exploiting Modern Microarchitectures: Meltdown, Spectre, and Other Hardware ATTACKS - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Exploiting Modern Microarchitectures: Meltdown, Spectre, and Other Hardware ATTACKS</b></h2><h5 class="post__date">2018-04-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CpGsJUA944s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today I'm gonna cover the difference
between architecture and
microarchitecture some some variations
of microarchitecture so what that means
in the real world I'm gonna talk about
caches and virtual memory and branch
predictors which are some of the pieces
that are you need to exploit to make
these kinds of attacks successful I'll
talk about side channel analysis which
is an interesting topic in itself and
then I'll look at the actual
vulnerabilities that you've seen the
mitigations we have for them and then
finally some related research in into
Hardware exploits that maybe you have
not seen before so architecture versus
microarchitecture for those of you who
are at the the risk five sessions you
probably saw similar content already but
when we talk about computers we have
this concept of architecture and then
architecture describes at an abstract
level how a machine operates and behaves
so it describes the kinds of primitive
instruction that the machine actually
executes the ones and zeros it executes
so when I see this particular sequence
do this right describes how to load and
store values from memory it describes
the registers the Machine state that I
have and it describes various modes of
operation privileged and unprivileged
modes of operation we'll talk a bit more
about in a moment and then there's more
detail as well it describes memory
models and other things that that you
might find very interesting if you
pursue architecture we also have some
software concepts we care about so if we
are Linux programmers or BSD programmers
we care about running programs known as
processes or tasks when they're running
and we care about the fact that they
execute different privilege levels right
so applications are less privileged than
the operating system applications run in
something we call user mode and we have
various abstractions that protect them
from corrupting one another and from
corrupting the colonel right so we have
this virtual memory environment we
define so that our application sees this
nice flat view of memory it thinks it's
the only thing running on the machine
unless we tell otherwise and it has it
requests services from the OS kernel
when it wants something performed it
doesn't explicitly have knowledge of
other programs running on the machine
you can find that out it could ask the
kernel but as far as its concerned it
has a view of memory that's all to
itself the OS on the other hand has a
privileged set of architecture
instructions and it uses these to manage
the state to manage the context of the
running programs and to switch between
them so when you get what's called a
hardware interrupt when you get some
event coming in or otherwise need to
switch from one program to another to
give the user the illusion that lots of
things are running at the same time um
that's what the kernel is doing it's
using these interfaces to save and
restore the context of programs examples
of computer architectures obviously I
have to mention x86 first right I think
everyone here knows what x86 is I could
have put risk 5 as the second one I
decided not to I think you guys have
heard of that too but these two these
are two examples of architectures so you
know x86 it's a bit older than the
64-bit ARM architecture they both have
instructions one of them takes complex
instructions
one of them has simpler instructions
they both operate on registers and they
both have a 64-bit memory model they can
both use large amounts of memory and
provide that to applications there are
some differences but you know at a high
level we can compare the two let's talk
about microarchitecture so we'll detour
I'm a very bad graphic designer right so
you know don't ever come to me if you
want something pretty here's a picture
of a chip and the thing I'm trying to
sort of represent here is that a modern
processor as we think of it is actually
you know it's not just one little CPU
doing something it's not how we used to
think of it years ago
a typical chip and even your laptop or
your phone will have many cores we used
to think of these as processors back in
the day but there are many different
cores and each one might be running a
process or or several of them together
might be running threads in a process
they're all connected together on the
chip and they have these
high-performance interconnects so in
here you can see I've got 1 2 3 4 5 6 7
8 cores and they have shared access to
some of the resources so there's a
memory interface on each side of my chip
and whenever I want to load something
from memory into my one of my cores to
do some processing it's gonna come in
through the external memory interface
and it's gonna work its way up through
levels of cache we're gonna talk more
about that in a minute but just have
this have this picture in your head and
we'll talk a bit more so programmers
think of processors but they really mean
cores we have these systems formed from
you know many different pieces and they
all have to work together
microarchitecture refers to an
implementation of an architecture right
so we've defined a high level x86 or arm
instruction set architecture we've said
this is what an x86 machine has to
comply with and a specific x86 machine
might be implemented differently from
another one as long as they can run the
same instructions the implementation can
differ and some example differences can
include what we'll talk about in a
moment in order machines out of order
machines lots of differences can exist
at a microarchitecture level for example
simpler processors are often described
as being in order machines and if you're
not familiar with computer
microarchitecture this might be how you
think of a processor when it's running
your code so when you're when your
program is running every
Ingle instruction every single operation
in your program what it will do is one
after the other it will fetch the
instruction it will decode figure out
what it does and it will execute that
instruction and it will rinse and repeat
that one after the other the this kind
of example is it's got your classic RISC
machine if you were to take a simple
risk five machine for example it
probably will start out there are some
more more performant implementations but
it will probably start out as a simple
inorder machine and what we might do is
we might add some features like
pipelining so instead of having I
mentioned the different stages there
before we might overlap them a bit we
might say I fetched one instruction I
start we're figuring out what it does
and I'm already fetching the next one
and I get a little bit of parallelism
here you can see here I might have five
instructions looking my working their
way through the machine at different
stages right that's called pipelining
that's when I split out how my machine
executes instructions into smaller steps
it still does them one after the other
though in order machines are easier to
implement and they're much more
efficient in some ways for a power
perspective right so I'm not gonna get
very high performance but I'm going to
need potentially less power so that's
why you tend to go Bo's in in little
widgets and they'll also use less area
they physically are smaller to build but
they're susceptible to things like
pipeline stalls so if I'm working my way
through the different stages of running
instructions and I'm trying to load
something from memory I might have to
stall my machine while I wait for some
data to become available they've got a
limited capacity to hide the latency of
instructions as a result now what I can
also have is an out-of-order machine
right now this is very diff
from what you as a programmer are
thinking of so when you write a program
you think I do this and then I do this
and then I do this but what the industry
has done is spent the last few decades
working out how to take your program
that has a defined sequence of
operations and to automatically work out
dependencies inside your program so
here's a simple program I'm allowed two
values I'm going to add them together
and here I'm gonna load two valleys and
add them together those of you who are
familiar with assembly language will see
that I'm using
registers inside my machine I'm sorry
I'm loading register 1 register 2 I'm
storing the result of adding them
together in register 3 and here I'm
doing the same load register one load
register to add them together but these
two sections here they're actually
independent I could renumber these and
use different registers there's no
reason that I have to have the same
register numbers here that you see above
you know in a simple machine it might
run through that program
um and execute it exactly as you see
here in a more complex machine what it
might do is work out well actually these
two sections are completely independent
and the only thing they share is that
they're using the same registers but I
could actually change that so behind the
scenes what an out-of-order machine will
do is it will reorder all of these
instructions it's called dynamic
execution what it will do is it will say
actually the moment that these values
are available I can run this instruction
and then what it will do is something
called inorder retirement so you start
with your program that does one thing
followed by another you turn it into
what looks a bit like a dataflow machine
and then you do in order retirement so
at the end when you've worked out your
results you keep track of where you are
and you say ok I have I maybe I may
actually be executing this stuff here
before this stuff here but I'm gonna
wait and I'm gonna retire only in the
sequence the programmer expects so you
as a programmer
not aware of this happening and then
there's some complex machinery that's
added for exception handling so if
something erroneous happens during
execution I might have to back up what's
happening inside the machine and present
it to the programmer in a consistent
order and say well you had a failure
here and now that is actually I'm
presenting it to you in the order it
would have happened from your point of
view so this is very complicated
I want you to understand though that
machines that you have even your laptop
in front of you right now may actually
be executing programs in a completely
different sequence from how they were
written or how you imagined that they
would work out of other machines are
very common in high-performance
microprocessors the concept was invented
by a gentleman called Robert to Tomasulo
who unfortunately passed away a couple
of years ago and it would be interesting
if he could see kind of a media
attention recently and and get his
insight right because what people have
done I'll talk more about speculation
and how that builds on this in a moment
but what people have done is they've
said well this whole thing doesn't work
actually it does it's working exactly as
designed
there just can be some flaws in specific
implementations of this but Tomasulo
invented this back before done keeping
out of the time because this is gonna go
along otherwise he invented this for the
mainframe the
estrecho 360 model 91 so quite a long
time ago and then over time it's worked
its way down into computers that you
have on your desktop on your laptop and
in most phones as well and as I said
instructions are dispatched from an
inorder front-end as we call it they're
executed in this out of order machine
and then they retired back in sequence
and the size of the structure I alluded
to here detect eights how many of these
instructions I can have out of order at
a single moment
and these can be quite large so in a
contemporary machine for example a
skylake processor recent x86
implementation from Intel
well the skylake microarchitecture has a
reorder buffer that's 224 entries long
so that's quite a few instructions I
might be ahead at any one moment that's
a lot of housekeeping they have to do
that's why these things are very very
complicated you should also know the
average one of these processors costs
about a billion dollars it takes about
four years and needs at least 300 people
just for the basic design right that's
why you don't have open source you have
open source out of order machines but
one reason we haven't yet seen xeon
class open source design is frankly the
amount of cost that someone has to throw
it doing that it's not to say it won't
happen but it's very very complicated
and very expensive I'm gonna skip I'm
gonna skip oh well and this last slide
here just talks there are lots of
questions you can ask about architecture
right so I have an architecture
specification for example x86 I then
talk about how I implement that right
when I'm executing my instructions do I
do them in order do I do them out of
order these are all design choices the
machine ultimately does runs the same
programs but I have trade-offs I can
make choices based on how complex and
how performing I want my machine to be
examples of implementations of
architectures so here's the skylake in
my laptop that's why I'm using that
example it's a little bit older and you
can see it's got 224 instructions that
it can have in that Rob and then you've
got an IBM power8 but you also can have
224 instructions but they call it a
global completion table because IBM is
different in lots of ways good ways and
you also see you know how many
instructions the machine could have in
flight so on an x86 machine typically
it's a couple at a time on your on your
laptop
it will take these x86 complex
instructions and decode them further
into these macro and micro ops and all
kinds of things you can read more about
later it will run a few of them at a
time on the really big servers it might
run as many as 8 or 10 it may dispatch
it 8 to 10 at a time
ok now store that what I just said let's
talk about virtual memory ok so I talked
about the separation between
applications you have user space you
have user applications running you have
the operating system and we try to
isolate the two for obvious reasons
right we don't generally want any old
application began to interfere with the
operating system and we have a define
interface between the two so when an
application wants to do something it
uses a system call interface an API
through which it requests things from
the OS applications when they're running
as known as processes they use system
calls I think I mentioned all of that
here's an example of a program when it's
running so if I were to just on my
laptop type this command tap proc self
Maps I could see the view of memory that
that cat program has because it's
counting its own memory map and I might
see various memory ranges but I really
want to draw your attention to a couple
of them okay so every process every
running program will have a range of
memory that represents its own text its
code and its data and its FAQ and some
other stuff and then every process until
recently will come on - why that changed
until recently would also have this
range of memory at the top of its
address space which contained all of the
kernel and all of the memory that the
kernel has access to and you might say
well why is that well we had we have
mechanisms that are supposed to protect
the application from being able to see
or touch that range of memory
and it means that whenever we want to
whenever we want to go whenever we want
the colonel to do something on our
behalf we could have a very lightweight
entry and exit from the colonel
it already has access to all the memory
it needs it's already set up we just
jump into a different execution state
that can access that memory we do
something we go back to the application
so the application shouldn't be able to
see any of that memory we maintain this
separation using something called page
tables which take the view of memory the
application has and they translate it
into the view that seen by the hardware
so I'm trying to access you know this
address whatever address up here it's
going to go through some page table that
tells me where we're in physical memory
that address actually lives
that's an expensive operation doing that
there are software of things we call
Hardware workers in our chips that
actually have to go down through these
tables and they have to work out this
translation that's an expensive
operation so we don't do that every time
what these chips have in them is
something called a translation lookaside
buffer actually probably several and
what these do is they store these
translations so if I want to touch a
piece of memory I can actually look up
very quickly on the last few
translations and again by keeping kernel
memory translations are in place while
my application is running again I get
some performance because I want to go
into the kernel to do something these
entries are already populated they're
already present and so typically what I
will do is I'll leave these in place
until I switch from one process to
another then I have to flush this stuff
out and switch to another process
because it's view of memory is different
okay let's skip that slide skip that one
and okay so I'm going to throw caches in
here as well
so I've said that you have these ranges
of memory or application C's you
translate them before they hit physical
memory well you also have a caching
hierarchy that sits between your program
accessing some memory and the actual RAM
chip in your machine and there are
multiple levels of this cache memory all
right with names like level 1 level 2
level 3 level 4 things like that but
basically what they are are ways of
accessing data that I'm using frequently
faster write memory chips in my machine
is slow relatively speaking the core is
inside my chip are much faster and the
laws of physics tell me that I can't
have both I can have big and slow or
small and fast so what I do is I have
some memory on my chip it's a bit faster
or a lot faster and it caches valleys
I've been using recently so when I when
I touch a piece of memory what will
actually happen is it will get pulled
into the caches and so for example I may
have a cache entry for a user a piece of
data from from a user application I may
at the same time in my cache have a
piece of data from my kernel and again
I've got protections in place that
should mean that there's no way of ever
accessing a piece of kernel data from my
application code my page tables say
that's not accessible it doesn't matter
if it's in the cache when I try to
access it that's not accessible probably
skip this slide but this is an
optimization actually of how modern high
performance caches are implemented that
you can read a bit more later on if
you're interested this will tell you for
example if you ever wondered you know
why is a level 1 cache 32 kilobytes in
every CPU with 4 K pages this will tell
you why you can read it later let's keep
going
ok I'm gonna skip how caches work so
let's talk about side-channel attacks so
side-channel attacks are based on
deriving in from a
by exploiting the physical
implementation of a machine right so we
have our instruction set that describes
how any x86 machine should operate for
example then we have an implementation
the side-channel takes advantage of the
fact that an implementation might have
some vulnerabilities into it classical
things that we've done in this space
have involved electromagnetic emissions
right put your hands up if you've heard
of tempest you guys heard of tempest
right the sort of secretive governments
agencies watching your screen from afar
right that's based on analyzing the
emissions coming from your machine right
there are similar attacks with
differential power analysis so I can
monitor how much power a chip is using
and I can infer what it's doing another
thing that I can do is I can measure how
long it takes for certain operations if
different operations take different
amounts of time and if I can actually
perceptively measure that so caches can
behave as side channels because they're
a shared resource as you saw from my
diagram earlier whenever I pull some
whatever I want to use a memory location
it's going to go in through my cache
hierarchy meaning that the cache is
shared by everybody and I can actually
measure a difference in the time it
takes to access a piece of data based
upon whether it's in the cache or not
and in fact it gets even more scary you
can actually work out what level of the
cache it's in well that just rebooted so
all right if you're watching the videos
Freeman it cut out your video streaming
machine just rebooted but okay so anyway
so I can measure the amount I can
measure where or if not in the cache
based on how long it takes to read a
piece of data in fact there are even
more exciting attacks on some
architectures like x86 I have a special
instruction called CL flush and I can
say as a probe
unprivileged any code can do this flush
this location make sure it's not
anywhere in the caches so I'm guarantee
it's not in the cache and actually what
I can do is if I do two flushes I can
actually measure whether when I flushed
it whether it was in the cache to begin
with so I don't even have to load
something to measure whether it's in the
cache Oh some really exciting attacks I
can do here's an example so I use I have
these interfaces on on most
architectures I have a way of measuring
time on x86 it's called our DTSC read
time stamp counter I can read the
current time stamp counter I can access
a piece of memory I can read it again
and I can work out the difference and
based upon that amount of time and some
calibration I can work out is that
memory access is that thing I'm
accessing in the middle was that in the
caches or not and as I said a lot of a
lot of architectures provide
instructions that let you do this you
don't need an instruction there are
other ways to count time and some
architectures provide a way to guarantee
you've flushed something from the cache
that gets very useful a bit later on but
there are other ways to do that you can
look up displacement flushing if you're
interested and as I said you might even
be able to optimize it with some of
these other variants as well to see if
data is in caches well why is that
useful let's let's think about that
we'll come back to it I must get
prefetching okay now we're talking about
branch prediction it all come together
in a minute this is a complicated topic
guys you know you're getting a deep dive
here right let's talk about branch
prediction so when I'm running code on a
machine
I may need I may hit points in my
execution where I'm trying to decide is
my program going to go one way or
another if this do that or do that
instead
right now when when I hit a branch in my
program I'm going to test for example if
it's raining do this thing well the vet
I may not actually
the value of raining available to me at
that moment for a couple of reasons it
might be in slower memory I need to pull
into my caches that might take a bit of
time or it might be some calculation I
have to perform and for those reasons uh
there can be cases where I hit a branch
in my code to go one way or another and
I don't instantaneously know which way
it's going to go so I can I store my
machine in weight or I can continue
running I can guess which way my branch
is going to go and I can build on this
build on my out-of-order machine I can
build this concept of speculative
execution what I can do is I can say I
have this condition here if r1 is zero
do this other stuff I don't yet know the
value because I'm loading it I'm waiting
for it to load so what I'm gonna do is
I'm gonna go into a special mode of
execution called speculation and I'm
gonna keep running these instructions
I'm gonna guess it's gonna go this way I
don't know that so I'm going to tag each
instruction and say that it's
speculative right if later on I discover
which way that branch is supposed to go
and I'm wrong I will flush everything
that's purple here I will forget about
it because I've tagged it especially
I've not retired it I've only kept its
interim State the idea is that you're
never aware that I did this
it's an optimization if I'm right the
machine keeps going it's a bit faster if
I'm wrong I have to throw away some
state but I'm no slower than if I just
waited to find out the result of that
conditional check so speculation is
something that we build in out of order
machines it's part of our branch
prediction hardware and we use it to get
a performance optimization when we're
speculating if we have any erroneous
conditions in our
Oh Graham what we will do is we will
also tag them here right so if I try to
read if I try to perform an illegal
instruction or do something that's not
permitted I won't actually take an error
do you take a trap I won't do anything
about it because I don't know if this is
actually supposed to run I'll just mark
it and later on if I decide that that
was supposed to run then I'll handle
that later so as I said if I'm correct
when I hit a branch
it's called resolving a branch if I'm
correct then I continue and everyone
just gets the speed a speed up if I'm
wrong I have to do some housekeeping but
the idea is that you can never observe
that machine you can't observe the fact
that I did this speculation it's
supposed to be visible to you I can talk
a bit more about conditional and and
indirect branches but I think I will
just skip to how branch predictors work
a little bit so if I have two different
applications running a my machine how
does how does the branch predictor
actually work well what it does is it
has a data structure in memory and it
will look at the actual memory address
of a of a potential branch instruction
and it will in different ways because
implementations vary record the history
of that branch so the last ten times I
saw this branch I went that way probably
means the next time I'm going to go that
way as well in fact in some hardware I
even have fancy stuff like loop
predictors they can work out not only
you know is this is this branch probably
going to go that way but I even know
it's a loop I can just magically work
that out using some complicated hardware
but there may be many different
components to my branch prediction and
fundamentally they will use some
structure that tags the history of
branches and I want you to think about
the fact that this tagging that I do it
could be expensive I could I could need
a lot of memory
if I would try to store the address of
every branch my program ever took so
instead what I do is I optimize this
structure and I may only use a little
bit of the address for that branch so
consequently I could have two different
programs with two different branches and
my branch prediction hardware may not be
able to tell those apart okay and then I
have a variant of those conditional
branches I talked about called indirect
branches that's when I have what you
would call a virtual method or some kind
of function pointer I don't know where
I'm going to go to I also have hardware
but a bit like what I described before
can guess indirect branches in my
programs I'm gonna skip through the
optimization all right now I'm going to
talk about these particular attacks
because where I took a bit of time okay
so you learned a lot that was a whole
semesters worth of various computer
science stuff I'm glad you're still
awake let's talk about these two
vulnerabilities and how they layer upon
that so you know these are branded
vulnerabilities they were discovered by
both academic researchers and also by
Google project zero and because they
were discovered by researchers I love
the researchers but you have to give it
a cute name right so you know meltdown
inspector it is because variants 1 2 &amp;amp; 3
don't really sound sexy do they you know
now we were at by the way we were
actually tracking these guys for a while
and I knew that this was the research
team working on the project and of
course there are websites you can go to
where you can track everybody's domain
registration so I was using a side
channel for some time to monitor the
researchers to see what they would name
it
so we discovered the meltdown inspector
domains the moment they registered them
in December and consequently had a
little bit of time to you know figure
out how they would position it
and what these attacks do is they
exploit the things I just described to
you to bypass normal system security
boundaries and let's go through how they
do that well firstly if you're on a
Linux machine don't panic
don't let your machine panic because
very recent Linux kernels and certainly
those from the distros will very soon
start to have this directory system Isis
system CPU vulnerabilities we are
thinking there may be more over time
it's good to leave room right and you
will see entries in there for these
attacks and then potentially future ones
along with what your machine is doing to
mitigate this right that's not to fix it
because fixing it would require that we
change the hardware in some cases but we
can mitigate it we can take a
performance hit and do something to
remove the ability to exploit these
attacks so meltdown relies upon some
implementations of speculative execution
literally following what Tomasulo did
and the key piece is that they handle
exceptions they handle problems from
accessing data you're not supposed to
write at the end they allow you to
speculatively do something but then they
say before I retire before I ever
complete that operation I'll I'll just
make sure I'm supposed to and I'll throw
it away if I'm not so you might see a
piece of code like this don't worry
we'll talk through what it does in a
moment in fact I think I have it on the
next slide ok so so I might have some
secret data who knows what it contains
some magic data in my Linux kernel I
want to read and if I can arrange for a
little piece of code to run
speculatively it means it's not actually
necessarily going to be part of my
program I put it inside a branch that
may or may not run um what I can do
inside that piece of
is I can read that pointer quite happily
now if my program ever retires those
instructions I'm gonna get an exception
and it's gonna crash that's not useful
right but what I can do
while the speculation is happening I can
use the value of that data to access
some other data that I do have control
over and I can actually influence which
data I access remember I said before I
can also determine whether something is
in my cache based upon the access time
for it so I've got all the pieces I need
I figure out the data I want I mask out
a little piece I want to read I then
access some other piece of data I have
control over and the offset I access is
based upon the value I just read and
then what I'm gonna do
back outside of my speculation is I'm
going to measure which of those two
locations I load it if I load one
location for example 100 that means one
thing if I load if I if my code load it
from the other location that means
something different the actual value I
read isn't visible to me right
the speculation Hardware took care of
throwing all that state away but because
they because there's a shared cache and
I can observe what happened from the
point of view of the cache I can see
that value and I can they're there for
consequently use the same piece of code
I gave you before to work out which of
those locations
that's 0-1 signaling I can reconstruct
that piece of data I can do that in a
loop and I can read out the data
now the actual meltdown exploit you'll
read online is a bit more detailed and
they've got some optimizations this is
the version that I put together in
December because we had to mitigate this
and we weren't given reproducers just
enough to be dangerous and then some
folks I think I was told that we were
not that sophisticated so don't worry
well I don't like being told that so
wynton figured it out and made a
reproducer that really annoys me when
someone says that so that was great so
this is what my code does the actual
code the researchers published is a
little bit more optimized but you get
the idea
you can't read that secret data but you
can observe what it was based upon what
it did to the caches and here's here's
again here's the example code you can
read through the slides to kind of get
that to make a bit more sense for you as
well so when the right conditions exist
I can exploit this how can I mitigate
for it well there are certain
circumstances required to make this
possible to it abuse for example in some
implementations it might have to be in
my very innermost level one cache I
might be able to flush that cache
whenever I leave the kernel there might
be some people out there that are
mitigating that way the other thing I
could do is I could change how I do my
page tables so that the kernel memory is
never visible when I'm running an
application I can do that it's just a
performance hit because now every time I
go into my kernel I have to twiddle my
page tables around that's a technical
term and I take a hit that costs me time
right that's why meltdown mitigation
with page table isolation has a
performance hit there are optimizations
there's something called a certs you can
you can read more about this okay let's
do Spector I'm running on time uh
Spector all right so we have this
concept of gadgets gadgets have you read
about return-oriented programming ROPS
these are common kind of stack smashing
attacks in security arenas gadgets are
pieces of code that already exist in a
victim or target program and I'm gonna
cause that code to execute it's already
there I'm just going to influence the
environment set of that piece of
particular code I found ah that does
something I want similar a similar
sequence to what you saw before you know
load some data I can infer what the data
was base
done the address it loaded that kind of
thing I find a piece of code that's
particularly interesting and I abuse it
so here's an example respect the variant
one I might have a piece of code that
reads some data in from the user
untrusted data I might then do some
other stuff and it turns out some micro
processors will keep executing before
they know whether they should and so if
I find a place where particular bad
sequence of code exists I can I can
exploit that
that's Specter variant one it's
difficult to do because I've got to find
just the right code it's got to be kind
of on an entry point into the kernel
there's one reproducer it's a bit messy
let's talk oh and the mitigation for
variant one is well don't do it so what
I do is I shove an instruction in here
that prevents the code from continuing
past the point of doing of doing
potentially a speculative load and that
requires that I rebuilt my my operating
system my kernel uh I talked about
branch predictors before so I've also
got a very into of Spectre which is the
branch predictor poisoning and that's is
since I know how branch predictors
behave they're maybe not fully
disambiguated addressing I can exploit
that I can have one process running
that's training my branch predictor to
guess wrongly when something else is
running if that something else is more
privileged or the kernel I can exploit
what will happen there so I can poison
my branch predictor in particular my
indirect predictor to guess that it's
going to make a jump into some code that
it in fact is not it will then
speculatively execute whatever gadget
code I want so now I've got control over
where that gadget code is that's much
more interesting to makers in a very
large thing like the kernel I'm probably
going to find a particular instruction
sequence that's interesting to me I set
up in ver
I trained my predictor and I can exploit
it I'll rap quickly mitigating it there
are two ways I've got a big hammer which
is expensive I can turn off my
predictors when I go into or out of my
kernel or I can use a technique that
Google came up with called ret Pauline's
which you can be more about in here
which change in direct calls into fake
return calls it's kind of interesting
it's like if it hurts doing this don't
do it right so they have a way
particular code sequence where they will
modify what looked like indirect
function calls and make them look like
function returns so they won't use the
indirect predictor it's a cute hack it's
not nonsense it's it's it's very it's
very interesting and it does
unfortunately also require you change
your compiler and rebuild lots of things
so we are switching to that one because
it's much more performance than turning
off our branch predictors everywhere and
it also this cute stuff like put a
harmless infinite loop in if you're
going to speculate just speculate that
go away it's kind of fun okay and there
is there are other variants of this
coming I'm gonna wrap now so related
research so this is just beginning right
architecture junkies like me you know
I'm not gonna say we're excited because
that's a bit unfair it's not to be very
serious
and you should update your machines
straightaway and all that kind of stuff
but it is interesting that a lot more
people are paying attention to these
classes of attack now and that means
that researchers will find more of them
hopefully we will make better machines
as a result and other related research
will happen right so you guys can can
read about these later you can read
about the row hammer attack and you can
read about magic which is my favorite
one that one is writing special
sequences of instructions which when you
execute them will physically age your
hardware right the bottom line is
everything you thought you
may not be possible now is a good time
to go back and think about it again
right in summary we talked about a lot
of things and you guys can go on twitter
at John masters and you can find the
links to all the slides I will happily
take this time for one or two questions
now otherwise I will be around after and
I'd like to thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>