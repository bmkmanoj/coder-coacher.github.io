<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>TensorFlow 101 (Really Awesome Intro Into TensorFlow) | Coder Coacher - Coaching Coders</title><meta content="TensorFlow 101 (Really Awesome Intro Into TensorFlow) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>TensorFlow 101 (Really Awesome Intro Into TensorFlow)</b></h2><h5 class="post__date">2018-02-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oxf3o8IbCk4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon and welcome to our
discussion about tensorflow which is an
exciting new framework and toolkit from
Google I've got some good news and I've
got some bad news the good news is it's
an incredibly powerful tool kit it does
some really really cool things if you
know what you're doing it will enable
you to do some some pretty cool stuff in
machine learning space the bad news is
if you don't have a background of
machine learning it may be completely
useless to you that being said I want to
try to contextualize it I want to try to
help you understand what it does for you
I want to put it into the larger
workflow of a machine learning and data
science activity and at the end of the
slides which I will get to the
conference organizers I have my email
address and if you don't have a
background in machine learning and are
interested in sort of next steps I do
encourage you to drop me a line and I'll
try to push you in the right direction
in terms of books and videos and courses
and things like that we are seeing lots
of toolkits like this be released by
Google by Microsoft by Facebook by
Amazon and a lot of people are sort of
curious like why would they if these are
so powerful why would they give them
away and I think the answer comes down
to a couple of things number one without
good data it's pretty useless right a
lot of companies are heading down the
machine learning path thinking oh we can
take these magical tools and our crap
data and produce good results and that
won't happen if you do not have good
data if you do not have enough data or
enough data of the right type these
kinds of toolkits are not going to be
all that helpful to you so there's a lot
of excitement and and people pursuing
this without understanding the the
appropriate path towards benefiting from
something like this so number one
without good data the tools themselves
are just commodities they're their
algorithms encoded into useful libraries
number two there's there's a war for
mindshare particularly on the hiring
front particularly than the hiring of
machine learning experts front and so
Microsoft and Google and Amazon are are
interested in having researchers use
their toolkits
so that they are able to hire people who
understand the tools that they're using
excuse me more easily and then the third
reason I think that they're giving these
away is because they want their own
developers to to be comfortable using
them and some of the early initiatives
say Google did with the disbelief stuff
were useful from a predictive
perspective but the developer experience
was not all that hot and so I think by
giving these tools away to other
developers they will get feedback that
will make them more useful environments
for perhaps non machine learning expert
developers who understand kind of what
they're doing but are sort of mark
traditional developers and so tensorflow
sort of falls into that space where if
you have good data if you know what
you're doing
it can be a useful technology now there
are other ones out there but what makes
tensorflow different is it moves beyond
the training phase and I'll talk about
the training phase a little bit more in
just a minute it's not just about
building the models it's also about
operationalizing them optimizing them
and supporting a variety of
architectures such that we can run
portions of the application on a phone
or on an iPad or tablet or something
like that and we don't have to hand down
the huge models that were created during
the training phase so it's got support
not just to help you figure out the
models from your data but then also how
to turn them into production systems and
one of the things that people often
don't think about is once you generate
the models if you retrain them in the
future you don't want to just drop them
in place into your production system
because you could completely tanked the
quality of your predictions unexpectedly
so tensorflow through some of its
surrounding technologies also has the
ability to do some a be testing on
models to make sure that they're they're
the new versions the new retrained
versions are seeing the same kind of
quality predictions that you're seeing
in the past briefly my background in
machine learning started about 18 years
ago when I worked for a company that
built an Internet distributed
platform to think SETI at home but a
general-purpose pre-emptive Java based
ecosystem for taking advantage of idle
computers all over the world and so we
were trying to build a platform to allow
cheap cost-effective computation for
computationally expensive learning tools
that were starting to emerge at the time
and the the company did not succeed in
in creating that platform as a
marketplace but they've also moved on
and done some very interesting vertical
things as well so again about 18 years
ago we were doing things like exhaustive
regressions with genetic algorithm based
feature selection and Monte Carlo based
digital rendering and things like that
and so I worked on the infrastructure
and this was like pre j2ee and all that
kind of stuff but we also had a labs
group that was doing some of these
machine learning techniques and and that
was where I was first exposed to it and
so over time I've gotten more into it
and I've had a lot of fun along the way
and so seeing tools like tensorflow
emerge helped the learning curve quite a
lot but there is still like a quite
steep learning curve so let's talk a bit
about the data science and machine
learning process everyone's excited I
mean you cannot you cannot look online
on Twitter or blog posts or things these
days without seeing people getting
excited about machine learning and AI
and data science and one of the things
that nobody really goes out of their way
to point out as you know even though
this is one of the sexiest jobs of the
21st century and we have all these sort
of Dark Arts machine learning tools
available to us that really represents
only one part of the process right a lot
of the process of what we go through
involves sweeping and mopping
so get ready rock star grab a mop
because you're gonna have a lot of
cleaning to do because a lot of data
that we have is just not in a good
enough shape so a very common activity
for data scientists is to collect some
data from the world clean it up produce
a report and there we go there's no real
analysis necessary we don't we don't
have to spend a lot of time thinking
about things like web analytics now if
you want to start building models and
predictions about how
Drive web analytics then things start to
get a little bit more interesting and
then we have to perhaps go into what we
call exploratory data analysis which is
a fancy way of saying you're gonna screw
around for a while right because you
don't necessarily know what the right
technique to use is you don't
necessarily know what aspects of your
data are going to be the best to to do
the training and learning phases on so
you're going to do some some feature
engineering you're gonna go through and
play around with these things if you
have the ability to use some of the deep
learning techniques and platforms these
are helping with sufficient amounts of
data helping us do some of the feature
extraction in a more automatic way but
without that you're gonna have to spend
a lot of time playing around and seeing
what works so there's equal parts art
and science involved in this process at
the end of the day once you think you're
sort of ready to go then engaging the
learning algorithms the machine learning
capabilities can be a successful
strategy and help find patterns and
build predictive models and whatnot that
we wouldn't know necessarily how to
build or explain from scratch as
programmers we instead we we create
tools and ecosystems that allow them to
sort of learn from the data and the
characteristics that we have and so at
the end of the day there are basically
two main outputs for a data science
activity like this one is to tell a
story try to change somebody's mind
weight whether this is science or trying
to build a data-driven organization and
decide where the organization should
move next and that case you're trying to
change people's minds the analytical
tools that you're using probably have to
have a pretty high explanatory
capability right you have to be able to
explain how you came to the conclusions
that you came to and some of the high
dimensional clustering sorts of things
at work but are impossible to explain
would not be a useful tool there so if
you're trying to help people understand
where they need to go or the decisions
that they need to make then you're gonna
have to translate it into sort of terms
that your audience will understand and
have your process be able to excel
what you're doing now other times we
want to release some kind of tool back
to the world and then use that tool in
some sort of predictive way and that's
where the the support operationalizing
and productizing a be testing
capabilities tensorflow really shine and
most machine learning tools will help
you through this phase and then say okay
you're on your own from here on out to
try to figure out how to operationalize
this as i said that's where tensorflow
really kind of shines is that it has
tools for quantizing the datasets and
reducing the size of the models and
supporting arbitrary architectures
they're doing cool stuff with like the
word lens capability where you can take
your phone point it at a sign and
translate it visually on your screen
you know from models that it's it's
interacting with on the backend so
that's that's where tensorflow really
shines is it it's an ability to build
runtime production systems other kinds
of systems that you're familiar with
obviously search is driven by some of
the machine learning models that we have
spam filters or another example of a
data product that we train and then
release back into the world but anything
with some kind of predictive model
you're gonna want to be able to turn
back into running software and have a
strategy for treating it like you do
with regression testing on software you
want to be able to regression test model
tweaks and whatnot so tensorflow
sort of grew out of the Google brain
team as I said they had some earlier
releases called disbelief that had
pretty good predictive results but the
development experience wasn't all that
great 100 was released in February and
they kind of broke the api's in subtle
but annoying ways I will say overall
they made good choices and they
standardized and cleaned up the api's
but all the books and tutorials and
things that have come out in the last
two years suddenly did not work anymore
without understanding how to rename some
of these functions and capabilities and
so that was there was a bit of a drag
for people new to the to the platform to
be able to be successful early on
without a lot of headache they've moved
on and we're seeing more
one o plus compliant tutorials and
examples and things coming out now
version one two was just recently
released so that's two relatively strong
releases since February there's a lot of
momentum behind this if you google
tensorflow developer summit there's a
whole series of like fifteen to twenty
hours of videos talking about aspects of
how it works how people are using it a
lot of fun kinds of examples as well so
there's there's a lot of material for
you to play with
but basically tensorflow is a fairly
low-level toolkit for doing complicated
math in the support of machine learning
deep learning deep neural network
capabilities it's targeting researchers
who know what they're doing to build
experimental learning architectures to
play around with them and to turn them
into running software now it's it is a
data flow graph based system right so
we're gonna define these graphs and
these graphs are gonna represent
arbitrarily complex kinds of operations
and then we will invoke those graphs
within some kind of run time session
that will pass data into it attach them
to computational devices like CPUs or
GPUs and in in newer releases also then
allow you the capacity to be able to
create the jobs and then schedule them
against distributed knows so you can
take advantage of clustered technology
or clustered instances of CPUs and GPUs
as well the nodes themselves represent
operations like addition multiplication
solving certain kinds of problems we'll
see lots of little examples of this and
then the data and the shape of what we
call tensors flows through the nodes as
part of a invocation of the the run
activity on a session one of the nice
things even though it is fairly
low-level it does abstract up above the
kinds of coding that you have to do with
an open CL or a CUDA kind of interface
to CPUs and GPUs in this case you define
the graphs and the graphs themselves can
generally be scheduled two CPUs or GPOs
either automatically or you can have
complete control over it if you want to
be able to do that but the
romantic interface is not all that
different and so it's it's it's a it's
an abstraction above sort of low-level
GPGPU general-purpose GPU computing but
operationally it's still a fairly
low-level API and so people are building
higher level abstractions on top of it
so you have to do less code to take
advantage of it as I said now we were
able to do distribution across clusters
as well it's got ecosystems of tools
that surround it in terms of as I said
optimizing the models visualizing the
graphs seeing what the workflow is going
to look like seeing what the results are
doing as well and what we'll touch upon
many of these so you should do this if
you want to play around with some new
machine learning capabilities build some
models if you want to iterate on your
model testing strategy in an exploratory
data analysis phase this is a great tool
for that compared to trying to write
this stuff by hand in C++ or or whatnot
it's a lot easier one thing I should
point out the training phase is still I
believe completely Python based so if
you're not familiar with Python that
that may be a bit of a hurdle the
runtime operational models can be
deployed into C++ JavaScript Java go
other kinds of operational
characteristics but the training phase
is still going to require you to be
functionally capable with Python as I
said the the a be testing support is
really useful so if you want to say how
does it work if we do this how does it
work with if we do that if you take the
same inputs and run them through
different models simultaneously and see
what the answers look like as a way of
deciding when when you've done well
enough when your predictions are
successful enough to move forward all
right so the name tensor is really just
a fancy abstraction over an
n-dimensional matrix part of the the
focus is that the API is do not change
substantively when we're doing scalars
or one-dimensional or two-dimensional
kinds of matrices
so three is a rank zero tensor one two
three is a rank one tensor with a shape
of
three three three values we have have
arrays of arrays arrays of matrices for
rank two tensors but again the API is
when we say add these two things
together it doesn't matter as long as
the shapes are similar
they'll the underlying capabilities will
will do that for you so don't have four
or five different ways of doing the same
thing we see this visualization and I
know it's hard to see up here but this
is tensor board showing you the graph of
operations and as your as your systems
run it can dump everything to a log so
you can go back and visualize after the
fact and sort of see how the learning
process accumulated over time so this is
one of the nice things that comes with
it and we'll see some smaller examples
that are easier to read installing
tensor flow simply like just out of the
box here's the Linux installation here's
the Mac OS installation is relatively
straightforward if you do need to get
into more tweaked configurations to take
advantage of GPUs and things like that
it is more detail-oriented I will say
even though there are several steps the
handful of times that I have installed
this on multiple platforms it always
works as long as you're sort of careful
and pay attention but it can be more
complicated to to tweak it to take
advantage of GPUs and whatnot but the
simple installations are as
straightforward as pretty much anything
else so let's look at some basic
operations this is Python so in a Python
environment whether it's in a
command-line session like this or
through a Jupiter notebooks or something
you can import tensorflow as TF as long
as it works your configuration and
installation is pretty happy and then
you could interact with the tensorflow
API to create a couple constants so it's
got the the typical data types that you
would expect floats int boolean strings
things like that in this case we're
creating two nodes one is a constant
with a value of three and we can tell it
what type it is another time you can
give it a value and it'll and
the type from the value now at this
point we have nodes in our graph right
but they're not actually going to
execute until we attach them to a
session so if I say you know print out
nodes 1 &amp;amp; 2 will see that they are
constant sized temperature constant
shaped tensors but the actual values
aren't evaluated until it's part of a
session so we need to create a session
so in this case I'm creating a local
session the computer that this is
running on has two GPUs two GeForce GTX
1080s
which I got to take to conferences like
this and I paid attention to all the
numbers to put together a really nice
computer except the weight and two gtx
1080s
is like a 20 pound notebook which is
kind of hard to take on airplanes and
things so alas I will show you outputs
from running on these things without
actually having that computer here
you'll notice that it identifies the
various GPUs that are available to it it
can take advantage of these in its own
scheduling algorithms or you can also
take control and schedule operations to
particular computing devices whether
it's the CPU or one of your GPUs but at
this point we have a session and we can
then say run and evaluate the the two
nodes in that session and at that point
we see the values of 3 &amp;amp; 4 printed out
right because we are we have a context
the context is attached to a computation
of device we're able to evaluate the the
default graph in this case which just
has these two nodes in it now you can
build more complex nodes so I can create
a new node that is based on creating an
adder of the two constant nodes that
we've seen so far so at this point we've
created a new node in the graph and if I
run that against the session it will
it'll it'll show you this is obviously
visualized with tensor board we have two
constant inputs we have the adder and if
we run that through the session it will
print out the values seven now one of
the nice things that
floo does to make the development easier
is it's got operator overloading so a
lot of you know I think two to three
dozen operators have been overloaded so
you do not have to manipulate the graph
directly you could just say run node 1
plus node 2 and that will create the
equivalent unnamed adder function for
you in in the current default graph and
the the results are obviously the same
we don't want constants in most for the
most part we want to be able to pass
data into our graphs so we can have
placeholders in this case I could say
I'm going to have a placeholders I'm
going to call a and it's gonna be a
floating-point number I have another
placeholder called B and I'll create an
adder that will add those two things
together so again we're invoking the
operational overloading to create a
graph that has the adder and the two
placeholders and again this represents a
structure of computation it's not until
we actually pass data into it as part of
a running session that it'll execute so
now with this adder node in place I
could say pass in a dictionary and
assign the value 3 to a and 4 point 5 to
be and obviously the value is 7.5 but I
could also pass in a different shaped
tensor in this case 1 1 comma 3 and 2
comma 4 and then it'll add the the
tensor the matrices together the
operational interaction with the graph
is is identical so this is this is where
the tensor abstraction over different
shaped data becomes useful and friendly
to developers we can take these nodes
and we can save them off we can use them
in different parts of the graph
structure and so in this case if I
wanted to do something with the outer
node by tripling the result then here's
another operational overload where the
graph would be manipulated to have a
multiplier to take the output of the
adder node and multiply it by a new
constant so now the graph is extending
we could pass in our placeholder values
they will the value get added together
that serves as an input and then there's
a constant that's associated with it
to to have the ultimate activity so in
this case we're going to add in triple
run the atom triple node through the
session pass in the values three and
four point five seven point five times
three is going to be twenty two point
five now tents are bored I said it's
going to help us visualize these graphs
obviously these are very simple graphs
the as they get more complicated we're
gonna want to be able to see what's
happening so here we see two constants I
can you can provide your own names for
them you can multiply those things
together you can create some more nodes
with their own names in them build a
session and run the session and then
we're going to dump the results of the
session graph into a directory using
what's called a summary writer close the
writer close the session and now we can
start up tensor board pointing to the
directory where we dumped out the logs
and navigate to it through a browser and
we'll see in this case the shape of the
graph and depending on the events of two
fire
activities of evaluation of the graph so
these are the kinds of things that you
can do with tensor flow from a fairly
low level let's look at some more
complicated examples I will try to
explain this if you do not have a
background in machine learning I
apologize but what we're gonna do is
we're going to build a linear model and
a linear model can be used to make
predictive purposes it can be used as a
classifier and so in this case we're
gonna say I don't want placeholders I
want what are called variables and the
reason we want the variables is because
as we iterate over models in learning
phases the the variables will get
updated whereas the other the
placeholders and things get reset each
time we pass through the graph and so in
this case I want to take the weight the
weight value and the bias value is
ultimately our line is weight times X
plus B or in its more familiar form y
equals MX plus B the y value is equal to
the slope times the X input plus the the
y intercept so in this case we're
building up a linear model
we're seeding it with two values the
for our X inputs are going to be wrong
so the results are not going to look
great but the whole point is we want to
then learn what these values should be
and then build a predictive model from
the the data that we pass through it so
in this case we're gonna say let's set
the weighting variable to 0.3 and the
the bias to negative 0.3 build a linear
model out of multiplying that weight
times our placeholder X values add in
the bias and now we can go about running
this variables have to be initialized so
here we run the global variables
initializer
and then they are not reset until you
explicitly reset them the init that is
created here is another node that has to
be executed in a session so we
initialize the variables by running that
node in the session and that can
obviously get arbitrarily complex and
then we can run the linear model and
interpret interpret interpret it for the
values 1 2 3 4 4 X so we're going to
pass in each one of those X's run it
through the wait times the X plus the
bias and generate these results now
without doing some visualization or
whatnot it would be hard for us to sort
of get a sense of whether these these
are good results for that linear model
based on our data so if we say ok here's
some sample data that should work with
that we're gonna pass in the values 0
negative 1 negative 2 negative 3 to have
a learning phase you generally are gonna
have to have what we call a loss
function or an error function and we
want to see how far off we are then the
learning phase is going to try to
minimize how wrong we are so in this
case squared deltas I want to see for
each of the X values and the y value is
how far off we are we square it because
you could be off in either direction and
we don't want that to play into it so
I'm gonna build from the linear model
minus the Y values a squared Delta's
node and I want to reduce that down to a
flat value and so when I stake
my inputs 1 2 3 4 and these Y values for
the linear model that was set up with
the wrong weighting and bias we see that
we're off by quite a lot right those are
the wrong weight and biased parameters
for that model for seeing the the data
that we have so what we want to do now
is we want to train that to find the
right weighting models just to show you
what will happen when we get there if we
reassign W and B to be the correct
values of negative 1 and 1 which is not
how you do machine learning right you
don't tell it what the answers are and
then say ok we're done but we see if we
have the right answers in there and we
rerun our loss function with those
values we now see that we have zero
error right these are the correct models
for those X and Y's these are the right
parameters for those x and y's with our
linear model now to get there we're
gonna use some kind of optimizer and a
very very common optimizer in machine
learning is called gradient descent it's
a way of looking at the error curve and
determining when you're at the minimum
so how how how much less wrong can I be
if i if I'm able to iterate over this so
I tweak the results am i less wrong
tweak the results am i less wrong so
that approach is going to then try to
minimize the loss function for our
linear model we can then say for our
session initialize the variables iterate
a thousand times train it against that
data set and without more data you know
we're not going to get to the perfect
results but we can see now if we execute
and run this the the waiting has been
reassigned to negative 0.9999 which is
roughly the negative one that we hand
put in there and point 999 which is
roughly the 1.0 that we put in there to
correct the results so the point is with
the training data with our linear model
we've used an optimizer to minimize the
the result so that's that's kind of how
machine learning works at least one type
of machine learning you feed
in you you let it try to become less
wrong
you've generate the model and then the
model can be used for some kind of
predictive or classifying purpose we see
fairly complex graph structures created
behind the scenes by just simply
invoking the use of the optimizer over
the loss function now let's look at how
this actually works in practice right
what are the kinds of things that you
can do with this and I'm not going to go
into the details of exactly what's going
on behind the scenes but operationally I
think you'll understand what's happening
and and how this is useful now this
example comes from a professor who works
in this space and he's got some really
cool blogs about learning different
machining learning frameworks
he puts them through the same kinds of
tests and sort of sees like well how
hard is it to get up and running how
easy is it to verify the results and so
he's intentionally going to create data
of a particular shape and then try to
see how tensorflow uses it
how tensor flow lets you figure out say
a linear classifier or a neural network
classifier for this so the first kind of
data that he's going to create looks
like this and what's interesting about
this is most of the values are on one
side and the other values are on the
other side which means we could build a
linear classifier down the middle and be
able to say all right here's a new data
input which side do you end up on I'm
gonna categorize you as this side versus
that side so if this is good customer
bad customer good credit risk bad credit
risk whatever your categories are a
linear classifier would work very well
for this data now this is not real data
this is this is data that's shaped
intentionally this way for his his
testing purposes but with that data in a
spreadsheet he's able to run he's got
his little Python script he's using a
classifier called softmax which takes a
series of values turns them into a
probability distribution that adds up to
one and then says okay what's the most
likely category so when we have two of
these things that's basically
to say Oh your your this category or the
year that category so with the training
data with the the simulated data we he
says let's use five training sessions
printout four mostly we see it go
through the training process we see it
generate the weight matrix and the bias
vector for a linear classifier so now
we're trying to operationalize the kind
of thing that we just did for the for
the heck of it and we'll see now that
the line that's generated accurately
with a hundred percent accuracy is able
to categorize that data set so obviously
real data wouldn't be look quite like
that but this is an example of going
through the process of being able to
accurately predict that so that's the
expected result right if you have data
that is nicely linear classifiable you
should be able to get a very good result
to classify the results how well do you
think a linear classifier would do on
this data set not nearly as well right
but doesn't mean we can't try it right
so we can try these are he calls these
the moons the intersecting moons data
sets if we say use the same soft max
classifier give it 100 chances you know
100 training sessions to go through we
see that we come up with an accuracy of
0.8 six one that may be a passing grade
in school but that's a pretty poor
result from a machine learning mechanism
now you'll see like we get a lot of the
data right because most of it falls on
one side most of it falls on the other
but these things that are sort of in the
middle and either side of the line is
where our inaccuracy comes from so
basically a linear classifier would be a
bad attempt here so what we can do
instead is to try to use a neural
network with a hidden layer to find a
nonlinear classifier that based on the
shape of the data will be able to we'll
be able to intuit that shape and be able
to use that from a classification
purpose and we can see we're able to
very quickly jump that up to 0.97 which
is a much stronger predicted prediction
capabilities
and it's gonna look something like that
now there's a lot that goes into this
into this process when you go through
the training process you what do you
want to avoid overfitting where you're
basically reflecting the shape of the
training data and not generalizing well
to data that you haven't seen yet that's
that's all part of being a discipline
machine learning expert but in this case
where he's just trying to learn the
framework and learn how to come to
conclusions that he's expecting it's a
relatively straightforward process to
get to this point now here's a final
data set again how well do you think a
linear classifier would do here probably
not well at all right so let's try and
we see that we get an accuracy of 0.4 3
right it just it doesn't work well
because of the shape of the data but by
using his hidden learner because of the
way the data is shaped it's actually
able to get a 1.0 100 percent accuracy
by building a nonlinear model that looks
like an amoeba but accurately reflects
the shape of the training data so these
are the kinds of things that you're
going to do with with tensorflow
he's obviously just trying to test it
out you know take it out for a spin and
sort of see what it's like but you're
gonna have data you're gonna train
models you're gonna test it against its
predictive capabilities then you're
gonna want to take that and
operationalize it somehow we don't have
time to go into all that but I just I
want you to understand procedurally what
it is that we're trying to do so let's
look at some some fun examples that are
possible here as well there's a this is
coming straight from the tensor flow
tutorial there's a set of scripts and
and the tensor flow models github repo
that you can check out and using what is
called the inception v3 model which is a
model that was trained off of I think 14
million images and has a what we call a
top 5 error rates in this low single
digits I can't remember it is like 3%
tremendously successful way of
identifying
individual objects within a scene so
this is this is doing much more
complicated deep learning kinds of
things behind the scenes but these are
all tools that are available to you
intents are flow and you can walk
through it so to do this kind of thing
yourself just clone the repo go into the
imagenet directory and within there
there's a script that allows you to run
against an image and if you don't
provide an image it will use its default
chip which in this case is obviously a
panda and so if you say run classify
image and don't give it something to do
it will say sorry it will say yep we
think this is a panda and the score is
off the side of the screen but it's very
high right so you say okay well that's
your test image I hope you get that
right what happens if you provide
something that you know obviously it
looks like a you know canonical panda
what happens if you give it something
with two things in it well under the
hood it's using convolutional neural
networks which are mimicking how the eye
works it's finding edge detection and
patterns and finding out like
mathematically what's the most important
part of the scene so in this case we
have a flamingo and a duck and it turns
around and says 84 and 0.8 for
confidence I think that's a flamingo
right it just kind of ignores the duck
but that is a canonical flamingo right
shape color wise etc so I thought okay
what what happens if we run something
that's less canonical so most pugs the
dog are in my estimation black and tan
so I thought what happens if you give it
a pug that is all one color right again
keep in mind the model is based on the
images that have been passed into the
network so if there weren't a lot of
pictures of all black pugs it may not do
as well but as we see it does quite well
so then I thought what happens if I pass
in
pictures of my dogs now my dogs are cute
as hell but they're a breed called
Norwich Terriers and it's a fairly rare
breed and there are only like six or
seven hundred puppies a year though that
are born and so I thought all right well
maybe there wouldn't have been a lot of
pictures of Norwich Terriers in those
images I don't know
Norrish Terriers are quite similar to
what are called Norfolk Terriers the
only difference is whether their ears
prick up or flop over they're often
confused for Australian Terrier
Australian terriers and every once in a
while somebody will come up and say oh
is that a toto dog and I say no no no
Toto is a cairn terrier these are
Norwich Terriers I wasn't like this but
I I became like this so I was surprised
when I ran this and saw that not only
does it strongly and confidently
identify the dogs as being Norwich
Terriers it also makes the same quote
mistakes that humans do right and says
oh well that could be a Norfolk terrier
it could be a cairn terrier could be an
Australian Terrier and generally my
explanation for this is either people
made mistakes in the training phase
right they've labeled images incorrectly
and that has been captured in the model
or the models are so successful at like
distinguishing what's unique about the
breeds that those mathematical
differences are are sort of clustering
in weird kind of ways as well but we can
you can play around with this you can
you can build systems around using the
image of the inception v3 model to do
your own image detection tens of flows
got good support for for that one really
amusing thing that came out of the
sensor net from the tensorflow summit is
there was a Japanese guy who was not a
machine learning expert per se but he he
was a researcher biomedical researcher
or something and his parents were
cucumber farmers and they were getting
older and aged and he said what can I do
to help you I'm imagining without
actually going out and farming
was in the back of his head and his mom
said that the most time-consuming thing
that they had to do was to sort the
cucumber's because different markets
were more tolerant of funny shapes than
others and so he figured out what those
shapes were came up with training
examples of images used a Raspberry Pi
in a couple of cameras and tensorflow
and trained it to recognize and sort the
cucumber's automatically and so again
this is somebody who did not have a
background in this but the problem was
fairly well constrained the tools he
understood how to use the tools you can
go through and build systems like this
as well right you're not innovating
necessarily in this space but you're
taking advantage of the tools that are
available to build something that could
help classify images into simple
categories so what do you do next once
you understand tensor flow once you play
around with it once you get your models
looking at the various api's they've got
tools for targeting Android and iOS
they're in Raspberry Pi they're
interested in moving into new mobile
platforms so if you have one that is not
being met by their existing libraries
they're very open to to people asking
for support for these platforms and then
you could start to build architectures
where some portion of the code runs and
the mobile platform but you don't
necessarily want to download the huge
models one of the things I didn't point
out was the first time I ran this it
went through and downloaded the
inception v3 model which was about a 90
megabytes compressed format that I think
expands to a couple hundred megabytes so
you're not going to want to have a bunch
of those things necessarily showing up
on your mobile phones
so using tensorflow serving to serve up
the models in a back-end capability to
distributed architectures that want to
take advantage of those models is a
useful way of supporting more complex
architectures sir tensorflow serving
also has support for operationalizing
optimizing and doing the a/b model
testing so for a given set of tests you
could say here's this particular input
and here's the result with this model
and here's the result
with that model make sure that dropping
new models into place is not going to
tank your predicted results recently
Google announced support for their
research or research research cloud so
you can spin up cloud instances with
support for tensorflow GPU based
computing and things like that for a
fairly inexpensive approach it's not
generally available there's I think an
application process to get access to it
but we're gonna see more and more of
those sorts of things showing up
recently distributed tensorflow has
gotten to be more prominent it's always
been part of the story but the tooling
did not make it as easily or accessible
so the idea is that you'll probably
control your your containers with
platforms like kubernetes or something
like that but you also have the ability
to programmatically start and stop these
things within tensor flow graphs
themselves but beyond that then when you
create a session rather than creating a
simple local session you could use
Google's binary G RPC protocol to to
target an instance that's running on a
particular port on another machine so
you can say I want this to run over
there or I want this to run against a
cluster of things and you can even do
things like do the the training
operation locally for a particular task
of a particular job and then say
schedule that to run against clusters
targeting you know particular district
distributions of how the data is being
broken up there's a lot of cool things
going on in this space as I said the the
tutorials are getting better and more
up-to-date to the api's we're building
higher level abstractions on top of
these lower level capabilities we have a
wide variety of learning models and
visualization tools and optimizers and
things like that available to us new
things coming all the time being able to
take advantage of clusters of CPUs and
multiple GPUs is largely managed for us
by the the framework and then once you
go through your training phase generate
your your mod
turning them into running code that
you're able to keep into production and
keep happily and successfully produce
making good predictions is all part of
the tensorflow ecosystem so I encourage
you to dig into it more as I said my
email address is here please feel free
to contact me if you want to say like
I've got a background in programming but
not statistics what's a good next step
for me I I'm happy to try to help push
you in the right direction</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>