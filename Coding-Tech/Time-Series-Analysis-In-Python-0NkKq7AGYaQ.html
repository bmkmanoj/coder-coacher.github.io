<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Time Series Analysis In Python | Coder Coacher - Coaching Coders</title><meta content="Time Series Analysis In Python - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Time Series Analysis In Python</b></h2><h5 class="post__date">2018-02-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0NkKq7AGYaQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so welcome to time series analysis
with Python my name is a Lee Nielsen I'm
a software engineer at a company called
one drop we do diabetes management we
have a mobile app and a web platform
what it means is I look at a lot of time
series data because like many other
chronic illnesses diabetes is a stream
of time series how's your blood sugar
what kind of carbs are you eating and
it's a heterogeneous carb series and
it's in a regular time series because
people don't do things at regular
intervals right none of us do even if
you have lunch at the same time every
day I can tell you none of our users are
quite at the same time every day so I
think time series are really interesting
they're also really timely so time
series are something that's been around
for decades and decades and even some of
the best stuff was done in the 40s and
50s but they're newly relevant and newly
challenging because the whole Internet
of Things big data I would argue that
everything as a time series there is
very little modern data collection going
on where we don't in some way keep track
of what we're tracking right so we don't
just say at my website right now I have
2,500 people and then in an hour I have
2200 people because actually I have the
technology to know how many of those
people are the same right so as you know
just like with Google Ads that follow
you around or Bluetooth monitors that
follow you around in stores right we are
following individual actors right now
so almost all big data is time series
data or can be construed that way so
here's an outline of what we're going to
talk about today why time series a quick
panda's intro I am NOT assuming that
people here are full time data analysts
I'm trying to make this friendly also
for people who use Python but maybe
don't do data analysis then we're going
to talk about dealing with dates and
pandas reading and manipulating
timestamp data which as we know can be a
pain in the butt in all languages I
happen to think Python is really easy
and friendly once you get to know it
which is very nice so I like people to
become aware of that and then we're
going to talk about some common
analytical tools for data exploration
and do a little bit of prediction and
classification some caveats to get
started time series analysis is a
particularly tricky and controversial
field I'm going to show
why and adjust a few slides you need to
read more if you want to do this right
if you want to do this at work or at
school and draw real conclusions or make
real arguments this is not going to get
you there in three hours
this is to make you aware of what's
possible and what's interesting test for
goodness of fit right that there's
always the caveat right your AIC is
never good enough right you are a person
and not an automaton for a reason I feel
that's especially true in time series
domain expertise especially important
you are trying to figure out how a
process is working over time so the more
content you have the more understanding
you have of your system the better and
the better informed your models are
nobody can fix a lack of domain
knowledge and I'm gonna be fast and
loose today about assumptions are that
are required for models etc whenever I
don't specify but should you should
probably be thinking that things are iid
normally distributed mean zero if you're
not a stats person that just means it's
a fairly normal type thing we're not
going to assume that things are
dysfunctional or especially weird and
that tends to be true ish for real-world
data most of the time so why why time
series where did time series pop up well
all over the place many of the most
controversial questions in human
existence I would say arise from time
series and if you see on the side I've
got two plots on the top I've got a plot
of polar ice coverage over time
worryingly it seems to be going down
right and it's a big question is it
really going down are we fitting the
time series properly below that I've got
a plot about whether crime is going down
over time right is it going down or is
that just a blip these are questions
that actually drive how our society
functions how we allocate resources
whether we call ourselves Democrats or
Republicans right is partly like what
you think about these time series so
these things are everywhere
there's also speech recognition right
that's another area when you're thinking
about how do computers figure out what
you're saying or what you're going to
say that's a time serious problem in
another way right you could argue that a
lot of natural language processing is a
time series problem that's one way to
think about it
science experiments also these are
actually taken from when I was in a
physics PhD program
we used to just sort of pull on
molecules and see what they did as we
pulled nobody called it a time serious
problem but that's actually what it was
we would run many thousands and
thousands of five microsecond time
series experiments and we thought we
were just looking for one feature but
really we were collecting data about
time oriented behavior and of course
economics government policy right trying
to predict what will happen it's very
important that we be able to allocate
resources for our economy for our
government programs etc so the better we
can predict things the better we can run
our country lots of people who work for
the government actually spend their days
trying to do these things right all
these government actuarial type people
they're actually time serious people and
finally in the news right many news
stories you see that are about what's
going to happen they're actually about
time series right so there was a lot of
press once Donald Trump was elected
saying what's gonna happen to the stock
market and looking at a what happened in
the stock market when past presidents
were elected and then sort of
specializing well what happens when the
Republican presidents win or what
happens when pro-business candidates win
and then on the other side you have a
time series comparing how unemployed men
and women spend their time right so you
can also do sort of sociological type
news stories looking at how do you spend
your time and that's a time series -
final warning be careful again it's
especially true for time series that you
need to know something about your data
so what I've got here is divorce rates
in Maine versus per-capita consumption
of margarine for the entire country
so you might try to Gaslight me and say
well maybe they really are related but
I'd say it's Maine versus the entire
country so apart from the margarine
versus marriage a bit there's also the
geologic or geographical mismatch you
can actually look at entire web pages
that are full of these time series of
these fake causation correlation type
things they're pretty funny I don't know
how people have the time to find them
but just be very aware of that and that
can pop up even in your own work so
something to keep in mind okay we're
gonna jump right into pandas raise your
hand if you have used pandas at all okay
so it looks like 70 ish percent of us
have at least used pandas
raise your hand if you've used pandas
but you don't feel too comfortable with
it okay so we're gonna blow through
pandas probably in less than ten minutes
it's just to orient those who haven't
seen it at all
to give you an idea of how it works
because we will be using it today okay
okay so here's our Jupiter notebook and
we're gonna start by creating a data
frame so data frame is a little bit like
an Excel table or like a sequel database
of some kind it's rectangular data you
can make a data frame in all sorts of
ways I'm making one with people's names
their ages and their phone numbers every
data frame has an index if you don't
know what it is you can take a look at
it in this particular case up here I
actually assigned my index it's
important to understand that indices
exist and that we can use them we're
going to use them today to index by time
so that's why I want you to know about
indices you can also create a data frame
with numpy arrays and column names so
that's what I did here same thing right
so I have an index in this case I'm
using names and I can provide a list of
column names in a variety of ways you
also have Ceres which is really just
like a single column in a data frame
that's also really easy to do here's my
series and I can look at my series and
then you'll notice here my series is
index just 0 to 5 but maybe I don't like
that index and I want to change it I can
just directly assign to my index like
this and rename the indices does it work
and I can also for example easily detect
where I have not a number style of data
data frames and series also play nice
with plotting right so you can just plot
like this just dot plot it's a class
method you can plot a series you can
also plot an entire data frame it will
color things nicely for you etcetera
you also get easy indexing which is one
of the nice things about both series and
data frames so for example in my s data
frame maybe I only want things with an
index greater than C boom I can do that
right so if I want to look at a subset
of people by alphabet
or by age or whatever I can index on
that and just very quickly select who I
want what if I just want data that's
clean that doesn't know I can do that
too what if I only want to look at one
column that's also very easy to do and
there are many ways to look at it right
so I can either quote it in brackets or
I can just act access it like an
attribute can also locate things by a
label and what you'll notice here is
even if I put in like column names that
don't exist yet it will play nice with
it so pandas is just very forgiving good
to know meant to be encouraging for
those of you who haven't used it before
and I can locate things by conditions
right so let's say I only want to see
the students who are over 17 years old
also very easy to do easy to summarize
your data right so just like you do
group bys and sequel for those of you
who may be of use sequel but not pandas
it's easy for example to get a mean on a
column a max on a column a min on a
column let me just actually put my data
frame here so you remember so that
stuff's super fast I can also easily add
columns like this I can very quickly for
example find out who had the highest
grade super fast I can do group bys just
like with sequel I can group by age and
then get a mean of some other
characteristic I can also bend things on
the fly right create categories on the
fly so if I wanted to do that I usually
could and finally I can do things like a
plyo lambda function really quickly
right so if I just define a lambda here
you can see I want to just age up my
students by a year boom it's done right
and there are my students instead of 17
and 18 they're 18 and 19 now a couple of
built-in methods that you should be
aware of will use a lot today and that
are just handy or mean counting
correlation and hue max okay so if you
have questions about pandas as we go I
think the best thing is to just treat
each other like a resource you don't
need a ton but it's it's a good tool
even if you're not a data person okay so
that's our pandas
so now we're going to talk a little bit
about dealing with time so we're going
to jump right into the second notebook
dates and times
and we're just using pandas I don't even
know for actually using numpy pandas
just has an enormous enormous quantity
of time related functionality I would
encourage you to look at the docks but I
want to show you some really nice things
so the first thing is we can easily
generate series of times I can specify a
start date and the number of periods and
the frequency so if you look at this one
it reads very nicely too right I have a
range and I just want to go from July 15
2016 at ten fifteen ten periods and I
want frequency of M what do you guys
think M is monthly probably right we can
always confirm and here we are
I've got 731 2016 8 31 2016 9 30 2016 so
what kind of monthly is this yeah last
day of the month right there are ways to
set it to be first day of the month
right you just look at your parameters
you can get first day of the month you
can get seventeenth day of the month
right it's all about just knowing what
parameters to adjust but what you'll
notice right away is it takes human time
right which is what we want right when
you're dealing with time stamps data
maybe you're doing science experiments
and you've only got to deal with 20
microseconds at a time if you're dealing
with anything related to humans you're
gonna have to deal with calendars and
that's what pandas has really built for
right so I can have a monthly frequency
what other frequency might I like maybe
I want a daily frequency now I've got
daily frequency what's this one put in a
B bi-weekly well I went from 7:15 2016
to 718 2016 so that's I didn't skip a
week 719 mm-hmm weekdays business days
exactly right so it's got a lot of
functionality to deal with business
related things you can define a custom
business calendar you can use a standard
business calendar so you can do things
that will say match the stock market or
maybe match your company if your company
only does Monday to Friday etc okay so I
don't know if we can all look at this at
once but I just want to make you aware
of some of the parameters right so a
date range
you can specify some of these you don't
have to specify all of them and if you
want to know how many of them you need
to specify you just have to think about
it logically right so would I specify a
start and end number of periods and
frequency no why not it would be over
specified right so it would probably
forgive me if they're all consistent but
if they were not consistent it's going
to throw an error right like if I say I
want to go from May 17th to May 19th at
a daily frequency and I want 20 periods
pandas is going to come back and say
well that's not actually possible unless
you want me to do some bizarre
time-traveling nonsense
so it will not let you do things like
that which also means that you can be
lazy right we've all been in the
situation where sort of like I don't
really want to figure out what those
dates are I want someone to do it for me
welcome to pandas right pandas is
actually now taking care of that for you
so of these four you need to give it
enough to know what dates to actually
generate to have it be determined but
not over determined there's also things
that are just extra nice things right
like clothes make the interval closed
right you can do left right or both
that's 1st of the month last of the
month beginning of the day end of the
day beginning of the business day end of
the business day you can name your index
a if you are going to use a series with
a named index for those of you
comfortable with pandas and this one's
also nice time zone right I don't know
how many folks have worked with time
data in the past time zones are a huge
pain in the butt and let's also remember
daylight savings which has to do with
time zones pandas takes care of all of
that and we'll see a bit more of that
later but it's just really beautiful as
far as how effortless it can be once you
get comfortable with the API so where
did I where in my notebook go here we go
okay so there's our time stamp that
there's our date range now what if I
just want one right maybe I don't want
to range I just want one moment in time
well that's a timestamp right so PG dot
timestamp I can do this what do you
notice about my PD timestamp there's a
couple of things to think about here
it gives me the time of day exactly
right so a timestamp
I might just specify it as July 10th
2016 but actually a timestamp is a point
in time it can't just be a day that
would be more like a time period and
we'll see that soon but a timestamp is a
specific moment in time in pandas
they're actually specified down to the
nanosecond some people actually do use
nanoseconds for the rest of us we can't
ignore that but you do need to specify
something sensible if you don't it's
just going to put you at midnight right
with the zeros so you want to be aware
of that so there's my timestamp we're at
specified time of day how far do you
guys think I can go although I actually
told you nanoseconds so how am I going
to do that I've got 10 15 15 which 10 15
I am 15 seconds in what if I want 15
seconds and a thousand milliseconds well
you can't have a thousand 999 million
seconds decimal right 0.999 there we go
and look you see actually how it starts
adding one thing also to be aware of I
don't think this has been fixed it
displays up until the sixth decimal
point but it actually does store up till
the ninth decimal point so just a weird
glitch in case you ever are checking ok
and well here we see how much detail can
you add right 0 5 6 that's 3 7 5 7
that's 3 5 7 so if you force it to go
out to beyond the six digits it will
display and those are always there ok
now I don't know if we've got any
non-americans in the room maybe okay so
you guys will be especially interested
to be thinking about what's going on
with these strings here so pandas is
very forgiving as far as you can pass it
all sorts of things that feel intuitive
to a human talking about the day and it
will process them but is they going to
do it in the weirdo American Way or the
non weirdo non American way the rest of
the world the rest of the world way and
in particular I'm looking at the eat so
go ahead and experiment if you have the
notebook open and tell me which is
pandas universal or is it this snarky
American
format it's American so we've got 7 1
2016 vs. let me put this down 1 7 2016
if I want to test which one it is let me
go ahead and put in what I as an
American what I would think the date
should be and which one doesn't match it
matches the American formatting so
that's just something to be aware of you
could also just get caught unawares one
very cool thing is if you have a date
that's not ambiguous and you put it in
European format pandas will catch it and
fix that or not fix it for you but it
will know you're using European
formatting but in a case like a 1 and a
7 it will assume American there's a day
first parameter you're going to want to
set if you are using more standard
non-american dates so again just shows
how beautiful this API is how much
detail and thought went in although it
does favor Americans a bit okay
what are some properties of time stamps
let's go ahead and look at the time date
components you can see all sorts of
things if you click on that link that
are just waiting for you so all sorts of
things you might normally need to figure
out right like if you work in finance
and you need to know what quarter a date
belongs to it's there if you work in
health informatics like I do we care a
lot about what day of the week it is
because people tend to behave
consistently right maybe people are
stressed on Mondays and more relaxed on
Fridays and on Wednesdays they eat too
many calories or whatever so I can get a
day of the week very easily with dot day
of week you can also get day of year
right so if you find that people behave
seasonally it's very easy to match these
things up rather than having to compute
it yourself so that's very very handy
whenever you're wondering how do I get
this thing chances are that Wes McKinney
and the other folks working on pandas
have already put it in for you so those
are time stamps right so just to give
you a demo right what's the quarter July
10th apparently is the third quarter and
this is just an example of a really
smooth interpretation of the data
okay any complaints about time stamps
have I so like this is just a solution
to all your problems I'll tell you no
because I have the solution to all your
problems in the next section so what it
what do you not have yet hmm yeah you
don't have intervals right so there's
you're like yeah but most of the time I
don't meet my friend exactly at like May
17th at 1:45 p.m.
Oregon Central Time that's zero zero
zero for nanoseconds right I mean
usually there's some understanding meet
me in the next five minutes or like this
tutorial the understanding is it's gonna
be about three hours right so we care
about interest intervals part of the way
we get there is first we need offsets
right we need this idea that there's a
starting time and then we add some
component of time so first you've got
time offsets these are time deltas with
a lowercase D and here's an example
right one day in one microsecond and
there it is one day in one microsecond
one day in one millisecond there it is
one day in one nanosecond it's there you
just can't see it okay PD timestamp can
be added to PD time Delta right so if
you have a timestamp you can just add a
delta to it very intuitively here's an
example
I've got your life first at 8:00 a.m.
and I want to add one point at five
hours and that gets me to 9:30 exactly
as it should
PD time Delta 15 nanoseconds showing up
as a zero is that a bug no right it's
actually there it just Prince's zero I'm
probably harping on this a little too
much but for those who do use
nanoseconds okay range plus PD time
Delta one day what did I do here I took
my whole range of dates rather than just
a moment in time right so a time Delta
can be added either to a timestamp a
single moment in time or to a time range
now you might feel like well a time
range is probably just a list of
timestamps so what's special about it
well is that really true no because it's
got a frequency so that's something to
keep in mind the reason a time range is
a time range it's got a frequency it's
not just an arbitrary collection of
times
if you want an arbitrary collection you
can just generate your own and now we
get to our intervals so we've got time
spans these are called periods so let's
look at PDI period versus PDI time stamp
we've got a period 7 2016 again it's
American style right so that doesn't
mean like well I don't even know what
that would mean as a non American style
but we've got July 2016 we've got a time
stamp if I want to check if that time is
in that time stamp I check whether it
comes after the start time and before
the end time so just like a time stamp a
period has all sorts of useful
attributes such as start time and end
time just like I have date ranges I've
got period ranges a lot of the stuff
that's built in pandas is there because
it's going to assume you want to index
data by time so all sorts of things are
built ready to be indices so here I've
got a PDI period range we'll start from
the 1st of 2016 at 12:15 I want an
hourly frequency 10 periods here are my
periods right so how do you know it's a
period and not a date range well it says
period otherwise it looks very similar
right they play nice in exactly the same
way I've got my frequency it listed out
in this very human readable format etc I
can also do 60 T right that's just 60
minutes and what do you notice is
different here so I did a period range I
have the same starting time I did a
frequency of hourly versus a frequency
of 60 minutes yeah so what's going on is
here it's on the hour
because we looked at before right it
sort of picks a logical sort of human
friendly thing right so like when we
looked at the monthly monthly date range
it always just goes for the last date of
the month or it can go for the first but
it's going to pick some sort of nice
round point to benchmark you so that's
what it's doing with hourly on the other
hand with minute ly it's going to stick
to the minute you gave it okay and
okay so we looked at how you can
determine whether a time stamp falls
within a given period but try to remind
yourself how you would do it try to do
it before you load this in a bit okay so
if I have a period I have a start time I
have an end time how do I check if a
time stamp is within greater than start
time less than on time exactly right so
that you do have to write out for
yourself
okay we've also got combos so I also
want you to be aware when you've got
these offsets or frequencies you don't
have to use nice round numbers like 60
minutes or one hour or one month you can
arbitrarily combine stuff right so if
your boss comes in and wants customer
stats on your website every two hours
and 20 minutes because that's what his
hairdresser suggested that morning you
can easily do it so all those really
annoying arbitrary demands are seemingly
arbitrary Annoying demands become very
easy right so oops look at that so you
can easily compute whatever frequency
you want just good to be aware that
everything you your heart could possibly
desire with time related data is super
easy to do okay so how do I make a
pandas time series with some of these
aliases well we have already done that
but here are a couple of examples and
what's special now versus I've just been
showing you date ranges and period
ranges right so there are ways to
compute moments and time at periodic
intervals or periods that cover periodic
intervals now what I've done is I've
made a pandas series indexed by a period
right so now I have a pandas series
where each value these I just randomly
counted from 0 up I believe is indexed
to a time interval so instead of in
students gray it's according to their
names I am now doing the exact same
thing but I am indexing values according
to time of day or day of the year or
what-have-you right so I've got July 1st
2016 11:15 the value was 0 July 1st at
1315 the value was 2 I now get all that
nice pandas functionality for those of
you who are comfortable with it with
time I can do the same thing now I'm
going to do one that would normally be
really messy I'm looking at 60 business
days at a time and again I can just
index with time right I can now know
that my values correlate to a particular
time interval right so like 825 2017 it
was 5 for the next 60 business days
that's the interval so how do we index a
time series with a date range I showed
you above here with a period range
they're very interchangeable instead of
a period range I just type in date range
and what that means is I'm going to have
selected timestamps a specific moment in
time indexing my data rather than an
interval so you have to think about
which of these is appropriate for your
data right are you doing something like
monthly counts or are you actually
saying at this exact moment I just did a
did account right at this exact moment
in time I counted people on my website
versus over this month I counted people
on my website okay so what are the
different use cases well here's an
example
I've got TSTT so that's the one that was
defined with the date range it's indexed
by moments in time not intervals if I do
TS DT and I try to index it by July 1st
2016 11 I get this one thing 11:15 if I
do the period what do I get 2016 July
1st at 11:00 to 2016 July 1st at 13 I
also only get the one thing but it's
overlapping time now rather than a
single moment in time okay if you need
to convert between a date/time index and
a period index you just do two period or
two timestamp super easy to remember
right so a lot of this stuff you
fine you just play with it for a day or
two it just sort of starts rolling off
the tongue because it's very sort of
natural language driven as far as the
names okay so I'd like you guys to try
two things first go ahead and try to
create a time stamp with the European
style formatting good to be able to do
that and also figure out how you can
generate a string representation of your
time stamp so I'll just put two minutes
on the clock for that and then we'll
talk about it
okay so it's sort of been two-ish
minutes how do I set that day for a slag
did anyone do it yes okay so day first
equals true so hmm it would help to
spell timestamp correctly so if I am
American actually this doesn't make
sense at all if I'm American I'd have to
reverse it but if I am European what's
this date is in November or September
September am i really using the day
first bit right now though no right so I
have to if I want day for September I
want to do it this way is this this is
November for European and what's this
there's a September okay so day for
No thank you thank you okay
let me just get my snippet that's
probably better okay
to daytime here we go here's our day
first thank you and then string from
time is the other function you want to
be aware of and here's your format say
if you want a year two month today and
we can take a look at these and see how
they look
okay so questions about this are we sold
on pandas time functionality best thing
ever made your day and it's so easy and
cool okay
let's also talk about the usual pain in
the butt of timezone handling something
pandas has really just done a wonderful
job of making a non-issue versus the
nightmare it might be if you don't use
pandas okay so first we'll generate a
date range so we're just gonna have
March 6 2016 I want 15 periods
I want a daily frequency I look at my
range I've got my range if I look at my
time zone I'm not printing anything why
is that mom maybe I need this none okay
well that's why my pen and I Jupiter
notebook wasn't outputting anything I've
got no time zone because I haven't set a
time zone so that's handy
for two reasons one you can have a time
zone naive objects so that might be
helpful right maybe you actually don't
care about time zone maybe you're a
sociologist you're studying what time
people eat you don't care where they're
eating what time zone you want it to be
naive it's really convenient that you
can keep it that way right it's also
really nice that it doesn't
automatically send you to UTC right so
anyone who's sort of like as one of
their first computer projects started
working with Postgres or my sequel and
everything's in UTC and you didn't even
know it so you put in one date and you
get out another and you don't know
what's going on it also helps you avoid
that trap so it won't give a time zone
if you don't give one it won't sort of
stick you in and make it the wrong time
and it also respects that sometimes you
data that doesn't need a time zone on
the other hand if you do it's really
easy you can just put it in your
constructor as a TZ and there's your
time zone all right so that is very easy
to do if you are ever unsure of what
time zone you want or how it should be
spelled you can use the pi TZ package
it's very easy to get common time zones
of which according to the package there
are 436 let's just look at a few of them
to give you an idea it's not hard to be
a common time zone there's definitely
more time zones in the common than I
have ever heard of but there's actually
also like an additional hundred and 50
or so that are not common so just be
aware of that those are already in
Python for it to use what are some time
zones that aren't common well apparently
Indianapolis Mendoza etc okay so how are
we gonna localize a time stamp we're
gonna start with a naive time stamp PD a
time stamp I've just made a naive time
stamp if I want to localize it after it
exists right let's say I miss the
ability to pass it in in the constructor
I can just do dot TZ localized pass in
the name of my time zone and I will get
a time stamp with that time zone I can
also convert if it already has a time
zone I can convert so we've got T which
now has a time zone it's in US central
what if I want to put it on Asia Tokyo
time well I can do that and what do you
see has changed as I went from T to T Z
convert it converted the time so it
doesn't just change the label and leave
the time of day that's the thing to be
aware of it actually updates the time
which i think is what you'd want to do
that's the more difficult task if I just
wanted to change the label I could
easily just have set it myself to begin
with right so this once you give it a
time zone you fix a real moment and time
anchored to a place and then whenever
you adjust it you're adjusting relative
to that time zone if you haven't naive
you're always just going to have that
label of whatever time you said it was
okay what's the difference between TZ
convert and
easy localize will try to run TZ convert
on a naive timestamp what do you think
is gonna happen it'll throw an error
right which is what you'd want it to do
you have a valuable object when it's
when it's time zone naive right that's a
legit way to want to store your data so
you wouldn't want to just randomly slap
a time zone on without being aware that
you weren't properly converting okay
there's also some fun with daylight
savings right you will get weirdness
with time zones based on daylight
savings but you now don't need to deal
with this problem yourself pandas takes
time takes care of it so let's look at
the date range from March 10th 2016 10
periods and take a look at what that
looks like what do we see here we jumped
right we had our daylight savings time
and we jumped pandas is aware of that it
will take care of that for you be aware
of that right it's not going to not do
it either
right you'd have to go out of your way
to have it not take into account
daylight savings time what else do you
notice then like I this will just
default to daily period the reason it
defaulted to daily period is that's the
level of specificity I gave the date
range so just pointing out another
helpful feature if I don't give it a
frequency it says well if you talked
about a date you must want daily if you
gave me an hour you must want hourly
right more intuitive functionality there
but what do you what do you notice
didn't really give me daily or did it
because I went from 5 to 4 so in what
sense is that daily 24 hours later is
one way to think about it and another
way of expressing that is UTC right so
whenever you're sort of not sure what it
would do think well what would it be in
UTC what would it be in sort of a non
dysfunctional time zone and that's what
it's also preserving so those are the
two ways of thinking about it and
they're both consistent you will
sometimes run into ambiguous times
however so let's take a look here I've
got range
hourly which I have as
let's take a look at what it is so we've
got November 6 2011 I go from 0 to 1 to
1 why is that well because I said it
that way no I actually I said it that
way if you look at the if you look at
the list hmm it's time so naive but if I
localize it I get an ambiguous time
error why did I get an ambiguous time
error well yeah so some of these are
maybe the same time right maybe
something really weird and dysfunctional
is going on with this 1 o'clock one
o'clock thing so let's figure out how we
can deal with it how do we deal with the
ambiguous time error well actually you
can try using the ambiguous argument
right so when you make this error or you
run into this issue it will actually
tell you how to fix it and it will do it
sort of the intuitive way that I would
personally fix it if I knew the problem
myself which is this so what did it do
what happened here so it's 1 o'clock 1
o'clock that part is still kind of
dysfunctional and weird but it went from
minus five hours to minus 6 hours so it
recognized that what I was trying to do
was account for the fact that we had
fallen back so that we actually did have
that 1 a.m.
twice I want to express that with a
label but it says hey once I dissemble
you with this and related to UTC there
are actually two different hours so boom
it takes care of it hopefully you'll
never have this kind of esoteric problem
but if you do it's really nice that it's
taken care of and you can see here if I
just convert it to UTC it is now nice
and functional it's 5 o'clock 6 o'clock
seven o'clock eight o'clock nine o'clock
so very nice to work in UTC when you can
that is how pandas does its thinking
right it works in UTC which is really
just a way of saying it works on time
that isn't shifting around depending on
the calendar day etc also pandas goes to
amazing lengths to try to figure out
things for you right so
what's going on here I've got March 13th
2016 US Eastern non-existant time error
right so it will catch really funky
things that you're doing right I think
there was never a 2 a.m. on march 13th
2016 because of the time change right we
probably just went ahead to 3 a.m. right
because it's spring ahead in the in the
springtime so it will figure things out
like that
yes 3-4 for the hour it's so it's it
says it's 3 a.m. 3 24 a.m. so it gives
you the UTC time you guys probably have
a nice
yeah I'm 0.2 0.0 what are you
so you've got windows you look like
you've got a Windows - what's your
version of pandas Oh so-so maybe they've
made a different decision about what
would be logical so it's but it's three
let's talk at the break if one of you
can show me because I'm really curious
but the the idea is there are weird
times that either don't exist or have
weird meanings pandas does its best to
try to help you and give you sensible
output that we can summarise we'll
follow up on the the new functionality
okay so let's also talk a bit about how
you read in data right usually when
you're gonna work with time series data
maybe you'll do something like download
stock data from the Yahoo API or
download some governmental climate data
from some government website or maybe
one of your colleagues will give you
some printout from Google Analytics that
also has timestamps right and you want
to get it into Python so you can work
with it so here's an example of
something you might get you might have
somebody's hand you a text file and it's
got something like this which is not
actually how this looks
so let's actually look at the text file
what is that looks my bad so you've got
1950 tab-separated 1 and then a value
1950 tab-separated 2 and a value what
kind of data do you think this is go go
a bit further we've got 1950 all the way
down and we got one to 12 so what is
that
it's monthly data right so sure I can
read this in and then maybe write some
like row by row parsing to turn this
into some PD dot period because it's a
monthly data I want interval but I'd
rather not have to do that right I'd
rather just read it in properly to begin
with so the naive way I might just read
it and get something like this I don't
even know my column names and maybe I if
I'm lazy I'll just do it this way
originally 1951 2 etc and especially if
I look at at least 12 of these or 13
I'll figure out that it's the monthly
data and then I'll say you know I should
really do this a little smarter have a
better script especially if I'm going to
do this more let's look at the read fwf
Docs in particular I'm interested in
some of the things we can infer and this
one is one that jumps out at me
something like infer date-time format
right so just like pandas has a
beautiful library for dealing with time
it also extends that to reading and
writing data in common data formats so
let's think about how we can modify we
can pass in for example a parse dates
parameter which will tell it which
columns to use and then we can actually
say why don't you go ahead and figure
out that format for me so let's try that
and then take a look at what we've got
and now we've actually got dates if we
look at the index we've got a date time
index and then I can maybe give it some
name so it looks a little better and
I've got a month and a value now that
I've got a month and a value in an index
that is time-based I can actually find
out some useful information right
assuming I got this and I don't even
know what I've got I can find out
quickly what my min and Max are right so
get an idea of what timespan I'm looking
at for this data I can also thinking
about it realize well I don't really
want a timestamp right because this data
is not actually like at this second I
took this value it's actually a monthly
average of some sort so I'm going to
convert it to a period remember we've
got date times and we've got periods
this feels much more like an interval to
me right climate data what was the
particle count for this month versus
that month so I'm going to convert it to
a period and if I do that this looks
much more sensible right and the way I
can tell very quickly that it's done a
period is it's not even bothering to
include a date right if it puts that
date in I'm going to think oh it must be
using that but if it's only giving me
1950 o1 1950 o2 that's telling me it's a
monthly period it's not the first of
January 1950 it's just January 1950 so
there we've got that representation with
reading and dates with a PD doubt read
function you usually have several time
related parameters and sometimes the
inference can be faster sometimes
passing it a parameter string can be
faster with this file I'm using it's
really small it doesn't matter but if
you ever have like a few hundred
thousand rows or a few million rows some
mega data file you probably are going to
want to be a bit thoughtful about how
you ask it to parse the dates because
parsing dates is not necessarily that
fast so I've got a few examples here
right you can give it parse dates and a
date parser like this so if we give it
both the columns to parse and also a
date parser which tells it what kind of
string it's expecting let's see the
best-of-three 21.5 milliseconds per loop
seems okay and this second option what's
different I'm still giving it the parse
dates column it would be kind of
dysfunctional not too right I don't want
it to always assume 1950 as a year but I
am not giving it a date parser I'm just
gonna have it figure it out on its own
now and here this actually cuts down
right so sometimes you might think
you're being helpful but maybe you
haven't expressed something that helpful
so in this case I thought oh I'm gonna
be so nice I'm gonna make this really
efficient by providing a date parser but
there must be something about the way I
did it that's really inefficient versus
just the way that pandas already has in
the background so that's one thing to be
aware of is your custom solution might
not work so well in this last case I'm
going to just explicitly ask it to infer
the date-time format let's look at what
that does and that's about seven point
seven so in the background that might
actually just be the same as the one
above or it might not the point is try a
few things out with your data
find out what's fastest you can also
let's say you've made the mistake of
already reading the data in and not
parsing it as you did it or let's say
you just had it in a pickle or something
but you don't have things labeled as
timestamps you do have the information
you can actually have a data frame that
has columns with the separate components
of the date and easily combine them so
let's say we have a data frame that has
a day hour a month in your column sure
you could write your own custom function
to combine them but actually it's pretty
straightforward to just do a PD
date/time on that data frame and it will
intuitively figure out how to put them
together right you have to use proper
names for your columns but once you give
it that it will look at the date data
you give it and it will convert it to
the appropriate values now what do you
notice here if I happen to have added a
column that's not very helpful that
throws the whole thing off so that's
also good to be aware of okay
and finally truncating really easy to do
let's say you have a time series like
this one right going from July 2015 up
until April 2016 let's say I don't
actually want it all right I want to get
rid of things before 10:00 31 2015 and
after 12 31 2015 that's super easy to do
and it preserves frequency right so
there's a lot of functionality in pandas
where you can truncate index and you
will keep that frequency bed which is
very useful you can truncate it in a way
that does not preserve frequency and
you'll see that then you lose your
frequency parameter and you can't easily
get it back right you would have to
reset your index that's because there
isn't any frequency right if I'm going
from August to do
annually to February there isn't an
obvious way to higher-frequency that's
why pandas doesn't do it
pandas will try to preserve frequency
whenever it can though so if you see
here I'm indexing in a regular way right
I'm going to take every other value from
my original series in this case it is
keeping the frequency it's by monthly so
pandas will bend over backwards to try
to make your date data make sense okay
so that's sort of working with times and
working with strings that talk about
times
now we're gonna progress into thinking a
bit about how you're gonna analyze time
series data or how you're going to label
it very common analysis tax tasks so
often what you want to do is you want to
have a time series and you want it at a
particular frequency but you don't have
it at that frequency
that's called resampling and usually
you're gonna do this because you want to
match it to existing data or because you
just want to think about things in a
certain way right maybe you got data
that's time-stamped every minute but you
only care about every 60 minutes or
every day or maybe it's the other way
right maybe you have sort of an
irregular time series of some some
length right you've grabbed data when
you could and from that you'd like to
have a more regular series right don't
sort of tell me all the time
just give me that hourly snapshot even
if that's not how you collected the data
so if you want to do that that's called
resampling and we're gonna take a look
at an example where you would do that
let's start with just a time series I'm
generating here so we start with January
1st 2011 and we are sampling every hour
randomly picked 72 periods and these
values here are just you know random
value 0 through whatever let's say
though that I'd actually really like to
think about this data not hourly but
every 45 minutes I'm gonna use the as
freak class method go ahead and try this
out I'm gonna try every 45 minutes and
I'm gonna try a forward fill method
before I press ENTER or before you press
enter I'd recommend thinking about what
do you think that means
forward fill well let's think about what
our data looks like right we're starting
with
our early data we're converting it to
every 45 minute type data so instead of
this series right of Jan first at
midnight Jan 1st at 1:00 I am John first
at 2:00 a.m. I'm going to have a new
series that looks different in two ways
it's both under-inclusive and
over-inclusive in the sense that some of
my data points will just be gone for
example if I start at midnight my next
data point is 12:45 right so I have
midnight which I had originally but now
I have a new time 1245
let's go 45 minutes in advance 12 45
plus 45 what does that get me 130 that
means I have dropped 1 am so I will both
have new data points and I will be
missing old data points I have to think
about how I'm gonna fill in the values
right so if I drop data that's not a
conceptual problem it's just gone right
I dropped the data who cares but if I'm
adding a new point right a new index
what do I pick the value for that index
well there's lots of ways to do it one
basic way is called forward fill so what
do you think that means think about it
forward fill going forward in time
what's gonna happen so my 1 a.m. or my
12:00 a.m. is 0
what's my 12:45 gonna be right I'm gonna
carry it forward that's the forward fill
so let's take a look at that if we look
at it like we were saying at 12:00 a.m.
I've got 0 at 12:45 I've got 0 I skipped
that one I am but at my next data point
in my new series 1:30 that 1:00 a.m.
actually has influence right because
this one I am with a value of 1 has now
forward filled the 1:30 right so even
when I drop those data points as I'm
resampling the value and the information
there is carried forward to some extent
now the opposite of forward fill would
be a backward fill right that's my B
fill so let's go ahead and try out the B
fill option what's that going to look
like yeah the ones going to be in the
12:45 right that what's going to happen
is how we pick the values changes right
when I'm when I'm at a new data point
that didn't exist in the original time
wise instead of looking one direction
gonna look the other direction to figure
out right so now that it's backfill if
I'm at that 12:45 a.m. point in time
instead of looking for what came before
I'm gonna look at what came after to
fill and this is the sort of place where
that domain knowledge matters right if
I'm thinking what's the sensible way to
do this it probably depends on sort of
what the people who are consuming my
data want to know how the process works
right whether it makes sense to backfill
or forward fill what if neither of these
makes sense though
right you could also imagine a case
where someone says well I don't actually
want to see fake or imputed data I just
want to know what the value was if you
had it and if you didn't have it I just
want to know that you didn't have it
well you have that option too so if we
were to just have the method of none
what do we think will look like now
there we go so like the real values
where I actually knew at that time I'm
indicating what the measurement was I
will retain the measurement right so I
actually took that measurement at 12
a.m. I actually took that measurement at
3 a.m. but on the other hand the the
fake data points where I don't have an
exact measurement I will have that
indication when might this be useful
maybe you want to join with some other
time based data but you want to know
either the actual value or that you had
nothing right so all of these are
legitimate ways to fill your resample
time series depending on what you want
to know I don't know
yes yes you can make it interpret late
and I think we will talk about that in a
little bit actually and there's also of
course different ways to interpolate
right you can do a linear interpolation
you can do a polynomial interpolation
etc okay so what is the above code due
to the size and content of your data
frame well we already saw this right you
get more data points and in particular
you'll want to think about the fact that
you are actually like making a larger
series if you're up sampling right if
you're going to a higher frequency you
will have more data points you can also
do the opposite though right you can go
to a lower frequency okay so that is
that is the dot as freq method we can
also look at other options we've got
here right we can normalize we can also
have a fill value good to be aware of
okay let's say I want to go less
frequent rather than more frequent right
so I had hourly but maybe now I want
every three hours so let me just print
out that TS again so we remember what
that looks like so here's our original
time series I now want to sample it only
every three hours and here I've got 3 6
9 12 15 etc 3:00 a.m. 6:00 a.m. 9:00
a.m. a couple things to notice one is
that it started by including the time on
the later end rather than the earlier
end that's the left versus the right
that's parameterised you can easily
change it so that it would be 12:00 a.m.
3:00 a.m. picking the leftmost option
another thing here is I'm not missing
data right because I'm strictly taking a
subset of my original data points but
it's not super interesting in the sense
of this does just lose information right
instead of knowing that I had 0 1 2 3
I've just got the 3 6 9 and maybe that's
what you want but maybe sometimes you'd
actually like to do something different
which might be what you might want to
average right you might say well since
I'm dropping down to every three hours I
actually want some summary statistic for
that 3 hours
maybe the average maybe I want to know
the min for every three hours the max
for every three hours the sum you name
it you can do it so what you're gonna do
there is you're going to do something
called resampling it's the exact same
idea except that now you can do a
summary statistic rather than just
grabbing some arbitrary values so as
freek will always just pick one of those
points forward fill or backfill if you
want to do something fancier you're
gonna go with a resample so for example
here I'm going to resample every two
hours for the label right that's picking
whether I start at midnight or 3:00 a.m.
for example in the case right do I pick
the left value meaning the earlier and
time value or the right value meaning
the later in time value for my label on
this statistic since I'm going to be
dropping points so if I do this I take
every two hours and I take a mean we can
see this here it's we've got point five
two point five now I'm taking a mean I
have a way of sort of describing the
whole interval as I am down sampling to
a lower frequency if I want to get an
idea of some of my options here you can
see that resample gives me a lot more
options than as freek right I can have a
limit I can have a label I still can
have a fill method if I want I can pick
an axis it's just a much more flexible
function and what's particularly useful
is that we can resample even to even out
irregular time series right so if for
some reason you have some time series
that wasn't sampled on the hour right
maybe you had one a.m. and then you had
3:15 and then you had 727 but you would
actually like to present it in a more
regular format you can also do that so
if we look at my irregular time series
here you can see I did 11 p.m. on the
first and then 2:00 p.m. on the second
if I want to just resample that I can do
an ass freak and that will get me down
to what I want okay so why didn't that
work what I did I took my regular time
series I sampled it as frequency daily
and it just gave me back January 1st
2011 it's not really what I wanted
and I I certainly have other days here
right I thought I was all done what went
wrong
pandas is terrible this is a wash I need
a new library no right it's out of order
so that can also really mess you up one
thing you want to be aware of with
pandas is does expect your data to be in
a sensible order when you're dealing
with time so if you just go ahead and do
a sort index like this and then you try
that daily method again you will now
find that you have sampled and it has
behaved as expected I can do a back fill
I can also say do a resample and take a
mean so it's a nice easy way to clean up
your irregular time series just because
they're irregular though that's not
enough they do have to at least be in
order okay so a couple of exercises to
try and digest this stuff let's go ahead
and take five minutes let's also go just
call this a 5-minute break if anyone's
been waiting to run out and get a drink
of water or use the bathroom please take
advantage and we will reconvene at 2:30
one
let's talk a bit about resampling so
what if you want to go to a higher
frequency up sample right like our first
example where you're actually adding new
data points times that you didn't
originally measure and you don't want to
backfill or forward fill what's the way
to do that this method equals none right
so if we were to do an as freak for
example and we're say up sampling from
hourly to every 45 minutes if we just
passed in method equals none those
values where we didn't actually take a
measurement will come back as Nan's so
that we know we didn't actually take a
measurement real-world question why
might you actually want to do that
what's a use case where you might want
to do that
nobody has a use case from work or
something so I can give you one so we we
look at when people with diabetes say
take carbs in we might have this idea
that people eat every three hours so we
say well let's sample every three hours
from our every six hour data that we
originally collected and so those people
who happen to have every three hours
because they put an extra data points
will get that data and those that don't
will have the Nats right so if you say
have people with different levels of
data density where sometimes they'll
have it and sometimes they don't that's
one use case another use case is maybe
where you're doing a join with another
data source so for example I have
climate data I'm collecting particle
counts in Portland and I'm just treating
Portland as Portland I don't actually
care about neighborhood and one sensor
went off every 30 minutes and another
sensor went off every 17 minutes and so
if I start sampling every 17 minutes or
something like that if the one sensor
was firing off I'm good right so as long
as that sensor was firing I'm good if it
wasn't I'll put in the Nance so when you
have say parallel sources of data that's
another time when this might be useful
to put in a nun
okay what's the difference as far as you
can see between resample and AHS freak
so we started with as freek which just
up samples and down samples and it just
forward fills or back fills and it's
pretty plain vanilla and then we move to
resample when we wanted to start doing
summary statistics of some kind right so
every sample a Daughtry sample is almost
like a dot group I on sequel it gives
you a resample object which you can then
sort of pass down your pipeline to do
things like take the mean take the
median take the max take the min so if
you just want to convert your series to
a different frequency just go at as
freak that's more direct but on the
other hand if you want to somehow
especially when you're down sampling
right if you're going from say hourly
data it's a data that comes every five
hours and you want to have a way of
using all of the data right rather than
just grabbing the point in the time
series that corresponds to that time
period you will use resample right and
then you can do something that uses all
the data you use during that time period
okay what if I want to forward fill
right so I'm going to up sample I'm
gonna go from a lower frequency to a
higher frequency but now I want to
forward fill only a few days or only a
few minutes or whatever like only a few
periods so what if I have this idea
let's say with the particle data let's
say I am sampling let's say I am
sampling every seven days now let's
let's do six days that's easier every
six days and now I want to up sample and
pretend that I was sampling every three
days for some down-the-line pipeline
issue so now I'm going to up sample to
every no let's say every day from six
days to every day and I have the domain
knowledge that the particle count that I
take on one day is generally accurate
for say two days so it's not generally
accurate for six days but it is accurate
for two days that means that when I go
from my every six day measurements to my
daily measurements it is sort of
scientifically valid to fill in part of
my time series right so if I measured
every six days I can actually take that
day like day one and carry it forward
two days but I can't carry it forward
for the full five
days I need to fill in that will be
blank how would I do that well let's say
I don't know for some bizarre scientific
reason that it can go backwards in time
it can only go forwards for my fairy
fairy tale example right but I would
certainly want to think about that in
the real world right and then at some
point when you're back filling in for
word filling then you might as well just
interpolate but if I have it only going
in one direction what might I do so what
one thing is just a fill na right so
just standard pandas methods that aren't
even time related right so I could for
example do an as frequency bring it up
to my faster frequency start with a
method of none so originally everything
is an an then once I have all my Nan's
pandas gives me all sorts of ways to
just deal with them even apart from time
functionality so I can do a forward fill
on my Nan's and put in my limit there
and that's an example where if you have
that domain knowledge you can do a
partial forward fill and then again with
the resample just a hammer in number to
a resample or a resampler object is just
like a group by it's just a way to do
these sort of aggregate statistics over
larger periods of time as you are down
sampling okay I'm not asking for
questions because I'm going to just
assume people are shouting them out when
you have them that fairy or do we we
want a separate question no okay so keep
shouting out the questions as they occur
to you moving right along to moving
window functions
okay so a couple things you want to just
be aware of if you really haven't dealt
with time functions before when you're
looking at time series there's a couple
of things we do with time series very
standard exploration one is a lag
function right where you just want to
sort of move your function back or
forward in time and when you do this
it's usually because you're thinking
about the difference right so when I
talk about a lag function all I'm doing
is taking my time series and I am
literally just kicking it back one
period I am taking the whole time series
and just shifting it back one unit in
time and I'm gonna do that just because
I'm gonna want to look at the difference
between them so when people talk about a
lag time series that's what they're
talking about
you can also lag for any number of
periods you want so maybe you won't lag
one period maybe you lag ten periods or
twelve periods that partly depends on
your domain knowledge and what you're
trying to figure out another common way
of exploring time series data is window
functions right and what you're doing
here is you are basically trying to
smooth out your information that you're
getting from that time series and there
are two common sorts of windows to use
one is called a rolling window this is
the one you are probably more familiar
with that you have seen more you can
almost sort of think of this as
smoothing right it's not quite the same
as smoothing but it's similar to
smoothing so let's say looking at this
left hand side I've got this time series
this jagged data you can see that sort
of jumps around right all natural data
will jump around you have measurement
error you just have normal statistical
variations but I really want a smooth
line right because I'm a human being I
like to think that there's some true
mean or some true behavior that is in
that data so one way you can do that is
you can sort of have a window and the
window moves over a few points at a time
compute some way of combining that data
usually it's some kind of weighted mean
and assigns a point right for time what
do you notice though about the rolling
window function it's not just smoothing
right if you think about it look at
where the dips in my real data are the
dips here versus in my rolling window
smoothing functionality it's here so
what is the other thing about a rolling
window versus just smoothing or yes yeah
there's gonna be a lag right you're a
rolling window will slowly respond to
the changes but it's always going to be
sort of behind your data not at the same
point as your data that would really be
more of an estimation of your time
series expanding window can anyone sort
of guess from this graphic what an
expanding window is let's think about it
so it's mm-hmm so it's some kind of
cumulative summation right rather than a
moving summation in this case it's just
keeps using the information as you get
it so with a rolling window if you use a
rolling window what you're basically
saying is the valuable information is
the information information that's near
that point in time and really just
behind that point in time right because
you don't want to usually use the future
so you're just sort of moving forward
computing right and this is like for
example the stock market if you were to
think about it you don't know what the
stock market will trade in the future so
the best you can do is sort of maybe
take a five-day average and move that
five-day average forward or a five
microsecond average depending on what
kind of stocks you're looking at on the
other hand you might have a process
where you don't actually think it's
moving around or for some reason you
have domain knowledge to believe that
all information is relevant to now so
there could be some kind of data I have
measured for a very long time yours or
eons or nanoseconds depending on your
field where you believe that the first
measurement even though it's more
distant in time is somehow still
interesting to tell me what the
measurement is now so that's when you
would use an expanding window does
anyone use expanding windows in their
work or can you think of an application
mm-hmm so if you have something for
example that's a time-series but might
actually be measuring a static property
right which often happens that is one
case where you have an expanding window
one case that comes to mind that's kind
of controversial is political polling so
some experiments indicate that people
don't actually change who they vote for
they might start becoming more honest or
open with pollsters but they tend to
have the same opinion maybe even a year
before an election so you could argue
that rather than a rolling window which
is what most pollsters do use that you
actually want an expanding window where
you can view sort of the responses to
the poll over time as really just giving
you that mean that was maybe true a year
ago right as most people don't change
their voting identity so anything where
you think that the information from a
long time ago was just as valuable okay
so why we use a rolling window function
well we talked about it a bit usually
people are looking to smooth out data we
talked about already how what's a little
funky here is your smoothing smoothing
with your rolling window is always going
to be a little bit behind what's the
difference between these two they're
both rolling windows the window size
right so that's the other interesting
thing to think about when you look at
rolling windows is you can make your
rolling window really narrow like this
one in the bottom what's the advantage
of that yeah so it's closer to your
actual time series and there's a tension
here right because you'd say if you want
your actual time series just use your
time series right why smooth it at all
why do you still smooth it even if you
mostly want your time series mm-hmm yeah
so we want to get rid of noise right as
long especially if we believe our noise
is some sort of iid Gaussian we just
want to sort of delete that but
otherwise have our time series something
with a fairly narrow window just
sufficient to get rid of that noise can
be nice on the other hand why might I
want to use something more like the
smooth curve above yeah so like if
you're if you are more interested in
long term trends right so if we imagine
again that this is safe
financial data maybe I'm the kind of
investor I'm getting ready for my
retirement in a few decades I don't want
to respond to these shocks right I
actually maybe even psychologically I
don't want to look at them right when I
look at my retirement portfolio I just
want to see this blue line I don't want
the stress of looking at that green line
right and I have certainly had even jobs
where I have noticed that people in
upper level management also didn't want
the stress right and they preferred when
the analytics people sent them up like a
really long rolling window rather than a
shorter one and everyone was happy so
when you're thinking about your window
size it's both domain knowledge and also
sort of what you want both
psychologically or sort of like what
your purpose is with that rolling window
so with that rolling window stuff let's
go ahead and look at our notebook and
see how we can do it
okay so we are looking at our moving
window functions first thing we want to
do is learn how we can compute a lagged
time series so if we have a time series
we can just do the lag by making it TS
shift and let's see what happens if we
do that so my first time series I set up
as a daily frequency from July 1st 2016
my lads time series I just did TS that
shift let's see what that gives us well
it starts on the same day but now it
gives me an and for my first day right
why is that because I shifted everything
back which means for my first day I
don't actually know what that value was
right there there isn't a value there's
nothing before that to shift up so that
one's an N what else does that mean what
do we think the tail is gonna look like
the tail is July 20th what about my
original time series also July 20th so
what do you notice here I just kicked
off at a time point I just lost it
something to be aware of right so it
really does just shift it but preserves
your initial indices which can be nice
if you don't want to start adding
indices right you really just want to
compute the lag I do this usually
because I want to be able to do a
difference right one way I can do a
difference is I can just subtract it out
like this you can also go instead of
computing it yourself
there's TS diff which will actually just
give you the exact same time series so I
don't have to explicitly do the lag
myself although it's good to understand
where it came from
what if I don't want to do one period
all right this assumes I just want to
shift by one period but there are legit
reasons to do more than one period let's
say I have yearly data and I want to
look at year-over-year trends I'm not
going to shift by one right or if I have
monthly data and I want year-over-year
trends if I shift by one that'll just
give me month over month trends how
would I do year over
shift by 12 right and actually both of
these you can just pass in a different
number and it will shift according to
what you pass in right so if I could put
a 2 here and I'm going to shift two
periods I can also put a minus 2 in and
shift the other way in time so it
handles both positive and negative very
handy to know
okay so pandas does it to you so quick
question how do I shift my time series
into the future instead of into the past
just go ahead and type it out for
yourself
so I just did it here what's different
in my code just this one line I shifted
minus one right so instead of leaving it
blank which is just one it defaults to I
do minus one and you can actually see
here look my red which is my lagged
except now it's sort of future lagged
time series has just been moved to the
left by one so very very flexible so
that's my lags window functions let's
look at our window functions we were
just talking about how helpful they are
how you want to think carefully about
how you do them how am I gonna do a
window function well let's start with a
data frame and take a look at the data
frame I made what have we got here we've
just got some randomly generated numbers
and I'm correlating them to dates notice
I'm not using periods it's not important
but you just want to be aware of what
you're working with so here we're
actually looking at time stamp says
indices I want to do now a rolling I do
a rolling so I do D F dot rolling a nice
class method there I just get a rolling
well what's that why do I just get a
rolling haven't done anything yet with
it right I've just told this data frame
prepare like please aggregate with a
window of 20 periods at a time but I
haven't actually told it how to
aggregate right it's like doing a group
by and then not giving it an operation
to do on the group so you get these
rolling these rolling objects let's
actually do something with it so I can
take my R and maybe look at the mean and
here we go right there's my nice moving
average looking at the mean and you'll
notice I've got some points here on the
left where I don't have anything why is
that yeah I don't have my 20
observations right so it's not going if
I say a window of 20 it doesn't mean at
most 20 it means exactly 20 so until I
get to 20 I don't have any that's
something to keep in mind for example if
you have a really short time series you
might want a really large window but you
might not be able to afford it right
because then you might end up only
having a trend line for some very small
subset of your data okay what if I want
to look at the min instead I can do that
too right I can do any sort of function
I can put it in I can put in a custom
lambda function if
want one interesting thing here look my
rolling mean is just sort of this kind
of jagged right because it's still only
an average of 20 points they're still
fluctuating enough that I see jumps but
it's fairly smooth right and I can
certainly smooth it more by just making
my window a lot bigger right so for
example the bigger my window gets the
smoother my average is gonna get because
each data point is less important right
on the other hand there are certain
functions I can do like this and this is
not really giving me a smooth function
at all right so here I'm doing a rolling
window and selecting the min in that
window so why does this look so awful or
does it it's accurate right so it just
we're used to thinking of window
functions as something where you just do
a rolling me into smooth but you also
might just want other information that
will jump around right so let's say I
want to know in diabetes terms like what
was the lowest blood sugar this person
has had recently right with this 20 or
50 period look back and that's actually
just accurately describing right so if I
pick a function that's not going to have
a remotely smooth transition there's
nothing wrong with that it's different
kind of information okay so let's try
out a couple of others just play around
on your own with our agar apply our
count our dot max see if you find
anything interesting just take a minute
so for my rolling object I applied the
quantile function at the 30th percentile
and this is sort of an interesting way
to get a happy medium between something
that jumps around a lot and something
that's really smooth and seems like no
individual data point contributes anyone
see anything they want to share anything
interesting okay not so much so just
good to be aware of the rolling function
just like a group by whatever statistic
you want to do to summarize okay what is
that a custom function what if I have
some really weird way that I want to
aggregate my data or get information
about my data see if you can formulate a
way to do your own custom summary you're
going to get in a list of 20 values or
50 values or whatever you set your
window size as what are you gonna do
with it
I see
so I've got a fairly dysfunctional
example here of what you might want to
do da F dot rolling let's say I've got a
window of size 10 I'm gonna just do an
apply and stick my lambda right in and
I've got a bunch of non Stu start with y
those are just my again at the beginning
right if my window is looking backwards
I need to get to 10 before I have
anything so let's just go ahead and look
at the next set and I've just got
numbers that describe the second value
in that window divided by the third
value in that window I don't know why
you'd want to do that but if you wanted
to one dysfunctional way to do it is
you're rolling function I would hope you
wouldn't do this though right because
then you don't really need a rolling
window to do this right let's see
another example here I'm just plotting
it right and again you can see it's it's
fairly dysfunctional but it just shows I
can do anything I want I'm just gonna
get this ordered list in so you get that
ordering information and you can use it
however you want what would be maybe a
less dysfunctional thing I'd want to do
that might be custom I could do custom
weights for example right I might have
some weird nonlinear or non functional
form type of waiting I want to do so I
don't want to do an exponentially
weighted moving average I have my own
custom weighting which might be informed
by domain knowledge so I want to put
those weights in although there other
ways to do it I might want to only take
every other value within my window but
just do that with my lambda rather than
looking for some way with the pandas API
I might want to if the values are high
weigh them one way and if they're low
weigh them another way right again may
be informed by domain knowledge the
point is in your domain no matter how
kooky it is if pandas doesn't handle it
your lambda function certainly can okay
expanding Windows less common but you do
want to know how to do them instead of a
rolling you just have a dot expanding
and you'll notice it does not you set a
min number of periods so for example if
you don't want to start with
one if you want to have still some
minimum window size you can easily set
that in this case I did just a tab min
period of one so I have values for each
yes
so the question if I understand it is is
there a way to have a rolling window but
not have nan so maybe have an expanding
window and then a rolling window so in
that case I think I would probably do
like a fill na so I'd probably do my
rolling window first and then backtrack
and do a fill and a with an expanding
window would be one way to do it you
could also for example say well once I
get to 20 I want to use a rolling window
of 20 and then back so with smaller and
smaller rolling windows that's another
way to do it it would depend on your
domain needs cool okay so let's try out
a couple of things here first question
how can you perform an exponentially
weighted moving average these are really
common in a lot of applications so it's
good to go through the docs and find
that secondly think a bit about how you
would how and when you'd use an
expanding versus a rolling window third
try to write a custom function to
replace the quantile function for a
moving average and use it the way you
would use a custom lambda just to make
sure you remember how to do that and
fourth think about how would you compute
more than one aggravate aggregation
function on a moving window so let's say
you wanted to take both the mean and the
median and maybe the sum how would you
do that all in one line of code to keep
your code nice and simple and clean so
go ahead and work on those I think this
is also when they roll out the goodies
at 3 o'clock
so if people's stomachs are growling and
stuff maybe we'll take another break
that will coincide with working on these
exercises so let's reconvene at 309 to
talk about the exercises but also go
grab some snacks
as a reminder we had four questions we
wanted to take a look at during the
break the first one is how do you
perform an exponentially weighted moving
average right you I mean you could code
it up as a lambda function you probably
don't want to because that seems like a
lot of work this is something that is
common in a lot of disciplines and also
for those who haven't seen an
exponentially weighted moving average it
looks something like this so what do you
notice about this shape it's not linear
so it's what it like if you use this
what are you saying about your data when
you're waiting you're not waiting all
points in your window evenly what are
you doing
more recent points are more important
right so you can almost think about this
as a little bit like the divide between
the expanding window and the moving
window right the expanding window says
all points are interesting and I want to
keep including data the moving window
says I only care about recency you are
applying this with a moving window but
you are saying even within my window I
care about recency I want to include
some things but weigh them less they're
less informative right it happened too
long ago so I mostly want to ignore it
right at this point out here maybe 20
days ago I mostly want to ignore it
versus the data point I just collected
that's going to be the most valuable so
you can do that with an exponentially
weighted moving average it's the most
common way across many disciplines in
pandas this is one way that you can do
it is with the dot ewm and you can take
a look here it will give you a very nice
you know lag smooth version of your time
series just like with your moving window
it's got a bunch of parameters to adjust
so you can sort of think about the
half-life that you want
think about how steeply you want your
weights to drop off and you can sort of
play around with that again depends on
your domain knowledge depends on what
you're looking to get out of it okay
second question we talked about this a
bit but when would you use an expanding
window versus a rolling window the
default tends to be rolling window so
it's more like why would you use an
expanding window is another way to frame
that question
right so when even data are really far
in the past is informative such as when
even though it's a time series it's
actually measuring say some static
information right so it could be that
your information is getting better
rather than that the situation is
changing and when you have time series
you often want to think about that is it
the underlying process that is changing
or is it only my measurement that is
changing if it's my measurement that's
changing an expanding window is probably
more interesting versus if I think my
underlying measurement and my actual
situation are both responding okay what
about writing a custom function to
replace dot quantile 0.5 for a moving
average how would I do that
so one way to do it is I I just
implemented one example here if I want
the 50th percentile I can just sort and
take the middle one the point is really
that you just want to remember you can
use a lambda right so you can come up
with any summary statistic you want and
then finally what if you wanted to
compute more than one aggregation
function at a time with your rolling
window well you can do that D F dot
rolling it's just like a DF group by for
people who are familiar with pandas
again I can do a dot AG and have lots of
methods that I want to compute right I
can do a sum n demean for all of my
columns at the same time I can also
stick in custom lambda functions as part
of my a Groening windows ok so now we're
going to start talking about things that
are related to trying to make
predictions about time series and we're
going to start by thinking about and
talking about self correlation so time
series data for in my opinion is always
really interesting because even a time
series that has only one value right I'm
only measuring one thing but I'm
measuring it over time is actually
really interesting because there can be
a lot of internal structure for example
if you have something that has some sort
of yearly pattern you're gonna see every
12 measurements if you're measuring
monthly every month you see some kind of
repetition so it's not just some boring
like I only get one number so I only
have one kind of relationship I actually
can have infinite relationships right
because I can look at how every point
relates to all the other points in time
I can look for periodicity and that is
what self correlation is about so
there's two functions we use to look at
the structure of time series one is the
autocorrelation function this is used to
help identify possible structure in time
series data it gives a sense of how
different points in time relate to each
other in a way explained by temporal
distance so when you're thinking about
autocorrelation you are necessarily
thinking about data that is segmented
according to evenly spaced points in
time if you're looking
auto-correlation for an irregular time
series that doesn't necessarily make
sense right so if I have my first data
point and then I have two days and then
I have 17 days right it doesn't really
make sense to be looking at those
measurements and looking for structure I
am assuming that these are regularly
spaced data points and if I do that I
might want to think about let's say I
have daily data well I would assume that
Monday's tend to be similar so I might
look at how every Monday relates right
so if if one Monday value is 10 maybe
the next Monday's value is more likely
to be 10 than to be some other value
right maybe they're highly correlated or
maybe they're negatively correlated
right so maybe there's some inverse
weekly cycle right where one week if
it's high the next week it's going to be
low then it'll be high I can have some
sort of negative correlation so what you
can see here is on the top you see a
time series that if you look at this to
be perfectly honest I would look at that
and say I don't know if I see any
structure like maybe this is just some
random you know people measured noise on
the subway and it had nothing to do with
anything but then if I look at the
autocorrelation function I can actually
see that there's some very high
correlations at certain periods so the
first point will always be one because
that's always going to just be like a
delta of zero how does a point correlate
with itself if that is not one there is
definitely a problem in your code or
there is an N somewhere so that should
always be one right how does a point
correlate with itself when I go to the
next point what I am saying is I am
tallying up the correlation for every
point in time I am looking at the point
in time lagged one behind it and I am
calculating that correlation so say you
have a time series that is ten long when
I'm looking at the autocorrelation
function of a lag of one how many
correlations am I looking at while I'm
looking at from 1 to 2 from 2 to 3 from
3 to 4 from 4 to 5 from 5 to 6 from 6 to
7 from 7 to 8 from 8 to 9 from 9 to 10
right so even though I've just got this
one string of numbers each of those
pairs is giving me the lag 1
autocorrelation value what if I'm
looking at the lag 2 autocorrelation
value from day one I'm saying
how does day one relate to day three
what's the next day I tried do I go to
day three and look at how it correlates
to day five no where do I go
day two right so it's day one to day
three day 2 to day four day three to day
five day for today six right so every
point counts what if I'm doing I only
have ten and I'm doing the night the log
of nine I only get one observation right
so intrinsically I am limited to the
amount of time I have observed that's
the farthest out I can go and let's say
I only have ten and I want to look at
lag nine do I really want to know right
for a couple of reasons all of which are
would be the you know saying the same
thing in many different ways if I've
only got one observation I mean what's
my uncertainty I have no idea right I
have no way of knowing what any sort of
metric of what the variation in that
autocorrelation is I can't have any sort
of confidence interval I have no idea so
you want to pick some sensible limit to
how far out you're looking because the
larger your lag you're looking out for
your autocorrelation the fewer data
points you have
okay so this is just a great example of
how you might have real order when you
think you have chaos that's the
autocorrelation function can you think
of a problem or at least a wrinkle with
the autocorrelation function
let's let's say that I have weekly data
I just think about humans and they work
in weeks and people tend to have high
stress levels on Mondays or yeah high
stress levels on Mondays so I might see
and every other day it's sort of random
and it washes out but I might see a
really high autocorrelation at seven
days right because all those Mondays
anyway right the Tuesday Wednesday etc
they might give low but still on the
Mondays I'm getting a really strong
signal so that'll give me a high
autocorrelation for seven days but what
else will I see 14 days right I'll have
a really high autocorrelation for 14
days and what else 21 days and 28 days
and you know seven times a million if I
have enough data I will also see a
really high correlation and this is some
problem you will have with periodic data
right you will see that it will just
spike but that's not actually meaningful
right what's actually happening is what
why you why do I have correlations 14
days well it's really because I have the
correlation seven days right it's just
like harmonics it's just multiples so
what am I going to do about that I
really only want the true correlation
right so I want the shorter time period
I'm going to use something called the
partial autocorrelation function so if
the partial autocorrelation function
does is it gives the partial correlation
of a time series with its own lagged
values controlling for the values of
time series at all
shorter lags so that means that if a
correlation can be explained by a
shorter time period it's not allowed to
take that same correlation and sort of
recycle it and give credit to that
longer time period we're going to say
that the shorter time period with the
correlation is the real one the more
natural one we're going to take that out
and then we're going to see is there any
more correlation right is there a 7-day
effect and also a fourteen-day effect
and that will be say the true 14 day
effect because we've taken out the part
that can otherwise be explained does
that make sense this is this is a bit
trickier yes thank you I like that
okay so that's autocorrelation and
partial autocorrelation so let's go
ahead and see how we can do this with
Python we're gonna now look at stats
models and in particular look at the TSA
module the time series analysis module
there's some random Yahoo stock data and
a CSV so let's look at the
autocorrelation first we're gonna start
with some white noise so just generate
some random white noise this looks
pretty random to my mind this doesn't
look any different from the one that we
looked at in the slides here right so I
always say try to keep an open mind just
cuz something looks like white noise it
isn't necessarily you really want to
prove out to yourself whether it has
structure or not so I'm going to use the
stat tools ACF and I am going to throw
my result curve which is just noise here
into that and I want to take a look at
what my ACF looks like remember zero is
just correlation with itself so even
with white noise your zero lag is of
course going to be one right you
correlate with yourself perfectly but
after that these dotted lines are just
sort of a significance cutoff kind of
arbitrary again you need to make your
own decisions but for our purposes today
it's just a standard significance cutoff
what do we see there's really no
autocorrelation right out here it does
pass significance this is the 95 percent
confidence interval so one out of 20
times I do expect a random value to pop
up over my confidence interval that's
all that has happened out here that will
happen you want to be aware of it the
shorter your time series the more likely
you are to have that sort of randomly
statistical blip but white noise doesn't
give me a nor autocorrelation right and
it shouldn't what about a periodic
function though right so now let's look
at a sine got a nice sine curve and
let's look at the autocorrelation there
and what do we see
it's like off-the-charts right and this
is what we were already talking about
that if you have something that can be
explained by a shorter time period but
you're not using your partial
autocorrelation you are gonna see that
sort of echoing out right sort of
infinitely like you're gonna still keep
seeing
significant correlation okay so let's
look at a real example we've got our
Yahoo stock info I think this is Apple
maybe I don't remember if we look at an
auto correlation now for real data what
do we expect to see well it depends on
the domain right if I'm looking at human
behavior versus if I'm looking at stock
prices about which I know nothing versus
what if I'm looking at astronomical data
you want to use your domain knowledge
otherwise you should look at things and
sort of interpret as you go but maybe
think well maybe I don't know what I'm
talking about so I have an ACF of stock
data and now I have this really weird
thing I've got a bunch of significant
values and then they sort of cross and
they go negative significant right so
positive correlations at short values
negative correlations out longer values
am i like learning anything
intrinsically about the stock market
here not really what's going on here
well let's go back and look at what this
particular stock did it went from a
lower value to a really high value here
right in this moment in time I think
this is over a couple of years of time
so whatever this stock was it enjoyed
this tremendous period of growth what
does that mean that means that values
that are closer together well they will
be correlated right because the stock
market closes and then it opens again at
the next day so days do tend to be
related to each other but it means that
when days are getting further apart here
they will be negatively correlated
because there was a trend right so it's
just telling me something not about the
structure that's recurring in smaller
segments but it's telling me that
overall there was a trend that's not
that useful as far as thinking about
correlations right for correlations I'm
sort of looking for things that repeat
this is an example of why I don't
usually look at our auto correlations
and this kind of untreated data and in
particular what I mean is there's a
trend here we're gonna soon talk about
why you want to deat Rend your data
before you try to analyze it because
these sorts of tools are not meant to be
looking for trends right I can spot a
trend just by looking at the line going
up when I'm looking at autocorrelation
I'm really looking for internal
structure
so okay so let's see let's see an
example where maybe it's a little less
random right the stock market defies
everybody you know if someone figures it
out they'll be a billionaire I guess
there are a few of those but let's think
about something where we have a better
sense of why the data is behaving a
certain way so this is a very very
popular data set the air passengers data
set it's you know count of air
passengers by month for decades and
decades and we can see here some data
where we can even visually identify the
structure and what we see is there seems
to be some kind of yearly trend right we
almost have the exact same curve from
year to year right they're sort of a
seasonal spike a seasonal low this is
something where I say I even see the
structure so for here I would certainly
expect to see some sort of internal
correlation now there definitely is
still the problem of the trend right
it's no different from that stock where
it was going up but there's something
else and I want to see that something
else so let's take a look at the
autocorrelation function here I see my
trend right which is the fact that it's
like a line but I also see this
recurrent structure which is the part I
care about so that looks meaningful but
worrying what I probably want to do now
is I want to de trend my data and while
I'm gonna do that by taking the log and
also taking the difference why am i
taking a log in a difference I'm trying
to get towards stationarity we're going
to talk about that in a second but the
idea is I want to make my time series
look more like something that is
relatively constant in time the variance
is constant in time the mean is constant
in time so then I can look at why
sometimes it's above the mean and why
sometimes it's below the mean without
having this distraction of an overall
drift so if I difference my data and
take the log I've got something where we
can see the variance looks fairly stable
the mean looks fairly stable and if I
now look at the ACF what do I see I see
something that doesn't have some crazy
trend so this is more what I'm looking
for right is some sort of internal
constant structure that's not just
superimposed by a massive spike and I
see periods what do I yes
the x-axis is the lag so zero is a lag
of zero to zero right so that's number
with itself one is the correlation
between a number and the number that
comes in the next period two is a
difference of two and we're counting up
all the points so here what you see
where do I see a high correlation at 12
and at 24 and at 36 not surprisingly
right because what I'm seeing is your
two-year periodicity now there are
problems with seasonal data as far as
recurring we'll talk about that in a bit
but what I do want to point out here is
we can spot the internal structure right
so if we didn't know how humans worked
if we didn't know about months and years
we would nonetheless be able to take
this data and see wow it has a really
high autocorrelation at 12 so there must
be something important about the number
12 so that's self correlation yes
so it's averaging across them and
actually like you can also usually put
in uncertainty bands right where you
will think about how many points you
have and of course the bigger the lag
the fewer points you have the wider your
uncertainty events yep so that's self
correlation okay so that's the first
thing we want to talk think about
because self correlation is part of how
we do Diagnostics in terms of thinking
about what kind of model we want to use
the next thing we want to do is munge
our data so that it looks like what we
need it to look like to do in a rima
model so we need to do some pre
prediction munging looking for
stationarity so to use an ARIMA model
which is one of the most powerful ways
of predicting time series one of the
most inclusive ways of predicting time
series one of the most general ways of
predicting time series you need a
stationary time series most real-world
time series are not naturally stationary
right the ones that are are probably
things like we talked about where you're
actually measuring something that itself
is fairly constant and you're really
just seeing your measurements jump
around in the real world most things are
changing right there
variance is changing they're mean is
changing right stock date at airline
passengers carbon air count whatever it
is so you usually want to transform it
but to transform you need to know what
you're aiming for a stationary time
series has a constant mean a constant
variance and a constant autocorrelation
right which is basically you're saying
the value is not drifting over time the
variance / unpredictability is not
drifting over time and autocorrelation
the structure is not changing over time
right you're saying you have a time
series that behaves a certain way and
it's always going to behave that way not
realistic in the real world but locally
it can be true so stationarity if you
look at this though can be tricky to
decide of all of these plots the only
ones that are actually stationary in the
sense of passing a test for stationarity
are B and G now I don't know about you
but I wouldn't put a lot of money on
these are the stationary ones and and
somehow being able to say well it's
obvious that this one isn't or it's
obvious that this one isn't so that's
just something to be aware of when you
look at data just like when we looked at
that function that has a really nice
autocorrelation function but it looks
random you can have the opposite problem
that something looks random and looks
like it's stationary but actually isn't
right and this is where time series can
get tricky and where you have to adjust
your data to fit your model as best you
can so what do you do if you don't have
a stationary time series because
undoubtedly you will almost never really
have a stationary time series the first
thing you can do is difference so if you
look at this time series on the left
that's clearly not stationary right
you've got big jumps you seem to also
sort of have a trend right it seems to
be trending upward if you difference it
well that that looks pretty good
actually right it might still have some
problems with the variance but we have d
trended it just by taking a difference
that's one very common way of dealing
with non stationary data your first step
just difference you need to remove the
trend and seasonable
seasonal elements before forecasting
most interesting data in the real world
will show both trends and seasonality
most models require data that show
neither of these you need that
stationary time series so you can
difference that's one way to get rid of
it D trending in general right
you can also detrained by taking some
kind of moving average which we see on
the left right you take a moving average
with your window function and you
actually subtract that moving average
out that's fairly computationally taxing
right it also means you subtracted a
different value from each point that
makes me kind of uncomfortable in terms
of having to explain later how I got to
my model so you can do that it's not
optimal you can use a linear regression
as we see on the right that's another
way of doing it the linear regression is
nice because I have just the one
parameter I used right it's like oh well
I just used you know y equals MX plus B
I fit for my am I fit for my B and I
applied that uniformly so that's very
tempting as far as simplicity of the
model but I'd say overall it's worse
performance wise why what do you think
of this this D trend versus this D trend
it's it's still not stationary clearly
right I mean it has dips and troughs and
it's still needs adjusting right so if I
do an adjustment and I still need to do
more adjusting that again has this
complication factor it's a different
complication factor but arguably I'd be
at my linear regression and I have to
just go back to my window moving average
method after my linear regression and
that's even worse right so there are
pluses and minuses you have to think
about your application seasonality well
we saw some seasonal data with the
airline passengers example here are some
other examples there's two kinds of
seasonal data you're usually dealing
with if you look at these what do you
notice has the difference in the
seasonality
exactly right and so what they call this
is additive seasonality versus
multiplicative seasonality so on the
left here where we sort of have like you
can see the variance is about the same
really it just looks like your
seasonality is up shifting or down
shifting but otherwise your data is the
same right the sort of band of values is
the same on the other hand on the right
we have multiplicative seasonality where
we might have some underlying cycle but
the values changing as the values
increase that your variance is
increasing so you have this multiplier
effect in your seasonality rather than
an additive effect in the real world
most at least with human behavior most
effects tend to be multiplier rather
than additive but it depends right you
might have some underlying domain
knowledge again removing seasonality
tends to be fairly tricky it's not like
removing a trend so the simplest is you
just want to average D trended values
for a specific season so you could just
have like a spring time adjustment a
summer time adjustment or like a May
adjustment or whatever your season is
for your domain what's more common
because it tends to work a little bit
better is you use a louis method a
locally weighted scatterplot smoothing
you might have come across this in other
use cases it's not just for time series
you have a window that's specified with
you have a rate a weighted regression
line or curve fit to that data with
points closest to the center of the
curve having the greatest weight so it's
not usually an ordinary least-squares
regression is something that's weighted
you reduce on points farthest from the
regression line curve and calculation is
rerun several times so this is very
computationally taxing but it is the way
people tend to get rid of their
seasonality
so basically seasonality is very
computationally taxing but you have to
deal with it so you get rid of your
seasonality you get rid of your trends
with differencing what else do you still
have to worry about seasonality trends
there's a seasonality but it can also
just be increasing variants usually with
increasing variance your solution is
like a power transformation or a law of
transformation so let's think about the
things we've got so far we've got power
transformation log transformation
differencing linear regression fits what
do we like about all of these things
Python can do them I like that right
where a computer can do them pythons
even better the other thing I really
like is these are one-to-one
transformations so once I fit my model
or predict my model no matter how much I
have sort of screwed around with my time
series with these functions I can always
go back the other way so the thing I end
up predicting often doesn't look very
much like this you know the data that
somebody handed me and asked me to
predict right like my boss says can you
predict chair sales by week in the
Netherlands for the next 14 weeks and I
end up deciding that I'm going to do
some kind of log power difference
transformation in my actual model but
then when I turn around and talk to my
boss I'm gonna have to retransform it
right so transformations are fine as
long as there are these sort of
one-to-one monotonic transformations
okay stationarity you're gonna test once
you think you've got it stationary right
you want to make sure you've done your
best to get rid of the variance changes
get rid of the trend get rid of the
seasonality autocorrelation is even
harder to do so we won't even get go
into how you might deal with varying
autocorrelation my answer would mostly
be don't because that means you might
have fundamentally different processes
but for these other concerns once you've
taken care of them one standard way to
see do I have a reasonably stationary
time series is the Dickey fuller test
basically you just want to make sure
once you can put your process in these
terms that your Rho value does not have
unit value so this test is just one sort
of benchmark it's a statistical test
right so it's only giving you a
probability there's no real guarantee
that your process is stationary or not
stationary right you're just you can say
I'm five percent certain or 90% certain
and you want to understand what that
means but this is a good benchmark to
keep yourself honest right if you test
it and your stationarity is still
terrible and you're still like clearly
very close to having a stationary
process you probably want to keep
transforming or give up if it's truly
not stationary okay
so once you do that you move into
forecasting right you for forecasting
you knew that stationary process once
you've achieved that stationary process
you want to think about how your model
is working so we're going to think about
ARIMA models
ARIMA includes two kinds of processes
the first is a moving average process so
a moving average process is defined as
having this form your value is your mean
plus a bunch of error terms going back
in time
mu is the mean of the series the Thetas
are the parameters this is a stationary
process regardless of the values of the
Thetas right because your error is our
iid Gaussian you know mean 0 etc so this
is guaranteed to be a stationary process
if you've got your moving average right
part of the reason we can only model
stationary processes is that's the
underlying assumption of our model right
we're saying oh if you have a moving
average you'd better be modeling
something that's also stationary if you
look at an MA one process right that
would mean that you are only using one
prior term and that will have a theta
right so then that would be your process
your x2 your current time is a function
of your error at that time plus your
error at the previous time that theta
though still has quite an impact on what
your model looks like right so if you
look at a positive theta you'll get this
value on the Left versus if you look at
the negative theta you'll get this
series on the right how are they
different well the one on the left is
smoother right the one on the right
tends to jump around more right so the
one on the left is basically saying well
if this if I have this positive right
this is going to be positive too
this tends to make it bigger right if
this tends to be smaller they tend to
counteract each other versus if it's
negative that's almost like a bit like
having a negative correlation right it's
like saying if this one was high I'm
gonna go low and so you end up with a
more jagged jumpy process so sometimes
even when you're looking at data and
thinking about well what how would i
model it there are more formal tests but
you want to think well does it jump
around a lot or is it sort of more
meandering and that gives you an
intuition as to whether you have a
positive or a negative
theta and also how many terms so a
moving average process is part of your
process but we've modeled it as a
combined process so it's both a moving
average and also an autoregressive
process an autoregressive process means
that your value at a particular time
depends on your value at previous times
right so here we're assuming you sort of
oscillate around a mean and then you
have sort of an error term at each time
and that error term carries forward a
bit in an autoregressive process and
think about it auto regressive
regressing on yourself you are saying
that your past values have something to
do with your present value and even the
form right it looks like my form at time
T is you know it's almost like a fitted
value to a regression on prior values
plus an error term so if we were to
think about an auto regressive process
of value one then my value at time T is
merely a function of my value at time t
minus one plus an error term it's like a
y equals MX plus B and again you can
have a positive or negative theta and
you see the exact same thing that that
positive or negative theta tends to sort
of indicate how much your time series
jumps around right so if it's positive
it's smoother because they tend to
correlate with each other on the other
hand if it's negative they tend to anti
correlate with each other so those are
Auto regressive portions and then
finally we have what's this wonderful
ARIMA where does the I come from the eye
is differencing the eye for different
thing is just what we would do to de
tren the data so that differencing we
did just before that can actually be
done by the algorithm you don't even
have to difference sometimes you can
just tell it while difference once or
difference twice or difference three
times now to tell it how many times the
difference you have to know it right it
won't figure that out for you but that
is included now the eye in the ARIMA
part is really not that important
because you can also just do an ARMA
model where you've done the differencing
so the eye is sort of optional more
record-keeping but the auto regressive
and the moving average portion of your
model are what's important so your ARIMA
model is the most general class of
models for forecasting a time series
which can
made to be stationary I cannot say that
enough if it's not stationary this is a
no-go so your statistical properties
have to be constant over time
short term random time patterns always
look the same in a statistical sense
right so that's what the ARIMA model
takes advantage of you need your
autocorrelation function and power
spectrum to remain constant you can do
nonlinear transformations to get there
but they have to be monotonic one-to-one
and from that you can extrapolate the
signal from for the forecast ok so how
do you apply this right we get that we
get the magic wonderful magic stuff from
stats models we don't even have to
figure it out ourselves but we do have
to diagnose our model properly right to
do that you need to figure out what
ARIMA model to use how many auto
regressive terms how many moving average
terms to do that we're going to use a
plot of the data the ACF and the P ACF
remember the ACF is our autocorrelation
function the P a CF is our partial
autocorrelation function with the plot
of the data we're looking for a trend to
determine whether to transform the data
and once we've done that then we start
doing tests to figure out the order of
the moving average and the
autoregressive one thing to keep in mind
as we're diagnosing we're going to use
the partial autocorrelation function for
the auto regressive model diagnostic and
we're going to use the autocorrelation
function for the moving average model
diagnostic so each of these plots gives
us different information about what we
want to use for our ARIMA model the part
that I find easiest to understand is
using the partial autocorrelation for
the auto regressive so let's think about
that for a second this is our auto
regressive process let's say instead of
the partial I just used the full
autocorrelation function and I have an
AR 1 process well my lag of 1 will have
a high autocorrelation obviously right
because my time T value directly depends
on my t minus 1 value but what about my
auto correlation with lag 2 is that
going to be 0 as I would like because
then it's clear-cut
well think about it hmm it would be
somewhat less but it won't be zero
because this t minus 1 itself has T
minus 2 inside it right because 2 this t
minus 1 was also in my AR 1 process so
it's sort of like Turtles all the way
down this t minus 1 contains all the
other values right in this
autoregressive sense so I'm not going to
use my autocorrelation there my partial
autocorrelation is perfect for this
because all all that T minus one can
explain if T is all that t minus 2 can
explain of T so if I use my partial
autocorrelation the entire explanatory
value of this Auto Court or aggressive
process is contained there and so my
auto my partial autocorrelation will
drop off steeply does that make a little
bit of sense seems like we're OK okay so
that's our ARIMA model and i just want
to point out before we jump into the the
code that ARIMA models really are
everywhere especially in econometrics
but also in the sciences and the social
sciences it's just a very general way to
try to predict time series but it does
have one limitation which is a it's hard
to predict the future but be thinking
about the model let's say I come to the
conclusion that I have a purely auto
regressive process right no there's no
moving average component a one an order
one auto regressive process and my boss
is asking me to predict carpet sales 17
weeks out with my weekly data can I do
it I have this week's carpet sales data
can I predict next week's carpet sales
data reasonably well right if I think
this is an AR one process I'm going to
fill in this value assuming I fit my
model I can get next week what about the
week after my error will just keep
compounding and compounding and a few
weeks out I'm just going to have these
massive error bands right so your error
bands are going to be a function of what
you think the process is in this sense
it can be very tempting to have a high
higher order model right because you
think then at least I have more
information right like if I have a six
order model maybe a you know I can just
pack more information in do not give in
to that right most things are very low
order if you're going a high order
because you think it'll somehow narrow
your error bans you are totally wrong
you are just overfitting your data so
keep that in mind these things have
limited power based on the underlying
model and that's just how it is it's
tough to predict the future if we could
predict the future we'd be billionaires
and some are okay so first let's play
around with some auto regressive and
moving average models just to make sure
we know how to generate them right so if
I wanted to make an AR one what do I
need I need a Phi I need an initial
value and I just need to set like how
many I'd want to generate so what am I
going to do well I'm gonna say there's
an error value at every point I've got
my time series with my initial value and
then for each period what do I do
I append the error plus Phi times the
value one point in time earlier and
that's my AR one in Python even with my
random error thrown in if I want to do
an AR two slightly more complicated but
still fairly straightforward now I need
two fives right one for each value so I
have a Phi one I have a Phi two I have
however many I want to generate and I
need that initial value right because my
first initial value will tell me where I
go from there
and I bootstrap from there so my time
series is going to be my error for
whatever term period plus Phi 1 times
the value one moment back in time plus
Phi 2 times the value two moments back
in time and you'll notice here because I
have an AR 2 process I need two initial
values right pretty is pretty
straightforward so if I do this and I
take a plot that just gives you an idea
of what an AR process can look like by
the way is this is stationary
process know right so the thighs will
determine whether it's a stationary
process not all auto regressive
processes are stationary okay and what
about a two also really not stationary
not surprising right go ahead take a
minute try to code up the moving average
process right and I'ma I will put up
that code for the moving average for
this slide so you remember what of this
moving average is a constant average
with a sum of previous error terms
determining the present value so how
would you do that
okay so here's how I did a moving
average one again we need a SATA or a
fee I mean you can call your parameter
whatever you like you have your time
series you have your random error now in
a moving average model your value is
really just a function of the prior
error plus your current error right
you're just sort of accumulating errors
or doing a moving average literally of
those errors and that's your value and
then if we look at what our moving
average plot looks like we see something
like this right so this one is by
definition stationary and it's gonna
sort of oscillate around whatever that
mean value is so let's look at that
autocorrelation and partial
autocorrelation for each process so if
we look at an autoregressive order one
process here's our autocorrelation one
for itself and then still significant at
one arguably still significant at two
why is that I know it's an AR one
process but it's showing up like an AR
two or AR 3 process just not enough data
right like I still will have some
statistical uncertainty the fewer points
I have the less certain I can be about
what my underlying model is right
there's just gonna be some sloppiness
and here's my partial autocorrelation
function right this is the one that
really matters remember we talked about
with an auto regressive model it's the
partial autocorrelation model that's
gonna fill in the parameters this one is
rock-solid it's not always that way
right I can still have statistical
errors you'll notice here for my auto
regressive value of 1 I have a
significant term or my lag of 1 but not
after that means that it's only the lag
of 1 that really explains a correlation
or variation as I look at my partial
autocorrelation
so that's auto regressive right for my
auto regressive Diagnostics I look at my
partial auto correlation function ok
what about my moving average it's kind
of the inverse mathematically and I'm
not going to prove it to you today this
is very standard stuff you can look it
up for moving average you expect the
autocorrelation function to show you
where you should cut off
your model and you expect the partial
autocorrelation function to just sort of
taper off so let's look at that partial
autocorrelation function and we see here
it's not very informative we're not
expecting it to be we ignore the partial
autocorrelation function for the moving
average and we use the autocorrelation
function and here we see we've got one
term at here right there's our one and
everything else sort of falls in the
band and here again I've got some error
my series wasn't long enough right so
this is where your domain knowledge
still comes in these statistical tests
will give you some pointers but at the
end of the day if you have a sense or a
domain knowledge of how your process
works that's really the best okay so
we've done our model Diagnostics or
we've learned how to do them let's
actually do some forecasting so if we do
some forecasting let's go back to that
air passengers data right let's say
we're an airline and we really want to
figure out how airline how airline
passengers come into being right what
the underlying process is because once
we know that underlying process we can
sort of try to make a few predictions a
short period of time in the future right
we don't want to go too far in the
future because of course we just rack up
lots of uncertainty so we'll look at
those air passengers we're gonna do a
lot of transformation because we had
noticed that the variance gets bigger
and bigger right over time so trying to
get to a stationary process we're gonna
log transform it's one to one so it's
easy to backtrack we're also gonna
difference it and if we look at this
difference logged process it looks okay
have I removed the seasonality not sure
that I have right I mean I I think I
still sort of see patterns I'll worry
about seasonality later let's sort of
ignore that Bugaboo and just see if we
can start predicting so one question for
you though when I differenced what did I
lose what information the Empire State
Building is 200 feet taller than the
apartment building I live in do you know
how tall either building is no right so
you lose the information about the mean
that's
you can recover it but it's one thing to
think about right is for ARIMA you're
really just looking at a mean of zero
the absolute number doesn't matter and
shouldn't matter okay we're gonna just
drop our NA and we are going to run our
ad fuller test this is a sticky fuller
test we talked about four stationarity
we want to make sure that we have a
stationary value and what do you think
so our critical value for one percent
certainty is minus three point four four
five percent minus 2.8 I'm actually only
at zero point zero seven so I can't
reject the hypothesis that this is not
stationary that stinks
it shows that you can do a lot of
transformations and not be entirely sure
we're gonna leave it at that for now but
you probably wouldn't want to proceed
with this time series normally with this
model right and it just goes to show
that stationarity is not that easy to
achieve that said given how few data
points I have and given I've already run
two transformations I don't really want
to keep differencing right I can
difference in difference in difference
and I will eventually get stationarity
it becomes kind of ridiculous right so
that's another thing to think about you
might apply the model know that you are
not quite conforming to the assumptions
but at least still have some basic
intuition about what your modeling
versus following the letter of the test
but not necessarily modeling something
intuitive okay
we're also going to do our Diagnostics
on this data right so we did we just did
Diagnostics on theoretical AR 1 and ma 1
models to see what they should look like
let's see what some real data looks like
so we've got our stat tools we're going
to compute our ACF and our p ACF and we
are going to plot them so this is my ACF
what do I see here well I've got some
seasonal data which is why I've got this
thing at 12
I'm going to ignore that for now and
just notice I've got a lag of 0 and a
lag of 1 is significant and I'm only
going to look at the first time it
crosses that line for significance and
I'm going to say well that looks like
for an autocorrelation that suggests to
me an MA one process right it's a
process where the first leg matters and
then it drops below significance and
this I'm going to chalk up to
seasonality for the time being I'm also
going to look at the partial
autocorrelation function to look for my
AR term right my auto regressive term so
this was my moving average term now I'm
interested in my auto regressive turn
and I hear it's a little it's a little
dodgy alright so again this stuff out
here I'm gonna chalk up to seasonality
but after my lag of zero I have a lag of
one which looks significant right that
is above my sort of rule of thumb
significance dotted line my 95%
confidence interval this one is just on
the brink so I'm sort of thinking this
looks like an AR one or an AR 2 process
I'm not entirely sure personally if I
think about Airlines well I could see
that the month before would correlate
but I could also see that maybe two
months before it would be important
especially because people don't tend to
just buy tickets you know and use them
the next day there tends to be some
lead-up so it wouldn't be surprising if
two months we're also a useful amount so
I'm gonna keep that in mind as I look at
my model I'm gonna import my ARIMA model
and I am going to play around and see
how things look so in ARIMA ARIMA model
has a PDQ the D is the differencing
right so how many times do I want a
difference I happen to think that one
differencing term is nice so I stick
with that the P is the auto regressive
component right so it's P DQ for now the
Q the moving average component I'm just
gonna look at zero because I want to see
if I just model this as an
autoregressive right trying to be as
parsimonious as I can how does that look
and see how that looks and I'm starting
with the auto regressive because it very
clearly has some auto regressive
component right the moving average
component I can always sort of throw in
the auto regressive bit though is very
clear partly because I had a trend in
the original data so if i graph that
I've got my red as the fitted values my
blue is the true values transformed it's
okay I've got a
of square error of 1.5 which doesn't
really mean anything per se until I
start comparing with other things so now
let me try a model with moving average
model and see how that does right so I
get rid of the P term for my auto
regressive I have one difference right
that's just intrinsically there and I do
a 1 order of 1 moving average I guess
that looks ok - I mean they they sort of
look about the same and if you look at
the sum square error they're about the
same right so it's not clear that either
of them is doing such a bad job or such
a great job I can try to combine them so
now I have an auto regressive model of
order 1 with a moving average model of
order 1 let's take a look at this the
sum square error is a little bit better
as we can see it's dropped down to 1
point 4 5 not necessarily that much
better disappointingly but I'm gonna
sort of call it a day there and go back
and map it to my original data and think
about how that does so let's go ahead
these are our predictions
we're just backing out to our original
form of the data and now let's compare
that so the ARIMA model with let's go
back and see what we did with the 1 1 1
that's what we settled on gives us this
right so it captures a trend when we
untransformed captures a trend but it
doesn't really capture a the seasonality
or be the change in variance right why
is that should I expect it to not really
right so mm-hmm right so there's this
multiplicative aspect that I haven't
accounted for for the seasonality and
also the fact that I'm really trying to
model a stationary process right so with
an ARIMA model i differenced it so I
dealt with the trend I can build that
back in but I didn't really factor in
the changing variance and the chasni
changing seasonality so that
it's sort of the limit of a basic ARIMA
from here you can go to a seasonal ARIMA
what you do with a seasonal ARIMA is you
basically decompose your model into a
seasonal component which has its own
moving average and auto regressive term
and a non seasonal which is what we've
done here
that would be your seasonal so this this
is about as good as you can do with like
a plain vanilla ARIMA why is it useful
well it's still useful because you've
you've still got the underlying trend
you've got some model for how it works
so you can try to develop some insights
from that and you also sort of can see
exactly where you would put the seasonal
bit on right so if you don't want to do
a complicated seasonal ARIMA where you'd
have six parameters rather than the
three we already have you could even
think of sort of doing your own additive
model here combined with your ARIMA so
the ARIMA can be combined with other
models so no more we're not going to
follow up but i'd very strongly
recommend looking into the seasonal
ARIMA just to understand how it works
and also things like confidence
intervals etc so I'm going to leave the
ARIMA here we have 30 minutes to cover
time series classification we're going
to take a five minute break and then
finish off with that
so we'll reconvene at 409
so let's get started last thing we're
going to talk about today is clustering
and classification of time series which
is yet another route to prediction for
those of you who work in areas related
to machine learning this will probably
be very familiar because it uses many of
the same techniques and just applies
them to time series so clustering and
classification are especially
interesting because they deal very well
with the regular time series right so
ARIMA assumes you have this stationary
world it assumes you have this non
seasonal world until you put in all
these fancy modifications and it assumes
you have regularly sampled your data and
they sort of mean the same thing across
time if you have a regularly sampled
time series one very intuitive
straightforward way to sort of deal with
that is to think about how they relate
to each other and use that as a way to
predict right so I can look at a time
series and I say well what does this
time series look like based on what I've
seen and the one it looks most similar
to might be a really good way to predict
it so we're going to be thinking about
how we do that how we look at sort of
the whole time series and related to
other time series to do that for those
of you who know a bit about clustering
right and comparing things when you want
to compare things and determine how
similar they are you need some kind of
quantitative measure of how similar they
are right like if I want to say how
similar our people I could say well if
you have the same gender I'm gonna say
you guys are identical that could be one
metric right or another metric would be
a measure of similarity between people
is I'm going to take the difference in
their weight and square it and multitude
at adds three times however many miles
apart they live from each other right
there's like many ways you can come up
with a metric to compare things for time
series it might be very tempting to just
use a Euclidean distance right because
you say well it's not like people where
they have all sorts of different kinds
of features it's just a time series
what's so complicated about comparing
these well let's first by say let me ask
which of these two are the most similar
right I
think all humans will agree which are
these are most similar which of these
two time series are most similar to each
other the green and the blue does anyone
disagree anyone at all
no right I mean it's it's so obvious
right it's so stupid I'm even asking
this question and yet if you use
Euclidean distance those are not the
most similar right so this is the proof
that you don't want to use Euclidean
distance when you're comparing time
series okay if you use Euclidean
distance it's actually the blue and the
red that come up much closer together
for some reason than the blue and the
green so there's your proof that you
need a better distance metric what is
that distance metric the one that seems
to work really well and that is fairly
intuitive and it might even be what our
brains all did when we decided those two
time series were the most similar the
two signs is dynamic time warping what
you do with dynamic time warping is you
line up two time series next to each
other as we do here and you try to make
them similar to each other by saying
okay for this time series to compute the
distance with the next one I'm going to
give a sort of the closest you know best
match I can so that for each point I
will just pick the point in time that
best matches it and that could be for
example in this case right of this green
we see like one inkling you can get is
like this sort of curve here or this
kink I probably want to match it up here
so match this up that means this first
one this first point has to match up to
a couple of points right so it's sort of
like a shape-shifting way of matching
things up you sort of shift your time
seer like imagine these are my time
series right I want to sort of shift
them until they match as well as
possible and this is a very Pat example
but you get the idea I'm going to move
them around but it's not just that I
shift them and that's why the word
warping comes in I can also shift the
amount of time per time series right so
I see here between the green and the
blue if I condense these three points
into one on the green I'll get a much
better
later so I do this search to find the
way to line up points including
compressing time to zero such that I can
match them best and you can see that
sort of grid layout here this one time
doesn't move whereas for this one it
does move so that's dynamic time warping
that's the distance metric is first I
line them up and then I compute it why
is this great because I have a distance
metric that works the way our brain
works for example this blue and the
green will now be closer together with
this dynamic time warping because I can
sort of shift them to have that sine
shape reflected in both of them and I
can also do nearest neighbor cluster
nearest neighbor classification and
clustering so let's go ahead and get
started with that with some code so you
can see how simple this is we'll go to
notebook 10 clustering and
classification
don't worry too much if you don't have
these functions I don't actually think
you need them here we're going to read
in a CSV which has actually the words as
time series so people speak words they
are recorded as time series the
inflection in our voice but we all say
them a little bit differently and we
have 50 different words as you can see
they're just labeled word-for-word 12
word 13 let's take a look at some of
them to see what I mean here by our time
series here what you see is I'm actually
grouping the words by their value and
taking the mean so these are different
kinds of words let's take a look at word
type seven this is what the average word
type seven is I'm sort of at each point
in time I'm averaging all of the 7s and
let's take a look at what just one of
them looks like right so this is sort of
an idealized time series I've come up
with by averaging all of the spoken
words of a certain type but this is this
is really more what my data is going to
look like is something like this
something kind of fuzzy
what about the full sample of all
these words well--that's here right so
word type seven you can see they all
sort of follow the same shape right they
have one bump and then they have a
second bump and then they have maybe one
or two bumps in the middle and then they
have a bump at the end and as you can
see though there's like some outliers
like this blue one here which it almost
seems like this person is speaking a
different language right they don't even
have the same number of rising and falls
and this gives you an idea of why
computers recognizing what people say is
so amazing right because we all say it
differently to quite a large extent so
the first thing we need to do is code up
a sensible distance function right and
that's that dynamic time warping I won't
make you do it I'll just load this
snippet here and we can take a look
together so we're going to define this
dtw distance between two curves we're
first going to just set the distance at
each point to infinity and then we're
just going to go through and for each
point we're gonna try to find a way to
match them up so that we minimize a
distance right so for every point in the
first time series for every point in the
second time series we try to compute
that distance where we're sort of
minimizing right we're getting the
minimum of either moving one forward
moving another point forward keeping
them matched up and getting that minimum
distance so that gets our dynamic
time-warped distance once we have that
distance we can start computing
differences right so here I just pick
randomly two words I just grabbed them
out of that data frame and I'm gonna
compute the distance it's six point six
that doesn't really mean anything on its
own though right so unlike Euclidean
distance dynamic time warping distance
is only meaningful in the comparative
sense right and in that sense it's not a
true distance metric right like
euclidean distance means something in an
absolute sense dynamic time warping only
means something in a comparative sense
so we can look at some distances and we
can plot
so here we've got three curves let's see
two of them are quite similar right the
yellow and the red which is s1 s2 s3 the
black one is fairly different from the
other two and so let's see if that is
borne out by our metrics right so s1 to
s2 red to yellow which one is it
I didn't compute s1 oh yes we did six
point six right s 3 to s 1 is 11 point 1
s2 to s3 is 10.5 so so far I'm satisfied
with this metric in the sense that s3
this black one is showing up as more
distant from the yellow and the red than
they are from each other so we say okay
at least the ordering of this distance
makes sense right okay
on the other hand if we look at the
Euclidean distance which I won't make
you code out but there would be our
Euclidean distance if we compare the
values here so s1 to s2 right those are
the two that are similar versus s3 to s2
and s1 to s3 well s3 to s2 those are
supposed to be far apart and the
Euclidean distance makes them far apart
but s1 to s3 is almost comparable to s1
to s2 and here's another example of why
the akkadian distance failed so this I
truly generated randomly I wasn't even
sure that it would fail when I first put
this example in but it does so that
example in my slide is not necessarily
all that artificial Euclidean distance
will often lead you in the wrong
direction so let's think about can we
classify a random row by determining
which mean curve it is closest to right
so we looked at the variety for example
of words right these are all the exact
same word we see quite a bit of variety
but we also saw that we could take a
mean or a median of all of them combine
that and get some sort of mean shape per
word so a first reasonable question we
can ask is can we sort of use this
distance metric to figure out for an
individual messy data sample which
idealized form is it closest to and is
that a good way of classifying a time
series so to do that
we can first compute that mean curve
right so for each word type grab all of
the examples take a mean and say that's
my idealized word that mean type so
that's what I've done in this code here
I'm going to just randomly grab a test
word boom I've done my test word in this
case my test word is of type 5 I'm gonna
calculate a distance dictionary where I
calculate the distance from my test word
to each of the idealized forms of the 50
kinds of words I have so I'm gonna do
that it's probably gonna take a little
while right because it's not super fast
okay that has run and now I'm gonna do
an ordered dict
and look lo and behold when I run my
dictionary and sort it on the Left I
have my word type on the right I have my
dynamic time warping distance and I
randomly picked a type 5 word low and
behold the closest thing is indeed the
idealized curve of type 5 great news is
it the best possible news though what do
you guys spoke spot here type 36 is also
awfully close right so in this
particular case it worked out can I
guarantee that it would know
classification and clustering are not
perfect but this is how you can do it
and you have you know a pretty good shot
of matching to what you want so that's
one thing you can do right this is that
nearest neighbor classification right
which of these idealized types is
nearest to my actual data sample another
thing I could do is I can say well this
is kind of computationally taxing right
like this this is a very small data set
and it still took a little while on my
computer right like it didn't instantly
compute which is already kind of scary
on such a small fake data set so you
might say well can you cluster the words
using this dtw metric right this dynamic
term time warping metric where instead
of calculating the distance to idealized
forms can I calculate all of the words
and just
organically cluster them yes but that
would take more time at least on my
little computer than we have I mean I I
tried running it once it took more than
three hours it's kind of an old computer
but you can do that at home if you want
to what you can do though in the real
world if you start running into
situations where you have lots of time
series of different shapes and you want
to start matching them is you can do
clusters based on features rather than
clusters based on dynamic time warping
right so there are many different ways
to define distance and this is actually
where we go back to treating a time
series like a person remember how I
talked about how there's different ways
we could measure distance between people
one way to do it would be to use
features and this is very common in
large data sets in reality so dynamic
time warping is sort of what you might
do in a perfect world but in reality
feature selection is so much faster and
so much sort of more intuitive when
you're trying to explain the model that
people often go with feature selection a
nice thing about feature selection is it
also gives you more opportunities to
incorporate domain knowledge so dynamic
time warping is more sort of automatic
right you just sort of match things up
and you sort of warp time to make them
fit each other but that doesn't give you
the opportunity to say what you know
about the subject right whereas if
you're an astronomer or you're a
business retailing forecaster or you are
a stock trader or whatever you might
actually know quite a bit about what
matters in the performance of a time
series and you can extrapolate that so
for example in this particular case we
could go back and we could sort of look
at a sample of the words even as not
domain experts but a couple of things
jump out at me the inflection points are
really important right the location of
the max is really important if I start
thinking about where this purple differs
from this blue well the max for the
purple is early the max for the blue is
a bit later
the purple seems to have what 1 2 3
local minima whereas the blue has one
also has 3 local minima but the green
this green one only has two local minima
or Maxima rather so I started seeing
ways in which these series are different
and I'm going to just generate features
based on the way they're different so a
few things I'm going to pick
I'm gonna look at the max location I'm
gonna look at the first local max the
second local max assuming I have
verified there's always at least two I'm
gonna look at the total number of
inflection points right so something
very bumpy versus just kind of bumpy I'm
gonna look at the ratio of the first
local Max to the absolute max right I'm
just adding things to make sure I have
enough to distinguish these fifty kinds
of words and I'm just gonna put together
vectors of all of these qualities and
I'm gonna make a data frame out of them
and here's what I see
so my first local Max were just my first
recording is at 23 my max location is at
187 etc the word type is the label right
so that part is what I know to be true
I'm gonna actually leave that out now
then I'm gonna try to do some clustering
I'm gonna just do a k-means cluster you
can do all sorts of clustering right I'm
gonna look at those labels and I'm gonna
just see how I did with this clustering
so let's see this is my word type and my
label my labels don't need to match in
the sense of like a 5 for my clustering
is not a 5 for my word type but what I
do want to see is whether all of my word
type 5 ended up in the same cluster and
what am I seeing well I'm seeing that
they didn't all end up in the same
cluster that's not surprising when you
work on real-world data it's usually not
perfect I am pretty happy that there
seems to be a lot of 37's a lot of two's
a lot of sixes so I made 50 groups in my
model and my model came back and took
one group and broke it up into three
clusters I guess that's not so bad right
that's probably quite understandable
given the amount of variation that I see
right and and the way that we saw that
this blue was the same word as this
turquoise it's hardly surprising that
they didn't end up in the same cluster
but I get some sense that the clustering
can accomplish something so that's a
straightforward example of how you can
deal with time series when you maybe
have a regular time series data or where
maybe you just think that the shape is
more important than the actual data
points
right so for an ARIMA you're sort of
looking very fine
finally right you're sampling regularly
you are making a stationary time series
that might not even look like your time
series and what do you get out of that
you try to predict a little bit in the
future maybe one period in the future or
two periods in the future with
time-series clustering and
classification you can do a regular time
series without having to up sample and
down sample and sort of impute data or
make up data in some way you are also
emphasizing something that's slightly
more intuitive right and for certain
domains might make more sense like
astronomy or human behavior it might
make more sense to think well does it
look like this or does it look like this
and there might be some underlying
reason you care in that case you're
going to go more for clustering and
classification and clustering and
classification in particular is very
transparent in a way that the ARIMA
model sometimes isn't right so I Rima
has three parameters or if you do
seasonal it will have six parameters it
does this very complicated maximum
likelihood estimator you get back these
numbers you can really only predict one
or two periods in advance in contrast if
you can cluster a time series just based
on an early portion of it you might have
a much better idea of what your later
portion of your time series looks like
even though it's still very distant in
time so that can be a an upside of
clustering and classification so I just
want to talk briefly about some other
things you might want to look into if
you are interested in time series the
first is vector Auto regression I
actually had a question about this
during the break what if you have
regularly sampled data of many kinds
right so we did an ARIMA model where we
just had one time series in the real
world you are often blessed with
abundant time series in parallel for
example you might want to predict
unemployment in the US economy and you
have all sorts of info you might have
your consumer happiness index you have
the price of bonds maybe you have some
global economic measure all of these
things regularly sampled and they all
tend to predict each other you can take
advantage of that structure and as you
can see it's an auto regressive model so
it's very similar to the ARIMA model we
looked at it's just that much more
fitting and many more parameters because
now you're using parallel
sources so that's handled in stats
models if you have that kind of data
that's probably what you want to start
with very common in econometrics
standard workhorse is used all the time
common filters and filtering in general
signal processing in general right
that's really just time serious if you
work on the sort of data where you are
handling live data and you are trying to
track a process that is moving as you
track it common filters and other sorts
of Bayesian filter methods are probably
what you're looking for with these sort
of methods what you do is you take the
information that you have you sort of
update what you know about the system
you make a prediction then you measure
the next prediction you update what you
know about the system you make a
prediction you update what you know
about the system and it sort of feeds
off itself in a live format so that's
common filtering another area that you
can learn more about and that is handled
in many python libraries hidden Markov
models these are very common especially
in the sciences with a hidden Markov
model what you say is I have a time
series it has some kind of observable
but it's not the thing I'm actually
trying to measure right the thing I'm
actually trying to measure is hidden so
this might be for example you work in
biology and you have some kind of cell
that can be in let's say five different
phases and depending on which phase it
is it gives off maybe a different level
of some hormone right and all you can
measure is the hormone you can't
actually measure the underlying phase
right that's sort of hidden information
this is what you're going to do with a
with a hidden Markov model is you have a
way to try to back out what phase
something must have been in based on
what you saw so this is sort of an ex
post model you're not going to use this
for forecasting but you will use this if
you think there was something hidden and
your time series only gave you a partial
indication of what that was so you can
use hmm learn in a situation like that
and then finally the thing that scares
me most is deep learning because there
is always the possibility that deep
learning will just eat everything else
so I feel almost like I need to make a
disclaimer about my whole presentation
people are having tremendous success
with recurrent neural networks both in
natural
which processing which is arguably a
kind of time series and also with time
series prediction it's still in early
days but I have a feeling that ARIMA
might get blown out of the water or you
know time series classification can
easily get blown out of the water with
this so this is something you definitely
should keep an eye on but it is still
very good to understand the traditional
methods and then finally as far as more
resources so it's this sort of like a
bird's eye view of time series and time
data there are a lot of really great
resources out there so these are just a
couple of my favorites that I would
recommend as like next steps if you want
to keep working on time data so that is
the end of my presentation thank you for
coming and I will hang out if there are
questions yeah so the slides will be
available as soon as the PyCon folks
post them and if there isn't a formal
PyCon route I will also put it on the
initial git repository that you
downloaded from so those up I'll put up
actually later today</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>