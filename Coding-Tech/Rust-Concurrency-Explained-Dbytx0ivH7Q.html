<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Rust Concurrency Explained | Coder Coacher - Coaching Coders</title><meta content="Rust Concurrency Explained - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Rust Concurrency Explained</b></h2><h5 class="post__date">2018-04-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Dbytx0ivH7Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">first off what I would like to talk
about is kind of why we're talking about
concurrency and why concurrency so
interesting today especially in the
realm of C and C++ and so this is a
graph of a bunch of data collected over
40 years of just kind of CPUs in the
various trends among them in a couple of
different properties but there's two
that actually show out high detail here
well the first of which is this aspect
that CPUs are not getting faster CPUs
have flatlined over the past decade and
this whole Moore's law where we're gonna
double our CPU speeds every six months
or a year at this point that's all out
the window and that's not gonna work
anymore well then the trend we see is
that we have this exponential increase
or we see a very large increase in the
number of cores available in your
machine so we can't make everything
faster but we can add more of it at the
same time and so it's our job as
programmers if we want to kind of keep
up with make our programs continually
faster and faster to make use of all
these resources to actually use all the
cores we have available to us and
actually kind of make sure we're doing
that in a productive fashion that's not
accidentally crashing all of our
programs so this sounds great and so
we'll open some bugs and so this is
actually a real bug on the firefox
rendering engine saying we should
parallelize CSS selector matching and so
that's that's easy to do we but the
problem here is that this bug was open
seven years ago so for seven years this
bug has remained inert saying all of
these possible speed ups all these
possible benefits so kind of using all
the concurrency on the hardware we
haven't been able to take care of this
and the reason for that is that Firefox
is a multi million line C++ code base
and we run into this issue which is
perfectly depicted by this sign and
actually this is massive so I don't
really need to blow it up but there's
this sign in the San Francisco office of
Mozilla which is low I think it's like 3
or 4 meters in the air something you
have to be this tall to write
multi-threaded code and it's really
showing off this aspect where to
actually use concurrency in a language
like C or C++ is incredibly difficult
and especially in a multi million line
code base trying to retrofit that in
there and trying to retro actively add
that is the next to impossible tasks so
we have these bugs that are open for
seven years with no-one actually making
any progress because we're never sure
when we've fixed it or we're actually
complete and what we don't have any bugs
and so this is where Russ starts to
enter the picture
this is a blog post entitled fearless
concurrency and rust which came out just
before West rust 1.0 was released a few
years ago and is showing that when you
the programming language of rust is kind
of directly targeted at solving this
problem of kind of freeing you from
these ideas of you don't have to worry
about data races or seg faults or all
these issues you see in C and C++ and
you get a compile time guarantee that
when you compile code none of these
issues exist and that allows you to kind
of have the freedom to actually use all
the cores in your machine and actually
make use of all the resources that we
have available to us now it is so before
we go much further I want to give you a
brief overview of rust itself like I
said a little bit earlier I was talking
about these strong safety guarantees
these know seg Falls these know data
races which is kind of the thread safety
aspect but also this without
compromising on performance you get all
these safety guarantees with the same
level of c++ or the same little
performance the c++ and kind of that
same underlying architecture of
compiling double ax machine have a
relaxed raw access to the machine code
no runtime and all of this kind of comes
into this package of confident
productive systems programming this is
kind of a slogan which we've been using
for rest nowadays where the system's
aspect is very similar to c++ at being a
low-level but the safety in kind of all
of the aspects of rus that are providing
you the freedom to not worry about these
seg faults and databases gives you this
confidence to actually be more
productive and to actually do more than
you would be would do otherwise and
other languages the perfect example of
being like gecko bug which is actually
been solved now and I'll be talking a
little bit more than about that in a bit
so in this talk first I'm going to talk
a little bit about concurrence you just
kind of make sure we're on the same page
of what it is and kind of give you an
overview of what I mean by this I'll
give you a bit of a small insurer about
rust so it's okay if you've never heard
of rust before and then I'll dive into
the libraries this is kind of what you
can do with concurrency and rust
some of the primitives we have how they
work and how we actually make them safe
using the guarantees that rust has and
then finally not entirely concurrency
related or a little bit different is
futures specifically about asynchronous
i/o and the kind of the what rust
stories that fit for today and kind of
how it all works in a bit of a deep dive
and kind of how that's implemented and
how it's so fast sorry this is kind of
the Wikipedia definition that you
Seifer concurrency online which is a lot
of fancy words but in general it just
means that you have a bunch of actors
executing at the exact same time we're
kind of slightly interleaved kind of a
lot of stuff happening all at once and
so we can kind of take a look at an
example of this with a small seed
program which this calls fork and prints
out the results and if you haven't seen
fork before it just creates a new
process on unix and returns twice once
in the parent process and once the child
process and so this could print zero and
the pit that was actually returned by
fork or it could print out the pit and
then zero and this is showing that kind
of just this smallest fundamental unit
of adding concurrency to a program
already starts creating this kind of
exponential growth of states in our
program that we have to worry about and
so this is what happens where when you
start adding threads to a much much
larger program it's effectively
impossible to understand all of the
possible interleavings here it's if you
can never fully comprehend how exactly
every single state and are they all
gonna work and they all valid and so
this is where concurrency has all of
these bugs associated with it these data
races these race conditions these seg
faults and some of these are actually
exploitable which is really bad in the
sense that if you're just kind of an
innocent library maintainer and you'd
like to actually use concurrency you'd
like to use the CPUs you have available
to you the problem is that if you do so
you might be opening up your users to
see V's to remote code execution to all
of these vulnerabilities that are
targeted back at you
and so this is a real problem because
concurrency is also super super nice
this is actually that the parallelizing
CSS matching and gecko has now been
implemented in rust and is actually
shipping I think in about ten hours in
the when the US wakes up and we get
these nice speed ups so Amazon starts
rendering for instantly almost twenty
percent faster and YouTube when some
computers goes up to 30 percent faster
and so we have this problem where we
have we have all these resources and we
want to use them because we put some
clear speed ups and some clear winds but
we're afraid of doing that because of
all these problems these data races
these seg faults these races these race
conditions and so this is where Russ
starts to end of the picture and I want
to talk about first you hear about the
safety aspects of Russ kind of how we
actually fundamentally build up safety
in rust and then that's gonna kind of
lead into how we actually use those
fundamental aspects to build up
concurrency primitives as well
so rust is kind of made of these two key
ingredients these your cost abstractions
which you're very familiar with from C++
and then also this memory safety and
data rays freedom aspect and so I'm
going to zero in here on this memory
safety and inner ease freedom kind of
specifically how that actually works and
what the fundamental concepts are within
rust itself and to start off I want to
show you a small example written in C++
so kind of what I mean by safety and as
we do this I want to dig in so what the
bug is here and kind of what's going to
happen so naturally in C++ we'll start
with a program here we have a vector of
strings the vector itself is stored
directly on the stack and then the first
thing we'll do is we've got a pointer
into it that will push some data onto it
transfer it out so as we come down here
we try and actually push some new data
on the vector those of you who have
implanted vectors before or familiar
with this might know that okay this is
that vectors length might equal the
capacity so now what he does allocates a
new data copy over the existing data
push some new data on and then the next
step is that we're going to update the
directors data pointers the vector now
points this new chunk of memory and
we're going to deallocate and free the
previous chunk of memory and clearly
this is the problem that were going to
see here which is that we have this
dangling pointer into freed memory now
our element pointer which means that
when we actually try and print this out
this is undefined behavior this is could
do effectively anything at this point in
time it could psych fold it could
actually succeed it could then kind of
do whatever and this is what we
explicitly want to avoid because these
are the kinds of problems where you
write it into your code it works today
and then ten years from now you get an
email saying oh you broke my program and
you're not around to fix it and I had to
debug it for six hours or thirty hours
to figure that out but so in any case I
want to dig into here kind of what
actually happened like why did this go
wrong and what can we learn from this
and how can we kind of generalize this
to try and solve more problems than just
this one specific issue and there's two
key ingredients the first of which you
find here is aliased pointers so we have
this element pointer and this vector
pointer which are pointing to the same
chunk of memory and then the problem
there is that that alone is fine but
it's when you add in mutations so you
have this mutation of the vector kind of
pushing on some new data and it's this
simultaneous act of both mutation and
aliasing which is causing a problems
here if you only had mutation then the
vector itself was always internally
consistent if you only had aliasing then
they're both pointing
same piece of data it's not changing so
it's totally fine typically this is when
a garbage bucket comes in and saves the
day but in rust we've kind of have this
constraint where we cannot have a
garbage collector we want this kind of
systems level of performance we also
want to have this lack of a runtime and
so the rest solution to this problem of
kind of preventing some simultaneous
mutation and aliasing is called
ownership and borrowing these two
aspects this cadiz kind of two pillars
are the foundation on which all safety
and rust is built is kind of the two
fundamental language features that rust
gives you and I'll talk about these in
some more detail but at a very high
level the first thing that ownership and
runtime or ownership/borrowing gives you
is no runtime this is not a garbage
collector this is purely a static
analysis so there's no extra metadata
tracking and there's no extra layers of
indirection it's kind of all executing
is you actually wrote it and just
running static analysis at compile time
we as I've been saying this is kind of
the foundation for memory safety get so
this is how we're going to be preventing
databases how we're going to be
preventing seg faults kind of this is
where the thread the thread safety
aspect comes into rust as well we're
kind of building off of ownership and
borrowing and it's really nice to
compare and contrast this in the sense
that in C++ you have the freedom of not
having a runtime which makes it very
embeddable makes it very running in a
very large number of contexts but
obviously in C++ you don't have memory
safety you don't have freedom from
databases other languages like Java or
Ruby or JavaScript they have a garbage
collector which gives the memory safety
but they obviously have a very large
runtime associated with them it's very
difficult to embed it's very difficult
to run in constrained environments and
the very interesting thing about these
languages is that they don't actually
protect you from data races and so this
is kind of the key aspect of ownership
and borrowing which is it's giving us
all these benefits effectively for free
and this in the sense of we have no
runtime we have memory safety and we
also have no data races sorry I want to
give you an overview of first ownership
and a little bit of borrowing I'll talk
more about these in the talks tomorrow
this is gonna be a bit of a fast tour
but so here's an example of some Russ
code where the first thing we do is we
create a vector on the stack and as we
saw with C++ the vector is stored
directly inline the kind of data the
length code the capacity on the stack
itself we have this word mute here which
means that we actually can mutate the
vectors saying okay we can push someone
onto it we can push
two-on-two it put some data onto it onto
the heap and then we're gonna call this
function called take and so what's
happening here is that on the right hand
side we have this bear V of X 32 that's
kind of like the type that it's
receiving and then we're calling the
function take with the binding V and
what's happening at runtime here is
we're transferring ownership which means
the function main is gonna push a copy
of its vector for take to have so it's
gonna kind of make the copy of that data
the length and capacity not the data
itself just kind of the vector on the
stack and then it's gonna transfer
ownership to the function called take
but simultaneously as part of all this
the function main forgot about kind of
quote-unquote its copy of the verse of
the vector so it's not actually gonna be
able to access it anymore and now the
other thing about ownership is that take
as the sole owner of this piece of data
it's kind of the unique owner of that
vector when it executes and actually
finishes we know that it has gone out of
scope there are no possible owners of
this data so as the owner you have
control over the destruction of this
giving us deterministic destruct
destruction and rust very similar to C++
and this is where you free resources you
close sockets you free memory you
destroy everything internally and so as
we come back to the function main this
is where may no longer has access to
this vector it has relinquished
ownership it no longer can even touch
this vector which means that if we take
this code and we add a kind of extra
push on top of it if we were we were to
allow that this is obviously free data
at this point so this would be used
after free but the Ross compiler will
reject this saying you have a use after
move of an own value and so this is
where again this is a static analysis
happening at compile time which is
preventing you from using anything after
you have moved it elsewhere but you have
one owner you can transfer ownership and
then as the owner of a piece of data you
can drop it and deallocate it and free
it and I'd do anything associated with
that so the next thing in rust was
borrowing oh sorry in the the key aspect
of ownership I'd like to go through was
the why I was talking earlier about
owner or aliasing in mutation we're
trying to prevent both of these from
happening at the same time because
that's where all these bugs are coming
from and so what ownership is doing is
its allowing mutation but it's not
allowing aliasing so borrowing is kind
of the other aspect of that where if we
only had ownership transfer we'd be kind
of on ergonomic to just keep passing
everything around by value so here we'll
have a vector of on the stack as well
the first kind of borrow of two and rust
is a mutable borrow denoted with this
ampersand mute on both the caller side
and the collie side on this but in this
push function now a mutable borrow as
the name implies does allow mutations so
as we come over here we'll create a
lightweight pointer that's kind of what
what references are doing in rust and
will be allowed to mutate this and
actually push some data onto that vector
now because this is allowing mutation a
mutable borrow does not allow aliasing
so once you have acquired a mutable
borrow we are no longer allowed to
modify this vector so we can't read the
vector we can't create a new mutable
borrow we can't take it look at the
length old this can only be accessed for
a small period of time in this one
region where we have a mutable borrow
but as we return from this function as
we return from push this mutable borrow
has gone out of scope so now we can
continue to read the vector and do what
else we want with it and the second kind
of borrow and rust is a shared borrow
and so this is denoted with this
ampersand sigil on the right hand side
and also on the left hand side and a
shared borrow allows aliasing but not
mutation so we can create many shared
borrows we can read some data but while
we are doing that we cannot mutate it so
this this read function cannot push any
any more constants any more contents on
the vector you can't create any more
mutable borrows but you can pass around
all these shared borrows and whatnot
sorry that was kind of a very quick
overview of state of ownership and
borrowing I'll be going in more depth
tomorrow on this but the key thing here
is that this is all happening statically
in rust this is kind of one static
analysis pass which is guaranteeing
these these aspects of never having
simultaneous aliasing and mutation and
it's kind of preventing all these bugs
where ownership for example is you never
have a double free because only one
person fries it and as the owner of it
and then borrowing prevents these use
data freeze like we saw in C++ where
once we had a borrow on that vector kind
of once we have the pointer into it
we're not allowed to mutate it we can't
say okay now update it behind the scenes
and we forgot to update one or the other
and so the coolest thing here is I've
also been saying that rust does not have
databases and these are the three key
ingredients for a data race this sharing
this mutation no ordering kind of with a
c11 member model it can cost over that
if you have all those all at once you
have a database and this sounds pretty
familiar at this point in terms of
by preventing either aliasing or
mutation never never happening
simultaneously rust kind of for free
with these two systems frees us from
data races and we no longer have to
worry about data races at this point and
so I'll show a few an example of this
later but it suffice it to say this is
kind of why rust has no data races and
how ownership and borrowing are
preventing that purely by saying we
should never simultaneously have
aliasing and mutation sorry
now I'm gonna kind of dive into given
these foundations Early's quick
foundations talk about some of the
concurrency primitives that rust has and
how they're leveraging ownership and
borrowing to actually give you these
this management of the of the machine
and a productive sense of not having
data races not having seg faults and all
that so the key thing to know about rust
is that rusts concurrency is all baked
into libraries not the language itself
historically we actually had
message-passing and we had a bunch of
stuff baked into the language but we
ended up removing all that by only
having ownership and borrowing so what
we're gonna see here is actually purely
baked in the libraries are the standard
library or the rust ecosystem and all of
its do all it's doing is leveraging
ownership and borrowing to give you this
ironclad guarantee of safety and I'll be
going over some examples of how that all
works so first thing we need to do is to
actually introduce concurrency we need
to actually spawn a thread we need to do
something that adds a new actor to our
system so this is an example in rust or
use the stood thread module which caused
a spawn function this will just have a
closure internal if that double bar and
braces or a closure in rust in this case
will compute evallo world and eventually
we can actually have a synchronization
point waiting for that thread to exit
and so this is a relatively simple
program which will just print hello
world now what we can also do with
closures and rust is actually close over
out our data we can kind of seamlessly
pull it in and start working with it and
this is an example where we're going to
crease start by creating a vector on the
main thread but then we're gonna
transfer that vector to a child thread
and start actually modifying it and the
key word here the key thing here is this
keyword called move saying that we are
moving the contents of the closures of
this vector into the child thread so
this destination vector it's starting on
the main thread we're then transferring
ownership to the closure and then that
whole closure is gonna go run on some
separate thread because it's mutable we
can actually start pushing onto it and
it's gonna all work out but so what
happens if we
actually come here and try it and add
some modifications afterwards this would
be a typical day to race if we actually
were pushing onto this vector from two
different threads or this one actually
called some internal seg faults but rust
will prevent this at compile time is
saying that say use after move because
we have actually moved this content into
the child thread we no longer have
access to the main thread and so you can
no longer touch it you can no longer get
any data races that way and then you
have been wondering well what happens if
we remove that move well what happens if
we remove that move keyword so this is a
case where the closure here is actually
only going to capture it by reference
and so with the borrowing system this
will actually work out in terms of we
have a closure that refers to some
vector and then we can actually push on
the vector afterwards but this would be
a data race or could cause various
problems at compile time or run some
otherwise where if the main function
returns before the thread actually
starts then you're gonna be using a
vector after it's been freed and so rust
actually gives you an error here saying
that this value does not live long
enough and so this is where kind of the
borrowing aspect also has these
lifetimes associated with it where when
you spawn a thread you only have to
capture everything by ownership you have
to capture everything by value you can't
have any shared references in there or
mutable references in there because they
do not live long enough and so this is
one where only we can only actually
successfully compile this code if we
transfer ownership of a vector into the
into the actual closure that responding
or the main function no longer has
access to it but that's not always the
most useful things so the first thing I
might try to do is to actually share
this data and kind of use it on both the
main thread and the child thread is used
reference counting and this is where we
have this RC type the standing for
reference counted pointers where this is
creating a new reference kind of pointer
with some vectors inside and then inside
the child we're gonna try and print it
out and then use it don't use it
externally as well but the problem with
this is that this reference kind of type
is actually not anatomically reference
kind of type so this is just using plus
equals 1 it's not actually doing any
instructions under the hood and so
rustic compile time will say that this
type cannot be sent across threads and
so this is another aspect of rust is
helping you in the sense the rust
internally has this notion of what types
are send a ball across threads in which
are not and we know that reference kind
of types for example that are not
atomically managed are not safe to send
across the
so when we actually captured this V and
we move it into the child closure even
though it was passed by ownership kind
of that one reference of it it was not
actually safe to mutate that reference
kind of multiple threads and so this is
rejected at compile time this is one
we're kind of whether or not a type is
sent or send able or not send able
across threads as one that's
automatically inferred by the compiler
and kind of works out internally so
instead what we can do actually to
actually successfully compile this which
I'll dig into some some more detail here
is use this type called arc and this
stands for atomically reference counted
and naturally is therefore safe to send
across threads we're gonna be atomically
manage the reference the reference count
so it's safe to frog that on multiple
threads at a time so here the first
thing we're gonna do is we're gonna take
a vector put it into an arc that's going
to create some data on the heap this
kind of big blue box will initially have
a reference count of one because that's
the the one reference that is being
returned in our V pointer is pointing to
that and then we'll have our data inside
our vector of our one and A two when we
come here and call this clone function
what we're actually doing is creating a
separate reference to the same reference
kind of data so how this just this extra
v2 pointer that is pointing directly at
this rough count it'll update the actual
reference count to two and now we can
start actually moving these and kind of
moving them in two separate threads so
our thread here is actually executing
with the first reference and when we try
and actually print out the length of
this vector what's happening is that
you're getting a direct pointer directly
inside this arc so you're kind of going
past all the data going past the
reference kind of data that extra extra
header with the reference count if
you're reaching directly into their arc
and kind of printing all that out and
there's some key things happening here
in terms of how this ends up all being
safe the first of which is that arc when
you construct it is taking ownership of
the data as you pass it in and so this
is this is key in the sense that we know
that as the owner of this vector there
are no outstanding aliases ownership
does not allow aliasing so the arc here
is the sole owner of this piece of data
and then as the sole owner of that piece
of data you can then decide how others
get access to it so for example here an
arc is only going to allow shared access
because we have some aliasing with extra
reference counts here so with the shared
access that arc is now giving us we can
now have effectively a safe program at
that point and so because arc is
atomically managed when this thread
it's in kind of one the other function
exits will come down here and we'll
decrement the reference count to zero at
that point we can start freeing all this
memory and releasing that back to the
system and so I was saying earlier that
we only allow shared references here
which means that if you accidentally
were to say oh let me actually try and
mutate the vector behind this even
though the memory is actually managed
for you and with our canned atomically
and actually correctly this could still
be a data race in the sense that the
other thread may not be expecting you to
mutate this vector at the same time but
Ark is only giving you the share
reference it's not giving you a mutable
reference and so this is preventing you
at compile time from mutating the data
behind this Ark which kind of is again
preventing those data races providing
those seg faults preventing these kind
of classical concurrency bugs but so
this also isn't weave minute to share
memory now they can share some memory
between the main thread and between a
child thread but it's not so useful if
we can't mutate it so this is where
mutex has come in this is similar to a
mutex or a lock in other languages so
mutex and rust is denoted with this
mutex of I 32 this kind of type
parameter in here or what's inside of
the mutex and this is another thing like
with ARC we are taking ownership when we
create the mutex which means that we are
protecting this data inside of the mutex
and as the sole owner of that piece of
data it is now arbitrating access to the
underlying content so you you must go
through the mutex to actually try and
acquire it and so this is kind of like
we'll have our blue mutex box which will
say is originally unlocked our counter
just kind of points to the outside with
some data inside it and we'll just say
it's a zero for now for this integer so
the first thing and the only thing that
you can do with the mutex is luck it and
this is kind of codifying the a the
necessary pattern that most people would
agree that if you have data Bob
protected by a mutex you should only
access the data if you actually acquire
the mutex itself at that point if you
actually gone through that and this lock
function is going to return what we call
a guard type and this guard is actually
a proxy through which you can actually
internally access the data so here we'll
lock our mutex
no one know of the thread can exit at
this point we've blocked and made sure
that no one else is accessing this and
then through data we can now directly
access the underlying contents of the of
the integer here and so we can add one
two we could update our zero to one
do whatever we like to the actual
underlying data so again this is using
ownership to make sure that once you put
data inside of a mutex you have to
access it through the mutex there's no
outstanding aliases there's no
outstanding references there's no other
way to touch that data and now once it's
inside there you can only touch it after
you've locked the mutex kind of the only
safe operation is you wait for all
threads to not be touching the mutex and
then you have access to read it to
modify it you can get them shared or
immutable borrow from this guard type
and also as I was saying earlier we have
deterministic destruction in rust and so
this guard as an own type we know that
it's coming out of scope here at the end
of the function and so naturally you're
just going to unlock the mutex allow
some other threads to come in there and
actually modify it and do whatever they
like to it so this is one we're very
similar to C++ and rust you will
explicitly acquire resources a goal a
commute X you'll allocate some memory
you'll open a file but you very rarely
explicitly deallocate references you
just kind of let them fall out of scope
so the locks here you just let it fall
out of scope you never deallocate memory
you just let it fall you just let it
fall out of scope like the vector we saw
earlier so the next thing I want to talk
about is that we not only have mutation
through mutexes those are some little
heavyweights sometimes so we also have
Atomics where these are very similar to
C 11 or C tommix with the same memory
model and the key thing here is that we
have we do not declare this number as
mutable
we're actually mutating this through a
shared reference now that sounds kind of
bad in terms of rust was all about
preventing simultaneous aliasing and
mutation but the key thing here is this
is still not a data race in the sense
that one of those ingredients was know
ordering but you can only access these
atomic variables with some ordering this
seek kissed here the seq CSCS T is kind
of sequentially consistent and so once
you have a tommix you can have atomic
fetch ads atomic swap stop clothes Tomic
store was kind of all you would expect
from c11 or kind of that meant that
atomic memory model you can do in rest
as well this is a little bit lighter
weight to share to share mutable memory
amongst threads than just having a mutex
at all that all the time so the last
thing I want to talk about in the
standard library itself
are these MPSC channels these ability to
send messages across channels and the
ability to kind of pass these own values
between threads and so previously we've
we've been mostly looking at shared
memory concurrency where you have a big
chunk of data probably managed with an
arc or some other different matter
memory management scheme and then you
have some sort of internal mutability
with Atomics or mutexes or it's just
being shared concurrently amongst a
bunch of threads but message passing is
also quite useful sometimes in terms of
it's much easier to pair a paradigm to
work with you don't have any extra
sharing of memory it kind of depends on
like the application at hand so to start
out we call this channel function in the
NP SC module which creates a T X and an
R X half standing for transmission and
receiving for sending messages and
receiving messages with these two halves
we can actually clone the TX half this
is the NP the multi producer we can't
clone the RX that's the single consumer
aspect of this and so we have all of
these pointers that are pointing to kind
of the same chunk of memory the same
channel in memory and these are kind of
a managed externally well start up two
threads that are kind of going to send a
five and a four and so we've now
isolated one over our TX and one
threaded our th2 in another thread and
then these messages will return will be
actually will come to us and some
non-deterministic order will say it
comes at a four and then a five MPSC
channels our FIFO first-in first-out on
the sense the the this will print out
four and five the kind of whatever order
they come in will actually come out the
same order on the other side so once
we've actually freed the t act the T the
T X and the TX two now we know that the
channel is now closed because it can no
longer actually receive any messages at
this point no one else can possibly send
messages in and so we can actually
iterate over all messages left here and
kind of atomically close the channel
anatomically handle handle all the
messages and free all the resources
associated with that so that's kind of
an overview of what the standard library
gives you these kind of multiple
multiple paradigms of shared memory
concurrency message passing kind of
whatever fits the bill for you we have a
couple of various tools there but the
actual concurrency story and rust goes
far beyond just the standard library
that's kind of just what we give you but
the aspect of this is again this is all
using ownership and borrowing kind of
everything you've seen here is just
building on these fundamental language
concepts and giving you these
concurrency primitives and this can be
done externally as well in the ecosystem
so rayon is a crate or a library which
we call it in the rust ecosystem which
is kind of primarily focused on giving
you very easy access to the
concurrency available and kind of making
very easy to add parallelism to your
program and I could give an entire talk
on just rayon but as an example of this
we'll start out here with a small
function it's just kind of iterates over
lists squares everything and sums it up
and this is only uses one core it's kind
of just running on one machine but if
you have a giant giant list you might
actually want to execute it on multiple
cores at the same time to kind of make
use of all those resources and with
rayon all you have to do is take this
one function called itter
and switch it to a call to par it or
which stands for parallel iteration and
so that's the kind of the key aspect of
rihanna's very very lightweight
concurrency and so what will happen here
is this little divided into big chunks
throw it all at a bunch of work-stealing
thread pool with threads and kind of do
this all on parallel and machine and
then we've had some very very nice
benchmarks with with rayon in terms of
it's very very productive confer
concurrency and does a very good job in
terms of slicing this up the
work-stealing kind of nicely spreads out
the load and everything and there's a
whole lot more you can do with robe with
rayon itself but this is kind of just a
taste of how on the own once you can
actually get outside the standard
library we have kind of further
abstractions that are all leveraging
these these underlying language produce
and what's in the standard library as
well the key thing though about rayon is
that what I saw you is not too too hard
to implement especially in C++ you can
have a library that has all these works
doing aspects but the the real benefit
of doing this in rust is that you might
forget at some point that this is a
parallel iterator you might forget this
is actually running across multiple
threads so you could introduce a bug
here by saying oh well let me just count
up every time this map this map function
happens but obviously this is a data
race this is cannot actually be shared
safely amongst multiple threads this is
not Amelie updating this local counter
and so the key aspect of rust is that
this is rejected at compile time with
this error saying that this variable
cannot be shared concurrently amongst
multiple threads so again like what
you're seeing with a standard library
where you cannot misuse these these
fundamental primitives it's the same
with the ecosystem you cannot miss use
rayon if your code compiles then you
know there are no data races there are
no side faults no double freeze
none of that stuff you'll have to debug
anymore and so the last create that I
want to talk about is a crate called
cross beam this is a crate in the crate
size ecosystem which is primarily
targeted at actually porting algorithms
from GC
languages into rust and so the problem
is that there's been a lot of concurrent
data structure research but it's
primarily done in languages like Java
which have a garbage collector which
notably means that most of the research
most of the algorithms most of the data
structures do not explicitly deallocate
memory they kind of assume that there's
some garbage collector to take care of
that and so what crossbeam is doing is
giving this technique of epoch based
memory reclamation which is kind of like
a mini GC just kind of localized this
one crate and so it's making us making
it very easy to port these libraries
from other languages like Java and kind
of put them into rust and and implement
them there as well so inside of
crossbeam will find these work seedling
decks that Ryan was using will find MP
MC cues for multiple consumers which is
kind of extending the standard library
aspect as well and a whole bunch of
other things built up on this internally
and so again to kind of wrap up the the
library aspect of a concurrency in rust
though the key thing here is everything
we just saw was a hundred percent safe
there's no way to misuse these you
cannot get a data race you cannot get a
segfault no matter what you do this is
code is going to compile it run as you
expected and yes sorry so now I want to
actually shift gears a bit to talk about
features instead so this is more of not
so much the concurrency aspect on the
machine with multiple cores but more so
with asynchronous i/o kind of having
that aspect of lots of high scale
servers lots of various actors there
lots of TCP connections and all that
good stuff and so again we have the the
rust rust the language itself is the one
that's actually fueling the shared
memory parallelism with ownership and
borrowing we have the ecosystem with
kind of all these extra paradigms we
solve mutexes and locks and arcs and
channels and features are kind of the
async i/o story in rust kind of this
aspect of you're not necessarily running
in parallel but you have highly
concurrent with tons of tons of
connected clients that you have to be
managing all at one point in time now
async IO is obviously a very contentious
topic in a very they're very very
exciting space with lots and lots of
development happening all the time and
so I want to focus specifically on rust
story here in terms of what we're
providing and kind of how we're
leveraging features and how we're
actually building all this up internally
and I always like to start with this
graph in terms of this is how we
actually published the the first
iteration of futures
implemented or this is a civil benchmark
of just kind of a hello world HTTP
server so I get slashed and it returns
hello world back to you the y-axis here
is request for a second the x-axis is a
bunch of different frameworks and the
one here on the far left that super-tall
is actually futures written in rust and
so this is what this is showing is that
we've created servers with these
features that have the utmost highest
performance that we can possibly eat
like squeezed out of these servers and
so this is something to keep in mind
where I'll be talking a lot about how
everything is features we have features
all throughout the stack but the key
thing here is that this is not quite the
same as languages as futures you'll find
in other languages they end up being not
quite as costly and not quite as
expensive and end up giving us this
level of performance and this is kind of
showing off the the zero cost
abstractions and kind of though the
possible performance you can get from us
as well so I'm gonna start off by
talking a little bit about just async
i/o itself and so the first thing to
talk about is synchronous i/o kind of
the contrast of async i/o where you'll
eventually you'll tell the colonel I
have a TCP socket I like to read some
data into this buffer and the kernel
will block your thread you will be
prevented from executing anything at
that point in time until some data as
actually received on the socket at which
point the kernel will say all right you
got 4 bytes I filled it in for you and
you can now go and progress with this
and contrast though what async I always
doing is you ask the kernel I'd like you
to fill in this buffer but it
immediately tells you oh that would
block I can't actually do that operation
and you'll have to go and figure out
when to do that later and so this is
ends up being much much more difficult
to actually work with we're now we know
that no i/o ever blocks but we're going
to have to somehow dispatch these events
otherwise we're will have some interface
to the kernel saying ok well I'd like to
block my thread now because I have
nothing else to do and then eventually
the kernel will tell you ok well while
you were waiting I had these 30 events
come in and I have a lot of luck at 5 is
readable suck at 6 is writable you can
these bytes have been transferred kind
of all that good stuff but then you as a
user are now responsible for actually
figuring out where do I put all these
events and how do I actually execute
that and what is it what does this
actually translate to in terms of
executing my code so for example you
have this kind of high-level request you
just want to fetch the contents the
rustling homepage but this is actually
quite involved in terms of what's
happening here you're not only opening a
TCP socket but you're doing name
resolution you might have TLS with
encryption you might have some sort of
compression here your decoding HTTP you
kind of do it all this internally but
all the kernel gives you is this okay
descriptor five is ready now it's up to
you to figure out how to do that and so
this is where previously kind of working
with asynchronous i/o it tends to be
very very difficult very very difficult
to compose do have one library that kind
of works in what other library that
works but you're not really quite sure
how to fit these together and kind of
how to mesh them and so we can we want
what we want here is the way for two
libraries to be entirely independent and
then start working with one another but
all still built on this asynchronous i/o
aspect and so this is where futures come
into play where a future is kind of a
sentinel for some other value being
computed on a separate thread but the
the key part about a future is it's kind
of like an object or an object-oriented
aspect to this where it internally is
capturing everything necessary to
actually compute that future so you know
that you have a future of a string or a
list of bytes but you have no idea how
it's being computer that's kind of
abstract from you at that point like
internally it knows how it's being
produced how it's being executed
asynchronously but you as the consumer
just know that at some point you're
going to get a list of bytes and you're
you're gonna get a string and this
primarily allows us to start actually
doing this composition that we wanted to
do a future for example we can say when
that's done I'd like to run this I'd
like to sequence some computations just
kind of run things one after another we
can also say I'd like to execute these
two things in parallel and wait for them
to wait for them to both finish or maybe
I'd like to wait for one and not the
other and so this is very difficult to
do sometimes where I have this
high-level concept where I want to fetch
some home page like wrestling org but I
want to give it a timeout I just want to
very quickly throw on some timer on the
on the side of that and what features is
doing is giving us this level of
composition giving us this kind of
interface where it's actually almost
trivial to do those high-level
operations and so this means that if you
actually come and say I would like the
wrestling whore homepage instead of kind
of these weird arcane things what we're
getting out we get alright here's a
future of a list of bytes we're
internally this is doing the DNS the
name resolution the TCP connections they
encryption kind of everything internally
here is now captured inside of that one
feature and you can now kind of just
sequence extra data onto this you can
say okay well now that I have this
object that
represents this computation I can
continue interposing that or composing
that with with other operations all
right so this is a bit of an this is an
example of what it actually looks like
with using futures without i/o kind of
but before we've actually touched TCP or
D DNS or anything like that so first
thing we'll have here is we'll just have
some sort of thread pool and we'll spawn
some computation onto that or we're
gonna say just go copy the hundred
Fibonacci number this result though is
an actual future to an integer so this
result is kind of representing that an
integer will eventually come here but
internally it's just happening
concurrently and then that'll get
resolved once that computation is
actually finished on that remote thread
and the meantime though this immediately
returns so the number is actually being
computed on some remote through it so we
can do some other aspects we can get a
coffee or do or do whatever but then
eventually we can actually come down and
say I'd like to wait on this I'd like to
actually block and say please give me
the value inside of this future and then
we'll do the necessary synchronization
to say okay now I'm going to wait for
that thread to finish and or if it's art
there's gonna peel it out for you and
then once you actually have it this
result is now an integer and you can
start working with it and doing whatever
you like with that and so the the
interesting part come here comes here
now with how do we actually deal with
futures and i/o so we have all these TCP
objects this I think is DNS stuff we
want to work with we also want to
package it up in features and rust this
is done with a library called Tokyo
which is kind of pulling together these
two existing libraries in rust called
mio and then the futures crate itself
where mio is you don't have to worry so
much about the internals of it is but
it's effective to say that it's like
cross-platform async IO library or for
example on Linux you have a poll you
have this this one interface to actually
talk to the kernel and then get events
out of it but on OSX it's KQ which is
slightly different and then on Windows
it's iocp which is entirely different
and so what mio is doing is providing
this small shim which looks like eople
and as otherwise a small shim as it can
be in other platforms it's kind of where
all the platform specific logic goes and
then features is kind of basically
saying that okay now that we actually
have all these underlying I hope
relatives we can start building futures
with TCP
it's an UDP sockets and kind of all and
all that good business
so Tokyo is kind of this package giving
you fuel and giving you all these i/o
permits to build up these features and
kind of have that all be an internal
logic to implement those features
themselves because it's using me it
works across all major platforms doesn't
have to worry about iocp versus a poll
or anything like that and this is rusts
implementation of an event loop which is
actually blocking the thread what's
dispatching all these events and kind of
what's doing all that internally the
futures crate today has a number of
various abstractions inside of it so we
have a future which represents kind of
one value becoming available we have a
stream trait which means that multiple
values are coming over time this is
similar to kind of our excuse or kind of
reactive programming in Java or
JavaScript in a sense streams are very
pull based so we have sinks which is the
dual push basing push data into them but
the key thing here is that futures gives
you a nice toolkit when you're working
with it as well so you have these
one-shot channels for I just have some
computation on another thread and I want
to make a future to complete that and
push it over here we have channels as
well so you can have a stream of values
being produced over time and then rayon
like I was showing earlier not only
gives you data concurrency but also
gives you these future aspects as well
where you can spawn some work onto a
thread pool and say I'd like a future to
the result you can start composing that
and doing all that internally so with
features you'll find kind of a nice
library integration and kind of a nice
set of tools to immediately get off the
ground running and another important
aspect of features which you those of
you coming from JavaScript or C sharp
note or Python it was absolutely vital
to working with with rust or working
with features which is async syntax a
single wave so in rust we have this
async attribute saying this function is
actually returning a future it's not
returning a result but it's kind of
internally being transformed to a state
machine that is going to actually
compile down and kind of into one nice
feature being returned internally we can
use this a weight macro which is saying
that I would like to block on the value
of this feature but not actually block
the thread just block my own personal
feature itself and kind of men manages
all the concurrency there for you we
have early returns where you can just
immediately return from a function you
don't have to kind of have any extra
fluff around that and then we also have
handling of async streams or you can
kind of iterate over time over every
value over-over every value coming on
the stream and collect all the data and
the vector collect all the data from
this HTTP request and kind of package
all that up and so this is kind of a key
aspect of making futures based
programming kind of very approachable
very nice to actually use and in rust
itself where if you're only working the
futures you tend to have callbacks or
Combinator's that get kind of unwieldy
pretty quickly and so this is a nice
aspect of making this much more
approachable and much easier to read
much easy to write and maintain over
time the Tokyo crate that I've been
talking about has a number of primitives
as well it not only has it has a some
organization internally and then but
effectively what's what he gives you is
kind of all these bare printers that you
would expect of TCP UDP named pipes
processes signals and a number of
protocols as well such as HTTP HTTP to
WebSockets and all that so the key thing
here is the like you have a nice package
to get up and ground running pretty
quickly and we've seen this deployed in
production and a couple of companies and
kind of using it all internally and
using it to a lot of benefit so that's
mostly what I want to talk about about
Tokyo and futures itself that kind of a
high layer but I want to switch now
switch gears a bit to actually how we
implement these features and how the how
this is all actually happening under the
hood and so here I want to try and build
up this concept of a future from scratch
in rust I was showing you earlier that
we have this very very tall bar showing
that this entire stack is indeed quite
fast but I want to show you a deep dive
into how features work how they ended up
being designed and kind of why some of
the classic pitfalls of other languages
we managed to avoid those as well so to
start off first thing I might try is
making us structs this is just kind of a
struct feature with some generic type
parameter that we're going to get with
it but the key thing here is that as we
said structs we have now said it's kind
like a class in C++
this is the one implementation of
features that will ever have which
that's not necessarily something we can
always declare this might be a thread
safe imitation and we might not need
thread safety this might be some kind of
coordinated implementation that
allocates memory but I'm not know I
might I might not even need to allocate
memory and so having just one
implementation of a future is actually
not going to cut it we need some extra
flexibility here X
flexibility here to say oh well I know
how to if want a future for my very very
specialized scenario and so instead
we're gonna use what in rust we call a
treat a treat is similar to an interface
in Java or kind of like a type class in
Haskell if you're familiar with that but
it suffice to say that it's kind of a
collection of types that all implement a
common interface and so what this is
going to allow us to do is implement a
future for any number of types that are
all having their own internal
implementation we just have to say
what's available we have to actually
fill out this treat or first thing we'll
have is just some item there saying this
is the actual type that we have that
we're actually going to resolve to but
then what are we actually going to put
as a method here in terms of how we
actually implement this future and you
think about it a feature what I was
saying is a sentinel for a value being
computed at some point later in time and
so the first thing we tend to think of
and actually this is how it's
implemented in most languages is to have
some sort of callback based solution
where this is what this function is
saying is I'll have a function called
schedule
it'll take the receiver of the actual
future itself and then a callback that's
this FN once business well the callback
receives this T which is the actual item
being produced in the future so here
we're basically saying when the future
is done run this callback the problem
with this though is we have this kind of
bracket F in this bare self which I'm
not gonna go into too many details here
but it basically means we can't do
virtual dispatch we cannot erase the
type so we have to always kind of use
this as a bare value and there's nothing
to really abstract over multiple
different kinds of features and so
sometimes it's not always the or you
don't always need virtual dispatch but I
want to make a brief digression to kind
of explain why virtual dispatch is so
important here in the Contin the context
of futures so this is an example
function where we'll just say we have
some computation we're gonna cache we
have some key and then if our cache has
it we're gonna immediately return that
saying that we are now done with that
but if it's not in the cache then we'll
go in compute it's very very slowly and
then it'll actually fill in the cache
later and we're turning some different
feature but the problem is that this
doesn't actually compile we have one
branch of the if statement returning one
type of feature and now we have the
other branch of the estate Minh tree
turning a different type of future and
so in rusty to have a well type program
you have to return the same type in all
branches and so the way to solve this we
might think is okay well let's just add
what we call an enum
and rusts we're an enum is like a tag
Union and C or a tag goon in C++ and if
actually just says you have multiple
variants of kind of one type or we can
say okay well the left-hand side of the
returning the a variant or the left-hand
variant the right-hand side is returning
the B variant on the right-hand variant
and then we have this kind of
implementation of a future for this
either type which kind of just
dispatches internally and so this will
compile and this will work and this
actually works for this one particularly
use case but the problem is we might
actually add some extra code here might
add some more if so it might adds a
whole bunch if there and it's kind of
unclear how scalable this kind of adding
an enum solution is going to be kind of
adding the static dispatch aspect where
did we get to a a do we get to zze it's
kind of unclear where this stops and so
what we really need here and kind of
what we really want is this at this
notion of virtual dispatch where in rust
this is done with this box aspect we're
so box here stands for heap allocated so
a box of T's just a heap allocated
version of T and so this box of future
means that it's a heap allocated feature
that is virtually dispatched so we knew
we have erased the type we no longer
know what was actually underneath there
but we can know that we can call it
through some virtual dispatch and then
go and actually use the feature itself
and so this is kind of an example to
show how important virtual dispatch is
in in working with features and why we
need to enable this in the trade itself
and kind of why we need to be catering
to this use case so what we can do is
give them this trait definition we can
tweak it a little bit saying okay well
let's remove this little F that's remove
this bear self which I'm not gonna go
into details of why it's not safe for
virtual dispatch but sufficient to say
that this aspect is where we have this
ampersand mute and we have this box
which kind of is a allocated closure on
the heap well it's that's a very
important aspect we're now instead of
having just kind of a bear closure in
memory we now we only are compatible
with closures allocated on the heap now
that's a not immediately obvious as to
why it might not be desirable so what I
want to talk about here is making
another digression of how we expect to
see servers built with futures and kind
of how we expect futures to be used in
the ecosystem and so the idea here is
very similar to finagle and that
in Scala that kind of Twitter has
produced well the idea is that every
server is a function from a request to a
future of a response kind of this
asynchronous function here and all of
your logic is gonna go internally and so
you might have a request that's kind of
you receive a request and after that you
load some information from a database
you'll do some are pcs you do some more
database requests and finally you will
render a response internally what's
happening here is that a bunch of these
aspects each of these states are
features so loading from a database will
take some time so that has to be a
future and RPC take some time so it has
to be a future and so we'll take a look
at this from kind of a state machine
diagram what kind of a state transition
diagram well we'll start with a get
slash well then we'll do some sequel
queries so moxtra are pcs and whatnot
internally what's happening is we were
boxing all this up literally boxing on
the heap but kind of like putting this
wrap around this saying this is our
future that we are returning kind of our
server is entirely representing kind of
internal processing is represented by
all of these happening together so
internally in the one feature that we're
returning is kind of this is how it's
working internally this is how it's
actually being executed now we're to
actually implement this what's happening
is we're using this schedule function
saying that when a future is done we're
going to execute some more data later on
so for example once we have the original
request we're going to issue the
database loading we're gonna say
schedule when the database is finished I
want to use that to start actually
issuing the RPC now when the RPC is
finished I'd like to schedule again to
go move to the next state and so on and
so forth and the key thing here is that
between all these state transitions is
where we're executing the schedule
function kind of the schedule primitive
is being used to transition between
states of the future in this case we
have five states but internally we kind
of have even more states possibilities
so our server itself is kind of one
giant feature that we're returning
internally that it has many many
features inside of it and then
internally of those and kind of
externally as well we have tons and tons
of state transitions which kind of means
very very quickly the number of state
transitions that we are accumulating is
very very large and the key thing here
is the schedule function we have that
box parameter which means that every
single state transition is now an
allocation we have to allocate some
callback
to say that progress between states this
is how we actually execute that and so
overall this ends up being very very
costly and kind of having a not quite
the runtime performance we will we would
like and so originally when we had that
very tall graph it was much much smaller
when we were kind of doing all this
allocation and internally and so that
was kind of the primary thing we wanted
to solve but there's this other
threading related aspect which is a
little bit more subtle and not always
readily apparent when you talk about
futures and have you have these
callbacks and the thing to note here is
that we have this closure here which we
did not say whether it was send or sync
which in rust is a way of saying we
don't know whether it's sending all
across threads or we don't know if we
can invoke it multiple aeon are kind of
concurrently on multiple threads at a
time and this is very important for
futures in the sense that we don't
actually know can we how are we going to
execute a feature where the result is
being computed on one thread and I'm
actually consuming it on a different
thread so we can actually only do that
if we add in the send bound here you're
saying this closure can be sent across
threads but then that incurs even more
costs where what if we didn't have that
what if we only had one thread by
requiring this one closure to always be
sending all across threads now we're in
this conundrum where now we have extra
synchronization we otherwise wouldn't
need it and so dealing with this ends up
being very very difficult in terms of
how we make features thread safe how we
make them appropriately non thread safe
in a sense for this single threaded
scenarios to not have too much overhead
and to and it cost associated with them
and so for all these reasons this is why
we ended up not actually going along
with callbacks not only are they're far
too slow but they also have these
drawbacks of with the threading scenario
we have these issues where they're too
fast they're too slow or they just we
can never strike the right balance there
so we're back to the drawing board and
it's helpful now at this point to
actually enumerate what we've gone over
so far we're first thing we saw is
features have to be a treat we have to
say that there are many implementations
of a future we can't suffice by saying
there's only one and everyone's got it
use it similar though once we have a
tree we kind of have this interface it
must support virtual dispatch it must
support the ability to erase the types
to some point where you have no idea
what's underneath that box what's
underneath that feature it's kind of
anything internally but similarly we
notice that they're the kind of the way
we envision building these features
there are many many state transitions
happening within any one server and so
that aspect of this needs to be very
very cheap namely it cannot evolve
involve any allocations are kind of
moving all those data across these
threads and then finally this thread
safe aspect is one we would also love to
solve as well saying that we don't want
it to require any extra gymnastics in
the non threaded scenario but also still
work in the threaded scenario to make
sure that actually can work on a server
like that so to kind of make one final
digression before we get to the actual
solution which is to take another look
at what's happening here and see if we
can kind of look at this architecture
and extract some commonalities for kind
of how we might be threading this needle
so we have our future of response here
work on this one nice package of what
we're returning from our server kind of
how we're actually implementing that
process function but we can zoom out a
little bit even further in saying that
there's actually more happening in the
server other than what's happening
internally here where we have we're
actually reading bytes for a TCP
connection and we're decrypting now or
decrypting or decoding HTTP and then
doing the opposite on the other end and
so all of these aspects are kind of
happening in addition to what we are
running inside of our server itself but
then this is kind of where it ends we
can say that for for example one
connected TCP client this is kind of
literally everything they're doing it's
nicely all packaged up and so we call
that a task we can say okay we have this
nice boundary we have this nice
delineation so we'll just give that a
name and we'll see what we see what we
can do with that and it turns out server
we can actually zoom out even a little
farther and say that while server
actually has tons of tasks executing
concurrently for many connected clients
from any connected various aspects of
the server and so we've seen that a task
that we're talking about is composed
internally of many many features kind of
i/o based features or two server based
features that you have landed yourself
and then the key thing though is that
all these features kind of as that are
manufactured and destroyed over time
over the lifetime of that one TCP
connection was all connected to the same
task this task we're gonna want to be
the actual unit of concurrency kind of
like a green thread in a sense but not
in the same implementation aspect just
kind of in the semantic aspect if we
have these kind of lightweight threads
lightweight units that are being
executed concurrently and especially
with a single weight syntax set account
looks
but what I'll do is stack something like
that so we'll come back here to say all
right well our call back based solution
on this trait didn't work so how are we
actually going to do this and it turns
out the next thing we might think of is
okay well instead of saying when you're
done do this what if I ask you just are
you done yet
say have this function called pole where
this says we still have a trait so it's
still nice and we can implement it for a
bunch of types and then this signature
I'll just suffice to say it does support
virtual dispatch so we don't have to
worry about that we don't have to put
that type parameter for anything but
then the key thing here is happening is
the actual protocol around what's
actually being returned so there's this
option type that was returned it can
represent either none with nothing else
or some with one particular value like
that item in the future so we'll say
that a none value says that a feature is
not ready yet
we're not ready to make progress and so
you're gonna have to come back at a
later date if you receive some then okay
we ran our resolve future here's the
item and you can't use the feature
anymore
but the key thing is that if we see none
we need to know when to come back again
and try again so we know that this
feature is not ready yet but we don't we
need to know precisely when it will be
ready because we don't want to just sit
there polling it as that's just a bunch
of busy waiting and not actually going
to make an efficient server so we ended
up building this kind of layered
protocol kind of like con implicit
contract around this poll function
saying that we know that futures are
owned by tasks and we know that this one
task is persistent for the lifetime of
kind of everything in this one unit so
what we can say is that the task is the
one that needs to be realized that
actually needs to realize when the
future is ready to go the task is then
going to accommodate that it's actually
going to pull it and continue to make
progress so not only does none mean I'm
not ready but it also has this implicit
contract saying that I'm going to notify
you when I otherwise would be ready so
that's kind of like the the magic of how
this poll function works and how this
kind of simple signature ends up
building up these really quite efficient
features but to show you an example I
want to dive into kind of how we would
implement the poll function for a simple
feature like a timeout where this is a
future where it's just going to wait for
some period of time it's kinda like a
sleep but it's like an
synchronous sleep or doesn't literally
block the thread it just takes some time
to actually resolve so we'll say there's
two fields here the first of which is
just one our timeout is gonna fire this
instant is just a point in time saying
that this is the point at which point
before which we will not be resolved and
after which we will have become resolved
and you can just pull a unit value out
of this teacher this timer is just some
fictitious library support we're not
gonna worry so much about that here but
it's basically gives us the ability to
run something at a later date so we can
just say I would like to run this piece
of code at this particular point in time
and you can just assume this is managed
and turtley and kind of have some
separate thread maybe but as efficiently
a flooded so we'll start off by saying
we'll implement the future trait for our
time out type the type parameter here or
the type here the item is just a unit
we're don't actually gonna we're not
going to pull anything out of this
feature it's just going to become
resolved it's just going to say I am
resolved or we won't actually get any
data associated with that well start by
filling in this implementation by saying
okay if we have a lot like if the
timeout has elapsed like if we were
actually done now at this point then we
will return some saying okay the future
has resolved we have waited an
appropriate amount of time we're ready
to go
but the trickiness comes here in terms
of how do we actually implement this if
we're not ready yet and we need to
somehow return none and so remember we
have to make sure that the current task
is ready to receive a notification when
it otherwise would be ready so we'll do
that with this task current and this
task notify function start off by saying
ok well pull the current task looking at
the futures crate and then we'll use our
library support to say when the timeout
is ready we're going to send a note of
notification saying that now we are
ready so the key thing here is that this
none value means that I am now going to
actually notify you when my future would
otherwise be ready which in this case
means for our timeout structure when the
timeout has elapsed and so we're just
saying run that notify function one the
time it has elapsed after I period in
time sorry
taking back I look again at the
constraints we set out for ourselves the
first thing the first two things we said
was this has to be a treat and this has
to be virtually dispatch Abul so
naturally what we what we've can
converge on at this point is indeed a
treat does indeed support virtual
dispatch so these two these two
constraints are nice and solved at this
point now we've also been saying though
that this needs to be very cheap this
needs to have
cheap state transitions very cheap
transitions between all the aspects the
features here and that was primarily
driven with these tasks where every time
we look at it the state transition is
governed by notifying that task by
figuring out how to manage that task
itself it turns out that acquisition of
the current task is actually very easy
to do it tends to just bump a reference
count I kind of income at the reference
kind of that arc and notification is
very similar as well it's just kind of
in queueing a piece of data and kind of
putting it on some separate thread to
execute for a while so namely these two
operations are indeed very cheap we're
kind of leveraging that persistent data
kind of how it's been the task is always
tracking that feature any one point in
time and so sure enough the state
transitions here are now an order of
magnitude cheaper than they were before
as we could see with the giant graph
that earlier now the final thing we
wanted to solve here was this thread
safety aspect where we wanted it to be
thread safe when necessary but not
actually incur any costs when it was not
thread safe and so we can do that by
saying that the tasks themselves which
are routing all those notifications are
indeed send able across threads that's
the only thing the features themselves
never never need to be send of all they
can just be routing notifications from
other remote threads so indeed we have
that we solve these constraints the
future trait itself has no unnecessary
synchronization or kind of no extra
synchronization layered on and then the
tasks are indeed allowing you to kind of
source notifications and actually
execute this on multiple threads
concurrently all right that's kind of a
brief ish overview of how futures in
rust are working kind of how they're
implemented underneath the hood but the
last thing that I want to talk about is
how we're actually routing these
notifications into futures themselves so
how we're how Tokyo is implemented in
the sense of we have this e pole system
has this is EP system and we have all
these features that have all these test
notifications well we need to match
these up to make sure the program is
actually making progress
so in Tokyo literally everything as a
future from top to bottom you'll find
everything in the stack is a feature
from that reading TCP bytes all the way
to writing TCP bytes everything there
was powered by features all the way up
and down which we've empowered by having
making these state transitions making
these common operations between features
so cheap we don't have this typical
overhead you see from futures and other
languages and so we
can ever leveraging this abstraction
across the entire stack if you choose
however they will have to say okay I
need to actually wait for some IO I need
to wait for some TCP sockets I need to
do to wait for that until Kia's gonna
figure out how to actually route all
that internally so inside of Tokyo we're
gonna have the same aspect of async with
that we saw originally that kind of the
system fundamentally gives us where well
try and read some data and Tokyo will
immediately say oh that's gonna that's
gonna block and so I'm not actually
gonna do anything at that point but like
with the future trait where you poll and
you saw you're not ready yet there's
this implicit contract here as well
saying that if you tried to read some
data and you're not ready yet you've
implicitly registered your global tasks
to be routed to to get notified when you
otherwise would receive some data so
it's very similar to the poll protocol
only it's kind of i/o based as well I'm
kind of looking into the i/o system
saying when you do i/o and it's not
ready I'll be notified when otherwise
would be ready that's kind of
effectively how a poll works where you
get a notification saying you're slack
it's not readable and then you'll get a
notification layer saying that oh now it
is readable and you can continue to work
with that so we are representing these
two things with traits we have the async
read and the async write trait and this
colon syntax here means super trait and
theirs inherits from and so the async
read trait uses the read trait from the
standard library internally saying that
all it basically gives you is the
ability to read into a buffer from an
object right just says write that buffer
into yourself and so these are
effectively though just marker traits
just kind of a static distinction saying
that this i/o object is futures aware it
has this implicit protocol where it's
non-blocking and when it would block
it's going to register your test to
receive some sort of a notification that
it hasn't but it is ready to go after
that so an example of how you implement
this is kind of how this is under the
hood and the Tokyo crates itself where
first up we say that we will actually
check to see if we're readable if we're
not readable then we can just return
immediately again that's internally
actually registering it right
registering interest but well actually
reads once we've read read some data if
the kernel says it's not readable then
we're gonna say okay let's actually
block our tasks on this and say that we
need to we need to wait or we need to as
part of a protocol because we're
returning not ready actually register
ourselves to receive
vacation at a later date so this
basically means that now I have summed
all this up where this original thing
that the colonel was giving us which we
had no idea what to do with Tokyo now
says precisely oh I know exactly how to
match that up to a task because that
previous task read from file descriptor
four or five and said that it was not
readable and snow now I know precisely
what to do
given these notifications from the
colonel and how to write all right all
that internal aim so to wrap all this up
took use event loop is effectively the
actual thing that's responsible for now
blocking your thread the Tokyo event
loop will kind of wait for all those
events and then dispatch all of them
internally and make progress in all the
various features and whatnot and then
the overall it's kind of like a
glorified translating Colonel Colonel
notifications to task notifications in
the future system all right
that is the concurrency aspect of Russ
today both the shared memory in terms of
parallelism and kind of multiple cores
and whatnot as well as well as the async
aspect of kind of having concurrent
tasks soon on one particular core
there's a few links here the book is
probably the best place to get started
if you're brand new to rust though we
have users forum just for asking
questions and whatnot and if you're more
curious about the async i/o stack you
might want to start there as kind of the
the current library for futures
otherwise thank you so much for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>