<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>V8 Today &amp; In The Future | Coder Coacher - Coaching Coders</title><meta content="V8 Today &amp; In The Future - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>V8 Today &amp; In The Future</b></h2><h5 class="post__date">2018-01-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HDuSEbLWyOY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Thomas Nanosat I'm the
product manager for the v8 team on
Chrome as well as web assembly and today
I'm happy to be here to share with you a
little bit about what the v8 team has
been working on over the last year and
some of the things that we plan to
tackle next as well as to give you some
recommendations about how to write
JavaScript and so v8 for those of you
who don't know is the JavaScript engine
that powers all of the execution of
JavaScript not just in chrome but in
other places such as nodejs
to briefly provide a sense of scale
every single day v8 spends more than
31-thousand years of time executing
JavaScript written by developers such as
yourself this means that when the v8
team makes even just a 1% improvement to
the poor performance of the engine it
translates into more than 300 hours of
saved user time every single day most
importantly though we've just recently
gotten a new Twitter account at v8 Jas
and I encourage anybody who hasn't
already followed to do so so to start
off I want to introduce v8s mission
statement which is to speed up
real-world performance for modern
JavaScript and enable developers to
build a faster future web and to start
off this discussion I want to focus on
one seemingly innocuous word within this
mission statement which is real world
and the real world web is the one that
each and every one of us uses every
single day
but that also means that the real world
web is different for everybody and part
of the job on the v8 team is to make
sure that the investments and
improvements that we make into the
engine actually translates into real
benefits for end users and so towards
that end I want to talk to you about our
work on real-world benchmarking and
start off with a very brief history of
benchmarking when modern JavaScript
engines were first getting off the
ground we were able to utilize what are
called micro benchmarks which
essentially tested each language feature
in isolation this was great in order to
be able to pick the lower hanging fruit
of engine optimization but as engines
became more and more performant we had
to make various trade-offs in the
performance of those
engines and it became much more
important that we had a more holistic
and representative sample of the
JavaScript that people were executing
and toward that end we started utilizing
what we call static test Suites which
are essentially collected workloads of
some JavaScript performance that you can
then execute your JavaScript engine on
and get back results to give an update
on two very useful static test Suites
that we pay a lot of attention to on the
v8 team is speedometer and airy six now
speedometer is a benchmark that actually
implements a real test to do MVC app
written in a variety of frameworks such
as angular backbone ember and others and
then basically just runs that app
through its paces
adding to-do items and removing them and
on this one I'm happy to say that we are
22% faster since just this last time a
year in addition to this benchmark we
also pay attention to one called Ares
six Ares six aims to be more
future-looking and tests new es2015
features in beyond such as classes for
of as well as testing more
future-looking workloads of JavaScript
we've only been tracking this benchmark
for about a half year but in that time
we've already managed to improve it by
40% so really great results from the v8
team there to return to our timeline
static test Suites have been really
useful and they've been great at
empowering the team to optimize our
engine but we think that we can still do
better ultimately returning to our
mission statement we care about that
real world aspect and what better
representation for the real world is
there than actual real world web pages
and so on the v8 team we actually have a
set of real pages that we use you heard
these briefly mentioned in the keynote
we call them the top 25 and these are
actually snapshots of some of the most
popular pages out there on the web with
some diversity thrown in for size scale
and functionality and this list includes
very useful sites which is LinkedIn
Instagram and most importantly Taylor
Swift's Twitter page and on these top 25
sites I'm happy to say that we've
improved by as much as 5% which when you
take it across the scale that these web
pages get translates into massive user
savings
in addition to those loading stories of
loading these real-world webpages we
actually also have interaction stories
and so to highlight just one of those oh
sorry this is actually reddit comm which
is particularly close to my heart where
we were able to speed it up by as much
as 10% but like I was saying we also
have real-world user interaction stories
and one of those is image or media
browsing where we actually interact with
the media on imager and then benchmark
against that real-world scenario and
then that one we've gotten 24 percent
faster so I want to return to the
mission statement and have a look at a
second piece of that mission statement
which is performance and specifically
around performance I want to discuss the
work that the team has been doing in
order to speed up the core of the v8
engine and to start off this discussion
I want to introduce an enemy of all of
us which is jank
we've all experienced jank but to
describe a little bit of what it is
let's have a look at this nice swinging
pendulum ball
courtesy of Lim Clark and then compare
that nice smooth swinging ball to this
horrible atrocity where they've taken
where we've taken out just a single
frame of that pendulum and you can
really see that stutter on the right
bring it right swing it's extremely
infuriating and we've been working hard
to get rid of it and one of the things
that causes jank on webpages is when the
garbage collector hash has to kick in
specifically the main thread that's
responsible for doing all of the
painting of content to the screen is
responsible for doing a lot of things
including executing the JavaScript and
things such as garbage collection and so
in this example what you see is memory
grows over time until suddenly the
garbage collection system has to kick in
and actually collect the garbage and
this can cause potential frame skipping
that you see there along the bottom in
order to combat this we started what we
call project Orinoco which is a mostly
parallel and concurrent garbage
collection system and to explain what
that actually means let's return to our
example and explore the first phase of
project Orinoco which was to actually
split up that very large chunk of
garbage collection that you saw into
smaller pieces so that we can better
utilize the idle time that exists
between
frames in order to do the garbage
collection then and prevent as much
large blocks of garbage collection
causing frame skipping but on systems
that have very little idle time in
general this can still occasionally
cause frame skipping and so we've now
taken project Orinoco to the next phase
where we actually started moving garbage
collection off the main thread and so we
still leave some small slivers on the
main thread for garbage collection but
we're actually able to take the majority
of the garbage collection off the main
thread in order to better utilize the
multiple cores that modern machines ship
with this has had measurable impact and
I'm happy to say that we now do as much
as fifty-six percent less garbage
collection on the main thread to take a
step back for our next piece of this
discussion I want to go over to
classification of trade-offs that we
often have to consider when designing
our engine the first of this is the
trade-off between being able to get a
fast start up and going getting going
with your JavaScript execution quickly
versus having that longer term peak
maximize performance a second trade-off
is between having low memory and keeping
a low memory footprint as opposed to
optimized optimizing your functions
using all of the available memory so you
might ask can I just have both well
luckily with v8s new architecture that's
exactly what you get and so v8 has just
recently completed our transition from
our old architecture to our new
architecture and to go into very briefly
some of these pieces we have v8 ignition
which is an interpreter making it
extremely fast is getting going with
some amount of execution very quickly as
well as taking out a very low memory
footprint this makes it perfect for
functions and code that you execute just
once or twice and then in addition to
ignition we have v8 turbo fan which is
our optimizing compiler and the job of
our optimizing compiler is to take the
functions that get executed multiple
times and utilize the free time that we
have after that startup and to utilize
the available memory that we have to
really just optimize these function
maximally and generate that max optimize
performance that's necessary for really
great longer-term experiences and to
explain what a monumental shift this is
B
for the team I want to very briefly go
over the RER Kotecha that happened and
so here you see a diagram where we start
with our source file we put it through
our Jas parser and then in our old
architecture we would put it into what
we call full chip code gen which would
then generate unoptimized code we would
then want to optimize it with our old
optimizing compiler that we call
crankshaft but in order to do that we
actually have to go all the way back to
the JavaScript source file reparse it
and then put the result of that into
crankshaft with our new ignition and
turbofan architecture however we're able
to take the results of ignition which
generates an intermediary bytecode
format and then take that bytecode
format and directly optimize it through
turbofan without having to reparse the
file and I want to show off this commit
because it's one of my favorites it's
the commit where we removed all the
hundred and thirty thousand lines of old
architecture code and replaced it with
just two to connect the new plumbing
hopes really you know we were like all
holding our breaths like is it going to
work and and it did so we were very
happy and one of the big advantages of
this new architecture is that we're
really taking a new approach and how
we're being prescriptive about how you
should be writing your JavaScript in the
past we had various patterns and anti
patterns that we recommended that you
follow if you want on your JavaScript to
run performant ly but that's really kind
of a sad way to have a language because
you're basically saying here are all
these amazing features but don't use any
of them if you actually want your code
to run well and so with this new
architecture we're now finally able to
say go out there write the idiomatic
JavaScript that makes the most sense to
all of you for your application and then
trust that the v8 team will pay
attention to how you're writing it and
will optimize it accordingly and so
we've really gone from this list of
don'ts like don't use leptons don't use
try-catch just saying please do use all
of the functions available to you in the
language to write the idiomatic
JavaScript that makes the most sense to
you to highlight just one of these
examples to explain here we have in in
the older versions of Chrome you had
various patterns around try-catch where
we didn't really recommend that you use
it but there's this weird function
wrapping thing that you could do that
you see here on the left in order to
actually get a 27%
provement over the more idiomatic
version that you see there on the right
and I'm happy to say that in chrome 60
both of these implementations now run
equally fast and in fact both of them
runs substantially faster there than
they would have in for example chrome 51
so to once again return to our mission
statement I want a next focus on another
word and that's modern and for the v8
engine modern means speeding up the
bleeding edge of JavaScript and we do
this by actively participating in tc39
which the standards body responsible for
ECMO script or ES and then we also put
additional work into optimizing the
language features that come out of that
process and we actually track all of
these features and track their speed and
we've gotten very fond of graphs such as
this one here where you see kind of you
know the features going along and it's
executing and around 500 milliseconds
and then we're able to make some
performance improvements and we get it
down to just 200 for the same workload
additionally you have graphs like this
one where we kind of go up for a few
commits and then boom we're able to land
the optimization and really improve
things and then here's my personal
favorite where we were able to take a
workload this is template string tag
that took 250 milliseconds and reduced
that to just around 11 12 milliseconds
now I'll be the first to admit that not
every single graph for these features
look like these but the hope is that
over time the VA team will be able to
tackle more and more features and make
more and more of their graphs look like
this and really speed up these
performance new features we actually
write about a lot of these optimizations
that we work on in our blog which you
can find at v8 project blog.com if
you're interested in some of the
lower-level details of how these
features actually get fast in addition
to the fact that we keep making them
faster
another inherent advantage of es2015
features is their size and so when you
compare it to something that for example
has been transpiled with something like
babel you can actually get a
significantly larger result and so to
highlight just one relative
the extreme example of this here you see
simple eight lines of code defining a
class extending it and setting a symbol
name property on it and then you can see
here what happens when you transpile
that code with babel to generate all of
these lines of code they have some like
variety of checks and stuff in there and
it's really cool that we can do this
transformation but it is ultimately
larger and so to highlight one
real-world example of this this is from
a co-worker of Mines blog where he
actually ran the test of having his
entire blog written in es2015 and then
transpiling it with babel and what he
found was that the overall size in
kilobytes was about half the size with
es2015 features but in addition to that
he also found that the parse time of
that javascript was less than half
saving users time and one last point I
want to make about why it's really great
to use es2015 features where you can
which is a little more subtle and that
point is that when you write less
idiomatic JavaScript it's actually
harder for the engine and for the engine
developers to optimize that code and to
explain that a little bit further here I
want to return to the same example we
were looking at before and as an engine
or as an engine developer you look at
the version there on the left and you
can kind of reason about what it is that
you're trying to get done you're trying
to extend a class instead of Property
understandable but when we look at the
version on the right it's going to be
much much harder for us to figure out
what you're actually altom utley trying
to accomplish and so from the
perspective of a engine developer
transpiling your es2015 code can
actually serve as a way of data loss
that hides the meaning of what you're
actually trying to accomplish and can
ultimately reduce the level of
optimizations that we're able to
accomplish so with all these reasons you
might be starting to think should you
still be transpiling with Babel I know
this question actually came up yesterday
during the leadership panel and Alex
Russell answered it by basically saying
that it depends it depends on your users
and depends on what you're trying to do
I'm in the very fortunate position of
not having to disagree with Alex Russell
as I don't think that would go over well
for me but rather I want to expand on
his answer by actually giving you a
little bit of detail on
how you can actually accomplish these
settings and so the question around
babbling should you still be using it
well in the VA team we actually think
babble is awesome and in fact babble is
so awesome that allows you to relatively
easy can easily configure it to either
transpile es2015 code not transpile or
transpile intelligently and so to show
off just how easy this is I'm gonna try
and do it live in a demo fingers crossed
see what happens I'm gonna go ahead and
sign in and if we could switch the
screens perfect and so here you see a
very simple little test application that
I have set up that's utilizing webpack
and babble you can see here I'm on my
package.json file where I have my
beautiful v8 demo name I know how very
creative and then just a couple of dev
dependencies like babble HTTP server web
pack I have my two scripts that start
web panic an HTTP server
pretty simple stuff index dot HTML
equally simple we basically just load
our bundled javascript file which gets
reproduced from just this single file
which uses the fetch API to fetch fetch
pets JSON and do a little bit of nice
string templating utilizing some newly
es2015 features with that as well as
with these nice arrow functions and then
we basically just added to a Dom element
and in case you wanted to see it this is
our pets dot JSON very simple structure
great so when I actually want to run
this I can go ahead and say NPM run web
pack and that'll go ahead and generate
our files using the configurations that
we have set here and our simple web pack
config and then I'm able to do just NPM
run start which will go ahead and start
our server let's switch over here reload
excellent and so now here we're seeing
those settings that page load with my
adorable little pets and you can see
here that when we go under sources you
can see that we were actually
transpiling now we've removed all of
that kind of nice syntactic sugar and
we're utilizing some try caches
potentially unnecessarily
great so now to show off how easy it is
to actually configure this there's
something called Babel preset n which is
an NPM module that integrates with Babel
very easily and allows you to actually
transpile face not just on the browsers
that you're trying to target but
additionally on some intelligent
settings and so to show just how easy
this is to install I'm going to go ahead
and npm install - - save - dev babble -
presets - n F and it's going to go ahead
and fetch that pakil fingers packet
fingers crossed
excellent and then now we have that here
in my package JSON battle preset n
beautiful and then I just need to change
this one line of code in order to stop
transpiling es2015 and I'm gonna cheat
slightly by copy pasting from this blog
that I have here to save you all the
time of me having to write this out and
we'll go ahead and indent it slightly
accurately okay that's still a mess but
you get the idea basically what this
does is that you give it a set of
browser targets that you're actually
aiming for in this case these are the
ones that support all of the es2015
features and then it'll actually pay
attention to that when you run once
again npm run web pack and this will
then go ahead and actually build these
using these settings and now you can see
when we go in and actually look at our
app dot or a bundled app verse now
seeing these nice features of es2015 and
the smaller code size as well and just
to make sure that we're seeing it
alignment production as well we'll go
ahead and switch back and boom there it
is in the sources graph as well all
right thank you
so that was just one demo highlighting
how you could turn off es2015
transpilation altogether but there are
other options as well and so one of
these is that you can actually give a
Babel preset and different conditions
such as greater 1% last two versions
which will actually check against the
table of supported es2015 functionality
and actually intelligently transpile
only the things that it needs to which
is really great because you can go home
you can set these settings and then as
adoption grows as upgrades rollout
you'll start transpiling less and less
automatically in addition to this though
there's even more that you can do there
is a way that you can have your
transpiled and non-trans pile code
living together and there's an excellent
blog post by one of my colleagues it's
the one that i just stole that piece of
code from you can find it at philip
walton comm slash articles that's the
point yes 2015 code in production today
or you can just google deploying yet
2015 code and it's the first link on
Google Yahoo etc whichever one you care
to use and and this is really great
because it utilizes the module know
module trick that some of you might have
seen in order to actually have the
browser load whichever one it needs
conditionally which is really great so
to return to our mission statement one
last time I promise I want to actually
highlight that last word which is wet
and you'll notice here that we don't say
build a faster future Chrome or even a
faster future browser we say faster
future web very intentionally because we
actually care about the entire holistic
experience and for v8 that means that we
love node and over the last about a year
and a half two years we've actually been
getting a lot more involved with the
node community given the growing
popularity of node it's likely that many
of you out there have used node directly
and even those of you that haven't have
likely used some kind of node vase tool
such as the ones we were just looking at
web pack Babel but also ones like
typescript and we've been engaging a lot
with the community and it's really borne
some great results and so one of the
structural changes that we have made is
which we announced at the keynote is
that you can no longer land a commit in
v8
if it breaks nodejs and so this is
really making nodejs
a first-class citizen within VA which
we're extremely happy about in addition
to that we've actually also been putting
together a bit of an early version of a
benchmarking tool set that actually
benchmarks the tools that developers
such as yourself use every single day it
includes things like typescript babel
web packet the ones that we just looked
at and we recently got this together and
over just the last three versions of
chrome that we've been able to test it
on we've already gotten 38% faster on
that tooling benchmark and that
ultimately means that that every time
that you hit execute run build whichever
and run your build process you're able
to more quickly see the results and
you're able to iterate faster enabling
you to be faster approaching that future
web that we all want to get to in
addition as one last comment of one of
the results of this collaboration we
were able to land that new architecture
that I was discussing earlier in node
8.3 which is going into LCS support long
term support and the results have been
really great we have a very active
Twitter community that has been tweeting
at us with great enthusiastic results
here you can see the dip there on the
bottom right where they were able to
utilize the new version of node with the
new architecture in addition to that you
have people like this one doing some
benchmarking other system deployments
and people overall just being very
enthusiastic and showing their
excitement which we love and in
encourage you all to continue and so
that basically brings me to the end of
my talk and as a product manager I love
giving action items and so if I can give
just two recommendations for all of you
to go home and consider I would
encourage you all to always be writing
the idiomatic JavaScript that makes the
most sense to you don't do patterns and
anti patterns that you might have heard
about before we're doing away with those
additionally for the second action item
I would suggest that you go back and you
seriously consider how much of your
es2015 code you really need to be trans
and whether or not you can start
transpiling
more selectively to gain some of the
awesome benefits of es2015 code that's
the end of my talk very thank you very
much paying attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>