<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python Web Scraping Tools: A Survey | Coder Coacher - Coaching Coders</title><meta content="Python Web Scraping Tools: A Survey - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python Web Scraping Tools: A Survey</b></h2><h5 class="post__date">2018-04-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ind-mugxMxk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">that looks good all right so I'm going
to attempt to do this with links to the
code examples which are on the github
repository so we'll see if the Wi-Fi
makes that smooth or not all the
examples everything shared public I
assume Martin's sending the links out
afterwards or something all right so I'm
going to give a survey of web scraping
tools so what is web scraping simple
process where you're getting a computer
to consume content that's aimed at
people so copying prices off of shopping
websites financial data off of stock
exchange websites you can pull tweets
off Twitter if you wanted all sorts of
things there are lots of legal and other
issues in this space I'm not talking
about any of that all the examples are
either pages I wrote or pages that are
totally fine with you doing this if you
want to do this for fun have a good time
if you want to do this as part of your
job speak to your legal department
seriously it's just very complicated
it's not bad it's just very very
complicated so all right so python has a
tremendous number of packages for web
scraping which cover all kinds of use
cases and Python handles dealing with
text really well so when you grab a blob
of text off a website pythons grade
getting that blob of text off can be a
little bit of a mess and many of the
packages kind of overstate what they can
do so it's also important to say that
web scraping is entirely dominated by
network performance so we're even gonna
see here in some of the examples the
Wi-Fi here or Chrome I suppose are the
thing that's slowing it down so no
package is faster than any other it
doesn't make any prints it spends no
time in Python it spends all of its time
waiting for DNS lookups and JavaScript
to run and all kinds of stuff like that
all right
and then web content so this is what's
going to drive what scraping tools you
need if you're looking at really really
simple so here what I've got in the text
parsing box so this is basically HTML
and it's well-formed HTML I assume most
people here have authored some kind of
web page before and you made mistakes
with where to put the greater than and
less than symbols if you were doing it
by
yes okay I'll take that as a yes so if
it's actually valid and it's just HTML
you can maybe even get away with no
package and just using Python built-ins
if you've got real-world text so down
here in this lower left-hand corner
you're going to need something that
handles invalid text and you get invalid
text on government websites on
e-commerce websites on really simple
websites where you can't believe in a
hundred and fifty characters they have
invalid text but they do okay upper
right hand corner we've got stuff that
includes JavaScript html5 any of the
hundreds of web add-ins that exist on
some browsers and knots and all over the
place you're not going to be able to do
that yourself you're going to need to
use something that processes that
craziness and then you've got in the
bottom right-hand corner 99% of web
pages which is pages that have errors
and use ridiculous add-ins you can't
possibly handle yourself so we've all
experienced constant upgrades in Chrome
and Safari and everything else that all
lives in this corner down here on the
right okay so just a brief delve into
what doesn't doesn't count as
complicated stuff that's not complicated
as stuff you can probably do yourself
that kind of means content from the 90s
so forms you can post and get yourself
people have done forum posting I'm
assuming anything that's got JavaScript
css3 anything where the browser version
matters anything that freezes your
mobile phone anything that crashes your
whatever version you're really not gonna
be able to do yourself
if it's just plain texts lower your own
very thin tools okay
so we're gonna write the simplest
possible scraper I'll bring up the
example HTML here which needs to get
zoomed in a bit is this text large
enough can people see that HTML okay
I can render this if people want but I'm
assuming everybody's good on basic HTML
yes okay so that is a very very simple
web page that's about as simple as you
can get all right so some scraper code
alrighty so we import a couple of
packages here we can come back to what
packages do what so this is the
destination we're getting to I've put
all the stuff up on github as I said
before so this uses the requests package
that's a simple network access getting
stuff package pull the information in
then we're down here parsing so we just
say here's a page assuming it's a valid
page parse it and then here and we're
gonna dig into this a little further
we're writing an XPath which is a way of
describing in the HTML XML blobby text
what stuff we want and then we just very
simply go through run it pull it out and
we're printing it so go back to this
simple page here getting the entry under
strong and then this pulls out the text
again we'll go in a little bit to how
that works in a second should pull out
one two three four five so if we go in
here make our text big enough and three
we run example 1 it should there you go
1 2 3 4 5 so very very simple scraper
simplest possible scraper probably
okay so his XPath language we had buried
in there is a really big heavy
complicated language most of which is
for XML craziness that is not relevant
to normal web scraping but you can still
use it it makes life very simple so
basically your data is in a big tree
tags are nested and you navigate through
tags we've got slashes to separate one
level of tags to the next wildcards
queries so this tag that should look
familiar to people you would be asking
for the image tag with the attribute
source whose value is XYZ like this
pretty straightforward and then if you
want be a tag whose href is some text
you do this you can give probably a
full-day talk on the weirdness in XPath
there will be a couple of further
examples throughout this but it's a
really long deep topic that's not
necessary at this point you want to pull
out the contents of a link using that
very very similar code we're gonna be
looking at this XPath we pull up example
code for that one oh and you know what I
should pull up the second HTML blob so
here we've got an href and a link to
some actually US government website so
if we go back we look at our example
code here everything is exactly the same
except we've got this is our extract
other than that no change and when we
run example two we get back out all the
links from that page so it's not gonna
start it's not the links in that page
forget what my own code does ah sorry
this one grabs the link ah and then we
go we get the link so we go we scrape we
parse it out we retrieve the next page
and then I'm just printing the headers
out cuz the page is actually a big blob
with Java Script and all kinds of stuff
you don't want so you can this is I
guess the minimal not scraper but web
crawler so we've grabbed a link off a
page and we've chased it somewhere else
simple place to start okay one last
little tidbit on XPath this is going to
come up in a moment when we look at some
of the more famous simple Python
scripting packages you can navigate
through this tree using standard
directories sort of dot dot dot slash
type things so this right here find is
stylized you're going up a level from
whatever element you've got grabbing a
div and putting on whatever conditions
you've got here so we can look and this
is some real code pulling from a all
writing so this is a non-trivial webpage
we've got a table here where we're
talking about fruit so we've got a bunch
of types of fruit there's apples and
pears and stuff colors and prices okay
now we go into our code what we're
looking for here I hope this isn't too
much XPath weirdness we're gonna look
for the entry that is the Apple and then
we're gonna go up one over and look for
where the price is up one over and look
for where the color is this is a little
bit contrived because it's a pretty flat
table here but you can imagine on a real
webpage that's got twenty level deep
nested HTML this is a lot better than
searching for incredibly long paths from
scratch every time and they could even
change where the tags go where the ads
are listed headers and this kind of
stuff in general will still work we'll
come back to maintaining scraping code a
bit later so we run that we find the
first element here that we listed the
Apple text and then we can search off of
that element by path for the relative
entries and print them out just as
before this is not a very exciting demo
I think it's example three
nope I'm wrong again it's example 2a and
it's gonna print out the color the color
and the price from our table here so
that was Apple red 5 relatively
straightforward navigation okay mmm
okay now we move on to real webpages
which are not things I wrote and post it
to github myself this afternoon so
there's a well known package called
beautiful soup that many people have
probably heard of and some people may
have used to try to scrape webpages it
is famous because the learning curve is
incredibly short it is basically a to
function API parse and search and that's
fine because you can do really simple
stuff really really fast as we'll see
maybe you start to have a lot of
complexity very quickly because there's
a to function API so if I now look at a
this is a proper US government web page
that is invalid as our surprising number
of government web pages around the world
so if we try to run using the broken
code here so if we look at exactly as we
had before requests get and just parsing
with El XML for this page and this is
example 3 a we're going to get a really
unattractive complaint about how the
stuff isn't valid you can't fix that
because you don't control the webpage so
you need to do something else that's
something else is
something else is yeah run link is to
load up this other package that I seems
many people of use so beautiful soup
we run beautiful soup on our page
instead of running the simple straight
XML parser and now we can run it's only
real method which is find all and then
we can get stuff out of that print or
not again really simple to use I've just
taught you basically the entire API if
you want to do more complicated things
however it starts to get really bad so
this is an example piece of HTML or an
XPath for an example piece of HTML so
this is should be relatively familiar to
people you've run a search on some site
and you're paging page 1 page 2 page 3
whatever out of 10 pages page info so we
can build these little things in Python
that's all pretty straightforward that's
what pythons good at but then trying to
search for this combination of
conditions because beautifulsoup doesn't
have the world's greatest search api we
end up building some ridiculous set of
for loops and if conditions this can
spiral out of control pretty fast or we
can use here find all we can pass in has
functions objects whatever Python we can
pass in a function of that then does
this comparison none of this is
particularly quick right so our simple
to function API just turned into a to
function API plus me writing my own
functions are passing them into their
functions for every case this is not
fantastic we'd be much better off
writing this XPath and just passing it
in ourselves there's no real complexity
in that logical
all righty mmm okay we want to grab
fields out this is gonna look pretty
familiar once you've got your elements
you're searching whatever you'll find
all whatever your tags are you just grab
expand on text that's where your stuff
is
that's where whatever the thing you're
scraping for however you get there for
loops if logic whatever it is and then
because it's Python and really this is
clearly not code anyone should use but
it fits well on a one slide presentation
you can then do what you need to do to
the text you got back out and in this
case we're think building a date/time
object from it I mean don't silly I
don't anybody think I'm recommended
right Matt but it fits on one line so
you know but that's why we're in Python
in the first place because it's really
simple to play around with the text that
we've extracted okay mm-hmm next next
now javaScript beautifulsoup
has no interest in javascript javascript
needs its own interpreter its own stuff
to run and as everyone who's ever used a
web browser knows it can behave poorly
at times okay so now we need a proper
browser driving package so we're gonna
use selenium there are a few that's I
think the most full-featured one that's
around and then we're gonna need to use
a browser we're gonna come to this a
little bit later but your only real
choices for browser or Chrome and
Firefox you can try to use others but
the thing that attaches Chrome Firefox
whatever to Python is an evolving piece
of software that doesn't exist for most
browsers so if for some reason you can't
use Chrome or Firefox you really can't
do most of what I'm going to talk about
from here on out
okay the code is very straightforward
you'll notice this is no longer than
before so we're now bringing in the
selenium package this is all we need to
do to launch Chrome at this point we are
attached to a running chrome that we'll
do within reason whatever we tell it
it's some limitations pass it some URL
get so pretty much the same as it was
difference so here you've got a couple
more searching functions than beautiful
super but not tremendously more so this
is some crazy name for a field that's on
this webpage we can go through chrome
developer mode but I think that's a few
minutes of people's lives we don't need
to spend everybody's used the inspect
element stuff I assume so and then we
just get the element back very very
similar search there's simple similar
versions that take XPath and whatnot
that we can call that clicks on the
button and the browser does whatever you
do when you click on the button so if I
run example five we will see a chrome
pop up load up that webpage so my hands
are not on the computer you can see it
pressing the download button and then it
got unhappy because I didn't fill all
the forms out because to fill the forms
out makes the code longer we'll do that
in a second so everybody saw the button
press and the stuff up here okay so quit
this and we kill that
all right so let's download the file
that that button pressed this is a
little bit more work you have to do
enough stuff to make JavaScript happy to
give you the file and file downloads are
not part of the browser the browser gets
the content and says hmm and throws it
off to the operating system so you end
up having to do some slightly ugly
things here sometimes there's a terrible
while loop while we wait for the file to
show up okay so this code is a little
bit longer so we need to make ourselves
a place to put downloads this is just
stuff you end up copying I guess from my
examples or Stack Overflow because
there's no documentation to tell it
where to put the Downloads okay then you
create your chrome telling it where to
put the downloads and again these are
just strings you have to figure out fine
google locate somewhere and then it's
all very straightforward so we fire it
up as before we get the URL so here
there's some stuff we have to fill out
on the page so I'll go through that as
it clicks but you have to click some
buttons and set some fields this is a
brief brief overview of the selenium API
you can search by X paths this sort of
thing here you can get a select object
so this is how you fill out the value on
a form so it clicks it pops up it enters
this text as though it's the entry that
you clicked on you can scrape the texts
out figure it out and feed them back
yourself if you want the code gets
longer and longer and longer
this grabs a button clicks the button
all this stuff that's needed to get it
ready and then as before we grab and we
click our download button now this is
the unfortunately somewhat ugly part
because the browser as I'm sure you've
noticed downloading files just leaves
you alone man until the file shows up we
end up sitting here in some terrible
while loop waiting for a new file to
appear and for that to be a valid zip
file I not going to apologize because I
didn't actually write Chrome so fair
enough
so if we go ahead and we run if I can
remember example 6
and depending on how fast the Wi-Fi is
okay so we got this all right
okay so we've filled out the form we've
clicked the button on the way I'm
pointing here I guess and then we got a
download going it says five minutes so I
imagine we've just got a there we go so
it's just gonna sit here waiting we're
not gonna wait with it the whole time
believe me it gets there eventually
because I can leave it running for six
minutes all right so at this point we
can navigate JavaScript we can pull down
files we can chase links we can do most
of what it is we need to do once you've
got the browser up and running once
you've used the marginally more
complicated than basic text parsing code
the browser is your parser and the
browser can parse anything you can read
with a web browser cuz that's how you
read all your web pages which browser
matters in different ways than it does
when you're doing this as a person so
you may have a personal preference for
Firefox or Safari or Internet Explorer
or whatever much of the complicated web
content out there interacts with
browsers in different ways so things
that change tag names don't change the
tag names the same way in different
browsers so you end up having to stop it
halfway through use the developer
console inspect to see what they're
called I've come across pages that will
only scrape in Firefox or that'll only
scrape in Chrome
or that won't scrape it either many
scraping both so you really care how
complicated your page is if you're
looking at really basic stuff none of
this actually matters and the second
slide is probably enough if you've got a
really complicated page that's a real
mess it's likely to work at best in one
browser or only be scrape Abul in one
browser people that don't put ID names
and class names or put you know non
unique strings for things that are
supposed to be unique it gets to be
really difficult so again you really
don't care much if your Java scripts
pretty simple as stuff gets worse
things run down and you end up just
having to test as versions change as the
page gets rewritten you end up having to
retest retest retest which is pretty
terrible because no one
tells you when the pages change okay now
as I said before your only real choices
are Chrome and Firefox for doing this
this is not because I care it's because
they're the only ones that have well
supported drivers between the two things
there are a whole bunch of other
browsers out there that sort of you can
hook up to Python for a while-- phantom
Jas was okay it sort of slowed down
there is an opera driver it doesn't
really work there's kind of a driver for
these two but not really and they don't
work and that you know that's sort of
the end of that it has nothing to do
with how they appear you get stuff
that's not visible that's different so
when the HTML changes of the way it
chooses to cache JavaScript values you
can only tell by running it and seeing
what came out the other side none of
this is documented in any way having
spelunked through parts of the chrome
source before that's also a complete
waste of time because you're never going
to figure it out because it's too big
now all the browsing I've done so far
and this that's still running there's a
screen up here we don't need a screen
because the screen isn't actually
helping us because the computer is doing
everything you can run these things
headless you can run Chrome and Firefox
headless some of the others support
variants of headless modes this puts you
in a strange position where you'll be
debugging your code and you'll get this
element isn't visible at this time and
you can't see anything and sometimes
those error messages are not the same as
the ones you get when it's not in
headless mode because none of this is a
commercial product debug by printouts is
quite literally the only thing you can
do because you can't really attach a
debugger to Chrome I did try that once
it doesn't there's no point you're not
gonna manage to do that so I can show
you how to do headless so first this
brings up something a bit this brings up
a fire this is the headless version iron
with head this brings up a Firefox so
it's exactly the same code we had before
except we now call webdriver dot Firefox
that's it
I don't think you could get much easier
if we want to do
headless this I promise you is the same
code except it adds the option
- headless there's a different set of
options you have to add to get chrome to
do this again I mean I'm happy to give
people these things or I guess you can
maybe I'll put up an example for Chrome
so if we run this is example 7a so if I
run examples oh there we go
file appeared I think it's set to sleep
afterwards so I'm gonna kill it anyway
but if I run example 7 should bring up a
Firefox nope that was the head list
sorry so this just - there we go there's
a Firefox one line change half line
change
Firefox loading the same page perfectly
happy I'm gonna kill this because we
don't need it and I can run a headless
Firefox and it's going to go through and
pull out some text from that page
I think it's a date field again it's
doing fine nothing else different here
same ridiculous date logic same fine
element it's just called the different
call the different thing up here
alrighty you can run these things in
docker containers you can run these
things on AWS and Google Cloud sort of
there are problems that you don't really
expect cuz I guess Google's had some way
of doing some kind of headless browsing
for a long time but they don't share
that with people and they don't care
about JavaScript so it's probably not
chrome getting this to run in a docker
container on AWS took some weird
adjusting of docker default flags and
things that you just shouldn't have to
learn how to do because Firefox needs a
lot of shared memory and weirdness that
isn't explained anywhere because it's
just not a common use case but you can't
get it to work the browsers themselves
are very easy to install you get your
docker image to run a bun to apt-get
whatever and everything's happy the
driver glue between the browser and
Python is not I guess I can give people
links you have to go download the gecko
driver for Firefox and manually unzip it
and put it in the right place once you
get all that working it's all quite
smooth it's just again it's not
commercial stuff so it's produced you
know by Google the chrome driver is a
Google product but they don't you know I
guess they don't care that much okay so
we've been through a sampling of what we
can do with this so we can come back to
our original little matrix if you've got
HTML that's well-formed you can use the
really basic stuff if you want you don't
have to you can use a browser poorly
formed you end up having to go a step up
and then we end up over here using a
driver and some kind of browser for
stuff that's more of a mess
okay so the this is this is really this
is really important so I wasted a lot of
time trying to figure out how to work
around problems in text because I didn't
want to have to use a browser because
it's much heavier and of course there is
no chance I can write a better piece of
software than Chrome for handling
mangled web pages and I just needed to
accept that and then once you realize
that if you use the library properly
it's super super short you feed the next
pass everybody's happy you should never
have wasted your time doing that in the
first place the memory usage is not a
big deal yes it does require more I
don't think you can run it on the
cheapest model of Raspberry Pi although
maybe you can testing gets a bit harder
because versions change all the time
there's new versions of these drivers in
Chrome all the time and they they do
matter incompatibilities between the
little glue driver and the browser you
go a couple of weeks without updating
stuff it'll fall apart so similar to
when I said before if you can't use
Chrome or Firefox you can't do this if
you can't change the big pieces of
software in your environment frequently
you can't do this sorry so in practice
what are we really looking at you can
use the simple stuff if your HTML that's
well-formed which is really saying if
you're looking at init something like a
REST API if it's not JSON objects it's
just HTML but it's generated by a
computer that's fine otherwise really
just learn how to use a browser driver
at browser and spend the time to learn
enough XPath to get the job done in and
move on the really short learning curve
for some of these things doesn't get you
very far up the mountain so there we go
that's fine API use these are real
comments from recent releases of the
driver software they are not meant to
slack off the driver software they're
meant to let you know that this all
works but you need to stay on top of
versions and checking things that chrome
crashes when you navigate it to Gmail
tells you that this is not the world's
most mature piece of software and that I
believe the second one is from Firefox
they weren't logging the arguments
correctly again that's fine it's just
this stuff does not have heavy test
coverage if it works for your case
that's great web scraping you're not
gonna do any damage particularly if it's
in a docker can
if it crashes who really cares as long
as it gets the right answer out the
other side but it's you know only
partway there and because I'm sure there
are still some people who are thinking
you can do this yourself you can these
things are much worse than you think
they are till you've tried to do them
redirects and errors associated with
redirects are really terrible just at
least use the requests library and not
straight URL Lib with no assistance or
just opening sockets yourselves you can
do this with the HTTP and socket server
that are just built in parts of Python
that isn't going to work and most
importantly you're not going to make it
any faster because your JavaScript
interpreter is worse than Google's I
think that's a pretty non-controversial
statement actually I believe at that
point yes okay so I hope that's a
reasonable overview I didn't get too far
into any of the api's because that's a
long talk in and of itself to introduce
a package and the biggest problem I had
was figuring out which package I needed
to use not how to use the package once
I'd figured out which one I needed if
that makes sense
so
questions anybody please
Oh
and then speak out
okay so there is for the L XML package a
parser version that's called super
parser that uses a gentle parser that's
more error tolerant you can use that you
can't get back out from it what it
thinks it was supposed to be what you're
really saying it what you really want to
do I think is load it up in chrome and
then walk the the XML tree walked up
whatever object tree in chrome and then
you could dump it out yourself if you
wanted you can see what decisions it's
made but and I've looked at bits of this
from time to time real webpages once all
the add-ins are rolled in get to be so
huge and the stuff that gets referenced
changes so often you're not gonna means
you're testing your own application fair
enough but beyond that you're not gonna
manage to make your way through it it's
just horrible
okay okay yeah so okay there are other
tools you can use this crappy does more
with expats I just think a lot of what
it gives you you should really just rely
on chrome to do that so I've been
through that exercise and feeding the
expats into chrome just does everything
Scrappy's treatment of JavaScript and
whatnot is basically non-existent so
there are better tools for weird cases
but all of a sudden the page you're
scraping is gonna add some new
JavaScript add-ins that the thing
doesn't handle and then you're going to
start over so if you really if you're
testing your own application have a good
time because you know it's not gonna use
JavaScript but nothing is gonna support
more than you know Chrome IE or Safari
maybe but there aren't really good
drivers so it's a great package for the
slice that it works in and again this is
what I was struggling with in the
beginning but it just doesn't do
everything
Oh
well you mean to figure out how hard the
page is out there
okay so here let's let's do this this
may be a little bit hard to see at this
size I don't know if people can read the
yeah maybe okay so just to figure out
where this download button is and this
is a page I already know my way around
so okay we have to find some unique way
of identifying this to click on it in
this case the name turns out to be
unique it's not too bad but we have to
select and search then we got to figure
out how to click which buttons we need
and here that's you know this is alright
similarly you're gonna have to select
this you know in the menu and the rest
of it I already know my way around the
page remember I'm not starting from
scratch this page isn't too bad I've
been when you have to you get pages of
results you have the page page 1 page 2
page 3 it takes longer to figure stuff
out you can just keep calling get
repeatedly and wait there is in selenium
and again this is a thing where it's
crappy for example doesn't have this
concept you can tell it wait on this
line of code until such-and-such object
is visible so if I click on page 2 of 3
I can say click all right now wait until
the thing for page 3 of whatever is
available and it'll hang out till that's
done move on that stuff can take a
really long time and then you have
horrible webpages that pop up in the
middle a dot dot dot processing dot dot
dot and then you have to wait for that
to appear and in case it was too slow
catch the error when it didn't and then
wait for it to disappear and then wait
three seconds anyway and then search for
something else and then move on and that
can take three days to figure out
I wouldn't post the job with Mechanical
Turk
I mean after doing a fair number of
these pages you get better and better so
that problem with adopt-a-thought
craziness I can now do fairly quickly
the Peruvian government webpage that has
that problem was frustrating for a while
but you see the same problems over and
over if you only need to do one and you
don't enjoy it
yeah you should probably do that but I
don't know the weirdness associated with
some of this I'm not sure how much luck
you're gonna have finding somebody who
can do the really weird pages I mean
they're clearly I'm up oh well that's
the spray fit once but if you want to
scrape it every day you want to spare
twenty thousand versions of the page I
mean you can't really do this for
example
well sure how to say this is to be
recorded if you wanted to go to a large
e-commerce website and try to collect
prices from there they may have anti
scraping defenses and that may require a
work or not or whatever and this is true
for all kinds of websites you want to do
that in bulk you can't pay people to do
that I mean I guess you could maybe but
you'd have a huge amount of staff and
then when they change the webpage you
just be screwed so what I do for my work
after he's mostly financial business so
they're okay with it and the format's
don't change that often but the volumes
are such that a human just can just
can't do it sorry
okay yeah it depends a little bit how
that works so you can also scroll the
the portal so there are commands for
resizing the window scroll movent so you
can tell it to scroll down until
something's visible as much as you can
wait for something to be visible now
whether it Scrolls to a certain point
and then it covers your thing so you
can't click on it you then might have to
click on the X on the little pop-up
window it depends exactly how the page
was done yeah you can tell the browser
to scroll down now if I mean some
versions of auntie scripting defenses
might I mean I've seen this as a person
not actually trying to spray pages
that's honest that's not for the camera
you see stuff that sort of half follows
your mouse around actually I remember
the Citibank UK logon years ago used to
follow your mouse so you could click the
pin numbers I mean there might be
something somewhere that properly
defeats you I guess the question of how
much time you're willing to spend trying
to defeat that page
kinda makes also small videos when they
go into this in panic mode you can see
new requests way to the network and then
illogical public sometimes you can I've
been surprised by pages where stuff's
going back and forth and there are big
blocks that look like it's not a cookie
but some and you realize after a while
they're not changing so it's encoded and
if you just save that in your post
request it'll work fine but then
sometimes it doesn't and you have to
chase it and you yeah it's yeah but if
you can see it on the screen somehow
through some scheme of clicking and
scrolling you can probably get either
the Google either the Chrome or the
Firefox driver to get the thing out for
you without having to do that sort of
stuff if you're willing to do that you
may not need it but then you can't run
the JavaScript without it so it said you
know Inge cases maybe please
so all of the time is being spent on the
network and in the Chrome JavaScript
interpreter all right I mean if it's
straight HTML and you really care about
performance you can narrow some of this
stuff off but then you're saying you
have to rewrite your entire system of
that page changes if you're going
through Chrome right none of the time is
spent in your five lines of Python code
or 50 lines of Python code it doesn't
matter
ah yes
so XPath has all kinds of complicated
stuff you can do contains it's not
exactly regular expressions but yes it
yes yes it's a pretty complicated
language you can get a big books on
XPath and XSLT and XML parsing and
searching and I'm not an expert in that
I can use it but it's not that's it XML
is a huge pubic beast of the thing sorry
in us in so far where I didn't have
so I cannot useful so you can run I do
run headless Chrome and Firefox in
Amazon rented computers off in the
middle of nowhere that's fine
you can fire them up in a docker
container and they're perfectly happy
headless you need to get it working
headless in the same exact container on
your test computer because the error
messages are not the same for headless
and non headless I guess they'll get
better in a year maybe they'll be the
same but in it it works fine
headless is no faster or it doesn't use
any less memory I think they just
suppress the writing of the window it's
a very mild
you can use Chrome and Firefox with
selenium
I mean you can you could do that that
containers running but the thing is
these browsers are stateful in ways
attached to your computer that they try
not to mess around with your bookmarks
and cookies in the rest of it but it
leaves bits around and you really don't
want to run it on a computer that's
doing anything else I mean the web cache
is sort of disabled but it's not a
hundred percent disabled and I've seen
weird entries in my browsing history
that I don't mean weird entries
embarrassing I mean weird like there's
no chance I ever entered that ridiculous
computer-generated URL so you really
want to container that up
I mean it depends on what page right so
some stuff we process data from I don't
think changed since the 90s and that's
not a bad thing necessarily the US
government census web page is all based
on forms has a tremendous amount of
information and I don't think the data
parts meaningfully changed in decades
other stuff seems to change all the time
and people use features that they you
know the more you're in the commerce
space probably the more likely you're
gonna see stuff changing and the more
you're in some kind of old line business
you know it'll be very different
shipping manifests are probably not
changing very often I I don't actually
know
yeah I mean I'll be honest the way I do
it is when error messages start to
appear I go check out what happened but
that depends on you know people and how
important is the data be there correctly
you don't I mean maybe the mechanical
torque idea is not so terrible for
checking the pages
Oh
sorry okay so this is Tom as long as you
use a browser driver and you've got all
your Python UTF stuff correct and you're
not calling encode and decode when
you're not supposed to be and we're not
trying to write anything out to files or
anything like that - of the screen or
anything like that it's all fine in the
browser version the other stuff is not
quite so fine because I mean the
browser's have to handle UTF everything
correctly you also start to have some
education issues with something other
packages where some of the koreans stuff
is in utf-16 and i can tell you lots of
stuff don't do utf-16 correctly
okay okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>