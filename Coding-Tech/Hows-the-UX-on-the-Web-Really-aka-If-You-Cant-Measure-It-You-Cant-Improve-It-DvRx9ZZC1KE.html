<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How's the UX on the Web, Really? (aka If You Can't Measure It, You Can't Improve It) | Coder Coacher - Coaching Coders</title><meta content="How's the UX on the Web, Really? (aka If You Can't Measure It, You Can't Improve It) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How's the UX on the Web, Really? (aka If You Can't Measure It, You Can't Improve It)</b></h2><h5 class="post__date">2018-02-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DvRx9ZZC1KE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so continuing with the mystery theme I
have a good one to ponder that I want to
ponder with you guys today which is what
is they use experience like on the web
today and how exactly do we measure it
because if we can't actually measure it
or agree on how to measure it then we
can't really objectively improve it and
answering this was of course pretty
complicated because one we need to talk
about how do we measure it what tools do
we use and do we have the right tools to
begin with and then second what kind of
metrics are we actually looking for how
do you capture something as wide and as
big as user experience on the web
so of course we can take a quick look at
some of the common metrics that we've
heard a lot about at this conference and
in the past conferences and I'll
actually know it as a brief aside I was
recently reading the Ted guide for
speakers and one of the first things it
tells you is that if you want to have an
effective talk you have to open with
this like groundbreaking personal
breakthrough or some shocking stats just
jolt everybody's attention and get them
engaged so I'll spare you the the former
but since we're talking about
performance the shocking stat is easy
all right I just want to each be
archived and just pull some numbers for
how the web is doing so let's take a
look page load time the median page load
time is 15 seconds according to the HTTP
archive which is pretty depressing well
okay fine how about first meaningful
paint slightly better at five seconds
but maybe it's the number of transferred
bytes that we care about and that's
about fifteen hundred kilobytes and then
finally time to interactive which is how
long you have to wait before you can
actually process input nine seconds you
shocked
is there outrage I see some skeptical
looks in the audience and I think part
of the question here is are these the
right metrics and where do we get these
metrics from if you know HT archive and
you know how you collect the data it's
collected in the lab is that lab
representative of the broader
environment and of course the metrics
themselves for example the page load
time metric we know there are fails in
many cases it doesn't actually capture
the progressive experience that you get
when you load on
well optimized page what about first
meaningful paint well or should we use
first paint first content full paint
which one is it we have half a dozen to
choose from number of transfer bytes
well I don't know because technically or
not technically I'm not sure like many
people I talk to say hey I'm on 4G like
it's not a problem anymore
and time to interactive well it's a good
Diagnostics metric which tells you gives
you a prediction from one we can process
input but it doesn't actually tell you
when the user clicks or how it actually
affects your user experience what about
the general jank as well just skipping
frames it doesn't capture that either so
the short answer here is well it's
complicated we have lots of Diagnostics
metrics and putting them together to
answer a question like how is the user
experience on the web is actually pretty
challenging and requires a lot of nuance
but let's shift gears for a little bit
and also talk about tools
so aside from metrics we also need a set
of tools to help us understand this and
we have lots of great tools at our
disposal that we can run in the lab and
in the wild so why do we have both so
first of all we have tools like
webpagetest and lighthouse which you can
run locally and they give you high
fidelity metrics that you can debug when
you have an issue you can use those
tools to debug an issue and get deep
performance in size to drill in on what
the specific problem is you fix the
issue you deploy it into the real world
and then you pray that something could
happen and of course this is why we have
real user experience metrics which allow
us to actually tell if the thing that we
deployed does it match that as the user
experience of the real world users match
what we set up in the lab it allows us
to calibrate our settings to get
feedback from the real world so this is
good because it allows us to capture
user experience in the small on our
particular site locally and also
validate our model on the web when you
real users in experience the actual site
experience if you want to answer the
question that scale for how the web is
doing it starts to become a little bit
more challenging because we actually
need to look across the web at large and
for that well we can I guess take our
local tools and just run them against a
large portion of the web and that's
exactly what HTTP archive does so twice
a month we crawl about 500,000 websites
and just collect as much data as we can
in a hope that somewhere in there
there's gold that will help us uncover
what the what the systemic problems are
on the web or how the experience is
changing and that's good but of course
the question that you should ask
yourself here is it is also lab metrics
so how do we know if these metrics is
something that we can trust do they
represent the actual overall user
experience and then finally we have a
small gap in our tool box which is can
we actually gather the data real user
experience data at scale can we
calibrate it our overall lab data
against the broader experience on the
web and I'm sure we can all think of
many examples on a day to day basis that
we come across where there are many
sites that deliver great user experience
you can find case studies you've
experienced them yourselves but at the
same time all of us have also found or
can find examples of the contrary the
dark corners of the web where
performance is not as great as it should
be and Darren is actually our biggest
challenge as an industry is to
understand what are the best examples
where can we learn from and where do we
need to focus our attention on and until
recently we actually haven't had the
tools to answer that question well how
many people here are familiar with the
chrome user experience report a few
hands you heard a little bit about it
yesterday from Patrick so back in
October we published the chrome user
experience report which squirrelly aims
to provide infrastructure and some
metrics to answer this particular
question and the good news is after this
talk or after the next 30 minutes
hopefully you'll be the world experts
and chrome user experience report or at
least understand what it's for and what
problem is trying to solve so what
problem is it trying to solve the chrome
user experience report provides user
experience metrics for how real-world
chrome users experience popular sites in
the web so it tries to answer the macro
real-world metrics question that we
asked ourselves at the beginning of this
talk
and since october have actually been
pretty busy and i don't know if you've
monitored the blogs but we announced the
launch of this back in october and we
actually launched it with a fairly small
set of data just to test it and get
feedback from the community for whether
the format makes sense of the data that
we're exposing and whether the metrics
are useful in themselves later in
November or December actually we
launched and expand the data set which
covers the top 1 million sites on the
web and in January we actually worked
with the PageSpeed insights team and
we'll come back to that in a little bit
to integrate the field data with the lab
data to allow us to start calibrating
how the real world experience is what we
model in a lab and then most recently we
actually broke out the data into country
specific datasets as well so you can
actually look at how does the user
experience compare for users that are in
UK versus the ones in US versus the ones
in Japan and so on and so on so it's
been an interesting journey so far and
we continue expanding our data set and
hopefully more metrics in the future as
well so how does this actually work
what are we actually doing here the the
gist of it is that when when you open
Chrome and if you have usage statistics
reporting enabled chrome will collect
some performance data from the sites
that you visit and beak them back to our
servers this applies for the chrome app
itself when you run it and also chrome
custom tabs if you use it in your app
once we receive this data on the Google
servers we go through a series of steps
first of all to anonymize all the data
that is being uploaded so we strip all
that information we filter out
non-public URLs and a non public URL is
something that is not crawlable on the
web or non index ml in indexable on the
web at Google we happen to have a copy
of the web lying around which comes in
handy so we just check against that and
see hey is this URL publicly accessible
and have we been able to discover it
before and then any URL that's not on
that set we just excluded then finally
we count the number of times we've seen
any of the public URLs and remove any
any URL that has a low number of visits
to ensure that we're not revealing any
particular user any particular user's
data finally because we
once you have the country breakouts with
geocodes the with country resolution the
IP address of the upload of the report
and then we run all kinds of
aggregations to compute all the
statistics that we then share with the
data or with the you guys and then
finally the two outputs that we have
today
one is the bigquery table that is a
public data set that you guys can access
and query and download and do what you
want with it and the other one is
PageSpeed insights and the difference
between the two tools is that page speed
and size actually allows you to query on
a per URL level so you can actually type
in a particular page and get real-world
performance data for that page whether
that's your page or not which is
actually really interesting and then for
the public data set we aggregate all the
metrics to an origin so we don't publish
the individual pages and just publish
aggregate analytics about that origin so
let's take a look and let's start with a
hard one first first up we have bigquery
so if you guys are now familiar bigquery
effectively it's just a big data
warehouse that we run at Google it's
part of the public Google Google cloud
and you can query it with sequel like
syntax if you actually open up bigquery
what you will find is just a set of
tables one for each country and also one
global rollup that applies across all
the geographies and as I said earlier
it's keyed by origin so for example you
will find a record there for example com
which happens to be a real site and it
appears to get a lot of traffic which is
kind of interesting
so it's keyed by origin next we actually
have two dimensions that we found that
were critical in our own analysis
perhaps we'll add more in the future as
we get more experience with and feedback
on what we need but so far we're
starting with two the first one is form
factor and it's exactly what you would
expect it tells you whether the traffic
is coming from a phone device or phone
like device a desktop or a tablet how do
we get this data we get the same we get
this data the same way as you would when
you collect your own real user
experience data we look at the user
agent so if you look at the MD undocks
it'll tell you that if you want to
detect whether the user is coming from a
mobile agent or a desktop you can look
at the UA and a hu a has some set of
tokens that describe
this so we look that's how we collect
that data pretty straightforward
then we have effective connection type
and this one is pretty interesting and I
actually were taking a short detour on
to understand how this works so network
information API is available in Google
Chrome and it actually provides a set of
different metrics which are then
distilled into the effective connection
type so the first one is downlink and
downlink is an estimate Chrome's
estimate of the throughput on the device
as observed through recent navigations
on to recent network traffic on the
device itself so it's not based on some
theoretical limit it actually looks at
the past traffic and recent traffic that
has happened on a device and gives you
an estimate based on that which is
pretty nice second one is the round-trip
time and same thing applies here in
addition to observing what the
throughput is it looks at how quickly
has the server been responding or the
servers have been responding to our
outgoing requests and gives you a
feedback based on that and you can query
it via navigator that connection that
RTT to get your current estimate so this
is available in Google Chrome both on
Android and on desktop and then finally
we have actually let me backtrack for
RTT one interesting observation here is
it's not the network RTT or not not the
tcp RTC that we're used to it's actually
the application RTC and application RT t
if you're familiar with the resource
timing means that it's a start of the
request so you've sent a request for the
HP request up until you get the first
response byte and that may include
connection set up if you need a new TLS
tunnel or a new to speak connection it
does include server response time so
it's effectively time to first byte of
the response itself that's the
application RT t so then finally
effective connection type what what the
heck is this thing so if active
connection type takes those two things
the downlink and RT t and maps them into
an experience what we consider to be a
3G 4G or 2g alike experience based on
based on some of the thresholds that you
see on chart here and the observation
here is that we're not actually telling
you whether you're on Wi-Fi or an
Ethernet or on 4G connection we're
just saying all of those things are kind
of the same it doesn't matter what
actually matters to the user is how fast
is responding and what the throughput is
so you may be on Wi-Fi but if it's very
slow because it's overloaded then it may
feel like a 2g like a connection and
vice versa you can be on relatively slow
3G but if the 3G is not bigger not very
loaded and then it may actually have a
pretty good experience you may have a
pretty good experience so that's what
these labels are indicating so let's get
back actually let's pause for a second
so of course you can query all of this
data as I was sitting in in in a room
yesterday I was pulling some of this
data from our computer and based on the
Smashing conf Wi-Fi these are the
numbers that are that I was seeing
yesterday an RTT of 300 milliseconds two
megabits per second and that roughly
mapped into what we consider to be a 4G
like experience now of course over time
these goal posts may change and we may
update these defaults but that's what
our us today and then finally you can
actually observe changes to the to these
estimates if you subscribe you can
subscribe to them via the change
notification so if chrome observes
changes in downlink or RTT you'll get
notified you can react accordingly which
is very handy so this is all cute if you
want to play with it I built myself a
little extension it you can go and
github download it and it'll pop a
little notification in your chrome
toolbar saying what the current estimate
what chrome believes the current
estimate of the connection type is
you'll have to install it yourself it's
not up on chrome store but it's just a
fun little toy maybe a fun to debug and
and play with so then finally we have
ease we have form factor and ect those
are the dimensions and now we get to the
metrics what are the things that we're
tracking today and right now we have
four metrics that we started with we may
expand to more in the future
in fact I'm hoping that we will expand
to more in the future but right now
we're looking at paint API which is a
standard API in chrome that you can use
to collect your real user experience
theater as well which exposes first
paint and first console paint and then
of course Dom content loaded and unload
the two metrics that we've had for a
long
time so paint timing API is now familiar
you can query for first paint and first
content full paint in Chrome in this
case or in this page that I loaded the
two happens to be the same the Consul
paid is the same as the first paint and
sometimes they're different but you can
gather this for your site as well and
then finally in addition to those
metrics we break out the metrics
themselves into a set of bins each bin
has its a histogram and the histogram is
broken down to a set of variable bins
there's a start and an end and a density
to each bin so we can say that 10% of
users fell into the bucket between
thousand milliseconds and 1200
milliseconds and then you can just
aggregate all of that up to a total sum
of one oh so this is all great this is
all pretty low level and like whoo how
does this help alright how does this
help me how does this help you and why
should you care so you probably you're
probably hoping to avoid thinking about
sequel coming to this conference so I'm
sorry but bear with me so if we load up
bigquery we can do some basic and fun
things with this data so first of all we
can run account on the actual table so
in this case we are we're querying the
global data set we're just looking at
the three tables that we've published
over the last couple of months and you
can see that for December 2017 we have
roughly two million origins in this data
set so it's been growing in size now
let's start to do or start asking more
interesting questions now that this data
is available you can actually look at
real world data for sites across the web
and in this case comparing google.com
and Bing and there's a fun comparison
lots of Google engineers were interested
to see how this is going to turn out
when we're gonna publish the data and lo
and behold if you look at the global
stats for google.com and Bing you will
find that more than 84 percent of page
loads for Google com have a first
console paint of less than one second
and roughly 80 percent for bing.com
which is pretty impressive I mean these
guys are going next neck
then we can scope it down we're querying
the global data we can change this to
just look at the UK and you can actually
see that we're actually doing a little
bit better Google search is doing a
little bit better it gains another point
over Bing in in the UK so 86% of
sessions have an FCP of under one second
which is pretty cool
let's pivot a little bit instead of
looking at speed and aggregating based
on that we can actually look at the
dimensions as well so one of the
interesting questions you can ask is
what is a breakout of mobile versus
desktop traffic the web is going mobile
first everything is mobile first is that
actually true and are there differences
between geographies between sites
between different audiences so in this
example we're looking at the Guardian
and we're actually comparing US and UK
and at least for the sample of users
that we're looking at here we can see
that the tablet traffic is pretty low
both in US and UK for three percent but
there's an interesting gap between US
and UK in mobile traffic where you're
actually seeing roughly 11% more mobile
traffic in the UK I'm not sure why
that's the case I think it's an
interesting observation and that's just
for the Guardian and of course we can do
comparisons against other sites and see
if some sites are more appealing to
mobile users versus others or just look
at your competition and see how they're
doing
so three core use cases that this data
set enables one is competitive analytics
because now you can actually look at
your industry at large at your vertical
and compare yourself against the rest
and gain insights from that as well and
then finally we can also look at the
macro trends on the web across all the
sites and start
attempts to answer questions like how
are we doing on the web what is a good
user experience as users see it
themselves and also we can at this point
start calibrating what we do in a lab
versus how users see it in the real
world so I mentioned that earlier this
year we were working with the PageSpeed
insights seem to in to integrate this
field data and if you go to page speed
insights
today you will and run tests you will
see two boxes pop up so before it was
just one it was the optimization score
now we show you two which is speed which
is powered by this chrome user
experience data and a lap and you can
find some pretty interesting examples so
for example here PageSpeed insights
running the test in the lab looks at
Amazon and says it's doing a pretty good
job but we found in this case a bunch of
scripts that are blocking painting and
we believe that there is a room for
improvement to make this experience
faster on the other hand when we look at
the speed data of how real users
experience the site load and we actually
see that Amazon is in the top third and
actually doing really well of overall
pages on the web so clearly the Amazon
engineers know what they're doing and
they're probably making right trade-offs
so perhaps they don't need to go and
optimize even more I'm not sure that
worth worth more investigation so in
this case for Amazon roughly 73 percent
of page loads have a first control paint
under 1.6 seconds so you can test this
today you can play with it and you can
see what the distributions are for your
origin or for any other origin on the
web on the other hand we have sites like
Reuters slow and low and I guess slow
and low is good if we're talking about
barbecue but in this case I don't think
they're even trying so lots of room for
optimization here and then 43% nearly
half of page loads are seeing first
console paint just under three seconds
so definitely room for improvement here
and vice-versa you can actually find
examples where sites have done a good
job of optimizing they've tried to but
at the end of the day for some reason
they still have a slow user experience
so for example CNN here is showing first
paints first console paints 70% of the
time with three seconds plus so clearly
something is going wrong here and now it
allows us and that this data allows us
to now start calibrating what we see in
a lab to how users experience the actual
site
okay it's 2018 somebody recently told me
that because we're sending cars to Mars
unless you have that's unless you have
an assistant agent that you can talk to
in voice you're not doing it right and
know which is going to take you
seriously
so let's attempt the impossible
excellent let's see there's so many
reasons that this can go wrong
let's try it one more time worst case we
have a backup okay well I guess you'll
have to trust me let's try this this way
okay Google I feel the need for speed is
Walmart is smashing con-com cassia is
Smashing Magazine contest yet alright so
what is this black magic it's actually
pretty simple I'm just using the peach
pita API and it actually includes both
the lab data that we have from chrome
user experience report alongside the the
tests that we run the optimization score
so in this case I'm just clearing the
API and empowering the agent so that's
that's fun yeah you can use that in your
own tools so what does this tell us
about the web at large how are how are
we actually doing so I went back and
started looking at HP archive data which
is our lab data at scale versus the
chrome user experience report and if you
look at HTTP archive you'll see that the
median for mobile pages is about 16
seconds I know the font is very small
here but that's actually not the point
because when I pulled the chrome user
experience report data the distribution
looked very very different and of course
the axes are actually different so don't
pay attention to that but the main
observation here is on our lab on our
lab data we see median times of 16
seconds on the real-world user
experience data we see a very different
distribution and the question is why
would that be the case and there's
actually about a million reasons that I
can think of that that could be the case
so let's start with a few in the lab
we're using an emulated mode of 4G
device why well we consider to be
representative of the wider web we had
to pick something so we picked moto 4G
in terms of the network throttling we
said 3G is roughly where the median
experience is on the web so we're going
to use that on our tests also we're
going to start with a clean cache so
every time before we load the page we
wipe everything we shut down all the
sockets and start fresh we only index
the in we only evaluate the performance
of the index page we can't emulate the
user we don't know how to interact when
they actually visit the site and those
are shortcomings but on the other hand
we do have the deep traces and we have
the deep analytics and metrics that come
from running this in the lab on the
other hand in the real world data you
have hundreds of different devices on
all different connection types in all
different battery States
have different states of their HTTP
caches their memory caches that Network
caches their socket caches DNS caches
theirs caches everywhere and all of that
affects the actual user experience also
it does account for all pages on the web
and but on the other hand it only gives
us a small set of Diagnostics metrics so
if there's only one thing that you take
away from this entire presentation is
that you should be using both lab and
ROM data because you need to have your
real world data to calibrate your lab if
you don't have that you're flying blind
you're making assumptions about the user
experience of your real users and if you
can't validate it then you you're just I
don't know what you're doing actually
so let's come back to the beginning and
look at however how are we actually
doing according to the chrome user
experience report so first of all
unfortunately we don't track number of
transfer bytes nor time to interactive
time to interactive is a lab metric at
this point maybe we'll add more I'm
hoping that we will add some something
that tracks responsiveness in the future
but if you look at the numbers based on
the aggregate chrome user experience
report this is globally not specific to
UK we see that the page load time the
median page load time is actually 3.4
seconds and the 90th percentile is 14
point 3 which is definitely a lot better
than the median of 15 seconds first
console paint 1.4 seconds which starts
to feel respectable and 98 at 4.3
seconds is this good is this bad is this
sufficient to describe the overall user
experience I'm not sure I think we need
to do more thinking about that but I
will say that after I pulled this data I
did sleep a little bit better at night
so kudos to you guys for making keep
making the web faster and with that
thank you and please take a look at the
chrome user experience report there's
lots of interesting things that you can
explore there and I guess we'll take
some questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>