<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning with Go | Coder Coacher - Coaching Coders</title><meta content="Deep Learning with Go - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Learning with Go</b></h2><h5 class="post__date">2018-05-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/P5QObw_kqbc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay I'm Chris Benson deep learning with
go quick thing about me do AI and
machine learning as a strategist and I
do architecture for deep learning I'm
the organizer of Atlanta's deep learning
meetup which is actually one of the
largest in the worlds we have over 900
members and we're growing we've only
been going for six months and we have 60
plus people that that come Georgia Tech
is there which is a big AI school and
it's really driven that going forward
gonna go for for about three years now
developer for about 20-something 25
introduced to deep learning in 1992
before it was called that and I'll talk
about that a little bit later last year
did the Stanford University machine
learning certificate on Coursera and
there is a particular technical
publisher based in Birmingham that I'm
having discussions with maybe about a
video and book series but that's not
locked down yet because we're in the
London in the UK
my wife and daughter or British I'm
obviously American but you know that's
that's my legit thing right there
oh and that was that was Harry Potter if
you didn't recognize it here in London
last year so about 25 years ago I was a
college student way back and got into an
interesting introduction to deep
learning at the time before it was
called that and so this is airplane
called the f-22 it is currently the top
air-to-air fighter in the world that may
change at some point in the future but
it's pretty amazing I've seen in person
a number of times as made in the
Lockheed plant near Atlanta and it had
an interesting beginning because there
was a point when it was still a
prototype there were two of them in the
world at the time called the yf-22 and
it had a crash landing at Edwards Air
Force Base in California when they were
doing test flights it was April 1992 and
it was due to this avionics error and it
was supposed to prevent this pilot
induced oscillation this or the the test
pilots trying to crash the plane and the
avionics software is there to prevent it
from being possible well this very
grainy video it didn't work out so why
so he hits the runway and a fire erupts
out the back and he goes skidding for
thousands of feet down the runway and so
suddenly the leadership along he goes
this is really bad and the United States
Air Force goes this is really bad
and so they were trying to figure out
how are they going to handle this well
the problem was assigned to this man who
was a veteran lockheed engineer he had
really focused on new technologies
trying to solve some of the really
complicated AI based issues that the
avionics had involved in stuff and he
would apply these different things and
one of the things he did was he got into
what was what would eventually be called
deep learning it was called neural
networks at the time very bleeding-edge
stuff he happens to be my father
so the interesting thing was I didn't
particularly have an interest but dad
was really struggling with this and he'd
come home and for six months to a year
this was our dinner table talk about you
know how you solve this and what he had
tried and all this stuff and so it
invested in me a lifelong interest in
this topic and I went away for a from it
for many years but eventually came back
and so I owe it to my father that I'm
actually standing here today with an
interest in this topic um things come
around so it makes the question what is
deep learning and I want to hit it at
several layers as we go forward the very
first answer is the cocktail party
answer it's the easy hi fluffy stuff
that you can talk about no this is
probably not the kind of deep learning
that we're talking about it might have
been what people were calling deep
learning a few years back and though the
next one makes me feel very relaxed this
is also not the kind of deep learning
that we're going for today what deep
learning is is it's modern artificial
intelligence that's working today this
is no longer in the future kind of stuff
deep learnings that approached machine
learning and it is the thing that is
driving the current AI revolution in and
you can you can feel it month by month
as it's coming on last year it was
touching the mainstream media somewhat
here and there
and the technical news was all about it
these days you can't get on BBC and CNN
you know in any given day without having
a I machine learning deep learning
stories hitting and it's just going
everywhere it's become a huge thing and
it wouldn't be such a huge thing if it
didn't at least impart work at this
point there's there's also a lot of hype
will get around we'll talk about that as
well as we go forward but the
fundamentals are there so this is where
you are learning from data without
explicitly programming and all of us as
programmers have have been through our
infinite number of you know if thenns
and variations of that trying to
accommodate all the things that we have
to do in our programs and and this takes
a different approach this is where if
you have a whole lot of data you can you
can take an approach where you give this
to your program and it sorts it out by
itself and it learns on its own you're
not teaching it explicitly you're giving
it a method to learn with so deep
learning is the modern application of
deep neural networks to achieve machine
learning it's not the only form and
we'll talk about that in a moment but it
is it is it is an incredibly useful way
of doing it obviously we all hear about
Google self-driving cars and now other
companies self-driving cars that are out
there and you cannot possibly go very
far without hearing about chatbots these
days it's one of the biggest topics
these days I've talked to companies all
the time that want to do chatbots
and the thing behind the the user
interface is deep learning as well
that's figuring out what it is that
you're saying to that system or what
you're typing into the system and how
its going to respond and get context and
everything so these are some of the
other use cases that are that are really
common and there's a big list and you
know highlights are basically security
and the marketing world on the second
line different types of computer vision
and recognition speech recognition and
understanding different languages
obviously transportation we hit
healthcare and financial and these this
is just the tip of the iceberg in in the
last year as I've really kind of
I've kind of made a major career change
really specializing this area instead of
being kind of a generalized software
engineer and I have conversations with
companies all the time and I I don't
think there is any industry on the
planet that will not eventually be
touched by this technology there are
applications everywhere for this as it
comes into being so this is talking what
kind of how it fits in nice and simple
you know AI I would even go so far as to
say AI a machine learning have almost
become the same thing in people so there
really isn't meaningful AI
and there probably someone who who
argues this but I would say there's not
a substantial body of AI outside of the
machine learning world there is a
substantial body of machine learning but
within that body deep learning is the
thing that's made this take off in a
huge way and when people are talking in
in the popular media these days about AI
machine learning deep learning they're
really all talking about the same set of
technologies it's just different levels
of specialization of the labeling that
you're applying and so if you put it in
a little bit broader space this is that
same circle here and then we have kind
of big data deep learning assets right
there at the middle of it deep learning
as we'll learn in a few minutes only
exists because of the amount of data
that's available right now and it would
not be where it is now if we did not
have that in place so data science and
big data are are tightly integrated in
to deep learning being a functional
science unto itself so it's really this
juxtaposition of where all these things
come together where it lives and this
gives just kind of a timeline it's to
some degree the boxes say the same thing
as you saw on the other screen and it
kind of gives you a sense of where
things were you know I was hitting it
back here and and really deep learning
in the form of neural networks really do
extend back here a little ways so this
you'll see this reference in a number of
my slides and this is from the
definitive postgraduate textbook on deep
learning so if you're in the deep
learning field you have that textbook it
is it is really the only it is kind of
the Bible of the field if you if you
will so in recent years deep learning
has seen tremendous growth and it's
popular
usefulness to do in large part and this
is important to more powerful computers
larger data sets and techniques to train
deeper networks and I've talked about
larger data sets already the advent of
cloud computing obviously and the
various platforms that are out there are
really critical to the fact that this is
happening now and it's as powerful
computers it's talking about Asher and
AWS and Google's cloud platform if those
were not there for today and that's not
to say that it will stay this way going
forward there's a lot of talk about
going back to the edge but it's it's the
juxtaposition of all these things that
have allowed this to happen and if any
one of those was not present today deep
learning would not be something we're
talking about at all so why does this
matter
deep neural networks can precisely
approximate any continuous function
that's a really kind of a boring
statement to say right there but I would
argue that when you can say if input and
output a function I have stuff going in
some stuff happens some stuff comes out
and it's useful and we're being
productive that in considering the fact
that just about any process imaginable
in life I'm not just talking about
computing I'm talking about as we go
through our daily lives and there are
real things that we're trying to
accomplish you know if I want a cup of
coffee in the morning and I have water
and I have the coffee machine and I have
the coffee grounds and out comes my cup
of coffee there and and things happen
and that's an incredibly simple you know
function that we're talking about and it
doesn't need deep learning obviously but
there are many things in life that do
and if any process imaginable involves
computing a function then then that
means that when I'm trying to solve real
life problems and they're hard this is
something I might want to consider this
is huge neural networks are universal
because they can approximate any
continuous function they they may not
get it exact but they typically can get
it after sufficient training and when
you find the right architectures and the
right algorithms they can get so closest
to make it a negligible error so well
I'm really saying here is the reason you
should care about deep learning
is that it is a universal approach to
solving complex problems it is more
versatile than nearly any other thing
you could possibly come up with and I
can't come up with a single other thing
that is as versatile for solving complex
problems that exists in today's world so
that's cool Chris but I'm a gofer not a
data scientist so why should I care
about deep learning so several things
deep learning is just entering another
huge metamorphosis and this is this this
evolution process is going so fast it's
after being a software engineer for the
last couple of decades I've never seen
anything moving as fast as what I'm
seeing with this technology so we
probably know this guy he is Google's
CEO and something that he's been saying
now for the last year and a half is that
he likes to talk about the last decade
being about mobile first and you know
we're all trying to get our you know a
you know we're not using desktops hardly
at all anymore except for specialized
skasas even laptops are going away we
all have our mobile phones we've all
been using through the conference
non-stop including me and but that's the
thing in the past this is this is old
news this is where things are going is
that now that we are figured out how to
use different types of edge devices
we're trying to make those devices smart
and so in an AI first world he's talking
about and so are so many other CEOs
about going through their entire product
and service line and and where it makes
sense applying aai capabilities into
these and in most of these cases what
they're talking about is deep learning
so artificial intelligence and machine
learning are no longer just for data
scientists putting artificial
intelligence into production requires
gophers so one of the one of the things
I like to note here is that one of the
processes going into the AI world right
now is it is being democratized in terms
of its availability and the cost to get
in to AI has plunged so three years ago
it was still quite expensive to get into
AI and and you had to have you had to
get data scientists on your team that
had highly specialized knowledge for
good Python understood
the domains and that presented a barrier
to entry to get into this space
well now that what really changed wasn't
so much a technical thing but a
marketing thing when tensorflow which
i'll talk about in a few minutes was
open sourced that made big news because
it was googles and it was already you
know battle-tested you know through
their services so suddenly you have this
amazing package it's not necessarily
better or worse than other competing
packages out there but it caught the it
caught the world's attention when it
went out there and that started the
first phase but as we go through now
this is speeding up and it's going
faster and it is now any one of us can
go and sit down at our laptop now and
over the next you know hours days or
weeks turn out some deep learning models
that we can use in real life to solve
hard hard problems this is something
that is not it's not beyond any of our
reach at this point in time and so in
the years to come
you all of you as programmers and
developers will certainly consume and
you will probably create AI and machine
learning micro services that will just
be as normal as making a UI as doing
anything else that you possibly do is
just another service out there and
you're using it many times a day over
and over again so when you think about
it most companies don't have big data
science teams inside and yet the
majority the companies in the world are
small companies midsize companies it's
not going to be every startups data
science team that does this stuff what
they don't have tons of data scientists
what they have is they're hiring
developers they have their development
team and just like they do now they're
doing DevOps they're doing the coding
they're doing the websites they're doing
everything and they're gonna be doing
deep learning going forward and you'll
have tool sets that you reach into and
use to get the job done for those small
companies and so it is very probable
that even if he never really thought of
yourself in that role
you're probably gonna land in it sooner
than you think this is another CIO and
he makes a point it's it's less about
the man and it's less about even the
exact timeline but he's he's noting
something that's really important here
he said this I think about a year and a
half ago
he said you know it's basically too
early you know if you were looking in
the past to get into it and three years
from now it'll be too late because
you're never gonna catch up and so where
he talked about a three-year timeline
here and we're basically halfway through
that now I don't know if this timeline
is right on but I do know that his point
is really important and that it is time
to start thinking about it the cost has
come down it's been democratized there's
lots of tools out there this is the time
to start jumping into it if this is
something that you want to be into and a
lot of us may get pushed into it whether
we like it or not like I was saying
before so let's go a little deeper what
is machine learning
the mad scientist answer kind of a real
answer so we'll start with this deep
learning is a particular type of machine
learning that achieves great power and
flexibility by learning to represent the
world as a nested hierarchy of concepts
with each concept defined in relation to
simpler concepts and more abstract
representations computed in terms of
less abstract ones got it we got it no
problem
I can stop now um this one I'm sorry so
let's try again allow computers to learn
from experience and understand the world
in terms of a hierarchy of concepts with
each concept defined in its relation to
simpler concepts by gathering knowledge
from experience this approach avoids the
need for human operators to formally
specify all the knowledge that the
computer needs and the hierarchy of
concepts enables the computer to learn
complicated concepts by building them
out of simpler ones if we draw a graph
showing how these concepts are built on
top of each other the graph is deep with
many layers again a very very precise
definition so I'm betting that those
last two slides did not make a huge dent
in your understanding of how this works
so we'll sorry just once more promise
deep learning is an approach to machine
learning that has drawn heavily on our
knowledge of the human brain statistics
and applied math not bad deep learning
is an approach to a is specifically a
type of machine learning a technique
that allows computer systems to improve
was experience and data in recent years
deep learning has seen tremendous growth
and its popularity and usefulness do
large part to more powerful computers
larger data sets and techniques to train
deeper networks which is what you heard
a few minutes before and that's really
important so I put this up because this
is really these guys set what is the
world view on deep learning and so
putting a few textbook definitions and
making them a little bit more accessible
as we go was important but let's delve
into how it works
aside from the the precise definition so
as we've alluded to there's a lot of
data involved and you can't really do
deep learning without a lot of data a
very very very small data set that's
more of a toy than anything might have
tens of thousands of records that you're
training off of as you start moving up
and kind of getting a little bit real
you're moving into hundreds of thousands
and quite honestly all the serious work
out there when they're doing training
they're using millions of Records so you
gotta have a lot of information flowing
and this is starting to happen in this
world as we go into Internet of Things
and we have data flows and we're just
storing everything that could possibly
happen out on edge devices we we
suddenly have a wealth of data and and
after it can be engineered by a data
engineer to prep it for this kind of
work we're starting to have the raw
materials that we need to actually move
things forward so what training does is
enables generalize predictions using
known correct results and what that
means is known correct results means I
have a data set of labeled data that is
designed to train a network for a very
specific purpose so I get a specific
business problem that I want to solve
and I say ok what are the things that go
into that I'm going to go out and get
data from my my data lakes that is
specific to that problem or I believe it
to be at least and I'm going to have to
go through and this is the pain part
right now you have to label that data so
that you know what's right and wrong and
there's a lot of work to that and and
that's changing over time there's
there's a new technology that's coming
about to change that but right now
that's where the most pain is in deep
learning and then generalize predictions
that means your output when I say that
means that if I said take 10 million
records and I'm going to train this
network to get the right output after
going through all these records
what happens when I get a completely
different record based on this
experience it's gonna go back and even
though it's never seen those inputs
specifically in exactly before it's
going to be able to give you the right
answer it figures it out even though
it's not something that's ever done it's
not memorization the computer system is
not memorizing the right answer there is
nothing in its system or a database that
it references if this then that it sees
something new and because the training
was specific to that problem it figures
it out so that's as I mentioned before a
lot of known correct results once one
strain you're able to make those
predictions with completely new inputs
so I want to talk a little bit about I
won't spend too much time in the guts
but I want to give you a sense of what
one type of basic neural network and how
it works what it is this is really the
granddaddy of all them it's called back
propagation it's it's a play that is the
method of training it has applied to
what's called a feed-forward Network and
the basic idea here is that I have these
layers this is my input layer and I have
data that flows into that and then I
have layer layer layer layer with an
output and the jargon on this is that's
called an output layer that's the input
layer and I have three hidden layers to
qualify as a deep neural network a
neural network has to have more than one
hidden layer if you just have one hidden
layer at which incidentally one hidden
layer will allow you to approximate any
continuous function there is out there
but it may take a long time and a lot of
effort to achieve that but it is
mathematically possible deep learning
these days in real life deep networks
may have 10,000 hidden layers in them
and there could be thousands of nodes on
a per layer basis and these are all
operating concurrently as we'll talk
about in just a minute so it's it's
quite computation intensive thus the
need for the the cloud computing that we
were talking about so what happens here
is that I have you know let's pretend I
have a flow here with a bunch of
iterations of records that are about to
flow through a nice steps up with my
first
arrey or more precisely it would be a
tensor where you're talking multi
multi-dimensional data that's available
to go through this but for simplicity
cycle and explanation imagine I have an
array of data and they each line up to
your input notes so they go in and at
this point each one of these nodes is
connected both front and back will
actually start from this to all the
previous nodes in the layer before it
and all the nodes in the next layer
they're not connected on the same layer
so they feed forward through there
thus the arrow is going that way and
what they do and I'll talk about it on a
per node basis shortly but they flow
through there's some operations that
happen they flow through operations flow
through and they get to an output so
that output that you arrive at is going
to have an answer but on that first
iteration out of the ten million that
we're talking about doing it's probably
not the right answer it would almost
certainly not be the right answer
there's so there is an error between
what you expected to get and when I say
expected meaning remember that you have
training data here that is labeled you
know for that iteration what the right
answer is you have the right answer for
training purposes so you're trying to
get this actual output to match your
expected output and so and before we
actually go into the next say I want to
talk about what each one I'm gonna go
back for a second I'm gonna zero in on
what one of these is real quick just so
you have a sense of what we're actually
doing there this is the inside of that
neuron that we're talking about and so
you're going to have it to where this is
the one of the inputs coming in from
next earier node before it it comes in
and it's weighted the W stands for
weight so i've weight one two three and
four right there and every connection
has it starts off with just a randomized
default weight it doesn't matter what
the number is for the first iteration
they get summed up in the transfer
function and then they pass it through
what's called an activation function and
that's where the non-linearity comes in
the activation function and there's a
whole bunch of different things out
there in the examples here we're talking
about a sigmoid function which most of
us have had in school but there's
there's a whole toolkit of different
types of things you could have and that
activations function
is to decide on whether the output
should be up or down for that node is it
good do I use it or is it bad it's not
saying is it exact it's just saying is
it is it close enough to suit my need as
one node in the larger network and so
the activation function kind of gives it
life or doesn't based on here and then
that would flow into the next one so
these these summations and activations
are happening in every node your network
and if you're talking about thousands
per layer and you might have ten
thousand layers that's a lot of
calculation happening and it's a lot of
calculation happening concurrently so
when I get to the expected output versus
the actual output I have to figure out
the error and the way I train my network
here is I I figure out I'm not going to
get into the the math of the air stuff
and I'm happy to talk to people
afterwards on that but I don't want to
go down that rabbit hole at this point
you can calculate the error and then you
push it back up and you do that each
time calculate the air calculate the air
and figure out how to adjust your
weights and so all these connections
going back through all the way to the
beginning are all tweaked all the
weights and then the next time I'll pop
it back to to the previous you start for
iteration two and as it flows through
each of these weights are different now
on these eras and so the amount of you
know authority that that node is able to
lend to the network changes rapidly and
and every time you do that it goes back
and forth so I go there and then I go
back and I go there and I go back and
every time I'm figuring out how close
I'm gonna get to what's an acceptable
level of error there and this is the
actual learning process by going through
all this data and teaching itself and
adjusting the weights based on you know
how close it is to what you wanted this
is the basis of today's world of AI
revolution this is what's what's driving
it this is not the only thing but this
basic idea of back propagation started
yielding out various architectures that
are becoming really popular so I'm
giving you the simplest possible case to
talk about right now but this is where
it all originated from and this was
around 25 years ago when I was first
starting to do it at the dinner table
talking to dad so
and this is what you get at the end so
this is like a train Network I've gone
through it I've gotten to where I want
it to be and the the wits of the arrows
are kind of representing you know how
much that lends into it and so once I
have a train network I stopped training
this because I got down toward the
actual and my expected or close enough
to what was in it was within what's
acceptable to myself in terms of solving
my problem if it's a life-and-death type
thing it might be a very small error but
for many things you don't have to have
perfection to get there you just need to
be close and so maybe I was able to do
it because it was a I don't know a
remote control car and it was having to
to do some calculations as it went or
something
Toye doesn't matter that much but once I
have this that goes out and becomes a
micro service we'll address that later a
little bit but that that's no longer
needs training that is a piece of
software and out doesn't it just does
the calculations one time and you get a
good output that's a that's that is a I
ready for production
so you're updating each of the weights
in the network so that they cause the
actual output to be closer to the target
output and you're minimizing the error
each one along the way and that does it
as they flow back through it does it for
the network as a whole so training is
essentially a search you're searching
for the set of weights within a given
architecture that allow you to get to
the lowest global error for the training
set and when you when you get within
that that area that's acceptable for
your use case you're there you're done
so we just talked about the
backpropagation which is the top it was
kind of the granddaddy of them all and I
want to take just a moment nod toward
some of the others because you're
hearing about these all the time
whether you realize it or not
convolutional networks are all the
things so Google cars and all the things
they're doing visual identification and
anything that has to do with object
identification I is that and what
basically this is done is it's taken a
rat propagation network up there and
they've added a couple of other
components in here that are specialized
on visual and really grid grid oriented
problems when you're looking at a visual
picture a bunch of pixels there's a
relationship between a pixel and the
pixels right around it because in you
know you have the visual gradients that
go off and so there's some tool
that this introduces and it allows that
stuff to work recurrent right here allow
the sense of time back into the network
because in addition to connecting they
have loop backs where it'll actually
take data and look at it in a temporal
sense by looping it all the way back
through and making other adjustments to
it and then this is the hottest coolest
thing right now in AI and I kind of
threw it in just to tell you so one of
the big problems I eluded to is training
data um and with training data you
having to label you might spend months
labeling your training set before you
ever actually get started on the AI
stuff this thing here is saying I can
take a small training data I'm gonna
have two neural networks that fight it
out with a zero-sum game and so one of
them all it does is analyze training
data coming in and it has to
discriminate whether it's real or not
real and it and if it's real data it
uses it for training if it's sighs it's
not real it pushes it back and it fights
against a generative neural network and
all its purpose is is to take the
training data and make it look real if
it's not real enough it takes the output
and they fight about each other and what
it does is actually enlarges your
training data set so you can start with
a data set this like this which maybe is
that ten thousand records I talked about
and you need it to go to ten million so
you can use this technology to actually
create going back to mr. Trump's comment
about fake I I you kind of create fake
data that's that's it's fake but it's
good enough to be real for your training
purposes and therefore you get a massive
data set to train off of so you're
almost generating almost real data as
you go and that's how that's the kind of
the leading way of solving this problem
it's still very early days for this but
it's a really cool technology because it
might mean I don't have to spend six
months getting training data ready in
the future to solve a problem I can keep
solving problems in a much rapid pace
so right now I want to throw in I'm
looting to the go tool go reality check
on this so despite the fact that we've
established that it is a dominant
computer trend computing trend you know
potentially for the next decade we go
furs are being left behind in this AI
revolution and so you know I'm kind of
transition
a little bit right now I kind of wanted
to bring you into what this technology
is at a basic level and then I wanted to
talk about the fact that we're not
really participating in this the way I
think we should I think we've all seen
you know the stats last year we were the
55th you know most popular language and
I think we're up to ten in the last Toby
Index and yet even though we are surging
forward as this amazing community that's
enlarging we haven't we're not really
playing in this space yet so this is
what it looks like if you're a gopher in
the middle and you have pythons and C++
all over the place and and as a gopher
you're a little bit left out in the cold
in this space this is really real to me
because I was not a Python programmer at
all really before very recently and the
only reason I'm at all a Python
programmer today is because I can't do
it and go I can't do all of it and go I
can't do it at the level to be a
professional and go right now I can go
write custom neural networks and we'll
talk about that and that's an option and
we'll talk about a couple of others
going forward here but this is where
we're starting from right now so I'm
saying pretend that the Cobra is a
Python I know it's not a Python but
that's us today as we sit here in the in
the face of the AI revolution and it's a
bit of a tragedy when you think about
the the what we all in this room know so
well about this language that we love
you know it's the performance is amazing
it's so simple to use it has all these
qualities you know from other languages
that we can go haha or statically but
yet we still have all this capability
you know that the dynamic languages have
always claimed it's just a wonderful
language and by the way concurrency is
first class in every way and for neural
networks boy howdy do you need that but
that's where we're at right now we
haven't really jumped in so these are
debatably the top deep learning
frameworks out there
some of them are not just deep learning
they're they're larger machine learning
with a big deep learning focus in them
but this is kind of if you're in this
space in the AI space these are the
packages that people talk about and care
about for large scale implementation and
so of all those the only one that
has any support at all at this point in
time for NGO is tensorflow
so there are other options where you can
you know as we've done in other areas
outside of AI rap other languages and
stuff like that and that's possible and
you might get great results from that
but one of the things that I point out
is I love go I want to do it and go and
even if I can go into other languages
and do it in Python or rap Python or
whatever else I want to do it and go
just because that's what I want to live
in and so right now tensorflow has some
support and it is the market leader
really not so much from necessarily
being the best
I love tensorflow I'm not I'm not down
you but there's several that are really
really good out there but it was from
Google and that makes a huge difference
and so I'm glad that if there's only one
of those group it is tensorflow
because it's a lot easier to go into the
board room at a company and sell
tensorflow than something that they may
not be familiar with or maybe the only
only the academic world is familiar with
and so I throw in this is kind of what
you can do with tensorflow here is there
basically you'll see the comment which
is a quote from the docs by the way
they're basically saying go out do all
your training in Python and then after
you have a train network we're going to
give you the ability to create the graph
with the primitives you need to use a
train network in production so we can
use it if it's trained but I will say is
someone in this space I want to do the
training the training is where all the
meat of it is I want to do the training
and go as well so and this is another
option right here that I want to throw
out if you want this one to show because
it has more stars on github than any of
the other frameworks there's quite a few
out there you know where people have
experiment and tried stuff I'm not
necessarily saying this is the best or
anything but it's the one that has the
most MindTree right now and the
gentleman who is the author is from
London and I checked yesterday to see if
he was here today because you know he
lives in town but it apparently he's not
at the conference to the best of my
ability to tell but if you have an
interest in this go in go to the neural
section and if you go through here the
things that
talking about about the the neurons and
the activation functions you'll see all
of that in his source that all that is
there you'll see it applied as go now
every go library I see that applies
itself to no tackles it a little bit
differently so you won't see the same
exact implementation every time but
you'll see the general ideas implemented
each and it's we don't have time for
today but it will if you have an
interest I encourage you to go there and
see what it looks like to do it's not
actually that hard there's not nearly as
much code as you might expect and also
we have the gentleman here who did
machine box and you may have heard that
recently in the last few months it's
been in coming out and all the various
go specific news outlets machine box is
an excellent machine learning framework
it's free for developers there's a go
SDK and it gives you models that are
ready to use and so I very much I'm
hoping that this takes off within our
community as one of the the go-to things
so there's almost two levels I want
everyone to try deep learn and go and I
also want to get the big frameworks that
have the mind chair and or in in large
businesses to also cater to it so the
future of deep learning and the role of
go in its ecosystem and and in general
this is the last quote from the book I
promise the years ahead are full of
challenges and opportunities to improve
deep learning even further and bring it
to new frontiers well I want the go
programming language to be one of those
new frontiers when I've every time I see
that statement that's what it means to
me so this is a call to action to all of
you in this room and to anybody who
might be watching this on YouTube later
on right now status quo we're not going
to really have a big role in the larger
frameworks that the larger companies are
implementing unless we go out there and
claim that proactively so we need to
have fully functional native go api's
that start getting built into all these
networks so that we can actually do this
without running the C++ or running the
Python to get it done and then coming
back to go after it's done and we need
to go and I would say we already have
you know Google is obviously where go
came from tensorflow already has partial
support for it
and so I reached out yesterday and you
don't need to read all this but I
literally got this late yesterday from
the tensorflow core team I said I'm in
London I'm about to talk about
tensorflow tomorrow we have people who
really want to use tensorflow beyond the
ways that it can be done now and the
short of this is and you can read in the
deck later is you know he notes what
they have and and he also talks about
some big-ticket missing items and by the
way he knew that I was going to share
this with everyone so it wasn't like I
took a private email and stuck it out
there but he's basically saying you can
do this and we the tensorflow team are
willing to help you get there we don't
have the capability of taking it
ourselves that's it's it's one of those
things they just don't have the
bandwidth but if the NGO community has
an interest in deep learning and they
want to make this work for tensorflow in
a more comprehensive way they'll help us
get there and that's my mission today is
to start recruiting those of you who
might have an interest in this let's go
do this so this is what I think deep
learning is going toward I'm very much
hoping that go becomes a major language
and deep learning in the years to come
if we can go out and grab that you may
all these are obviously go you know go
packages and go software that we already
are using or loving I'm sure you know
docker and kubernetes I hope you have
heard about patchy Durham over the past
year which is a data Lake written and go
by dan daniel white nack as part of the
organization that does that so you guys
probably know him from the data science
presentations that go fork on and this
is I really think Minoo other than it
being deep learning it fits into what
we're already doing this is normal stuff
for us and the years ahead this will
continue to be normal stuff so there's
hope for there's also hope for go in the
in the hype cycle of deep learning so we
all see these hype cycles that Gardner
puts out well this just came out last
month so right now deep Lin there's good
and bad news to this d deep learning has
been this hot topic it's right at the
very top of the cycle so over the next
year Gardner's saying how are get a
plunge everyone's gonna be all that deep
learning stuff it's terrible doesn't
work and and move away well I'm gonna
make an argument we've already been left
out of the out of this particular
playground so I actually am kind of glad
that we're about to take a plunge before
it actually really happens because
they're also noting it will get picked
up by everybody ultimately so let's take
the downfall right there and go build
the go tools for this so that when we
get to the bottom and we start the long
term trend up that were players in the
space so while everybody else is
frustrated let's go build it's a great
opportunity right now so that's that's
what I'm encouraging that's what I'm
hoping that folks will do and that's it
I wanted to give a special thank you to
Daniel white knack of go data science
Fame because he did a lot of mentoring
with me in previous months including
helping me get my proposal in and he's
just a great leader in the larger data
science world for the go for community
so I want to say thank you and feel free
to connect thank you very much
any questions yes sir
hi hi very good presentation you talked
a bit about how you need a lot of data
to do a lot of deep learning and you
talked a bit about generative data stuff
as well which might come at that kind of
thing but from what I understand it
becomes much easier if you have a lot of
data so how do we prevent large
companies that have access to large data
from stealing all of this innovation but
essentially that's not possible for
smaller companies or individuals to do
so a couple of thoughts on that actually
I'm gonna challenge the premise in part
so we do have the there's there's both
the kind of the regulation of the data
because there's a lot of obviously you
know we talk about privacy issues and we
talk about security issues around data
but the challenge that the thing I'm
going to challenge you on is the idea
that the big companies only can do that
and the small companies we're going
forward if set aside a I for a moment
let's talk about Internet of Things the
the multiplication or exponentially of
edge devices is going to just fly
through the roof at this point and I
went through a whole interview process a
few months ago and I talked to several
companies about deep learning but they
were also intimately tied into IOT and
and in each case these there are major
global companies that you've heard of
and the plans that each one we're
telling me as a candidate about what
they were gonna do I mean it's just the
amount of data being collected out there
it's just phenomenal going forward the
big companies can do it startup side
there's a there's a healthy startup
community in Atlanta that I participate
in and within that community will have
companies that are you know four people
five people and they have huge amounts
of data coming in and being stored like
an AWS redshift or s3 for instance
that's a really common approach just
massive amounts of data for a five
person company I think it's so cheap now
to just collect everything and you have
you know analytics applied to all of
your software out there or whether it's
AI specific or not that I think this
capability is gonna go forward it may be
that if you're a small company starting
up you might have
to run for a little bit of time to
generate that data but I don't think
it's an I don't think it's specific to
large companies in the future I think
that it's going to be something we can
all do any other questions
you want to go first and then you know
yet
hi so you mentioned some things are
missing for tensorflow do you think
there are other tools around in the
language are missing for go to be a
viable choice something like the
notebook in Python or something oh
that's great not only that but daniel
has you know has a go version you can do
go code in in Jupiter notebooks if
that's what you're referring to so
that's a great tool I think there's
there's a lot of tools that we need to
do we can wrap things today and and
that's a perfectly valid way to start I
would like to see but you're when you're
wrapping something you're still you know
you're not on the inside I still want to
see us as a community
rise up say we have the perfect language
for this you know maybe there are other
really good languages but we're as good
if not better than all the others out
there for this kind of application and
we need to fight for our way in to have
a seat at the table at the big boys
table if you will and so yeah there's a
lot of tools that I think we need to
create and all those frameworks that got
grayed out when we were talking our way
through there I think that there's
opportunities a lot of those have C++ at
the core currently and they might be
wrapped by Python and so that you know
that's their way of saying well that way
we get the performance at the core for
the calculation but Python for
accessibility well you know a good start
on there is let's do api's and go that
then you know you see go and go in and
that's that's a start you know I know
that C go is not the most popular thing
but if it gets me in the door then I'll
take it and then we can talk about the
core later on and you sir
so question about the neural networks
are trained and then used really I'm
showing my neurons here because I've
done Bo Dietl neural networks something
some time ago I'm kind of puzzled that
you say that the neural network can
approximate a continuous function
and we do all this training work which
is computationally hard correct data
that's difficult to assemble but we get
to a really good place at the end of
that which is that we now know when we
feed in unfamiliar data we can be pretty
sure that we can categorize it correctly
on the output yeah so we've determined
the continuous function that does that
correct it still has to be specific to
the function that we've gone and find
but when you're training the neural
network you're basically saying there is
a function out there and today I do not
know what that function is right that's
that was my question really which is can
I then interrogate than the trains
network and then produce Cola later or
something that is a that is a simpler
implementation of the same function so
there is research in that area that's
going on as we speak but as the the
answer today is you can't interrogate so
a neural network between that input
layer and the output layer is a black
box yeah so you can go in and you can
say you know I you know I know it all
for any given layer I know what the
activation functions I know what other
any other functions I've added in are
and I know what the weights are based on
the architecture that I've put in place
but you can't look at any of those
things in isolation say this has context
it has meaning it is literally the
aggregation of all those many thousands
or millions of connections together that
form it and in that way that's where
that that kind of attribution to the
human brain comes in or or any you know
any mammalian brain in the sense of you
you know you can't go and say this is
the neuron in my brain that has that
information I'm seeking it's spread over
a network in the brain and it's the same
thing here it is that black box if you
could read out that function then you
could implement within embedded devices
like Mobile's or whatever the capability
to post post s that data much more
cheaply so there's our that's where
things are going and so this is one of
those things um that I didn't put in
this because it's not consensus in the
community yet but there is there are
people in Silicon Valley in California
today that are that are trying to
address edge devices in an IOT world
and right now there is actually a
sentiment that it won't be very cloud
specific for long you'll start having
distributed training between edge
devices and cloud and ultimately it may
be that more and more that is happening
on an edge devices and then it kind of
gets pushed up to the cloud you know to
save it in the long run but just as
we've seen in larger computing where
things go from a centralized model to a
distributed model and back again as new
technologies come out this deep learning
world is also doing that right now we
are in this centralized model where
we're using cloud resources because
that's the only place we can get what we
need but in the near future near future
we're actually going to be pushing that
out to edge devices and you'll have
distributed training and there's been
some work by Nvidia where they've
already done some successful tests where
instead of it being on one computer and
they're running the network over it
they're distributing it the entire
training process across I think it was
sixty-four servers that they had where
it's no longer just distributing among
GPU cores but actually distributing
among completely separate machines and
two to my knowledge that was the first
time that it happened so this is racing
I mean if I were to come up here three
months from now I have a very different
story in a lot of these areas it's going
that fast one two second follow-up is
it's moving so fast right now that once
upon a time in data science you know
you'd there'd be a conference like this
you know for data science and you come
up and you present your paper on it and
that's kind of how your peers get to
review it and hear it for the first time
doesn't happen that way in deep learning
anymore because it's happening so fast
that they just put it out on the
internet and one of several different
venues and five days late you know
Google may do something here and then
Microsoft takes it to the next level
five days later and two weeks after that
Amazon comes out and says this is what
we do it's moving that fast it's just
phenomenal
any other questions okay feel free to
talk to me afterwards if you'd like or
at any point today through the
conference thank you very much for
coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>