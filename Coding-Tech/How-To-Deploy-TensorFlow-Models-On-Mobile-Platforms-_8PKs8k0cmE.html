<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How To Deploy TensorFlow Models On Mobile Platforms | Coder Coacher - Coaching Coders</title><meta content="How To Deploy TensorFlow Models On Mobile Platforms - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How To Deploy TensorFlow Models On Mobile Platforms</b></h2><h5 class="post__date">2018-02-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_8PKs8k0cmE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so the idea here is you want to deploy
some models on on your mobile phone and
that's what I basically do for most of
my day job when I tell this to people
they usually tell me that sounds quite
crazy like I usually know that mobile
cannot really support deep learning
stuff like if you have inception if you
have big models they're gonna take
hundreds of megabytes of space they're
gonna take gigabytes of RAM on your
computer so you need to be powerful
cloud instance to run them and that's
actually not really true again as Martin
said you have a lot of weapons now at
your disposal
first of all architectures have evolved
to make things smaller and faster and
now even you want to really be crazy and
use a very big architecture there are
methods such as quantization and weight
rounding that allow you to actually do
it so we're gonna take it from the from
the top here we're gonna take it very
slow and I know here in this meetup
usually we use Cara's so let's say we
want to use Cara's and we want to deploy
our Cara's model on mobile the idea here
is that unfortunately if you know how
Cara's works after the Cara's training
is done it usually tries to save your
model in a sort of its own format okay
and we need to try to convert this into
something that then the mobile is
already sort of a keep to read which
would be the tensorflow protobuf format
that is the tensorflow sort of vanilla
tensorflow way of saving models so
fortunately we can do that now caris
supports tensorflow backends so we can
do some match you can try to make tents
Cara's model saved in tensorflow format
so what we do is again we try to build a
very very simple networking in Cara's
just just for show we're stacking some
convolutional layers and values if you
really have a good eye you can see
something weird is happening here with
some lambda don't worry about it for now
back to later again very simple model
we're gonna train it we're not going to
do it right now on this very simple
small MacBook as the CPU who will take
will take a while to train a good model
but anyways the point is while you train
your model you want to be able to save
whatever your training whatever
parameter you're creating in tensorflow
format how to do that fortunately Kerris
has a very powerful tool that are
carrots callbacks
caris callbacks i think sam talked about
carrots callbacks last time are
something that get called automatically
whenever a special event happens in
cares in this case for example we can
create a callback that happens every
time a network of learning an epoch all
learning is done whatever what we want
is a callback to do is once we have set
up our back-end in Cara's which is the
tensorflow back-end and we have sort of
got the session out of this back-end we
want this session the tensorflow session
to save the model to save whatever the
Charis sort of front-end let's train so
far so this is very very simple to do we
create a class to create a callback
we initialize this class with a saver
and a session and on a bookend so every
epoch we're gonna save whatever Kara's
is trained and this save model this save
function it's simply a tensorflow
function there is a you can go online on
the tensorflow documentation and find it
so what's gonna happen here is after you
actually call this callback and you go
on your model you're going to have these
checkpoint files checkpoint files are
the way tensorflow saves a model in
progress so you have not finished you
have checkpoint files you cannot just
use this checkpoint files on mobile but
it's something you're basically already
converted your Kerris model to
tensorflow so now that we have
checkpoint files let's say we finished
our training so we
a lot of checkpoint files that were
saved at every epoch we want to take
maybe the last checkpoint and really
freeze the model in tensile format so
that we can deploy it on our mobile
platform ok there is a few ways we can
do that is actually not too difficult
one way is actually we can start writing
the definition of our tensor flow graph
in protobuf file just by calling write
graph from tensor flow and that's it
very simple since we already sort of
extracted the session from Karis
front-end we extracted the session from
of tensorflow backend so we can use this
session to just say print out in tensor
in a PV file or PB txt file whatever
it's in the session so this is very
powerful and can be very funny as the
names that you use to sort of choose
your models your modeling errors
whatever name you use for your
convenience if your save your model in a
PB txt file these names will still be
there so you can actually then if you
choose to save your model in PB txt file
you can open double click on this PB txt
and really see whatever layers
tensorflow
vanilla layers the Charis model created
once you called it but say even simpler
than that we don't want to sort of save
the tensor flow graph definition
separately we just want to create our
Kerris model we want to train it and we
want to freeze it in tensorflow format
all by using python we don't want to use
any sort of google tools to do that
you can use Google tools there is a very
famous tool called freeze graph
it's the C++ utility it has Python
bindings link is over here if you want
to check it this notebook file will be
put on github anyways what we try to do
here we don't want to use too many tools
that we don't know exactly what they do
we want to try to do things ourselves so
let's say we trained our Karras model we
now have a lot of check points what we
want to do is we want to prepare our
model for freezing and then we want to
actually freeze it so when I say prepare
the model from freezing
there is a few things that we might want
to do we want to maybe clear out
whatever remain there is left and we
want to maybe if we have some
assignments of specific devices assigned
to every node we might want to remove
that so for example if we train the
model on the GPU and for whatever reason
we really assigned is not on the GPU
then we don't want these assignment to
remain there when we actually deploy on
mobile because we're going to use CPU to
do inference of mobile so this is very
simple again you get your model
definition you get the check points and
this is again another tensorflow method
to get the check points the check point
folder here will just be the folder
where you saved your check points and
you create a saver and again what you do
is you set up tensorflow session and you
link it to Chara's and you restore your
your checkpoint in in memory after you
finish the training and you try to write
it down in TV file so here basically
what you can do is that you can load the
checkpoint remove all the assignments of
hardware or such and then save it back
once you have done all your
pre-processing you can freeze the model
in order to freeze the model you have to
actually specify a few things you have
to tell tensorflow which layer is your
input layer and which layer is your
output layer there can be more than one
output layer for example object
detection models sometimes have three
output layers to say where the objects
are which objects are those and how
certain I am that these objects are
there so that's not a problem you just
specify multiple sort of output layers
but in this case we created a very
simple model again and the only output
layer that we have is a simple softmax
node so nothing nothing we are happening
here so what we do again we have our
check point that we saved through our
Kerris callback we wrote our checkpoint
and we specify what is
input what is the output and simply at
this point we can load the weight that
we have been training through Kara's and
save them in proto buffer file format in
order to do that we simply use have to
again use the tensorflow plane method
convert variable to constant that really
frees whatever you have created so once
everything is converted into constant it
can be put into a PP file through the
method of write sterilize the string and
again all these methods have
documentation and we can link it to to
the github and you can go and check it
out but again as you can see it's very
very simple to actually get a Karass
model save it in printer buffer file and
and just deploy it like that so at this
point what we have is a sort of standard
vanilla PB tensorflow model file what we
want to do now is we want to try to
understand exactly what happens we're
not really just happy to save it and say
hopefully everything is gonna be okay
and fortunately Google provides a lot of
tools to do that
first we're gonna use is the tool called
samurai's graph this is a C++ tool that
comes integrated with tensorflow
when i say it's a c++ - that comes
integrity with tensorflow means you
actually have to go download test or
flow source you cannot just do VIP
install tensorflow on your Python
unfortunately that's not enough so you
go on github you clone tensorflow
you build it it's not difficult there
are very detailed instructions to do
that and you have access to all these
sort of nice utilities such as summarize
graph summarize graph allows you to
actually take a PP file and really
inspect it to understand whatever
operations you have in the file which
ones are your input layers which one of
your output layers even which one is the
estimation of the power necessary to run
this graph so you have an esteem
of exactly how much computational power
you need to run this graph based on
basically the input size and the
operations that go through so this is
for example let's say we try to
summarize the previous graph that we
created and as you can see we get one
possible input the image input layer not
a surprise and one possible output that
is the class result softmax layer that
we created before and we have just just
a few operations right we have some some
constant operations and relu some
convolution everything seems fine
actually there is some caches but we can
talk about it shortly again this works
for every kind of model if you have for
example three output layers instead of
just one in one output layer it's still
gonna tell you for example this is the
standard output of a samurai's graph for
an object detector so you have still one
input image input you are trying to
create an object detection model you
pass an image to it and you have three
outputs so again the bounding box that
is where the object stays the
probability score in this case means how
certain the model is that the object is
there and the class index which means
which objects which object the intimate
the network things is there and again
you have your operations and everything
seems fine but actually for both of
these models if we tried to deploy those
on mobile it would not work we would
have some bugs and the application will
crash so what I would like you to
understand is you can actually use this
C++ tool to actually diagnose what's
going on in your model and understand if
you're gonna be able to deploy it on
mobile so for example on your previous
on the first model that we have right
here we can look at the operations and
we see there is a random
uniform operation okay and I just made a
big mess okay and there is a random
uniform operation we think okay why do
we need the random uniform random
uniform is an operation that creates a
random number with the uniform
probability so it doesn't seem some
something like we need for our inference
and in fact answer four dozen sheep with
this operation on mobile so if you
compile tensorflow on mobile by default
it will not have this operation inside
because it's not used during inference
so then you when you go and deploy your
model on mobile it will not work because
this operation is not there so either we
can try to recompile tensorflow with
this operation inside or we need to try
to understand what's going on and why we
have a random uniform in our in our
model if we're not gonna use it for
inference turns out random uniform is
used for drop out so drop out means you
have a layer on which with a uniform
distribution you're going to turn on and
turn off some of the nodes so for
example in order to avoid overfitting
you might want to turn off 20% of the
nodes every time but you don't know
which nodes because you don't want to
turn off always the same notes so you
have a random number generator that
tells you which nodes you want to turn
off so of course dropout is always only
using in training it's not useful in
inference so that's why the answer for
doesn't ship with it so what we would
like to do now is we would like to
remove the dropout layer from our
network so let's think about it we want
to remove our dropout layer but is that
safe to do so again by sort of just a
simple no understanding that would not
be really a safe thing to do you are
taking out 20% of the nodes every time
you are passing through the dropout
layer and now that you are in inference
you're not going to take out these nodes
anymore so actually after this layer
whatever comes after is going to be in a
different format in in a different shape
right it's going to be different because
before 20% of your input was cut off and
now you're going to have
percent of your input coming through so
actually what historically has been done
when you're removing dropout is you
substitute it with a multiplication
layer so that you actually say if you
remove 20% of your nodes in training you
want to maybe sort of multiply all your
inputs by 0.8 so that the they are still
at scale when when you do the inference
however this is not really convenient so
fortunately tensorflow actually doesn't
doesn't actually do dropout it does
something called the inverse dropout so
without you knowing tensorflow is
already rescaling things at training
time meaning that whatever if you de
plot for example take out 20% of your
nodes the inputs are already scaled by
1.2 meaning that then when we have our
inference time we can just safely remove
dropout without any issue because the
weights were already scaled before we
started so again we know we have
tensorflow we can remove dropout how to
remove dropout I mean there are a few
methods to do that some of them are very
funny you go on the internet you see
people suggesting you get your graph you
print it out you assign a number to
every node of your graph you see which
number is dropout and you cut out the
number of the number of dropout on the
graph and you try to rewire the inputs
and the outputs of the sort of
neighboring nodes or maybe you just
parse all the nodes you see whatever has
dropout in the name you remove it so
these are sort of not really Orthodox
methods to remove dropout the safest way
to do it is if you have your model you
already know you're putting dropout over
there
simply write your function so that you
can have a boolean when you write the
function to get your model if you are
training you build the model we drop out
if you are not training you build the
model without dropout so let's take a
look and see and in this case we are
building again a very simple model with
a boolean if we are training dropout
gets put in if we are not training
dropout is excluded this way actually
when we then we build the model and we
freeze it dropout is going to be out and
we're not going to have any problem with
random uniform we don't have to
recompile tensorflow with random uniform
we are just safe to use it here you can
see nice bonus we are using lambda here
meaning we want to use the dropout layer
directly from tensorflow vanilla this is
just because you want to really make
sure of what's going on when you try to
remove layers and you don't want to just
assume how things are going so if you're
sure tensorflow is using inverted drop
out use the vanilla layer actually
cameras also uses inverted drop out but
for drop out Karis does something a bit
weird as it splits the graph in two and
one direction is for training one
direction is for for inference so again
you can still work with that but you
might want to avoid this this weird sort
of dual graph going on so you might want
to use the tensorflow vanilla layer
which just has a single path and just
rescale the input and that's fine again
now the dropout is gonna go out in in
inference and that we safe to use in
mobile let's take a look at the other
network that we had in in our inspection
inspection tool as you can see here it
says that three hundred and six nodes
assigned to device GPU zero so here
whoever has built this network actually
didn't really follow the advice the way
that we said before that we have to
remove the device and we don't have to
have anything assigned to the nodes if
you try to deploy these on an iPhone for
example or an Android it will say I
don't have any idea what GPUs 0 means so
I cannot actually run this model so
let's say ok you you don't really want
to refreeze the graph because there
would be a lot of work fortunately again
Google comes to rescue and they have
another very interesting tool that can
really help you deploy in your model on
mobile and suggest if you really want to
deploy your models on mobile
always use this tool
and this tool is called the graph
transform tool so Google GTT which again
you can find here then on github you can
go and explore the Google transform tool
the graph transform tool really allows
you to do a lot of operations to
optimize and sort of change your graph
so that then it can deploy in mobile
without any problems so it allows you to
for example fold constants fold
constants means if you have for example
a set of notes and these nodes always go
out as a constant in the end it's gonna
just compress them all and just gonna
output this constant because it doesn't
matter to have many nodes so you're
gonna cut out nodes
you're gonna give it the notes that you
were gonna want in the input and the
output he's gonna go through all these
nodes he's gonna check for example which
nodes are not parsed here if you have
some notes for example that just output
some summaries you don't want these
notes to be there so you just cut them
out and these to this tool is amazing
and can do everything for you without
you I need to doing anything by yourself
so again is the C++ Google tool so you
download tensorflow from source you
build it with your basil and then you
can use this tool you just have to
compile this tool specifically and then
you can use it how you use this tool is
very simple so this is the sort of
simple comment you just specify which
one is your graph in input graph is just
the model that we saved which one is
going to be your output which means if
you want to overwrite this graph or you
want to give it a different name simply
which ones are the input layers and the
output layers and which transforms we
want to apply to this graph so in this
case for example I'm stripping away note
that I'm not using I'm stripping away
identity nodes because why would I want
to keep identity nodes over there and
again I'm compressing constant
compressing the batch norms and removing
the device so this is sort of the basic
stuff that you really want to do to
deploy the model of mobile gdt is so
awesome that can do even more advanced
stuff for example you can do
quantization
and the weight rounding that Marlon was
talking about before for so for example
if I actually want to transform my graph
and I want my graph to be quantized
automatically I just have to add one
transformation that is quantized weight
and now if I ran this comment in C++ I
will actually get my original graph
transform takeaway on the constant of
the identities and such and quantize all
the weights just to give you a sample of
what happens here is I have sort of a
few few models so for example this is
your original model and let's say let's
take a look at that this is 12 4
megabytes is simply a source quiz net
based model so it's very small 12
megabytes but it's possible that you
might want to have a bigger model or you
might want to have many models in your
in your phone you have a quantized model
the quantized model is 3 megabytes so
it's the same model quantized you just
have to go and add the quantized
transformation and it's gonna go down to
3 megabytes unfortunately sometimes the
accuracy of the model can also degrade a
bit there is something called fake
quantization that you can do during
training to sort of improve the
quantized result if you really
interested in fake quantization is a bit
extreme topic you can go check out
Google employee called Pete worden who
has a wonderful blog I would put the
link on the on the github notebook and
they is trying to teach a bit how to use
fake quantization at least he's
explaining what it is and how to use it
so that's very interesting but say you
don't want to be so extreme you don't
want to really you lose accuracy then
you want to find a compromise you still
want to sort of reduce the space
occupied by your models then you can use
the weight rounding weight rounding if
you look at this this model is still 12
megabytes so it seems applying the
weight run in transformation did not do
anything but when you go and compress
this model so we try to compress this
and now this is 2.8 megabytes
while if I go and try to compress the
original model we have still 11
megabytes so actually way rounding
works very well especially if you think
that when you deploy a model for example
on an application the application is
going to get archived before it's sent
to any App Store so the model is always
going to be compressed so you don't have
to zip your model and then extract your
model and so on and so forth you have to
do that if you want to actually keep
updating your model without getting the
application and then you want to might
have your mother in a download folder
and you keep it zipped and you unzip it
when you want to use it possibly but if
your mother is already right there
and you're gonna keep it constant and
whenever you want to upload upgrade your
model you send an update to the
application then the compression really
comes out of the box so let's go back to
the back to our Python notebook here as
a sort of a bonus if you guys are crazy
enough you can really do crazy stuff
with tensorflow
say you really don't want to take out
drop out because you don't trust taking
out notes so you can add drop out and
recompile tensorflow
for both iOS and Android so the links
are here basically what you have you
have oh yeah no I don't have internet
here so I cannot show you but what you
have is you have two files that
basically list all the operations that
tensorflow is gonna ship with for your
platform Android iOS anything if you
want to add operations you can do it but
on the other side if you really want to
be extreme on the space right now
tensorflow occupies about 20 megabytes
you want to be super extreme on the
space of your application you can remove
all the operation that your graph is not
gonna use for example if you know your
graph is not gonna use concat and it's
never gonna use concat and if your going
to start using concat you can update
your application to change your
tensorflow then you can go and safely
take out conquered from tensorflow
recompile it and it's not going to link
all the Konkan files and
gonna have a smaller footprint so you
can have a tensile is actually much
smaller than what gets shipped by
default I mean Google actually suggests
you try to do that again it's at your
risk but if you feel very brave you can
actually do that you go get those files
and select the operations that you want
and you can actually check the
operations that you are using through
the samurais graph to the Whitsett bill
that we checked before so you can use
the samurais graph tool I have a list of
all the operations that you're using and
just keep those operation and delete
whatever else TCO you don't need and the
dance-off was gonna be much smaller so
again bonus if you really want to be
brave and son okay now now we are
finally ready to use tensorflow home
mobile and there are some example that
that Google provide for example this is
an iOS example we can take a look at
this example but you can already see
this example here is in objective-c
actually is Objective C + + 1 we really
would like to do is we would like to do
we would like to write our inference
engine in tensorflow
so that we can only have to write it
once we write it once and then we deploy
it on Android we deploy it on iOS
Windows whatever we don't want really
want to rewrite all the sort of session
operations for every single platform so
we can do that we have just have to use
the underlying sort of tensorflow C++
API again so let's first take a look at
what Google is doing here so to be
simple like they just if we take a look
at this run in CNN on frame function
they are taking an image from the camera
right so the idea is always you're gonna
have a few components in your mobile app
you're gonna have your UI right where
you do all the normal mobile stuff then
you're gonna have a wrapper in between
the UI and the tensorflow engine that
can actually talk to both components and
then you're gonna have your C++
underline okay so what is doing here
what Google is doing here they're gonna
take a photo through the camera they're
gonna sort of pass it through the
wrapper convert it into a format that
look and understand in this case if you
take a look they just try to get the raw
data and sort of reshape it in a format
that is nice for tensorflow and and then
try to run a session through through
this data and we're gonna try to sort of
mimic what they're doing but we're gonna
try to do it in C++ so in order to do
that what we gonna use for example is
open CV you don't need to use open CV if
you're gonna use words for example not
images you don't need to use open CV if
you go use computer vision open CV is a
nice tool to have it's easy to do
pre-processing it's easy maybe you
already know the tool so we're gonna
have a quick demo of in this case a soda
object detection model running on an
iPhone so we have our iPhone here we're
gonna get some images so the photos yes
wow this is very small okay so in this
case I trained my model to actually
recognize soda products on on a mobile
device so what I'm gonna do on the left
you have the sort of streaming of the
mobile phone and on the right you have
the photo I'm gonna take so I'm gonna
take this photo and you see in half a
second you get all these colored
rectangles around these objects this is
a good sign it means the objects get
detected we might want to check if the
object sorry what they seem to be so
let's just I mean you you have a list of
these objects let's just go and check
yeah it seems quite all right
you have some cans in the top two levels
Coke normal coke light and such I'm not
partnered with coca-cola or any
affiliates so yeah no you can go and buy
Pepsi that's fine so that I'm on video
so both both companies are great
anyways so as you can see in half a
second a sort of tasks like object
detection that is thought to be even
more difficult and more complex
that then object classification can be
done in just half a second by an iPhone
you take a phone of three four years old
you're gonna agree do it in three five
second but still if you had to do this
this task on the cloud you would have to
take the image and maybe you need a sort
of high Ridge resolution image to be
able to detect all the details you're
gonna have to push it on the cloud so
depending on your internet connection is
gonna take a while all the processing is
going to happen on the cloud and if you
are deploying at scale it's possible
that you have thousands of users sending
you images concurrently so it's possible
that this image is gonna be cute and
processed when if the cloud is ready and
then the result is gonna be pushed back
so it's not gonna be as fast as a 0.5
seconds it's gonna be probably even 3-5
minutes
so actually deploying or mobile
especially if you are an enterprise can
both save time and cost as you don't
have any sort of cloud infrastructure to
build you don't have any cloud computing
to maintain and the users themselves
would bring the infrastructure to you
and you're gonna have a decentralized
tensorflow engine inference engine
infrastructure that works cross-platform
and you just build it in C++ once and
you can deploy it on Windows iOS and
Android okay I think I'm done very
simply just let's just summarize what we
learned today
we built a Karass model and we saved it
in tensorflow format we froze it using
the tensorflow C++ API and we checked it
you through the summarized graph tool to
be able to understand exactly what was
going on and again we try to use this
tool as a diagnostic to understand
exactly what was not going well with our
models and we deployed and improved our
models through the graph transform tool
and then we created a simple sort of
cross-platform inference engine in C++
and I guess in in 40 minutes
that'sthat's enough stuff if you want to
follow up again if you see whatever
you've seen here is cool you want to
join the team this is my personal
professional email if you want to follow
up on anything else
the advice or just need to chat about
tensorflow because it's awesome you have
my personal email here feel free to
contact me don't spam me please okay I'm
done so if there is any question not
happy to reply</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>