<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Statistics For Hackers | Coder Coacher - Coaching Coders</title><meta content="Statistics For Hackers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Statistics For Hackers</b></h2><h5 class="post__date">2017-09-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CRnnarKkm9Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and statistics for hackers I've noticed
this has caused a little bit of
confusion when I've told people about
this you know because people think of
hacker as a person trying to steal your
grandma's bank password or something
like that but I'm not using hacker in
that sort of sense I'm using hacker in
the sense of probably what a lot of you
are in this room someone whose natural
approach to solving problems includes
writing code or understanding the world
through writing code and what I've found
it's really interesting is that
statistics is hard but if you apply some
of those skills that each of you have in
this room and use those programming
skills those statistics can be easy or
at least much easier and the trick in
statistics really comes down to well
first I should say my feet my thesis for
today is that if you can write a for
loop you can do statistics and I want to
I want to come back to that later but
what statistics really comes down to is
asking the right question about your
data and when I work with students or I
work with other folks who are trying to
learn how to apply statistics well it it
comes down to this to asking this right
question I came across this excellent
dr. Seuss quote a little while ago he
said sometimes the questions are
complicated and the answers are simple
and I think this encapsulate the field
of statistics
the hard part is asking the questions
correctly and once you've figured out
how to ask those questions well the
answers sort of come on on their own so
as with any statistics talk or or
lecture or or course we always like to
start with a warm-up and the warm-up
usually involves flipping a coin because
it's something that we can all wrap our
minds around so let's think about
flipping a coin let's say you toss a
coin 30 times and you see that it lands
heads 22 times and the question you want
to answer is is this a fair coin and now
you get it you could have an argument
about this you might say yeah a fair
coin should show 15 or so heads so this
coin is probably not fair right but then
your friend comes
it says ah but but even a fair coin
could show 22 heads every once in a
while so how do you how do you answer
this question this person on the left
saying the fair coin should show 15
heads well we'll call them the Advocate
and and on the right this person saying
you know it could be a fair coin and
just show this by accident we'll call
them the skeptic and basically what the
way we proceed here is statistics is to
assume and in the in in this sort of
test is assume the skeptic is correct
we're gonna test what's called the Nile
to null hypothesis and we'll ask what is
the probability of a fair coin showing
22 heads just by chance and so I'm going
back to your undergrad or your high
school days you probably learned about
you know the probability of a coin toss
is as 50% and the probability of two
heads in a row would be 50% squared so
you square one-half a probability of two
heads followed by a tail would be 50% to
the to the three 50% cubes but then if
you're talking about like two heads and
a tail in any order you have to multiply
that by the number of possibilities the
number of arrangements and then
eventually you from this this simple
thought process you end up with a
formula that looks something like this
it's the number of arrangements of your
heads and tails times the probability of
heads to the end comes probability of
tails the end right and this this is a
nice thing this is called the binomial
distribution and if you plot it out you
see that this gives you a plot of the
the number of heads you can get as it's
a histogram of the number of heads you
get in 20 tosses or in 30 tosses and
what we can look at here is where are 22
hits right there and ask what percentage
of these tosses would be that big or
bigger and we get here for the bun out
binomial distribution that it's 0.8%
right and this actually if you've heard
of these pvalue things this is actually
a p-value we have a P of point 0 0 8 for
the data that we've looked at that says
that
assuming the null hypothesis is true
assuming there's no effects that we're
interested in the probability of getting
our data but just by chance is 0.8
percent and we can say this is less than
0.05 which is the arbitrary bound that
someone long ago decided was the right
bound to set and therefore our coin is
not a fair coin now this is this is nice
and this works for a lot of people but I
find that that many people who think
encode who-who think about the world
procedurally from from the standpoint of
writing Python code there are other
approaches that might be more helpful so
for example why deal with all that
binomial distribution thing when you can
just simulate it right we can do a loop
of a hundred thousand we can do a loop
of a hundred thousand repetitions we can
draw an random integer 0 or 1 we can ask
what the sum is and if it's greater than
equal to 22 we add 1 to the result and
we finds that point 0 0 8 percent of our
point 8 percent of the time point 0 0 8
we get this result of 22 heads or more
right so we've just done in five lines
of Python code what we did previously in
a few slides of mathematics and the
point I want to make here is that that
you all have a way of looking at the
world as hackers right and you can
express some of these statistical ideas
in this language that makes intuitive
sense to you the other way works well
right there the other way works as well
but for many people who think about the
world in code I think this is a really
good way to solve this problem alright
so I'm in general computing the sampling
distribution that's this binomial curve
that I showed you is a difficult thing
to do but also in general simulating the
sampling distribution can be very easy
so what I want to do today is I want to
show you a few recipes for doing this
sort of simulation of the sampling
distribution so you can answer
statistical questions appealing to the
the types of coding intuition that you
have so this first one direct simulation
we just saw that works well when you
have some
a priori model of the world like you
know that it's coin toss is gonna land
50% of the time and you can simulate
that but even in cases as we'll see even
in cases where you can't get an a priori
estimate of what what the outcome will
be we can still do some sampling
approaches so the next one I want to
talk to you about is shuffling and
sticking what the dr. Seuss theme I
don't know does does anyone know who
these guys are right here these are the
one of my favorite dr. Seuss stories the
snitches so the snitches were this group
of creatures and and half of them or so
had stars on their bellies and the other
half didn't and you know over time those
were stars on the belly star started to
think they were pretty special and lord
it over the rest of those snitches
without stars upon dars right so let's
say you are a researcher and you want to
go in and and answer whether the star
belly sneetches really are better than
those without stars upon the arse you
might go in and administer some sort of
test right so here's a distribution of
test scores you give to the snitches out
of a hundred points and the snitches
with stars have a certain number without
stars have a certain number you compute
the means of those and you find that the
star bellied snitches have a 73.5%
average and the non star snitches have a
60 6.9 average so obviously 73 is higher
than 66 but the question you should be
asking is is this significant could it
just happen by chance right and I want
to say the this section that I'm going
through here is drawn from an excellent
talk by John rouser that he gave a
couple years ago at Anna Reilly
conference so he didn't do the snitches
but he did something very similar so is
this significant and and you can do this
the classical way if you if you go back
to your stats 101 and kind of read the
table of contents so remember that
there's this thing called a t-test and
what you do is you subtract the means
and then you divide by some weighted sum
of the standard deviations so we can do
that we can plug in our numbers and we
get point nine three all right all right
so what does this mean you know you go
three a little further and your stats
101 text and you see that this this T
statistic should be distributed this way
all right
that that upside-down l-shaped there is
a gamma function and the little v-shaped
thing that's a Greek letter nu which
means degrees of freedom and you scratch
your head and you remember try to
remember what degrees of freedom are and
you go to Wikipedia and it says the
number of independent ways by which a
dynamic system can move without
violating any constraint imposed on it
and you're like okay so but but then you
dig a little further and down the
Wikipedia page they say that this welch
Satterthwaite sequela and you and you
get 10.7 so that's your degrees of
freedom and then you go to a chart like
this and you look up eleven ish degrees
of freedom for a point O five and you
find that the T statistic is one point
seven nine and you have to ask if your T
value is greater than that and remember
we computed a t-value way back it was
like 0.9 right and this is not true
so the difference of p 6.6 is not
significant at the p equals 0.05 level
right excellent and you sit back you're
like did it but so the problem here is
we have entirely lost track of what
question we're answering right I know
there are probably a few Stata
statisticians in this room for whom that
logic makes sense and helps you
understand the problem I'm not one of
them I don't know you you may not be one
of them either but so so what we can do
you know some people would say well why
are you going through all that
mumbo-jumbo and you can just do this you
just import t-test
in from stats models and plug in the
right you know form of the form of the
variance in the right form of everything
else and you know you get out a p-value
but again this might be the right level
of abstraction for some people it's not
the right level of abstraction for me
and I'd guess for a lot of folks in this
room this is not a helpful level of
abstraction for writing the question
because you know the problem is what
question is this answering if you're you
know if you're four years into
statistics degree you might be able to
answer that off the tip of your tongue
but I certainly can't right now so
stepping back or we go back to this this
is the the real key of this problem this
sampling distribution the student's
t-distribution and it's very similar to
what we did before with the binomial
distribution where we're asking how
often do we get 22 heads with a fair
coin here what we're doing is we're
drawing a different type of distribution
and we're asking for the cutoff and
trying to integrate everything above the
cutoff so why don't we use a sampling
method instead something that you and I
can wrap our minds around well the
problem is unlike coin flipping we don't
have a theoretical model we can't
simulate a sneech right all we have are
these test scores so we need some way
for those test scores to be the
simulation themselves and what we can do
in this situation is something called
shuffling so the idea here with the
shuffling is that we're going to
simulate the distribution of of possible
test scores by shuffling these labels
around and the motivation is that in the
null hypothesis if if star bellied
sneetches and regular sneetches are
really the same it shouldn't matter what
the labels of these numbers are so we
can label them however we want so this
is what we do we first we shuffle the
labels we've kept the numbers in the
same place and just shuffled around the
red and green squares we rearrange them
take the mean and the difference and we
can plot it on our little graph there
we've got a mean a difference of 4.8
then we do it again we shuffle the
samples we rearranged and we plot a mean
we shuffle the samples we rearranged we
plot the means shuffle the samples and
we in this way we can build up this sort
of sampled proxy to the real sampling
distribution and we we get an idea there
of in a very intuitive manner of what we
can expect these differences in test
scores to be and of course we draw the
line where where our measured data lies
at around 6 and we compute that
something like 16 percent of samples
show a score difference greater than 6
so if you're keeping track this point 1
6 is not less than 0.05 right so in the
classical statistical sense of looking
for P of 0.05
we're gonna stay that this is not
significant so you know you go in and
you perform the study and there's hugs
all around all the sneakers are reunited
with their long-lost brothers and
sisters and you've you've done your good
in the world but so that so the key is
that this shuffling approach works when
the null hypothesis assumes that the two
groups are entirely equivalent so I'm
sure you can imagine situations where
this comes up the classical the
classical thing is testing whether a
drug works or doesn't you know you look
at the drugs effects you look at the
placebo effects and you can do these
shuffling things to figure out whether
the drug is effective I should say a
little caveat like all methods this is
only gonna work if your samples are
representative you know if you're if you
have a data gathering problem that's
something completely different and you
also need care for independent trials
like if you're if you're let's say
you're you're measuring this different
test scores of the same sneech over and
over you might expect them to improve
with time so there's there's sort of a
dependence there of the data on previous
data and in that case you have to be a
little more careful with shuffling if if
you want to read more about this there's
this Simon's resampling the new
statistics it's a it's a book that you
can go and look into that kind of goes
into this in a little more detail but
anyway so shuffling is a nice way to use
this sampling approach when you don't
actually have an a priori model of the
data that you're looking at another very
similar a similar approach is something
known as bootstrapping and that's what
I'm going to talk about next so if you
remember your doctor Seuss there was a
this king of the turtle pond named
Yertle and yertle the turtle wanted to
be the most powerful tallest richest
turtle in the world so he decided to get
on the back of his fellow turtles and
and make them stacks higher and higher
and stand on the back so he could be the
tallest turtle in the world now you as a
turtle researcher go out to this pond
and you're observing these every day and
you see that you know you turtle has a
distribution of turtle Heights that he
gets to each day and you want to know in
the long run if you are
observe Yertle for an infinite amount of
time what would be this the average
turtle height of turtles that he gets to
and what would be the spread of that you
know how can you characterize this
distribution of of yordles turtle
heights and so one way we can address
this is to use the the classic method
which is to compute the sample mean and
you've probably seen things like this
before you can compute the standard
error of the mean and you get some
numbers and you find that it's 28.9 plus
or minus 3 and again this this is works
and is fairly intuitive for many people
but I imagine it's probably not
intuitive for for a lot of folks in this
room you know in particular you may not
know what assumptions go into these
formula that you're using and you might
wonder can we use some sort of sampling
approach instead where we can wrap our
mind around what's going on and like
before we don't have a generating model
and unlike before we're not comparing
two different groups so we can't just do
the shuffling approach but there is this
interesting solution called bootstrap
resampling and what bootstrap for
sampling is is you take this to this
data and you basically treat the data as
measurement of its own distribution and
what that looks like in practice is you
sample from this data set with
replacement and to generate new samples
so we're gonna draw random numbers from
this data set and you'll see already
that there's some repeats we've hit
forty one twice right that's that's what
you want to happen in bootstrap you're
sampling your Santry sampling with
replacement and you continue on and you
get some other repeats in there and you
end up with a sub sample or a resampled
version of your data that has a mean of
30 1.05 all right so we can repeat this
several thousand times and we end up
with the distribution of these mean
values and it turns out that almost
magically this distribution is very
close to the analytic result that we
computed earlier and we can do this as a
simple for loop right we go for I in
range 10,000 we pick 20 random points
from the sample this is a way to do it
using the number
i package and we can say that the mean
is the eighth mean is the mean of the
sample and then we look at the mean of
all the means and the standard deviation
of all the means and we find something
that describes our data now this is a
little bit simplistic but the nice thing
about bootstrapping is you can apply it
to even more complicated statistics like
let's say we go out and we measure not
only the height of your Dalls towel
towers but we also measure the wind
speed and we want to know we we want to
know how the wind speed relates to the
to the tower height and we can we can
plot this line on the data and we can
measure the slope and intercept and it
turns out if you do the bootstrap
resampling here and you compute the
slope and intercept on the resampled
data you get a nice estimate of what
that what that confidence interval might
be so this gives us a general idea of
what range of slopes and intercepts we
can expect from the small sample that
we've looked at and the the cool thing
about bootstrapping is that it's
actually really really well studied in
the statistics community you know I
don't want to give the impression that
I'm giving you like these recipes that
statisticians don't know about they know
about all of this stuff and
bootstrapping in particular is something
that's been studied for a long long time
within the statistics community there
are a couple caveats to it that you can
read in in other places for example it
doesn't work very well for ranked based
statistics like if you do the bootstrap
of the maximum value it's not going to
give you very good results
it works poorly if you have very few
samples and there are rules of thumb
that you can read about in different
books about how to apply this if you're
gonna if you're gonna really dig into
this stuff and as with the other stuff
as the previous ones I want to warn you
to be careful about selection of biases
in your data and non independent data
and things like this you have to have to
think about it a little bit but the
point is that you have this procedure
whereby you can you can reason through
what you're doing to your data and wrap
your mind around what the answer is
saying and you can keep your keep the
question in minds
the whole time that you're writing code
now the the last recipe for hacking
statistics that I really like is
cross-validation and this is something
if any of you have been working in
machine learning you'll be familiar with
cross-validation basically
cross-validation is a way to determine
how well a model is fitting data when
you don't have some a priori description
of the data this statisticians have
worked long and hard to learn how well
aligned fits data right and lines are
relatively simple and you can even bring
that along to two even more complicated
models but when you start talking about
something like a neural net with sixteen
hidden layers there's no statistician in
the world that can write out the the
analytic statistics of how that model
fits the data I know someone's gonna
raise their head and said but I did that
if you've done that tell me I'd be
interested to hear about it
so cross-validation is interesting and I
don't know if you any of you have read
the lorax book is way better than the
movie by the way you know the The Lorax
is about this this faceless beast who
goes out into the truffula tree forest
and starts making needs and needs are of
course finds some things that all people
need and and the Lorax gets rather angry
but let's say you are a data scientist
who was hired by once-ler industries and
once-ler industries wants you to
projects need sales so that you can help
them more efficiently you know do their
business and and hack the truffula tree
forest to bits and you notice combing
through their data that there seems to
be this relationship between temperature
and sneed sales right and now that's an
interesting relationship and you'd like
to predict based on the temperature what
you think the feed sales are going to be
for any any given day right so the
question is what model better fits the
data you might have a linear model this
blue line or you might have a quadratic
model this red line and both of those by
I sort of seem plausible but how do you
choose which is the better model for for
actually fitting your data and you might
think you know I'll just
I'll just measure how close the points
are to each of those models right you
compute the root mean square error about
the model and you find the for the blue
points that's 63 but and for the red
points it's 51 but this is something you
have to be careful about um the red
model is more complicated and a more
complicated model will nearly always fit
the data better all right if we start
doing instead of a linear model or a
quadratic model or if we do a cubic or a
quarter or an even higher model as we
add more terms to that equation the RMS
error gets better and better and better
forever basically until we have as many
terms as we have data points and then
your fitting the data perfectly and
these higher-order models actually look
pretty silly right if you were gonna use
this blue line here to predict need
sales and you say that at what am I
looking at here at 20 degrees you have 0
because that's what the model says this
is a silly model you don't want to use
that one to project your need sales so
you know statistics has this figured out
what what we can do with these sorts of
models as we can we can look it up in
the book and we see that the difference
in meets squared errors follows a
chi-squared distribution you can
estimate the degrees of freedom because
these are nested models and you can get
these degrees of freedom you start
plugging in your numbers and pretty soon
you're in the same situation as before
you're you're thinking what question
were we trying to answer and one way
that I find that that I can keep my eye
on that question a little more is by
doing this cross validation approach so
what we do in cross validation is we
take the data set and we essentially
split it
in randomly and separate those two parts
and then for each of those sections of
the data we fit our best model and once
we have those best model fits
we then flip-flop the data and we ask
how well does the red model fit the blue
data and how well does the blue model
fit the red data and this this will
protect against what's known as
overfitting right if we have a model
that's really drawn to that top-left
point and really highly influenced by
that top left
then testing the model on a dataset that
doesn't include that top-left point is a
better and more robust way to see how
well the model is doing all right so
then once we've done that we can compute
the RMS error for each model and we
repeat this for as long as we have have
patience to repeat you know fortunately
you can write a for loop and you can do
this as many times as you want and
basically a split second and then we can
compare this cross validated
root-mean-square so here this blue curve
is what we saw before as we make the
model more complex it just fits the data
better and better and better but the red
curve is a cross validated area where
we're controlling for this overfitting
phenomenon and as we make the model more
complex it hits a minimum where it fits
the data well and then we go on and on
and we're basically to the point where
where the model is influenced by the
noise and the data more than the data
itself so doing that we can fit this
second-order model to our data and we we
can be sure that we have a solid model
for the data that we're looking at so
one that min of I minimize is the cross
validated error and then of course you
know we've we've done our job and now
now one slur industries can can go on
and take over the world and ship their
needs for the southeast on the west and
the north maybe I should have using a
different example anyway so the cross
validation this this was known as two
fold cross validation that's because we
split the data set into two folds and
then we fit across each of those there's
also different ones there's things like
k-fold cross-validation where you take
you split the data into K different sets
and you fur fit on the first let's say
you split into ten sets and you fit on
nine and then test with one and then you
shuffle them again and fit on nine and
test with one there are other
cross-validation routines that we can
use and if you look in the scikit-learn
package documentation there's a pretty
good description of how to use those and
why you might use one over another I
mentioned this before the
cross-validation is really the go-to
method for machine learning
and this is because often in machine
learning we're working with models that
you can't really do the analytic
statistical distribution of the results
it's not like fitting a line to data
where you can you can write down the
sampling distribution if you're doing
something like a random forest or a
neural net it's not something that you
can compute analytically very easily and
then again I want to put the put in the
caveats about selection bias and and
data independence and things like this
so um these were the for hacking
statistics recipe is that I wanted to
present to you guys number one the
direct simulation this is where you have
an a private a priori model of your data
generation process and you can actually
simulate the whole thing end-to-end
they're shuffling where you're trying to
decide whether one group is different
than another group and you can shuffle
the results to generate that sampling
distribution or to mimic the sampling
distribution you can do bootstrapping
where you're drawing with replacement
and computing a statistic computing a
distribution of those statistics and
then cross-validation for comparing
models where you don't have an a priori
statistical way of looking at those
model comparisons now on I should just
do one one aside I think direct
simulation out of all of these is
probably the most appealing one and
you'd be surprised how far you can take
direct simulation like for example in
astronomy there's this there's this
project called the large synoptic survey
telescope and one thing that they're
doing with that essentially this this
telescope is going to start in a few
years and it's going to take an image of
the entire night sky a couple times a
week over the course of ten years and
give you huge huge amounts of data and
where we as astronomers are going to
want to ask statistical questions about
this data and one thing that a group at
University of Washington is doing is
they're bringing this direct simulation
approach almost to its logical end and
there
simulating from the ground up the entire
survey from the photons leaving the
stars going through the atmosphere going
through the telescope optics landing on
the CCD displacing an electron the
electron going across to the to the
detector and and basically their goal is
to to be able to apply this direct
simulation method to something that's
extremely extremely complicated so if
you have the computational power you can
really do some interesting things with
with this type of sampling so anyway to
wrap up I just want to tell you that
sampling methods are I want to leave you
with their sampling methods allow you to
use these intuitive computational
approaches in place of often
non-intuitive statistical rules and I do
want to hedge a little bit before
because I've gotten in trouble for
saying things a little too strongly I'm
not saying that classical statistics is
wrong or that it produces bad results or
even that it's a bad way to look at the
world what I'm saying is that I think
for a lot of people in this room the the
computational approaches will be more
intuitive and will allow you to keep the
question that you're answering in your
mind while you're working with your data
and that in statistics is the most
important thing that you you leave you
keep this question in your mind because
sometimes the questions are complicated
and the answers are simple so a couple
of things I didn't have time for I
actually talked fast I would have had
time for some of these but Bayesian
methods are a really interesting way to
to move forward if it's there's a lot of
sampling that goes on in Bayesian
methods so one thing you can look at for
this you might look at Bayesian methods
for hackers by kim davidson Pilon it's a
kind of all online book that he wrote
several years ago that's pretty good
this idea of selection bias of you know
worrying about how your data is gathered
and whether you're really looking at the
data set you think you're looking at
there's a phenomenal talk from last
summer by Chris Faunus Beck who's a
professor out in Nashville called
statistical thinking for data science
you can find that the video of that talk
online and I'd highly
recommended and then detailed
considerations of these sampling
shuffling and bootstrapping approaches
there are a couple books out there that
are interesting I'd recommend taking a
look at statistics is easy by Shasha and
Wilson and also resampling the new
statistics by Julian Simon and there are
excellent ways to dig into a little more
of the meat of these approaches because
there are some subtleties that you have
to worry about ok and with that I'm
finished if you want to if you want to
look at this these slides online I just
tweeted the link and I'm at at Jake VDP
right there so thanks very much is a
pleasure to be here and we'll have some
questions yeah we have time for some
questions if you could please queue up
at the microphones and please remember
to keep your questions a question thank
you
can I ask now or okay sorry okay so when
you're getting sort of a new sample that
looks like the sample you already have
is there is it convenient to use
something like important sampling where
you don't need to know the exact
distribution that the thing is coming
from do you have another distribution
that kind of looks like it and you know
how to sort of apply a correction factor
or like a weighting factor does that
ever is that useful for making it easier
to kind of see how well your models
doing yeah that's a good question the
question was about important sampling so
you're essentially using other
information that you have in order to
drive the samples and yeah that can be
useful I think it's it's a more advanced
technique and I'd be I'd be sure to you
know read up and make sure you're doing
it correctly I'm I don't think I have
much more to say than that
and thank you for your talk Jake sir
approach that very much speaks to how I
think I guess and so I'm wondering when
doesn't it work like what should I be on
the lookout for folk yeah yeah when does
this not work so I tried to give some of
those caveats in there but especially
when you have all of this basically
assumes independence and then
identically distributed data that sort
of thing where I gave the example before
of you know maybe maybe a snitch is
taking the test and they're not passing
their notes off to the next stage right
the next data point does not depend on
the original data point in in that case
if you start to do shuffling tricks
you'll end up with results that are
completely off so you have to be very
careful about that and sometimes that
non independence in your data can be
really really subtle so I'd watch that
talked by Chris Vaughn SPECT that I
reference for some of those situations
he has some great examples of things
that have happened in in the real world
where these sorts of subtleties come up
the other thing is if you have very very
few samples you know if you have three
samples and you start shuffling them
you're not going to get a very good
proxy for your sampling distribution and
then you there there's some other more
detailed considerations that you can
look at in those books that I I
mentioned so is there anything one of
the implicit assumptions here is that
you're when you're writing code like
this is that the random number generator
is actually a random number generator is
that like you have any advice
specifically about Python about like are
there any caveats about the random
number yeah any advice
that's a great point so this like like
you said this all depends on your random
number generator actually being sort of
random and and one thing about random
number generators is none of them almost
none of them are actually random right
there all deterministic even the one if
you import the random module in Python
you give it a seed and it creates a
deterministic stream of random numbers
now those
numbers have properties of randomness
and they sort of reflects what you would
expect random numbers to look like but
this this distinction of pseudo-random
versus random is is really subtle but
and really important here so if you if
you use a very unsophisticated random
number generator one with like a very
low mode or something these sort of
approaches can fail and they can fail in
ways that are hard to detect but the
good news for us in Python is that we
have built-in random number generator
based on based on some some pretty
sophisticated methods this one same
random number generator is also built
into numpy and for all intents and
purposes you can treat the Python random
number random module as producing real
random numbers but as hackers you all
know the difference between pseudo
random and random number generators so
so you can keep that in mind as you're
answering these sorts of questions Jake
thank you very much that excellent
presentation I also a buddy and I know
it's not the right Python function cause
I think it's a gamma distribution I'm
not sure but where you can the gamma
distribution where you can specify the
mean and the standard deviation of your
sample set and it can create however
much you want is there a reason why you
didn't use it for generating more sample
data in your examples here yeah yeah so
I guess the question is if you if you
know if you know your data is coming
from a gamma distribution or a normal
distribution that's completely valid but
you know the normal distribution is only
really in most cases there's only really
an approximation so you're if if you use
say a normal distribution and you
generate more data from that
distribution what you're learning about
is the normal distribution that you've
assumed fits your data and what we want
to do in these methods is we want to not
make those assumptions about what the
distribution is but rather kind of let
the data speak for themselves in terms
of the distribution and that's why we
that's why we don't do these things so
it's it's a great solution if you
actually know that your data is normally
distributed with them certain mean and a
certain standard deviation thank you
very much yeah I just wanted to confirm
on the example where you had the points
plotted and you split it into two graphs
where did you just randomly pick the
values like depending on which graph it
was gonna go on they were random yeah
yeah so that that's the key to this
cross-validation is that it's a random
split okay I'm especially you know you
might have a data process that's kind of
biased where the first half is slightly
different than the second half and then
if you do a split down the middle you
would you would run into problems so for
packages like scikit-learn they'll
automatically do the right kind of
random split to make sure that you're
you're getting the right answer on your
data thank you
yep give you one last question sure when
you talk about cross-validation and and
it's not a good idea for smaller like if
you only have 20 points or something
like that what do you do yeah that's a
good question of points or there are
there are a couple things even if you
can just give me words I can look up on
yeah there are a couple things you can
do so one you could do something like
leave one out validation so rather than
splitting your data in two you just hold
out one data point and you fit it on the
other nine and then ask how well it fits
that one so I guess that gets you a
little closer to having the full data
set but I think the the broader answer
is that if you're doing machine learning
on ten data points you have bigger
problems
okay thanks well that's all thank Jake
for his great talk</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>