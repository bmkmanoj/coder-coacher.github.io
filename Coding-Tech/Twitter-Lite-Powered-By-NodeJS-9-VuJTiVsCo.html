<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Twitter Lite Powered By NodeJS | Coder Coacher - Coaching Coders</title><meta content="Twitter Lite Powered By NodeJS - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Twitter Lite Powered By NodeJS</b></h2><h5 class="post__date">2018-04-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9-VuJTiVsCo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm James and I work at Twitter on
Twitter light
Twitter light is a recent rewrite of the
Twitter mobile website it's optimized
for interface speed and network
efficiency while also providing a high
quality Twitter user experience we
launched it three months ago after it
was in development for a little over two
years and when it came out users
primarily saw a visual update to our
mobile website as well as a bunch of
cool new features like push
notifications and offline mode but what
they didn't see and what was not obvious
to the user was that this launch was
also a complete rewrite of the Twitter
web stack and the first large-scale user
facing service at Twitter that we've
deployed on nodejs
so today I wanted to talk about why we
chose node and what it was like to
introduce node into a large organization
but first maybe there are some folks who
haven't heard of Twitter Twitter is a
free service where users can share what
is happening our users include
celebrities and world leaders and they
use Twitter to discuss things both lofty
and kind of a name and the engineering
or all really hyper aware of the reach
that the Twitter product has so we more
or less try to take a job seriously
so let's get back to business let's hop
into the DeLorean and go back to the
time before we had started work on
Twitter light the year is 2015 and we've
just met your dogs POTUS has also just
joined Twitter and though we didn't
realize it at the time we had also just
hit peak selfie stick in 2015 the
Twitter mobile website looks like this
this web app is built with a substantial
amount of JavaScript on the front end
and that front end is married to a
considerable amount of Scala back-end
that supports that website the Scala
backend for this website is in charge of
things like form validation and fetching
data from internal sources as well as
most page rendering so architectural II
that looked something like this
we've got half of the application
running in JavaScript in a browser and
the other half runs on a server inside a
Twitter data center where it's able to
talk to various backends that support
the site and in 2015 this is a pretty
typical stack on which we've built most
of our Twitter web properties and this
stack does a lot of things really well
like because it's running in a JVM it
can use some shared Scala libraries to
talk to all the other Scala back-end
services and that helps to get data
really quickly and efficiently across
our back-end and also because it's
running in a JVM it has a lot of an
executor and a thread pool that it can
leverage for rendering all of our web
views which is really useful because
we're trying to serve a variety of
devices not all of which connects food
scripts and because we can still write
parts of the application in JavaScript
we can also shape the web app into this
rich experience that the 2015 user has
come to expect on the other hand this
framework does have a couple downsides
the server here is doing all data
retrieval and view rendering and that
puts it on the critical path for all
pageviews and in 2015 this service has
so many back-end dependencies that it is
slow to compile and it is hard to
maintain and it pages the on-call
engineers often also the stack is hard
to develop adding feature in this
environment typically requires that an
engineer go and build out the user
facing parts of it in JavaScript and
then they go back and they write in the
server support into the Scala web
back-end and that sometimes means
reimplemented the same logic in both
JavaScript and Scala which nobody likes
to do and that's a lot of context
switching for engineers in addition to a
lot of waiting for Scala C to compile
lastly hiring wise it's been really hard
to find people with both the front end
and back end D skillsets and on top of
that when you do not a lot of them know
Scala so there's usually some extra ramp
up time before an engineer can really
get going so technically this stack was
generally working well enough but at the
same time
we weren't able to move as quickly as we
wanted and the result of that was that
our mobile website was falling behind
our own native iOS and Android apps and
some of our users were beginning to
notice so while we were working slowly
in our web stack the web platform itself
had improved browsers now broadly
support making cross-origin requests
which gives us a lot more flexibility in
how the web client might talk directly
to the Twitter REST API without having
to go through a service proxy that we
then have to manage and maintain Chrome
and Firefox and now edge have added
service workers which enables push
notifications and other very flashy
native e features that can run inside of
a web browser and react j/s is a thing
obviously you react has been a thing for
a little while now but in 2015 it was
new and exciting and it also opened up
some new doors for moving view rendering
out of the service and into the browser
so when we were thinking about
rebuilding our mobile web app we looked
at this new landscape of the web
platform and thought about what we
wanted to achieve we knew that we wanted
to build a web app that could optimize
Twitter for speed and network efficiency
and we wanted that web app to work
offline and we also wanted to work well
on low quality networks in developing
countries and the last thing perhaps the
most important is that we really wanted
our new stack to enable engineers at the
company specifically we wanted to one
short in the amount of time it takes for
new contributors to get up to speed and
to shorten the amount of time it takes
to implement a new feature fundamentally
we were looking at removing the server
from the critical path of most user
interactions and what we absolutely did
not want was a service that was in
charge of all the rendering or all the
data fetching because that would make it
very difficult for the app to work
offline what we did want was a service
that could execute parts of the
JavaScript application in areas where it
might improve the user's experience and
so we are arriving at a new architecture
one where the applications
center of gravity has moved away from
the service and into the browser and at
this point replacing the incumbent Scala
service with node really just made a lot
of sense so our web team was pretty
jazzed about rebuilding our web app and
in particular in particular leaning on
node to supply the service needs but
before we could get into anything
technical we kind of have to convince
the larger organization that this is
where we want to go and so I want to
spend just a brief moment on the softer
side of architectural change atwitter
this started with teaching up within our
end organization and trying to shift
some of the biases that we carried as a
company now in this room probably a lot
of us have a background in web
development but you'd be surprised to
learn that not everyone is so lucky and
so it falls on us web engineer's to
educate the organizations in which we
operate and to evangelize the use of new
technologies where we think it can have
the greatest impact and in the overall
goal of shifting our architecture this
campaign to educate humans was by far
the most critical and the most
challenging and the least certain to
succeed and it was mostly done by these
three Twitter engineers Kenneth Catholic
Ryan O'Neal and Nicholas Gallagher who
really deserve all the credit
so this education campaign was centered
on resolving the differences in shape
between node and your typical Scala
service operationally the two platforms
are very different and so introducing
node with some sometimes a bit of a
square peg into a round hole and so we
had to come up with some new tools and
procedures in order to accommodate this
new kind of animal one of those
fundamental differences was in the use
of third-party libraries for example our
normal Scala based web service uses
around 100 external libraries and these
are either direct or transitive
dependencies and each of these libraries
is reviewed by our security team and
they each need an owner who will stay on
top of updates and patches now 100
libraries shared across the
engineering org is a pretty manageable
number but our node service has nearly
2,000 dev dependencies which is vastly
more than we are able to audit much less
even understand
so because trying to audit 2,000
libraries would literally be a denial of
service attack on our security teams
we've had to find other ways to mitigate
the risk of running third-party code and
so we've ended up treating the node
service with the same level of hygiene
that we do any of our other untrusted
client applications and effectively what
this means is that our service layer
uses only the public Twitter REST API it
runs in the sandbox and it has it is
subject to the same rate limiting rules
that client applications have to follow
on another front Java Script does not
enjoy the same level of compile time
checking that we get from the Scala
compiler and for the fans of strong type
systems there was some concern about
would we be able to develop a very large
application spanning hundreds of
contributors without a typesafe safety
net that enables engineering is to do
really fun things like big refactorings
with confidence so in order to develop
more confidence in our own code we've
really really emphasized the development
of integration tests and in addition
we've leaned especially hard on using
sentry to improve our error detection in
the wild if you haven't heard of it or
aren't using it sentry is an open source
browser-based air reporting system and
we use it to identify when a release
contains new bugs that were not caught
in testing or only happened in exotic
locales or when the app is running in an
unusable browser with you know bizarre
chrome extensions so it has been
absolutely invaluable for building
confidence in our application so that's
it for teaching up in a more local scope
our own team was about to start building
with technologies and tools that we did
not have a lot of team wide
organizational experience with and also
themselves had maybe never been tested
at the scale of Twitter so generally
speaking it was really important for a
team to be
to learn as quickly as possible and that
means one learning in detail about the
web platform and the libraries and the
tools that we're using and to
identifying our own mistakes as quickly
as we can and learning as much as we
could from them so to the first point a
really big part of quickly getting our
team up to speed on the technical
details was watch Wednesday's every
Wednesday we'd get lunch as a group and
we'd bring it back to a conference room
and then we'd suffer through a lot of
like really loud obnoxious eating sounds
and we'd watch Tech Talks that were
pertinent to our project and then we'd
hang out afterwards and we discussed
some of the big ideas that might be
distilled from the video and then we'd
go off and experiment with them and
report back in our weekly retrospective
and that brings us to maybe the biggest
and best thing that we've done which are
weekly blame free retrospectives these
have been absolutely essential for a
team to fail faster these are your
pretty typical agile flavored retros
where we take stock of what is going
well and what is not going well and we
talk about the things that maybe we
should be giving more attention to but
crucially we record all of this in a
spreadsheet and this lets us look back
and count how many weeks or months we've
been worried about some thing this
helped to see that for a while we'd been
concerned about the implementation of
flux that we were building on this
system underpinned the entire
application but as a group we could see
that we were starting to find it hard to
test and to debug and we were building
up this record of all the issues that
hid it had produced and having this
record right in front of you every week
makes it really easy to add up the total
amount of pain that something has
produced seeing that we've been
concerned for so long about a flux
system really gave us the motivation to
invest resources into moving our state
management over to redux another example
is our spreadsheet showed us that our
infinite scroller
needed some love this is a component
that manages scrolling a list of things
potentionally infinitely long and of
varying item Heights and getting this
kind of
opponent to work performing Li and with
scroll restoration when a user navigates
forward and backward and making a work
on all the classes of devices that we're
trying to support is really critical for
making the app feel good and it's
important for making users feel like
they're using a native app we've taken a
couple stabs at rewriting her scroller
but our retro spreadsheet had a record
that her scroller was going to be a
special project and it was going to need
as much time as it takes and that was
not going to fit neatly into just one or
two sprints so marius lucky duck
pictured here spent several months on
the project he produced design Docs and
developed prototypes and ran experiments
and production and analyzed results and
iterated many many times and eventually
shipped a replacement scroller that we
were all extremely proud of now on a
high level I think both our shift to
redux and doubling down on a scroller
we're both large engineering efforts and
with large undertakings like that there
is a risk that they can turn into a
black hole of resources from which
nothing ever escapes and so it's really
important to be tactical about which
large projects that we take on and our
spreadsheet was able to give us very
very clear evidence that both these
projects were necessary as well as it
gave us a way to measure and reward the
incremental progress towards getting
them shipped into production and they
also both ended up working out really
well so that came out great one more
thing here is that we wanted to keep our
eyes on the prize which ultimately is
user happiness so we made some stickers
from one of our favorite mean tweets and
we put them on our laptops so that we
may never forget I think I have one here
yeah so after all this soft stuff with
the org and the team the actual
technical challenges of deploying a
large note service were surprisingly
straightforward one thing that was a
surprise though to us was in JSON
parsing we knew upfront that there was
going to be a pretty sizable difference
going from consuming a thrift API and
Scala to going to a compatible or more
or less replacement
API in JSON and parsing that in v8 but
we were surprised by the difference and
the impact that I had and the ways that
that manifested so to elaborate one of
the things that our node service does is
it calls a bunch of REST API is to
bootstrap to the initial state of the
application and it streams that state
down to the client where the client
picks it up and then carries on about
its business and this helps us really
accelerate the boot up time that the
user sees and one of these api is
returns a configuration for any of the
product experiments that might be
running at the time this api has been
used for a while in our native apps but
this was the first time that we'd be
calling this rest json api from a server
context at twitter we really like our
experiments so this experiment config
can be pretty big and after
decompressing the parsable payload size
is around 400 kilobytes so for every
incoming request our service calls this
api and parses this response and it does
stuff with it and during development we
were looking at the low test results and
kind of scratching our heads and
wondering why aren't we able to serve
more traffic per core and really it just
boiled down to that parsing 400
kilobytes is a significant amount of
work so here's a flame graph of our
service under load by the way these
flame graph tools were developed by
engineers at Netflix and have been one
of the very best if not the best tools
for identifying node performance
bottlenecks they're kind of the first
thing in our toolbox that we go when we
go to when we think there might be a
problem if you haven't seen them before
they represent sampling the v8 and then
the JSON stacks over time with leaf word
stack frames being drawn toward the top
of the image and in node a healthy flame
graph should have a mostly pointy look
and any wide flat areas show functions
where the CPU is spending a lot of time
and in this flame graph we can see that
we're spending a lot of time in JSON
parsing wrangling these enormous objects
out of enormous strings and also because
this object is so big we end up paying a
secondary cost shown on the right in
processing it after it's been parsed now
not obvious here but the worst part of
this is that both the parsing and the
processing are both these synchronous
blocking
and uninterruptible tasks and this is a
very very poor workload for nodejs
the v8 event loop is really happiest
when it can be constantly dispatching a
lot of very small tasks so when we're
spending a lot of time in these
cpu-bound workloads that never yield it
reduces the responsiveness of the event
loop and we started seeing cascading
amounts of congestion all over the place
and this impacts all in-flight requests
and it destabilizes our service and
makes it less predictable so we're
looking for a way to reduce the cost of
using this API one option was to just go
and improve the v8 json parser and we
looked at that and it looked pretty hard
so another simpler here and perhaps more
obvious option was just the parse less
so we did that we made improvements to
the API and we added a way to call it
and get only 10% of its previous payload
and the impact of this change in her
node service was dramatic our CPU
utilization was immediately cut in half
and that effectively doubled the number
of requests that were able to serve on
the same hardware so if we take another
look at our problematic flame graph
after reducing the parsable size of just
that one api we can see that the time
spent parsing JSON is massively reduced
it doesn't go away completely but it is
much better than where we started and
the best part of this is that because
the improvement was made outside of our
node service and made at the API level
the optimization is available to all
clients of that API really everyone wins
the take-home message for us was that
our node service was more sensitive to
the api's that are consumed more so than
some of the other clients and services
that were using those same api's and
that really informs how we approach
service optimization so looking down the
road we're interested in finding other
ways to reduce api over fetch because so
far that has been one of the biggest
capacity limiters that we've seen and
we're particularly excited about graph
QL which gives us much tighter control
over what we have to parse we've already
moved some of our API over to graph QL
and we'll be exploring that more later
this year so we've been running our new
node parts
in production for over a year now and we
can take stock of how we've done one of
the biggest wins is that reducing the
scope of the service has made the
application cheaper to operate if we
look at just the compute costs of
running the Twitter light node service
and compared to a JVM web service of
similar scale at Twitter the node
service shown him shown here in yellow
is around 6% of the compute resources
and by a compute this is essentially the
cost of CPU and memory kind of like the
cost of an ec2 instance however this 6%
is only part of a picture we should
consider that we need less service
capacity not because the application is
doing less work globally but in part
because we've moved some of that work to
the browser where it is less visible so
it's helpful to look at the relative
cost of supporting 1 billion page views
regardless of where those views are
rendered and if you do that the node
stack also here in yellow is only three
and a half the compute cost of the JVM
stack per billion renders we had a goal
for improving engineering efficiency on
the new stack for that we can look at a
couple benchmarks around normal
engineering workflows one area we can
look at is the amount of time it takes
to deploy and update to the service
compared to the previous Scala stack
deploy times per datacenter were 30
minutes and they are now 6 which is an
80% reduction and we can also
investigate the time that it takes an
engineer to recompile the project
locally after making a change to the
service and that's gone from 4 minutes
down to 1 second which is at this point
essentially instant so the last thing we
should look at is if we've been able to
leverage some of these technical
improvements to deliver a better user
experience and so far we've gotten some
good signals that have been very
encouraging so generally speaking we
really like our new web stack it is
developer able and it allows us to build
high quality experiences very quickly
and with a minimum of fuss looking
forward this year we're looking at
expanding the reach of both the
twitter-like product as well as the node
based web stack that we built it on
finally a couple
that's a Twitter we are super super
lucky to be able to use the tools that
have been developed and open-source by
others and this includes node and
Express and react and web pack and
globalized and more than I can list here
but special thanks to some of the humans
that have helped us along the way all
these teams have engaged with us early
and often and it's really only with
their help that we've been able to build
one of the largest and most trafficked
progressive web apps on the Internet
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>