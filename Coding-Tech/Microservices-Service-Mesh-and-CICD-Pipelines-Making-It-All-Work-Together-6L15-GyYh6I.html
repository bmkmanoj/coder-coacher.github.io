<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Microservices, Service Mesh, and CI/CD Pipelines: Making It All Work Together | Coder Coacher - Coaching Coders</title><meta content="Microservices, Service Mesh, and CI/CD Pipelines: Making It All Work Together - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Microservices, Service Mesh, and CI/CD Pipelines: Making It All Work Together</b></h2><h5 class="post__date">2018-04-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6L15-GyYh6I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">just a first off Who am I my name is
Brian Redmond I work for Microsoft
my last name is Redmond that's been my
name ever since birth I didn't change my
name I didn't do any of those things to
be honest I've been at Microsoft for
about 17 years so feel free after the
talk kind of bring on all the
traditional Microsoft jokes we can make
our Clippy jokes we could make our blue
screen of death jokes I'm used to all
that and quite comfortable with it these
days I'm a part of the Azure Eric global
black belt team and I help customers
deploy cloud native applications on
Azure and use our Asscher cloud platform
I've been working at Microsoft as I said
for 17 years doing all kinds of
different things but really working with
Azure since its inception and I'm a big
fan of our Azure cloud platform so you
can find me on Twitter when I'm not at
work I like to run so I do a lot of
marathon running so various marathons
start line selfies and things like that
you'll see but I do like to spend my
time running when I when I can so when I
when I submitted the idea for this talk
to be honest I did not think it would
get accepted I thought huh I'll throw
out this idea I know about these various
technologies maybe I'll try to piece
them together and put something
interesting and it'll never get picked
in fact I also submitted a safe topic
that did not get selected but but when I
got the email back much to my surprise
and my shock it was kind of oh I
actually have to do this and so you know
based on the amount of response here I
think it is something that that people
want to see and if anything I'm really
good at writing those CFPs and coming up
with with interesting sort of tag lines
to them what am I going to talk about
well we I think we all know what C icd
is we're not going to spend time here
defining that we do want to make sure we
understand kind of the difference
between C I and C D for this talk we're
going to spend a little bit more time
talking about the continuous deployment
and and automating things that are that
are a part of our deployment process and
we'll talk about different ways that we
can do that we'll talk about concepts
like Bluegreen deployments or blue-green
testing Canary testing we'll spend a
little bit of time if you were in the
talk and the last time slide in this
room you heard a lot about that concept
so we'll talk a little bit about
also around pipelines as code to me it's
really important if you have this
pipeline to have that be declared in a
code library so we can treat it with the
same rigor that we treat our actual
application source code and so forth and
of course I'll also talk about service
mesh there's a lot of excitement around
service mesh there's a lot a lot of
different opportunities and different
kinds of service mesh technology you can
use in particular with kubernetes all
right sound good so when I think of CI
CD I like to think of it as a an
assembly line you know I've got these
various robots in this case
manufacturing something for us
this is robots before the robot
apocalypse before Skynet and before they
actually take over they do they do
automation they do it consistently and
they do it very well and our CID our CI
CD pipeline is the same kind of concept
we'd write this pipeline it'll
automatically repeat that process over
and over again the same way every time
we're feeding in a new version of our
application
the problem with that could be that the
application could actually have a
problem in it and what we don't want to
end up with is this automation actually
deliver bad code into the environment we
do a bunch of testing we shouldn't Lee
hope that we did all the testing we
could we did integration testing we did
load testing we hope our test
environment really is like production
but in the end sometimes eventually
we're just crossing our fingers and what
we don't want is to learn about bugs and
issues from the customer we don't want
to find out there was an issue by
hearing about it directly from the
customer or even worse hearing about it
on Twitter and hearing about all the
ugliness and so forth like that so we
need better ways to solve that solve
that problem for us and I would argue
obviously canary testing is something we
can do to help us with this problem and
this is a pretty common technique I
think most people here at least if
they're not already doing it or
understand what it is I have this
application I maybe go through some kind
of proxy I put out the second new
version of the application alongside the
the the existing production one and I
route some of the traffic over to it
this comes from the concept of canary in
a coal mine I'm afraid to go into the
coal mine so I I submit a poor bird to
go through into the cave and hope that
it's safe and
so this can help us understand what's
actually going on does the application
actually work while I'm getting real
production traffic but if it's gone
wrong it's easy for me to switch back
the impact is much smaller so this is a
great technique and something I'm sure
everybody here is familiar with and
already doing but what about when I have
a microservices application now I've got
lots of services it's not quite so
simple to take those same techniques
that I would apply at the front end
across a mesh of services that basically
all of the services are talking to
everything so this problem becomes a bit
harder I need a way to do canary testing
anywhere in this environment anywhere
without or across my application of
micro services so what's missing from
this and I would argue there's a lot of
different sort of implementations of CIC
be out there in general we need we need
a few things to make this better the one
thing that we need is advanced routing
we need something that can route traffic
to particular versions of particular
services by rule and help us be able to
understand what's actually happening and
do that canary testing at any layer of
my microservices application i also need
something that gives me observability so
I do need to know what's happening I
need detailed metrics coming out of this
I need to understand that when traffic
does hit that canary test is it taking
longer is it worse or better than
average how is how is my application
performing overall and how is that
particular API performing and maybe even
down to source and destination what what
is the actual impact of this so I need
meaningful detailed kind of
observability and I may also need chaos
testing so like this we all know
applications things go wrong in general
everything that could go wrong will go
wrong it would be nice in a testing
model to actually see what would happen
if things went wrong to throw problems
at my application and actually see how
it behaves see how the end user would
actually experience that that chaos so
as an example I won't spend as much time
talking about sort of dev and test
certainly we're gonna have some kind of
a developer development environment and
we're gonna do unit and integration
testing we need to make sure the code
actually works so make sure it does what
expect it to do we may even do a test
environment where we throw that code out
there with all the other micro services
and this would be a great place to
inject that sort of chaos testing and in
fact there's a lot of different ways to
do chaos testing that you'll hear about
from from different technologies here at
this conference but that's outside of
production in the in the production
model we may actually want to take an
update to our code as a pull request and
actually deploy that as a canary build
to play that individual micro service
out into the environment alongside the
production one and modify some of the
routing to get some of the traffic over
to that release and we could then score
that we could get some kind of metric
about it and I would argue that we could
actually collect enough data and almost
create an index or a score and say if
it's above a certain score let's just go
ahead and automatically merge it let's
try to automate this as much as possible
however if it's below that you know
maybe we need some kind of human
workflow to actually take a look at the
individual statistics and decide that
it's okay and maybe if it's below a
certain amount we don't do anything at
all we remove it and then we go ahead
and update production accordingly if
it's passing those kind of tests so
generically we might want some kind of
pipeline like this well to me this is
where sto comes in so sto is a service
mesh technology as it said it's an open
platform to connect manage and secure
micro services and there's a lot of
different service mesh technologies out
there I didn't pick this one because I
thought it was the best frankly I'd like
to learn about new technology so that's
one that I picked just because it was
new and interesting and certainly being
well adopted in the kubernetes kind of
community what this helps me do is it
does help me with the things like
service discovery and routing it
provides this sidecar model to be able
to control where traffic is going I get
some of these other interesting things
like health checking and policy
enforcement helps me with security it
helps me with a lot of different things
how do we do that from an architecture
standpoint each of my services and this
in this picture right out of the sto
Doc's I have service a and service B
service a in its pod has the Envoy proxy
as a sidecar all the traffic from
service a going anywhere else outside of
that pod has to go through that proxy
various route rules will be applied any
kind of other other kind of delays that
I want to inject any kind of telemetry
that I want to gather will all be
handled by that side car and that side
car is actually the Envoy proxy that
came out of the lyft organization and is
one of the CN CF projects here so this
is super handy this means that at the
control plane layer there are sto
components that allow me to manage how
those proxies behave and in the
kubernetes case we can actually do this
with custom resources so we can create
an actual resource call it our route
rule and tell our side cars how to
behave and what to do so when I think
about some of the things that I wanted
earlier and I want to gather statistics
and I want to control routing and so
forth SDO is a great a great model for
me to do this now how does that envoy
proxy get into that pod well there's a
few different ways that you can do that
Waianae can manually inject it using a
command line tool from from the SDO
platform I can also automatically inject
it by name space so I can say I can use
a take advantage of initializers which
is part of a part of the latest releases
of kubernetes and allow that that gets
automatically injected in pods I don't
actually have to change our deployment
process to make sure that that sidecar
is there for me and there's other ways
to actually make sure that that's there
across cluster or by namespace or
however you might want to do it but so
what can I do with this do just to kind
of summarize around that well I needed
advanced routing I can do I can do that
with route rules and traffic shaping and
I can I can apply whatever advanced
routing I need to do this kind of micro
service level canary testing very simple
to get that observability I'm going
right out of the box and in the in the
the docs you can you can see how easy it
is to write all these interesting
metrics out to something such as
Prometheus so that I can pull up a
dashboard and see what's actually
happening with the traffic and see how
long this new version is taking or any
kind of metrics around it and of course
I can also do things like chaos testing
so I can inject a delay or even a
complete HTTP fault and see how my
behaves does it handle that does it do
we have the proper retry logic to handle
that scenario now this isn't everything
that sto does you can do mutual
authentication and in an automated
fashion
I can implement circuit breakers I can
do all kinds of other things with this
do these are the things that I'm focused
on here that are gonna help me with this
kind of see ICD process but certainly a
lot more that you can do with this do so
what about a C ICD tool well the one
thing I would argue in this case is you
shouldn't need a new C ICD tool to do
this so hopefully your C ICD tool can
really automate some of these same
processes that I'm going to show you
here and that would certainly be the
goal if you're if this tool isn't isn't
sufficient for what you need then
there's obviously it could be a good
time to take a look at it but hopefully
what we're describing here today can be
done with what you're already using and
that certainly would be the goal now for
my personal purposes as I'm building
this demo and tuck and doing this talk I
trust to use Brigade and honestly when I
submitted the talk or frankly even when
I started building my demos Brigade
actually wasn't something that existed
so Brigade was an open source project
actually created by the the team
formerly known as Deus that is now a
part of Microsoft and they released this
open source project a couple months ago
at the OSS summits that was in Prague
and basically Brigade is an event-driven
scripting engine that runs in kubernetes
and we're gonna take a look at it and
get it and get a good look at how it
works but essentially what it allows me
to do is the various steps of my
workflow can be encapsulated as
functions inside of containers in my kin
my kubernetes cluster so I can then run
these these particular jobs that make up
my CI CD process by taking advantage of
kubernetes and my cluster and containers
so if I have a particular thing such as
helm I can just put that in a container
and tell Brigade to actually execute
that for me these can be run in parallel
they can be run in serial or some
combination of that and or both and they
get triggered by various events or
various web hooks by default sort of
built in you get kind of github and
docker registry but they could be
anything that you want you can you can
trigger one of these workflows from
anything
the pipeline is described as JavaScript
so this is very simple and I honestly
hadn't written JavaScript before very
easy to actually kind of build one of
these pipelines and it is really
important that this is code actually
stored in the github repo for the for
the application itself so that we know
anything that changed of our automation
process is actually tracked and source
control just like it would be for our
application the other nice thing is we
talked about secrets that might be
needed by my pipeline like how to
connect to my container registry how to
talk a secret on how to talk to github
or anything such as that Brigade
actually helps me with that as well it
creates the concept of a project and
injects all of those meaningful
configuration items as secrets in the
cluster and easily takes care of pulling
those out for me and using them in my
pipeline and so it's well-suited for CI
CD pipelines it isn't only a CI CD tool
so if you're thinking to yourself wow I
could use this for something different
than that great I think that would be a
great use of Brigade so brigades in its
early stages I want to say we're in like
a point for a version release like I
could be forgetting but I think we're
seeing a lot of interest already and how
what could I use this for and certainly
see ICD pipelines is an easy place to
start we also had another another open
source project that was announced by the
same team that was actually announced
only a couple hours ago
that's called cosh T and cosh T is a is
a web dashboard that's really made to
sit in front of Brigade and show me
what's actually happening in the C sed
pipeline and really this is a beautiful
web-based UI that shows me a bunch of
information that would have otherwise
been distributed all kind of throughout
my kubernetes cluster I had a particular
pod that ran it did a job for me I need
to find the logs I need to find what
triggered it how do they all fit
together cos he pulls that all together
for me what event triggered this how
long did it run what are the different
steps it shows me in kind of a waterfall
diagram and we'll get a good look at it
here in a moment in the demo but it
gives me a chance to actually see the
results of my CI CD pipeline as well as
the configuration and all the details
around it so cosh T was just announced
as I said earlier today these are I
encourage you to go take a look at these
try them out and give us sort of
feedback on them to that to the team so
time for a demo
I don't know timing wise where I'm at I
think I'm right on schedule so my demo
is called micro smack I'm not going to
explain the backstory to micro smack if
you catch me after I'll explain where
that comes from
but the actual application itself I like
to do live demos I will tell you the
application itself is unimportant we're
trying to see what automation actually
happens here so I have a really
rudimentary bad web UI I'm not a I'm not
a UI guy so the important thing is you
actually see the front-end and you see
it call the API and we're gonna affect
that behavior using SDO and we're gonna
drive all of that with Brigade now
before I jump over to the demo and I'll
walk you through what the what the demo
will actually look like so what we have
is we have a web UI we have just a
simple go application that's displaying
that it makes 25 calls back to the api's
that are running in a separate in
separate pots these API is return a
color and they return some build details
but that color actually drives how the
HTML table is painted and we call it 25
times not because that's efficient
because it obviously isn't we want to
generate some traffic and we want to see
if we are getting some kind of canary
deployment in this we want to see the
table actually paint differently and so
you'll see that when we run this but by
default we're just all of the traffic is
hitting the main one and we see some
kind of blue table that I'll show you
here in a moment now in each of those
pods we have the envoy side car running
in all of them and that's where sto
comes in is controlling how the traffic
is actually being shaped here in this
cluster we will take a look at slack as
developers we're gonna get details of
the build over in our slack Channel and
we have brigade in our code and github
so when we make an update to the code
and github will see a webhook occur
webhook will talk to brigade brigade
will fire up the steps of my pipeline
and it actually is going to spin up pods
in my kubernetes cluster that's going to
do the various steps of this of this
workflow so we're gonna go and create a
go a go application actually build the
go application itself we're going to
build a docker container and put it in
your container registry and we're gonna
take advantage of helm to actually make
updates to the deployments around the
API as well as some of the sto
components to control how the traffic is
done so I do all of that with helm as
well and we have one to update slack so
when we actually kick off that that PR
it's actually going to do a deployment
of that PR put it alongside the existing
one and route some of the traffic over
to it now you might be saying okay
ninety percent of the traffic that
doesn't sound like a canary billet it
actually sounds like the reverse of a
canary build and yeah you have to kind
of use your imagination we're up here
doing a demo on stage we want to see
some kind of interesting activity that's
kind of like watching an action movie
and wondering how Bruce Willis doesn't
get shot running through a you know a
bunch of machine guns so we're showing
90% just so we can see a little bit more
activity but it really would be 10% if
we were doing a canary build or
obviously some small amount of percent
we'll be able to see what's actually
happening in our define a dashboard
that's talking to Prometheus if we like
what we see we will merge that PR and
it'll kick off another workflow and it's
actually a slightly different workflow
but it is the same Brigade pipeline
that's actually running but it's it's
responding to a different event and in
this case it's actually taking away that
that that other version routing all the
traffic and updating the production and
release with the new one alright and
again we'll see everything in the
dashboard and so forth alright so
hopefully that makes sense and sets the
stage for kind of the demo that I'll
show you here right now so fun fun see
how we how it goes here so this is the
application again you can hold your
applause for my UI design skills the
important thing to note is it this table
is painted it is 25 separate calls to
the API on the back end and that color
that that we're seeing there which is
officially steel blue comes from the API
on the back end it may have hit one of
the different pods that's running but
it's not important which one that it hit
they're all the same version and we do
get some detail back on the build ID and
some stuff around how it was actually
built through our CI CD system so that's
what that's what the home page looks
like in the background I am
curling that page here I'm looping
through it just seeing so we're creating
some traffic so if we take a look over
in Gore fana we can actually see my
dashboard of the activity here so this
is again Griffin our front ends
Prometheus the data got into Prometheus
because we had sto as a side car and
I'll show you where that side car is
running on the top here we have the
activity around the API and on the
bottom you have the activity on the on
the web application and you can see that
we're seeing this red line here which is
essentially all of the traffic is going
to the production release the the graph
on the left is the requests per second
and the graph on the right is really how
long those requests are taking so it
gives us a flavor for what's really
happening here now if we take a look at
our cluster first of all you're gonna
see here that I have some pods in the
background running that are Brigade so I
have I have a few different brigade pods
that are running I have a gateway a
control latern an API and then I also
have the the new cache d web UI running
there as well
and we'll take a look at how those run
here in a moment but just so you can see
the pods that are a part of my micro
SMAC namespace they're running in a
separate namespace here so that we can
automatically inject the sto or the
Envoy sidecar and you can see for each
of those pods we do have two containers
running in them so one of them is the
application itself and the other one is
that sto sidecar that's taking care of
the the routing that we need for this
alright so from a standpoint of Brigade
I mentioned that we had various Brigade
projects so a project ends up being
described as a helm chard in and of
itself so we installed Brigade using
helm and then we install a series of
Brigade projects for for each really
each application that we want Brigade to
do a pipeline for us and really that
project ends up being a series of
secrets or configuration details so if
we actually later take it we'll take a
look at my github repo that's where some
of the secrets are and I don't have that
project file in that github repo very
intentionally alright and we can
actually take a look at the list of
Brigade projects that we have by using
this brig client and we can see I have a
few different projects
of them aren't actually real but we do
have this cube Khan project here and it
was spawned by a yam will file and and
it is my ACR credentials my credentials
for github x' and some shared secrets
and so forth to define how to access
that so what we want to do is actually
go and make a change so I'm gonna go
into the dev branch and make sure I have
the latest and I'm going to open up vs
code and we'll take a look at some
interesting parts of this application
but just from a standpoint of the color
here we're actually going to change the
color from that steel blue to red I'm
gonna get this kicked off and then we'll
take a look at actually what's happening
alright so we're gonna actually kick off
a PR based on that update and then I'll
explain what's actually happening here
so because I kicked off that PR down on
the bottom here you can see some
activity actually happening in the
cluster based on that that github update
we kicked off a web hook that actually
called into Brigade and has you have
this worker that's running you could see
some other Pods startup that are cut
that are prefixed with job Runner and so
those those are the the pods that are
actually running my pipeline and I'll
show you how those are defined but just
real quick and I look at the logs on
this particular Brigade worker you could
see some things that are happening it's
created a doctor pod for me it's now
running that particular docker pod and I
could go in and look at the logs and see
this sort of process that's running but
if we actually go back over into our
code here let me show you what that
Brigade JavaScript looks like so you can
see I have have some some events that I
handle so I have various event handlers
in this case there's a push event
handler and there's a pull request one
so we're actually running this pull
request run right now I've set up some
basic config here this is where I've
actually pulled in my my add row
container registry credentials into my
my code very easy to pull those out of
just the project object here and in
brigades so that I don't have to
manage or think about getting those out
of some kubernetes secret and so forth
and then I set up my Brigade jobs so I
define these these four containers by
creating Brigade jobs and I'm actually
using some JavaScript functions here to
configure how those jobs act that way I
can use these over and over again maybe
the pull request and the merge have do
the some of the same things I can pass
them parameters then I create a pipeline
group and I call run each and run each
ensures that they run in serial so one
happens after the other and if anything
goes wrong it will fail and and be
considered a failed failed build all
right and what we should see at this
point is it should be done with that
pull request and let's go over and see
what actually the application looks like
and we can tell it's done nothing's
actually happening over here and Brigade
let's see what the application looks
like now that it's red now you may
notice when I refreshed it it took it a
few seconds there to build notice that
we're now seeing the PR and we actually
should see more of that 90% of the
traffic appear here if I keep refreshing
it I should actually still see some of
the traffic going to that other version
and so we've actually have deployed this
sort of canary build but it doesn't seem
like it's actually working very well in
fact if we go over to Gravano we can see
what's actually happening so the API
chart on the left up there the top left
we can see that the traffic shaping
occurred so again we used helm to
actually make sure that those sto rules
were changed we do see that some of the
traffic is now hitting this new release
which is the yellow line in the graph
but man the the performance of this has
gone to hell a really slow massive
amount of increase in what's actually
happening so maybe red wasn't the right
color choice and maybe we need to
actually make a change now again we we
may have some sort of automated process
to this again we're doing this in a demo
fashion to actually see everything but
we know that we do have a problem here
so we're gonna go back over to our code
and our API Handler here I'm going to
change to my favorite HTML color you I
guy on our team that again was at Deus
Matt butcher told me about a real HTML
color called Chuck Norris this is
actually a valid HTML color and
so I would imagine if we're having a
problem Chuck Norris can save the day
right so we are going to update to Chuck
Norris and we're gonna update our PR
with Chuck Norris and we're gonna see if
that does any better now again over in
our cluster we're gonna see Brigade
kick-off again we could see the workers
have kicked off and they're gonna update
our PR with this and we're gonna see if
that does any better now in the meantime
but by doing that update I want to show
you what Kosti looks like so I mentioned
at the beginning you saw in this cluster
all the things that we're doing with
containers and all the detail that we're
seeing by a command line but that's all
that would be a hard way to troubleshoot
builds and see what's actually happening
and that's really where cosh T comes in
cosh T is this web UI here that shows me
all of my Brigade projects he saw I had
three projects earlier so that
corresponds exactly to what I see here
and you can see that the cube con one is
the only one that actually has activity
in it and you can see the various events
that have occurred they have all have
separate build IDs you can see you know
how long they ran when they ran you can
even see a fun bug there for the one
that's currently running that one
started 2000 17 years ago funny that the
cause of that is the the different way
that different languages handles sort of
date zero if you want to if you want to
understand the background to that again
just release today
fun to have that the bug and see it
actually in action so if we want to see
the actual history of a build that
happened earlier we have this build
history you can see when it started we
can see when it finished in this case it
passed you can see that it came from a
pull request you can see there's a
commit ID to it if I want to see
individual results you can see how easy
it is to see these various steps these
are the same same jobs that are defined
in that Brigade JavaScript that we
looked at earlier and for example in the
helm in the helm job Runner you can see
the actual logs all encapsulated right
here in this screen very easy for me to
see what's actually happening in that
Brigade process on the back end and you
can see when I go back to my main screen
here that that Chuck Norris pull request
is actually done that's the one that
finished a few seconds ago so we can
actually jump back over and see see how
that went and when I click refresh here
it's certainly refreshed fast
so it's more than likely working
correctly but we still are in this sort
of canary build if you will so we're
gonna go back over to our dashboard and
take a look you can see we're still in
that we're still seeing the traffic
going going to two different releases of
the API but we certainly are seeing
performance back to normal
so Chuck Norris did in this case save
the day and certainly a much better
scenario for our application so it's at
this point when we should go ahead and
merge this pull request and so we will
say Chuck Norris saved the day and again
if we jump back over and we look we
should see activity and Brigade again
now in this case we're actually seeing a
slightly different slightly different
event that's occurring over in this
pipeline and and again this is actually
the one that's calling the push event we
configure really the the details of it
pretty much the same as before but we
are actually configuring each of these
Brigade jobs slightly differently
because it is we're merging back to the
master and if you're paying attention
here at home we're actually saying route
a hundred percent of the traffic to prod
and zero percent to that to that that
that really that's staging or Deb's slot
that was for the canary build and if you
see my my pipeline you can see it looks
very similar to the last one except we
actually are only executing the code if
it's on the master branch and you may
have noticed actually this thing kicked
off if you were really paying attention
when I push something to the dev branch
this thing did run it just didn't do
anything and you might have caught that
that happened if you were really
watching closely the other thing that I
want you to see just from is from an SEO
standpoint if we take a look at the sto
components that are running in our
cluster you can see that the various
components that are in the sto namespace
we do have have the sto control plane
the mixer the pilot there is also the
initializer that's the one that I've
added that knows how to automatically
inject this I configured it and told it
only inject into the micro space or
micros Mac namespace so all of these
Brigade paws that are starting up
they're not actually using this do they
don't need sto for what their work is
done here we do also have the Griffin
out prometheus components as well
running in this namespace they're not
required but they're certainly useful in
this case the other thing that you
should see is the route rule so let me
just make sure I get my route rule name
right so I have a route rule that was
driving some of this traffic and we've
been modifying it using helm and you can
see again this is a standard kubernetes
object it's a custom resource that's
been added to my cluster and you can see
we know that it's already done because
we've removed that sort of canary
deployment and merged that build into
master so we actually see now that a
hundred hundred percent of the traffic's
already hitting this updated release so
odds are when we go back and check our
application we should actually see this
no longer being the PR we should see it
be that master build and if we go back
over to our graph on a dashboard we can
see now that over on the top left there
the API is now shifted all the traffic
back over to that sort of master primary
release we did that at that micro
service level alright so from a demo
standpoint that's what I wanted you to
see you know again we just have one API
in this case you could expand that to
many of different api's and be able to
control them in this fashion we take
advantage of sto to do all the route and
route rules and traffic shaping and we
have brigade to help us actually do our
C ICD process and again costea is this
beautiful sort of UI that gets me a good
look at what actually happened and I can
go back and see the history of these
results and see what's actually
happening with it alright so before I
before I wrap up I do want to say that
just before I came up here I did publish
a blog so that you can go try this out
yourself perhaps expand upon it do
something more with it maybe make a
better UI or a better front-end to the
to the application but it is out on on
my blog which is on medium I'll probably
tweet about it later but I'd
I did post that live I haven't really
tried it enough times to know that it
absolutely works that from the
instructions I know I can get it to work
but did I build the right instructions
for you to get it to work but please try
it out and let me know what you think
about it and certainly uh like I said
I'll say more about it on Twitter and so
forth later but with that I have time
for questions I don't remember how long
I had I have one one minute so if you
have a question please go to the mic</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>