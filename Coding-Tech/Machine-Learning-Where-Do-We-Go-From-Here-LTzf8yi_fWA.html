<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning: Where Do We Go From Here | Coder Coacher - Coaching Coders</title><meta content="Machine Learning: Where Do We Go From Here - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning: Where Do We Go From Here</b></h2><h5 class="post__date">2018-01-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LTzf8yi_fWA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today I'll be talking about where we
are in terms of state of the art and
machine learning and where do we go from
here right so today machine learning
happens at scale you know deep learning
is where there is enormous performance
boost in many domains especially domains
where we have large amounts of label
data and oh wow okay so I guess it's
just sorry about that
and once we want to do machine learning
at scale we want it to be distributed
right so it happens on multiple GPUs
within the same machine it can happen on
multiple machines over a network so how
do we design frameworks that give us the
benefit of scaling out to multiple
machines and reduce the overhead reduce
the communication bottlenecks and at the
same time make it easy for the
programmer to deploy these large scale
models without extensive background
either in parallel programming without
extensive write work because models are
constantly changing people want to try
out new frameworks and most of them only
work when you have this game right it's
not like you can prototype on a small
data set and then move to the larger one
many of them just only work at scale and
they don't work with small data and
small models and so for that you need
frameworks that easily also let you
scale up to a large number of machines
and give you that efficiency and at
Amazon Web Services we always put the
customer first and for us you know
having cost savings for our customers
when they run on the cloud where when we
give them efficient frameworks is of
utmost importance so this is something
while we are passionately working on and
also contributing to open source so I'll
talk about the MX net effort that
enables doing distributed deep learning
at scale and the last part you know
for now scaling out to multiple
dimensions is something at the core of
my research you know for the past five
years or so I've been working on
extending methods beyond matrices to
tensors right so one way to view deep
learning is multiple layers of linear
algebraic operations with some
nonlinearities right so but why do we
only limit to matrix operations you know
why can't we go up to higher dimensions
and what would that entail and so I'll
describe our latest works that brings
the world of tensors and deep learning
together and see like what gains it can
provide in terms of network compression
in terms of being able to exploit
multiple modalities in terms of being
able to do much higher order predictions
so so there's a lot of benefit you can
get by exploiting the dimensions and so
in summary this talk is about how
machine learning today is deep
distributed and multi-dimensional so as
I'm sure all of you have seen deep
learning is very much in the news it's
touched every domain possible and is
going to new domains it all started with
the image understanding domain right so
they imagined that contest is where deep
learning framework for the first time
gave a significant performance boost
compared to the classical machine
learning frameworks and you know two
things were responsible there one was
imagine it was at least an order of
magnitude bigger than the earlier data
set so you had more data and also our
compute technology drastically improved
in the last decade so you had these
modern GPUs that could run these
operations at scale and since then we've
made rapid progress you know we have now
deep learning systems deployed
almost universally including at Amazon
Web Services we have the recognition API
for computer vision and since then we've
also moved to a number of other domains
right so speech recognition deep
learning has given enormous boost
natural language processing is a much
more challenging task especially because
getting expedited data is expensive but
even there we are making progress every
day and lastly you know autonomous
driving or any application that involves
deep learning in a control loop which
involves decision making is one of the
most challenging tasks and again there
we are making rapid progress in deep RL
and safe RL so you know lots of exciting
areas to work on indeed the canonical
Hardeep network has these stacked layers
of operations and if it comes to image
recognition you start out with
convolutional layers and then you move
to these fully connected layers at the
end and the intuition is convolution
operation enables you to exploit the
spatial information efficiently because
it extracts features in a translation
invariant manner so it doesn't matter
where the object is in the image it
extracts the same features and so this
is the network that you are all aware of
what I want to spend more time talking
about is how do we deploy such networks
at scale right so the network that I've
shown here is the inception v3 Network
one of the state-of-the-art networks
right so you can see that it has more
than a hundred layers it has you know
somewhat complicated set of loops and
branches so there are all these
operations that happen in fact a billion
operation per image so this is not
something you can program from scratch
so that's where a lot of effort has gone
in the recent years to develop
if you
and software frameworks you know that
make it easy for programmers to easily
write down such a complex architecture
at the same time is efficient you know
as you scale up these networks as you
include more and more GPUs you get the
corresponding benefit in terms of
throughput right so ideally you want it
to be linear scaling so you double the
number of GPUs you want the throughput
to double does that happen in practice
so what are the frameworks that enable
that so so that's the second
consideration and the last one is
portability so we want now the
flexibility of having deep learning on
multiple different platforms according
to the needs including deep learning on
the edge so AWS has AWS Greengrass for
enabling iot and we are always looking
for efficient frameworks for doing deep
learning on the edge and MX net also
offers good support in that space and so
that's the idea like how we can enable
these large networks make it easy for
the programmer it get high efficiency so
MX Net the name stands for mixed network
or mixed paradigm of programming the
idea is you know it started out by
thinking about the two paradigms
imperative and declarative style of
programming and it turns out imperative
is Python like you just write what you
need to do right line by line and that's
the most natural way for humans to think
and easy to code but it doesn't give you
good performance out of the box and
that's what declarative programming was
preferred in fact some of the earliest
packages started with that including
transfer flow started with declarative
programming where you first declare the
variables you know you talked about the
relationships between them and then now
you know you can instantiate that right
and declarative programming is quite
important if you want to do
parallelization so first of all
how you can do parallelization given a
declarative program that generates the
symbolic graph and then show how MX Net
now has a new imperative API that makes
it easy to write imperative code but you
can generate declarative code by just
writing the imperative one right so this
example is a toy example of two GPUs and
a CPU and you want to run now you know
you'd want to do data parallelism you
want to split up your data into these
two GPUs run your forward and backward
pass aggregate that gradients out in the
CPU right so you can see that even just
this simple Network and just two GPUs it
gets very complex right so I'm making a
point here that the classical frameworks
like MPI that supported parallel
programming are no longer you know the
right framework for deep learning
because they're too low-level and
complicated right so one feature that
I'm extranet supports is given this
symbolic graph of computation it Auto
paralyzes so meaning you just write your
code it figures out give as it traverses
the graph that in this simple example
that these two parts of the program can
be done in parallel so it Auto paralyzes
so this removes a lot of effort from the
programmer to specify such operations
and then the other part it supports is
an efficient parameter server right so
let me show what it means in the context
of data parallelism so data parallelism
is the simplest form of parallelism
because what you do is you take your big
batch of data and then divide it into
each of the machines right so each
machine has its own mini batch to
process and after processing that mini
batch
what it does is computes each GPU in
this example computes its own gradient
and then pushes it out to the central
server right a key-value store which
aggregates it and then Bob pushes it
back to each of the GPUs
so you have this iterative operation at
the end of each mini batch happening
right so as you can see this is
communication heavy and that's what
makes it challenging that's what makes
it different from the traditional
MapReduce operations that were meant to
be doing one time rather than this
iterative one and so what MX net does is
reduce this bottlenecks in several ways
so one approach is to do it in a
hierarchical way right so where you
place the parameter server is important
and if you are operating on GPUs you
want to designate one of the GPUs itself
as the parameter server the level one
server because if you see the most of
the communication happens is in terms of
the individual GPU sharing their
gradients and you know by having it here
you reduce the communication bottleneck
significantly because if you see the
bandwidth for GPU to gpo communication
is much more when it comes to CPU to GPU
communication and indeed with the latest
technology this is even getting faster
and so by moving most of the
communication to within GPUs and the
computation to GPUs we are able to have
much better efficiency and speed so let
me show you just one experiment on how
MX net is efficient at scale there is a
github repository with a whole list of
experiments and details so you're
welcome to visit that and so here what
we did was we took the ec2 P - 16 X
large instance these are the k40 ioan
video GPUs in fact we have the P 3s
recently launched which are the alter
GPU so it's very exciting that we have
the latest GPUs available on the cloud
but these experiments are still older
so in this example what we did was we
kept the mini-bat size in each GPU to be
constant and then we kept increasing the
number of GPUs and so and what we wanted
to see was how much time was spent per
batch right so ideally this curve should
be flat because you just keep adding
your GPU so yours
ideally linearly scaling your throughput
and adding no overhead right so that
would be the ideal scenario and that
would mean it's flat and you can see as
the batch size increases you do have a
lower overhead you know it's more
efficient and that's because you are
reducing the communication bottleneck
the fraction of communication cost is
reducing as you increase the mini batch
size and what we've seen with MX net
experiments is overall you get about 91%
efficiency that's the you know highest
efficiency recorded in these frameworks
and also you can we had larger
experiments you know in some of the
other experiments we even went up to 256
GPUs and it only reduced to 88%
efficiency so the idea is you have you
can seamlessly scale up to multiple GPUs
within the same machine as well as to
multiple machines and retain a high
efficiency and so you have enormous cost
savings if you are running these large
scale models and especially if you're
running them continuously right so the
latest framework within MX net is gluon
so gluon is the high level api with a
maximum back-end that we recently
released a few months ago and what I
would argue is it tries to combine the
best of features from all the previous
frameworks it is the latest one to be
released and so what's special about
gluon is you can write simple imperative
code right so in this example what I'm
showing here
you're initializing two arrays you're
doing simple multiplication and addition
right so if you had only this imperative
program you couldn't generate the
symbolic graph the computation graph
that's needed to say what the set of
operations are what the variables are
but what MX nut with glue on does is it
automatically generates the symbolic
program so this program a hybridization
process gets you the symbolic program
for free and also does many automatic
optimizations you know I talked about
macho parallelism but in addition it
does memory optimizations so for in this
example here you know you're multiplying
a and B and then you're immediately
summing up so this intermediate result
does not need a unique chunk of memory
right it can be overwritten
so all these optimizations can be easily
done once you have the symbolic graph
and there's also more complex operator
fusion that would give us further speed
ups one cell we have this symbolic graph
so it has a lot of optimizations built
in for memory for efficiency and at the
same time it's easy to program yes so to
summarize gluon combines best of both
the worlds it's easy to write an
imperative style and at the same time it
has the efficiency of symbolic programs
so I want to now break from the slides
and show some notebooks on
puron so this is a set of tutorials the
primary contributor is Zachary Lipton on
my team who is also finishing up his PhD
from UC San Diego so you can see here
there is an extensive set of tutorials
you know it by the way whether you're a
beginner or an expert I think you will
find good use of these tutorials for
beginners there are many you know
options from scratch so you can either
try out building a
multi-layer perceptron Network from
scratch or directly and gluon and see
the difference like how it's just one
line or a few lines in gluon but at the
same time if you want to dig deeper into
what each of these principles are so
there is the option of also looking at
the code from scratch so you have this
almost two facets of options one for the
experts one for the beginners and so
that helps you also quickly get up to
speed so what I want to show here is how
easy it is to get high-performance and
run multiple GPU and multiple machine
code so I won't run the notebooks
because of the you know I want to cover
on tensors as well but what I want to
show here is you know so you're right
you're you define your network here
right so you're defining now a
convolutional layer max pooling layer
and flattening it doing a dense layer
and if you want to now we initialize
this network on multiple GPUs it's as
simple as just these two lines of code
right so multiple GPUs on the same
machine all you say what your context is
what your GPUs are and then you're
initializing the network to collect
parameters from these GPUs right and
what you do is and to split the data all
you have to do is have this instruction
and so you can see that everything
happens in the back end and so it's
really like whether it's one GP or
multiple GPUs in the same machine right
it's the complexity is almost the same
so training with multiple machines is a
bit more involved but not by much so as
I said the key value store is the
parameter server if you wanted it to be
in a single machine you would create it
as local but if you want it to be
multiple machine you create a
distributed key-value store and then you
want to specify where the pull and push
operations are so I won't go into the
details here
given this tutorial you can see that
it's also very easy to scale it up to
multiple machines so the last part I
wanted to show is how to convert
imparative core to generating symbolic
graphs in gluon and the benefit of this
is portability right because once you
have symbolic graphs you can run it
across different platforms and so in
this case again we are taking a network
in this case multiple layers of fully
connected networks and as you can see
this is you know being initialized so if
you do it in this way this is still an
imperative program right you're writing
a set of instructions and you're
initializing your network and you know
if you output what the network weights
are it is just what has been initialized
so now instead you can convert it into a
symbolic program by saying that door
divert dies so just this one line of
instruction will get you the symbolic
graph and you can see this hybridization
process even in the simple case has a
significant performance boost because
once you generate the symbolic program
then the backend can do all the
optimizations for speed and also
efficiency and so getting the symbolic
program is now you know you can output
the JSON file and you can see like what
the different operations are what the
nodes are so you can now have this be
portable and so gluon enables all this
to be done now quite seamlessly so I
encourage you to look at the notebooks
for you know it's very detailed and also
contribute to it we have over 50
contributors we've had more than 200
thousand page views so it's a very
active community that's forming both
here and also in China so that is
you know something that I would like
more people to participate in because
also because one of our goals at AWS is
to democratize AI and have it be
accessible to everybody on the planet
and this is one of the you know thoughts
going behind open source frameworks and
having educational resources so now let
me switch gears and say you know deep
learning is great but where do we go
from here right so what are some ways to
take deep learning from where it is to
the future and I would argue that
incorporating multiple dimensions is a
way to take it to the next level and I
also want to call out that later in the
day Tammy colder is giving a talk on
tensors and she'll bring a perspective
on tensor decompositions so I'm happy to
see more tensors come to the mainstream
right so indeed the world around us has
tensors right so this is a toy example
of how you want to think about tensors
you know we start from scalars which are
single numbers two vectors right one
dimensional matrices are two dimensional
and tensors involve higher dimensions
and if you think about our data right
how we express it it's inherently
multi-dimensional so even in the
canonical example of deep learning for
image recognition what you have is an
input is an image with width and height
and typically RGB or some number of
channels and as it goes through
convolutional layers you have you know
these number of channels increase and if
you're doing spatial pooling you have
the dimensions decrease right so you
have you know if you want to think about
deep networks in terms of tensors one
way is to think of it as a box that is
operating on tensors right it's
transforming in ninpo tensor into an
output tensor and in most cases that
output tensor may be just away
if you're doing image recognition but if
you're doing pigs to pigs you know that
it's itself could be also a
three-dimensional object so thinking of
it in this way we want to then ask if
you are transforming one tensor to
another through a deep network why not
incorporate operations on tensors in all
the layers alright so that was their
thinking so the first thing we wanted to
do was what are these set of operations
how do we extend matrix operations to
tensors so the primitive for that is
extending matrix products to now tensor
products alright so this picture here is
self-explanatory if you want to multiply
a matrix with a vector here you take
linear combinations of the columns and
now with the three dimensional tensor
you have options to multiply in multiple
different directions and in this example
now I've contracted along two of them
and you can see that it's taking multi
linear combinations of the fibers right
so you can now operate on tensors in
different ways you can now you know
operate along the individual dimensions
you can even operate jointly along
multiple dimensions so there is a very
rich set of operations that we can do on
tensors that just you know is a it's a
very smaller part of it that can be done
on matrices so tensors really expand the
transformations we can do from our input
to the output or in these deep networks
and so what we started with was one of
the oldest deep networks that's the Alex
net right so as you can see here you
start with the RGB image and then you go
through these convolutional layers that
are increasing the number of channels
and doing spatial pooling but what
happens is once you come to the fully
connected layers right this is where you
lose the three-dimensional structure so
in the traditional alex net two what you
do is you convert this three dimensional
object into one dimensional vector you
flatten it and then you send it through
a matrix product operation
some nonlinearities in multiple layers
and then output the final vector and so
the first thing we said was why not
retain ten search operations throughout
the network end-to-end right so we keep
the three-dimensional structure and
let's see what benefit that gives us and
so the operations we added were the
tensor contraction so once you have the
output of this convolutional layer you
don't flatten it but instead you
contract it across the multiple
dimensions and the output is still
another tensor typically of a lower
smaller size because you're doing in
these layers you're either retaining the
size or shrinking the size right and so
you can do multiple layers of that with
non-linearity and we wanted to see what
benefit this would give I mean this is
just a symbolic graph in MX net if you
want to apply it through matrix products
by doing transpositions and so what we
found was with vgg and alex net we got a
huge amount of space savings have well
having almost no performance drop right
so as high as 65% and space savings in
these fully connected layers and so the
idea is by you know simply just
retaining the three-dimensional
structure right so that's all with it
you can get a huge space-saving and a
compressed network right so that says
like the power of dimensionality that we
seem to be missing in many of these deep
networks and so many of my recent
research has been to ask from here where
do we go and try to exploit multiple
dimensions in these networks so the next
thing we did was let's also see at the
output layer how we can treat it as a
tensor operations because these tensor
contraction layers were the intermediate
layers that were replacing the fully
connected layers so in the last layer we
want to do a regression like operation
because we have this contracted tensor
the activation tensor as input and then
there is the weight set of weights and
typically when you do this in a product
right so you get each coordinate of your
output vector that's the softmax and so
now what we did was we also parameterize
the weights in this last layer as a low
rank tensor so if you parameterize the
weights directly as a low rank matrix
the performance drops significantly
I mean people have tried it that's a bad
idea but instead if you instead think of
it as a tensor and I expressed it as a
low rank tensor a low rank tensor will
be a full rank matrix in general so it
has much higher representation power so
you can reduce parameters without
compromising on the rank of the
effective matrix and so that's the power
that again allows us to significantly
compress the output layer as well so as
you can see up to like 65% or 70% there
is hardly any drop in accuracy and so
you can reduce the network size
significantly even in the last layer and
so this framework and end-to-end
operation of tensors you can train it
end to end and get this savings in space
so you know so I wanted to now ask so
given that tensor operations we believe
are the future in deep networks we want
to like now also go level lower and
think about optimizing for these tensor
operations effectively so this is a work
I did with n media researcher Chris
Sacca and the question we wanted to ask
was even the simple operation so you
take tensor you multiply it with a
matrix on one of the directions right
there's a single index contraction I
mean if you do it try to do it naively
you lose out quite a bit in fact and
some of small to moderate sized
SURS the way you would have to do it is
to transpose both the transfer and the
matrix transpose the output for
different you know if you want to
contract in different directions and
this losses you know you spend a lot of
time just doing transpositions and also
you're doubling or tripling your memory
requirements right because you're doing
explicit copy and what we've seen with
all the development of matrix libraries
is we have in place transpose
meaning you don't explicitly transpose
when you are doing matrix product if you
type the command a transpose B there's
no a transpose explicitly formed right
so our goal was to have similar
capabilities for tensors as well so can
we just do this in place can we try to
do it at the same cost as a single gem
operation that's a matrix multiplication
operations and avoid these
transpositions
so I'll skip the details for the lack of
time but to what we found was we could
use existing primitives some when there
is something known as batched general
matrix multiplication primitive and then
we could specialize it by fixing this
right I mean those are details but what
we saw was with that you can nearly
match the performance of a single gem
operation so you almost get the same
efficiency as matrix multiplication
operation but now this is much richer
right so this is contracting a
three-dimensional tensor and so to me
this is just the start so if you have
expertise in this area if you want to
build efficient tensor kernels right so
to me tensor kernel is more than just an
Eric kernel so I I know the word tensor
gets thrown around a lot to me like
tensor algebra where you're doing
contractions is a tensor object is very
different from treating it as a
multi-dimensional array and so these
operations are not present in any of the
existing libraries although the name
tensor is very much attached to you know
many of these and so that's where there
is a need for extensive library
development or in this area
and so what I want to show and so we can
get good space-saving
I mean savings in time with this so what
I want to quickly show is in terms of
library development one of my interns
are John Kesava developed this stencil
e-library and I've been in one census
internship and this one you know allows
you to code in Python and recently we
also added MX net and PI touch back-end
so you can easily combine your deep
learning with traditional tensor
operations so there are again notebooks
on the repository you can go and run
them and so I want to show you how easy
it is to now also incorporate the tensor
contractions and regressions that I
described previously and so here the
first part is just importing the
libraries you're loading your data and
so this is the new part right so you
want to describe the tensor regression
as a new layer in your network and so
you can easily do that as a gluon block
so the part where the tensor operation
is incorporated is first you define what
the core and the products are because if
you recall this was a Tucker form for
the weight matrix and there is just this
one operation so what you do is you
transform your Tucker form so the Tucker
form involves keeping a cold tensor as
well as its factors and in the tensor
form you multiply it out so if you do
that you can obtain the weights of your
network and you can also back drop
through this yes so by combining tensor
Li package with MX net and PI torch
backend now you're free to try a variety
of different tensor architectures I want
to show you one other example that
involves visual question and answering
so in this case the task is to take an
image right there is a question about
the match you want to answer that so
again this notebook is available with
the gluon tutorial so
you're high you know you should go and
check that out in detail there and so
here what we do is we want to combine
let me first describe it in the slides
so you want to combine the image
features and the text features together
to answer the question right because
once it's a question you're processing
it either through some sequence model
RNA and LS TM right so some model to get
a text feature and you're also
processing your image to get an image
feature and now you know you need to in
combine both of them to come up with the
answer all right so the traditionally
the simplest approach people took was to
just first flatten this right either do
average pooling of this get a vector out
of this three-dimensional object as well
as take this vector concatenate them so
just put them together feed it to the
network after that right but it turns
out you can get a significant
performance boost by sketching on these
objects so a new sketch we proposed was
to treat this you know there's a three
dimensional tensor in this case it's a
one-dimensional tensor from text so can
we pull them directly can we take tensor
products of these to express higher
order moments over image and text
jointly and at the same time we don't
want to blow up the dimensions we don't
want this network to be too huge because
that may overfit that may you know
that's also hard to train so instead we
can do efficient dimensionality
reduction and that's where the term
sketching is saw incorporated so the
details are again in the paper but the
idea here is there is a notion of a
sketch on the tensor and your text
vector and you can pull them together in
their fifty domain it turns out that
this is gives you properties of the
count sketch the traditional count
sketch but aren't answers now so there
are interesting again research as well
as development in terms of how do we
compress these large multi-dimensional
data sets as well as incorporate higher
order moments in this data set right and
so these are interesting problems of
research and so there's also a notebook
and gluon that you know gives
step-by-step explanation of how to do
the pooling how to you know construct
this network how to train it I encourage
you to go and look at the details there
and so the last part now about tensors
right so far what we saw about tensors
is you can contract tensors so you can
take the tensor multiply it across
multiple dimensions so that trend
transforms an input tensor to an other
output sensor you can do regression on
tensors so that way you can express your
input and you have a weight matrix and
get the output and you can also use
tensors to pull across different
modalities and get higher order moments
of your data but I also get efficiency
so the last part about tensors is how
you can decompose them efficiently right
so just you can decompose a matrix into
its low rank components you can
decompose the tensor in multiple
different ways sign to its low rank
components I described the tucker form
earlier this is called the CP form and
Tammy colder will give you a lot more
details on how to work on tensors how to
do tensor decompositions what I want to
point out as some of my previous work in
wall transfer decompositions
and one application was topic modeling
so you have a huge corpus of documents
without any labels right so for this
unsupervised learning task
how can you detect topics automatically
in this article here you can see that
you know we can immediately parse this
article as being about University being
about crime right being about sports but
can the machine also automatically do
this and so we had a framework that
involved higher order moments of text
frequencies so you know in what it was
was looking at Corcoran's
multiple words in a text document like
how frequently do these words Corker and
decompose that large tensor in an
efficient way and be able to discover
what the topics are so you know we these
are dated experiments we also have new
experiments and what the benefit that it
gave us was very good perplexity or
likelihood of getting a good model from
the Stata set at the same time very high
speed and so tensors have the potential
to take matrix operations to the next
level while retaining speed and
efficiency yeah so in conclusion I had a
breadth of topics in this talk you know
we have deep learning revolutionising
many fields and to enable it it scale we
need distributed optimization so MX net
offers high efficiency across multiple
machines and at the same time ease of
programming so that you can directly
start running on this large cluster and
the gluon framework makes it easy to
just write imperative code and get
symbolic programs and have portability
and tensors I would argue are the next
level in deep learning in that you can
now generalize the operations to
incorporating tensor contractions
regressions tensor sketches and exploit
to multiple modalities and dimensions in
your data thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>