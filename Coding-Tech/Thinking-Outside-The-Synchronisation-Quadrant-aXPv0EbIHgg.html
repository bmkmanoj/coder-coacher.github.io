<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Thinking Outside The Synchronisation Quadrant | Coder Coacher - Coaching Coders</title><meta content="Thinking Outside The Synchronisation Quadrant - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Thinking Outside The Synchronisation Quadrant</b></h2><h5 class="post__date">2018-01-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aXPv0EbIHgg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this talk is entitled thinking beyond
thinking outside the synchronization
quadrant so I need to show you what the
synchronization quadrant is and if
you're a simple quadrant diagram
quadrant diagrams are great because you
know they kind of divide the universe
into four there's two axes you know and
there's only two things on each axis and
normally there's one quadrant that's
very very good or one quadrant that's
very very bad and it kind of simplifies
things for everybody and so what I've
done is I basically characterized the
relationship between basically code data
code and data or other code in threads
and the data in threats is the data
mutable can I change that data if the
answer is yes then it is mutable if the
answer's no it is immutable okay is the
data shared between different threats if
the answer is yes then it's shared if
the answer is no it's unshared okay and
so very simple idea and and so therefore
we kind of create this simple map now
just to clarify for you where the bad
stuff is where the good stuff is 3/4 of
the diagram is really easy to program in
1/4 is where everybody programs in all
the pain you are feeling is in the top
right hand corner what are you doing
there get out of it ok this is the
synchronization quadrant this is where
in your code you will need
synchronization now I'm not saying you
don't need synchronization in the other
cases what I'm saying is that that
synchronization is hidden that
synchronization either does not exist or
is hidden and commoditized from you in
the top right hand corner you have to
care about it and the other three
quadrants you either do not need it or
you don't need to worry about it ok the
top right hand corner is the
synchronization quadrant now how do we
end up with all our code in the top
right hand corner this is where all the
pain happens I mean this is when you
talk to people you sort of say why
you've got locks if you wanted
performance and they say well safety and
they say well why did you create Unser
code yes it's a so it's not always clear
there's an assumption that people have
and I often ask people and in fact I did
this with one team when I did some
consultancy for a few years ago and they
said I said so hang on
explain to me and this is a kind of a
good trip you gotta be careful with it I
played dumb which you know my kids think
is quite easy for me to do yeah my older
boy is a teenager so he things anything
that comes out of his parents mouths is
in the is automatically stupid
so I played dumb I said so why have you
got all these locks and they're kind of
looking at me well we need locks because
we've got threats well yeah but why do
you need the locks well because we're
sharing data I said I still don't need
to still don't see why you need the
locks and say well you know because
otherwise it's unsafe well why is it
unsafe you're sharing data between
threads there's nothing unsafe about
that and they're really looking at each
other yeah but what if the data changes
it's like oh hang on where did that
piece of information come from that was
the most important thing you told me yet
it was the last thing you told me you
opened with we're using locks you did
not open with we have mutable data let's
challenge and question that okay there's
so in other words it's so deeply
embedded people people think that when
you have data you're allowed to change
it where did that idea come from
changing data is a privilege it's not a
right okay you have to make a really
good case for it I would actually say
that about sequential code anyway but
the question is how do we end up here
and it's very easy to see how we ended
up here because historically where did
all code live before threads became
common all code was by definition on the
left hand side in the unshared thing
because if you only have one thread you
cannot share it and so therefore it's
automatically safe although sometimes I
see I see a lot of code where actually
one thread is one too many
but that aside the point here is that
over here it becomes a matter of machine
efficiency in history and historically
we have chosen to be very imperative
about how we view code and there's a
very very good historical reasons based
on resource availability compiler
technology and so on
so naturally we didn't just gravitate to
the left hand side we gravitated to the
top left hand side so when we added
concurrency all we did was move right
from where we were
and unfortunately that took us straight
into the synchronization quadrant so I
want to start looking at the properties
of a system to really try and understand
where where it is that the presence of
concurrency and surprises and our
assumptions of threading and locks make
our life difficult and are also
inappropriate so systems have properties
this is a very trivial observation and
we can use other words you know when
you've got a when you're dealing with
the system you go by its capabilities
its features its characteristics however
you want to frame this and it has these
inside and out and importantly what I
want to characterize here is the idea of
the properties of the running system and
the properties of the code base the
inside what are the prop what are these
properties now yesterday I presented a
very simple way of looking at the
properties of a system quick show of
hands who is in my talk yesterday ok so
a reasonable number of you so you've
seen this before
it's these fingers okay for those of you
who are not there as a simple right-hand
rule for organizing the properties of
your code to get us away from the
utterly pointless members for combative
non-functional the point is we have
functional properties the semantics of
our code okay what it does and also this
is where the functionality bugs live if
you have a bug it's a deficit on your
functionality axis we promised you this
functionality but you've got this
there's a shortfall ok the operational
axis how it runs its performance
characteristics for example and then the
developmental qualities tell me about
the code is it easy to be in in the code
is it easy to work with the code does
the code resist your changes that's
where so we want architecture to enable
the
support that and allow us to live in
there basically so as the good each axis
in turn from the point of view of
concurrency in our threading assumption
so first of all let's look at
operational and there is a there's this
very simple idea
operational is concerned with the
constraints the runtime so Shakespeare
had this one down in Troilus and
Cressida turns out Shakespeare was
actually a software developer a few
centuries ahead of time admittedly but
he carefully wove his software metaphors
into his plays okay some people think
they're just plays look closely look
closely he was talking about software
and here he says this is the monstrous
monstrosity and Lovelady that the will
is infinite this is it yeah we want to
do so many things we have such great
imagination all these possibilities and
the execution is confined we are
frustrated by the finite limits of our
machines but also those annoying things
that they call the laws of physics those
get in the way every now and then it
turns out that the speed of light is not
a configuration value that you can
change okay that's disappointing
and the desire is boundless what we want
to do has no has no end potentially and
the act of slave to limit so this is
what the operational axis is about now
first of all let us understand when
people introduce just basic threading
let's just take just basic threading
there's a very simple idea the idea of
multitasking pre-emptive multitasking
we can confine ourselves to the idea of
a single call for a moment and there is
even in the single-core era people were
still using concurrency and there are
reasons for using concurrency but they
are not primarily performance they are
actually decoupling between temporal
orderings in your code but there is a
cost it is also a cost that we bear
multi-core systems and people often
overlook it and actually one of the best
places I found that described this was
thing called the Cluetrain manifesto
from the late 90s this was actually
nothing to do with what I'm talking
about directly this was to do with
really rethinking business online and
it's a surprisingly good volume but
there's a point in here that is made
specifically by David Weinberger
and he's talking about multitasking in
humans but he's also it turns out what
he says is directly relevant to our
perceptions of multitasking within our
operating systems multitasking is really
just rapid attention switching humans
can't really do this okay you'll rubbish
at multitasking in fact it turns out
there's research that demonstrates that
if you think you're good at multitasking
you're actually worse at multitasking
than somebody who doesn't think they're
good at it okay so if you ask somebody
on your team so anybody here good at you
know multitasking lots of things I've
got so lots of bits of the work whoever
puts their hand up don't give them the
work okay so but it's rapid attention
switching what you're doing is you're
focusing your executive is focusing on
that and then it's focusing on that but
he as he observes that'll be a useful
skill except it takes us a second or two
to engage in a new situation in a new
situation we've grace with our focus
this is exactly the same with a
pre-emptive model it doesn't just cut
from one to another there is a cost and
we need to respect that and that is also
the case and we need to keep this in
mind in any system where you've got
threads and just because you've got
multiple cause they don't run somehow
magically independent using magic okay
there's there's a there's a few costs in
there as well
and what he says that the sum total of
attention is actually decreased so
there's a cost we have to work out
whether we're getting more for our
effort than not so there's a trade-off
and that's the important that's the
important phrase there's a very common
English phrase trade-off but we often
forget as a word what it actually means
it means you're making a trade and when
you make a trade you want to make a good
trade I'm giving you this what do I get
in return
yeah I'm paying you a little bit of time
for context switch but and and the costs
of threading do I get something better
in return if the answer is yes good
trade if the answer is no what are you
doing okay as he says and I love this
slicing your attention in other words is
less like slicing potatoes than like
slicing plums you always end up losing
juice that's a really good way of
looking at it there's always some kind
of loss you have to offset it so a
recent recent blog posting from the RIT
here I thought this is rather good
because it says this is a logarithmic
scale
not all CPU operations occur
obviously the he's talking about a
number of ranges how much this stuff
costs the link is lost at the bottom but
if you google this you will you will
find it but the things I wanted so
basically the relative costs of these
things he also shows distance which
light travels while the operation is
performed his laptop is clearly smaller
than mine because this is what I I
normally use this as the standard
measure this is one light nanosecond
from here to here okay which normally we
think of the speed of light is being
very fast but one light nanosecond that
gives you a cycle time of one gigahertz
which doesn't sound particularly fast
when you put it like that so he's kind
of shown us the relative cost now the
ones I want to draw your attention to
are these two thread contacts which the
direct costs and thread contents which
the indirect costs it is not cheap it is
not cheap so you better be getting a
really good deal okay
this is one of those things of like is
it how long is it good it's one of those
things I'm going to walk down the road
where what are you doing I'm going to
the shop where how far is the shop 200
meters okay so where you gonna walk well
now I'm going to take the car it's going
to take you longer to get in the car
drive down the road find a parking space
get out the car than it is to actually
walk that doesn't mean the car is a bad
idea it's great if you're travelling
multiple kilometers but a few hundred
meters not really worth it
so there is this idea we always do
trade-offs we just need to keep thinking
of that one we're working with code and
there are a number of other costs let's
just say we start dividing we start
thinking well I've got multi cores and
so on I'm going to run my threads well
it's a interesting I I stumbled across
the fact that I had slides for this but
I from a slightly different context but
it turns out they serve exactly the same
purpose so how long does something take
to do this is a completion time for a
single thread t1 okay so let's try this
what we can do is we are going to divide
division of labor so we're gonna divide
up the tasks across multiple threads
brilliant we've got end time speed-up
fantastic actually it doesn't work like
that it turns out that it's strictly
dependent on a number of factors
including the portion in parallel that
you can actually achieve
okay this is and our slaw
okay if it turns out that you can't
paralyze everything and the nature of
every task cannot be paralyzed sometimes
you need to do something before you do
something else there's got if there's if
there's lots of hand off some weight
states then this starts going down okay
and then there's another one which
actually I have to radically simplify
because it is more complex but we're
gonna do is I'm gonna give you a basic
worst-case scenario okay the worst case
scenario here is that we're going to
imagine all the threads are talking to
all the other threats now we might say
that's terrible well yes it is but that
doesn't mean it doesn't exist I'm sure
they're you know I've seen code that
does that and you may have encountered
it in other words it turns out every
thread is busy talking to all the other
threads for something it kind of emerged
by the way that's a point about
architecture significant design
decisions aren't just the decisions that
somebody called architect makes it turns
out that we make significant design
decisions in the detail of the code all
the time okay so it turns out you may
accidentally end up emerging with this
kind of architecture that is more of an
ad-hoc architecture than something
planned and intentional and turns out
all the threads are talking to all the
other threads oh I need something from
there and I need something from there
and so again there's different
communication costs so I'm going to say
happen I have to average it down to be
something typical okay is more a
function than a constant but if we just
take this as representative a typical
communication overhead that is the cost
of waiting it is the cost of all of the
switching and the actual locking and all
the rest of it and any data transfer
that we're involved in okay and that
gives us a curve that looks like this
and this is kind of important because
what it tells you is that every tasks
for a given architecture for a given
software architecture and given hardware
architecture and operating and so on and
so on for a given task you may have an
optimal range in other words this is the
sweet spot for concurrency the task time
to get everything done is minimized if
you have fewer threads through your
cause then it takes longer and obviously
if you have no if you have no threads
executing ie you're not running the
program it'll take infinite time but at
least you are how many bugs
but the important thing is that as you
start adding stuff the costs can add up
and they can end up over here so
sometimes we find people over threading
it turns out that they're slowing things
down by changing a couple of the factors
on the right you can bring this one down
but it's very difficult to get something
it's perfectly scalable yeah so here's a
very simple example if we take a sort of
MapReduce model so just just local
threading I'm not gonna do MapReduce
across a network we have the idea of
okay I've got a number of tasks let's
say I'm go it's right over begin to end
maybe that's a range from a vector it
doesn't really matter but these are all
things that are functional objects
functional in some way so what we've got
is we've got all of these tasks to be
done and then we're going to aggregate
the results so first of all we're going
to kick off with getting our threads
ready we're going to launch all of the
tasks in their own thread so we do that
we build that up
bang-bang-bang hit them all off and
we're good and then we're going to wait
for everything okay so what we've got
here is there are some costs the cost
we've accumulated is thread start and
also waiting for everything okay not
just the cost of a join but different
tasks we'll take different times so
we're waiting for everybody to finish so
there are some costs there we've already
potentially lost a little bit of the
value of P here but now we're going to
reduce everything we're going to bring
everything together how that's handled
is handled by the caller but this part
can't be done in parallel so the idea is
that P is going to be you know that's
one minus P and then this is effectively
the parallel part but even then we've
actually got some costs in there and
notice these threads are not talking to
each other and we still incurred cost so
we better have some really good stuff
going now it really is interesting when
you do this it shows immediately how
many calls you've got if you've run this
on a machine with two cores then you get
a sweet spot at two four runs slower
okay and this was a big surprise to some
people that I ran around a workshop for
a couple of years back you know they
kind of like you could tell the number
of cores and ask people okay let's
increase the number of cause increase
the number of threads and then find out
where you're best before
one says and it was really obvious going
around the room you know that the
numbers work I like two and four eight
and one kit so it's really quite obvious
in this case and this is truly
independent tasks that actually don't
have to communicate except for
synchronizing for a juice the other
thing is there is this assumption that
people go oh yeah we've got all this
stuff and so on and then they it gets
interesting particularly when we start
using the concurrency of the network and
there is a gain an assumption that needs
to be challenged it's most obviously put
in this blog from last year by Adam
Drake command-line tools can be over 200
times faster than your Hadoop cluster
this guy rewrote he basically showed
just that constant reminder the speed of
light you cannot beat it okay
very simply one of the things that has
changed and why you may want to consider
given that distribution is an extension
of concurrency why you want to consider
things locally again is that we now have
more memory available previously one of
the reasons people would distribute
stuff is because they wanted more CPU
and more memory you now have more CPU
and more memory but it happens to be
within a light nanosecond so use it okay
it turns out this guy was able to use
shell tools locally and it was faster
than using a JIT compiled language
across the network so always keep that
one in mind yeah it's not unconditional
that things get faster when you thread
them okay so what about the functional
axis the functional axis well perhaps I
should be talking about functionality
and I don't want to talk about the
functionality I'm going to kind of
assume that that's an obvious thing what
I want to talk about is what I referred
to earlier the deficit of functionality
bugs where do bugs come from this is you
know on the origin of bug species where
do they come from
do they sneak into your code in the
middle of the night yeah the code was
fine when I left it but overnight they
crept in and they're just oh there it is
there it is John core might read this
really nice blog a few years back
functional programming in C and C++ and
a large fraction the flaws in software
development
our jutsu program is not fully
understanding all the possible less
possible states that code may execute in
state this is to do with again this idea
of the the data in your home the data
within your program the state of
execution and it's not whether or not it
is possible to do something is whether
or not it is possible for you to
understand it this is what this is what
characterizes the developmental side of
things is we're going to get to it
but can I understand it and the problem
is that if you if we have lots of state
change then it's harder to reason about
this if we have global state it's harder
to reason about this and as he says in a
multi-threaded environment the lack of
understanding and resulting problems are
greatly amplified almost to the point of
panic if you're paying attention I like
this this point because he he uses
exactly the same wording that I've often
found myself using by saying that many
of the challenges that we have in terms
of things like defects or reasoning
about state it's we already had problems
but concurrency and distribution are
amplifiers they take any small
misunderstanding and really give it far
more severe consequences and these bugs
are quite difficult to track down
they aren't quite difficult to work with
so here's the problem you know what is
the problem with tracking state well you
have to read this tweet because I can't
actually read it out loud in the second
we end up with race conditions so what
do we do
well natural response is add locks add
locks
so bottoms from their ski it has this a
lovely way of putting it
shared memory is like a canvas where
threads collaborate in painting images
except that they stand on the opposite
sides of the canvas and use guns rather
than brushes the only way they can avoid
killing each other is that they shout
duck before opening fire
welcome to Locke based architectures ok
this is effectively what we're doing but
I'm doing the wrong thing I'm saying
Locke based but worried what I mean is
shared memory state so how so so what
other problems kind of emerge from this
well it turns out that what the minute
you introduce locks one of the
interesting things is you introduce the
possibility for deadlock and LifeLock
but you introduce these possibilities
there are all kinds of exciting cases
the minute you introduce any kind of
wait state you've got possibilities of
priority inversion deadlocking and so on
so in summarizing andrew Tannenbaum's
work there's this online course the
sauce that's right there are several
ways to address the problem of deadlock
just ignore it and hope it doesn't
happen this is a surprisingly popular
approach it's called the ostrich
algorithm you put your head in the
ground and just ignore it okay people
often think deadlock requires detection
and recovery if it happens take action
and a remarkable amount of intellectual
effort in academia and in software and
everybody still goes into this and I'm
going to try and posit the fact that
perhaps we're solving the wrong problem
instead of trying to solve the wrong
thing perhaps we should be thinking
differently yeah so we've got these
other things dynamic avoidance by
careful reason check to see if a
resource can be granted granted courses
develop and grant it again this is
another detection but it's a little more
tentative but I like the last one
prevention change the rules if you don't
like the game you're playing change the
rules if the problem is wait States then
try and get rid of your wait States try
and reduce them if the problem is
mutually exclusive access to shared
objects either don't share then we'll
see if you can get rid of the mutual
exclusion now we may not be able to do
that completely but it turns out that if
you start adopting this as your
philosophy most of the challenges that
people associate with concurrent
programming kind of disappear and allow
you to actually focus on the essence the
idea of expressing yourself within code
so this brings us to the habitability
axis or rather developmental axis the
first aspect which is indeed habitable
my habitability what do I mean by
habitable habitable is an interesting
word because what I want to do here is I
want outline some properties that we
want of our code base and one of the
things is that we want it to be
habitable if we're talking about
architecture then one of the things that
an architecture is is a place to live if
you are a software developer you are
living in the code you are spending your
time in it is it habitable is it
comfortable and this idea of
habitability comes from book by dick
Gabriel
and late 90s the patterns of software
and he had this lovely way of looking at
it he said he was talking about the
qualities of code and said well
habitability is the characteristic of
source code that enables programmers
coming to the code later in its life to
understand its construction and
intentions and to change it comfortably
and confidently isn't that something
that we want in code you want to be able
to look at codes I know what's going on
yeah unless of course you're trying to
keep your job and you know people kind
of say well you know his codes easy to
understand we can get rid of him and
it's just like yeah yeah yeah
she writes really good code and you know
what she's left is really decent so we
can go out of her because you know
everybody understands that code it's so
simple
do you know what it's really hard to get
simple comprehensible code it's actually
a challenge it's it's it's a deception
when we look at code and say that's easy
to think that actually solving it was
easy sometimes these things takes a
remarkable amount of effort but that is
a property that we would like it takes
forever it takes surprisingly little
effort to make code over complicated as
I demonstrated my talk yesterday so
that's what we want we want to be able
to look at a piece ago ion's piece of
code will fit it in our heads basically
say I understand what's going on here I
can reason about it I can reason about
its state I can change it conf
comfortably and confidently rather than
going like okay don't anybody touch that
piece of code now I think it works okay
I think yeah you know enough of this
hope based programming right so this is
what we're talking about habitability
makes a place livable like home okay
this is what we want in software
developers can feel at home place their
hands or any other item without having
to think deeply about where it is how
can you say that about all of the code
that you work on mm-hmm what about
testable okay why is it testability is a
really important property in code and
sometimes you go oh well you know yeah
we just test the system we don't want a
unit test unit testing is a waste of
time yeah of course it is
simple testing can prevent most critical
failures it turns out it's very
interesting paper from usenix last year
or earlier this year can't remember the
exact date by um a tower and it
basically demonstrates across a number
of distributed systems that over
three-quarters of the errors could have
been detected using unit tests so yeah
testability makes a difference and
normally what we do is we say well you
know the code is too difficult to unit
test ah that's different that doesn't
mean you don't want a unit test you're
just stating a problem that you have I
want to state that we want our code to
be unit testable and that's great so
we're done we want our code to be unit
testable where's a unit test it's all
very well saying you know I want this
property but what does it mean to have
that property so it turns out there are
number of different definitions
so Michael feathers has this very formal
definition or no enumerative definition
from a few years back a set of unit
testing guidelines a set of unit testing
rule sorry
a test is not a unit test if it talks to
the database it communicates across the
network it touches the file system it
can't run at the same time as any of
your other unit tests you have to do
special things to your environment such
as editing config files to run it a lot
of people get very disappointed when I
put that definition up because they're
kind of going like ah we have no unit
tests damn the point here is that it
also makes assumptions about your
execution model it's not just about the
network let's look at it from a
different point of view this is the
definition that I presented nine years
ago on read on the register it's a very
formalized definition but a unit test is
a test of behavior whose success or
failure is wholly determined by the
correctness of the test and the
correctness of the unit under test if
your test fails because of a of a
network failure then your code may well
be correct and your test may well be
correct but the test has failed for
reasons other than the correctness of
the code okay if your code fails because
of the way that concurrency has been
scheduled in your operating
that is not to do directly with the
correctness of your code and the
correctness of the unit under test is an
external factor the scheduling is beyond
your control in that sense unless of
course you're writing the operating
system so let's put it another way what
are our requirements for testability
from another point of view I've given a
formal definition here let's tell you
something slightly more useful what do
we want from unit tests if I ask people
what would you like from your unit tests
they often say when your unit test
passes it shows the code is correct I've
got some really bad news for you that's
never gonna work okay you cannot
guarantee that the code is correct just
by testing it okay Hey look mice
multi-threaded test it passed today that
one time I ran it and then every now and
then it fails mysteriously the point
here is that you cannot test for the
absence of bugs only their presence okay
it is Dijkstra observe that first but
this is really important because a lot
of people come to me so how do you test
how do you unit test multi-threaded code
my general answer is you can't and
they're very disappointed because what
they're looking for is a magic solution
and sadly I'm all out of magic okay
that's just not a thing that we have in
the world magic is a thing that they
make up okay it's not real unit tests
what you want to do is organize your
code so that the parts that you want to
unit tests are not are they either fully
contain their concurrency so pure sim D
parallelism as pure computational based
parallelism and therefore commoditized
in a library they either contain it
completely or you are dealing with units
of execution that are not themselves
concurrent and that the architecture
organizes the concurrency around them so
that you can unit test this but
integration tests the coordination of
them it turns out that what you want
from your unit tests is when a unit test
fails it shows the code is incorrect now
it turns out there's a lot of unit tests
that fail when the code is actually
correct but this is a good start so how
do we get such code because the point is
that we want the code to be habitable
and we want to know and change it in
confidence that means that way we write
our code should involve isolation and we
have many units of isolation objects
functions and so on but you
these units aggressively and properly
isolate concepts from one another we're
going to use this stuff asynchronously
okay
asynchronous suggests that we either
eliminate or reduce severely the amount
of locking in the code or to be precise
the amount of blocking okay that's the
thing I'm more interested in is the
blocking it's locking manifests itself
as a as a form of blocking now you can't
always eliminate all forms of blocking
sometimes people say oh we've got to
wait free architecture that's great
except what you find is that what people
mean is your code has to be busy for it
to be truly wait free if your code is
not busy then you end up with lots of
busy spin cycles okay the idea is in a
what we're actually saying is if you
there are certain tasks where you do
actually have to wait because you have
nothing else to do and there's a natural
consequence of things what we want to do
is eliminate the ones that are not
necessary
so a synchronicity is an important
property and interestingly so sequential
so I'm talking about concurrency and yet
I'm saying actually sequential proper
sequential code is what we want so this
is almost a almost a paradox in the
sense of what I want is sequential code
I don't want concurrent code
I want sequential code because
sequential code is easier to reason
about so how do we kind of resolve this
well the idea is that the things that
are asynchronous are internally
sequential okay and we basically
wherever you are in the you in the code
base everything looks sequential you
should not see that there are threads
running except perhaps at the point that
you launch them but wherever you're on
the KO ways it should look around you
yep looks like a single threaded world
to me okay that's how it should feel it
turns out this is so in other words what
we're after is not a virtual
multiprocessor but a virtual
uniprocessor it turns out you're very
familiar with this architecture you just
don't know it this your stream of
consciousness is a virtual uniprocessor
your brain is massively parallel but you
have the illusion of things happening in
sequence in your head it's a very sweet
illusion it's not what's going on
at all but it looks and feels sequential
an idea may pop in that was you know an
observation may pop in a feeling may pop
in a desire to do something may pop in
these are all just events delivered to
your single thread of consciousness and
you think oh this is how the world is
this is how I think not at all it's a
beautiful abstraction over a very
complex hardware so what we're talking
about here is effectively the idea in
some cases our architectures satisfy
what we might say all the problems
satisfy what we might say is an
asynchronous invocation
I would like to execute a function but
not have to wait for its result but how
am I going to get its result well at
this point we can go and go and go into
the pattern space a little bit so to say
well what I use a future what I'm going
to do is I'm going to execute a function
asynchronously because I don't actually
want to go and have to pick up the
result manually from and mess about with
mutexes so I'm going to say I do want
some waiting but I want to minimize the
waiting I want to commoditize it and
make it as simple and transparent as
possible so there is non interference
during the execution of something so
this idea of a synchronicity is strong a
future is the idea of a to proxy it's a
virtual proxy the idea of futures
sometimes known as IO use you return
instead of a value from a function you
return a placeholder and then when you
need it you can block on it okay but you
can also ask it by the way you ready yet
so if you want to truly avoid all weight
states if you have something else to do
if you have nothing else to do then
waiting is a natural consequence now C++
11 there is we have this kind of stuff
in here we've got a standard future
we've got an async thing and we got that
a number of years ago I put forward a
proposal that I still think is more
elegant obviously I do because I put it
forward it was actually based on the
idea that everything in a function
library is reading library should should
follow a standardized and simplified
metaphor and the metaphor was a function
so instead of having get a future object
that you use get on the idea is to take
a slightly different route and say
everything is like a function so instead
of saying 8th a sync the idea is that
you thread a function so thread is a
verb
you thread a function what that returns
is a function object so in other words
everything is a function everything can
be called when you want the result of
the function object you call it like a
function okay so in other words it's a
much more uniform simplified model so
you can pass it around as if it's a
function object and when you call it it
will give you that so it fits with all
the other kinds of ideas that the use of
dot get'em not my favorite really but
this idea was a much much deeper idea
now there's something else here not
every task can be turned into an
asynchronous plus rendezvous structure
so we mean we have to talk about
immutability because effectively what we
did in the previous one is we isolated
change the asynchronous function could
change as much as it liked as long as it
didn't interfere with the function that
was the caller okay so now there was
true isolation unshared data you can
have as much mutability as you want but
another access is to consider the access
of no change let have data and not
change it in functional programming so
this is from the source of all true
functional programming wisdom obviously
the Haskell homepage that's what the
Haskell programmers have checked would
have you believe in functional
programming programs are executed by
evaluating expressions okay this is in
contrast with the styles that we use
imperative programming where programs
are composed of statements which change
global state when executed now let me
clarify what I mean by global state I
don't mean global variables they don't
mean global variables what they mean is
state that can be shared between any
parts of the program so there's this
idea of modification global state
unfortunately it's it's a shame they use
the term global there cuz normally for
us global for everybody else in the
programming business global normally is
a scope related concept whereas they're
actually talking about overall programs
state functional programming typically
avoids using mutable state and they
observe and this is the important thing
it's not game over because they say well
many many programming languages support
programming both functional and
imperative styles and I'm going to say
C++ can support both and I've done a
number of talks on functional C++ but
the idea is the syntax of facilities of
a language are typically optimized for
one of these
dials there's a lot in C++ that kind of
resists you doing that importantly you
may find your existing code base resists
this kind of philosophy you know social
factors and coding conventions will
often force you to one of these styles
now the problem is that when you say to
a lot of C++ programmers hey functional
programming the first thing they think
of is all lambdas I'm not going to say
that that's not the case I think
programming with lambdas is I'm so not
object-oriented anymore I'm doing
lambdas I've got really bad news for you
lambda calculus was the first
object-oriented language this is an
observation that William cook makes in
his talk and paper or understanding data
abstraction revisited now he dates it to
1941 hey he's wrong it's 1932 so
basically when people get terribly
excited about programming in lambdas in
C++ 11 onwards
welcome to the 1930s yeah so really it's
nothing to do with it yes it is related
to functional programming but actually
if you go and look more deeply at object
orientation from a theoretical
perspective but also a few other
languages small talk and so on you'll
find that the idea of being able to pass
a piece of code around as an object is
kind of very object oriented indeed so
just using Landers does not make you a
functional programmer on the other hand
we do have some delights in the C++
syntax I mean that's actually valid C++
okay well like there
I did not pay him to say that so this is
you know this is a lambda that takes no
arguments has no capture and does
nothing and returns nothing and this is
executing it to really do nothing so you
know hey great yeah I know this is like
sometimes it's the case like right how
far can we take this what string of
parentheses can I make legal yeah I
leave disappointing isn't it yeah but it
still does not prevent you from rising
to the challenge in other ways I mean
yeah what string of punctuation can you
get that is legal oh that's a good
question somebody if yeah if somebody's
bored with this talk go and go on you
know go and kick off GCC and find out
okay so anyway what we really want to do
is a better philosophy of functional
programming Michael feathers observed
this a few years ago which I thought was
a very good way of reasoning about it
object orientation makes code
understandable by encapsulating moving
parts what you want to do is co-locate
and modularize the mechanics functional
programming makes code understand by
minimizing the moving parts so they both
have a philosophy of moving parts one is
containment and one is in the other is
reduction and we see that this can be a
very practical mechanism this is not
just a localized simple mechanism it's a
deeply practical one this one from
Facebook and describing there the
implementation of Facebook moments to
keep our C++ API boundary simple we
adopted one-way dataflow now there's an
important point about one-way dataflow
first of all with one-way data flow as
they save the API consists of methods to
perform fire-and-forget actions okay so
basically the idea these actions can
mutate data so the idea is that one-way
dataflow what does that mean it means no
callbacks one-way data flow no callbacks
if you have locking and callbacks you're
inviting deadlock livelock and all kinds
of exciting problems related to that
because you're basically saying here is
a piece of code that
a lot and then I'm going to go out and
they're going to come back into it what
is the state related between these two
parts it can get very exciting in a bad
way just to let you know so the idea is
one-way dataflow now that has important
effect it gives us an isolation effect
if we can say there are changes but they
are not visible to other parts so the
thing is half the trick is where there
are changes isolate them eliminate them
from public view do not share them what
they found is they eliminated they chose
a style of execution eliminated the
problems as they perceived them but kept
to a really strong core functional style
and where there was a bottleneck they
optimized it but the idea was to keep
the illusion of a pure function model do
clever caching if there are any issues
don't solve the problem at the wrong
level of abstraction right this gives us
this idea that sometimes the way we can
solve some of these things is by using
explicitly immutable objects an
immutable value its instances are
immutable the internal state of the
value asset at construction never
changes no subsequent modifications are
allowed now in C++ we have further
support for this idea and concept here
we have to start saying when I say Const
I really do mean it it's not just
logical constants its physical constants
now to avoid too much copying will often
do this in conjunction with passing
things by reference and we can take
advantage of our value references and
move semantics so that we are able to
support the illusion that we are just
communicating with values and the idea
is that there's an awful lot of
optimization that we can do behind the
scenes but also there's another side to
this it turns out that making things
immutable is not the only way of
isolating them copying is another
technique for isolating things so a
copying approach is also something that
is very instinctive and natural in C++
it's a very natural thing the default
way of passing an object if you put no
other syntax is to pass it by copying
define a value object whose type whose
instances are call people when a value
is used in communication with another
thread ensure the value is copied and
this is something very that is very easy
to do in C++
idea is that you are improving
encapsulation either by using
immutability or by using copying if I if
I offer you access to a value that is
not going to change and you have
reference or pointer based access to
that value then I have enforced
encapsulation within the object of
origin because you can't change anything
about that object to change my view of
it the other thing is that I can give
you a copy of that object
and you can change that copy as much as
you like but you can't change my view of
the object that it originated that so
there is this idea that copying is
another technique that sits side by side
with immutability for improving
encapsulation and isolation now we can
apply copying to very simple we can
sorry I apply this philosophy to very
simple classes so date class is given
that we people always getting date
classes wrong as an abstraction I've
left out left out the defaulted
operators I'll just concentrate on the
the copying the regular copying
operators but the so a typical way that
I've seen people go about doing date
classes is like right okay we'll have
get year get month get Dame month set
year set months at day month they have
this kind of pairing of getters and
setters really there is no rule out
there that says you have to have a
getter and the setter pair and in fact
the way that people often do it is they
end up with data members they think
inside out will have some data members
and then I'll provide a getter for each
one and because I've got a getter I will
provide a setter and in fact they even
have their editors set up to do the
wrong thing for them yeah you can now
optimize doing the wrong thing okay just
like that's great computers can make you
do the wrong thing faster so it's this
kind of thinking in fact I do
I have come across a company where that
was one of their coding guidelines for
every data member you have have a getter
and a setter now the point is you don't
change dates individually you don't take
a date and then modify just its month or
just it that's not how we reason about
them there is no symmetry the problem
here is that we often assume that
because you have a gap there's a natural
set and the problem is in English get
and set rhyme and they have the same
number of letters
everything oh look it's a natural
opposite no it's not a natural opposite
the opposite of set is reset or unset it
is not yet so let's get rid of those
well that simplifies things let's unify
this and basically recognize that if
you're going to modify something if
you're going to modify something then
you modify it all together I set it to a
new date but if we look closely it turns
out we've already got a function that
creates and validates a new date it's
called a constructor so we don't need to
set it at all we can do this and
manually do that but actually we can do
it like this I've got a constructor and
I've got copying operation now the
constructor does all the validation so
now I've got all the validation code in
one place rather than in two places
okay the compiler can do all the rest of
the optimization here so we can do a
line there and we can just put curly
brackets we can go like this we've got a
lot of choices about how we present this
the idea is that because you already
know how to construct it you've already
solved the problem all you need and
notice what we've done is we've actually
reduced the mutability of this interface
we only have query operations and
assignment in other words a binding
operation treat if you treat assignment
as a binding of a value then that is the
only mutation we are affording here now
this style is good just as a as a way of
thinking anyway so it's a slightly more
rigorous and mathematical and simpler to
reason about you're not taking a date
and modifying it with side-effect but
also there's another thing that also
I've noticed holds us back from
embracing a more reasoned approach to
thinking about stuff we are such bad
naming conventions get I mean really the
default meaning in English of get is an
imperative if you want to embrace
functional programming stop using
imperative naming get is an assembler
kind of thing you see look a lot of code
bases and you see what I call a
object-oriented assembler they're
getting and setting things it's like no
that's the point of using a high-level
language don't get anything it just is
or is not yeah
there is no get so you
the default meaning in English of get is
to have a side effect if you doubt this
then go through a number of standard
expressions get does not mean query you
know when I go to a cash machine and I
get money there is a disappointing side
effect on my bank account okay
it is not a query if you get married
that's a fundamental state change in
your life okay it's not merely a query
yeah would you like to get married is
but get married is actually a huge great
side effect based operation okay rebind
the pointer operations and all the rest
of it okay
so what does that mean it means just
name it for what it is you know it's the
year it's the month this is the day and
month that's it all right okay now
sometimes people when they're working
with this style some say well yeah but
sometimes I just want to take I do want
to take the day and I just want to
modify one thing well in which case we
would use a slightly different style
what we're going to do is this sort of
projection style I'm going to take that
date
I just want to modify the year and I get
a new date back just as when you take
the number five and you add two to it
you get seven back you don't change the
value of five if you want to do that go
back to old Fortran
yes you could change the value of
constants just by doing very naughty
things don't do this so this is
basically the Builder pattern what it
does is allows you to separate present
separate methods for constructing
different parts of a complex object or
performing cumulative changes to an
object yeah so if we do that then it's
as simple as creating that in other
words we offer people our convenience to
allow them to chain this stuff we could
potentially optimize it to avoid any
additional validation that goes on in
the constructor but this idea whichever
way we look at it simplifies the overall
interface and usage and the idea gives
us sort of a chain based model for
reason but it's a projection based model
here's a value here's how I transform
the value I don't change the original
state of the object and this is a this
gives us a valuable property which we're
going to take back into the idea of
concurrency Bertrand Meyer noted that
asking a question should not change the
answer okay we are we
to organize our code in terms of asking
questions of values rather than telling
them to do things that said I suspect
that Bertrand Meyer at the time he wrote
this did not have children because I've
noted with my kids that asking the
question always changes the answer and
asking it twice certainly does but
there's another aspect here asking a
question should not change the other
answer nor should asking twice this is
the property of idempotence the ability
to use the same thing the same query
twice and it has the same effect a tempo
tens same force so we can kind of
observe what we're getting here is this
idea of referential transparency a
desirable property implies functions
consistently yield the same result given
the same input input function evaluation
depends less ideally not at all on the
side effects of mutable state if it does
this this is going to aid us not just in
regular programming but in the context
of concurrency so ok date is a fairly
simple data structure but a lot of
people worry about their containers
because it turns out that most programs
do not deal with one piece of data they
deal with lots how do we organize the
lots so here's a very simple idea let us
imagine a hypothetical library the FTL
faster than light no sadly
well actually baby I presented warp
drive yesterday in my talk but a
functional template library what would
what would the FTL look like what the
STL looked like if we were to adopt
functional ideas into it well first of
all a lot of things become simpler
everything would be conveyed you'd still
be able to iterate the only side effect
operator on the containers would be to
assign and so which I'm treating that as
a rebinding operation all it erases
would be Const will value ties because
basically everything's constant now it
turns out this is kind of easy to do
here's a very simple idea a set a set
that we can go ahead and initialize we
can have a set of integers now the great
thing is that thanks to initialize the
syntax what I've now got here is that
it's kind of easy because the set
because it's never going to change you
don't need to do you know silly little
silly tree arrangements you can just say
right here's an array and the set is
already ordered during construction the
state changes but as constructor is a
transaction for completing a valid
object so basically the constructor is
the only place where state changes
afford it but by until the end of the
constructor you don't have a legal
object so therefore what we're saying is
that once it is fixed edie is fixed
that's set this is great for reference
data okay so therefore all of these
operations can work on just a sorted
array this is really nice and simple
very compact and a surprising number of
uses of conventional modifiable sets
will actually fall into this we can have
an array which is actually somewhat I
think curiously enough this is probably
more useful than the standard array
which is actually slightly not useful
this is fixed size but it's not fixed at
compiler at compile time it's fixed at
the point of creation okay you can get
the front bat you can do all these
various bits and pieces again this
satisfies a number of very common uses
of arrays reference data lookup data
data that's been passed around now
that's great but maybe I want to push
something or pop something you're
sitting there how's he going to do that
well that kind of width at chaining
approach is what we're going to use yeah
but isn't that going to be expensive
well if we do it not evilly yes but it
turns out there's a class of data
structures known as persistent data
structures and the idea of persistent
data structure is that it it actually
supports the illusion of immutability
even though things may apparently change
they don't change in a way that one
person holding the value will notice we
will take a value we will push to it and
somebody else will have the pushed
version and this person will still have
the version they originally had they
have seemingly isolated so if you want
to see what that looks like logically by
the way don't confuse it with
persistence as in databases if we
imagine that we have an array underlying
array we can say right we've got a
pointer to it got another pointer to it
okay B equals a I can do a pop
now see see has a so this is like a
stack basically see now has three
elements it can look at where as a is
still looking at four we haven't
actually changed what a is looking at we
haven't changed the underlying data
structure we have merely used navigation
to change the structure that's great but
sometimes we would like to be able to
push as well it turns out there's
another structure that allows us to do
this if we choose a link structure we've
got a and C is pop a we traverse the
link but importantly the other thing is
that now I can actually add now here's
the clever bit this is this is the bit
where is as I said earlier on there's no
magic okay so what is it the magician's
do a magician uses sleight of hand they
make you look at this while something
else is going on over here so a is busy
looking at this chain of four now I'm
going to do a push we're going to create
a new link a new note but nobody's going
to see it except D a has no idea that's
going on
nobody can observe the fact there's a
state change it's as if it had always
been there and there's nothing you can
do except look at the look at everything
executing at the instruction level to
prove otherwise it's as if it was always
there and this can tree out in memory so
let's see how we can take this so we let
s try vector so a vector has slightly
different capabilities to the stl vector
because i want to stick with constant
time operations it turns out that a
vector along these lines can support pop
front and pop back but it supports no
operation now I'm not wild about this
naming idea because notice that's the
imperative naming I don't want to
confuse it with the fact that it's not I
don't confuse it with the STL one
because pop front in the STL has a
side-effect
I don't want to say this is like pop
front but it doesn't change the object
well then it's not like pop front then
is it this is popped front ok so now is
I'm gonna use an adjective or sort of
description of its state I'm going to
describe the state of what I am
returning this is the current object but
with the front popped ok it's as if it
were like the
okay so that's great that's vector what
is list look like well list is
automatically going to be a lot more
like a good old friend from the late
1950s I happen to I happen to enjoy
collecting old books and this is from
this is a reprint of a book from the
early 60s through one of the original
Lisp manuals and I wrote about this one
and it's a very interesting book because
it tells you how to create a lisp
interpreter because back then you
couldn't really share stuff on github it
turns out so yeah I made this
observation in an article a decade and a
half ago as I said I still have a deep
fondness for the lisp model it is simple
elegant and something with which all
developers should have an infatuation at
least once in their programming life I'm
no Lisp expert but I've gone through two
or three phases of thinking it's like
the best way of thinking about stuff
ever there's a deep idea there and I
created a data structure in that article
called lisped it was a lisp based list I
thought I was very clever for coming up
with that so here's my class list it is
naturally a four word list it's a singly
linked list we're not going to worry
about doubly linked lists that's not a
way I want to go we've got a simple link
structure we got the pop front the push
front all the rest of it this is all
great and it works in the way that I
described earlier on this is fantastic
now there are two things one is that
that simple idea of illusion can be used
can be scaled up to trees that idea is
that we can have more sophisticated
models I'm going to stop here with a
singly linked structure but you can use
this persistent data structure to deal
with nodes millions of items in a tree
structure and and apparently return a
whole new millions of nodes except
you're not it turns out I'm going to
return a structure where a couple of the
nodes have changed and point in
different places okay that's the idea
I'm not going to go further into that
because there is one problem that we
have at the moment if you've been
watching the slides really carefully
you will notice I have done absolutely
nothing on memory management at all it
turns out Shakespeare was on to this
Shakespeare characterize the different
philosophies of memory management in
Hamlet okay so in Hamlet Hamlet observes
yay from the table of my memory I'll
wipe away all trivial fond records
Hamlet was a garbage collecting kind of
guy okay so so it turns out that if
you've got a garbage collector running
then and C++ allows garbage collection
to run and even basic kind of basic
political action it allows those to run
if you've got that run then my code just
works fine and it's simple and it's good
and all the rest of it there is only one
small problem
garbage collection is optional there's
no requirement that it exists now this
is unfortunate because I think it's one
of those dangerous problems of trying to
please all of the people people all of
the time it turns out you can't do that
because now I I can no longer write a
portable program that relies on garbage
collection the correctness the
operational behavior of that program now
relies on an optional feature you have
to be incredibly careful of this in
other was what we were effectively
relying is that the point of safety is
strict perhaps we could get away with
preferred but we're relying on that if
that is not true and trust me it is not
standard on C++ runtimes if that is not
true then the correctness of our code
has been undermined all of our
assumptions have been undermined this is
a problem with optional features it
actually means that although it looks
like you can do more actually you end up
being able to do no more than you could
before which drives us into Afilias arms
tis in my memory locked and you yourself
shall keep the key of it yeah you get to
do your own delete whoo okay so we don't
really want to do our own delete we're
gonna use reference counted smart
pointer my shared pointer there is a
problem and this was observed by Rob
Murray back in 93 when everybody back in
the 90s every was getting very excited
about doing reference counted smart
pointing and it's just like oh yeah as
he says a used counter class is more
calm
located okay well are you fine you might
say well that's fine I've got one in the
standard library a shared pointer and it
won't know if I'm not using C++ 11
almost I can get one I can get one from
boost fine there is a problem here
all this horsing around with use counts
takes a significant amount of processing
time it does you even leave you and even
in a multi-threaded program you're also
paying the cost of an atomic increment
and decrement so that's not necessarily
a bad thing it's just a thing you have
to take into account that this is not
for free but ok let's just say we take
on that cost we go ahead we fix up the
vector okay we have to put our own
deleter in because otherwise it will
trying to leet a single object rather
than an array so we think we're good and
we go for the list and we have shared
pointers and we think we're good
and we think we solved the problem I've
got bad news for you we've actually
introduced a problem but you introduced
a huge problem I remember looking at
this 15 years ago going oh this is not
what I expected this is most unfortunate
to understand this problem I'm going to
take let's just fill up that list with a
number of items ok you know front insert
we're just going to add in I don't
really care what that something is we're
just going to create a list I want you
to note where the curly brackets are
we're going to create a list we're going
to populate it and then it's going to
destruct at that closing curly that
destructor is going to automatically end
up destructing the shared pointer right
which decrements from a reference count
of 1 because we haven't shared it to
anything to 0 which means that the thing
it's pointing at will now be deleted the
thing it's pointing at also has a shared
pointer to another thing which you'll
drop the reference count from 1 to 0 and
it will delete it in other words you
will end up with a recursive delete as I
discovered it turns out that this blows
up your stack for surprisingly small
values if you want a list
anything more than a couple of thousand
long you are screwed to use a technical
term okay now some people sit there
think oh I've got a clever technique for
this I want you to stop right there I
don't doubt that you have it's the fact
that you need to do that that is the
problem it ultimately turns out yes
garbage collection is ultimately far
more superior for certain classes of
architecture and programming than manual
memory management the manual memory
management is not something we should be
forcing on everybody okay
C++ optional garbage collection no it
needs to become mandatory for the
runtime and then you can choose whether
or not to opportun or out okay and
certain classes of software can have it
disabled funnily enough I made a claim
just remember this because Shawn parent
is at this conference we were on a panel
about ten years ago as C++ connections
in Las Vegas and we're asked about the
future of C++ and I said there are four
things for C++ to be a proper systems
programming language in other words be
able to talk to its operating system and
describe the capabilities of its
platform and reflect those accurately I
said there are four features that it
needs okay one of those was threading
it's got that now finally another one of
those was garbage collection a lot of
runtimes have garbage collection it's a
standard feature but not one that is
accessible from C++ another one was
proper reflection and the other one was
a dynamic loading module system ten
years on we're almost half way into that
but only just so I'm going to point out
that this is a go if you really want to
do proper functional programming with
persistent data structures you really
need to start lobbying people for proper
GC we don't want more clever memory
solutions okay so what about the actual
execution of this stuff an observation
from Russell winder instead of using
threads and shared memory as our
programming model we can use processes
and message passing process here just
means a protected independent state with
executing code not necessarily an
operating system process now the
interesting thing is that he then gives
a couple of examples languages such as
Erlang which uses the actor model an
ahkam which uses the CSP model before it
I've shown processes are very successful
mechanism for programming concurrent and
parallel systems they don't have the
stresses of synchronization they also
scale rather nicely and they're easy to
reason about now my background funnily
enough many many years ago I did a
master's degree in parallel computer
systems and the CSP will the akka model
is exactly what I used and this is their
this is the thing that it kind of looks
like okay now some people kind of got
into this idea of channel based
computing because of a more recent
innovation the go programming language
but the point is that the child is not a
great new idea so let's build up a
simple channel model it's a very simple
channel model I'm going to start off
with the idea of a non-blocking receive
and ascend okay so we're going to do
that I can do that and I've got the idea
that I can just have an underlying deck
my send is just a push back and yeah
I've got a little bit of work my try
receive if it's empty then you know we
return false if it's not empty we pop
and the the caller gets the value okay
that's really cool it's also
surprisingly sequential because I
haven't put anything in there okay so we
obviously need to do something else so
I'm going to add in a lock you can go
and look up how to do lock free versions
and light locking versions of the rest
of it I just want to build up an
encapsulated locking mechanism through
an API so we're going to do that fine
you have to put in a lock guards so this
does all the right thing for the send
and the try receive becomes a little
more complicated but we've still got the
basic thing going so now try receive if
it can get a lock it'll get it otherwise
it returns false if you can get a lock
and it's the underlying
channel is empty then it'll return false
otherwise you actually get a value out
of this so this is the element of this
so building it up from sequential
thinking very modest sequential thinking
and then I probably will actually need
unfortunately to add in a blocking
receive because actually and logically
you should also by the way add in a time
to receive as well it
out that it is if you have if all of
your programming is composed of just try
receives and timed receives you can
guarantee termination which is kind of a
nice property
however the timed receivers of the noise
yes I'm going to leave that one to one
side but I are blocking receive for
those moments when really you have
nothing else to do we can go and add
that throw in a condition variable and
that file and yeah great so I've got
this slightly more involved version but
it gives me a nice abstraction so what
problem should we solve with it well
given that fizzbuzz was the problem I
dealt with yesterday let's go and do it
again today
so fizzbuzz for those of you that either
were not here or cannot remember because
yesterday was a long time ago simple
counting one two fears anything
divisible by three is a fears four buzz
anything divisible by five is a bus okay
13 14 fist but divisible by 3 and 5 is
fizzbuzz so not the world's hardest
problem but as a programming problem
it's normally defined over the over the
domain of one to a hundred so I can have
a simple implementation of that and you
know we would have fears bars fears bars
there's lots of different ways of doing
this okay now obviously this is such a
real real computation intensive thing
I'm going to set up a server for it
here's my server it's called a phys
buzzer and it infinitely serves fizzbuzz
requests how useful is that okay don't
say I didn't teach you anything today so
what I've got is I've got an input
channel this is where I received my
requests I receive them and I convert it
to fizzbuzz and I send them all okay
that's cool this is what main looks like
yeah we set up the two channels and fire
off a phys buzzer and then we run
through this and we actually completely
kill any point of doing concurrency
whatsoever but that's not really
important okay but the idea is we now
have a another thread running it turns
out that you can actually queue up
things in advance that's actually talked
about the asynchronous aspect here
because I'm doing it very synchronously
but the idea is I can actually send
through a whole load of requests and
pick them up later from the point of
view
of the fizz buzzer the world is
sequential is the property I described
earlier on it's trivial to test the
correctness of this you need no threads
to test the correctness of the fizz
buzzer or indeed off his buzz itself
we've got a separation the separation
here here is the core functionality here
is the message handling functionality
and finally we have the coordination but
I can test the messaging logic without
needing their additional coordination I
can actually do all of the sends in
advance run this I might have to limit
it in a particular way it runs
infinitely but the idea is I can test
the logic in isolation so these are the
kind of nice properties now I can do a
little bit of syntax sugaring instead of
send and receive I can use sort of
stream operators and I end up with a
more elegant approach and one that is
fairly familiar to people and if they
coming from go as well and also gives
them a certain familiarity from the
outside then it looks a little bit like
this ok so we built this up now there is
there is a little something missing here
is that if you look the output the send
does ascend but what does the receive do
the in the other streaming operator
appears to only support receive the
blocking one what if I want to support a
non-blocking one turns out that I can't
do that at the operator level but if I
introduce a little proxy I can do that
so the proxy will sense whether or not
it's being used if you if you need a
ball conversion from it then it'll use
try receive and if you don't need a bull
conversion from it then it'll be it'll
do a blocking receive so basically we're
able to support it I'm not gonna linger
on the code but you're going to end up
with something like that and then an
internal class nested class that does
all of that I'm just putting out there
for completeness I don't that's not the
technique that I'm focusing on but one
final embellishment here I want to focus
on another feature I said a moment ago
that fizzbuzz is defined on the domain
of over the domain of one to a hundred
normally let's support that let's be
strict about that
so if you pass in a value one to 100
then you're good if you pass in a value
outside one to 100 then we're going to
throw an exception threads and
exceptions how do we deal with it well
there there is a model for dealing with
this but what I'm going to do is I'm
going to change a few things here the
first thing I'm going to change is I'm
going to change from channel of string
as my return to channel of any I could
use a variant but in this case I'm gonna
use any one because I invented any and -
it's in the next year standard I know
but I didn't invent it so you know yeah
variant you're correct variant is also
in the standard but any is the open
protocol that I would use for any
messaging anyway so it also fits on the
slide conveniently so what we've got
here is what I'm going to do is I'm
going to separate this out I'm going to
capture the current exception and so
basically I'm either going to send out
fizzbuzz but if we get an exception I'm
going to capture the current exception
as an exception pointer and send that
back instead and I can go ahead and I
can pick that up I can ignore it I can
pick up the result and say hey look if
it was a string then I'm going to cast
and get my result as a string and I'm
going to ignore any errors and the error
ignoring I'm not comfortable with but I
also don't want to end up with a whole
load of choreography in my core logic so
something else I can do and this relates
to a question that I was asked yesterday
something else that I can do is I'm
going to add in an extra little feature
the extra little feature is I'm going to
allow a channel to be constructed with a
receive handler a callback every time
something is received you get a callback
on whatever you pass in and you can use
this to do a translation on receive you
can do a simple filtering if the receive
type is an exception pointer then we
throw the exception which is quite nice
which means that the core logic looks
like this when you receive an exception
now it'll start our domain error
automatically it propagates all
exceptions out so that allows me to
separate the core logic is actually
little bit higher than it was before
exception handling is now separated from
there so we can keep on doing this and
refined
but there's a couple of final thoughts I
want on this one one we can use this to
build a pipes and filters architecture
once you have a channel you've got a
pipe a lot of problems break down nicely
into the idea of I compute something I
pass something on I've received that I
pass it on I receive that I pass it on I
mean the classic idea of pipes and
filters is this idea of they are easily
chained and very composable and very
very easy to reason about the other
thing is you can build towards the other
thing that I studied during my master's
degree which was actor based computing
which has become very popular recently
and I think probably the easiest way of
reasoning about actor based computing is
to borrow from andres andres classic
quote multi-threading is just one damn
thing after before or simultaneous with
another but actually the idea of actor
based computing realizes everything is
purely sequential and hides the channel
logic okay actor based concurrency is
just one damn message after another
that's what you do you just receive a
message you receive you have a function
that executes and it just receives
things and then it may send things on
but you never even see the channels you
have the idea of communicating with
objects in a very a synchronous and very
natural way so I'm gonna bring this one
to a close I'm going to go back to John
Carmack's observations because the point
here is that there's a lot of useful
thinking it's not simply a mutable state
is isolation but no matter what language
you work in programming in a functional
style provides benefits and you can miss
sometimes I'd say it's a continuum you
can move towards that by using
mechanisms of true isolation and
immutability you should do it wherever
it is convenient importantly I like his
idea if you should check you should
think hard about the decision when it
isn't convenient this is the it goes
back to my observation the state
modification and you know basically
sharing mutable state and things like
that consider those to be some kind of
privilege not a right it's not a default
that you should do so this idea very
much of
it's making your state model explicit in
a way that you can reason about is a
very very powerful concept don't throw
it away
so to close here's why I want you to
think outside the synchronization
quadrant because if you are
synchronizing you are blocking you are
waiting and once there are some things
in life that we do have to wait for make
sure that that is a natural consequence
of the nature of the problem you are
working on rather than accidental
property of your architecture because
the thing you always have to remember is
all computers wait at the same speed
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>