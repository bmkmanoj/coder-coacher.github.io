<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>BUILDING MICROSERVICES WITH PYTHON AND FLASK | Coder Coacher - Coaching Coders</title><meta content="BUILDING MICROSERVICES WITH PYTHON AND FLASK - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>BUILDING MICROSERVICES WITH PYTHON AND FLASK</b></h2><h5 class="post__date">2017-09-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1X3_gQwfabI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I have a friend who's taking a software
development class he's not a developer
this was an elective class of hitter in
college he had to write a game game of
pong and he had a bug and he he came to
ask me for help and what he showed me
his code it looked pretty much like this
all game in a function so he called pong
and then that was the game and he had a
bug with the way the ball moved right so
so to help him I had to go understand
pretty much the whole game right it was
very hard for me to to figure out what
was going on so I used the opportunity
to show him a better way to structure
this this game right so I told him you
should do something like this where you
have a bunch of functions each function
does one thing and then when when you
need to do something bigger than what a
function does then you have a function
that calls other functions and all the
functions call each other and achieve
the big thing that is the whole game
right and I'm sure we all going to agree
that you know that the one's right is
much better than the one and left
so here but basically what we're saying
that you know short and focused
functions are better than long functions
to do a lot of things now what happens
if if we go one level above so this is
this is an example from my book the
flash book if you start the book it
teaches you how to build a flask
application and when you reach the end
of chapter six you end up with an
application that does quite a lot but
basically the whole Python logic is in
this this one module hello the PI then
you have a bunch of templates and a
bunch of static files but but basically
the whole Python thing is in one file so
so then you you get into chapter 7 and
chapter 7 is the one that teaches you
how to structure your application so
that it's more maintainable so so it
goes into something like this where you
have a starter script manage to PI the
configuration it's in its own module you
have the application is in a package and
then the tests are in
different package and if you keep
looking the application has a couple of
modules that are for specific things so
the database models are in one module
email support is in another one there's
this this main folder which is another
sub package that is a in flask we call
it a blueprint which for those of you
familiar with jungle will be sort of the
same as a jungle application and if you
keep looking then you have for that
blueprint you have errors forums and
views so basically once again same thing
as with functions now with modules were
saying that small and focused modules
are better than large modules that do a
lot of things right now what happens if
if we take this even higher level and we
talk about services web applications can
we translate this so this is an example
of a big web application that this is
actually an application that exists it's
on my github I use this application to
teach a class to demonstrate that flask
can scale and basically the idea with
this application is that you have these
two green boxes FLAC with the name of
the application and then the celery
worker it's one or more workers that do
asynchronous functions and the idea is
that you can run any number of slack
instances and any number of salary
workers and you scale those two
according to your needs then then you
have a database that both use and
there's a message queue that's used by
celery and also used to to allow the
celery workers to push notifications to
the client through two WebSocket so any
way that this is probably sort of an
advanced ish application very scalable
but it's it's one code base so can we
apply the same logic that we apply to
functions and modules to this and in my
opinion the answer is yes
and okay first I should I should tell
you about the problems with this so one
problem that we have is that the code is
is a single code base so it's very hard
to test all the things are basically
coupled you you have code that deals
with users in this particular case
FLAC this example is a chat application
so we have coded these with users and
messages or mixed up you don't even
realize that you are having you know
coupling between those two potentially
separate functions of this application
if you are working with the team you
need to introduce a new member to your
team that person it's gonna have a hard
time trying to figure out how to become
productive because they'll have to
understand a lot of things before you
know before you let them participate to
reduce the risk of them breaking things
in particular when you use celery
something that I find I find I don't
like that much is that if you need to
upgrade even though in this case we have
two separate services the main service
and the celery they all come from the
same codebase so there's no way to
upgrade them separately you have to stop
both or if you have many senses of each
all of that you need to stop then you do
the upgrades I break the database
whatever else you need to do and then
you start on them again so so basically
you have to take your application down
for their prey also if you have a
problem and the application crashes then
basically the whole application crashes
and the site goes down until you figure
out what what's going on and you can't
restart the scaling becomes difficult he
mentioned in the case of a chat
application like this so you have a
module that deals with users and modules
at least with messages
very likely there's going to be more
activity on the messages side than in
the user side right you're gonna have a
bunch of regular users that are already
registered and then we're going to be
chatting so we're be sending a lot of
messages so if you find that you need to
to scale your application you're gonna
be scaling the whole service so you're
gonna have you know the necessary the
necessary number of instances to satisfy
the the load for the messages but then
you can also be having a lot of
instances that can deal with the users
that are not there you're gonna be over
over scaling for the user side and
there's no way to have more fine control
the only control is basically the celery
versus the rest the main service also
consider the case where let's say this
is an old application you did it in
Python 2 and now you are interested in
going to Python 3 and that that's gonna
be probably gonna give you a headache
right because it's all or nothing you're
gonna have to upgrade the whole whole
application in one go so all these are
problems that are typical of what we
call monoliths so these big applications
that are they're built with a single
code days now this is also a real
application it's the same application
it's also on my github it's converted to
this idea of micro services and you can
probably guess that it is basically to
write smaller services and the services
then talk among themselves to achieve
you know the whole function of the
application and in this case because he
went from two green boxes to five and
Siri is not there anymore so we have
five services you can see in the bottom
you can see the client UI so this is a
service that serves the application that
runs in the browser so this particular
case I wrote that in Python in most
cases I'm going to guess that this is
going to be a no application and that's
totally fine because you know that
they're independent services you can
write each service with the best
technology
for that service so have a client you I
we have a service that into tokens that
basic this is authentication messages
and users are separate services and then
we have the socket IO service which is
the web socket push notification module
so they're all separate in this case
these are all flask applications some of
these are so small that you can open
them in your screen and see the whole
code and the ones that you don't know
probably two screens no more than that
they're all you know fairly small you
can see that we went from two orange
boxes we had a database and a message
queue on the monolithic case now we have
four boxes we still have the message
queue that serves the same function it
helps the these services communicate
among themselves
but then we have three databases we have
a database for the messages service
database for the user service and the
database for the token service which
stores woked tokens and then what else
we have we have a new box a blue box
called service registry I'm gonna talk
about that more later but basically this
is a very efficient database that keeps
track of all the services that are
running it knows what's running and then
he communicates with the load balancers
that load balancer knows what what - you
know what the services are now these
five green boxes they're all
independently scalable now so now if I
have more more load on the messages side
I can run more more instances of
messages and then keep users you know at
one or two for example so I talked about
you know disadvantages of the monolith
and all of those now translate into
benefits when we're doing micro services
the code complexity it's you know
greatly reduced
each service as I said it's a very small
flask application
you know reminiscing of the the hello
world type application you see in
documentation they're actually very
simple to decode very simple to maintain
because we're forced to keep things
separate it's less likely that we're
gonna introduce bugs due to coupling the
user service has no way to access for
example the messages database directly
it needs to talk to the messages
servants so messages will have a public
API that will expose to to clients or to
other services the users who do the same
thing and basically that that's a couple
design you know by force you basically
this is design promotes the decoupled
design that helps helps create programs
to have less bugs now the case of having
a new member in the team that that you
want to to make productive as soon as
possible that that becomes really easy
because you you can put that person to
work on one of the your simplest
services and like in the case of pong if
you have you know the code structure
with functions if I need to fix how the
ball moves I don't need to learn how the
you know how the players move or how the
collisions happen all I need to do is
basically go to the function that moves
the ball and this is the same thing you
can put a new developer to work on the
on the token services for example the
token services is very simple and you
know right away they can start being
productive you can you can even allow a
new person to create a new service
because it's it's a very simple
application one of the things that I
find most exciting is that you you can
upgrade like the big guys do without
going down we never find out when
Facebook Twitter etc you know deploy
operates because they do it you know
while running and we can do the same
with this I'm gonna show you an example
of that later probably you don't believe
me but you know give me you know benefit
of the doubt I'll show you
a little bit if you have a problem with
a service that crashes or hotspots or
whatever that's gonna affect a small
part of your application the rest of the
application will continue to work so
unless you are unlucky and your token
service goes down which basically means
that nobody will be able to authenticate
if if you have a big application with
lots of services and one minor service
goes down then the rest of the
application will continue to work so
it's a partial failure not a complete
failure at the case of a monolith I've
mentioned that you can scale
individually the services and adapt to
the loads and finally also very
important you can choose the best
technology stack for each service they
don't need to be all written in Python 2
or Python 3 they can all be written in
the best tool and in the example of
going from Python 2 to Python 3 if you
started this application with Python 2
then you could start migrating services
one by one to Python 3 and as long as
the communication mechanisms between
your services standard so you will do
for example HTTP for example then
everything will continue to work and you
can do a gradual upgrade to a new
technology likewise if you find that you
need to write a new service and for some
reason you find that goal or node or
ruby is a best choice it's absolutely no
problem you can do that service in the
in a different technology and that it
doesn't really matter of course it's not
you know all process and you know
benefits there are some some problems
too
one problem that I see you know I
enforce the the fact that things become
simpler this this is really true but now
not so much the complexity doesn't go
away completely the complexity goes into
the if you look at the diagram he goes
into the arrows so the complexity
migrates from inside the green boxes
into the arrows and now
have a web of connections that sometimes
gets pretty crazy so you have to make
sure that for example you don't have a
cyclic links the service calls service a
call service B and then B eventually in
subconscious a again you know you may
need to look for inefficiencies in that
sort of thing I suggested by showing the
the boxes that each service has its own
database so something that people like
me which like relational databases a lot
I suffer with being unable to do the
joins because now each service has its
own database so if you need to create a
join in this example between users and
messages you have to do it in the
application there's no way to use SQL
because it's to take a basis and you
know once every scanner access the
database from the other and we don't
want to keep things separate and be able
to operate these services separately
deployments are hard and you know the
Bob's people who tell you that you know
it's just greedy for for DevOps people
but you know there are so many moving
pieces that you know it requires a
full-time job sometimes to keep things
going when we have this type of
architecture and then finally you you
have this pinball effect right each
service that's small things so when the
client requests a complex action then
that that may require usually requires a
request to pinball through different
services like the entry service could be
messages messages may need to talk to
tokens to verify the authentication they
may need to talk to users to get user
information they need to talk to socket
IO to to push a notification to the
client so basically it becomes less
efficient so you have to keep that in
mind too
so response times for the client they're
not gonna be as great as when you have a
single codebase
so you you may wonder how do you go
about transforming or converting or
refactoring a monolithic application
into microservices and fortunately
that's that's pretty hard and but
basically there are the three main
strategies the the one that's the
probably the easiest is to say okay what
I have so far I'm going to keep I'm not
gonna worry about that but then anything
new that I start building from now on
I'm gonna build you know using small
services micro services that's that's
the easiest strategy not the greatest
one because you still have a monolith
that you need to grandfather into your
micro services platform another option
would be to to start with the big
service incorporate that but in overtime
you start breaking away parts of that
big application into small services so
eventually over time you are going to
end up with a micro services
architecture that's a pure but but then
there's maybe a potentially long
transition time where you will be you
will be working with a hybrid
that's probably what most people do when
they do this and then finally you can
you can use the line in the sand
approach and say okay I'm going to
refactor this application into my crease
is you know today it may take you a week
or two weeks or a month then when you've
done then you have a complete
application that's you know that that's
fully micro services enabled something
that's important and I see a lot of
projects that they say okay I'm gonna do
micro services and all they do they
start writing services and that's
probably obviously 50% of the equation
you need to have a platform that's
proper for micro services to live and
basically this if we
okay if we go to the sorry if we go to
the diagram this is basically the load
balancer and the service registry are
very important components that that need
to be in place even if you have a
monolith that you are transitioning into
micro-services you have to figure out a
way to incorporate that monolith into
the platform that allows microservices
to exist I'm gonna describe what that
means but but before I get into a little
bit more theory and I'm gonna do a demo
and then if I run out of time then at
least I get the fun part done okay so
this is this micro flag application so
I'm gonna show you
it's basically a chat application pretty
standard so you can go to another tab
you can see that things are looking at
the bulbs it's probably playing with
this before so now I can create another
user you know so pretty simple stuff but
let's let's look under the hood a little
bit this is probably gonna be whoa
what's what's this so this is an open
source load balancer so that the the
yellow the yellow box that you saw on
the diagram on the left
it's called H a proxy probably you are
if you don't know this one you probably
know nginx maybe you know traffic which
is another one that's uh that's kind of
becoming popular these days so all the
these tools do is basically you tell
them you know we're all your services
and then you have clients connect to
this thing
this is sort of a switchboard a control
traffic control that basically shares
all the requests that come from clients
among all your instances now it this is
super busy I'm not gonna explain
everything because it's irrelevant to
our purposes here but if you if you look
at the the sections there are six
sections the top section basically shows
you status about the this tool H a proxy
listening to requests we're gonna ignore
that
there's there's five more sections and
these these five sections are for the
five services that we have these five
green boxes you can see that messages is
running three instances or running
version four of the messages service and
we have three
so when requests come to this load
balancer they're going to be assigned to
one of these three and basically H a
proxy will make sure that all three stay
more or less equally busy we have one of
Sicario we have two of tokens and then
one of UI and one of users so you can
see that I'm scaling independently so
this is all running in a vagrant machine
so I'm going to log in to show you some
fun stuff so one thing I should say you
probably used to hear microservices
associated with you know platforms like
kubernetes you know that type of thing
which you can use I'm not using that
right now this is all you know I'm a
flask guy I like simple stuff so this is
all built using bash and a little bit of
Python so this platform doesn't use any
you know professional you know
professional-grade microservices
platform I have a chip Roxie I have a
service registry and then a little bit
of batch so for example I have a bash
script that runs a new service I can say
for example it's let's run users
oops geezers so this is gonna run a new
container this is all based on docker
containers so I'm running a second users
you are going to see in a little bit
each a proxy please update and show me
to tell you so the the way H a proxy
operates is a little bit clunky blanks
the screen but that doesn't mean it goes
down it's just the web panel that it's a
little bit clunky
but anyway you can see at the bottom now
I have two users so all I did was run
this I can show you it's looks awful but
somewhere in here here at the top you
have the new this is a new users
container that I started so just by
starting the container the container
itself talks to the service registry
which is this database that knows about
everything and then the service registry
knows about it and that that gets
communicated to H a proxy so it cheaper
obviously puts that service online
immediately now I'm gonna just be nasty
here and I'm going to kill that guy so
the moment I stop it a chap Roxy's gonna
notice that something's going not right
so it's gonna blacklist that that
service so immediately you know any
requests are coming they're gonna go to
the good one there the other one right
and in a few more seconds since this
isn't coming back then it's gonna go
away completely so this is one cool
thing that you can do with
micro-services super easy start and stop
things I mentioned before that upgrades
are really fun and very efficient so as
soon as this this red guy goes away I'm
going to show you that have another
another bash script that's that's going
to upgrade messages I have three we go
have three messages instances
they're running before now on this
instance I have already here a version 5
that I'm about to deploy so I'm gonna
say enough upgrade roll messages and pay
attention to what happens now so you are
going to see a v5 messages come up
please thank you so so one B 5 and now
one v4 is going down another Wi-Fi is
gonna come and another before it's gonna
go down so basically as you see there's
always at least three that are running
so we never stop
we never happen we never have less than
three which is what we intended that are
running eventually you know the three V
force are going to be killed and they're
gonna be replaced but the three v5 s we
have one more that needs to go down
there you know so that that's how you do
a parade without stopping and so I think
it's time if if I look at these two guys
there's the grind somehow I got two
bombs there but other than that these
are still connected they never lost the
connection so so people using this
service will never notice that you're
doing upgrade okay so that was the demo
so in the time I have left okay
not much yeah so five minutes in five
minutes I'm gonna try to rush through
this I'm gonna describe the pieces that
that make this that the build is so we
have a local answer I mentioned that I'm
using H a proxy basically having a load
balancer when you do micro services is a
must you you can try to not use a lot
down
but it's really you lose a lot of
benefits you you basically get to do
very simple rolling uprights I showed
you a bash script that can do a rolling
upgrade without going down and that's
only because we have a lot down so that
that supports this this architecture you
can do a/b testing green blue diplomacy
all these cool things you hear the you
know the the the very popular companies
the fables the Facebook's and the
Netflix is talking about but they do the
chaos monkey you know all those things
you can do and the load balancer is the
main piece that supports this so super
important that you have one in place
before we all you start doing this the
service registry is it's it's a database
that usually it's designed to be highly
available you run multiple instances of
it redundant and
basically super fast it caches stuff in
memory so that you can you do queries
are very quick and basically the service
registry is basically collected from all
the services when they start so that it
registered themselves and then if the
service dies the the connection depends
on the system but usually the connection
has a TTL so if the service doesn't
refresh that connection and says hey I'm
still here hey I'm still here
when stop saying that then the service
registry will remove it and then
immediately will talk to the load
balancer and it'll remove it from the
browser as well so basically that that's
the whole magic if you want to know I
didn't put it here but the registry that
I'm using is another open source project
it's called HCD etcd it's it's very
simple it's actually the one that
kubernetes uses as well
containers are a big part in usually you
see micro services platforms are always
on containers it's only because it makes
things much more easy the container
provides a layer of isolation they're
great better than
processes for example it allows you to
to work with virtualized network ports
all these services that you've seen that
these five services in the example plus
all the instances of all these services
are all running on port 5000 which is a
flask default and and then docker takes
care of you know mapping that into some
other port that I don't even care right
I don't know what it is but but you know
for me writing the services it's for
5000 every time so that that's really
nice you can do that yourself if you
don't use docker but you know it makes
things a lot more difficult you know of
course if you're using different
different technologies then having
containers make sure that you have
collisions between conflicting
dependencies as well we have the orange
boxes so storage your storage containers
usually give database your service
registry can also be considered storage
a message queue you know all of those
for all those for production platform he
would look for something that's highly
available something that you can make
clusters off so typically if you're
using for example my sequel you will
look at Galera which is a cluster
solution or auroral if you double yes
you know all those things that basically
make the service very reliable not
running one instance that if it dies
then the whole thing goes down right the
same thing for queues and and so on and
then you have your applications these
are the green boxes and these
applications are stateless so I need to
rush these are stateless so that what
allows me allows me to start and stop
these services they have no data in them
they use the storage services to store
data so this allows me to start kill and
it doesn't really matter they all
basically disposable I recently scale
them for free I can run as many as I
want and basically the more around the
more load I can
and alone that service and I think I'm
gonna skip this one in spirit of saving
time I already talked about this
basically the lifestyle of the
microservice it starts to talk to this
earth to the satellite III and then when
he dies stops talking to the service
right history and that translates into
the load balancer removing it from from
itself configuration and then finally we
have service to service communication
which is the mechanism by which the
services talk among themselves this
example I'm using HTTP as a way for
internal communication I mean there are
many projects that decide to only use
HTTP 4 for a client communication into
the into the project but then internally
they use different mechanisms which is
totally fine as long as it is a
mechanism that doesn't restrict your
choices of technologies so you should
find standard RPC mechanisms for example
that would be a very good way to to do a
more efficient less chatty communication
than HTTP so I'm gonna give you a link
to the slides but if you want to try
this example yourself you can this runs
in a vagrant virtual machine these are
the instructions and the requirements
depend on this application and play with
it like I played here and tomorrow I'm
gonna talk in more detail about this if
you want to learn how this was built</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>