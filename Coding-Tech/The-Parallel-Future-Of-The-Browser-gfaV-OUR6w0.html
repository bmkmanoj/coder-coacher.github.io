<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Parallel Future Of The Browser | Coder Coacher - Coaching Coders</title><meta content="The Parallel Future Of The Browser - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Parallel Future Of The Browser</b></h2><h5 class="post__date">2017-11-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gfaV-OUR6w0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi i'm lynn clark and i make code
cartoons I also work at Mozilla where
I'm in the emerging technologies group
so that's things like the rust
programming language and web assembly
which is a fast way to run languages
other than javascript in the browser we
also work on servo which is a new web
engine and I'll be talking about all of
these things today because they're all
part of the future of the browser so why
talk about the future of the browser
because browsers are facing a challenge
and whether or not browsers meet this
challenge could change the web as we
know it what is this challenge browsers
need to get faster let's look at the
trajectory of the speed of the browser
over the past two decades in the early
days of the web speed really didn't
matter that much we were just looking at
static documents so as soon as that
document was rendered to the screen the
browser was pretty much done with it it
might need to do a little bit more work
if you scroll the page up and down but
that work wasn't too complicated then
people started pushing the boundaries
they started thinking what can we do
with this web besides just delivering
static documents they just started
getting interactive they started having
animations like do you remember back
when everybody went dropped down crazy
and you had drop-down menus everywhere
people created these fancy sliding up
and down and in and out ones with jQuery
once those were part of the page the web
page wasn't just being painted once to
the screen with every change it needed
to be repainted and sometimes like when
you had this motion it needs to be
repainted multiple times for that change
to give you that sense of movement
so for every one of those changes there
were multiple repaints of the screen and
if you wanted interactions and
animations to look smooth those repaints
needed to be happening at a certain rate
there needed to be 60 of them every
second so that meant that you only had
16 milliseconds to figure out what
exactly the next version of this page
should look like browsers made all sorts
of changes to come up to speed to
accommodate these new applications and
get up 60 frames per second what as is
the way with the web content authors
started pushing the boundaries even
further
they started not just making their web
applications more interactive but
bringing whole new classes of
applications to the web like PC games
and companies started talking about
bringing their applications to web
applications like Photoshop these new
classes of applications are taxing the
web even more and making clear that
things like JavaScript need to get even
faster it's not just the content authors
that are pushing these boundaries either
it's also hardware vendors for example
the new iPad is going from 60 frames per
second up to 120 frames per second that
means that the browser has half as much
time to do just as much work and new
kinds of content are coming to the web
and pushing this even further for
example with VR you have two different
displays one for each eye and they both
need to be going at at least 90 frames
per second to avoid motion sickness on
top of that a lot of these are at up to
4k resolution which means you have a
whole lot more pixels that you actually
have to paint let's think about what
this change means
if we're running a website on a 13-inch
MacBook Pro we have 16 milliseconds to
fill in 4 million pixels with the next
iPad you have half as much time you have
8 milliseconds to do 3 million pixels
with a VR experience you have 11
milliseconds to fill in sixteen point
five million pixels and this doesn't
even include any heavier JavaScript
needs that is a huge leap that browsers
need to make in order to keep up
what happens if browsers don't keep up
well as more and more people buy these
new devices and as more and more content
moves towards these heavier applications
if browsers don't keep up people will
stop seeing the web as the default place
where they should put their content and
this could mean that the web as we know
it withers which is a pretty scary
thought but to be honest I'm actually
not too worried about this I'm confident
that browsers can make this leap and the
reason that I'm confident is that at
Mozilla we've been prepping for this
change for the past ten years we've been
looking at the direction that computer
hardware is going we've been figuring
out the new way that we need to program
to keep up with these changes
the answer is parallelism the future of
the browser is parallel
we've only just started taking full
advantage of this and Firefox but we're
already seeing big wins from it and
every indication is that this new way of
doing things can get the browser where
it needs to be so in this talk I want to
explain exactly what browsers need to
change in order to keep up with these
changes but before I do that let's talk
about what the browser actually does I'm
going to start with the rendering engine
so this is the thing that takes your
HTML and CSS and turns it into pixels on
the screen it does this in five steps
but to make it simpler I can split up
these five steps into two different
groups the first group takes the HTML
and CSS and figures out a plan it
figures out what the page should look
like and this is kind of like a
blueprint it specifies where everything
will go on the screen and asbestos
things like the widths and the color and
the element Heights of elements and then
the second group takes this plan and
then it takes it turns that into pixels
the pixels that you see on your screen
now let's look more closely at each step
in this process the first step is called
parsing what the parser does is it turns
the HTML into something that the browser
can understand because when this HTML
comes into your browser is just one big
long string of text it's kind of like a
big long paper ribbon that has a lot of
characters all in a row but what we need
is something different something that
the browser can actually use we need a
data structure that tells us what the
different elements on the page are and
how they're related to each other like
parent-child relationships
I think of this kind of like a family
tree we need to turn this long paper
ribbon into a family tree of the page so
what the parser does is it goes along
this paper ribbon with a pair of
scissors and when it sees the opening
tag for an HTML element it cuts out that
HTML element and it puts it on the wall
into this family tree so for example if
it came across a div it would cut that
out and put that into the family tree
and then the next element that comes
across goes under that div and it draws
a line to represent that parent-child
relationship this family tree is called
the Dom tree the document object model
and it's called that because it provides
a model of the document which we can use
to make changes whatever changes we want
to make to the page so if JavaScript
wants to change what shows up on the
page for example if we wanted to not
show up some of these paragraphs on the
page it can make a change to that family
tree and then those paragraphs would
disappear from the page at the end of
parsing we have this family tree the Dom
tree and that tells us about the
structure on the page and those
parent-child relationships what it
doesn't tell us is what these things
should look like in order to figure out
what they would look like we take the
CSS that we've downloaded and figure out
which Styles apply to which of the
elements in this tree so that's the next
step CSS style computation
I think of CSS style computation like a
person filling in a form for each one of
these elements in the tree this form is
going to tell us exactly what this
element should look like so for example
it's going to tell us what the color
should be and what the font size should
be
it has more than 200 formfields on it
one for every single CSS property and
this worm needs to be completely filled
out for every element in this tree we
need to fill in all of the form fields
on this form but for every element there
are going to be some properties that
aren't set in our CSS files you aren't
going to type in 200 different
declarations for everything that you
have on your page so we're gonna have to
figure out values for those missing CSS
properties this means going through
multiple steps for each element first we
figure out which rules in the CSS apply
to the element and the rule is the thing
between the braces you know all of those
properties that you declare for example
if you have a paragraph and it has
margin and padding the rule contains
both that margin declaration and the
padding declaration and if we're on a
Dom element if we're going through this
tree and we're on a paragraph Dom
element then that matches that rule
matches that Dom element and multiple
rules can match a single Dom element
this process of figuring out which rules
apply to the Dom element that you're on
is called selector matching so we get
this list of matching rules whatever we
match to this particular element and
then we sort those rules we figure out
which is the most specific and then we
fill in the form field values based on
that we take whatever properties it has
and fill that into the form and after
that we go to the next most specific and
if it has values for anything that we
haven't already filled in we fill those
in and we keep going down the list until
we get to the end of this list of rules
but we're still going to have lots of
empty form fields if none of the
matching rules contained a declaration
that for this property then that
property is going to be empty some
properties get a default value in this
case and others inherit from their
parents in that case we just look at the
parent and use whatever value it has and
that process is called the cascade so at
the end of CSS style computation each
element has its form completely filled
out with all of the CSS property values
but there's still a little bit more that
we need to figure out we need to figure
out how wide and how high things are
going to be and where they're going to
be on the page the next step layout is
what takes care of this so it looks at
the dimensions of the browser window and
from that it calculates those relative
values so if I div is 50% of its parent
and we'll figure out exactly what that
means it will also do things like break
up a paragraph element into multiple
elements one line for each pair one for
each line in the paragraph and that way
it knows how many lines hi the box needs
to be to accommodate that whole
paragraph the output of this step is
another tree in Firefox we call it the
frame tree but in other browsers is
called the render tree or the box tree
and this is the ultimate plan so this is
that blueprint this is what tells us
exactly what the page should look like
now we move on to the next part of
rendering which is turning that plan
into the pixels on the screen before I
get into the details of this I want to
talk about what that means what it means
to put pixels on the screen you can
think of the screen as basically a piece
of graph paper there are lots of tiny
boxes in rows and columns
and when you're rendering what you're
doing is you're coloring in each one of
these boxes with a single color of
course there's not actually graph paper
in your computer instead there's a part
of memory called the frame buffer in
that part of memory there's a box that
corresponds to every pixel it says what
color the pixel should be and the screen
checks that part of memory every 16
milliseconds it will check that part of
memory and see what the color should be
for each pixel whatever colors are in
that frame buffer whenever it checks or
what gets shown on the on the screen but
we don't just fill in this frame buffer
once we end up having to fill it in over
and over and over again any time you
have that interaction or animation
anytime you ever change to the page you
have to update that frame buffer even if
the user is just highlighting something
on the page though so even if you don't
have interactivity in your site that
acts of highlighting means that the
browser is going to need to refill this
frame buffer as well this frame buffer
can be pretty big depending on how big
your screen is you can have millions of
pixels that's a huge piece of graph
paper
it means that filling in pixels can take
a really long time especially when there
are a lot of things that are rapidly
changing on the page because of this
browsers have tried to figure out ways
to reduce the amount of work that they
have and make it faster and one way that
they've done this is by splitting up
what gets rendered into multiple
different layers these layers are like
you have in Photoshop or I think of them
like the layers that you would have had
if you were doing animation in the old
days like those onion skin layers that
they used to do Bugs Bunny cartoons on
you have the background on one layer and
then the characters on another layer or
multiple layers then if the characters
move you don't need to repaint the
background the background just stays the
same it's only that top layer that needs
to change so the first step in this
process is called painting that's where
you actually create these layers and the
next step is called compositing that's
where you take these layers and put them
together and then you basically take a
picture of it and that is what goes to
the screen that's how the page gets
rendered now I frame this as the way
that a web pages content goes from HTML
and CSS to pixels but what you might not
know is that there's another part of the
browser the tabs and the URL bar all of
that stuff that parts actually separate
it's called the browser chrome in some
browsers like Firefox rendering browser
Chrome is also handled by the rendering
engine so you have this rendering engine
and it has two different tasks that it
needs to do it has to render the inside
of this window which is called the
content and the outside which is called
the Chrome
actually it's not just two tasks though
because for any tab that you have open
there's going to be another content
window even if it's pixels aren't
showing because it's not the selected
tab it could be running JavaScript in
the background so the browser has
multiple things to do and they're pretty
independent of each other and this is
where one of the earliest places where
parallelism was introduced to the
browser in 2008 you started seeing a
browser take advantage of new hardware
to run these all at the same time
independently of each other but it
wasn't us that did that it was chrome
when chrome launched its architecture
was already using parallelism like this
it's called the multi-process
architecture and that's one of the
reasons why chrome was faster and more
responsive than firefox now I feel like
I should take a step back here and
explain what this all really means what
Hardware changed specifically chrome was
taking advantage of and what that change
made possible so let's do a little crash
course in computers and how they work
you can think of a computer kind of like
you think of a brain there are different
parts of this brain there's a part that
does the thinking
so that's addition subtraction any
logical operations like and or or and
then there's some short-term memory and
these two are pretty close together in
the same part of that brain and then
there's long term memory now these parts
all have names the part that does the
thinking that's called the arithmetic
and logic unit the short-term memory
those are called registers those two are
grouped together on the central
processing unit or the CPU and the
long-term memory is called random access
memory or Ram
now in order to get this brain to do
anything we need to give it an
instruction and this instruction is
going to tell us what we need to do with
some bit of the short-term memory each
box of the short-term memory has a label
so that we can refer to it and then we
can use these labels in the instruction
to say which value the instruction
should act upon for example we could add
a number to a value that's in short-term
memory to get a result so we could add 1
to the value that's in our for now one
thing that you may have figured out from
this is that we can only do one thing at
a time
this brain can only really think one
thought at a time and that was really
true for most computers from the
earliest computers to the mid 2000s even
though these computers have these
limitations for all those years though
they were still getting faster they were
still able to do more every eighteen
months to two years or so they were
getting twice as fast
you could run twice as many instructions
in the same amount of time what made it
possible for these computers to get
faster was something called Moore's Law
the little electrical circuits that you
used to build all these components like
the CPU they were getting smaller and
smaller
that meant that more and more of them
could fit on a chip so with more of
these building blocks you can make more
powerful CPUs and also there was less
distance for electricity to travel
between the circuits so they were able
to work together faster but of course
you can only make things so small and
there's only so much electricity that
you can coerce through a circuit before
you start burning it up in the early
2000s these limitations were starting to
become apparent chip manufacturers had
to think how are they going to make
faster and faster chips
the answer that they came up with was
splitting up this chip into more than
one brain basically making it possible
to think more than one thought at a time
in parallel these separate brains that
the CPU has are called cores when you
hear people talk about a multi-core
architecture that's what they're talking
about even though each one of these
cores or each one of these brains is
limited in how fast it can think if you
add more of them they can do more in the
same amount of time but the thing is in
order to take advantage of this you need
to be able to split up the work that you
have to do across these different cores
across your different brains unlike
before where the speed ups were
happening automatically and programmers
didn't need to do anything with this for
these speed ups they actually require
programmers to change the way that they
code this is harder to do than you might
think
imagine that two of these cores need to
work with the same bit of long-term
memory
they both need to add something to it
well what number is going to end up in
long-term memory at the end of this
calculation who knows it depends on the
timing of when the instructions run on
the different cores so let's walk
through an example we start with the
number 8 in long-term memory and both
cores need to add 1 to it so our end
result should be 10 instructions have to
use things that are in short-term memory
they can't act on long-term memory
directly and each core has to has its
own short-term memory so let's say that
the first core pulls 8 from long-term
memory into its short-term memory and
then it adds 1 to get 9 and then puts
the value back into long-term memory
that means that the other cores can now
access the result of this operation so
the long-term memory holds 9 now and the
second core is going to pull 9 into its
short-term memory add 1 to get 10 and
then put 10 back in long-term memory
that means our end result is 10 so all
is well but it wasn't guaranteed to end
up this way let's see what happens when
we change the order that the
instructions happen on the different
cores the first core pulls 8 from
long-term memory and then the second
core pulls a from long-term memory and
you may already see where the problem is
here then the first core adds 1 to get 9
and then puts 9 back in long-term memory
then the second core as 1 to get 9 and
puts 9 back in long-term memory so we
end up with the results of 9 which is
not what we wanted this kind of bug is
called a data race when you have
parallel code with shared memory so two
different cores working with the same
part of long-term memory at the same
time you're very likely to have these
data races one way to get around this is
to choose tasks that are pretty
independent of each other so that they
don't need to share memory now let's go
back to the chrome and content example
that I was we were looking at before you
might remember that I said all of these
are fairly independent of each other
that means that they're perfect for this
kind of parallelism where you don't have
to share memory between the cores and
that's called coarse grained parallelism
that's where you split up your program
and some pretty large tasks that can be
done independently of each other so that
they don't have to share memory but
they're still going at the same time
it's actually pretty straightforward to
do this you just need to figure out this
you just need to figure out yeah woke
you all up so so this coarse-grained
parallel parallelism it's pretty
straightforward you just need to figure
out those large independent tasks so
chrome had this from the beginning the
chrome engineers saw that they were
going to need to have some level of
parallelism to be fast with these new
architectures around the same time that
chrome was seen this change in hardware
and seeing that if they wanted to have a
fast browser they were going to need to
take advantage of this parallelism we
were seeing the same thing we knew that
we were going to have to have this
coarse grained parallelism in our
browser to if we were going to keep up
and we do note now although it took us a
while to get there
it was a multi-year effort we started
testing our multi-process architecture
and Firefox 48 with a small group of
test users but it wasn't until this past
summer with Firefox 54 that we turned it
on for all users it took us a while to
get there because we weren't starting
fresh like chrome was we had a bumpy
road we were starting with this existing
code base which was developed before
multi-core architectures were common and
we needed to figure out how to break
apart this codebase without breaking
anything for our users while we were
doing it so we needed to plan for that
but we didn't just stop and making plans
for this coarse grained parallelism we
saw that we were going to need to take
it further because when you have this
kind of course brained parallelism
there's a good chance that you're still
not making the best use of all of your
cores
thank you thank you for reminding me
where I was so we thought we were gonna
take it further because when you have
this kind of coarse grained parallelism
there's a good chance you're still not
making good use of all of your cores of
all of the hardware that's in your
machine for example you might have one
tab that's doing a whole lot of work but
the others might not be doing much work
at all that means those other cores are
sitting idle so you're not getting the
kind of speed up that you could get from
a parallel architecture we saw that if
we wanted to make a browser that was
really fast we couldn't just add this
coarse grained parallelism we needed to
add fine grained parallelism too so what
is fine grained parallelism well that's
when you take one of these big tasks and
you split it up into smaller tasks these
can be more easily split up across the
different brains so across your
different cores but that does usually
mean that you're gonna have to share a
memory between the cores and you know
this opens you up to those data races
that I was talking about before these
data races are nearly impossible to
avoid when you're sharing memory and
they're incredibly tricky to debug the
thinking at that time was basically that
to safely program in parallel you had to
have kind of a wizard level
understanding of the language that you
were working in one of the distinguished
engineers at Mozilla actually put a sign
about eight feet high and said you must
be this tall in order to write
multi-threaded code now of course when
you have a project like an open source
browser where you have thousands of
contributors we're adding code to the
codebase you can't code in a way that
requires a wizard level understanding of
the language if you do you're going to
run into these data races for sure
and these data races and other memory
issues cause some of the war security
vulnerabilities and browsers so if we
wanted to take advantage of this fine
grained parallelism without the peril
that these data races introduce we
couldn't just start hacking a parallel
browser we had to find a way to make it
safe to do that so rather than starting
a project to rewrite the browser we
started sponsoring a project to create a
new language to write that browser in
this language is the rust programming
language as part of its design and make
sure that these kinds of data races
can't happen if you have code that
wouldn't introduce these data races to
your codebase it just won't compile we
started sponsoring work on rust around
2009 or 2010
it wasn't until 2013 that we actually
started putting it to use in a browser
though you know we started seeing
whether or not it could create the
browser that we wanted I don't know if
any of you have heard of the term hack
shaving where you have to do one
seemingly unrelated task before you get
to the thing that you really meant to do
but at Mozilla we have some pretty big
yaks to shave
so in 2013 with this language in hand
that allowed us to code in parallel
without fear we started looking at how
we could really introduce fine grained
parallelism into the browser the project
that we started to do this is called
servo we started by looking at the
rendering engine pipeline and asking
what happens if we paralyze all of the
things this means we're not just sending
the different content windows to
different with different pages to
different cores we're taking a single
content window and splitting up the
different parts of that page that means
if you have a site like Pinterest each
different pinned item can be processed
separately from the others for example
for CSS you could send each pinned item
to get a CSS filled out by a different
core this means that you can speed up
different parts of the rendering
pipeline by however many cores you have
which means that as chip manufacturers
add more cores into the future these
pages are going to get faster and faster
automatically this is the key this is
why fine grained parallelism is so
important and this is why we spent so
much time and risked so much in pursuing
it because it wasn't clear at the start
of this project that it was actually
going to work coarse grained parallelism
is pretty straightforward but this fine
green parallelism creating a language
that made it safe and then implementing
it in a browser that was a tough
research problem but that time and
effort is paid really paid off we found
out that these ideas work and that they
work really well over the past year
we've started bringing pieces from servo
into Firefox so we've been doing this as
part of project quantum which is a major
speed-up of firefox that we've been
working on for the past year it's kind
of like replacing the different parts of
a jet engine mid-flight one thing that
we brought over is are parallel style
engine called stylo that splits up all
of the CSS processing across the
different CPU cores as I mentioned
before it uses a technique called
work-stealing split up that work
whenever a core runs out of work it can
still work from the other cores and this
makes splitting up work efficient and
makes it possible to speed up CSS style
computation by however many CPU cores
you have another piece that we're
bringing over is called web render web
render takes the painting composite
stages and combines them into a single
rendering stage it uses the hardware in
a smart way to make painting the pixels
faster which means it can keep up with
those larger displays to do this it uses
another highly parallelized part of the
computer which is specifically meant for
graphics this is called the GPU or the
graphics processing unit the cores on
the GPU are a little different from the
cores on the CPU instead of having two
or four six of them like you have on the
CPU there are hundreds or thousands of
them but they can't do things
independently they have to all be
working at the same time on the same
thing you need to do a lot of planning
if you want to maximize the amount of
work that they can do at the same time
and that's what web render does so with
web render we can get rid of performance
cliffs that trip up web developers for
example if you anime background color
right now your animations start can
start and stop it can get janky it can
make it look janky because the paint and
composite phase have too much work to do
and because of this there are currently
a lot of rules about what you should and
shouldn't animate we're going to be able
to get rid of a lot of those rules so
web developers don't need to hack around
these performance cliffs we can take
pages that render in chrome today at 15
frames per second and bring them up to
maximum fps whether it's 60 frames per
second or 120 frames per second but it's
not just the browser's own code that the
browser is going to need to run faster
it's also going to need to run
application code faster so what I'm
talking about here is stuff like
JavaScript
now the JavaScript engine is a part of
this pipeline that I haven't talked
about yet so let's see where it fits in
JavaScript gives you a way to change the
document object model so that's the
thing that you built up that family tree
wit that we built up during the the
parsing phase when you change the Dom it
triggers the creation of a new version
of the page through this pipeline a lot
of sites these days are pretty
JavaScript heavy that's where they spend
a lot of the time that they have for
figuring out what the new page should
look like these things are using
frameworks like react which does a lot
of calculations in JavaScript to figure
out what changes it needs to make to the
Dom now browsers can make the JavaScript
engines that run this JavaScript faster
but JavaScript isn't really designed for
it to be easy to run quickly
instead it's designed to be easy for
humans to write the way that Jay s
engines get fast is by making guesses
about where they can take shortcuts with
your code but sometimes those guesses
don't work out for your code so it could
actually make your Co slower and there
are only so many guesses there are only
so many shortcuts that it can make which
means that there's only so much that the
jas engine can do to speed up this
application code but what if application
code could run in parallel
what if application code could take
advantage of these multiple cores in the
same way that the browser's own code is
doing over the past few years browsers
have been adding features that make this
possible one that you may have heard of
before is called web workers and those
have been in browsers for a few years
now they allow you to have J's code
which runs on different cores you may
have also heard of shared array buffers
which started landing in browsers this
past summer those give you the shared
memory that I was talking about which
you often need if you have this fine
grained parallelism
but like I talked about before it's
pretty tricky to actually manage the
shared memory on your own and web
workers can be really hard to use that's
why so few sites and applications are
using them today even though they've
been around for a number of years it
would be nice to have a language like
Rost which gives you those guarantees
that you're not going to have data races
and which makes it easy to work across
different cores across these different
workers without having to do too much on
your own well there's actually another
standard that landed in browsers that
can help with this it landed this past
year webassembly makes it possible to
run other languages besides just
JavaScript in the browser now it doesn't
quite have access to the Dom yet but the
working group is working on it and
they're already very close to getting
their threading support finished and
that will make it possible to
efficiently run web assembly across the
different cores there are also other
things about web assemblies that help
with application performance it was
designed for machines to run it quickly
it wasn't designed to be easy for humans
to write and the reason for that is
because usually when people are writing
web assembly they're not writing it by
hand instead they're writing in the
language like Rost or C or C++ that
compiles to web assembly so that means
that web assembly doesn't have to be
easily readable by programmers and so
this means that the engine doesn't have
to do that guesswork to figure out where
it can take shortcuts with your code
this speed of web assembly even without
threading even without multiple cores is
what's making it possible to run PC
games in the browser today
so these standards webworkers and shared
array buffers and webassembly they make
it possible for applications to take
advantage of parallelism - for example
in a framework like react you could
rewrite the core algorithm in parallel
and that way the work of figuring out
the changes that it needs to makes in
the Dom that could be happening across
the different cores an ember is already
starting to experiment with web assembly
for their glimmer vm so they may be able
to start introducing some of this
parallelism - and take advantage of it
and I think we're gonna see a big shift
towards using these standards in
frameworks over the next few years so
let's get back to this challenge that we
had here are the pieces of the puzzle
here's how we address this challenge
this is what browsers need to do to
support the new devices and the new
types of applications that are coming to
the web there's the coarse grained
parallelism of splitting the different
content windows in the chrome across
different processes
there's the fine grained parallelism of
splitting up the work of a single web
page so that it can be distributed
across different cores and then there's
an abling application code to be
parallelized as well the coarse grained
parallelism is already there in all of
the browsers chrome was the first to do
this but pretty much all of the
browser's have caught up by now
an abling application code to go
parallel that's something that's
happening in standards bodies and it's
being adopted by all of the browsers
it's this fine grained parallelism
that's the low question mark this is
where most browsers have done the least
so far and it's actually not clear how
to do it in most browsers because the
C++ that most browsers are written in
it's actually pretty hard to parallel
lies in this way but I think that all of
the browser's are going to need to do
this we may be the first browser
together we may be the first browser to
actually get this fine grained
parallelism in there and deliver the
speed ups but we really want all of the
browser's to get there too we want them
to all get there with us because that's
the way that we're going to keep the web
going that's the way that we're going to
keep it healthy and vibrant no matter
how much the browser's limits are pushed
thank you to be on teller and for having
me and thank you all for listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>