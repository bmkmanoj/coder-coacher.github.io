<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>7 Steps Of Machine Learning | Coder Coacher - Coaching Coders</title><meta content="7 Steps Of Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>7 Steps Of Machine Learning</b></h2><h5 class="post__date">2017-12-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FMIvrZB-6Nc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my talk is going to focus more on kind
of running through a framework to think
about how to approach a machine learning
problem how you can use this overarching
idea to then inform especially some of
the earlier steps to save you time and
energy down the road and what we'll do
is we'll walk through the conceptual
aspect of it relatively quickly and then
I want to get into a more concrete kind
of discussion we'll run through some
tensorflow code and I'll show you guys
some other tools that I use to achieve
each of those seven steps so with that
out of the way let's let's get going
also feel free to hit me up on Twitter
if you have any questions things like
that to follow up and that'll be at the
end of the deck too I think
okay so my like you know we've talked
about a lot about machine learning
tonight already my super short
definition for machine learning is that
it's just using many examples to answer
questions and you can split that up of
course into these kind of two sides
right showing many examples and
answering those questions and broadly
those kind of split into training and
prediction and the reason I showed this
slide is to emphasize the point that
machine learning is useless if you drop
either side in the sense that the best
model in the world if you can't serve
predictions to your users reliably
securely and scalable right assuming
that those are all important
requirements to you then that model
isn't very useful conversely if you have
this really great scalable system and
back-end but your model is really
inaccurate because your training didn't
work very well and it's not recognizing
the data patterns that's also not going
to work so machine learning is kind of
extra challenging because of that you
have these two aspects that are kind of
fundamentally different training
requires a certain kind of mindset
were you thinking about the data its
structure how you can get more
information out of it but when you start
serving predictions he kind of you have
to switch to another mode in your head
to think about how you can serve these
predictions and so we'll look at that
and all the steps in between right I
promise seven steps there's only two
here so don't worry you'll get your
money's worth and let's see I think
there's one more in here right sometimes
you know we saw that with the machine
learning API is for instance on Google
Cloud that's just the prediction side
that's a situation where you are not
providing your own training data so you
don't have to do the training but you
still get the benefits of having a kind
of production ready production scale
prediction service and I think the only
other point I wanted to make with this
slide was that you can do machine
learning both in the cloud and locally
but kind of Kaz has already talked about
that so now we're gonna do this really
random thing where I switched to a
different presentation because I didn't
stitch these together because they're
different they use different slide
software so that's good all right so to
talk through our seven steps we'll use
an analogy we'll talk through a
hypothetical problem okay let's say for
some reason your employer or a friend
perhaps came to you and said gosh I have
I want to build a system just for fun
maybe to tell the difference between
beer and wine okay and you might say
well maybe you know just have a sip and
then you'll know right you can just have
a drink but you know play along and if
we think through this how might we go
about doing this right we might say well
you know we got to have a model or we're
gonna make some kind of model and we're
gonna either predict beer or wine and to
do this you might say well some of the
things that tell them apart maybe color
right wine is red beer tends to be
yellowish of course there's lots of
exceptions to that and alcohol
percentage so where can we go with that
well let's run down to the store we'll
buy a whole bunch of the air and wine
then we'll you know go to the
electronics store that's attached to
your local grocery store and grab a
spectrometer to measure the color and a
hydrometer to measure the alcoholic
content right so then we hit that first
crucial step gathering data before you
can really do any sort of custom
training gathering data is paramount and
thinking about how you want to gather
your data and what data you want to
gather will make a large large
difference down the road in terms of
your model quality its accuracy its
longevity
etc and thinking about whether you're
gonna want to update it too right if
maybe you want to build a system where
if a new type of beer gets invented that
a thing that happens I actually don't
drink that much beer if such a thing
were to happen would you be able to
update your training data and update
your model accordingly and so maybe make
it this table right this is like the the
simplest case like I said conceptual
overview you got your color right in
wavelength nanometers alcohol has a
percentage and the answer beer or wine
so you have your data and a label and
that brings us kind of to our second
step once you've collected this data
there's typically some extra work that
you need to do this being a PI data made
up this is kind of a step that I feel
like gets and size though or discussed
more more often than in a lot of other
audiences in terms of making sure your
data is well shuffled making sure that
it's well distributed right across the
two different categories you don't want
to have a lot of data points in one side
than the other and and having you know
balanced data sets and things like that
so normalizing your data making sure
you're it's truffled and all that good
stuff and I think this is saying the
same thing right so then of course you
want to split that data so this is also
part of that data preparation step once
you have this giant body of data and you
want to split it into the data you can
use for training and then a subset that
you're gonna use to evaluate your model
which you don't show it during training
your test set or evaluation set or
validation sets of words there
but making sure that you do that
shuffling beforehand for instance
especially with really large datasets
when you're talking tens or hundreds of
terabytes how do you get a good shuffle
in so that you can have a representative
sample in your evaluation set and how
big should that set be right if your
data is only one gigabyte maybe holding
out like 20% is like a reasonable size
but if your data is just you know
running on and on terabytes and
petabytes of data your evaluation set
may be end up being a very small
percentage and still be representative
but that all depends on your actual data
structure right if the data tends to be
very consistent over time then you might
not need so much repetitive data just to
like test or set so a lot of this is
kind of a soft art rather than just the
science is there a category for data
arts like everyone calls it data science
but maybe we should have a data arts
next to that as well so that brings us
to our third step right we gather our
data we've prepared our data and now we
have to choose kind of what kind of
model we want to run against it and this
is kind of a meta step in a way because
you end up playing around with what you
use and it's not necessarily a decision
you make up front and is final and never
changes and the clipart that I chose for
this lovely slide comes to us front just
to refer to the fact that there are
models that are particularly well suited
for images some are better suited for
music or sound things that are more
developed toward text models and some
that are more geared toward structured
data your standard kind of CSV file or
spreadsheet data and so you take your
model you run it through and you see how
it does right and that brings us to our
training step so you do the training and
the training I'll kind of go into the
math a little bit here feel free to his
own out if you know the simplest version
right for those who are not familiar
with this this is just for like a linear
model this is literally thinking back to
middle
cool like y equals MX plus b like draw a
line of best fit and another way to
think about training is that it's kind
of like when you're first learning to
drive i didn't research singapore
driver's license laws so actually I hope
this analogy holds but basically you
know in the u.s. when you first become
eligible to drive on
most states it's when you're 16 some
states at 16 and a half
some states at 17 you get a permit
license and you're able to drive with
someone else another adult supervising
you and so you're a student driver
basically and then you start out you
don't know how the car works right you
sit down you're like wow all this all
these controls have a steering wheel I
have a gearshift brakes and pedals maybe
you have a clutch if it's a manual and
over time through experience and
hopefully not too many crashes you learn
how to drive better and better and so a
student driver then becomes a fully
licensed you know real driver adult
drivers if you will and in a similar way
training your model and machine learning
is like that right we're showing the
model lots of examples we're practicing
driving on the roads and of course now
that I say all this it occurs to me that
this has some really direct analogies to
all the self-driving car hype and such
so the analogy is perhaps a little too
close to reality
anyhow you train your model and going
back to the math that I promised you
guys I know some of you were really
looking forward to this part when you
when we do when we learned about drawing
a line of best fit through data right
we're just like the slope and the input
outputs with these animate in okay it
went for too much no oh gosh okay so
with machine learning what makes it
better than just drawing a straight line
right because anyone can draw a straight
line you any any good or even mediocre
spreadsheet software can draw a straight
line of best fit through your data so
what makes machine learning different
what makes using linear regression on a
bigger data so different essentially you
can think of it as the fact that your
data
more than one kind of layer of data 1/2
the level so that you're actually
fitting multiple lines of best fit for
each of the different aspects of your
data is typically different features of
your data set and for sometimes you know
Cavs mentioned the image data where we
flattened it out to 784 so that
basically has 784 lines of best fit one
for each pixel and so you can take those
slopes and in terms of a representation
collapse them down into an array and you
can take the y-intercepts collapse that
down into array and you can rear enter
as a matrix as a matrix multiply but I
don't think I've made a slide for that
slides to add for the future okay and so
we have our training the data it goes
into the model and we make some
predictions and so in a nutshell the
training process takes those predictions
and compares them against the true value
right so we say here's the color here's
the alcohol percentage and then the
model white which at first is basically
initialized to random guesses will
either say wine or beer and then we'll
look at an actual data that we collected
and will say hmm was this was this right
or wrong and so that feeds back in and
updates the model and so through many
many examples you get a better and
better model that reflects the reality
so that brings us to evaluation earlier
we had that morning where we split the
data and we held out that evaluation
data set and so evaluation is in a lot
of ways very similar training the only
difference is we do not update the model
anymore literally the same except we
don't update the model we just use this
as a opportunity to test to see how well
the model is performing and once we have
and then I left this tip in as an 80/20
split but the caveat here being that
it's really dependent on data size and
the kind of quality of your data and I
don't mean that in terms of like high
quality or low quality but like what
your data is it feels like because some
data sets you really
wide representation and I've seen
evaluation datasets that are split
fifty-fifty but others that might be
split like 99 to 1 just because of the
way the data is kind of aligned and so
that brings us to almost our final step
so we did our training we did our
evaluation and now we have we want to go
back and see what we can change what
other models are there the various knobs
and levers we can pull and turn on the
model itself and how we run our training
and so we can tune these quote/unquote
hyper parameters because the model
itself has parameters right and these
are the parameters of the model kind of
meta maybe we should call them meta
parameters and so that said of like why
are these slides repeating themselves
okay so then the data will kind of you
run through the training oftentimes in
parallel you can kind of kick off a
bunch of different jobs and this is kind
of where local running can kind of you
can get in trouble with that folks will
like ran out of space on their disk will
run out of memory things like that if
you kick off multiple jobs and have like
large data sets or large models and
that's where cloud etc can help but a
lot of times I'll just do them one at a
time to see kind of how it plays out and
so you you run your your parameter
tuning and say you're happy you found a
good set of parameters and so now we're
ready to make our prediction right
finally I said there's training and
prediction and eventually you get two
predictions and then we can predict on
new data like new data that's coming and
maybe your friend comes here you're like
alright let's test this new line that
just came out of Australia or something
and you know run out through your model
or try to get a prediction and hopefully
it's you know it is what it thinks it is
so that's kind of the conceptual
overview we have our our seven steps
gathering data preparing it
choosing a model training and prediction
I mean evaluation so use the same
training and prediction and then we do
our parameter tuning and then finally we
can do predictions right we make that
scalable system hopefully that
does a predictions so that that's just
the conceptual overview let's get into
kind of the meat of it a little bit more
of how how each of those kind of steps
looks in particular how that how it
looks with tensorflow
calves introduced tensorflow for us
earlier so I'll be brief in this bit the
the thing I'll add is that the name
tensorflow
comes from the two words tensor and flow
tensor just refers to a
multi-dimensional array or matrix and
the flow is referring to flowing through
a computational graph so this diagram is
like a super simple version of a
computational graph right the two and
the three feed into an addition
operation and a five comes out the other
end the notable thing of this like it's
very simple right this is a very simple
example obviously but it shows this
example that you construct the graph
first and then you can feed data through
it so this graph has two input nodes in
addition operation in an output it knows
nothing about what the input values that
may or may not come later and then the
two and three show up into five comes
out the other end and so another way to
conceptualize it is that the outputs are
pulled through the graph they're pulled
out by the the desired output node so
that five is saying I need everything
related to you know I need to calculate
this five to to be computed and you know
it's not shown here but if there was
like some other operation off to the
side like I've almost like a dangling
node that wasn't instrumental in
computing that output it wouldn't get
computed and so sometimes with
constructing computational graphs you
can run into trouble there where you
have this thing off to the side that you
want to run but it won't run because
your output doesn't need it so they
won't flow through there and I think has
touched on most of this as well
you know tensor flow runs on just about
everything under the Sun and beam open
source it continues to get more and more
support on lots of different platforms
even beyond where I've shown here
there's like a bunch of you know further
like embedded systems chips that are
you know getting tents for support or
tensorflow light support in terms of
tensor flows architecture the whole
thing is built on a distributed C++
back-end and that engine is all nice and
fast and a lot of stuff but thankfully
we don't have to interact with it and
that's why we're not this isn't called
C++ data it's called PI data right so
we're gonna care more about that Python
front-end and the various layers of
libraries above it so the first one is
called layers so it literally is just
like you're manipulating the layers of
your model directly and then above that
is what's called we call the estimators
library where it's kind of pre-made
framework for constructing your model
and running the training and evaluation
loops and then sort of above that this
is like an optional layer where it's
basically not only is it a framework for
constructing and running your models and
doing the training evaluation the models
themselves are already predefined and
all you have to do is supply some
parameters many of which are optional so
that's the layer that we're going to
focus on today because it allows the
greatest amount of experimentation and
initial flexibility without having to
get bogged down in the details of
constructing your own custom models and
so my recommendation is typically to
start at the top and as your
requirements arise as you need more and
more customizability descend into the
depths of tensorflow
okay so what does that look like in code
so this is one example of a canned
estimator so this is a DNN or a deep no
network classifier cat has talked about
those layers of neurons and such so this
is basically you're creating a network
by configuration we're basically saying
that this network will have four layers
and the first one will be 1024 and all
the way down to 128 so literally you
just supply an array and some of you may
be wondering what about all these
the parameters that I care about my
activation function my decay my
optimizer my learning rate you can have
those two if you want but there are
reasonable default supplied if you don't
so like you can get away with this but
you can also do that so you can you know
you can go for a while before I you
reach the point where you say well I
should I need to rewrite my entire
network by hand from scratch because the
options that you know they go for a
while which is nice so let's look at a
concrete example of this usually I have
a example here that talks about food and
fried chicken but I figured you know in
the interest of have it being evening
time and some folks might not be in
dinner so I went with something a little
less savory so this is just as exciting
as food I promised it census data and so
this is the 1994 census it's become
somewhat of a classic data set it's not
like the hardest data set in the world
and it also is small enough to fit into
memory it's easy enough to run on your
local machine and the task is simply to
decide whether or not given
socio-economic data whether a given
household has income of above or below
$50,000 and I have a couple more before
I have to so I'll stand for now here are
some columns right so we they're the
Commons you might expect right like age
and education marital status what
occupation they did what whether you
know what kind of relationship and I
being whether they're married or not and
then hours per week that they worked and
then finally income bracket right above
or below fifty thousand and this is
literally what the data is it's the
string greater than sign five zero K or
less than equal five zero K so you know
we'll do some pre-processing there when
we get into it okay so let's let's look
at this
concretely here
so I've got tensorflow fired up on my
machine through a jupiter notebook oh
another opportunity for a poll who uses
jupiter notebooks lots of people yeah
chips the best okay let's see folks in
the back can you see the words on the
screen I got some nods great so
importing tensorflow and installing
tensorflow is all kind of the standard
stuff right pip install tensorflow and
it's import tensorflow is TF her sounds
okay
I was worried it was coming from me all
right so will import tensorflow
I happen to be running 1.3 in this
particular environment but the ever
since 1.0 all the versions all the minor
releases are backwards compatible so 1.4
is what is out today in terms of what's
the most most current but we'll pull in
panda so we'll take a look at this oh I
didn't run this cell
good job me yes that is your job remind
me to run the cells
I always forget to run the cells and
then people think I've wrote bad code
but I just didn't run the code that made
the variables exist in the first place
so loading the the data into a data
frame just so we can take a look at it
and we have kind of those columns that I
mentioned lots and lots of columns and
that last column with the wacky kind of
data representation rather than doing 0
and 1 they went with greater than or
less than 50k and one of the things I
like to do is use the dot described from
the pandas dataframe to kind of show
some stats about my data and you can
find out some interesting things about
your data set through this and it's
stuff that you would probably be able to
figure out eventually but it's nice to
have a standardized easy call to make
things you might notice here apparently
in 1994 nobody worked more than 99 hours
per week perhaps the form just didn't
have only had to two digits that they
could enter in right and also nobody in
the united states was over
that filled out the census also
interesting you know in hindsight this
is the data set that was prepared from
the census data right this isn't
literally the raw census data so I
suspect they put some limits on both
ends of the spectrum because you see the
minimum age is also 17 so you can also
do that described on the categorical
columns we have you know education we
see that the unique row here that that's
what usually it's most interesting to me
is you can see you know which columns
have a lot of different values and which
columns maybe don't have a lot of
different values and so in terms of data
gathering I guess we kind of glossed
over that with this but the data sets
provided but the data preparation step
is interesting here because I wanted to
take this moment to introduce to you
guys if you haven't heard of this before
facets so this is a project out of
Google's pear lab which is I should know
what it stands for I think it's people
and AI research and it's a really neat
tool to visualize your data and and I
feel like they haven't done enough to
kind of tell people about it and the so
this is our census data loaded into
facets facets has two components to it
one is called facets overview and one is
called facets deep dive so facets
overview does what it sounds like we can
see our features and you'll notice that
it is very similar to what we saw in
pandas right it's very similar to
describe we got we got our mean we got
our standard deviation we got our min we
got our max we got our median all good
stuff but we also see some other things
we have a missing percentage we have a
zeroes percentage with the ones that are
very high turned red and bolded so it's
a even better way I would argue to see
any anomalies in your data strange
patterns weird behaviors data imbalances
and over here we can see the
distribution the blue is our training
data the kind of pinkish red is our test
data set so the test dataset is about
16,000 the trainings about 32,000
and we can see kind of the distribution
across different categories and
different values here and we can also
check that the training data and the
test data match and that that's really
what I'm looking at here is the blue and
the pink should roughly follow each
other right because if your training
data is skewed in a different way from
your test data well then you're not
gonna be training in a way that that's
gonna be favorable and then we have our
categorical feature which which have
again similar representation we have our
our unique we have our top values we
have our frequencies and we can flip
between showing the raw data and showing
a chart of the raw data one interesting
observation you might make here is that
in our test data set for the country of
origin there are only 40 unique values
whereas in the training there were 41 so
then you know maybe this might prompt me
to go dig into my data to figure out
like why isn't one of my countries
represented in my test data set perhaps
it should be or perhaps there's only one
example of that 41st
country maybe I should throw that data
set out as an outlier and so that it
really brings some nice insights to your
data and just really helpful in that way
and then I promise there was the second
thing facets dive deep dive this is
where it gets really fun so faceting is
kind of like when you're shopping online
and you it choose like which kind of
maybe you're buying shoes a shoe size
color brand type and there's like these
lists of different aspects right these
different facets of the product so here
we can I've already done a few things
here I'm splitting this by age and
education and we can see all the
different groupings and we can zoom in
and out all the way to a raw like data
point and click on that and on the right
hand side see the entire data you know
all the data for that one data point and
so maybe we say we want age based
faceting maybe when the order that do
I'll turn this to age turn this one off
and this is all written with like
polymer web code
and this way no hours per week yes
and when backed with typescript in or
JavaScript I script so you can embed
this in the web as well as in your group
etre notebooks so we can like really
become part of your workflow so what I
did here is I have across the top here a
age buckets and you can you know tune
the number of buckets you want maybe you
only want six buckets or seven buckets
or I can't make it say seven seven and
maybe we want to see do people work more
hours per week across different ages
right your intuition might say yep
probably but how does that affect
whether their income is above or below
50k and you know do we see that trend
generally and so I'm doing a scatterplot
sorted by hours per week on this axis
here and so you can kind of see this
general trend of this it kind of goes up
and down as you go through the different
ages you know when you're younger you're
working few hours when you're kind of in
your quote/unquote prime working years
you're working in most hours and then it
starts tailing off toward the end and
you can also see in the distribution of
the blue and the red now you know the
red is tends to kind of scatter toward
the top end of the hours per week but
there are certainly exceptions right
it's not like a perfect split and so
that that definitely will factor into
making it difficult for any model to
make accurate predictions on this
because the data is kind of messy you
know you look at this this block for
instance look at where the red and the
blue are you know we have red dots down
here we have blue dots up there so there
are people who make both above and below
50k so the bread I think here we have
the tart the legend red is above 50k
blue is under and the you know you have
the red down here mixed in with the blue
and you know if the human can't separate
it maybe the Machine can but also maybe
not so when we run the training you know
we'll see how the system does against
this kind of a data set so that's facets
I find that to be a useful
kind of step I wanted to kind of present
that to you guys as a perhaps another
tool in your data preparation tool belt
so tensorflow reads in data via what are
call the input functions basically it's
a way for you to configure your data
that pushes into that tensor flow graph
into that model in any way you want
right because your model is over here
your data is over here you just need
somewhere to connect it to and the way
that they've chosen to architect it is
to basically let you provide an
arbitrary Python function and so you can
do whatever pre-processing you want in
that input function so that's kind of
the idea there if you're pulling from a
distributed data store of some form
that's a great place to make that call
if you need to do any pre-processing and
call out to other functions to do that
pre-processing that's another great
place to do that here
we're largely just gonna read in our
data and and kind of push in onwards
tensorflow offers a handy utility
function that just constructs a panda's
input function for us from pandas data
there's also an equivalent numpy input
function if you have numpy arrays or are
able to shove your data into numpy
arrays and Oh at this point I should
have mentioned earlier there is a column
that some of you guys might have noticed
that is unpronounceable it is it was
used by the statisticians who processed
the census data as a metric of how much
the folks believed in the data like the
actual people filling out the census so
not terribly useful for predicting
income so we kind of throw that out of
our analysis so we make our input
function and this leads us to our next
step of kind of creating our model and
the canned estimators have a nifty piece
in that you know the model itself is
canned but the way it receives the data
that comes that is coming in through
what are called feature columns is very
much under your control and here you can
do a number of transformations so for
anything you didn't do in the input
functions or perhaps was too complicated
to perform in input functions at scale
you can do it here within the tenant
flow graph where it will execute in C++
it will distribute it will do all those
good things and so the first thing we'll
do is we'll split our sparse columns or
categorical columns and there's a lot of
different controls here I'll just show
two of them one is called categorical
column with vocabulary lists apologies
for the really long method names I don't
recommend typing them out just
autocomplete them and categorical column
with hash bucket one is if you you know
know exactly what the possible values
are in your particular data set for that
column and then the other version is for
when you don't know or you don't care to
type them all out so the hash bucket
size should be at least the number of
unique values right so a lot of times
you this you can literally just you can
automate it by just saying that column
not unique if you wanted to do it that
way if you had like you know hundreds of
columns you can just script them out
that way by just having this inside a
for loop essentially run that and then
with the continuous columns it's a
little bit easier we just make an
American column and call it a day you'll
notice that I just give a name and
nothing else here right these feature
columns are just that they're they're
kind of placeholders for the data to
kind of flow through when when the time
comes when so the input function pushes
it into the future columns which goes
down into the model and before it goes
to the model though you can take these
feature columns and you can do some
transformations so the first example I
have here is taking a continuous column
like age and turning it into a
categorical one by bucket izing it so we
saw earlier in the in the facets example
right we could do these facets with with
you know different number of buckets and
so you can control exactly where those
boundaries are when you actually go to
do your training by just applying those
boundary values and so now age buckets
is a categorical column and then we can
do what are called cross columns where
you can take two categorical comes and
make a new third column or a new extra
column which kind of does the end of the
two and when I say two what I really
mean is two or more like you can
actually take multiple including taking
the previously continuous column age
which we turn into age buckets and now
we're going to pass in and cross with
the other two
so you can do quite a lot in terms of
different combinations at different
columns and it really comes down to
understanding your data and looking at
how your particular how you think right
this is the the art part of the data
scientist artists work where you say
well based on my knowledge of this
domain my knowledge of this particular
data set and my explorations into it I'm
I wonder perhaps if crossing these two
columns or bucket heisting this might
yield a better result because I could
can kind of instrument it this way and
you might say this is you know we're
kind of leaning in the direction of
manual feature engineering and stuff
with this right but my argument is that
you know if you can do this in one line
and it yields great performance results
or you can add a bunch of layers to your
neural network and train an X for 20
hours like you know it's kind of one of
those trade off time versus energy kind
of things right so it may be worth
trying out and this is what I get for
not running other columns other cells
okay and then finally I'm gonna group my
columns into kind of in this case all
the categorical ones and the crossed
ones I'm just referring to these as like
wide columns categorical linear columns
and then the deep columns which I have
the numerical ones and then all the
categorical ones get in embeddings so
that they can be have kind of deep
continuous representations and when I
mentioned earlier about selecting a
model here's an opportunity to see how
see that in action right so I have three
examples here one is a linear classifier
so that's your that's kind of what I
showed earlier on the slides your
standard linear regression
applied to a classifier so we just pass
in our wide columns and then earlier we
saw in the slides that DNN classifier
your deep neural network classifier and
we can supply how many how big we want
it to be this case I made a small one
it's just 100 and then 50 just two
layers and then those deep columns and
then there's actually a third option
that's much longer and again don't type
this out but it's the DNN lynnie
combined classifier and there's
regressor versions of all of these where
it combines the classify of the wide and
deep columns to make kind of one
supermodel so you can put the wide
columns and deep columns in supply the
hidden units again modelled there for
all of these is just the string path for
where to store the artifacts of tents or
floats work and then so when you run
that it kind of creates the model object
in Python and we're gonna say that to
just m and when we're ready to train we
will call m dot train and when we call
that what we're supplying here is we're
supplying the input function right which
is we're calling and passing into the
model and when we created the model we
supplied those feature columns so the
core of the model with the feature
columns attached on top it's like your
custom model and then the input function
gets passed in our training time which
means that when we want to do evaluation
we can pass in the evaluation input
functions so it's it can be different
while still kind of having that same
call structure I'll show you guys a peek
here the evaluators just MDOT evaluate
it's the exact same same so we'll do the
training and I'll let this run and here
while this is running I'll show you guys
another tool for those of you guys who
use tensor flow how many you guys use
tensor board
crickets Wow okay so this is tensor
board tensor board is tensor flows built
in kind of visualizer and you can
visualize a lot of the stuff that's
happening in your model and if you're
using these can decimetres you get a lot
of metrics for free right you didn't you
didn't see anywhere in my code where I
was instrumenting any of this but you
get it for free and let me refresh this
and see if we got our new ones coming in
here so I have some old ones here and
you can drag and select to zoom in let's
see well there's a number of different
things that are in here but what I find
really interesting is the graphs version
here like we talked about how tensorflow
is a graph I showed the two plus three
equals five I said that was a really
simple version
and for some reason why are these not
connected it's really odd okay so
they're usually connected but they keep
updating this so maybe this is just like
the new look here's our linear model and
so this is kind of a visualization of
what's going on under the hood so I'm
double-clicking to expand here and we
can see here our columns that we used we
can see the ones that we created that
we're a crossed
there's our age bucket eyes and over
zooming out a ton here let me make this
smaller over here
I think note where is the deep neural
network did anyone see it I lost my
neural network
okay so over here was our deep neural
network we should see our our couple of
layers in there as well as the input
which I'm having trouble expanding that
alright so there's all the different
columns they decided to put the deep one
really widely on the screen
unfortunately but it's a little
backwards so there's all the different
columns that we had for the deep one you
can also see the district set of
distributions about like how the weights
are being distributed across the
different in each layer and you can kind
of use that as a sanity check to see if
there's any values that are like wacky
really big really small so here's here's
the actual distribution of the linear
one for instance and you can see that
it's it's this the most recent one like
I I built one at some point that was
purposely like weird and messed up okay
so these are actually different models
but sometimes you'll see them like not
change they'll be really tight or like
it'll like all be going down or all be
going up so that's like ways to see that
things might not be behaving properly or
behaving well let me uncheck most of
these just so we can see more so if you
hit the circle it'll just select just
that one and I'll just use the check box
and get both and so you can see the
linear activation here you know as the
training progressed so just the by way
of explanation
this view is like this first line is
that the first training step and then as
it goes down is like this is 400
training steps 700 training steps in and
so on and so forth so it starts out it's
a peaked no like you know specific
distribution that it initializes to but
then as the training happens it gets
distributed out and so sometimes if your
training is not progressing for example
then you would see this the tall peak
just continue the whole way and not ever
spread out and that's like one
indication of like Oh something might
not be working so great so yeah there's
real anyone and then the deep ones are
harder to see just because the values
are like the peak value is so big
but if I make this bigger you can kind
of see there's a couple of them that pop
up yeah and then there's like a whole
bunch of other values as well that we're
not utilizing here but like if you're
doing audio processing or like images
you can display those as well here and
it'll show up
so that's tensor board another kind of
tool for your tool belt that will help
you with seeing how your model is
performing and speaking of power model
is performing we got to 83% which you
know for being a relatively
straightforward dataset you know it's
like it seems like it's not that great
of a result partly because you know I
just kind of pick some random parameters
I cross some random columns and you know
I I suspect and hope that you guys can
do better than me so I'll leave that as
was it an exercise for the reader and
then in terms of making a prediction
it's it's gonna start looking really
familiar here we're gonna use that same
M and call tap predict so here I'm just
constructing a simple five rows of data
for it to do a prediction on I'm just
pulling this out of the test data set
and so yeah what this should be less
than less than and greater than less
than lesson so zero zero one zero zero
is what we're hoping to see and we'll
make our prediction input function and
call model dot predict passing in our
prediction input function so we had our
data that we constructed and so this is
basically just a little mini function
generator that supplies that in one shot
and then we have our predictions which
I'll run live and with any luck assuming
I ran the cells before it will get our 0
0 1 0 0 and you can see the
probabilities coming back and you know I
formatted this output a little bit but
there it does have a little more rich
data than only these a couple of points
so that's kind of the journey from the
where is it okay
yeah from
training together from gathering data to
preparing that data to training
evaluation and finally prediction right
so we saw a tensor board that's how you
kick it off by the way and the what it
does is it spins up a local Python
server and then you go to localhost
colon six zero zero six and the port six
zero zero six is anyone want to venture
a guess as to why the default port
number is six zero zero six anybody yes
yeah yeah
it spells out gook exactly I used to say
it smells that Google you turn it upside
down but then someone pointed out that's
like well it could be a capital G like
oh yeah that's fair and a personal plug
I am working on a set of videos called a
cloudy adventures on the Google cloud
YouTube channel we're what are we now
like 12 ish episodes in and basically
you know I step through some of the
concepts I talked about here but like in
more detail and each episode and we'll
talk about a specific piece and we'll
kind of play around with stuff and
recently like I've also been trying to
get time with some of the folks some of
my teammates some of other folks my
colleagues in Google brain and such to
sit down for like interviews and such so
we did one on natural language
generation and that was a lot of fun and
yeah I won't tell you who I have what
I'm trying to get later I'll tell you
when it comes out so yeah feel free to
you know reach out to me on the
Twitter's some resources and the code
that we looked at was wide and deep -
census I also have an example called
well it's called widen decode not the
best name in the world but basically
it's the same code before a different
data set so it's largely unchanged at
the core but the input function is
different the data processing is
different because that data set is 41
columns it's it has three different
sizes depending on how big you want to
work on it so I have like a subsample
that's like small and then the big
version is up to a terabyte and it also
has an
hash values as data in some of the
columns rather than like literal human
readable strings so that also has
interesting implications in terms of
doing machine learning on anonymized
data sets in terms of privacy and such
so if you're interested you know you can
use that as a way to you know compare
the two and say well now how do I apply
my own data set right let's see how what
changes I made to adapt the code to this
other data set and then you can kind of
use that as guidance to apply it to your
own data set so yeah that's all I have
thanks so much for sticking around I
know it's a it's a you know later
evening hopefully Felipe's pizza or
during work that may have the pieces not
available so deny so yes I was just
gonna say so I'm gonna have to tap-dance
up here for 20 minutes okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>