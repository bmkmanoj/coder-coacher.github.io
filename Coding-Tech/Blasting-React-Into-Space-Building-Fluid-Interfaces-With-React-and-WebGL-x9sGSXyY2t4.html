<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Blasting React Into Space: Building Fluid Interfaces With React and WebGL | Coder Coacher - Coaching Coders</title><meta content="Blasting React Into Space: Building Fluid Interfaces With React and WebGL - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Blasting React Into Space: Building Fluid Interfaces With React and WebGL</b></h2><h5 class="post__date">2017-10-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/x9sGSXyY2t4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I am ashy Krishnan my handle
basically everywhere is career violet
and in a past life long long ago I used
to do 3d graphics for the US government
so this is the most photogenic project
that I worked on we also had some
forecaster tools which sound very
impressive to describe them and then you
look at the interface and it's like
Photoshop and Maya can have got together
and someone put it into into a blender
and it just looks terrible so this is a
museum exhibit called science on a
sphere it's a big carbon fiber sphere
that hangs in the center for room and
has four projectors on it kind of my
first experience with projection mapped
art done for the US government
surprisingly this was my introduction at
least to some of the foundational
problems in graphics namely we were
trying to project a 4096 by 2048 video
onto the sphere at a time when that
resolution was patently insane like
nobody was doing it half of the tools we
were using couldn't handle it if you
want to if at the time to get ffmpeg to
even decode that you had to like
recompile it with certain flags it was
it was a nightmare and it was the kind
of nightmare that I've sort of learned
to love and associate with computer
graphics there are three I think main
fundamental problems that you have to
solve with graphics the first is buffer
management so you have to deal with
taking all of the data usually quite a
lot of it describing a 3d scene or in
this case describing the pixels on a
globe and putting it where it needs to
be so that's that's a problem that's
common between computer graphics and
supercomputers actually I later worked
at Google and it was surprising to me
how many of the data locality problems
we experienced working with sort of very
very small high speed systems like GPUs
we're also applicable to very very large
distributed systems
like Google BigTable the problems are
essentially data locality and they're
basically the same it's just a matter of
time scale like does it take minutes to
move data from one region to another
region or does it take microsecond
milliseconds to move it from one part of
the computer to another part of the
computer please tell me that's not going
to keep happening okay so so that's the
first problem is moving your data around
the second problem is telling the
specialized hardware what to do with
your data and to do that in OpenGL and
or WebGL and we write shaders we'll take
a look at what shader code looks like
later but the thing to remember is that
whereas your CPU is one or two maybe
very fast
cores your GPU is like 48 or 128 or some
large number of really really crappy
ones and so you can write little
programs that run on these little crappy
computers that do incredible things
because there are so many of them and
the last problem that you encounter with
graphics is compiling the programs the
tooling around OpenGL was terrible and
WebGL has improved it substantially but
as we will see later it continues to be
terrible it is just very very hard in
ways that it seems like it should not be
hard to get the right packages and get
them to talk with each other and like
encapsulate your code properly and not
end up somehow linking against a very
specific vendor driver that is not a
problem you have with WebGL but that is
sort of the general scope of problems
you have working in graphics so these
days I know I can use this thing right
yay so these days I don't work for the
government anymore I don't work for
Google anymore thank God
instead I teach and I teach primarily
women to code at the Grace Hopper
program in full-stack Academy as Alex
mentioned earlier
and we teach jf node and react and I
really like this combination we started
teaching react last fall and for about a
week
I like spent I spent about a week
learning it along with the students and
sort of hacking with it and my first
response was like oh god why do we need
more why do we need JSX why do we need
this why is what's going on and then by
the end of the week I was I was like
loving everything I was like oh it's
it's like a new kind of literal and you
can return you can just use all the
functional programming tricks you like
and it's not angular which apologize for
anyone who loves angular I was happy for
the switch and our students seem to take
two and they invariably love to push the
boundaries of what we're doing and I
like to think I'm I'm a little bit
helpful in this regard too because
students will come to me and be like we
want to like make a react app but we
want to use WebGL for everything and we
also want to use Web Audio to do audio
synthesis and it's going to be like a
living visualization and I'm like yeah
you should totally do that and so they
did that and here it is with some
obvious visual artifact this is they
called it PG be VSU this is a student
final project and it's a it's a
sequencer that you can go and throw
samples around on I'm gonna hit play on
this and I'm what comes out it might not
be so great but we'll just or it might
be nothing because sometimes these
projects don't keep working for very
long after they're up yeah nothing's
gonna happen that's alright though
so they built this using react and using
WebGL which is still kind of a like the
wild wild west of frameworks right now
like there isn't a really great way of
gluing react to an alternate render
target like WebGL or that is that there
is no currently accepted like here's the
library you should definitely use there
are a few techniques so one one
technique is to just say
okay um we want to output a scene like a
3d scene and so we should probably just
and react is very good at outputting Dom
elements so what if we just made some
Dom elements that described a 3d scene
and if you do that then you'll end up
with a frame which is this like pretty
little library from from the Mozilla
Foundation that lets you do you describe
3d elements and describe a 3d scene
using Dom elements and it also gives you
let me see if I can pull this off ctrl
shift nope not that it gives you an
inspector that lets you sort of move the
camera around and gives you a lot of
nice tooling the thing it doesn't give
you if you're coming from react land is
the ability to pass in props that you
might reasonably expect for example in
order to describe the position of like
this part of the building we're gonna
have to pass in a few data points or in
order to describe like the movement of
this cube cloud I think that's supposed
to be a cloud we're gonna have to be
updating a position prop on that object
and the way that you pass in a
three-dimensional position prop in a
frame is number comma number comma
number you literally take your vector
and turn it into a string by joining it
with commas and after you've done that
about 20 times inside your react code I
start to cry I just like literally how
to break down and was like we're never
suggesting a frame again there's there's
other ways of handling this of course
right you could wrap all of the a frame
tags in react components and have those
components passed down props in sort of
a more reasonable way or if you give
them say a function that corresponds to
an event listener they could attach an
event listener after the component is
realized after the final Dom that the
Dom element is realized but all of these
are kind of another layer of abstraction
on top of this layer of abstraction that
a frame already gives you and so what
the students
who made pgb VSU the synthesizer did is
they found react this react 3 library
and kind of hacked it all to hell but
that library was someone's science fair
project essentially and they ended up
like making a lot of changes and in the
process becoming very familiar with some
of the intricacies of obscure features
of react like context which maybe isn't
too obscure but it is one of those
features that when you go and read the
documentation it's the first thing it
tells you is you probably shouldn't be
here like go away you don't want to use
context but today we're gonna use
context to do some stuff so here's
before like we dive into a simpler demo
here's some of the like neat things you
can do with WebGL which may be what GL
doesn't need someone being like there's
a bunch of cool stuff you can do with it
so this is a part of Steve Witten's
presentation called the pixel Factory
which I really highly recommend all of
the math visualizations on his blog here
he is demonstrating that we have this we
have this shape it's a like a sinusoidal
shape and we are running the entire
scene through a vertex shader so in the
graphics pipeline we have the geometry
which is like the raw data here are all
the vertexes in three-dimensional space
and then that gets run through a vertex
shader which is a little program that
runs for every single point you've
specified which can be millions and
millions of points and turns it into a
position on the screen so in that step
you can do whatever you want and when
you run your geometry through a pixel
shader you are essentially warping space
and you can warp space very selectively
to affect only certain objects in this
case he's warping all the space in sort
of a random way in order to show that
you can and as you warp space you can
also attach additional data that then
gets processed by the last step which is
the pixel shader or the fragment shader
more properly which goes and
takes all of the pixels that are covered
even slightly by a given piece of
geometry and decides what color that
geometry should contribute to the final
rendered image so I want you to keep
sort of in your mind that when we are
doing when we were working with graphics
what we're really doing is writing a
massive distributed program there's a
lot of libraries we're gonna be working
with three j/s today there's a lot of
libraries that sort of conceal that a
little bit and instead say like oh
you're you're working on a set with like
a camera and lights and this and that
but the truth of it is sort of much
grittier that you are looking at math
and you're writing a program to describe
how math becomes pixels and that's it
okay so that of course is firebase we'll
look at that in a second just one last
thing I wanted to show you so this this
placid lake is evan Wallace's little
swimming pool and the swimming pool has
a couple of neat features it has a ball
that you can drag around in it and it
has a surface that you can draw upon and
anyone who's familiar with graphics is
probably looking at these lighting
effects and being like holy crap that's
that's pretty hard and these are pretty
hard the way the way this visualization
is working is the light front the wave
front is actually being rendered as a
geometry so the this cool pattern of
lights is itself a grid and as its
passing through the water which is
another geometry based on the distortion
of this of the water plane the light
plane gets distorted and then with that
and a few few other tricks if you
screen-space partial derivatives we end
up with something that looks very much
as swimming like a swimming pool and we
can also drop a ball down into our
swimming pool yay so these are the kinds
of effects that you didn't achieve with
WebGL they're all rather hard to do by
hand and so there are libraries like 3j
s and
there are sort of ecosystems like statue
L that aim to make some of the harder
parts of it like setting up a camera and
dealing with the pain of writing a bunch
of code running it and seeing just a
black screen they sort of aim to
minimize the amount of time you're
you're spending like not knowing what is
going on
so let's look let's look at a little
demo here so this is this is something
we're gonna be looking at the code for
for a second it's it's a pretty simple
treaty demo of a camera orbiting around
the origin which is nothing and there's
all these platforms sort of scattered
everywhere that give us the illusion of
maybe a game space that something can
run around in and this is all written
and react each one of these is an object
that is driven by a data source in this
case like a random assortment of points
and so let's go take a look at the code
for this alright so let's start here is
that big enough yeah so this is this is
the code describing the scene we have
we're importing some some things from
our free Jas to react binding that we're
going to walk through in a second and
then we are requiring three the three
jeaious library itself not importing it
because remember how I said that the
tooling sucks the tooling has a lot of
problems and you can't import this
because of course modules and babble and
it doesn't have a dot underscore default
and this works so we're going with it
this works that we're going with it is
also one of the mottos of computer
graphics there are in so three.js gives
us an abstraction on top of WebGL and in
this abstraction everything that's on
screen like every object is basically
the confluence of a geometry which is an
array of day
that describes in some sense the
vertices that are going to appear on
screen a material which is a shader
program that says for each and every
single pixel what should that pixel look
like so materials are where you get
lighting and shading and where things
start to look shiny and then a
transformation so a position is
typically how it's specified but you can
think of it more generally as the model
matrix as a general transform that
describes how the vertex shader should
transform every point in this geometry
and so all of our platforms have the
same geometry they're cylinders you may
be like these don't look like cylinders
but actually they are kind of cylinders
right there just skewed and the various
parameters to cylinder geometry let us
specify like how many different sides
they have and so on they use a mesh
Phong material which is some a material
a shader program that uses Phong
lighting and applies it to a mesh of
some kind the mesh is expected to have
surface normals which are vectors that
point away from the surface and are what
tell the program hade this should be
dark and this should be brighter and
then we go and create 50 random random
platforms all over the place using the
fill and map trick okay now here's the
exciting part here we have our three Jas
scene described as JSX which on the one
hand is kind of a dumb Petric like why
not just describe it as three Jas on the
other hand it lets us use the same data
binding that react gives us that we all
love so much in this case we're mapping
over some random positions and maybe
it's not that exciting but you can
certainly imagine how I could map over
like a firebase data source or it's
something from the Redux store really
anything like it gives us all of the
power of react but within
within the scope of a 3jf scene so
there's kind of a couple of layers of
hierarchy we have to cut through the
first is we need the canvas itself we
need the canvas and a renderer inside of
it and that's what the three tag gives
us it says okay put a canvas on the
scene and initialize a WebGL context
inside of it and the WebGL context we're
going to fill with this stuff so this
stuff is first of all a scene we could
possibly eliminate the scene tag but 3j
s has the notion of the scene and I
think it would be nice to preserve it
because if we are building a
general-purpose binding as we are kind
of edging towards doing then we might
want the scene to we want we might want
to have multiple scenes each with its
own camera and be able to switch between
them without destroying our context so
here we have our scene the scene has fog
in in WebGL because OpenGL was set up
this way fog is kind of a global
parameter that you can you can set
certain flags to describe basically how
how attenuated a given pixel will be
based on its distance from the frame
there's a lot of kind of weird things
that you might expect to be handled in
some sort of general-purpose shader
program that are not handled that way
yet in WebGL although I believe in WebGL
too they are we need something to look
at or we need something through which we
can look so we have a camera in the
scene and the camera is sort of by
convention is going to be the thing that
the renderer which is this tag goes and
picks up on so something something you
should be noticing is that we need for
this renderer to attach to this scene
and this camera so there's some kind of
there's a way that data needs to flow
here as we're implementing these tags
that is not the traditional react passed
down props and live with it model
although we could do it's for passing
props we are going to do it through
different trickery however so we've got
a camera we've got a couple of lights
two of them are directional so their
cones one of them is ambient so it just
sort of describes a light lighting value
that's going to be given to all of the
materials within the scene and then we
have all of our platforms these meshes
that come out of that math okay so this
this is nice and we can sort of imagine
describing a more complicated scene but
let's look at how we implement all of
these tags on the react end so by far
the most irritatingly complex I'm sorry
by far the most irritatingly complex tag
is the renderer and this was imported as
three over here and it gives us okay so
in this render method we throw a canvas
on the screen the canvas once it is
realized is going to call canvas did
mount perhaps perhaps a terrible name
perhaps a brilliant name not sure in any
case this canvas did mount function
receives the canvas and then sets up a
3g 3gs renderer on it so it goes and
instantiates a WebGL renderer it sets up
the clear color this should almost
certainly be a prop that we passed to
the renderer some things are not some
things are not currently as
general-purpose as they could be and
then we set we set the state of the
renderer which starts with a width and
height of zero to be the width and
height of the canvas element and the
reason we do this through react elements
state is because we actually want to
have our state get passed down as a prop
and cause re-render on our children in
this particular case because the width
and height that the renderer have like
they influence things like our camera
which needs to know the width and height
of the whole canvas in order to
correctly perform the projection
transform and so because the data is
flowing that way we're going to use
props for this and indeed whenever the
window resizes we set our state again
we have to handle retin-a stuff because
nothing is easy on the web and we tell
the three jeaious render also a better
on you sighs okay now down to the dark
magic
so our renderer passes down to all of
its dynamic children to context values
set scene and set camera these are both
functions that let any descendant say hi
I'm a scene hi I'm a camera and tell the
renderer about it so that when the
renderer renders a frame it uses that
scene and that camera we are not very
smart about this
in this demo right now anyone can any
descendant can call such scene and that
becomes the scene any descendant can
call set camera and that becomes the
camera so if we look at our scene class
here when we mount we call set scene on
the we call the set scene that was given
to us through context the reason we are
receiving it at all of course is because
scene has specified that it wants the
renderers child context as its context
so it's able to attach in that way if
you don't set this then you don't get
context it's one of the ways that react
prevents sort of spurious use of this
backdoor data passing functionality
within the tree so as soon as any scene
mounts it's going to set set itself as
the default scene likewise for any
camera so if we had multiple scenes and
multiple cameras this would not work so
well similarly if we have multiple items
that we want to get attached to the
scene in a particular order that is
actually something that's going to
happen here so the scene has this the
three j/s scene down to its children and
then we're gonna skip over the camera
because that's crazy
and then here each
each entity where the mesh is an entity
that gets loaded into the scene just
adds itself to the scene as soon as it's
constructed so actually the stacking
order of items in the scene is not going
to be the same as the stacking order of
tags in the JSX
which is probably not ideal it's not as
big a problem as it would be in other
context context because we have a depth
buffer and so the order in which we
render things matters for certain
classes of rendering problems in
particular we generally want to render
from back to front if we're rendering
translucent things but in the case of
this demo it's not a big deal like we
can just add things willy-nilly as they
get thrown onto the scene okay and this
this also you may have noticed is a
higher-order component because we want
to be able to take any given 3j s object
and create a react component out of it
so we sort of abstract this idea of
constructing a 3j s object and attaching
it to the scene into this component for
higher-order component and then we call
it on all the various types of
components that we want it on okay so
all that gets wrapped together and we
end up with a demo that we can like a
camera that we can drag around and a
bunch of platforms on screen
nice right okay but that's only half of
that sequencer right the other half was
the actual sequencer part and so here is
another here's another little demo
that's not 3d but that might play sound
oh my god okay that's you know WebGL I
think works better than Web Audio which
on my computer at least sometimes
crashes Chrome sometimes sounds like a
dying hyena like it's it's very strange
so this is perhaps a better demo of
using a bizarre render target but using
also use
reacts cool data-binding capabilities
because this demo which let's look at it
where are you I apologize I had these
layed out but then Mission Control is
bad at its job okay because this demo
actually gets all of its data from
firebase so this component receives a
firebase reference which we pull in from
here and the firebase reference gives us
let's see new array of measures yeah it
gives us this value on state which
contains basically a true if a given
note at a given time location should be
filled and false if it shouldn't and
then we build that all out into a table
which this this is a very funny
component indeed because it actually
blends the visual presentation all and
the audio presentational parts of this
little demo so it is a table inside if a
div this little input box tells us if
we're playing and then we also have this
tone transport voice and sample elements
which if we go look at them use the same
child context passing technique to give
us access to tone j/s primitives tones
yes being a library that sits on top of
web audio in order to sort of
declaratively describe a musical score
so here we're saying our our score is
going to move at 200 bpm
it starts off not playing but then
that'll change when this input box gets
checked and we have only a single voice
and we have a sample everywhere
everywhere the sample is true and I
don't see where I'm in making that check
but it works so I'm sort of assured that
I am somewhere and again if we go look
at these components we see a very
similar pattern where we're passing down
data through context
and primarily we're passing down
actually primarily callbacks through
context so that for example the voice is
able to schedule its notes on the
transport by saying for example TX dot
schedule and then the a note is able to
schedule itself on on the voice that
contains it by calling voice dot
schedule all right and so one of the
things I tell my students is that react
is a confluence of three things we have
the syntax which is JSX and we have the
semantics which are the lifecycle
methods the sort of idea that you're
gonna get will mounted mount render and
and all of the state management calls
and then finally the implementation
which is the reconciler so something
that I hoped I would have time to talk
about here but did not in the final
analysis was writing in new reconciler
which seems kind of like a good idea
like when I first when I first thought
about making like a 3d binding for react
I was like oh clearly the thing to do is
to not touch the Dom at all and instead
just write a reconciler that goes and
takes it basically does what react to
Dom render does but it's like react 3d
render and I think that might be a good
idea if you work at Facebook for example
or it might be a good idea if you really
want to get into react internals it
turns out to be kind of a pain though
because when you write your own
reconciler you're basically committing
to either digging into reacts internals
and using their lifecycle management or
to writing your own lifecycle management
for react components and that's just a
very easy thing to get in wrong
so changing changing the reconciler is
not something i recommend but changing
the syntax is something that is actually
relatively easy and you can do quite
fruitfully so i want to the last thing i
want to show you is
this presentation itself which is right
here and this is JSX
no it's not this is react and it's a
variant syntax for JSX the purpose of
which is to make it easy to escape code
snippets so you can sort of tell by
looking at it that this is indentation
based and it has these little tags on
the top of an indented block that say
what JSX tagged that block should be and
by doing things this way rather than by
using jos X's interpolation we make it
very easy to like add a code segment
somewhere in here that includes JSX
tags which just in markdown land I have
found that the markdown parser tends to
drip all over itself once you have
perhaps not this level of complexity but
once you have this inside of an HTML tag
inside of a JSX tag everything goes to
hell so in order to solve that problem
it turned out that the absolute easiest
way of doing it was to get into the
syntax layer of react and come up with a
different JSX syntax which in this case
is called many minor matters a mechanism
for maintainable managing many markdowns
that is I'm happy to take questions and
I am happy to I'm very grateful to be
here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>