<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Neural Networks For Recommender Systems | Coder Coacher - Coaching Coders</title><meta content="Neural Networks For Recommender Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Neural Networks For Recommender Systems</b></h2><h5 class="post__date">2017-11-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qeeRVCqgk80" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hi everyone so today I'm not going to
talk about scikit-learn but more about a
recent trends in applied machine
learning which is the move to neural
networks for building recommender
systems so first let's talk let's talk a
bit about recommender systems so the
canonical example of recommender systems
is product recommendations for instance
on Amazon website if you pick up a book
if you go on the page of a specific book
so this is the scikit-learn book by
unrest media on machine learning
automatically Amazon will generate two
groups of recommendations one for stuff
to put in addition to your basket and
another group for additional books you
might be interested in if you want to
change your mind so here the goal is
clear is just to increase the sales to
do up sales and so on it's also used for
recommending media for instance on a
streaming music streaming platforms such
as passive I you have an input in green
which is my profile and as an output the
recommender system will generate a
playlist that is tailored to my
listening a bit but it's making it
possible to discover new music because
none of those bands there I don't know
any of them but there is a good
likelihood that it's actually a good
playlist when I go to work so here the
goal is to reduce UI friction SAP lift
simplifying navigation and to increase
user retention on the platform but also
to hide holes in the catalog because
suppose if you had only the search
engine as a uux as the user experience
to access the content it might happen
from time to time that when I type the
name of band it's not in the catalog and
then I'm disappointed if you push stuff
that are more likely to suit my taste
then you can effectively hide this by
reducing the occurrences of such
disappointment so it's actually good for
increasing the revenue of the platform
as well and other applications of
recommender system
not as trivial is actually search
engines nowadays search engines when you
do a curry for instance on Google you
have two inputs you have the the keyword
that you put in the search field and you
also have your your personal account
because you are logged in most of the
time and then you get the output the the
web pages that both match your profile
and the keywords and in that case the
keyboard is Python and what I get is the
first result is not the animal it's not
the snake
it's the programming language and
actually if you scroll down there is no
snake at all on that page so it's really
really personalized and the second the
second recommender system that you get
on that page is actually the ads it's
also personalized based on your clicking
a bit so this is really important from a
business point of view because this is a
the main source of revenue of two of the
largest global companies in the world so
you can see that recommender systems
nowaday I have a really really big
impact and I didn't speak about
personalizing social networks status
messages like in Facebook or Twitter and
so on
so let's talk a bit about the concepts
behind recommender systems now that we
know what they are good for as we've
seen previously there are two kinds of
recommender systems based on the input
data that we feed to the system first
they are content based systems that take
a description of the user like it could
be the age the gender the geographical
location and description of the item
like metadata for instance if it's a
movie the words that appear on the title
the the picture of the cover of the
movie or the the name of the director
the publication date the language of the
movie and so on and there is another way
to another kind of input that you can
use is the description of the items but
the interactions between users and I
in that case we just represent the users
and items as integers identify us in the
catalog and the user base and we just
count the number of times one user is
interactive with one I'd item and the
kind of interactions can be of several
kinds
it could be stars number of players
number of likes number of comments
number of clicks in a search result and
so on so in that case we aggregate a lot
of data based on the user activity
basically finally there are also hybrid
systems that kind of mix and match the
two kinds of input into the system and
this is interesting because typically
when you have a new item a new movie for
instance you don't have any clicks on it
yet so you are not able to recommend it
so in that case we have the cold start
program so it's good to use the
description of the name of the director
to make a good prediction to recommend
it to the right users but once a movie
or user gets a lot of activity you get a
lot of data and it's good to leverage it
and then collaborative filtering tends
to yield higher performance there is
also two kinds of targets once is
explicit feedback where you have a
negative or positive feedback and
something in between for instance when
you give ratings using stars and the
other one is implicit feedback in that
case you don't have the user to give an
explicit feedback because it's too
costly it's a pain of in the user
experience but you just record whatever
is happening and you consider that all
the clicks and all the interactions
between a specific user and an item as
are positive you can filter out some
negatives like for instance when a user
stacks start to listen to a song and
stops ten-second after the stop then you
can remove that as a positive but you
keep all the other events as positive
feedback and in that case you can use
evaluate the quality of your prediction
by ranking items and making sure that
positive items are ranked above the
negative items that are sampled are
random and in the database all the other
items so the traditional way to do a
collaborative filtering need to use a
mathematical model which is called
matrix factorization you have a big
sparse matrix of all the data which
call our in this diagram the rows of
this matrix are the users description of
the users the columns are the
description of the items and the entries
are the ratings or the number of place
of a specific user for a specific item
and so one user will will see only like
list less than 10% of your catalog or by
a 10% of your product otherwise you're
very lucky so this matrix has a lot of
missing entries and so to the goal is to
predict the values for the missing
entries and we can do that by doing the
product of two smaller matrices that are
actually the model parameters U and V
and there are D dimensional space to
represent the users and the items by
doing the dot product of two at one row
and one column you get an estimate of
the ratings and you can obtain the good
values for those parameters by
minimizing a loss function using
gradient descent likely squares using a
stochastic gradient descent and and so
on so this is the traditional way to do
it if we want to use neural network we
have a problem because the input is
fundamentally fundamentally integer
identifiers for user and items and so we
will need the concept of embeddings so
here is the the the core of the database
is user number something interacted with
I know something if you feed those
integers directly to a neural network
you would get nothing it's just random
noise so you need to deal with those
symbolic variables to treat those
integers as symbolic variables and to
find a good representation for them so
for recommender system this could be the
item IDs and user IDs but we have
exactly the same problem for neural net
when we try to deal with text tokens for
natural language processing for instance
like characters words and by Rams but it
also happens when you have any kind of
categorical descriptors of for instance
tags for your products so it could be
additional metadata on your product in
in the catalog or movies on director
names the history of visited URL in the
past if you if you track the user
activity to do a personalized
advertisement for instance or skills on
a resume if your
like a licking of your day off for
instance or a product that you raise and
so on so for all those cases we have
this symbolic variable problem and we
will use the same notation we will use
the the symbol s in a vocabulary V so V
is the set of all possible values like
the set of all possible item identifiers
in your catalog or the set of all
possible words in the English language
so it's a very large collection of
discrete symbols typically the
cardinality is and the ten thousands to
several millions or sometimes billions
so the natural way to find a vector
representation a numerical vector
representation for those symbols is to
use the one hat representation one one
Hut encoding so if you want to include
the symbol salad for instance what you
can do is take all the words in the
English language in a dictionary
and assign them a specific dimension so
you have 1 million possible dimensions
for instance and you would put zeros for
all the entries except for the entries
for the dimension that matches the
symbol salad and you set it to 1 so in
that case you get a very sparse vector
that is discrete because the value are
not contiguous it continues there are 0
1 in a very large dimensional space
which has the size of the catalog or the
vocabulary each axis as a meaning a
priori which is good because it's easy
to interpret but it's bad because it
means that the distance between two
vector taken at random is always the
same whatever the meaning of the symbols
that are that have been represented that
way it means that for instance the
distance between the vector that
represents salad and the vector that
represent carrot is the same as the
distance between salad and cake or roof
or pig or whatever so which is a problem
because it doesn't really capture the
the meaning of the of this of the
concept so instead we might want to
build vectors that encode the symbols
into a continuous space that is a lower
dimensional space with continuous values
not 0 and 1 but positive negative values
and very rarely is 0 so we want it to be
kind
nice and dance the the dimensions in
that space of no meaning a priori and
but we would like to have those vector
the distances between this vector to
capture some some meaning relationship
between the concepts so this you can
quantify this by computing the Euclidean
distance or the cosine similarity
between the vectors or even just the dot
product so you can consider another way
to see the embedding is to see it as a
linear layer in a neural network
typically as an input that will
transform the Wanat representation of
the symbol into the continuous space the
lower the lower dimensional continuous
space so and you do this transformation
by just multiplying by a matrix which is
the basically the embedding parameters
so W here the matrix that does that
defines the embedding is actually
initialized at random at the beginning
and the entries of that matrix are part
of the trainable model parameters so if
you want to do this in chaos
maybe it's bit too small but it's very
easy it's just another layer that you
put in your model embedding takes
integers identify us for the symbols as
an input and the output a continuous
vector that are fed to the next layer so
how to train those parameters we have to
plug that layer in in the model and
define the last function that depends on
on the target that we want to predict
and basically we will get a trainable
architecture and use gradient descent as
usual to to adjust the parameters of the
embedding automatically and to end so
let's let's talk a bit of about
architectures now so if we take the
original problem of predicting the
number of ratings the number of stars
for that user I will give to item J in
the catalog what we can do is define two
embeddings to embedding matrices also
embedding layers that are next one
another one will take the integer ID as
the input and will output the embedding
vector of
for that user I and the other one will
do the same for for the item and the
model we make a prediction by just doing
a dot product of those two vectors and
we'll try to predict the the rating of
the number of stars for instance so this
works only for explicit feedback and to
optimize the model to find you in the
model we just need to minimize the the
difference between that dot product and
the true number of stars that we have
observed in the data so it's a least
square problem and if you see the
mathematical formula it's exactly the
same as the matrix factorization problem
so it's just a neural network way to
present an existing model but what is
interesting is using a neural network
building blocks is that you have a lot
more flexibility instead of just taking
the dot product as the way to capture
the interaction between the user and the
item what you can do is plug a MLP with
several layers you just concatenate the
two embeddings and then fit that to a
feed-forward multi-layer connected
perceptron fully connected receptor and
will that network will output the
ratings the number of stars and you
minimize the exactly the same loss
function but these times it's no longer
the dot product it's the output of the
network that you compare to the to the
ratings and when you do that you have a
lot more flexibility because now the the
size of the two embeddings can be
different if you have a lot more items
than users for instance you might want
to have a bigger embeddings for the
items and from a more you can integrate
metadata of about above about the users
and about the items and feed that into
the network so that you have a hybrid
recommender system and what is also
interesting for this is that if some of
the metadata is also a categorical for
instance the name of the director of our
movie you can also build a new embedding
for the director names and a new
embedding for movie jaws and so on and
so you have many embeddings as the input
of the network so you can you have a lot
more flexibility than just a single
matrix factorization model and
furthermore if you don't have explicit
feedback you cannot use a regulation
laws as we did here but
what you can do is do a triplet
architecture for instance so it in a
triplet architecture you also have a
user I that has seen a movie J for
instance and what you do is you also
pick up another movie that we call K at
random in in the rest of the database so
it's very likely that it's a movie that
the user has never seen and will never
see in the future because there are so
many movies and most of them are
negative those are movies that we will
never see we are not interested in them
so you can contrast a positive movie
with a negative movie for a given user
and so you compute the two interactions
by for instance in this case by taking
the product and you compute the
difference between the two and you want
to make sure that the positive
interaction between the user and the
positive movie is larger than the
negative interaction between the same
user and I never run the movie and you
minimize that loss that will maximize
that difference and at the end you will
train the embeddings of the model and
make reasonably good ranking predictions
what is important here is that the V
embedding that embeds the items is
actually the same for the positive and
the negative
same-same metrics would just take
different rows in that same matrix so we
train the same model parameters so those
are shared or tight parameters and if
you want to go deeper you can do the
same as we did previously but we
replaced the dot product by a
feed-forward neural network and here
again the the parameters of the two
networks are tied and in that case we
call them saya miss networks and the
loss function is very similar the whole
architecture is called a triplet loss or
triplet network this is not the only way
to deal with a implicit feedback and for
instance this is the architecture by the
YouTube team for making a video
recommendations and here you see that as
an input for a given user you take all
the embeddings of all the movies that
that user has seen in the past are the
videos that it has seen in the past you
think also the embedding of
all the keywords that you typed into in
the in the search engine of YouTube you
add additional metadata like the
geographic location the age the gender
of the person and you feed that to a
feed-forward neural net and the last
layer of that neuron that will basically
compute a nonlinear embedding of the
user at a given moment in time in a
given context and based on that you can
compare that to all the embeddings of
the items and to try to predict the the
next movie that user is going to watch
and by doing a gradient descent on that
last function which is just the logistic
basically the output is just some kind
of logistic regression and Loblaws then
you can train all the model parameters
the embedding for all queries the
embedding for the the movies and all the
parameters of the intermediate model so
it's very powerful and it's it's very
flexible because you can mix and match
many different kinds of data together at
your genius
so it's a bit of the philosophy nowadays
to embed all the things like discrete
variables you can use linear embeddings
images and sounds you can use
convolution on your network most of the
time in that case you will like to use
three tranq convolutional networks for
instance on the album cover so on as
we've seen previously on pictures of
locations if you sell travels and so on
sounds might be useful for us for
streaming and actually they have
experimented with that at Spotify I
don't know if it's in production on it
you can also do sequences of discrete
targets with recommending on network for
instance if you type a query the order
of words might matter so just doing an
average of the the the query keyword
might not be a good idea maybe a
recurrent net might be better or
convolutional neural net and you can
also embed like a more structured input
like for instance for molecules we can
take the graph representation of a
molecule and use a graph conv net to
embed it into a vector space and
obviously we're not very interested in a
recommen
molecules but it's just to highlight
with that we have a lot more flexibility
when we do that and furthermore we also
the deep learning ecosystem comes with a
lot of additional tools like optimizers
like atom regularization mechanism like
drop out and many open-source
implementation that that are very easy
to get started to experiment basically
on your data and to to do this iteration
so my main point here is this trend to
move toward deep learning tools it will
give you a lot more flexibility to build
recommender systems thank you very much
for your attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>