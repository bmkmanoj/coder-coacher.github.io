<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Rapid Prototyping In Data Science With Big Data &amp; Python | Coder Coacher - Coaching Coders</title><meta content="Rapid Prototyping In Data Science With Big Data &amp; Python - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Rapid Prototyping In Data Science With Big Data &amp; Python</b></h2><h5 class="post__date">2018-01-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/I-7Yv3FFMK0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so yes I will be talking you
today about rapid prototyping and data
science with big data and Python all
right so I want to parse this title up a
little bit let's talk about this thing
called rapid prototyping so basically
the process always starts the same way I
have some sort of problem I'm trying to
solve I generate some ideas and then you
know we're all here we all need to build
some sort of prototype some sort of
software to answer that question well
this can be very difficult in data
science because you know we don't always
know the right solution so it's sort of
this iterative process so we build our
first prototype we create some sort of
metrics or constrains to test it with
and then we analyze those metrics and
say did we answer the problem or did we
not and then usually that leads to some
sort of refinement so again rarely do we
get that on the first go this leads to
new ideas we build another prototype and
we do this as rapidly as we can we don't
want anything to bog down our process
all right so this idea of data science
how do we apply it to this rapid
prototyping scheme first question is
always you know what is the problem in
this case it might be we are some sort
of online advertiser and up to this
point we've been showing everyone the
same ad well we have this hypothesis
that if we can segment markets and we
show them a targeted ad we'll get a
better response rate so what we do is we
collect some data maybe what age group
are these people what are their
interests you know do they like cars or
they into I don't know outdoor
adventures and then location so
geographical location are you in the
Midwest are you in Ohio are you in
California and then we use that to try
and separate these different groups so
you can do that number of different ways
actually you can use a supervised
approach like classification or you can
use an unsupervised like clustering so
pretend that we had some sort of Oracle
that let us know that there were
actually two groups and we tried one of
these out say classification and we we
got two groups so we're on the right
track but we had misclassified one of
these red X's well we would reiterate
and hopefully by making some adjustments
capture the correct groups or at least a
better version so again this iterative
process rapid
typing with regards to data science
alright so now let's talk about the
slippery term big data alright so a
couple common definitions Gartner
basically if I distill this down for you
they're talking about high volume and/or
high velocity and/or high variety
information so large coming in fast and
then either coming in from disparate
data sources or some sort of nested data
Wikipedia a little bit more simply just
as data that's so large or complex that
traditional techniques completely break
down so I'm going to talk about these
guys which are synonyms here large data
and then a solution I'm gonna show you
at the end is going to be able to allow
you to handle high velocity data what am
I not going to talk about this idea of
complex data that's a completely
separate issue and I'm not going to
discuss that today so just to wrap that
up talk about data that's large in size
yes coming in fast
not complex alright so we have this
pyramid here right you know data
different scales so the question is
fundamentally where do we draw the line
what is Big Data it seems like this
changes every day
so do we draw it here when we get into
mega or gigabytes and above do we draw
it somewhere here here or beyond so why
don't we take a quick poll who thinks
it's gigabytes and Beyond raise your
hand
one two okay two souls how about what's
that
III ask you though is that true
so gigabytes plus we had two brave souls
how about terabytes and above more
people and then petabytes and above what
if I told you you're all right and wrong
it's a trick question okay so when it
comes down to let me define it for you
it's data that is too big to fit in RAM
that's fundamentally what big data is
oops wrong way so let me give you an
example so it could be at the kilobytes
scale if you're using Arduino this
particular Arduino you know it's only
got 256 kilobytes of memory my laptop
here 16 gigs I'm at the gigabyte scale
that is a big data problem so it really
all depends but at this point you're
probably asking yourself why does it
have to fit and ran in the first place
and he actually anybody know before I
get there
that's half the equation all right so
let's talk about it so if I was looking
purely at speed if you look at your
memory hierarchy here you know I go from
hard drive or solid-state I move up a
little bit in speed I've got Ram cache
CPU registers if I wanted purely speed
where would I be yes
so that's not the answer so if we take a
look at this in a different way and we
look at capacity how much data can I
hold if you look at the hard drive and
solid state this is where you're gonna
have maximum capacity right Ram a little
bit less so maybe on the gigabytes scale
typically cache you're looking at maybe
12 mega bytes and then CPU registers
like half a megabyte typically from an
order-of-magnitude perspective but again
if you guys are right if I look at speed
it's the complete opposite relationship
CPU registers wicked fast and then I go
down pyramid here but I also pick up
cost so I can't just have an array of
CPU registers that's not going to be
cost-effective so to capture this in a
really simplistic diagram what I can say
is cache and registers have great speed
awful capacity on disk awesome capacity
but no speed so Ram this nice balance of
capacity and speed and that's why this
is the sweet spot this is where we want
to be all right so couple common
solutions on how to address this problem
of big data number one we can just
probably most obvious we can just sample
our data
I can't ingest the whole thing let me in
just a subset couple issues though right
you know if I'm gonna sample how do I
know that my sample is representative
give me a given example here so we've
got some sort of cubic function right
this is reality but if we take a sample
I just randomly sampled maybe somewhere
in the middle could be somewhere else
it looks linear so if I build a model on
that I'm gonna be way off what happens
is my data continues to grow well you
know maybe early on we don't have a lot
of data ok you know it might pick up the
trends it might do a good job capturing
the signal but over time you can see
that sample because my memory is not
increased my ram is not increasing I'm
capturing smaller and smaller
percentage of the overall data so am I
really capturing trends or or signal in
that data probably not and that gets
worse over time and then three how do
you capture how you quickly capture
trends that change over time so sort of
a simplistic view of this would be you
know pretend that we're pushing out some
sort of product it's new to the market
this generates a lot of interest we see
a spike in sales maybe it's been out for
a while it sort of tails off and then
maybe we introduce a new feature or a
new model and it jumps up again and
actually will actually a better example
think of stock prices so this happens
very very quickly we have massive
changes very quickly so you know how do
we capture that with sampling probably
not going to do that effectively so
summary of the sampling here you know
you can use sampling to get data to fit
into RAM the pros are I can use my PI
data stack so thumbs up there I don't
have to recode anything so yes I've got
this rapid prototyping happening but
everything comes with a butt right cons
you have to make some extremely strong
assumptions that really usually don't
hold one that your your sample is
representative of the population too if
somehow capturing signal in this
mountain of data using a tiny little
piece of it three your population is not
changing over time so I think stock
prices again
another very common approach is to
completely move away from the PI data
stack or at least portions of it and use
apache spark so Apache spark can easily
be its own talk I'm not going to go into
a lot of detail for our purposes all you
need to know is that it takes the data
that's too big to fit into RAM and
breaks it into chunks so each one of
those chunks can be fed into memory
processed then pushed out and privately
process the next one and so on so we get
to use all the data that's great summary
so what are the pros here you can
continue using Python spark is built in
Scala so it's not native to Python but
we can still use Python via the PI spark
API we can use all the data for modeling
so this addresses some of the issues
that we saw with sampling so that's
awesome it's great what are the cons you
just say goodbye to scikit-learn
you have to completely change your
machine learning library so if you've
done a lot of work maybe you've put in a
month or even a couple of weeks of code
and work and you've spun up a number of
Jupiter notebooks and you feel
comfortable with your models too bad you
had to start all over you have to switch
from say scikit-learn to spark ml lib
and it's not a one-for-one translation
so there's a lot of scikit-learn
algorithms not included ml lab and the
Lib is pretty feature-rich but not to
the extent that psych you learn is and
of course you know you have this
learning curve so if you're in the
middle of your project you're coming up
to a deadline I need to push a model out
into production all wait time out guys I
need to revamp for the next two months
and redo everything I just did now
that's so great to tell your manager
right all right so a big problem here it
addresses all the issues that we saw
with sampling but it kills our rapid
prototyping process all right so here's
the big question right what if there was
a way to take our PI data stack and
somehow get the same capabilities we saw
with Apache spark that'd be awesome
who wouldn't be for that guess what
there's a way that's why I'm here today
so enter desk and combination with
scikit-learn
so these are both Python tools are both
native to Python they
both work really nicely together and
they provide some really cool
capabilities only some of which I'm
going to talk about today but if you
have never seen desk before if it's new
to you you should definitely go home
immediately and read about it because
it's a really powerful tool all right so
I'm gonna give you a high-level overview
of what's going to happen so imagine
this rectangular box here is the PI data
stack so again I'm going to start out
with data it's too big for RAM I need to
split it up into these chunks so each
one of those chunks can fit in around
and then I'm gonna push it into a model
so what's happening and the left-hand
side here all of this is being
controlled by desk so desk is gonna
split the data into chunks it's gonna
control the flow of these chunks in and
out of our modeling piece and of course
the modeling all the machine learning
heavy lifting is being done by
scikit-learn so I can actually put these
two pieces together and they can work as
one all right so a little bit more
detail about what's happening just gave
you a high-level overview let's go down
a level so we split our data just like
we did before we instantiate our model
you know this is gonna be some sort of
stochastic gradient descent regress or a
classifier more on that later I'm gonna
gasca's gonna push the first chunk into
the model the model is going to update
it see how I'm gonna be a better model
so I'm feeding a training data and if we
were to zoom in on this particular piece
we can see what's happening so if I have
training data because I'm doing this
stochastic lis what I'm doing is I'm
feeding in one observation at a time so
the whole point of this if you're
familiar with gradient descent
instead of doing batch right feet in the
whole partition and then update the
weights once I can do this very quickly
I can update the weights after every
observation so I converge much more
quickly on to what the weights actually
should be so I've pushed the first
observation into the model model updates
by using the gradient it adjusts the
weights and then it gets pushed back out
and then the second observation comes
into the model we make updates because
of our gradient and then it pushes back
out and we continue that process until
we've exhausted all of the observations
now I should probably mention that this
is called if your
Miller with online learning or
stochastic gradient descent this is a
single pass this only happens one time
you could definitely have this happen
multiple times what you'd want to do is
just shuffle all these you wouldn't want
to send them in in the exact same way
that you did before otherwise you'll run
into some some localization issues right
so we've read through chunk number one
we've updated our model desk pulls that
chunk out feeds in chunk number two the
exact same process happens
so our model updates and then chunk
number two is taken back chunk number
three goes in the model updates and so
on
all right so the big question how do we
actually implement this so you've seen
theoretically how it works so here we go
for the pandas users out there the
beautiful thing is that desk is actually
built on top of pandas so all the calls
are virtually exactly the same all I do
is I call desk data frame and then you
know here I'm reading in with hdf5 you
don't have to use hdf5
I happen to like it because of its
on-the-fly compression capabilities you
know specifically with blocks if you're
familiar with with compression but you
can use CSVs you can use parquet file
formats sequel table or because like I
mentioned earlier you know here we've
got on the top we have desc
on the bottom we have scikit-learn so
we've got STD classifier STD regressor
these are gonna be what are called
online learning algorithms the model is
a number of parameters there's way more
than what I have listed here but these
are some of the the key ones that you
should be aware of of course we can
provide some sort of loss function some
sort of way for the model to know is it
moving in the right direction
what is it that were chasing penalty is
regularization whether you want it or
not in which flavor the learning rates
this is think of step size for those of
you familiar with gradient descent power
T I should probably explain this to you
this might be new to some of you think
of power T as how you learn for example
if power T was one this would be this
would work great on an iid data set in
other words if you were playing a game
like checkers where the rules never
change this works really well you'd want
a power T of one
so very quickly you can figure out the
rules and then it can essentially stop
learning power T of zero would be I'm
playing against an adversary that's
constantly changing the rules so I have
to be very adaptive to what what's
happening in the world power T of 0.5 is
a great balance because you don't want
to be overly eager from a modeling
perspective and chase noise but you
still want to be adaptive to changes in
trends alright so there's a number of
loss functions I haven't listed them all
here for you but some of the key ones
say you want to do classification you
can do on-the-fly linear SVM you can do
logistic regression so you can use hinge
squared hinge log loss of course if you
wanna do regression there's good old OLS
you can use Huber loss this is a sort of
a robust version of mean squared error
it's much better about not chasing
outliers and then of course epsilon and
sense if you want to support vector
machine regression so like I said you
know reference the docs for there's even
more functions than this but these are
the key ones you should be aware of some
regularization options obviously if you
don't want regularization you can just
set that to none l1 if you want
typically leads to sparse models l2 or
elastic net elastic net is just some
combination of l1 and l2 some
proportionality which I can set with
this l1 ratio and then alpha is my
regularization strength all right so
we've talked about a lot of concepts at
this point but let's motivate with a
real example I'll show you real code and
output so has anybody worked with the
Higgs data set before just out of
curiosity nobody ok who's familiar with
the UCI machine learning repo okay just
a few hands
well this is an excellent resource if
you want to find some curated data sets
kind of like Kaggle but just you know I
want to try out some classification
algorithms I want to try some clustering
there's a whole host of data sets for
for the taking that you can work on so I
chose this one called Higgs it's pretty
famous a little bit about the details so
it's got 28 features the first 21 are
just sensor measurements and then the
remaining seven are just think feature
engineering derivative features that are
helped are designed to help you
discriminate between the two classes and
this is a binary classification problem
just saying okay you know 0 would be the
sensors are just picking up noise one is
something interesting actually happened
so for the purposes of this talk just to
keep it brief I'm not going to go
through the entire data science pipeline
show you EDA and splitting into training
and test sets just know that the the
data has been split already and the
training set has eight point eight
million examples the test that has two
point two million just for reference
alright so let's go ahead and get our
data so we import desk data frame and
then we just pull out our train and test
set you may have noticed here that I
have the exact same file Higgs beta h5
what's cool about hdf5 you haven't used
it before it kind of you call it kind of
like using a dictionary where you
provide a key and I can have data sets
all within one master data set it's
really cool all right so I need a couple
other key libraries so I am because I'm
using gradient descent I need to scale
my features I need to be able to do that
on the fly I import standard scaler so
this is mean of zero unit standard
deviation
I of course pull in my SGD classifier
and then I'm gonna be looking at the log
loss on the test set more on that to
come but I need to import that as well
all right so I need to instantiate my
standard scaler object I create an empty
list for log loss again this is just
gonna tell me at the end of a training
am I send in a chunk I update my model
I'm gonna see how that model
Dick's on the test that over time just
to see what's happening
so we'll actually see a nice smooth
curve counter zero and total partitions
this just says okay when when you tell -
Kay I've got this data set or multiple
data sets that are too big to fit into
RAM it'll split it up it'll parse it for
you into what are called partitions or
chunks this is just capturing the
strained out end partitions is just desk
telling you there's 10 or there's 15 or
there's 9 so essentially I'm going to
use this to just keep track of you know
what partition number is it and to show
me my progress over time all right so we
have to instantiate our model for this
particular one this is logistic
regression you can tell by the loss
being log I am using elastic net it's my
regularization with an L 1 ratio perfect
split and alpha 0.01 you know you can
you can tweak all these things but I set
the random state to 42 anybody know I I
set the random state looking at you
Bailey that's actually a good answer
that's why I should probably leave it on
that one but okay right
yeah reproducibility so you know if I
run this on my machine and then Allie
runs it on his machine will get the
exact same answer so you can imagine you
know there might be a little bit of
shuffling they happen so we'll get very
similar answers but they might be just a
little bit different if we don't set
this seed I like that though all right
so let's actually go through running
through the different partitions now so
simple for loop using range on the
number of partitions all right so this
data set has the features and the target
all as part of one data set I need to
split those for those of you who are
familiar with scikit-learn which is
pretty much everybody and then I need to
standardize so I need to use this
partial fit function is anyone use this
before no so it's like fit except that I
can do it iteratively so a partial fit
not all of the algorithms use partial
fit it's a small subset but you can with
a standardization so I fit on the
training set a little aside here
so a lot of people make this mistake
where you take the data set at the
beginning and you standardize all the
features and then you split in the
training test don't do that that's awful
practice what happens is you have
information leakage
you really should split first trained
eye or fit on your train set and then
use that to transform your test set
that's the proper way to do it so here
I'm fitting on the training set and then
let me explain this there's a little bit
going on here so notice that's I have to
provide you know what are the what's the
ground truth you know what is the actual
targets and then because I'm doing this
on the fly I have to tell the algorithm
upfront how many classes do I have so
you can imagine if I didn't do this see
my first partition had two of the
classes but they were really three and
then it makes you know it updates and
then the next partition comes in all of
a sudden it has three it's gonna break
so you just have to let it know upfront
how many how many classes so I'm fitting
a model on the transform the
standardized training set
I'm also providing at the ground truth
and letting it know how many classes to
look for so I've made an update on the
first partition then what I'm going to
do here is I'm just gonna use predict
Prabha on the transform test set to give
me back probabilities so logistic
regression is gonna say there's say
seventy percent chance that this is
class 1 versus thirty percent chance
it's class zero so that's an action it's
actually a pretty cool thing a big
differentiator between logistic
regression and svm SVM's you just get
class back you don't get probabilities
so something to keep in mind is anyone
use log loss before I guess maybe I
shouldn't gloss over that so okay so
maybe I've explained log loss to you a
little bit so you're all familiar with
accuracy
you know like did I get the class right
or wrong this log loss is a better
measurement your algorithm has to output
probabilities but essentially what it
says is how your algorithms gonna say
the probability of this class it's gonna
look at is it correct is that you know
skewing in the right direction and how
confident is your model and it's going
to penalize it depending on
and how it does so if it's very
confidence the wrong class it's gonna
get hammered and then I just append that
to my log loss list so of course you
know while I'm while I'm training this I
want to see some print out tell me how
I'm doing on the test log loss and then
what partition number out of the total
are we at right now and then of course I
increment my counter all right so here
are the results this is what's gonna
spit out so this is epic one I'm not
gonna explain all of these terms to you
but I'll hit on most of them this T
value over here is the number of
partition or sorry the number of
observations in this partition so
remember the training set at eight point
eight million observations here it's
brought in a million this is the average
log loss I need on the training set here
you can see the training time was about
half a second and the test log loss this
is crucial to keep in mind is about 1.75
which is pretty awful so this is the
first partition not surprising that it
hasn't done a very good job yet now I
bring in the next partition so the first
one is the readout we just saw the
bottom ones what we want to focus on
here you can see that the test log loss
is dropped from one point seven five to
one point one four and you can see a
partition one of eight so moving in the
right direction so I keep doing this so
now I'm on the third partition and I've
dropped below one on the test log loss
so my model is getting better and better
over time so I'll skip to the punchline
here won't take you through each one so
we get to the eighth partition here and
you can see note the T value here so the
T values 800,000 so you can see how desk
split that up the test log loss 0.7 so
we dropped a whole point now we're
getting into a much better range and
this whole thing the whole training time
took just over a minute on eight point
eight million examples not too shabby
what's that like the data set so this
data is that's not participate data set
but this whole process can scale to
whatever you need this is on 28 features
right yeah eight point eight million
observations so no data science
presentation would be good without at
least one graph so on the bottom here
let me orient you the x-axis here is the
partition number so when the model read
in that data it trained and then how did
it do from a test set perspective
looking at log lives you can see no
surprise at the beginning it does a
pretty bad job over time everything's
moving in the right direction and we
actually start to sort of converge
asymptotically towards the eighth
partition already all right so let's
recap what we've talked about here so we
split or in other words chunk the data
so we can consume all of it
we did that with tasks we built a model
by ingesting one observation of time so
stochastically what scikit-learn and
then really the crucial piece from a
rapid prototyping perspective all the
code even if we didn't start out using
this maybe we tried random for us and a
couple other methods we can we don't
need to completely change all of the
coding that we've done we essentially
just swap out certain pieces we're still
in the PI data stack so just minor
tweaks and we did that with a
combination of desk and scikit-learn
so in other words if I combine these two
rapid prototyping continue check we're
in good shape all right so we've gone
through a lot of material here today I
just want to step back and just say okay
where have we come from and where are we
now so we started out with this idea
that we want to follow this rapid
prototyping process with data science
because solving problems and data
science is hard it's almost impossible
to get it right the first go it's a
series of trial and error you know there
are Stocki algorithms stock approaches
that we take and sometimes those work
and sometimes they don't but it's this
rapid iterative
process that gets us towards an answer
as we saw you know from a scaling
perspective usually we start with
something small maybe a small dataset
couple stocky algorithms build on that
but ultimately we want to adjust larger
and larger volumes of data and we can
get bogged down so if I'm working on my
laptop I get to a point where I've maxed
out my ram now what do I do
do i pivot do I use a subset so to
common what I'm saying are suboptimal
solutions is sampling we saw the issues
associated with that and then you could
pivot to Apache spark which is an
amazing framework so if you're not
familiar with it and you you start
scaling into data it's a great framework
but again you know if we make the
assumption that we're trying to stay
within the PI data stack that pivoting
is gonna cause massive problems so both
come with baggage and then really here's
the big key takeaway so there's this new
library desk in combination with an
already familiar scikit-learn gives us
really powerful tools to find better
solutions faster and allows us to tackle
problems that before we're just
completely impossible so if you want to
connect with me if you have questions
you know you can find me on LinkedIn you
can find me on Twitter at David's AG
Anto email me directly I've got a bunch
of stuff up on github bunch of repos I'm
always adding to this so if you're
interested in some code that you can rip
right from today's talk
check out out-of-court computation just
know it's a work in progress I have a
lot of other things in the work so I'm
gonna keep adding to it and I do have a
blog I blog about all things data
science machine learning if you're
interested in things like online
learning deep learning how to ace a data
science interview check out my blog and
a big shout-out to Medus who gives me
time to work on projects like this
supports me coming out to speak to all
of you and of course thank you all of
you for being here at the end of the
conference on a Sunday getting out in
the afternoon and that's about it for me</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>