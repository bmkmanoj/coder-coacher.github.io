<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Instant Loading: 3 Key User Moments | Coder Coacher - Coaching Coders</title><meta content="Instant Loading: 3 Key User Moments - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Instant Loading: 3 Key User Moments</b></h2><h5 class="post__date">2017-07-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OJ8_MvYJqaA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the performance conversation has sort of
changed around loading over the last
year we're no longer just thinking about
it in terms of metrics like Dom content
loaded which aren't really closely tied
to the way that users think about you
know when a page is ready
instead we're now thinking about three
key user moments is it happening is it
useful and is it usable is it happening
means you know is the user actually
seeing something useful on the screen
are they seeing you know a spinner or
application UI skeleton screen so
something to say that you know the
browser process is actually kicked off
actually showing stuff is it useful is
are you actually delivering them value
are you showing any text content on the
screen any images any hint that they're
going to you know be able to start using
the process and is it useable is the
point when if they put their finger down
on the screen that they're able to
interact with the application now this
idea of interactivity is something that
we've assigned some targets around so we
say that if you're trying to build a
modern mobile web experience it's good
to try getting interactive in under five
seconds on 3G on an average mobile piece
of hardware now in this particular case
it looks like this phone is kind of dead
I'm pretty sure that this person is
going through withdrawal symptoms
because they're unfortunately although
our goals are being interactive in under
five seconds the reality is that for
most people we're still not quite there
the modern experience for a lot of our
users on mobile is a little bit
different you often end up waiting quite
a while for a page to paint any
meaningful pixels on the screen by the
time that it does you're usually waiting
on wet songs for the resources and then
it slowly just descends into madness
after a while so when a user navigates
to a page they're usually looking for
visual feedback to reassure them that
everything is going to work as expected
we're trying to get meaningful stuff on
the screen we're trying to make sure
that they're able to interact with our
experiences relatively quickly now this
is the current state of the mobile web
in 2017 the average web page is
interacted in about 16 seconds that's
crazy right 16 seconds it's not fully
loaded until about 19 seconds that's
like all the all the busy work of the
network is done the main thread is
relatively quiet and the average web
page trips down about 4
hundred 20 kilobytes of JavaScript now
we're all a lot of us you know again if
you set through a loading talk before
you've probably heard that images are a
really big problem on the web I'm going
to talk about another thing JavaScript
so let's let's expand on why JavaScript
matters on mobile web so this is a very
high-level view of how the network stack
in chrome works so let's say that your
user you know types in a URL on their
phone we fire off an initial request to
the server server turns in HTML we then
have to parse the HTML we get requests
for CSS JavaScript images any other
static resources and then everything
that comes down the pipeline still needs
to be parsed compiled and rendered
before we can actually give them a
useful experience now in the JavaScript
side of things the reality is that most
of us in this room are probably
developing our sites and our apps on
relatively high-end machines I see a lot
of MacBook Pros in this room for example
now unfortunately the reality is that
the time it takes to parse and compile
JavaScript on our high-end machines is
often four or five times faster than it
is on average mobile hardware that means
that the JavaScript that we're loading
up is often going to take a very very
long time before a user gets it booted
up and is actually able to offer you
know the event handlers and the rest of
the experience that you're trying to
deliver down to them one of the things
that we as an industry need to shift
towards doing is testing more on real
phones and real networks just a show of
hands how many people here use the
chrome dev tools device mode so network
emulation CPU throttling almost
everybody in the room now that's a great
first step but unfortunately it's it's
not quite good enough and the reason is
that real mobile hardware differs quite
a lot from what we offer you with the
dev tools you can have a different mix
of cores GPU CPU memory battery there
are a lot of things including packet
level differences for networks that are
going to impact your overall experience
and one of the first things you can do
to improve that is if you have a
relatively average piece of mobile
hardware you know work at home you can
plug it into your system use about
inspect and dev tools will give you
access to network throttling you'll be
able to get a slightly better feel for
what the experience is like for your
users if you don't have access to any of
that thanks to Pat meaning we also
access to a whole far
of moto G fours a device that we
consider to be relatively average now
available to folks now if you go to
webpagetest.org slash easy today you'll
find two or three profiles that are
pre-configured for this type of modern
mobile testing we've got faster 3G in
there we've got slow 3G that
emerging-market 3G in there we've got
desktop and this will give you not just
you know the results from loading on a
real phone over a relatively decent
setup of network but it'll also give you
a lighthouse report so lighthouse is a
tool by the chrome team that tries to
audit your experience for modern best
practices it also does checks for you
know whether you're progressive web app
or not but it's not necessarily tied to
that one of the benefits of having
lighthouse integration in webpagetest is
that it will give you those modern
metrics that I was talking about earlier
so the notion of interactivity and how
long it takes before your user can
actually tap and interact with your app
now when I'm trying to build an
experience that instantly loads there
usually three things that I try to do
the first is only loading what I need
for the user experience this is all
about prioritizing loading the code that
my users are actually going to
immediately use and trying to defer
loading other code until idle time to
some later point in time when it's
actually going to be useful if I'm a
single page application when I'm loading
up a route the site should only load the
resources that are needed for that
current view so let's say that you're
you know maybe you're a static site
maybe you're you know a publisher or
something low priority resources like
the comment threads the additional
product thumbnails all that stuff could
be lazy loaded only when it's needed in
the experience you want to really
optimize the stuff that the user is
going to immediately need now back to
the point about JavaScript often today
we're still shipping down large large
monolithic bundles of JavaScript and one
approach for starting to break it up is
code splitting so instead of you know if
JavaScript represents a pizza instead of
shipping down an entire people to our
users we're just shifting down a single
slice so just what they need and there
exists lots of good tools for helping
accomplished code splitting today
there's web pack there's browserify
close your compiler tons and tons of
tools and this is something that we've
actually found quite a few different
partners Twitter tinder lots of
I'm actually improving their time to
interactivity by anywhere up to 40 50
percent just by properly breaking up
their work now in case you're wondering
well do I need to code split I really
don't know whether this is a technique I
should be employing in the chrome dev
tools we recently launched a new feature
called code coverage and the idea here
is that you can load up an application
you can record a new profile with it and
we'll let you know
based on your Java Script bundles and
your CSS bundles what code was actually
used what code is actually executed so
in red and green will show what was used
in what was unused you can click through
to any particular bundle and we'll show
you in the sources view at a block level
across your source files what lines were
executed and which ones were not in this
particular case this is a photo-taking
app that I built and I actually have a
lot of unused code in here that just
never gets executed initially because
it's to do with image conversion that's
not something my user initially needs
when they're just trying to take a photo
so this work could be deferred until a
later point in time in case folks are
wondering whether you know this is a
good enough tool to be able to do
real-time analysis as well it is you can
start interacting with other parts of
the application and will actually update
in real time these bars of the very
bottoms so you can see how much of those
bundles get used as you go through the
rest of the experience another tip is
that a lot of folks building modern
experiences today are writing it in
modern JavaScript es2015 we're usually
using a transpiler like babel most
people are transpiling back to es5 and
the reality is that a lot of modern
browsers actually have decent es2015
support now so instead of trans palling
back to a much much older version of
javascript including those polyfills you
can use babel preset ends to only ship
down the code that your browser is
unable to understand so just a subset of
features that it hasn't yet quite got
and what we found is that actually can
trim down your bundles quite a lot now
I'm not going to be able to have time to
talk about all the techniques I use for
trimming down JavaScript but I consider
this sort of the ultimate webpack slide
this is my workflow for trying to trim
down JavaScript it's not just about code
splitting it's also about tree shaking
scope poisoning with newer versions of
web pack making sure I'm using the right
minify err depending on my
workflow transpiling let's code if I'm
using lodash or moment GIS or other
types of libraries using modules that
can actually strip back anything that
I'm not really using and it's going to
have a massive improvement to your your
interactivity to your overall load times
another thing that we often think about
when we're trying to build monitoring
experiences is what framework should I
use a lot of folks in this room I I know
we're probably going to be using reacts
or angular or something like that the
bloat of the baseline that you have when
you're trying to develop on mobile is
going to depend on what abstractions
you're choosing because they're also not
free they often have a cost that's not
to say you can't use you know whatever
your favorite tool is it just means that
some tools are better written with
mobile in mind and some are you to put a
lot more work in because if you're
trying to get interactive in five
seconds and your framework booted time
is taking up a lot of that you as an
application author are not going to have
as much time for your application code
you're going to have to split that up
into a much tinier chunk fortunately
there are plenty of goods lightweight
options for mobile today there's pre-act
view spelt polymer these all have a
relatively low lightweight footprint
they don't take a lot of time to parse
compile and boot up and I'm happy to
recommend them again you can use you can
use whatever you want but just make sure
that you're measuring and applying some
of these splitting concepts where they
make sense
I also thought it'd be useful to share
some of the newer byte saving techniques
that we're using at Google to solve this
problem some of these steps we literally
just got approval for so I hope that
they're useful as of last week all of
googles display ads are now served using
broadly compression and what we found is
that that's led to data savings of up to
40% 15% in aggregate over gzip this has
had a huge impact for us compared to
good old sort of gzip file compression
broadly is generally able to save an
additional 20 to 25 percent and it
achieves that by making use of a
dictionary that includes lots of common
words and syllables across a number of
different languages and it's not just
our display ads team that have found
benefits of using broadly across the
industry Google Play on Google Play are
now saving 1.5 petabytes of data every
single day using broadly we're also
seeing LinkedIn Dropbox other companies
also start to see benefits of employing
Brawley support
another new staff to share with you
Google has been heavily investing
continuously over the last while in web
P and I'm now happy to share that we're
serving over forty three billion image
requests a day using web P now web P
generally can save you somewhere in the
region of 25 to 30 percent over other
methods if you're if you're ok with sort
of glossy glossy image encoding it's
about 20 to 6% if you're okay with
lossless but this is something that
we're constantly finding as we employ it
across YouTube Google Plus Google Play
the Chrome Web Store Chrome's data saver
mode this is constantly giving us good
savings we're hoping to continue
investing on it moving forward now
something that we we've been a little
bit slower to adopt but I still think
has a lot of promise our service workers
now service workers you know you people
will often talk about them in the
context of you know they can enable an
offline caching experience that can
enable newer API to push notifications
in the case of Google inbox by Gmail we
actually saw a 10% improvement in time
to interactive by employing service
workers this is for static asset caching
so things like our bundles and this
means that if a user comes back to inbox
you know after the first time that it's
loaded we're able to just load that
stuff locally in many cases without
having to go back out the network
constantly and this is this is led to
some nice improvements on the topic of
caching the next thing that I try to do
is cache aggressively because the
fastest Network request is a request
that doesn't have to be made caching in
your applications and in your sites
should be granule enough that small
changes don't end up in validating large
squads of your site
now HP caching has got generally good
rules out there there are lots of best
practices have been well documented you
know use consistent URLs so that you
know your content doesn't end up getting
fetched and stored in multiple times
ensure your server is providing
validation tokens to eliminate the need
to transfer the same bytes when a
resource hasn't really changed on the
server consider using service workers
for sort of repeat visit control over
your offline caching story and then
we've got order of loading thoughtfully
and this is an area where I think that
there's a lot of promise I don't feel
like enough people have quite invested
in this area and we're going to spend a
bunch of time talking about this today
so I feel like a user should never be
blocked from interacting
with important site functionality
because less important non-blocking
resources are still loading and so I
want you to take a little more control
over your loading story but since this
talk is called the browser hackers guide
to instantly loading why don't we go and
tack chrome for example I don't think
this guy's like hacking I think he's
like scrolling through his web pack
config with something so we're going to
dive into some files in the blank source
this is resource fetcher I know it's a
little bit hard to see this code but
this is one of the places your resource
prioritization gets handled and I know
people can't see it so let's let's zoom
in on just one item here we've got a
special casing for stylesheet scripts
and media and we're able to assign
different priorities based on the
different resource types UC chrome has a
kind of a schema for the way that we
prioritize different resources layout
blocking resources like you know CSS or
fonts get assigned a higher priority
network wise load in layout blocking
resources like images that are in the
viewport and a medium priority async
scripts get a lower priority and if now
if you're wondering well why have I
never seen this before well first of all
Pat meaning has done a great job of
documenting this but second we expose
this in the network panel of chrome dev
tools under priority so if you take a
look at the priority column you can
actually get this information yourself
for your own sites now if we go through
all these different special casings what
if you know instead of saying some
resources had a lower priority and some
had a medium and somehow two higher what
have we set absolutely everything to
have a really really high priority on it
that would solve everything right that
would just that would just be the best
we could we could go home today we could
give our users a very custom browser we
could even call it blue infinium and you
know we could give it to all of our
users on a USB Drive except that doesn't
quite work see the original sites that
we were profiling here this is the Jas
comp site if first meaningful paint in
about 5.3 seconds with this change with
fluent inium where we assigned the
priority of everything to really high we
actually regressed first meaningful
paint didn't happen until about six
point six seconds the takeaway here is
that whenever
thing is high-priority nothing is I
ended up having to go back and actually
play around with different resource
types in the page trying to discover
what is what is the late discovered
resource what isn't and what you know
what what exactly should we before be
prioritizing a little bit better in this
case the CSS and fonts were particularly
important to prioritize so I I have this
sort of mantra that I try to follow
whenever I'm building something and it's
first do it then do it right then do it
better and so we're going to do that
with this experience so going back to
the way that chrome handles Network
requests we said that you know there's a
point in this phase where we're parsing
resources to try to figure out you know
reversing HTML by figuring out what
assets needs to be fetched next and what
can happen with that parser when it hits
a that's like script is that you can get
blocked and so some modern browsers have
got this idea of a preload scanner think
of it as sort of another parser that's
able to peek ahead in the document and
discover critical CSS and JavaScript
resources even when the parser is
blocked instead of having to wait until
the CSS object model for example is
completely constructed we can find and
request resources earlier this is great
the modern browsers have actually seen
some wins as a result of this
unfortunately that takes us to our next
problem which is resource discovery you
see the dependency trees for many modern
sites are actually quite deep and as
good as the browser can get at loading
you as authors of your pages and authors
of your apps know a lot more about what
is critical to your user experience than
we do and the way that we give you
control over that process over
prioritization is a feature called
preload one of the folks behind that is
you have wise to sitting in the front
row so thank you all for this preload is
basically a declarative fetch which
allows you as an author say that they
are some late discovered resources in
your page that you think are super super
important so that the browser is able to
load those with a slightly higher
priority now the way that you use this
feature is if you're declaratively
trying to use it you can drop in a link
rel preload tag you specify the type of
script that you're trying to load up
specify the URL for the actual resource
you can also use HP response headers to
accomplish the same thing these headers
can come in particularly useful if you
have a separate
Foreman's team that are handling this
work rather than you directly yourself
and in the case of things like
JavaScript bundles where you know if
it's particularly a league discovered
resource if it's something that the
browser is not able to detect until much
later in the page we can take requests
that are happening all the way over to
the right and we can move them all the
way up to parse time all the way up to
the left and what I've seen in modern
applications adopting this is great
improvements for their time to
interactivity in general so that's
preload another thing that we gave you
our resource hints the ability to tell
the browser hints the browser that
there's going to be some things that are
also important but they're not really
instructions they're more hints that it
could be useful to load them up so
things like DNS prefetch for pre
resolving DNS host names for assets
prefetch for hinting through the browser
that a resource might be needed but
otherwise delegating it to the browser
to decide whether it's important or not
now the way that I think about preload
and prefetch now this is a very
important distinction preload is good
for resources inside the current page
that you know are going to be used by
the user prefetch is good for future
navigations so if they're going to
probably go to other routes at some
point in the future and you want to
prefetch some of those resources
prefetch is good for that preload is
otherwise good for the current page now
an app that I think makes really good
use of a lot of our modern loading
primitives is shopped by the polymer
team in chrome now shop is an
application that uses granular loading
to only ship down the code that's needed
for every single route it takes
advantage of modern loading techniques
to intelligently prefetch resources that
are needed for other future navigations
so as we go through the shopping
experience making sure that things like
the product pages have got their
resources prefetch in advance and it
also takes advantage of things like
offline caching so that you're able to
offer your users an instant loading
experience when they come back for
repeat visits the way that shop
accomplishes this and this is this is on
average mobile Hardware over 3G by the
way the way that shop accomplishes this
is using a pattern we call purple or P
RPL purple is a pattern for structuring
and serving modern web applications with
an emphasis on the performance of
application delivery and launch
the idea behind purple is that it stands
for push render precache and lazy load
so we pushed the minimal code that's
necessary for a page or a route to
become interactive we render that
initial route we pre cache resources
that are going to need it's a pre
caching using serviceworker the
resources that are going to be needed
for future times the user comes back to
this experience so they're only going
out to the network when we absolutely
need to but were otherwise taking
advantage of proper caching and local
access these files so they load up as
quickly as possible and then we use lazy
loading to load up things that aren't
necessary for the immediate critical
user journey but are going to be useful
for later on in that process now purple
as a model is great for achieving a
minimum time to interactive maximum
caching efficiency and overall
simplicity of development and deployment
and one of the things we can actually do
is take a look at employing some of
these strategies across you know the
development of shop itself so when we
were building shop we started off we're
just serving it down over h2 with 3G and
we got this sort of network waterfall
but had this kind of step pattern to it
and we had this period of time at the
very start where we've got sort of you
know we've got sort of server think time
it's not really doing a whole lot at the
very start we introduced link rel
preload into the equation and you can
see that everything is now very very
flat it's slightly different we've
shifted some more of that work into you
know into another part of the screen and
in blue what we can see is the time from
first bites to first content full paint
is down to just three point three
seconds just from that initial request
don't have that stepping behavior
anymore now unfortunately we still have
all this idle time at the very start
where we could actually be doing
something useful to make sure that the
user experience is loading up even
quicker and the solution to that one
solution to that rather is h2e - server
push server push eliminates idle network
time between the server sending one
response to the client and waiting for
the next request now looking again at
the network stack in the way that we try
to process this the server is able to
use think time to push order
critical resources to the client
typically your CSS and your JavaScript
bundles so by the time that the think
time is over there's a good chance we've
already sent all the required critical
resources back to the browser so that
they're already in the push cache they
can already be used by the browser so
the idea is at the same time that I'm
sending down my initial request for my
HTML I'm also able to send on all these
resources that I know are critical to
user experience and that can save me
round-trips instead of having to go back
and forth between a client and the
server if you're wondering what this
looks like so this is an example using
node and Express we're just setting some
of our hasty response headers here we're
setting our pre loads for or h2 push
this is something you can accomplish
using you know PHP you can accomplish
you're using other languages but in
terms of the impact that this had on
shop is we get a very very different
timeline you know we've still got a
little bit of a cost at the very start a
tiny bit of it cost we're still spending
about 800 milliseconds in the initial
request but we've actually managed to
save at this point a ton of seconds off
of our overall loading time we're able
to get interactive in just a few seconds
much much quicker than we were before
we're able to fill up that server think
time intelligently so we're using HTTP
where I haven't shown it but we're also
using prefetch for future navigations
and you know it sounds you know that
like hasty pushes may be kind of a
magical solution for filling up this
time and saving us a bunch of work but
unfortunately it's not quite that
straightforward you see one of the major
things that people often do wrong as
soon as they discover hb2 server push is
that they start thinking to themselves
well hey I have all of these static
resources that my page might need I can
just configure every single one of them
to be pushed down on all pages and the
main reason that's bad is caching
because there's a good chance that some
of those resources are going to be in
the browser cache after the user visits
the first page and unfortunately hb2
server push is not as cache aware as we
would like it to be now ideally the
solution to this is some sort of bloom
filter a cache digest of sorts that
would be able to beacon back to the
server
what contents are in the users cache so
that we don't overly push resources when
we don't need to now they're in
sort of this notion of a cash digest
over in spec land that's still being
noodle on by folks it hasn't been really
you know implemented by any browsers but
thankfully there's a workaround for it
that we can use in the meantime and it's
using serviceworker so we combine h2
server push with our serviceworker which
will allow us an additional offline
cache now explain this it tries to avoid
that downfall server push where we're
pushing too much stuff down that may
already be in the cache so on first load
the browser makes a request the server
the server pushes down the critical
resources it pushes down the document
the document gets scanned by the browser
it knows what to request but oh you've
already pushed those resources so they
don't need to go back out the network
for them on the next load when the user
comes back to the experience we've
installed the serviceworker
at that point and the page doesn't even
have to hit the serve the server at all
it's the serviceworker and so we're able
to load those assets locally this avoids
one of those server push gotchas and
service workers are not necessarily the
only way to work around this problem I
have seen some people investigate using
cookies to also track what's in the
users cache and beginning that back and
forth between each server but I found
serviceworker to be a relatively low
friction way of accomplishing this type
of pattern so push versus preload push
is able to cut out an RTP it's useful if
you have service workers or at some
point in the future we have that notion
of cache digest but it doesn't really
have quite the same level of
prioritization or cache awareness that
we look like preload allows us to move
resource download time post or an
initial request
it's cross-origin and it's got load and
error events it's got content
negotiation support in general I found
preload to be a relatively low friction
thing to experiment with HD push does
have a lot of gotchas it does require an
amount of experimentation and
understanding of those gotchas before
you ship it to production now i just
wanted to show with our service worker
employed in this experience when a user
comes back it's not just that workaround
for server push that we have helping our
loading experience because most of our
resources are now coming directly from
the local disk cache we're able to boot
up this experience and get it useful for
our users in just a few hundred
milliseconds you can actually see I know
that it may be
a little bit small but this is loading
from our local cache and I'm actually
able to see meaningful pixels in about
58 milliseconds it's kind of amazing so
if you want to try stripping an instant
loading experience today use the
platform take advantage of the network
loading features that browsers like
Chrome offer now there are a ton of
server push rules of thumb that have
been documented by some folks on the
chrome team and lots of other folks in
the community that are in sort of the
web performance space try to avoid you
know pushing too much stuff try to push
things in evaluation dependence order do
use some mechanism to track what is
inside the client-side cache there's
more information available in this great
doc at bitly / HQ push in case you're
interested in these gotchas Jake
Archibald recently also wrote a really
great article where he talks about some
of the difficulties he ran into with HT
to push now if you're trying to employ
strategies like purple and you're
building a new application um you might
wonder where to get started if it's a
single page app I'm happy to recommend
polymer app toolbox or the pre access Qi
both of these have support for the
purple pattern right out of the box and
you can go and check them out so we've
been talking about the purple pattern
and this one demo app but I also want to
talk about an app that we we had the bet
the privilege of working on in
production and that's Twitter light I I
love Twitter it's it's kind of like
group therapy where nobody ever gets any
better but it's it's great at and they
recently actually completely revamped
their mobile experience they shipped it
as a progressive web app this is you
know Twitter Twitter's company that have
about 330 million active users
80% of were on mobile and with this
progressive web app they were able to
accomplish something like a million
daily loads from the home screen but
that's not what I'm interested in I was
actually really impressed to see that
they were able to get this application
interactive in about five seconds this
is a reactant web pack out by the way
when Twitter started writing this new
version of their application they
weren't in a great place performance
wise they were getting interactive in
about 15 or 16 seconds overall load was
occasionally taking anywhere up to 30
seconds and so they started looking at
the purple pattern for some inspiration
and so let's start off with push and
preload
their infrastructure was not able to
support h2 server push and so they want
to take advantage of preload and some
resource hints to at least get them a
little bit better than the situation
they initially had first thing they took
advantage of was DNS prefetching using
link rel DNS prefetch that's an attempt
to sort of resolve domain names before
user tries following a link and they
found that that actually led to an 18
percent improvement over their entire
load experience because Twitter ends up
connecting to quite a few different see
DNS for their content their static
assets for all that media that you see
the next thing they did was employee
link rel preload for their critical
bundles in the application that led to a
36 percent improvement in time to
interactive moving on we get to render
now making sure that we get pixels on
the screen as quickly as possible now
Twitter is an application that is very
media heavy as you scroll through your
timeline you are probably going to run
into lots of pictures lots of animated
cat gifs videos lots of things like that
and all of that work needs to be loaded
up Twitter ended up finding that request
I'll call back this browser API that
helps you schedule work when there's a
free period of time at the end of a
frame allowed them to see a for buy
improvement in render performance just
by using it to defer the JavaScript
loading of images that are below the
viewport using Java scripts another
thing that they ran into was they found
it was a little surprising that in many
cases they were actually shipping down
high resolution images to users when
they didn't need that they switched over
to only shipping down images at the
exact dimensions that were necessary for
the Twitter light experience as well as
introducing a brand new optimization
pipeline for their images to make sure
that everything was properly encoded as
well as it could be and that went that
took them from sort of image decode time
on many images being anywhere up to 400
milliseconds all the way down to 20 now
if you were trying to ship an instant
loading experience with images that you
want to load up relatively well or
efficiently I would try to adopt some of
these ideas so choose the right format
do some research around whether you can
get wins from web P or other image
formats size your images appropriately
so that you're not having a too
expensive amount of image decode costs
adapt intelligently using the picture
element compressed carefully prioritize
your critical limits
as lazy boot images that are not
necessary for sort of the initial user
experience and take care with tools
because we do know that some of our
tools don't do a great job of stripping
out the metadata that aren't necessary
in there something I was impressed about
with the Twitter light experience was
they also introduced the data saver mode
where they blur out images and videos in
the experience so that we don't actually
load them until you the user say well I
actually really want to look at this
media so if you tap on it it's going to
go and make that a request off but
otherwise by default not including all
of those requests in your overall
waterfall they're able to see up to a 70
percent improvement in their load times
using this technique next up we have pre
cache Twitter took a very incremental
approach to adopting serviceworkers they
started off with sort of a new
serviceworker that didn't really do
anything then they looked at static
asset caching for the Twitter emoji that
you see when you're trying to reply to
people as well as their JavaScript and
CSS bundles we eventually ended up
adopting the application shell pattern
for their UI caching and what this did
was rather than it taking six seconds
for their JavaScript and experience to
load on a good 3G network pre cached it
took only one point five seconds once
those assets were already in the users
cache that's a 75% improvement next up
we've got lazy loading so remember we
said that it took somewhere in the
region of 1516 seconds for Twitter to
initially get interactive and that was
because they had three large JavaScript
assets which totaled over a Megan size
that work on an average phone took
anywhere up to five and a half seconds
just to parse and compile which wasn't
great so they adopted support for code
splitting using webpack took advantage
of vendor chunking just making sure that
libraries and other abstractions that
were necessary across lots of different
routes were in their own chunk that
could get cached and what they found was
that taking this approach to sort of
intelligently caching things a little
bit better intelligently splitting up
their work meant that it only took three
seconds to load up the JavaScript
necessary for their experience we've got
case studies available on the entire
Twitter lights experience that we had
from both the Twitter team and RCM in
case you're interested learning more
about that now one thing that the
Twitter light experience didn't really
run into was web font loading as a
problem now 68% of sites on the web load
at least one custom web
on and so having a website optimization
strategy can end up being a pretty
critical piece of your overall
performance loading strategy a lot of us
are familiar with this notion of fout so
a flash of unstyled text where you can
try loading up a webpage and you'll see
text initially and then it jumps around
and starts getting really jittery and
janky once the web ball finally loads
and finally kicks in which is a kind of
weird you know we really weird loading
experience for users another is fight so
a flash of invisible text where the
browser can end up completely blocking
showing any content at all until your
web fonts are loaded web font
loading experts recommend having a good
comprehensive web font loading strategy
is being pretty critical if you want to
make sure that you're rendering these
custom fonts as well as you can be for
your end user experience this is a
really great visual from the Zack lead
blog he's got a really great blog post
that talks about having a comprehensive
web font loading strategy and I wanted
to quickly touch on some of the items in
there so we talked about preload today
wood web fonts you'll often end the
first needing - parser HTML parser CSS
before you can actually get to going and
fetching your fonts at all and so
similar to the idea that we saw earlier
with our JavaScript run goals and
prioritizing those a little bit better
with preload we can shift this work all
the way up to the other side of the
waterfall we can make sure that those
fonts are getting fetched much much
earlier now remember that's not to say
that every single asset in your page
should be getting preloaded the stuff
that you consider critical to your user
journey is probably not going to get
discovered until later on is the stuff
that you should be pre loading now again
if you're going to use preload for this
stuff you can use link rel preload to
specify type fonts sorry as type is font
you can also use a HTTP header to
accomplish the exact same thing I was
actually quite surprised digging into
the HTTP archive to discover that most
of the sites they're adopting support
for link rel preload are using it to
preload their web fonts it's led me to
discover that's Shopify we're pre
loading web fonts and this led to a 50%
improvement in their time to text paint
one of the benefits that this also had
for Shopify was that it completely
removed their Flash and visible text
completely
so consider that another thing that we
recently landed in chrome in chrome 60
was better control over font performance
using font display a new CSS property
for font face the idea here is that you
as an author control whether you want to
block swap whether you want to even say
the web fonts are completely optional
things so that you know if the browser
is not able to load them quickly why
should it load them at all why did she
use them at all now I'm going to touch
on optional because I think that
optional in font display is kind of one
of the most powerful things that we've
brought the platform lately the idea
here is that if the browser is not able
to quickly load a web font we will
continue using the fallback if that web
font is able to complete its request
we'll put it into the cache and so the
next time that your user comes back that
experience and they come to the page
they will get that custom web font it's
just something that won't be as jarring
third overall experience we've also got
the CSS font loading API in case you
need better control over manipulating
CSS font phases tracking download
progress we've got a lot of blog posts
to talk about some of these concepts
available lots of web font loading tips
available to you this is a quick cheat
sheet my colleague Monica over another
great blog post about the stuff you want
to check that out but in general has a
web font loading strategy now in the
last few minutes I thought it'd be
useful to talk about the future I think
one of the big opportunities that we
have for loading on the web is going a
little bit more progressive so instead
of this idea of waiting for all the
content in our HTML files to fully be
fetched before they can be processed and
renders what if we were able to fetch
things in chunks and then start to
render them on the screen as the rest of
the experience starts getting landed on
our side one of the things that enables
that is the streams API this is one of
the fastest ways you can serve content
blog like content benefits from this
quite a lot and if you pair this
together with other loading features
like service workers you're actually
able to get an incredibly competitive
loading experience off the back of it
we're able to relatively you know well
beat things like server-side rendering
or serviceworker rendering with client
render Jake Archibald has got some great
blog posts about this another thing that
we're exploring is you know the idea of
trying to line a little bit
with browsers like edge around how we
handle CSS in Chrome so edge does the
thing where it will block the parser
until the stylesheet has finished
loading but allow content above the link
tag to render what that means is that
style sheets are able to load in
parallel but apply in series and it
takes stylesheet behavior a little bit
closer to being like script source that
looks a little bit like this it's the
idea of like having a link rel tag right
above the content that you want to start
rendering to your user a big thank you
to Pat mean--and who's been driving a
lot of this work in chrome and one other
thing that I think we have a huge
opportunity around and I'm hopeful that
some folks in this room will explore it
at some point in the future is the idea
of us getting a lot more data driven
without loading I think today we make a
lot of guesses we you know we we eyeball
traces we make a lot of guesses based on
waterfalls of what makes sense for our
users I'd love to see more folks using
analytics data to drive their decision
to drive they're loading experience
using things like the analytics API is
we've got available to decide what
should be prefetch what should be
pre-loaded using machine learning using
tensorflow to make those decisions about
what makes sense to eagerly get in
advance for your users I hope that
loading is a topic that you know a lot
of folks in this room care about I know
that lots of folks are standing -
hopefully you do care about this topic
but performance is sort of this
continuous game of measuring for areas
to improve it's not just this one thing
that you do once and then say it's done
so I hope you found some of the loading
techniques and ideas and safe talk
useful if you have any experiments that
you're working on in this space please
feel free to share those but that's it
for me thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>