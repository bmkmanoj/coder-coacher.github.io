<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Make your Go go faster! | Coder Coacher - Coaching Coders</title><meta content="Make your Go go faster! - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Make your Go go faster!</b></h2><h5 class="post__date">2018-03-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-_05_OXYLHA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">computer program optimization making
things go faster is is like a passion is
it's basically my video game you know I
some people sit and they shoot it
zombies for several hours I cycles and
make things go faster so the most
important thing you need to do when
you're optimizing your programs is
measure things do not start optimizing
until you know what it is doing that is
the most important thing the second most
important thing is to still measure and
the third most important thing is to
keep on measuring so do always measure
measure measure never just go change the
code because you think it's gonna go
faster because most of the time you're
wrong and you're just gonna waste your
time what do I mean by that measure
first of all measure big things you know
people show up sometimes they post
online or whatever they they have a
profile where something is taking like
21 milliseconds or something like and
you know who cares unless you unless
you're high-frequency traders I used to
work in our chronic trading I hate those
guys so unless you're actually you know
paid by the micro second measure big
things also that the tooling and goal
like Sam the profiler samples 100 times
a second so don't tell me about
something that takes 21 milliseconds
because it's a sampling error so so
measure things that you could literally
stand there with a stopwatch and time
and if you don't have anything like that
then run it a million times and then
it'll take appreciable time so measure
measure big things first thing cuz you
won't just lose the effect unless you
bigger big things measure all the time
you're gonna miss it
if you're not measuring and you know
okay I'm standing here as a guy who
sells very measurement and observability
systems and so on but we this is this is
pretty useful to like have things like
your CPU usage all the time so you can
see when things changed all that kind of
thing and the other thing I got into
this this particular tool is Yeager who
uses Yeager
what cool so there's a there's a bunch
of tools like this their zip King and
dapper is the sort of daddy of this this
family and I used to write things like
this myself when I worked in a chronic
trading but the basic let's let's do
technology here
that's so basically ideas you got you
got a you got a timeline you get a
horizontal bar illustrating how long
everything took and then you got a kind
of hierarchy of the the breakdown of how
long things took within that and the
point is don't go optimizing your goal
code if it's not actually in the goal
code that it's slow you know so a first
approximation most programs will spend
all of their time waiting on i/o so
don't don't go hacking around there you
go code if it's waiting for i/o because
it's not gonna make any difference so so
this talk is about after the time when
you've measure measure measure after the
time when you've looked at the the
traces and your uh Turley sure that it
is using CPU time in your goal code now
the talk starts okay so what do you do
oh talk didn't start I'm gonna I'm gonna
draw examples from this thing which is
we've cortex this is our this is our
distributed time series database and you
don't really have to know anything about
it other than I'm using it in two
different ways I use it to draw these
charts and I also speed it up you know I
speed up the analytics engine and I
speed up the ingression of data so so
hopefully that's not too confusing if
you if you want to see this the very
specifics of what I've been doing go to
github.com we Forex cortex and look at
all the PRS with my name on because most
of them are about speeding this thing up
so yeah I just put that up it's it's we
ingest tens of millions of time-series
in real time from lots of different
customers and that's why I needs to go
fast right okay know the talk starts
profiling so everyone knows this right I
I guess I don't know hands up if you're
already an expert on this slide
yeah maybe just lazy me didn't don't
want to put your hands up okay so the
talk I'm going to get into more detail
but this is the this is the simple start
go read the blog this is the bit I'm not
gonna cover you know run run your
program under the profiler run be prof
tool and the the bottom if you do this
then you will have an HTTP endpoint
where in your production system you go
grab a profile which is really useful so
yeah that's all in the blog I'm not
going to cover that here's a profile
here's what you'd get out if you run
that go tool pre prof command and so
this is this is a quiz okay so who knows
who knows what the problem is
okay silence sorry
it only profiles when it's being tested
this is a profile from our production
system garbage collection I I made the I
made the words highlighted and made some
of them a bit bigger but the this is
very common that you will run the
profile and you'll see these guys show
up you know like runtime GC drain things
like that you'll see them in your
profile and you'll think well darn you
know I I don't know anything about
garbage collection so I can't I can't
fix that code and it's pretty
complicated code as well so but do not
fear I'll tell you what to do what is
going on with garbage but there's this
question do we have plans
yes I will explain I will explain the
question was how do I know garbage
collection is the problem what so yeah
what I mean is if you see those
characters runtime GC drain in that
order somewhere near the top of your
profile then you have a garbage
collection problem and and I predict
right now you have a garbage collection
problem because it's a garbage collected
language and unless you anyway let's
let's do the rest of the talk and then
come back to me if I didn't answer your
question so this is this is
visualization this is if you hook your
go program into Prometheus you can get
this startup for free and and this is a
sawtooth pattern right the the memory
builds up and then it goes bang down and
builds up and bang that builds up builds
up so it can be more complicated than
that the garbage collector kind of runs
along in the background it can be doing
other stuff but this is this is fairly I
want you to get that idea that it
started doing doing this sawtooth
pattern all the time okay now why is it
that interesting well let's talk about
memory so this is sort of standard
architecture of a modern processor
processor sometime in the last 20 years
and they the processor like the bit
that's actually making decisions and
doing things a something like a hundred
times faster than your memory so there's
a there's a block page numbers every
programmer should know where they
actually write down what those numbers
are in terms of nanoseconds and so on
but but it you know it's orders of
magnitude slower and in order to just
not have this thing waiting around all
the time we build it we build a
hierarchy of caches here typically to
this is not to scale typically the the
the level 1 cache l1 cache is like
hundreds of K and typically the l2 cache
will be like 2 3 4 megabytes
and so this the level 1 cache goes the
same speed is the processor the level 2
cache goes a bit slower the RAM goes
horrendously slower so for your program
to go fast you want everything you're
doing to be in the cache so there's a
picture of a cache as you can tell I'm a
great graphic artist as well as the
programmer no I'm not but I tried I
tried to indicate that that's how this
works there's there's different color
codes there's bits of bits of memory
which are being cached in the cache and
they're also in their one cache and the
processor is actually only working on
the memory in the l1 cache so think
about that sawtooth the action of going
through everything in memory and trying
to figure out what's garbage and what
isn't
basically wipes the cache maybe not
absolutely but but that activity of
going trolling through the memory that
and just just the activity of allocating
more memory pushes things out of the
cache if I move over the line the camera
cannot see me okay the moment for the
line okay sorry I can't do graphic art I
can't do interpretive dance but I do
know how to make programs go faster and
I'm trying to try to tell you yeah so so
you don't have to understand everything
about how a processor works but I am
trying to get across this idea there
there are there are like technical
physical reasons in the silicon why
rying around allocating memory throwing
it away and letting the garbage
collector clean it up for you is gonna
not only slow your program number
because the garbage collector is doing
work but it slows the rest of your
program down because it kicked
everything out of cache so let's let's
look at some anecdotes this is another
one of the oh that's a memory profile
yes
sorry that's important so in instead of
saying - CPU profile when you profile
your program if you say - mem profile
you will get and then you can ask for
things like the number of allocated
objects that the default you get is the
object in use right now which is not the
interesting number so you need to you
need to use a flag like this alyc
objects and I like to do the cumulative
so that gives me a sort of a top-down
view of who's been allocating the most
things so I looked at this and I see a
bunch of encoding and I thought you know
it's not you can go look at the PR if
you want to see the actual code but you
need to start if you want to speed this
up you need to stop creating garbage and
just as in the real world reuse reduce
recycle
I think recycle is basically a garbage
collection so so don't do that one but
but reusing objects and just reducing
the number that get allocated is the way
to go so a little bit more detailed
profile this one so this one is Alec
space there's a little bit of an unusual
one because there was this massive this
is actually a there's a little micro
benchmark that I did of one of the
functions in the earlier profile but we
were using this library called snappy
which is a compression library and it
turns out if you if you call this new
reader function it allocates like a TK
just one object ATK and and think of
that sawtooth is being driven much that
much faster by that that allocation this
is the timing so I'm running I'm running
like a real call in our is in our
staging environment every every five
minutes and just timing one particular
call and and this is the effect of
releasing that one change tick to using
async
dock pull which is a built into the bill
go standard library that's for reusing
objects that have a high cost of
allocation so putting that one change in
which is like like six lines or
something like that and takes that down
you know it's it's what from 90 seconds
to like seven and a half something like
a 20% improvement one allocation
I'm going quite fast because we're low
on time this was one where this is one
where this sort routine did a comparison
by calling this function which did in s
printf the the cumulative effect of this
is to create a lot of garbage and the
sort comparison is called a lot so
instead of instead of kind of indirectly
going through strings I just wrote a
comparison routine again it's like ten
lines or something that one got me
another twenty percent improvement here
so
and actually I should I should stress so
this is there's a massive data store
distributed data store this time
includes the time to get it off the disk
in others it is fundamentally
bottlenecked on i/o but I was making 20%
improvements in the whole call by making
effectively tiny changes to the go code
so do really worried about your garbage
ok stack versus heap everyone knows this
right the who who doesn't quite
understand stack and heap and go a
couple of people ok so first thing I
should I should disclaimer it's not it's
not the law that there's like a stack
and a heap it just happens to be that
way today and the implementation and
they you know they could make
improvements some of the things I'm
saying could become false in the future
but at the moment it's pretty likely if
you do something like this your variable
will be on the stack and if you do
something like this then your slice will
be on the heap so what I mean so there
you go stack versus heap stacks are very
neat the only thing you can do is stick
something on the top so when you're
finished finished with it you just
effectively just move the pointer don't
you just say well you know I don't care
about that stuff and you know bang gone
heaps you need to go carefully through
the whole heap trying to figure out
which things you're still using in which
things you earned so the stack is
enormous ly faster to work with than the
heap so that's why you want to you want
to watch out for things that are on the
heap how does things get on the heap
they escape ok quiz time again and I I
just i sat there for half an hour making
all the fonts bigger because I saw in in
earlier talks that it was quite hard to
read so um yeah which well we won't wait
for you to maybe compile it or whatever
so how would you find out what's going
on so this is a benchmark if you
run go - test - bench then goal will
will run your program enough times that
it takes a second so it ran my micro
benchmark like 30 million times and it
said every time around that loop it's
doing an allocation so why is that next
thing - mem profile so we can see how
many allocations and then you can you
can do this - list so - list says show
me the actual lines and put a count
beside them so I said this is the number
of objects allocated so it's on this
line that we're allocating an object and
kinda doesn't look like we're allocating
an object because you know it's a
constant string and there's no pointers
here so you know I didn't call make you
know why why is this on the heap so the
next stage in this analysis is to tell
the compiler to print out the escape
analysis so - GC flags GC GC does not
mean garbage collection in this context
it means go compiler - M says tell me
about escape analysis - M - M says no
really tell me about this kit you
because there you can add it's kind of
it's verbose I I filter down this one
command for that 110 line benchmark
program prints out like a hundred lines
so I filtered them down but they're all
they're all marked with which light I
don't want that they they're all marked
with which line in your program they
came from so this this is the answer
here it is a parameter to an indirect
call that's why it escaped and the so
buff is a IO dot writer in my benchmark
which I've sort of scrolled off the top
of the screen but if go back to the
example that's an interface that makes
an indirect call
if I'd actually said buff is a bike bite
stop buffer it's not an interface as a
concrete type and actually that
parameter will not escape in that case
and the program goes a lot faster so
take that as read that's the yeah scape
I'll skip analysis is my point if you if
you can't spot from the high level stats
why your program is creating garbage GC
flags - M - M and it will tell you why
the data is going to the heap generally
address passed out of a function I mean
that's the sort of favorite example you
can't do this and C but you can do it
and go you can just return a pointer to
a local variable it will escape to the
heap parameters of indirect calls that's
what I just talked about and that's
that's the things in the brackets and
the thing before the dot that's also a
parameter my time is up so there you go
and look at memory allocation it's
always memory allocation thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>