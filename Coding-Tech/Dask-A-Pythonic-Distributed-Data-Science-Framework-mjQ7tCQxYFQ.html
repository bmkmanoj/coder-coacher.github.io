<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Dask: A Pythonic Distributed Data Science Framework | Coder Coacher - Coaching Coders</title><meta content="Dask: A Pythonic Distributed Data Science Framework - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Dask: A Pythonic Distributed Data Science Framework</b></h2><h5 class="post__date">2018-03-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mjQ7tCQxYFQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I come from like the the numpy side of
the pie that the pecans dak super sort
of raise your hand if you're more
familiar with numpy than django it's
more familiar with django than numpy
yeah that's great it's like 50/50 this
is a crazy conference I like so so Jake
mentioned this in the beginning of the
keynote today which I loved that we are
a mixed group I think would be a talk
about mixing a couple of those sides
together we're gonna take the sort of
the networking side of Python and use it
alongside the data science at Python to
make a distributed computing system so
we have this really strong analytic
system and Python is I was like numpy
and pandas and scikit-learn they're very
fast they're very intuitive scientists
like them analysts like them they have a
flaw which that they were designed to
run on a single core mostly and on data
that fits and ran as that is becoming
less and as we're sitting at larger
large data sets and larger larger
clusters we so we now want to expand
those libraries that whole ecosystem
beyond just single core processing that
and that's hard to do so just one
paralyze a single library we want to
paralyze an ecosystem of thousands and
thousands of packages so I'm gonna steal
some slides from Jake so these are
slides Jake brought up in the keynote
this morning with a Python scientific
stack and there's Python at the core
there's some core libraries on top of
that like numpy on top of that we things
like like pandas and Syfy and all these
other sort of slightly more peripheral
packages as you go out there's actually
that was in the little small packages
packages for genomics and for people
doing you know sequencing and people
doing studying the Sun and there's
thousands a little small research groups
that are all producing software using
their expertise that they have alone to
support lots of scientists these are
people doing real work helping the world
and we want to sort paralyse not just
numpy or pandas but everything that's
that's that's a big challenge that's
sort of the challenge we're sort of
faced with and are foolishly trying to
train to solve these packages also we
have custom algorithms it's not just one
kind of thing they'll have their own
secret sauce they're made by smart
people so we knew parallel computing
library that can be flexible enough to
handle
both you know solar astronomers work and
the Janome assists work it is familiar
enough that it can be adopted by a large
number of people and this is again a
challenge so in order to try to solve
this problem and talk about tasks
something that I another's have worked
on for the last few years and I hope you
enjoy
so this talk will be sort of where to
start from sort of the more numpy pandas
east side so people who are more numpy
pandas will be sort of happier more
comfortable beginning I'm going to end
up more towards the sort of tornado
networking concurrency side towards the
end so hopefully there's something for
everyone
we're talking about first paralyzing
numpy and pandas which is sort of the
first intent of tasks then we talk about
paralyzing more general code which ends
up being necessary if want to paralyze
lots of packages will talk about task
scheduling and task graphs and sort of
compare with other computing systems
that can do this sort of thing things
like spark or airflow they will talk
more about how task solves this problem
we'll finish up with some sort of
general talking about Python API is and
protocols that is useful to to adhere to
order to gain better adoption and then
we'll sort of finish with final thoughts
so first we're gonna paralyze numpy and
pandas who has used numpy or pandas who
used numpy ER pandas in the last three
days okay so there's there some
familiarity I'm glad to see that so
they're about two sub modules of desk so
this is desk array which is a multi
dimensional array composed of many small
non Pyrates let's say I'm looking at
climate data I have the temperature of
the earth every you know square mile for
the entire earth it might be a very
large array I'm gonna be able to put
that array inside of one computer it's
not going to block that array up into
many different pieces and put them on
different computers or on my hard drive
and task array is going to logically
coordinate all of those no Pyrates so -
grey is a coordinate is a collection of
any numpy arrays and Oscar is going to
when you type in you know - grey dots um
it's going to figure out how to compute
that some by doing all this coordination
on the top I raise similarly
others products like to ask data frame
which is a logical connection of many
pandas data frames so for example we
might have you know a lot of maybe have
a time
serious and the data for every month is
quite large that can fit maybe on one
computer we need to use many computers
or our entire data set we need to use
our hard drive that is for many many
many months so what - matter whom dasker
I provide is they provide an interface
that is very very similar to numpy and
pandas but it operates in parallel
either on your laptop with many cores
scaling out on your disk or across the
cluster using many computers
additionally because they're using numpy
and pandas under the hood a lot of
things work very easily with them so
it's a very sort of lightweight change
to your codebase so I'm just switched a
couple of examples
hopefully the wireless is working a bit
better today than yesterday so I have
here a cluster running on on Google
Google compute engine and this cluster
has 32 machines each machine has two
cores I went to create an array let's
make that a little bit larger I mean
create array which is a thousand by
thousand or 10,000 by 10,000 but
composed of numpy arrays or two thousand
five thousand so sort of a 10 by 10 grid
of numpy raise each element of that
great as an umpire array a thousand by
thousand and now when I when I compute
that on my own my cluster gas is gone
ahead and is computed all those numpy
race for me we've seen that here on the
left so we're gonna see these plots a
fair amount throughout the talk so I
have 64 cores and they're on this this
axis here so every line here is what a
core has been doing over time and we see
that you know this core called the numpy
random function a couple of times and
created a couple numpy race so on my 32
machines I have a hundred numpy raised
in memory on them spread around them I'd
do something let's say I'll compute you
know the sum of this array it's a really
simple really simple computation you
know Dowson co has to be the sum of all
the intermediate arrays is then you're
gonna transfer some of the intermediate
values to other machines
that's the read read as data transfer
and it's gonna compute you know some
final result and it gives us back a nice
answer you know in in a sort of fast
time nothing more complex try to scroll
this down to people in the back do
something more complex you know it's if
you sort of are familiar with numpy this
this syntax should look familiar to you
and so we can we can do a lot of these
note by computations what look like
numpy computations but are actually
spread across a cluster so this is sort
of for the highest level use of tasks
it looks like numpy but actually runs on
you know terabytes of data if you wish
certainly will consider you know a pen
is data frame example so here i have a
bunch of csv files on Google storage
it's gonna go on s3 you're on Amazon or
as you like and there's the New York
City Taxi Cab data set it's around 20
gigs on disk around 60 gigs in RAM so
it's too big for my laptop but I need a
little bit of it with pandas and this
shows all of the cab rides in the city
of New York the year 2015 so how many
passengers are in the cab when did they
take off when do they when they just
stop as opposed you know a breakdown of
the fare so it's a large time series
data set maybe like you sort of seen
before it sort of convenient because
it's it's easy to understand and it's
it's infinitely large so can't read it
all on one machine but I can I can't
read it on many machines so here about
and use the the pandas read CSV function
I've used the task data frame read CSV
function where's the same API but breaks
down my computation takes those 12 CSV
files and breaks it into hundreds of
blocks of bytes then calls the pandas
read CSV function all those box of bytes
and so you're seeing here on the left is
all the machines in my cluster are busy
creating pandas data frames in there in
the local memory the audre be get back
isn't a panda's data frame at the desk
data frame which looks and feels the
same as we operate on that data frame
task will do the coordination to make
sure all the pandas data frames are
affected accordingly so again a sorta
like a very good secretary so I'm
actually doing any computation it's it's
coordinating many pandas data frames to
do the right computations so you know
this because it's all built on pandas it
looks familiar did all the d-type
sniffing we like from pandas wheat CSV
and it you know it it runs the same way
it sort of renders the same way I should
feel relatively familiar and this is
really important if you want to get all
of the people in the Python ecosystem to
use it they tend to like pandas it turns
out so you know a simple computation we
might repeat the length of that data
frame
and that's you know that's done really
literally simply we give you the lengths
all the intermediate values so if you
sort of zoom in over here
we compute the length of this particular
panda's data frame and it took you a few
microseconds and we did that a few
hundred times and we communicated some
data over one machine and we computed
call me to the sum this is very simple
computation but we could use a more
complex here we'll see how well New
Yorkers tip so we're gonna remove some
bad rows there were like some free rides
in New York City it turns out we're
going to create a new column or do the
tip fraction so you know the division of
the tip versus the fare was it a 10% tip
or 20% tip we're in a group by the hour
of the day and by the day of the week I
were to see the average of the
diffraction so how well the New Yorkers
tip grouped by hour of day and what this
does this produces you know thousands of
Python functions that then ran on our
cluster and it gives us a nice result so
you know task data frame turned this
pandas like computation into thousands
of Python function that had to run all
of our pennants out of frames and then
the desk task scheduler ran all those
functions for us in about like three or
four seconds look about the nice results
that New Yorkers tip relatively
generously as a recent migrants in New
York I feel not proud of this it's about
22 percent average without with a
startling spike at 4:00 a.m. it's 38
percent on average tips at 4 a.m.
understand this is last call at the bars
but so you know we did some data science
on a largely community data set with
api's that we already knew on a cluster
that was you know relatively easy to set
up so this is sort of like the first big
hurrah of desk - gives you a paralyzed
or distributed numpy array or
pentostatin frame and for some people
that is that is that is useful however
it turns out so we thought this was
useful too and then we'd like took it to
academic groups and to companies and he
said hey isn't suitable I said yeah but
our problems are just little more
complex but not all problems are a big
data frame or a big array or a big list
instead people often had it look like
this this were just normal Python code
with four loops this is very clearly
parallelizable
but it's not clearly a data frame an
array computation and so then our first
approach was to sort of try to force
this into being a data frame competition
maybe a sort of two data frames and
we're joining them and doing sort of the
filter that was an awkward process so
instead we developed other libraries our
other modules of tasks that can work on
more generic code so the system
underneath tasks array or data frame is
doing all the coordination the secretary
part that is actually fairly flexible
and as long as we can expose that with
these more flexible more fine-grained
api's we can paralyze far more clever
and far more custom code so this
actually ends up being far more used in
practice than the big data frames a big
arrays at least by sort of larger
institutions so let's see an example I'm
actually going to switch not to the
cluster but it's to my local machine
this runs on a cluster just fine but I
want to show you that it is easy to use
tasks on your laptop my goal at the end
of the this talk is real to use tasks on
their laptop and sort of start playing
with it so I'm going to instead of
connecting out to a particular cluster
I'm just connect so normally when we
create a client we'll talk about sin a
little bit we put in some some address
of sort of the head node of our
scheduler in this case we're not gonna
add anything that's a signal to das that
we should it should create something
locally so das is going to create a sort
of little task cluster on our laptop
every I'm sorry I don't let this
notebook at all I want the other one
so what I'm just saying so I have some
functions here that simulate work so
they're gonna do a small but it worked
we're gonna sleep for a while this is
maybe some code you're writing on your
own
ink as a number as one to a number
decrement removes one from a number and
add adds two numbers together
I can call these locally
I can call these locally and they take
you know a couple of seconds probably
sort of randomly or I can import desk
and I can annotate those functions to be
delayed that's what I mean here is that
we're not gonna when we call that
function now we're not gonna actually
execute the code so we're gonna put that
function and its arguments inside of a
task graph that we can execute later so
now when I when I run that code same
code from before it computes immediately
but hasn't really done any work yet
instead it produced this object Z which
holds on to a recipe of how it should be
computed whenever we want it to be done
and so when I call compute on this
object Z it's now gonna run in parallel
just on my laptop not on the cluster not
any set up I haven't said anything up
here I imported tasks
I called compute by default this ran in
a thread pool like we run a process pool
or other things as well but this is an
interface that we can use to build up
parallel computations in a sort of more
pythonic way now I'm gonna set up I'm
gonna start up a cluster I got on my
local laptop and when I think this gives
me this it gives me this nice dashboard
page this is a bouquet dashboard okay
about patient people interested and
what's been Sara bird there your thank
you sir and then we can run this on this
dashboard we can see we can see every
one so you know we we call decrement on
one of our threads we called increment
on another thread we then this little
red bit is communication between two
different threads processes and we
called add on to the results so tasse
again is is figuring out when to call
things moving data around doing all the
parts peril computing you don't want to
think about while still letting you
giving you the freedom to do the things
that you do so this is sort of neat it
becomes a lot better when you have four
loops and you can make sort of arbitrary
complex things so here I've got you know
doing lots more computations and we're
filling up these processes and they're
doing lots of work I'm noticing from
these progress bars is going to take a
while so I'm gonna start up some more
workers pointing it to this scheduler
this is you know making ten processes
each will have four four four threads
running those we're taking up to the
scheduler and now we're seeing that
tasks are responding to that is
paralyzing nicely I could remove them -
Gazala is resilient it can scale I can
do all the nice things you want to
cluster I could do so now that we have
this ability let's do something a little
more complex let's say have all these
numbers across all these from processes
on my laptop I'm going to add them
together one way to do this is just to
call summon all of them and they would
know I have to migrate to one machine
and add them together in one machine but
it was using me more fancy let's add
them up pair by pair like the first to
add those together take a second to add
those together and so here's some Python
code that does that and this Python code
isn't you know task code
it's just Python code if you look at
this for about a minute I think you'll
probably understand what it's doing has
a list of values and it goes through
that list and it adds neighboring pairs
in that list together and it keeps doing
that while while the list is is longer
than one and we actually visualize that
result miss expand a little bit for a
moment maybe you can see that but the
graph that we get it's actually is
showing us the computation that we want
to compute and you can see that you know
it is indeed you know producing the kind
of competition if we wanted to do
actually kind of looks like this image
up here and nothing has actually run yet
we have just created a recipe for how to
do these things if we go ahead and we we
asked asked to compute that result for
us it's gonna use our cluster use all
those processes to do that
you see it's communicating between press
office processes but necessary at the
beginning there's lots of work to do
because right sort of the base of this
tree and towards the end is we have sort
of fewer and fewer ads to do there's
only a few so
if you like come to the continuum booth
I'll give you this this demo and this
demo is demonstrating two things one big
data frames but two the ability to write
your own Python code and paralyze it and
it's being a lot more attractive or a
lot more pragmatic so let's go back to
slides okay so let's let's dive a little
bit deeper into what desk was doing
let's explain tasks graphs as explained
explained tasks scheduling a little bit
so to ask does two things for you one it
produces tasks graphs so you write some
data frame code and it figures out a
recipe for how to actually execute that
code to given such a graph if and a bug
cluster or laptop it figures out how to
execute that graph in parallel those are
two very different Rob ylim generally
numpy pandas kind of people like the
first kind of problem and sort of
tornado async concurrency people like
the second kind of problem but there's
sort of two things happening once we're
talked about the first part than the
second part so tasks graphs is this font
visible in the back yeah
no someone's saying this does this mean
yes it is visible or make it bigger it's
good excellent
oh no I'm seeing some Bad's okay let's
see what we can do reveal is not there
we go a little bit better hopefully okay
I think it's play about as good as I can
go so I make we're very simple array
computation and we're gonna see how dice
is producing that raikage into a task
graph so I'm creating with numpy I might
create an array of 15 ones with a numpy
ones function it produces a result like
that let's say that I only have like 20
bytes on my computer I need to split
this up between many computers then I
might use the task array ones function
so we're still creating rate of 15 ones
but we're chopping up into three arrays
each arrays of chunk size 5 so we're
calling the numpy ones function 3 times
and producing 3 and um Pyrates now if I
were to call something like some on that
array task we'll know I've got to call
some of the numpy ones function arrays
all those sums together and compute the
sum on all of them sort of a very simple
sort of MapReduce II kind of
so we're seeing no pie
no pie light code produce task graphs
now this one's more interesting if we go
to two dimensions stream creating a 15
by 15 array that's composed of numpy
rays of size 5 by 5 so I've got 9 blocks
if I sum along one axis I get sort of
the same summing behavior this is again
something you could do with not produced
with sparkles a sort of an easy
competition to do numpy array
computations actually become
significantly more complex pretty
quickly
sure taking that array and adding to it
to its transpose and so you know numpy
people will be interested to see that
there's you know the odd diagonal nodes
only talk to themselves the off diagonal
they sort of talk to their their
transpose neighbor and this this
difference is actually where I'll get to
this in a bit but this is where old
systems like or more databases systems
like spark or storm I started to fall
down when they sort of lose for losing
this these symmetries to ask them to
being very good when you break down
symmetries
it handles generic cases so no more
complex sorta matrix multiply let's
subtract off the mean let's take the
standard deviation and we can keep going
there's actually a very very simple
thing to do if you look at say the
climate scientists they do things that
are ten hundred times more complex in
this they produce graphs with millions
of nodes so we've made this graph and
now let's go ahead and compute it
so again whoops
that's not copy at all
nope okay import desk da
so this this object Y is still lazy
we've actually done any work yet and
when I call why not compute that's time
that the answer is zero at a constant
array did a bunch of stuff to it extent
deviation it's not surprising that the
answer is zero what's interesting is
that it returned the answer in about 47
milliseconds and in that time for
threads cuz I have four cores went
through every circle in this task graph
ran that function hold on to that result
passed to other functions around those
functions deleted the rules that didn't
have to do and proceeded to through that
graph and so that's roughly what the
task scheduler does it has to execute
this graph and give us a number back I
deal in a short amount of time you know
we're sort of supporting interactive
people who are sitting at the computer
typing at it waiting for result backs
you want to be very fast so
great so systems like das Guerrero das
that a-frame produce tasks graphs like
this one is actually from a secular
computation and then a scheduler
executes that graph in parallel so
here's a trace of a single machine
scheduler walking through that graph
with four four threads and is really
sort of switch from the numpy people to
the sort of more you know tornado
concurrency networking people how do we
run these graphs efficiently so before
we talk about how - does that I'm gonna
have sort of a brief summary of other
options inside the sort of Python space
sort of to motivate why we had to sort
of build our own thing
so we're talking about a few few systems
simple systems like multi processing
organ current futures big data
collections like spark or storm or flink
or databases or tensorflow
and then task scheduler is like the weed
your or airflow so let's consider map
who has used multi-processing before
lots of people
yeah almost everybody I'm gonna claim
who has not used multi-processing before
sorry to pull you out okay five brave
people and maybe some less pretty people
that's okay
so I take a function I have a bunch of
data and apply the functional all that
data multi-processing is a very easy way
to do this in parallel uh thing about
the pros and cons multi-processing it's
very easy to install it's in the same to
library and use it's very API is very
very simple it's also very lightweight
dependency cuz in the standard library
libraries don't mind depending on it
remember that's one of our goals we want
to build a library that many libraries
in the ecosystem you know all of these
and all the thousands more are actually
willing to depend upon they're so
willing to put it in the requirements
that txt file some cons there's some
cost to moving data across a process
that is unfortunate and it's also not
able to handle a little more complex
computations so you know all those
graphs that we had map can't easily do
those so let's go to something more
complex let's look at you know big data
collections things like spark or a
database so these systems are a big step
up right they give you a fixed API
things like map and filter and group by
and join
I merely use all of those operations not
just map and they will handle the
parallelism for you as this this allows
you to paralyze a much broader set of
applications I'm particularly many of
these were implemented for sort of ETL
or sort of data extraction and cleaning
or data based combo computations or some
sort of lightweight machine learning and
so using these systems you can do all of
those things so you're given a broader
set of API to work with like the sequel
language it scales nicely and it's
widely trusted by enterprise so I'm
gonna sort of take off my open source
community hat and put on my like
for-profit evil hat I also like you know
tasks open source and free and
everything but it's nice to pay
developers to do this and the more
people we get using tasks and and we can
like convince people to help us pay
developers to work on it to make it
better so I actually care about this a
little bit I'm a little bit evil and I
apologize for that
so some cons about these systems they're
somewhat heavyweight so it'll be it'll
be sort of hard to convince libraries to
depend on SPARC there's sort of this big
sort of transfer of you sort of had to
step into the SPARC world for a bit do
some computations in this step out it is
it is unpleasant for a few reasons it's
focused on the JVM Python sort of always
a second class citizen this is true with
actually most of the parallel computing
libraries out there storm flink they
were often they were built out of the
JVM stack and were sort of piggybacking
on their success also this is we're not
able to handle very complex computations
now what do I mean by that I mean the
graphs we saw before right these sorts
of very nice graphs without much
symmetry right sort of arbitrary dynamic
time arbitrary tasks graphs where every
every circle in here is one python
function to run on one piece of data so
they're not able to handle these systems
likes market out of aces tend be good at
mapping a function across many things
these are all tall shuffle
communications reducing things but
always sort of in lockstep it's not as
sort of fine-grained or granular as we
need to do some these more complex
computations they're gonna we need to
handle in order to support all of these
libraries however there are some
libraries that do handle more messy
computations these systems actually
commonly use this or the dad
Engineering space libraries like airflow
or Luigi or celery who in the room has
used one of those libraries okay that's
that's awesome this is that that is not
the case in like the SyFy our PI data
talks are conferences so these libraries
are able to handle much more complex
much more arbitrary tasks graphs they
sort of fit the model that we need
they're also Python native often so you
know these are celebrities that we can
hack on that our communities are willing
to depend upon they're nicer in that
sense how do those are some comms so
there's no inner worker storage or or
communication latency is relatively high
so like 100 milliseconds between tasks
is an OK thing for these libraries it's
not ok for us we work on sort of the
millisecond 200 micro second level
they're never more never we're not
optimized for competition that being
said to optimize for lots of things that
task doesn't do I don't mean to sort of
besmirched either of these libraries
both spark and Luigi and airflow do lots
of things that task won't do but task is
sort of good at this sort of mixing
between them so we want to test
scheduler like airflow and Luigi but
that is more computationally focused
like something like spark or flame core
tensorflow and that ends up so our sort
of attempt to build that system is
called task which so we've seen that
that a-frame and Oscar ray and those are
things built with tasks that is not task
task is a dynamic task scheduler at its
core so it gets a graph of tasks such as
you would give to Luigi in a complex
case and it executes stem on parallel
hardware however might be your laptop or
might be a cluster we're relatively fast
in various ways we're not as fast as MPI
but we're sort of faster than most other
things it's also lightweight we'll see
in a bit and it's well supported so a
little bit about task schedulers there
are two main tasks others within the
task sort of world there was one that
was built many years ago for single
machines this typically runs on top of a
thread pool or a motor passing pool and
this runs with with a very low overhead
it actually only depends so it's about
you know 50 microseconds per task
overhead it's it's fast it can handle
arbitrary graphs it's relatively concise
if I'd heard of a thousand lines of code
it actually depends on nothing except
for the standard library so very even
very conservative libraries are willing
to depend on this code is very it's been
stable that's been changed much and
people don't want to change its code for
a long time this is a stable code it is
lightweight it is easy to use and it's
used by many groups on many different
applications the diversity of
applications on these schedulers is very
broad so as likely that if you have
different application it will also
likely work they're not optimized for
like array computing or for data frame
computations they're optimized for
general computing maybe like a year and
half ago we start a building attributed
cluster scheduler which is more
sophisticated but a little bit more
heavyweight this is a tornado TCP
application it's a less concise it's
fully asynchronous so you can you can
submit graphs to the thing you notice
even though this running you can get
sort of a constant conversation back and
forth
it supports sort of the HDFS space the
Hadoop space and also supports the more
sort of traditional cluster HPC space so
we were seeing things running on cluster
we running this to ship it it should be
scheduler how it is organized
there is a single process running
somewhere under under cluster which is
the scheduler so what my call is like
the master or the head node this this
process is going to coordinate all of
the other processes in your cluster
there are then many workers that will
take instructions from that scheduler so
the schedule might say hey worker 1 I
want you to compute this task the worker
will compute that task and hold on to
the results and inform the schedule that
is finished the workers will communicate
peer to peer to share data around so
they're not communicating through one
central bottleneck and then we as a
client down here from our you know maybe
for notebooks or from some script were
running you know nightly or whatever
we're then gonna submit graphs up to
that scheduler so the workers and the
schedulers are all TCP servers they're
communicating over various interest
protocols and the client is just just a
client up to the scheduler so you can
set this up relatively easily you can
salt on your laptop like I did before
you can run on a cluster it's relatively
lightweight also so the bottom I'm
actually just running it inside of one
process and I can start this up
including your the fancy dashboard and
everything around 43 milliseconds so you
can you can import this you can run it
you can tear it down and it is not a big
thing it is cheap you should not think
of destructive computing as being like a
far away hard thing to do it is like
like this is faster than importing
pandas and reporting pandas takes like
200 milliseconds so you should think of
this as being a cheap thing to do you
can also if you don't like Conda you can
Pitt missed all tasks
everything was pure Python so I'm gonna
go through it is a time I'm not gonna go
through this but this shows a sort of an
example of how the scheduler might work
in this application pool so fast is easy
to use and adopt you actually already
know a lot of the api's that - presents
there's no single task API we largely
steal the api's of other languages of
other projects and you probably already
have the dependencies involved and
installed you probably have tornado
installed you probably have a message
pack installed if you want you know
that's out of frames you need pandas but
you probably have that installed if you
want that anyway so in order to improve
adoption in order to sort of reduce the
amount of like creativity we had to have
tasks largely uses existing pythonic
api's so we support a lot of the numpy
and pandas api's and protocols where
they exist
we support pep 3148 we're just
concurrent futures if you're there that
library task supports that perfectly we
also do async await if you like
concurrent async work it'll seize job
toplin so usually take existing
scikit-learn code and there's a way in
job lib which is second thorns
parallelism library to hijack how it is
parallelism as you can run your
scikit-learn code on a cluster it may
not work very well in all cases but but
we where there is an existing protocol
for parallel computing in python we have
usually implemented that so you probably
already know how to use desk you
probably also so it's also very
lightweight and sort of just you know
using existing libraries let's look
yeah a little bit time so let's look at
some of these AP is
so again we've seen you know if you know
numpy you probably know how to use
dasker ray here's a example at
scikit-learn this came from the did
example or the psychic learned ox and
you know someone so we're making a
pipeline we're gonna do a cross valid
good search across that pipeline just
normal scikit-learn code it takes 8
seconds or nine seconds and now someone
actually is name is Jim Crist Alberta
library task search CV which is a
drop-in replacement for a grid search so
now when you run that it's gonna run
that on our cluster it looks just the
same we're using all the secular and API
so it looks yeah it fits into it's the
same work though as well
it ran faster we also notice like
there's a lot of white space here a lot
of our workers weren't doing anything so
we have a bit more computing power let's
go ahead and increase this a little bit
well increase our parameter search space
if you don't know second learn that's
okay you should just understand I'm
increasing inputs and we'll get more
work and I won't be slow oh so now we
have more work more computing power
where's her to search this space a
little bit better
and again if you sort of notice I can
learn this should have been pretty
familiar to you so now the task work is
just helping other libraries paralyze
themselves using those same interfaces
now supports concurrent futures
interface so here we can you know do the
executor not submit model it's actually
fully asynchronous so like as work comes
in you can submit more work in a fully
real-time way which is fun we do things
like async await so here's you know
fully asynchronous thing you know all
the sort of Python code that you've seen
none of except for like the import Diane
this notebook you would you would
qualify as task code it was all the kind
of code you've seen before but it was
all paralyzed in a nice way and that's
again sort of one the objectives and
tasks
okay so we had a high level task gives
you parallel api's parallel pandas or L
numpy Perla second learn
a subset of all those not complete it's
also very good at paralyzing existing
custom systems at a low level task is a
task scheduler which means that it runs
Python functions on Python objects on
parallel hardware
well those Python functions are with the
place the objects are is up to you it
can be your own special object your own
special functions dass's I need to know
what it is it can just run it and it
will figure out where and when to run
those functions the machine goes down or
bring it back up all those sorts of nice
parallelism things okay so I want to
take a few minutes about sort of just
general ecosystem comments so I'm
normally a fairly critical or sort of
like pessimistic person I apologize I'm
gonna gush a little bit so I think that
I think is really the the best place to
build this kind of project we built ask
way more easily than you would expect us
to be able to that's because of all the
work that's already happened in the
ecosystem so let's look at the Python
strengths and weaknesses from a sort of
a parallel data analysis point of view
so python has a very strong algorithmic
tradition all the community who is like
writing code with punch cards or in
Fortran has moved over to Python right
and they know how to do those things
very very well they know algorithms very
well they know they have PhDs in math or
something a lot of battle-hardened
Fortran code which ones very efficiently
they use just the best you know sse2
instruction on your cpu we also have
alongside that a very strong networking
currency stack and it's very sort of
very rare to have both of those in the
same language we also are sort of very
standard in teaching so it's lawful
using Python weaknesses I think are not
actually that true a people think that
Python is slow
there's the Gil those are questions
about packaging so a few comments about
this
so the numpy in PI the see sort of
numeric stack on Python is very very
fast
he's running bare metal speeds so here I
have a thousand by thousand array and
I'm doing a matrix multiply
but about a billion multiplies and adds
in that computation and we ran that and
around 70 milliseconds it's about ten
billion operations per second
everything about my laptop I knocked off
only he like has like a 2 or 3 gigahertz
processor so actually doing more ads
than I have cycles in my in my system if
you know the past abuse that doesn't
make I surprise you but we're running up
our medal speeds the Gil people often
concerned about parallelism and the Gil
again if the numeric stack this actually
doesn't matter the Gil is not a problem
if you're mostly running numeric code
with libraries like numpy and pandas the
scikit-learn so the Gil stops to Python
threads from operating at the same time
but or to python Fry's wrapper text even
Python code at the same time but our
threads are just calling out to some see
function and they sort of wait until a
function returns so we're not actually
calling Python code we're calling C or
Fortran code so you can run Python with
many threads and saturate your hardware
very easily if using an umpire pandas or
that sort of stack you should use
threads freely unless you're doing you
know pythonic string manipulation but
use threads just general comment and
alongside that there's also this this is
a totally different community it's at
the same time simultaneously building
out a very strong and very intuitive
concurrency networking stack so here's
some code that you know such as you
might see inside of the side of desk
that does a lot of concurrent things you
might now write that with async await
own tasks we use tornado this code looks
simple it looks easy to understand and
that is actually remarkable now this
would not have been the case ten to ten
years ago
the same time it's also quite quite fast
so this is from a blog post about you
view loop let's see if I can get that
link in there talking about you know
various UV loops various event loops and
that's actually running on Tom tornado
which is way down here but even that is
running at 20,000 TCP requests per
second which is like totally fine for us
so it's this cutie also very focused on
performance not at a numeric repeating
Whitford away but in a concurrency sort
of way so python is was an excellent
numeric computing stack
and an excellent concurrency and
networking stack and that's actually
quite rare it's really a blessing that
we had sort of those two groups of
people in the beginning and they're both
in the same room together and it's
because of that that desk was actually
really easy to build so most of the work
of building tasks was was built you know
five or ten years ago by all of you
so I appreciate appreciate your efforts
and interest of time I'm going to stop
but many people work on tasks it's not
just me
there are various government
organizations and nonprofits that fund
tasks we like to thank them as well yeah
just show engagement in last 48 hours
these projects have mentioned or
committed code using tasks as well on
github and search for desk so it's sort
up</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>