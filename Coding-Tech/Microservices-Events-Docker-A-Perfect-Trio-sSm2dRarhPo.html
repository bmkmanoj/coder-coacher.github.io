<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Microservices + Events + Docker = A Perfect Trio | Coder Coacher - Coaching Coders</title><meta content="Microservices + Events + Docker = A Perfect Trio - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Microservices + Events + Docker = A Perfect Trio</b></h2><h5 class="post__date">2018-04-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sSm2dRarhPo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so talks comprised of three parts
first I'm going to talk about a big sort
of choice you have to make at the
architectural level are you going to use
the monolithic architecture or
microservice architecture because
there's trade-offs there then I'm going
to talk about how an event-driven
approach solves some key challenges that
you have to deal with when you're
building a microservice architecture and
then I'm going to finish up by talking
about how docker fits in and how it can
simplify both deployment and development
so let's get started so let's imagine
that you're building a complex
application which probably almost
everyone in this room is right we sort
of we don't spend our time building
trivial applications when the example I
like to use is an online store and I'll
show bits of the design in a moment but
I think it's safe they are safe to say
that in order to successfully develop
software there are three things that you
have to get right the architecture the
organizational structure and the
processes that you have to use there's a
strong consensus around the idea as
rather than having very very large
development teams you know 20 30 40 or
even more people really what you want to
do is break up your engineering
organization into a team of teams each
team is small comprised of a say of a
say over most 12 people ideally just 6
to 8 right those team should be
autonomous able to work fairly
independently and then there's also a
lot of consensus to say that we should
be developing using agile processes
right ideally using continuous delivery
or better yet continuous deployment
so that's organization small autonomous
agile teams who are delivering
continuously but what about architecture
so at a high level there's two choices
that you have to make am I going to
build a monolithic application or a
micro service based application so let's
look at the monolithic approach so
here's my online store and as you can
see it's both layered and
mochila so there's a presentation layer
that's why exposing a REST API in but
also generating HTML for for the browser
there's various modules that implement
different pieces of business
functionality like the catalog service
the reviews the the model the review
module the orders module and so on so
it's got a nice layered modular
architecture but when we use the
monolithic architectural approach we
package everything up as the application
right just this big monolithic
application so in the Java world that
would be an ear file or war file or a
jar file and there's analogous to
packaging mechanisms in other languages
as well and the application uses the
database so in fact most applications I
built over the years have had this
architecture and there's a good reason
for that at this architecture for
applications with this architecture in
the beginning are pretty simple to
develop test deploy and scale all our
tooling is around building the
application testing is easy and so on
the trouble that you have is that
successful applications have a habit of
growing we don't just build an
application and stop development we
continue to develop the application so
the team of developers is just making
the application a little bigger every
day
committing code right and you do that
for long enough in some cases ten years
other day I was talking to a client
where they started selling a debt they
had a very successful demo they started
selling it and ten years later they had
an application that was a million and a
half lines of code
the problem you have is once you have a
large complex application any notion of
being agile it just goes away is
everything about developing and
deploying that application becomes
extremely painful also from an
organizational point of view because
everybody's contributing to the same
codebase there's really no autonomy
between the teams the process of
delivering or releasing an application
involves like complex merging
days of pain and and so on you end up in
this situation that I call monolithic
hell and I'm sure some of you are expert
at everyday because because it's quite
common I feel like there's a certain
inevitability that given enough time
your key but your key application will
become painful to work with so what's
the solution well the solution of course
is to use the micro service architecture
and what got me interested in this
approach before the name was sort of
before actually before the name existed
was I read this book the art of
scalability which to me is a must-read
book written by some architects who
worked at eBay and in the book they have
this three dimensional model of
scalability another use of three and so
there's three different axes along which
you can scale your application so
there's the x-axis scaling which is
where you run multiple copies of your
application behind a load balancer
there's that's running multiple
monoliths there's that axis scaling
where you're running multiple copies but
instead of it running behind a simple
load balancer is behind a router that
inspects the request and uses some
attribute of it to route that request to
a particular server so in the database
world that would be sharding where you
use the primary key of a row to decide
which server that rows should reside on
both x and z axis scaling all about
monoliths but what's really interesting
is y-axis scaling or in other words
functional decomposition so that's where
you break up your application you break
up your monolith into a set of smaller
applications or services and that that
to me is my definition of might of the
micro service architecture is is
functional decomposition so when we
apply that to the online store we end up
with an architecture that looks like
this
so what were previously modules within
our application now become standalone
independently deployable and scalable
services each one of which has its own
database that's a key attribute of the
micro service architecture which
actually creates which has a lot of
benefits but creates problems that I'm
going to talk about in a little while
and then sitting in front of those
services is the API gateway which acts
as a facade it exposes an API the
clients of those services used clients
such as a web application or us
JavaScript client running in a browser
or a mobile client access fire REST API
so that's the Micra services
architecture in a nutshell and the nice
thing about it is that it enables agile
development and continuous deployment it
also enables the teams to act
autonomously so each team owns one or
more services they're able to develop
that service independently deploy it
independently and scale it independently
so it sort of fits very well with with
sort of modern organizational thinking
and also modern thinking around
processes as well so it's really good
and there's other benefits as well it
lets you much more easily adopt new
technologies because when you're writing
a new new microservice you can pick a
new technology at that point you know
you're no longer stuck with using the
same technology stack throughout your
application which will become
increasingly obsolete over time so there
are some drawbacks however right and by
far the biggest drawback of the micro
service architecture is complexity yes
things are sort of complex at this point
so number one you're developing a
distributed system so what that means is
that whereas before when module a called
module B that was just a language level
method call or function call you know
really really simple development you now
have to deal with inter process
communication which on the one hand that
yes there are frameworks for doing
messaging and frameworks for doing our
PC but it's just
complicated more complicated right not
only that you have to deal with this
concept of partial failure which we you
did not have to deal with before so in
other words when you call some other
service that service might not be debt
might not be up or it might be really
really slow so you might not get back a
response ever it might just sort of
permanently hang and you as a developer
have to be mindful of that and have to
program for that scenario also as I
mentioned we split up our databases and
and yet we still have business
transactions that need to maintain data
consistency across those databases and
that becomes really tricky especially
because we can't use the traditional
approach of two-phase commit
distributed transactions are not a
viable option for modern modern
architectures it's also testing is more
complicated as well because you might
want to test a service but that service
could have a whole tree of transitive
dependencies also just deploying and
operating your your application is a lot
more complex whereas before there was
one at one monolith and he just ran n
copies of it now there are tens or
hundreds of micro services and you need
to have so many instances of each one of
them so there's many more moving parts
that have to be deployed and monitored
and managed and so on nice thing is that
you know they're like as we saw in the
keynote today you know there's sort of
docker based orchestration technologies
because that can actually make that a
lot what can take away an awful lot of
the pain it's also challenging to
implement features that span multiple
services a whole bunch of planning has
to be done so that's just another
complication but the nice thing is you
know as I hinted at right like there are
solutions to most of these problems
either sort of process you're sort of I
even do scaling you can use
agile techniques to coordinate across
the Thames docker orchestration
technologies take care of a lot of the
deployment pain and later on I'm going
to talk about how you can use event to
ensure data consistency across different
databases so it's better and unlike with
a monolithic application where you there
there are no solutions to its complexity
and you just have to suffer quietly you
know we've got got viable solutions here
so in general for large complex
applications the benefits of micro
services tend to outweigh the drawbacks
so that that's good but having said that
there are a whole bunch of issues to
address like how do you deploy them how
do they communicate how to clients of
the application communicate with the
services how do you actually break up
your application and how do you do deal
with these distributed data management
problems that result from breaking up
your databases so there's just sort of a
whole bunch of issues and you know I'm
gonna touch on a couple of them namely
using events to manage data consistency
and using docker to actually deploy your
system ok so that's um look at the whole
you know event driven approach so if you
you know we think about well your
question what's the database
architecture in a mic in a micro service
system you know you've got two choices
you could have a single shared database
or you could have a database per service
and I've kind of already hinted at the
fact that you actually have a database
per service but let's look at the
concept of a shared database so in this
ecommerce application you've got the
order service that has the order table
and you have the customer service that
has the customer table and so the order
tables got columns like the order total
the customer table has columns like the
customers credit limit so each service
owns some tables but they also access
one another's
so for instance in order to check that
the customers credit limit will not be
exceeded by a new order the order
service goes and looks at the customer
table likewise the customer service in
order to calculate the available credit
for a customer goes and looks at the
order table and on the one hand that's
kind of nice you know this is actually a
kind of simple programming model right
we're used to programming with acid
transactions that enable us to you know
easily enforce invariants like credit
limits and so on so it's kind of nice
and also operationally it's good there's
just the database to manage so the DBAs
and the ops people are happy but on the
other hand there's tight coupling so for
instance if I'm working on the order
service and I want to change the
database schema I have to go to Talk I
have to go and talk to all of the teams
that are whose services are also
accessing the order table and as you
know once you kind of have to coordinate
with other people other teams inside
your organization
things slow down so the team's lose
their autonomy and what's worse in some
organizations this is common today
people lose track of who's accessing
what table and as a result you can't
change anything right so it's sort of
like you know it's it's a slippery slope
and best avoided so the recommended
approach in a micro service architecture
is for each service to have its own
database or another way of putting it
each service should have its own private
data that is only accessible through
that services API you want to properly
encapsulate the data so the order
service has an order database that has
the order table customer service has the
customer database that has the customer
table and so on and there's this ensures
that you have loose coupling which is
good on the other hand it's a bit more
complicated you might actually have to
be operating multiple databases
especially
if you've adopted a polyglot persistence
architecture and you're using a variety
of no sequel and sequel databases as
well so your ops people have yeah have a
more complex infrastructure to manage
plus you have this problem of well if
I've split things up how do I maintain
data consistency across the to across
multiple databases especially because in
modern applications you cannot use this
two-phase commit because of things like
the cap theorem plus just the
practicalities of using distributed
transactions it tends to be a very bad
choice for modern applications so this
creates problems so you know to use the
example I've kind of hinted out earlier
so in this application let's suppose
that customers have a credit limit and
so when you want to place an order you
have to verify that that order will not
exceed the credit limit
if orders and customers are in the same
database trivial you just begin a
transaction access the data create an
order commit it and you're done and the
acid transaction model handles the
scenario where multiple requests are
simultaneously trying to create orders
you get the serializable aspect of
database transactions but if you split
up orders into one database customers in
another how and you know and you're not
using two-phase commit
how on earth do you do that so the
solution of course is to use an
event-driven architecture and the idea
is that whenever something happens of
significance such as a state change or
in other words when one service updates
data it publishes an event another
service can consume that event and react
accordingly update its own data and by
by basically chaining together a series
of transactions where one transaction
emits an event that triggers another
transaction you can achieve eventual
consistency in your system so in the
order management system when a request
comes into place and order the order
service create
it's an order in a pending state it
emits an event it to indicate that it's
done that that event is consumed by the
customer service which goes and performs
the credit check and actually reserves
credit it keeps track of the fact that
order XYZ has allocated so much of the
available credit and then it it emits an
event indicating the outcome of the
credit check so if the creative the
credit reservation was successful it
would emit a credit reserved event and
if it was unsuccessful
it emits a credit check failed event
those events get consumed by the order
service which can then change the state
of the order accordingly so instead of
this happening in one a local asset
transaction what we have is a series of
well three local acid transactions and
eventually the state is becomes
consistent at that point so that that's
an approach that I first read about eBay
using to maintain consistency across
their partitioned sequel databases some
number of years ago but there's a little
challenge in order for this to work
reliably you actually have to atomically
update the database like insert and
order or change the state of the order
or a customer and omit an event so
there's two things that have to be done
atomically and if you think about it the
traditional way of doing that in an
enterprise application is with a
distributed transaction right you begin
a distributed transaction you update the
database you update the message broker
and then you commit the transaction and
that will guarantee that those two
things happen atomically but of course
you know as I discussed you cannot use
distributed transactions in modern
applications so you're sort of like well
how am I going to reliably publish
events it turns out there's a whole
variety of strategies that you can use
ranging from the application explicit
you were using the database table
database table as an it as a message
queue so that's the bottom one you can
actually do it with database triggers
you can tail the database transaction
log and you can all and there's also a
technique called event sourcing that
I've been using successfully for a while
now and I'm gonna talk about there's
actually a pate an article that I wrote
that goes into this in more detail I'm
just gonna focus on event sourcing which
is really really interesting and the
whole idea with event sourcing is it's
basically an event centric approach to
persistence and the IEEE and of course
in other words so for instance with the
way you'd actually store an order is as
a sequence of state changing events so
when you create an order you insert an
event into an event table saying that
the order was created you would then
when say the order was approved like the
credit check was succeeded and you'd
insert an order approved event and then
when the order was shipped you didn't
cert an order shipped event and that's
the that's the official sort of system
of record for that order so there's no
others that you don't have a row in an
order table that's got the current state
all you're storing is is the current is
a sequence of event and the reason that
helps in this scenario is rather than
you having to update the database and
publish an event you simply insert an
event into the event table which is one
atomic action so it solves the atomicity
problem and then if anyone is at least
conceptually if anyone is interested in
the in subscribing to the event they can
just pull the event table in practice
you use a much more elaborate scheme
than that but that that's sort of the
big idea and then if you need to
reconstruct the current state of the
order you load its event
and you you can think of the this
process of replaying them to reconstruct
the current state in a functional sense
you're actually doing a fold over the
stream of events to recompute the
current state
so that's event sourcing it's an event
centric approach to persisting your
domain objects that at the same time has
this wonderful benefit of you are
generating a stream of events out of
your system which among other things
solves the data consistency problems in
a micro service architecture and also in
a in scenarios where you're using no
sequel databases as well with limited
transactional capabilities it also means
that you've got this wonderfully
reliable event publishing mechanism so
when when anything happens within one of
you within your system an event is
published you can feed that into a
predictive analytics engine you can you
can use it to send out user
notifications via email or via SMS so
it's like whenever something happens an
event is generated which can be then
consumed somewhere else and get
something done
it also eliminates the object-relational
impedance mismatch problem because
you're no longer storing domain objects
we no longer have the problem of mapping
them to a relational database schema
because all we're storing in the event
that have a simplest structure also
because each state change is represented
by an event we have a hundred percent
reliable audit log we can save the
identity of the user that caused the
change in the event and if there was no
change no event the you know where
there's we're just sort of guaranteed
that we've got this reliable audit log
which is really really good as opposed
to having to like bolt on auditing as an
afterthought also because we were saving
the entire
history of each domain object in this
system we can actually go back in time
and ask what the state of an object was
you know historically so we can perform
temporal queries which is extremely
powerful you know there's some domains
that are heavily regulated so you need
to know who did what when or what was
the state of the world when a particular
trade happened and so this system
actually preserves that and they're not
only that we've actually from a sort of
system point of view we have a complete
history of everything that has happened
in the system you know at least
conceptually since the beginning of time
so when we today we implement a new
feature we can actually feed past events
all of the past events into that new the
the module that implements that new
feature and effectively as if behaves as
if we had implemented that feature from
day one which sounds really cool yeah
sounds cool I haven't actually tried it
but I think but you could imagine doing
that right but of course you know like
everything there are drawbacks right so
it requires an application rewrite yeah
it's a different way of structuring our
core business logic though the nice
thing is you know if you're migrating to
a micro service architecture you're
having to do a code rewrite anyway so it
could kind of fit in with that effort
it's also somewhat slightly weird and
unfamiliar way of structuring your
business logic so there's a learning
curve the nice thing is is the event
sourcing has sort of been around for
quite some time in this there's this you
know it's a subset of the domain driven
design community and so there is this
body of knowledge and articles and
resources that can help get you started
another interesting asked for a least in
theory is that these events once they've
been published never go away and that's
great because you've got this durable
audit audit trail for your system
the downside of course
if you have a badly designed event you
might have to live with it forever if
ever you do a replay of all the past
events you have to deal with some badly
designed event though usually events
have a pretty simple structure you know
they're very much rooted in the problem
domain so they tend not to be very
complex but there are issues around
event schema evolution also you have to
handle duplicate event so you've got to
have either idempotent event handlers or
you have to do duplicate detection but
there are usually very simple strategies
for doing that also querying the event
store can be challenging right like if
you want to imagine you're storing
account debits and credit event and you
want to write a query that finds all
accounts that have a particular balance
you your sequel query would actually
have to like kind of combine the events
you'd end up having to write nested
sequel queries which would be kind of
complex and possibly inefficient so
there's a challenge there so there's
some you know like everything there are
definite trade-offs and I but my sense
is you know event sourcing has been
around for a while and you know to me it
was always a curiosity but as soon as I
started building micro services it
became a great way of solving the data
consistency problems and what's nice is
that there is a strategy for dealing
with queries so let's imagine that you
want to find you know in your system
recent valuable customers so customers
who have recently placed high value
orders now if everything was in the one
database that would be a trivial sequel
query right just to join between
customers and orders with the
appropriate where clause now in a
microservice architecture that's no
longer easy customers and orders could
be in different databases so we can't do
that
and then what's more if we're using
event sourcing writing a query involving
the car
State is no longer trivial as as I
mentioned earlier so this is a problem
the nice thing is there's an
architectural approach known as command
query responsibility segregation where
you split your system in two you have
the command side that's implementing you
that has the domain objects and it
handles the commands the HTTP puts posts
and deletes you have a query side that
handles the HTTP gaps by querying one or
more materialized views of your data
which could be a could be Mongo could be
Redis could be neo4j could even be
sequel or elasticsearch depending on the
types of queries that you need to
execute in order to perform those get
request and then what's really cool is
that the two sides are kept synchronized
the views that's kept synchronized by
consuming the events that are coming out
of the command side and so this is a way
of basically supporting a diverse range
of of queries in a high-performance
scalable way while at the same time
using event sourcing so if any of you
are using say elasticsearch in
conjunction with my sequel in a sense
this approach is sort of a
generalization of that concept you know
there's definitely complexities there
but there are also also some pretty big
benefits all right
so kind of talked about some
architectural ideas right
you know micro services are the high
level using events and using CQRS
to deal with data management problems
and I want to finish up you know since
this is docker con I have to talk about
docker right
so but what's cool is you know I've
actually been using docker for couple
years now
just blues and actually this is the
first time I've been to Dhaka Kahn and I
guess this docker thing is catching on
right just blown away by the size and
the energy of this event so okay so you
know so you've decided you built your
system using the micro service
architecture then it's like well how do
we deploy it and we have you know you
now have some really unique challenges
right instead of a single monolith we'll
be at a large one you've now got lots
and lots of services so there's a lot of
moving parts that need to be deployed
orchestrated and monitored and so on and
there's a lot of sort of interesting
issues that you have to deal with so
services can be written in a variety of
different languages and frameworks and
maybe even versions of languages and
versions of frameworks so that there's a
deployment challenge there each service
consists of multiple instances for
scalability and availability reasons I'm
building and deploying a service should
be fast right we want to practice
continuous deployment so it shouldn't
take a long time to build a service and
have it up and running in production we
want to be able to deploy and scale each
service independently the services
should be isolated from one another so
we don't want a malfunctioning service
to consume all of the CPU all of the
memory of a particular machine and then
also the the process of doing deployment
needs to be reliable we shouldn't stress
about it right we should be able to do
it many times a day without having a
panic attack or having late nights or
anything like that you know which is
sort of the traditional deployment
scenario and not only that it should
also be cost effective right because
well most of us work for businesses that
have to generate profit right so so you
know having cost effect
of production environments is important
so that turns out of course there's a
variety of different deployment options
right so ranging from the traditional
approach of well just getting a few
machines giving them cute names and
running multiple instances of servers or
of services on each one right so I'm
calling that the multiple services per
host pattern so that's kind of the old
school way of doing it and probably err
it's generally not a good idea today and
so the better approach is to use a have
a single service instance on each host
and a host is is either a virtual
machine like an ec2 instance or it's a
container as in a docker container and
each and once actually once you get into
it you know there are various trade-offs
involved with each approach but you know
I think sort of the whole container
based approach is by you know starting
to prove itself as you know an
incredibly effective way of deploying
your applications so and of course you
know docker is is the way to do it this
by the way this was a coger container
ship Benjamin Franklin that visited the
port of Auckland at the end of last year
it's like 1,400 feet long and it has
18,000 containers on it so you know
gotta have the obligatory container ship
photo in the talk so of course the big
idea right is that you take your service
compile it if you need to and then you
package it up as a container image and
then at runtime yeah each instance of
your service is a container that is
probably running on on a virtual machine
or it could be running on bare metal
that's sort of emerging to be a viable
option as well and that's really good
right containers give you give you good
isolation between your services they
give you a good MANET good manageability
right also it encapsulates the
implementation technology once you've
packaged up your application as a
container image it doesn't matter that
it's a ruby
or a node or a Java or scholar
application it just becomes a container
image that can be started and stopped
also you know container mechanism is a
super lightweight os-level
virtualization technique so you get
unlike virtual machines you get very
very efficient resource utilization you
can pack multiple containers on a
virtual machine also it's incredibly
fast as well so like on my laptop it
takes a few seconds to build an image a
little long you know say 20 seconds to
push it into a registry 20 seconds to
pull it down onto a production server
and then when you start a container it
just starts up immediately so what's
that 45 seconds from the build finishing
to a container in the production
environment which is really really cool
so that you know - that that's really
good and then not only that I mean I
have found docker to be invaluable
during development as well specifically
with what docker compose gives you right
and this came up in the keynote we're
getting a developer desktop set up with
the right versions of all of the
infrastructure services that you need
has historically been a pain right
because especially in modern
applications where we're gonna have a
whole host of services now that just
gets represented by a docker compose
file and this one says well I need
RabbitMQ 3 5 3 and MongoDB 3.0 0.4 and I
just go docker compose up and my there's
my sort of infrastructure services
running which is incredibly useful not
only that when it's time to do sort of
some notion of end-to-end tests or
integration tests between the services
it's really really convenient to just
have a docker compose file that deploys
those services so I just go docker
compose up and it just in launches
instances of all of my services and so
that that you know it's that's a regular
part of my sort of day as a developer is
making use of docker compose for that
and that you know it's pretty basic
stuff
but it's made my life as a developer a
lot easier and then not only that so my
you know for every service I have a
Jenkins based deployment pipeline that
end that basically emits a docker image
and what's one of it one of the
interesting things that one of the
intermediate steps is after actually
building the docker image it uses the
docker daemons REST API to actually
launch the container and it just runs a
smoke test on it and that that's kind of
nice just to have a sanity check that I
haven't built a docker container that
that's completely broken and then you
know then the other thing that's sort of
one final point is that docker sorry
Jenkins itself is actually deployed as a
docker container along without a factory
that's sort of a Java repository server
as docker containers on an ec2 instance
and then any data that I want to persist
is actually on an EBS volume and then
volume mapped into the container and so
I've had you know been running this kind
of setup for probably a year year and a
half now if not longer and it's been
really really good so so yeah docker
micro services just works really really
well together and so so that's my talk
hope that you found it useful so it's
kind of in summary right like you use
micro services to accelerate development
you want to use an event-driven
architecture to maintain data
consistency across your micro services
and you want to use docker to simplify
not only the development of your
applications but also the deployment so
that's my talk hope you found it useful
and please you know if you've got
questions you know contact me email
Twitter and also check out learn micro
services dot IO</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>