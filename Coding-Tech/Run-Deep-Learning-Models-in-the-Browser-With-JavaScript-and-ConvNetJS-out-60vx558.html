<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Run Deep Learning Models in the Browser With JavaScript and ConvNetJS | Coder Coacher - Coaching Coders</title><meta content="Run Deep Learning Models in the Browser With JavaScript and ConvNetJS - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Run Deep Learning Models in the Browser With JavaScript and ConvNetJS</b></h2><h5 class="post__date">2018-03-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/out-60vx558" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">first of all I want to tell you why
actually running deep learning modeling
JavaScript makes sense
the second yeah it's it's it's
complicated let's say the second thing
is I want to bring you guys all on the
same page so I will give a very brief
introduction to neural nets and deep
learning it's it's very very simple I
use also one equation so I hope you I
hope that's okay for you um I yeah trust
me it's going to be simple and if
somebody completely has no idea what's
going on just raise your hand and I will
yeah either speed up or slow down third
of course I will show you the transition
you know state of the art of running
deep learning models in the browser
training them and so on and what
actually powers JavaScript to do all
this and then we see more demos yeah so
that's actually the main reason why we
want to do this why run JavaScript why
run deep learning models in JavaScript
is it just to satisfy you or you you
know are there any practical
applications and you know it started
like this you just as a proof of concept
to see what is actually possible but you
know there is much much more so first of
all I'm super excited about the browser
right what happened in the last in the
past years where the browser is today
it's it's such an amazing platform and
it makes you know every technology
accessible to anyone on almost any
device once it's runs in the browser so
that's really cool
once it's JavaScript you know you can
also run it server-side which yeah it's
also nice the most thing I liked about
actually that my whole journey in in
deep learning was once I was in
JavaScript I could just use the chrome
debugger right and put a breakpoint in
the backward pass or in the forward pass
and you know see the activations and see
the gradients so it was an amazing tool
or it is an amazing tool for for
debugging and of course I love
interactive visualizations and you know
once you're in the browser you can just
you know easily visualize everything so
that's a bit more for you know education
and and training and learning um but
there might
to be some practical applications these
are actually I have not seen any company
implemented one of those yet but they
are possible in theory so it could be
done so first you have access to a lot
of data right you're on the client you
run in the browser so actually you could
just you know use the cursor movement of
the browser of the of the laptop or use
the typing speed of the keyboard to
train them to you know evaluate the
model and find out the gender or the age
of the person that uses a laptop
something like this like you have access
to a lot of data you could do you know
massive parallelization you know imagine
that you could run a backward pass on
any device that uses your website like
imagine you're at the scale of Facebook
or Google and you embed this in your
website somehow I mean that's really
massive and last is also you can do
really cool things for applications that
have required privacy for example you
know imagine your protonmail and you
want to build a feature that gives you
smart reply in protonmail in the browser
so you could just apply the model on the
on the text in the browser and it
doesn't have to leave the browser at all
so yeah I think that's wonderful and I'm
very excited and I think we will see
some of this application very soon so
yeah here a few screenshots actually um
so I forgot to say you know everything
is running in the browser like from the
presentation to the demos all the
screenshots that you see are made in the
browser the slides are also available to
you guys so you can run the same things
on your smartphone or on your browser
actually right now so let me go back you
may saw it so here's the link it's bit
dot I just zoom 2017 - DL so if you want
to run any of these demos on your phone
yeah you can do it yeah so let's give
you a quick taste how this looks like
for example you you take an image right
everything is in the browser let's make
it a little bit bigger and here you see
the model structure that is used for
this it's pretty trained
imagenet and then you hit classify for
example in here and then you you
classify it you get the result of the of
the imagenet classification you can see
all the layers right everything that we
saw before in the previous talks but you
know now everything runs in the browser
if you want to know what's going on
you know just put the breakpoint in some
you know if you if you are excited about
this fire modules maybe just put a
breakpoint somewhere and see what's
going on so here on the right is a
screenshot of my same thing running on
the phone that's boring yeah so and
because it runs everywhere because it
runs in JavaScript and in the browser
integration into other tools that
built-ins and JavaScript is also simple
like imagine you can prototype now apps
in electron for example and you can just
you know add deep learning capabilities
I think that's really cool or to
JavaScript chatbots or you know to
Google extensions or so on
yeah so this is the debugging feature I
told this already some cool interactive
visualizations so this is a screenshot
demo from con finishes where you can
actually train a model in the browser I
will show it later
so now let's jump to the intro to to
neural nets so what is the technology
that powers all these things and I want
to build with you a simple a classifier
that separates either pedestrians or
cyclists let's assume you build a
self-driving car algorithm and you want
to classify if someone is a pedestrian
or cyclist and for this we are going to
use supervised classification so yeah
obviously we need to collect the
training set right so we go with our car
and we take some pictures of of people
and then we annotate the training set so
we have the green labels which are
pedestrians and the red which are
cyclists and and then we want to find a
function right that shows us that
returns us the class out of the label so
what we do how are we going to do it is
we first we draw a random separation
line to separate these two classes so
it's random so it's very bad you know
you can see here it doesn't really
separate these classes at all but it's a
good way to start so now I'm going to
tell you about the separation line
because it's you know somehow important
so and this is the equation that I
promised you and it's indeed very scary
yeah so it's the line equation yeah no
surprise
well it's multi-dimensional so it's a
hyperplane in theory but you know it's
okay it looks like a line equation so it
is a line equation you know you have
some weights W right you have a bias B
this is the things that you assign
random before and you have some into
input data and what you get out from
this function actually is a line
equation and if you put input data
inside the equation you actually get the
distance from the line so yeah that's
very cool and a great place to start
actually I also give you the schematics
of this because this is what you usually
see when you when you see deep learning
you see a lot of these schematics so
this is how the line equation looks like
in schematics you have the input that
goes in you have two double unit the
weights and the bias and you you get
something out so before you saw you know
X is actually we don't have just one
dimension so it's multi-dimensional so
make here two inputs and the Optima is a
little bit so we remove the bias so we
can you know append we could upend the I
want to the input and then the bias
could be included in the weights so like
this it looks a little bit simpler and
we can continue stacking them together
so now we know this is a line equation
and we know that it returns you the
distance to the line on actually if it's
if it's on one side of the line and it's
greater than zero and on the other it's
it's less than zero so now we want the
fact that actually just not gives you
this the the distance to the line but
the actual class so what we do is we
need an activation function an
activation function maps actually this
value to the side of the line so it
gives the other plus one or minus one
for example and there are different
activation functions that
like slightly different things but the
the the sense is actually the same so
the activation function looks like this
it's a simple step function for example
and it maps our well is to minus one and
one so now we simplify this again a
little bit we make a circle and this is
what we call a neuron so this is like
will be the building block of everything
that we use beyond this point so here we
have this plus one and minus one and now
we want to know so this random random
line how good is it actually we can see
it's not very good but we need a measure
to you know to measure how good it is
and so what we do is you know we somehow
need to find out the ones that are on
the right side and the ones that are on
the wrong side and we do this okay this
true jumped a little bit but we do this
by comparing the output the Y to the
ground truth data that we collected
before and we compare these two and we
compute an error or a loss and this
error tells us you are either good or
your bed and details also how good and
how bad you are and we use this error
actually afterwards to use a technology
called gradient descent to update the
weights iteratively so by updating the
weights we always shift the line closer
to the actually to the actual where you
might think that it should be so that it
minimizes like the day wrong wrongly
classified samples and so now we see
actually it's good but there are still a
few you know that are still on the wrong
side and also you want to get this
somehow but we cannot do this with a
single line just right so what we do
yeah we take two lines okay so with two
lines you know now you can see the blue
line and the yellow line and this plus
one and minus one and now we could see
actually that it could work out this
looks like this now so we have two
outputs but with two outputs you know we
still have to combine the output somehow
to actually get to this red class and
combining the output looks like this we
actually want the this black thing
we want the plus one if it's in these
three in these three quarters or we want
the minus one if it's in this quarter
and we do this actually by stacking
another layer into it and that's very
important so if you look at this picture
again another layer means a combination
of features from the previous layer to
its it's a little bit more complex
feature right now it's two-dimensional
before it was just either this side or
the other side now it's a region and
that will be very important afterwards
so we remember we have stacked neurons
in two layers the layers are actually
just line equations we have activation
functions and we output a class core in
the end and then we have also this
optimization phase right where we
compute an error and we go back and
updates the weights from before and
actually the first thing is called
inference or forward pass I will and the
other thing is called training or also
back propagation for example yeah it
looks like this so in Fearne's you go
through the graph in the front and back
you go into the back so there's a cool
demo I'm doing exactly what I showed
right now so it's a playground or
tensorflow dot-org it gives you a neural
net in the browser it gives you some
input data here and you can have a few
settings so how many layers do you want
how many neurons in the layer do you
want and then you can hit play and you
can train the whole thing and you can
see it converts very quickly and you
know builds a model that can fit this
training data so you can take off more
complex training classes right and then
you can continue and see also the output
of this so I recommend you to play
around with these to understand what
actually I just previously said about
the introduction to neural nets so let's
continue now we are at neural nets right
but we are not at deep learning yet so
what are actually the ingredients to go
from neural nets to deep learning well
it's we will have a lot more data so a
lot more data also require
a little bit more efficient you know
efficient layer structures because we'll
see this later you know if the input God
gets very large and then the number of
parameters gets very large so the answer
to this is something called
convolutional layers and I will explain
it in a second and there's also
something like pooling layers and a lot
of other layers that help you to be more
efficient and with more efficiency comes
deeper models and also wider models and
new model structures and so on so what
is the what is the theory become behind
convolutional layers so if we see here
on the left side you know if we are high
dimensional so an input image you know
if you have image data right it's you
can see typical dimensions of an image
that are used in an image net for
example so if you use three times 227
times 3 27 high dimensional space I mean
it's quite high and if you have only 2
neurons the number of parameters is
these times 2 right plus the bias so
that can grow very very large imagine if
you have you know not one neuron but
thousand years you know that can grow
very large and the idea of the come of
convolutional layers is actually to
share the weights so you don't run this
computation using different weights for
all the input you use the same weights
between these inputs so you run this in
in sequence and then you sum up the
values and with this you actually share
a lot of weights because now you only
have two times two weights in this
example the nice thing about this is
that these weights these convolutions
they are actually filters like they in
image in images they work like you know
edge detectors and blob detectors and so
on and so this in the very early layers
and throughout the layers in a network
you know the filters go get more complex
and detect more complex features so I
will quickly show you this this example
this is training a neural net
deep net in the browser using Hornet
chess on the M nice data set
amnesty is a handwritten digits data set
and you can see here that you can even
change the network model and here is the
training running and then you can you
can see the activation through its the
layers and you can see here the
predictions on the on the unseen test
set and you can see after a few
iterations this thing gets quite good
with only a few layers so there is like
I think to convolution and to pool
layers and you can see how they look
like so these are actually these are
actually the the the filters so they
look like this
and apply this filter you can see
actually black and white on the side so
this is like an edge detector right so
yeah on one side is very efficient and
the other side it's very nice because
it's it gives you this this filter
property ok let's stop this so I said
the models are getting deeper and deeper
this is a was very horrible for me to
see that you know every year the models
going deeper and deeper the number of
parameters are going up and up so we
started in 2012 and the model had 60
million parameters and it was two
hundred forty megabytes big and then
there was vgg and it was even bigger and
you know I wanted to run it somehow in
the browser I was said like that's never
going to work
unfortunately there's also some before
it going into making the model smaller
not necessarily so they even make it
deeper but they can also make it smaller
at the same time by using some efficient
structures and for example the Google
net architecture it uses only seven
million parameters compared to 60
million a sixty million nett that's
quite a good improvement and also the
error on the image net classification
test set it's it's much smaller so so
that's very nice and then there's also
the squeeze net architecture recently
which goes even further and so you have
the full model at like five megabyte
and you know if you can still improve
this with compression techniques then
you need customized in inference engines
but still you could improve it further
to explore these these models and what
is behind you can also just click on the
link and open the models in the
visualization so let's so here you you
get a compact view how I call it so it's
more like a module bias view so you see
here this inception models that we also
heard before and you can actually here
click on the button to you know see
what's behind these modules and then you
see actually that this is a you know a
white model that consists of certain
filters that apply it on the on the same
on the same input and then the filter
activations are just stacked together in
the conquered layer yeah so you can
explore this you can really understand
what's going on in here right so yeah so
this is a really cool demo the deep
dream demo it's you know you for sure
have seen it a couple of years ago so it
was quite popular why I like it is
because this demo really shows you how
the depth of network is detecting
features at different levels and the
idea behind this demo is that you
replace the loss functions in the end
with just with just of the way the
weights of the last layer like the
distance to this line if you remember
and what you want to be is actually you
want to find this line so you want to
find actually the separation line in
this certain layer so you go forward to
this layer and then you compute the
error which is just actually weights of
this layer and then you break back
propagate this this arrow back and this
gives you these wonderful images so we
start with a very early layer we get
very you know linear looking features
and as we increase the depth of the
layers we get more complex looking
features and you can see here you know
we have already circles you
kind of you know some geometrical shapes
and here is something in the very in the
very end you can see it looks like
houses and so on it looks like a city
and I picked here out a picture that is
using a layer somewhere in the middle of
Google net and you know it you can see
that the features are more like highs
and it's connecting you know eyes and
hair and so on also this you you can see
this running in the browser right yeah
it takes a little bit so this model as I
said before is a 30 megabytes big so it
takes a little bit this is one of the
few demos that runs completely in web
workers so it's running in a separate
task than that in the browser so this is
also really cool so you can see that we
have here like four octaves and every
octave we do we make the size we
increase the size and then we try to
interpolate the lines in between with
more with more features yeah turn
yourself okay and what is the good news
about this is so this uses cafe models
and there are a ton of models of
pre-trained models available pre trained
on different data sets for different
tasks for different frameworks so there
there are models available for cafe
there are models available for care sort
in the flow or for torture for all the
other frameworks basically and you can
find models for classification for for
segmentation localization detection word
embeddings face detection or whatever
you can just go to the links and see
there's a long list of models that are
trained on different data sets for
different purpose and right so yeah you
can find all these models and the nice
thing is that the idea of most of the
frameworks in JavaScript that I will
show afterwards the idea is to make it
as simple as possible for you to run the
model in the browser so you just
download like literally go download the
cafe model and I think you will see in a
couple of days that you can just load
this one model no pre-processing no
compiling directly in JavaScript and run
it there which is actually pretty cool I
must say so how it's done right now
right now it's you have to compile them
the the binary model before and split it
into different into different blobs
because when I started with the library
I started with Alex net and it was 240
megabytes and I started with Jason which
is apparently a amazing format for big
data like this no it's horrible so I
couldn't parse it so I had to chop it
down in pieces and yeah until I read
they're out they read buffers and but
it's another story so so the summary of
these deep learning introduction is that
because of the weight sharing we
shouldn't use mostly convolutional
layers these convolutional layers they
act like filters and the deeper the
networks go the more complex these the
more complex features these filters
detect and so they start with edge
detectors that it's more shapes and then
his eyes and and see this and what we've
seen before and yeah my name olds are
available and also you know all the cafe
mode like the models included in cafe so
they're like five four six they're under
a license for unrestricted use and all
the other models published they have
usually certain license agreements but
it's most it's most often it's open
source licenses and available for
commercial use also so so that's very
great so you can just download put it in
your application and run it that's
that's really nice so yeah this is about
JavaScript right so you've seen some
demos and so if you are a JavaScript
developer and you're excited what is the
technology behind I have one slide for
you here so actually it's really
fascinating because javascript has
something called typed arrays and typed
arrays is supported in every major
browser also on the on mobile phones
that's why this demos work on the mobile
and it has something called array
buffers which lets you interact directly
on on a chunk of
memory this makes it really really fast
you know to store luxuries to you know
iterate over arrays to multiply erase
and so on it also has great a GPU
support with WebGL which is nice so you
can even have you know GPU accelerated
applications I will show some of them
later and you can do multi-threading in
webworkers we saw this in deep dream
demo it works also like charm and what I
like the most actually is that you have
such rich rich api's right you have
access to the webcam you have access to
audio you have access to MIDI you know
you can parse sound you can generate
sound there's lots of cool complex you
know sound processors in in JavaScript
that that's really exciting I mean you
can build really nice applications
really nice demos just just in the
browser with these tools there are a few
challenges though one that I explained
was the network load before so had the
memory consumption you know loading 240
megabyte of Alex net you know that's not
that doesn't make your clients very
happy
so as the model size decreases though
it's it's getting possible another thing
I have to mention is that there's no
CUDA support in the browser which is you
know it's very very sad so you have to
you know transform your your tensors
into textures you know and then have to
run the computations in yeah in the
shaders so it's it's a bit annoying so
but if you small models it's it's it's
actually fine so here's a list of models
that is available today so starting with
confidence which was actually the oldest
and it you know is somehow the
foundation and showed what what is
possible it actually emphasized that we
should we could do both in fear ins and
backprop in the browser so it
implemented this both in only CPU and it
also led you export and import models in
JSON files so and the train model so and
to load them so actually if you go to
the Stanford website of the neural
networks course there's a No
that running in the browser in the
header and predicting classes so that
that was built for you know showcasing
this yeah then yeah I developed a cafe
chess to run you know deep learning
pre-trained models in the browser but
because I built on top of con finishes
so most of the work was done by in in
confidence yes I just added a few things
and so with this I could also do
inference and back prop which was for
example necessary for the deep reme demo
that you so before though i wanted to
make this as easy as possible to port
cafe models and there's a few you know
tweaks in how to compute the filter
sizes and this is only specific to cafe
so it's only you know it supports like
only cafe models then there's an more
recent library which is Cara's chess
it's it's really nice I need to say it
focuses on GPU support but only
interference so you can do really really
nice things influence only so you have
an image and you you you do something
with it and I can show something was
also afterwards and which has
compatibility for for Karis models and
then recently so two guys from Google
they develop deep learned GS which does
all the things and it gives you Karras
and tensorflow
compatibility and terribly very soon
cafe support I I think so and yeah it's
it's a really nice library it's very how
to say it gives you more the foundation
right so you don't have to write the
shaders yourself it gives you more the
access to the to the sort of technology
behind and then you can build on top of
it yeah it's it's really cool yeah so
and we still have a couple of minutes
for demos so let's jump right in so this
is it's a really really nice table demo
implemented by a lot of people not me
unfortunately but it's very cool so I
will show it to you it does actually
basically the exact same thing that I
showed you before like before we saw
this pedestrian cyclist detection and
now we will make like a little bit more
sophisticated we make like cheering
laugh
and set face detection and we trained it
completely in the browser you know just
using the webcam and let's see let hopes
let's hope that it will work so this is
how it looks like it's very simple
interface and then you have these three
classes you have the green class you
have the purple class and the orange
class and then you can map these classes
two to two gifts so let's make let's
make some cheering yeah good let's make
some laughing okay so you know and now
we can just press the button and
unfortunately have to hold the mouse so
you could be implemented a little bit
better but so I have to hold the mouse
while I trained it so I can use only one
hand from chip for cheering but I will
start and you will see that the thing
will collect the training data you can
see the number of samples that it's used
for for training so now I cheer and the
guy is applauding me oh that's fantastic
so now we can continue with with the
laughing so I have to smile a little bit
I have to shake a little bit so that
it's detecting me and it's collecting
the training samples so if I laugh yeah
and I cheer all right that's nice sure
so you still have the sad face right so
we have one more
well it seems like yeah so
but not to so it always thinks I'm
super-happy which is nice
okay so so this also has you know sound
support and speech so it's really cool I
strongly recommend you to check it out
it doesn't work very well on mobile so
it's very experimental and mobile but
yeah it just works completely in the
browser it's it's completely astonishing
yeah that's teachable machine so also
you have seen fast neural style transfer
I'm pretty sure you everyone has seen
this but there's a cool demo here
running also entirely in the browser you
know this you can just again use the
webcam take a picture and use the style
and you transfer your style to this
produce picture entirely in the browser
which is unbelievable I find this really
nice alright so there's also there's
models working on text so there's for
example sentiment classification that's
also really really cool demo so but you
know text for me so I always use images
because images is super nice it's visual
you know and you can understand what's
going on so I will show you a different
demo this time which is a convolutional
a very thick variational auto encoder so
an auto encoder it tries to it gets the
input data and tries to predict the same
as it got from the input ATAR and on the
way it learns an feature representation
of this and what we see here is this is
the M nice data set that we saw before
so it's handwritten digits and we see
here a box of this you know feature
representation and what we will do is we
will select a point in this box in this
feature space in this latent space and
it will output it will output the image
that is generated from this feature
space point and what this lets you do is
lets you interpolate in feature space oh
that's that's really really nice
so it's it's kind of morphing right but
it's not morphing in pixel space but in
feature space so this is really really
cool
and last time I have seen this in action
there was a presentation in Vienna from
a guy from solando and he showed like
interpolation from I believe it was a
head to toe shoes and it because the
training set also included you know a
lot of clothes it interpolated to the
shoes I'm using t-shirts and handbags in
between so it was really really
impressive to see this and this is a
like a small example you can see here
but you can you can imagine where this
is going if you use for example faces or
you use objects or something or
something else than images so there's
some more another cool demo so this is
Karis chess by the way all these demos
so this is also really nice this is this
is a gent adversarial model so you train
two models a generator and discriminator
and the discriminator tries to identify
if the image from the generator is true
or not and learning this along the way
you get a generator that you can feed
with random random noise for a certain
class and it outputs something that
looks like that the discriminator should
say that it's through simple example we
have here like we select the class one
and we feed it with some random noise
and now we generate random looking
handwritten ones that that's the idea
behind it and we can cycle the three and
also we feed it with random noise and we
can create random random samples this is
great actually for for synthetic
datasets right for an animation so
that's that's a very nice thing
so almost towards the end so there's
also something for for creating melodies
you know also running in the browser
there's a lot more you you find all the
links in the presentation and I strongly
recommend you to go through them and you
know to play around see actually what
what is already possible today
and so I will wrap up this session with
some take-home messages um yeah what I
just said you can do a lot of really
really great things already today I do
using pre trained models right for a
certain task
or you train the models in the browser
so depending on you know what stage the
prototype is you can do a different
thing everything works on image audio
and text
I mostly showed images because it's
visually more appealing I must say you
should really keep an eye on the model
size you know 30 megabyte model you know
that's on the limit I would say to play
around with everything you know bigger
than that is yeah I mean it still works
right so but you don't want to download
this model in the browser or all the
time and if it's possible you should use
GPU acceleration which is you know
available on many devices yeah and then
I have one last thing to say I think you
already got it I think it's it's very
very exciting today what is possible and
I think it's an super exciting time for
JavaScript developers why is this
because you know you see all this AI and
deep learning stuff emerging you know
you see all these cool demos you know
working on a lab setup or somewhere but
really the JavaScript developers they
write software for the browser right and
the browser it it makes technology
accessible so you can bring all these
cool fancy stuff all these technology
you can bring to all the people around
the world on every device and that's
what's really fascinating me so I say
thank you and if you have any questions
I will be happy to answer</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>