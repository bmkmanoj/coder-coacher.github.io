<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Dive Into Computer Vision | Coder Coacher - Coaching Coders</title><meta content="Deep Dive Into Computer Vision - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Dive Into Computer Vision</b></h2><h5 class="post__date">2017-10-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qxkfVlvStBs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Nigel I work at Accenture and
I help run our data Science Center of
Excellence team so we have a broad suite
of people who do analytics and modeling
and various kinds of stuff and my team
is about eight people and we pretty much
take problem from like inception and
need to be taught about quite
extensively to doing the kind of
engineering work around it and then
delivering some end output whether
that's some findings of a report or put
you know or a model or some working code
that's kind of what we do we generally
short projects and relatively difficult
problems so this is a problem that I got
asked to tackle about a year ago and I
have a mass background before this not
really an image recognition and the
presentation is designed to take
somebody from kind of no knowledge of
what computer vision is up to a pretty
decent understanding of where things are
now and I'll try and get through it in
45 minutes so we'll start with a basic
problem which is from identifying people
and font where they are in image
tracking them through multiple image
sequences then we'll go and talk about
improving accuracy and so the approaches
I'll start with are kind of these really
classical approaches that have been done
for about 20 30 years but more recently
there's been a lot of advancements in
neural networks which I'll talk about in
detail in the second half and then I'll
talk about what's next because there's
some really interesting research going
on all the time in image recognition
when I was running this project a new
technique came out that literally as
we're in sort of like the second week
where we were like we want to use that
technique and we want to use their
models and their paper and stuff from
their paper so and they're even since I
put this presentation together there's
been some stuff that I'll touch on at
the end so we'll start with the finding
people problem and if you take a grocery
store code I guess guess is what the
project was on and we want to count the
number of people who enter into that
grocery store and what you might have is
in a frame you want to draw some useful
information around so the image on the
right you'll notice there's a green
outline around
person just entering the door that's
useful information for you if you want
to count them or track the number of
people see where they where they hang
around in the store and maybe even
observe you know how long they they
spend in the store and that kind of
behavior so the process we'll go with is
really simple and it's just called
background subtraction with contouring
background subtraction is really the
process of kind of averaging lots of
images than figuring out what the base
background is and then just taking that
background away from another image so a
quick example of how to do that would be
using lumpy pandas and OpenCV OpenCV is
called a open computer vision and it's a
really big library for doing many
different kinds of image operation it's
quite weighty and there are more
efficient ones for other things but
OpenCV covers lots of bases with the
kind of capabilities it has in it so if
you imagine what we might do is we might
have this kind of empty store which is
our background subtractor or which is
our background and we initialize a
subtractor and we take away kind of the
current frame of a video from it and you
end up with this ghostly image on the
right which is basically kind of the
pixel difference between the empty store
and the store with somebody in it and
that's and it's nothing more than that
it doesn't tell you that this is a
person it doesn't tell you that this and
you know in a dog or a cat or some items
in the shop and you'll notice that it's
particularly noisy as well and images
from video are inherently noisy they
contain things like heat distortions
yeah from say a refrigerator emitting
heat will cause this kind of you know
pixel values to change so you just end
up when you take one image away from
another you end up with some inherent
noise in there but there's probably
enough information in that in that image
to at least recognize those two people
at least and that's kind of the
information that we need to make
progress there's as I said the results
messy and there's even some kind of we
know weird computer screen thing on the
left that we don't care about
what we can do is we can do a process
called opening which if you imagine it's
kind of like sipping where you
effectively you just lose pixels that
are really small and on their own and
you and what that results in is you kind
of remove a lot of the noise but you
also take away a little bit of the edges
of some of the big stir shapes that
process is really useful for because
what you don't want is lots of noise in
your image you want large usually you
want kind of large approximations of
where people are and again this is a
really old technique this isn't like
something new or particularly
interesting and this is just something
that's very tried and tested and use
kind of a lot in the security industry
for motion-activated security cameras
and that kind of thing the other part is
basically combining the the same two
processes that we used on the outside
opening but they're doing it the other
way around and the sieve metaphor kind
of doesn't work that way around but
effectively if you look at the image on
the left you'll see that say it's it's
got a lot of thin and kind of wispy
detail around the woman's arms and on
their image on the right what's happened
is it's actually kind of filled in some
of those wispy details around the
neckline and the other image and what
that's done is it's just resulted in a
again a more cohesive shape because what
I'm really after is a blob that the
computer can draw around rather than
something with holes and and fuzzy edges
we can do it again with kind of changing
the size of the corset II of the sieve
effectively and that just results in a
slightly different different case now if
I was going to spend loads of time on
this I'd work on it until I didn't have
any noise queue because you can see the
last processes in brought through a lot
of noise but that's kind of an example
of you know when you don't necessarily
do it quite right in a really well tuned
example with a really well initiated
background subtractor and the
backgrounds of Traktor has to be some of
you average over time and it's kind of
quite a manual process to get it set up
and you you could even eventually how
you can strut that background subtract
can have a large impact on what this
ends up looking like so effectively the
process that happened kind of three or
four slides ago was there
and kind of just rudimentary at best and
when if you were doing this properly in
a production scenario or you know in a
more commercial scenario you would spend
a lot more time working on that initial
background subtractor and all the
conditions around getting it set up so
we're going to move on and eventually we
end up with this kind of shape and from
there what you can say is just draw a
route draw polygons around the largest
shapes for me
so from there you can end up with a nice
draw image of a green line around the
interesting shapes of the person by the
door so by those simple bits of maths of
taking one image away from another and
doing some various processing to reduce
the you know the grains of sand and
maybe but make sure you've got a
singular large blob you can end up with
the useful information I was talking
about on the third slide so that's the
real basics of tracking somebody or
finding somebody in a singular image and
you can generalize it by effectively
doing multiple frames and then just kind
of doing some basic vector tracking
which of where those frame where those
polygons go from each from one more
frame to another again very
straightforward and prone to errors as
well so really prone to errors when two
people overlap quite obvious because
what happens is the process ends up
making a really large blob instead of
two distinct blobs so you have to have
some quite careful logic in there around
making sure that you don't suddenly turn
two people into one and then one into
two again which can very easily happen
with this approach but it's pretty if
you have enough cameras in the right
places it's perfectly adequate to get
you where you need to go but the problem
is it doesn't actually tell you anything
about the type of thing that it's seeing
so you don't know whether a large blob
is a person or a shadow or you know a
car it doesn't you can't tell the
difference between a car and a person
which is a really common use case or
common scenario what you really want is
something that looks like this where you
individual thing is actually recognized
and localized on the image and then
labeled with the class that it thinks it
is so in this case it's recognized the
tie and the you know umbrellas and
people and and it's recognized them in
very in situations where there's
occlusion so on the left hand side you
know where the shoulder is basically
including most of the person's body and
it's recognizing you know the umbrella
even though it's misty and there's no
way you get to this with the subtraction
I talked about before you know this is
something a class entirely different so
they're a jet there are a few different
classes of problems and these are kind
of the ones I'll focus on for now
there's the classification which is
literally just showing it an image and
and it's just saying there's a hot dog
in it there's identifying where on that
image the hot dog is there's drawing a
polygon around where you think like or
drawing a square around the multiple
objects within the same image so rather
than just choosing the best like the
most obvious class in that image you
actually draw all the different types of
things
that's generally classed as a detection
problem and then there's the drawing
polygons around stuff now the instant
segmentation part that's one of the
things the kind of Holy Grail stuff of
self-driving car technology and it's
actually a really slow process so the to
do and this this can actually run about
60 to 90 frames a second on a Titan X
GPU which is kind of the latest graphics
card from Nvidia whereas and that
basically means it's real time and
faster than humans can do at an 80%
accuracy but for the segmentation
problem which is inherently useful for
some of those self-driving technologies
and this process is really slow I mean I
think that it's you're talking about
like kind of Vikings 1.6 frames per
second is the very best that you can get
in the moment which looks really it's
just really really choppy obviously for
humans to see in me and that's kind one
of the kind of main just processing
power is kind of holding back a load of
use cases and when that happened and
also the techniques are holding back
some of the use cases you could do if
you could draw around different types of
objects because then if you could do
that you could do have things like you
know
midair interception of objects by a
computer whereas if you're only getting
one point six frames a second of image
segmentation drawing polygons around an
image you're never going to be able to
have a machine intercept stamp something
in real time really accurately and you
can get that pretty close but you're not
going to get that real high precision
accuracy and this section is all about
accuracy so I'm going to talk about how
you measure accuracy and principally I
hope how many people are comfortable
with precision and recall hands up for
precision and recall okay yeah so
precision is like how when how often do
you say something is a hot dog you're
usually right about it and the recall is
and can you identify most of the hot
dogs in the population so they measure
different metrics the one of the metrics
that's commonly used is f2 score so a
lot of people are familiar with f1 and
f2 score um it's basically the harmonic
mean of F kinda like this derivative of
the f1 score and what it does is it and
it weights precision more highly than
recall which for image classification
problems is usually a preferred thing
because actually if you have a little
bit of error in your like in your reef
you have more error in your precision
it's generally seen by humans as worse
in these kind of scenarios so f2 kind of
relates more to just the way humans
perceive something to be good in this
scenario and but the problem with these
is they're really really good for
judging classification problems but they
tell you no they don't tell you when
shapes overlap or how accurately all
right you are with your localization so
a really common metric to use for
classifying when the localization has
worked correctly is something called
Jaccard distance or intersection over
union and that basically measures how
closely two sets overlap and I if you do
it for polygons how closely those those
polygons overlap and this is a really
interesting one because as a metric and
it actually it really heavily penalizes
inaccuracy so sometimes you might see
something that can
kind of an insertion over Union that a
computer is done and you might say well
that's a really amazing polygon that
it's drawn around that you know the I
know the piece of cake in that image and
yet it'll have a really bad insertion of
a union score because if the D the ring
around the edges of the polygons and
that's kind of quite heavily penalized
so it's kind of an interesting metric
because even a 60% in section every
Union can look quite good to a human and
you know whereas in the other ones you
need Union you need to like 95 percent
for them to be like a considered human
scale and quality of drawing and so how
do we get get this and I'll do a intro
to kind of neural networks and in a way
that hopefully will be understandable
and so if you think about the inherent
logic through all of this presentation
is that images are ultimately numbers
underneath and red green red green and
blue values or black and white in this
case and those pixel intensities can
vary information that we can then use to
try and figure out what is in the image
so I'm going to go into neural networks
and then I'll discuss kind of types of
neural networks later so if you imagine
passing every pixel value kind of just
like linearly through a series of
weighted equations and then passing it
through what we call an activation
function which effectively just imagine
if something's now if if that the weight
takes you over a certain value it might
trigger the activation function or if
the weight takes you over yeah it might
do it smoothly or it might do it like
non linearly to think about that in all
simple terms what you're really doing is
effectively the weights initially
usually they're pre randomized and then
you kind of have a process that tries to
guess the weights you add you add a beta
on determined just to is like kind of
like a slack variable I guess you could
call it
so the
is usually done by the best kind of via
an example so if you match in and this
is kind of just from the tents flow
example so there's the good stuff a good
thing with all of this the stuff I'm
doing here is there's loads of material
online for it
there's loads of material on from Google
and Facebook and academic research
papers the and this is literally this is
literally the kind of tensor flow 101
basic example of how to learn in your
own network if we're trying to predict
three class labels they're just seven
eight or nine we ultimately want the
probability to look basically to trigger
when and trigger for the class eight so
he gets the label of eight correct we
measure accuracy in this case biometric
called
cross entropy this function is really
important and depending on the type of
problem you're trying to solve and cross
entropy is like quite a good waste place
to start but there's more you can do
with it if you're solving different
kinds of problem but we'll stick with
that so that so we basically what we're
saying is we're comparing a labeled
accuracy which is our ground truth with
what the network thinks the label should
be so we said in this case the legwork
says you've got point nine for class two
and the ground truth is it's actually
one for class two and we want a way of
kind of busy and card doing that class
of the machine learning I we want to way
of kind of the machine to try and
improve its accuracy of predicting class
eight so what you do is you use a you
ethically run series of iterations which
is generally known as epochs to try and
improve that accuracy metric and so when
people talk about like this and the
machine learning of a neural network in
my head this is kind of the main step of
that learning where effectively every
time you're trying to improve your error
loss or I decrease your your error score
and and and therefore you it's kind of
improving itself and improving its model
as it goes along so that series of
weights and in if you look online often
you can actually download that series of
weights and there's a little just you
know numbers that initially started as
random and via this process of scoring
for accuracy and then RAM trying to
optimize that loss
has become more and more accurate at
predicting the classes and the weights
are usually quite and quite sizable in
terms of how like their you know their
megabytes or gigabytes that they take up
just because they're and if you imagine
this is a two layers you can add more
and more layers and what you end up what
ends up happening is you end up kind of
being able to identify higher more and
more higher level features usually the
deeper your network goes so whilst
initially you might have simple patterns
eventually you end up with eyes
eventually you end up with faces
eventually end up with you know scenes
and landscapes and everything else and
in jets sometimes these things are the
the initial step there and they're
similar to gabion filters or they are
gabion filters which kind of and the
rods and cones the reason this is
interesting is because some of the basic
pattern detection your brain does uses
gabion filters which are kind of ease
very straightforward kind of like
gradient fill type and if you imagine
the kind of PowerPoint background
gradient fill things where you kind of
got like a circle with different
radiating different colors or like a
diagonal line with one color on the side
in different gradients that's kind of
analogous to what a gabion filter is and
you can combine different gabion filters
to end up with more and more complex
patterns from there and so that you can
actually recognize multiple classes with
a named image and to make the processing
and the kind of transformations more
easy there's this convolution step which
is literally the process of just kind of
jumping around the image and to try and
find basically only pass through smaller
sections of it the interesting thing
with convolution neural networks is they
initially where there's a guy called
Jana Kern who kind of wrote the initial
paper on them and he initially they
weren't used they weren't kind of seized
upon straightaway but they they're
Conville dudes there have been various
kind of competitions to try and improve
the performance of image classification
tasks and these have kind of just come
out as the model of choice for those
problems
so you do it for every time you out us
and then effectively you just kind of
keep doing more and more layers or than
your on network so there are the thing
when you're on networks and this is kind
of one of the astonishing thing that me
and my team found when we were learning
them is you can kind of keep going
deeper and deeper and your understanding
of them so you can keep learning more
about different network architectures
that solve different problems better you
can keep light and kind of changing and
tweaking settings to try and improve
performance and it's definitely the
point where there's if you read kind of
the literature I think it's definitely
more kind of art than science in terms
of how people are choosing the network
architectures because it kind of just
often it feels like someone's taking a
punt on a different approach and that
approach is you know worked miraculously
for a certain type of problem rather
than because the computation takes so
much effort it definitely feels like
people aren't necessarily that I'm doing
so many kind of comparisons of different
models when they're writing their papers
you get that kind of comparison when the
big competitions run or when people
publish different types of model there's
a linear ranking of accuracy and other
stuff and other kind of metrics but for
on a kind of paper by paper basis it
very much seems as people are just like
really creatively trying different ways
of improving their performance for very
specific different problems so one of
the things you can do is change the
network architecture so initially we
talked about you know one layer network
we've added and then we moved on to a
three layer network well people have
kind of gone crazy with it and just
gotten to like you know 20 30 layer deep
networks with all sorts of different
functions on them the U net one is
interesting because this is if you see
it's kind of unbalanced you know one
step goes kind of straight to the end as
it were and then you've got these other
steps that kind of go deeper and then
back up that's really really good for
the image segmentation problems so if
you look at for example if your map
provider and you want to trace over maps
and so say where's the grass where are
the roads where the water the if you
look at the latest caracal competitions
it'll be units that generally will you
net-like architectures that win those
problems so for instance segmentation
problems they're really good the problem
is they're very slow to run like they're
not real times they're not a cables
solve real time problems or
do it quickly enough to be semi
real-time the one that's really cool
than was kind of that the Daniel Craig
James Bond example I gave was Yolo which
stands for you only look once and is a
really cool model that's all fully open
source you can either download
pre-trained versions of the model it
runs really fast and it does it can do
9000 different classes of objects in 60
frames a second which when it right at
about 78 percent accuracy and what that
means is it pretty much runs faster than
your eye can see stuff which is pretty
crazy when you see it working and fast
those YouTube videos and lots of demos
all that online as well which I've
encourage you to go and look at because
it's kind of crazy now there's one other
piece of thing you can do in network
architecture with the network
architecture and this is this is kind of
been around for a while but it's quite
new in terms of people applying it and
we just called like defined by run your
own networks so if you can imagine the
define and run is what premium the vast
majority of you on that song and that's
where they you furtively spell out what
the network looks like and then you just
say iterate over it just do more of that
and defined by run basically a kind of
allows the computer to just change the
network as it goes depending on set
criteria so you might have differently
different layers drop out or different
things change as they as time goes on
the defined by run is very interesting
because it solves NLP problems quite
well a lot of the new natural language
processing approaches tired that use
CNN's will use a defined by run Network
and that's usually more it's more the
case and then say some of the image
recognition problems which is quite
interesting so clear there's something
about defined by run that makes them
more performant generally and LP
problems for on an accuracy basis and
then there's tons more stuff so you can
modify the activation function so those
activation functions they're really
important they they kind of basically
just act to speed up the learning rates
by effectively making a decision when
the weights and when the weights
happened so effectively by they kind of
they activate to just speed up the
process
of the weights and the the learning
algorithm and happening a lot of
networks yeah there's like sigmoid is
really popular like rectified linear
units are really popular and and often
these are various different activation
functions will be combined in the same
network at different points and because
they have kind of various advantages at
different stages of the time kind of
features you're trying to do so
sometimes imagine you're doing the face
example sometimes you might say what for
if you're trying to detect an eye you
might want and say a sigmoid which has
like a really smooth curve if you're
doing and what if you're trying to say
whether somebody is the same person or
not maybe you want it like a rectified
linear unit one because it's going to
make a very nonlinear decision you can
change the your learning rates if you
imagine the kind of if the that saddl
graph of a colorful one as effectively
like the optimal solutions you can like
alter your learning rate to get to a
roughly accurate solution much quicker
for example max pooling is probably the
atom of the most important other step
where you need to know which is
basically the process of just summing up
pixels and it's a way of really speeding
up the processing time and all of this
stuff is like a real trade-off between
accuracy and performance and and max
pooling and pooling of any form is like
if Utley a performance improvement
method yeah other things to do with like
early stopping if it's clearly looking
like an unsatisfactory learning approach
and bagging for effectively just trying
to summarize results quickly again these
are all methods of performance tuning
some of them have they have accuracy
trade-offs but those trade-offs are
usually deemed to be worth it
and then drop out so this is one of the
things that a and that defined by run
neural network does really well and if
idli being able to customize your
dropouts so kind of just not running
stuff by certain neurons is a it makin
substantially improve performance over
the long term because you're basically
just having to do the left graph
traversals and and defined by runners
and if you imagine then you don't
usually want to be too static with your
with which things you choose to drop out
from your network so I think this is why
it works better if there's kind of my
hypothesis or why defined by run
networks work better for NLP
problems because your fertile your your
allowing the kind of graph to adapt for
many different types of solutions and
whereas an image recognition you're
going to see like similar patterns I
probably not explain that very super
well but dropouts are super useful I
think as well to do in most networks so
a quick example of you know training and
that would be and there's a great
library called TF learn for lowing
tensor flow since close Google's library
for and doing image recognition or
specifically tensor calculations but
it's like predominately used for a lot
and most of their models you can import
tense flow learn you can you the key
part is to have a set of images and then
a set of labels and so all the data sets
that you'll get for and this kind of
problem will have a set of images and
sometimes for the segmentation ones
you'll have a label and you know some
coordinates for where those where the
polygons are for when you're drawing
around stuff in an image and maybe
you'll haven't so therefore maybe you'll
have like a separate and another
database you might have an XY and Z but
in general you'll have like x and y
classes for images and labels you import
those and you do a little bit of
pre-processing that bottom section there
the network building is basically where
you define that network design
you basically just defining inputs and
outputs and defining what kind of
operation you're trying to run and then
eventually you train it's a really
simplistic model that and a classifier
and this this is just designed to run
this one live example here it's designed
to run really quickly it's not intended
to be accurate they definitely all
you've got is you know you input your
libraries you put your datasets you do
some image pre-processing and there's
some really important stuff about color
correction so you know images can be
shot in various different lighting
conditions and so normalizing for those
lighting conditions can be really
important and then you and maybe you
might one of the things that can be
quite useful in a is to say add a bit of
randomization to your training data and
within certain bounds and that allows
you to effectively just have a more
diverse training set for your when you
actually plug it to your model and that
data augmentation pieces are focused off
like a lot of work and research
because it's quite an interesting the
fact that you can add noise and it still
kind of helps out the network the image
that the data augmentation part is also
the source of some of the really
interesting works I think I'll talk
about at the end about what's next and
then you just run the network where you
just trigger it and set it off and let
it go so if we focus back just and I've
covered these kind in my in my
conversation it's like the first three
problems are actually in a I'll argue
like a pretty decent state from like a
timeliness and accuracy point of view
there is a lot of work that can still be
done to get the accuracy up to human
levels but there's you know they're
relatively fast the segmentation problem
is just slow and kind of there's it
seems like there's a lot of work to do
in that area and it's generally reserved
to some you know processes that can be
offline like drawing round maps or kind
of post event analysis for example so
you know the problem is this one of the
real advantages with the field is that
authors share their neural networks and
one of the real trends is someone will
share the paper they'll share the data
and they'll share pre train models that
so you basically you can kind of enter
into the process at any step you won
right you can enter it into the
theoretical step you can enter it
they're trying to train your own model
on their data and or you can enter it
they've already built a model that you
can go and pre like reuse immediately
for whatever you want and that last step
is actually was something that we and we
find really advantageous because if you
want to so it basically required no work
to get to the work the image of the
solution on the right so all of that you
know previous work that would take a lot
of time because somebody's pre training
the network and giving you the word
those weights and the classes you can
get to the results really quickly
usually they're published under and
pretty generous licenses and either like
fully open this is what this one is done
with I think basically a license that
says do whatever you want with it and
some of them are published under like
you know MIT or various other kinds of
pretty generous licenses and then you
know is another one like you know why
don't you train your own model and
people there's a guy in Israel called
Tao husana who takes that same approach
of you know Paper Co
data and pre train models so he
literally even releases his code for a
PM for you to use usually for research
but you know he's really really
something which estimates gender
accurately like eighty six percent of
the time that's a pretty incredible
accuracy for having to do eventually no
work where you know from scratch and the
same for age as well ages were a little
bit worse but it's still pretty
incredible I think that you can go
online and procure the end get these
things really easily like free and
openly and contribute back to the
research as well so for example when we
were doing this project I found bug in
one of the and the tense flow
implementation of Yolo and I may raised
a pull request and it got like boos
merged within an hour into the library
by people because that's going to have
collaborative the image recognition
community is and how I've receptive they
were over like certain types of fixes
and improvements so in terms of what's
next I think this is this is all stuff
that came out about within the last
three months the one on the top left is
by a student call called Carl von Drake
he's I think is a PhD or postdoc at MIT
and it's all about predicting what the
future of and generating what the future
of image sequence might look like so in
this scenario the green frames are
effectively what the computer sees and
the red frames are what the computer
generates and so you can see like it
gonna get strained with that with that
initial just somebody kind of going to
throw a basketball but the I sort of
going through baseball but it doesn't
know what happens next
their approach is actually on the the
kind of top left image that's actually
their prediction so the red is all
predicted and generated from that
initial image set of images and the
previous best approaches in the middle
and then a very kind of basic optical
flow which is just kind of taking the
vectors that were from the underlying
video and kind of just playing them out
into the future so L STM's are really
popular for patter for image sequences
and pattern detection and passive well
pattern classification
and their approach achieves kind of this
unbelievably freaky superhuman way of
predictor of generating and predicting
images into the future they the reason I
really like that paper is because they
take a really novel approach to data
collection so they basically downloaded
lots of YouTube videos they were all
unlabeled and they ran through and
they're kind of most accurate image
classifiers and then they use that to
train up their and their sequence
classifier and so I thought I really
liked it because it's a really novel
approach to kind of the problem of not
having enough data and kind of a bit
meta as to kind of you know using a
image wrecking the recognizer to train
up a sequence classifier and the results
are super impressive and he's always a
really great person to follow on Twitter
his name's Carl von Drake he's always
tweeting just like it's like a hundred
percent quality like it's really nice to
just just to get you see this thing pop
through and you're like that's awesome
to put into lists to read and follow and
the deeper line representations I think
is also one of his his works where they
basically proven that by a specific unit
within the neural network is activated
for when it receives say the sound of a
swimming pool an image or swimming pool
or the text describing a swimming pool
and which if you think about the kind of
generalized learning stuff is quite a
major advancement because it means that
effectively it has reached a Janna the
network has generalized enough to know
that it has this abstract concept of a
swing pool which is triggered by
different types of data all right so
that's kind of like one of those things
where you could imagine
I know layering in say more movement
data from from a robot and maybe
eventually you might end up with it
knowing what swimming feels like for
example as the same hidden unit I'm
using anthropomorphic talents which
probably doesn't help but you know the
same unit would be activated for say
swimming and swimming pool or that kind
of stuff so it's kind of like a more
generalized version of the network and
the dexterity network is super cool
again on the image but on the data
collection side of stuff so they they
were this is from your researchers at
Berkeley they generated 3d
shapes that they'd never existed or
they'd be just young made 3d models and
in the mid about six point seven million
3d models and then they trained this
network to try and optimize the grips
that would tie ie
if you had a robotic arm it optimized
like how it would grip the objects but
it obviously did that without ever
knowing the physical world it did it
only on the 3d generated objects and
when they trained it on a thousand
unseen object types in the real world it
had like 99% accuracy for picking up
objects so a busy only dropped like
wearing one one objects and because it
always figured out the optimum pose of
which to pick up the objects and I love
that approach again of just like it's
not like the YouTube approach of using a
classifier but it's still being really
creative with the data generation and
the data collection approach some of the
kind of the original researchers and
neural Nets have made a lot of work done
all work on kind of the would you
believe the understandability of neural
nets which is that bottom left-hand one
so just trying to figure out what it has
learned as a representation or what is
abstracted as a representation of an
object so when people talk about that
kind of I think or whenever some people
talk about like you know what has it
learned and we need to understand it
more like it we don't want it to be a
black box that's one of the papers
that's really gone down that route and
from a very technical angle and then
this other one is something that
literally came out like a feudal in the
last week and which is pretty kind of
absurd where it basically takes a 2d
image of an object and tries to give you
a 3d reconstruction of what it is it's
it's so on the surface of it it seems
kind of very straightforward but if you
think about it it has to have learned
like how a 2d is shape and the 3d shape
effectively like what the kind of
mapping might be between them and
therefore be able to kind of do the
transformation accurately from a 2d
picture of a chair to a roughly 3d model
of that chair and there's more even more
examples of you know
to not just type cities like types of
chair it's not just a chair in a car
it's like types of different car like a
pickup or ax or an estate car or types
of aeroplanes like you know big a big
airplane or a prop or something smaller
so these are all kind of these insights
into the research that's going on and
kind of little insights into you can
imagine the dexterity Network on a
production line or picking you can
imagine the generating the future and
they're generating the future ones
really funny because it's like it's done
like sliding doors so they say they make
the example that the future is
inherently uncertain so they do some
really clever stuff about you're picking
the most certain future but if future
versions of situations rather than so
it's like it's not just picking the
average future scenario it's picking
like it's saying well is like three or
four or however many different scenarios
that could play out and then trying to
give you the most probable scenario in
that from that image but then one of the
things where I was asked by a client
that I was working on was basically for
like Minority Report asked can you
predict the future of can you predict
when there were going to be accidents
like in a in a retail environment you
know like a slippage or so basically
health and safety type of incident and
so I was like that's a really hard
problem I'll go away and researcher and
the very best you're going to get is
kind of a rough generated version of the
future about a second in advance and
unless you take a different approach but
again like you the accuracy falls off so
quickly that was just not there yet as a
way of solving the problem so that
pretty much concludes my talk and thank
you very much for having me</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>