<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Complex Analytics Explained (3 hour course) | Coder Coacher - Coaching Coders</title><meta content="Complex Analytics Explained (3 hour course) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Complex Analytics Explained (3 hour course)</b></h2><h5 class="post__date">2018-03-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-3oTHKdITcM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the information revolution that began in
the mid twentieth century is entering a
new stage of development the confluence
of major trends in cloud computing new
data sources advances in algorithms and
the rise of the Internet of Things are
assuring in profound changes that take
us into the era of big data
the first wave of the information
revolution based around the personal
computer and the World Wide Web
has created a torrent of new data
sources from web blogs internet search
histories large-scale ecommerce
practices retail transactions RFID tags
GPS sensor networks social networks and
mobile computing have all worked to
create what we now call big data
but this mass of data would be no use
without computing capacities to process
it throughout human history computing
power was a scarce resource however with
the recent advent of global scale cloud
computing high-end computing is now
available to organizations of almost all
size at low cost and on demand
the third major element that has fallen
into place is a powerful new set of
algorithmic approaches breakthroughs in
machine learning and deep learning in
particular now provide the software
systems to process these ever more
complex data sets algorithms that learn
from data that can deal with millions of
parameters that can coordinate fast
digital platforms optimizing logistics
networks automating financial trades
predicting maintenance on electrical
grids
with these new tools we are now peering
into massive unstructured data sets
using ever more sophisticated
algorithmic frameworks to see what we
could never before see many compare this
to building a new kind of microscope or
telescope but whereas with the
microscope we revealed the microscopic
mysteries of life and with the telescope
the stars and galaxies this tool lets us
see the complex systems all around us
data is opening up our ability to
perceive things around us that were
previously invisible are evolved social
economic and technological systems that
have become so complex we can no longer
see them are being revealed to us in new
ways the implications of this are huge
just as the telescope changed our
understanding of our place in the
universe complex analytics is changing
our understanding of the world around us
the systems we form part of and this
opens the door to a shift in the nature
of how we make decisions and management
is conducted
more and more governments business
sectors and institutions begin to
realize data is becoming the most
valuable asset and its analysis is
becoming core to competitiveness today
data is becoming a new universal
language mastering it can win sports
matches can make movies of success can
win elections can build smart cities can
make the right trade at the right time
it may even win wars
this course explores the world of
complex data analytics information
systems that are able to analyze big
data and transform these restless
streams of data into insight decisions
and action complex analytics focuses on
how we extract the data from a complex
system such as a financial market a
transport network or a social network
and process that into meaningful
patterns and actionable insights
starting the course with an overview to
the subject we will look at the
emergence of big data and the expanding
universe of dark data we talk about the
ongoing process of data fication the
quantification of more and more aspects
of our lives and the many issues that it
brings with respect to privacy
in the second section we will talk about
the rise of algorithms as they are
coming to effect ever more spheres of
our world we will introduce you to the
workings of machine learning systems and
the different approaches used we will go
more in depth on neural networks and
deep learning before assessing the
limitations of algorithms
the third section is dedicated to smart
systems as the convergence of machine
learning with the Internet of Things is
beginning to populate our world with
systems that exhibit adaptive and
responsive behavior which are autonomous
and can interact with humans in a
natural way here we look at
cyber-physical systems
smart platforms and autonomous systems
before discussing security issues
the final section deals with the
relationship between people and
technology and the emergence of a new
form of analytics and data-driven
networked organization we talk about the
fundamental distinction between
synthetic and analytical reasoning as a
way of understanding the distinction
between digital computation and human
reasoning and as a means for
interpreting the rapidly evolving
relationship between the two
this is not a technical course where you
will learn the details of data modeling
or how to build machine learning systems
what it does provide is an overview of
this very exciting and important new
area that will be of relevance to almost
all domains researchers engineers and
designers business and the general
public alike
the course aims to be a comprehensive
overview to complex analytics it aims to
be inclusive in scope we try to provide
an understanding of the context to these
major technological developments a
conceptual understanding of the methods
and approaches of big data modeling and
analysis an overview to the underlying
technology and address the issues and
consequences both positive and negative
of such technological developments in
this module we're gonna give an overview
to the area of complex data analytics
touching upon many of the major themes
that we'll expand upon during the rest
of the course computers do analytical
processing of data in the past this was
largely about individual machines acting
on individual well-defined data
structures we called this data analytics
which is the use of computers to analyze
data and find meaningful patterns within
its that could be used by organizations
to make decisions today computing is
evolving to cloud platforms advanced
algorithms and big data and we can call
this advanced analytics or complex
analytics complex data analytics is the
use of advanced algorithms to process
big data structures with the convergence
of cloud computing platforms advances in
algorithms the growth of unlabeled big
data sources and now the Internet of
Things the ongoing revolution in
information is entering a new stage with
the capacities of information technology
being greatly expanded the creation of
personal computing the internet and
mobile devices has created a flood of
new data sources in response computing
is moving up from individual machines
with well-defined instructions acting on
well-defined individual data sets to now
running on clusters of machines on
massive amounts of unstructured data and
using qualitatively different algorithms
in the form of machine learning in this
process we're collecting ever
more data about ever more aspects of our
worlds we're bringing that into data
centers and applying ever more
sophisticated mathematical models and
computer science methods to building
algorithms that allow us to look into
this big data to see what we've never
seen before a world that was previously
only accessible through our imagination
is being presented to us as real data
and visualizations our data production
is currently on an exponential growth
curve with no end in sight the global
information network is now growing at
some two hundred and five thousand new
gigabytes per second a constant barrage
of web searches email ecommerce
transactions chats blog posts social
media feeds data streams from production
lines cars closed-circuit TV from
financial markets transport systems
mining equipment and buildings or
creating a continuous stream of
structured and unstructured data more
information crosses the internet every
second now than was in the entire
internet just 15 years ago as we begin
to instrument our worlds with sensors
and mobile computing our every action
becomes data it's sent to the cloud were
huge modular algorithmic Promax process
and cross correlated with the data from
everyone else everything starts to
become data your movement purchases
traffic and the data gets moved to the
cloud where it gets processed and
compared with data from other devices
it's no longer just what you do but what
everyone else is also doing and in this
data we begin to be up to see and
connect our individual actions with
those of others and the whole of the
system that we form part of when
hundreds of millions of people and
devices start to contribute data we can
start to see patterns emerge
from across society or across the whole
worlds but the challenge is the 80% of
all this data that is created is dark
unstructured data this is data that the
computers we've developed in the past 40
years are not able to analyze
effectively we're missing 80% of the
knowledge inside of this data
unlocking this unstructured complex set
of data sources requires new models and
algorithms and this is the other part of
the puzzle that's clicked into place
only just recently not only do we have a
new computing infrastructure available
to organizations on demand and a wealth
of new data sources but now we have a
new paradigm to algorithms in the form
of machine learning systems the
algorithms of the past were well defined
rules that were pre specified and
hard-coded into the software
there were mechanistic in nature recent
breakthroughs in machine learning
neural nets and deep learning techniques
have opened up new possibilities for
processing large and unstructured data
sets efficient named today a deep
learning algorithm can easy deal with
tens of millions of parameters and
billions of connections meaning they can
do things that were previously
unimaginable drive cars to tech security
anomalies analyze job applications
process insurance claims coordinate
traffic and the list is ever-expanding
these machine learning algorithms often
take the form of self-organizing
computational networks as exemplified by
the hugely successful approach of deep
learning this approach enables computers
to act on large unstructured data sets
and drive insight from them as a
consequence algorithms are no longer
confined to the internal workings of
your computer but can now expand out
into the worlds acting on ever larger
more complex data structures an
algorithmic revolution is underway as we
shift more and more of our system's
Forgan ization to cloud platforms at the
heart of these platforms will be
advanced analytics which is used to
coordinate and optimize the network
whether we're talking about car sharing
platforms ecommerce or logistics
platforms with the current rise of cloud
platforms we're in the process of
converting centralized closed
organizations into large open networks
these networks will be based around
market dynamics but to coordinate an
optimized such
complex systems were will require the
use of advanced analytics just as the
vast user networks of Facebook
uber Alibaba and Amazon are coordinated
via advanced analytics the same will be
true for almost all organizations in the
future mastering this new paradigm means
understanding not just data science and
machine learning but also how they
operate in the context of this emerging
platform economy the last major
components is the integration of complex
analytics with the emerging Internet of
Things machine learning will be
delivered as a service over the Internet
and the smartness that it delivers will
flow to all kinds of things as physical
systems of all kinds start to exhibit
new forms of adaptive responsive
autonomous and smart behavior the
Internet is starting to come off mine
into the physical worlds and machine
learning is a central element to this as
it enables the ingestion of large
amounts of unstructured data it enables
machines to interpret and understand the
physical environments human behavior and
likewise interact with people in a fluid
fashion not only to these advances in
algorithms enable mass automation and
the proliferation of autonomous robots
into the everyday worlds but more
significantly advanced analytics is
increasingly being connected into whole
physical infrastructure systems the
smart grid will throw off massive
amounts of big data and be coordinated
via complex analytical systems
performing dynamic load-balancing
dynamic pricing performance reporting
predictive maintenance etcetera the same
will be true for Transport Systems whole
metro systems like that of Dubai are now
automated the same for mines and for
fleets of ships for example Rolls Royce
is partnering with Google's cloud
machine learning engine as they research
and develop the next generation fleet of
autonomous ships the rise of big data
and advanced analytics represents a
profound change in both how we
understand the worlds
decisions and act on those decisions the
depth scope and significance of which is
difficult to overstate in a recent paper
by Erickson the author's capture some of
the significance of the process that's
underway when they note in contrast to
digitalization which enable productivity
improvements and efficiency gains on
already existing processes data fication
promises to completely redefine nearly
every aspect of our existence as humans
on this planet significantly beyond
digitalization this trend challenges the
very foundations of our established
methods of measurements and provides the
opportunity to recreate societal
frameworks many of which have dictated
human existence for over 250 years
advanced data analytics can be
interpreted as simply how we manage our
worlds in an age of complex systems the
technology holds out the possibility of
actually seeing and understanding the
complex systems that now run our worlds
from transport networks to social
networks to cities and global supply
chains we actually have the possibility
to manage these systems in a new way
Shane Gordy founder of quit States it
clearly when he says we live in a very
complex worlds there are seven billion
Minds now and these seven billion minds
have created a worlds that no one of
them can understand and yet we still
have to make decisions we have to decide
whether or not send troops to Iraq and
we have to decide what to do about
climate change and we have to decide how
to deal with the global financial market
that doesn't want to stay still complex
analytics enables a new form of
data-driven understanding to our worlds
in that it enables us to visualize and
in some ways see these systems that have
become so complex that no one person can
comprehend them big data and new
visualization methods can abstract away
from the underlining complexity to
present a quick high-level overview to
an otherwise impenetrably complicated
data set financial markets that today
are hidden behind layers of a pack
complicated obscurity could be seen and
grasped by every trader in the markets
billions of data points from around the
planet could be ingested cross
correlated and visualized to deliver a
real-time vision of global security
threats to everyone on the planet via
their smartphone in a way that any one
of them could understand in a few
seconds the threats of climate change
the risks of cybersecurity the real
social and environmental impacts of your
current purchase all could be made
transparent to any one of us enabling us
to take responsibility for our actions
and incentivizing us to make the right
decisions
as MIT professor Alex Penman puts it
this is the first time in human history
that we have the ability to see enough
about ourselves that we can hope to
actually build social systems the work
qualitatively better than the systems
we've always had this is not just true
for societies it's also true for all of
the complex systems that now make up our
engineered environments
likewise data fication changes the
actual nature of how decisions are made
within society organizational
decision-making processes have undergone
a tremendous shift in the past 20 years
enterprises are changing the center of
gravity and their decision-making units
from human expertise to big data driven
systems this shift can be attributed to
people's limited information processing
capabilities in relation to the
explosion of data take for example the
shipping company mask lines which
operates a global network with a total
Seabourn Freight of over 2 million
containers that traveled to 350
different ports as they work to move
about 15 percent of the world sea
freight the company estimates that
they're spending more than 1 billion
dollars a year
moving empty containers back and forth
no human could begin to reason about how
to effectively coordinate such a system
but mask is using data analytics to
automate and optimize where every
container goes next and thus strip out
the waste of resources in the network
this is though a relatively simple
example compared to the data challenges
that our organizations and societies
face going forward as a sea of data gets
larger the haystack gets larger and it
becomes more difficult to find the
needle while those who fail to evolve
get lost in the noise and paralyzed by
the complexity the winners of this game
are those who can use this technology to
see through the complexity to find the
signal in the noise with which to move
fast and strategically in so doing
radically outperforming their peers in
an information economy is not the big
fish that need the small fish
but the smile fish that are able to see
what's coming and adapt the fastest that
survived for large organizations to be
the smart adaptive fish is a huge
challenge but mastering complex
analytics is at the heart of ads the key
feature of successful organizations in
the age of data fication is their
ability to capture and effectively
analyze the wealth of data available to
them and quickly convert it into actual
insights so that they can effectively
adapt and responds not only does complex
analytics offer new ways of knowing our
worlds through data and visualization
and new ways of making decisions through
advanced algorithms but mass automation
likewise offers new ways to execute on
those decisions for better or for worse
mass automation of physical systems and
basic services is now here around the
planet from Germany to Japan
physical systems are being automated and
connected up to cloud platforms with the
rise of cyber-physical technologies and
automated systems the nature of how we
manage and control our environment is
also changing as Steve law the
technology writer for the New York Times
put CERN indeed the long view of
technology is that it will become a
layer of data driven artificial
intelligence the resides on top of both
the digital and the physical realms and
today we're seeing the early steps
towards that vision although Steve laws
statement has a touch of science fiction
to it the surprising thing is that
science fiction appears to be becoming a
new reality in our worlds the more data
we get and can effectively use the large
of the problems we can solve but these
powerful tools also have a dark side to
them such powerful technology also has
profound philosophical and ethical
considerations coupled with its a dark
side of extreme concentration of power
of control and manipulation on an
unprecedented scale of the civilizations
spinning out of control the stakes have
never been so high but these issues need
to be addressed in the context of the
underlying technological changes that
are happening and will address each of
them at the appropriate stage as we go
through the course in this module we're
gonna lay down a basic understanding of
what we mean by this term analytics or
data analytics let's first take a look
at some of the definitions that are out
there so as to give us an intuition for
what we're talking about Wikipedia has a
straightforward definition analytics is
the discovery interpretation and
communication of meaningful patterns in
data the business dictionary expands
upon this analytics often involves
studying a past historical data to
research potential trends to analyze the
effects of certain decisions or events
or to evaluate the performance of a
given tool or scenario the goal of
analytics is to improve the business by
gaining knowledge which can be used to
make improvements or changes or finally
a definition from David cloud who
defines it as how an entity ie a
business arrives at the most optimal or
realistic decision from a variety of
available options based on existing data
analytics then can be understood quite
simply as using data to answer questions
it is the process of analyzing and
studying data in order to derive insight
from which we can make decisions and
take actions that lead to effective
outcomes on a more general level we can
understand analytics as simply the
information processing activity that
takes place within all organisms
individuals and organizations whereby we
take in information process it and
create a response that enables the
development of the organization within
its environments the central aspect here
is that of information information is
understood on a technical level as a
measurement of uncertainty
if we take a binary digit that can have
two states 1 or 0 before I'm given any
information I am uncertain about the
state of this system it could be 1 or it
could be 0 however when you give me that
piece of information I can check to see
what state it's in and in so doing
reduce the uncertainty about its value
indeed information is not just about
uncertainty but by extension it is the
capacity for an organization to grow and
develop over time this is due to the
fact that by reducing the uncertainty we
increase the certainty that our actions
will be successful when we reduce the
uncertainty we can increase the
efficiency with which we allocate
resources and can thus develop faster a
simple example of this could be seen in
finance when we're trying to hedge our
bets if we know that a certain outcome
or set of outcomes will not occur then
we do not need to hedge against them and
spread our resources instead we can
concentrate the allocation of our
capital to a specific set of outcomes
and thus increase our returns this is
important because it's a general
condition the more information we have
the less we have to expend resources in
uncertain conditions and the more the
organization can invest in those options
that will lead to growth when I walk
into a train station that I've never
been into before I will have to expend a
considerable amount of time finding
where to buy a tickets what time the
train leaves which platform where the
platform is etc but the next time I
entered the train station I will have
all this information from past
experience I will walk straight to the
ticket machine and then straight to the
Train thus conserving time and energy
less time and resources expended mean
they're available for me to invest in
other options the same is true for
technology if I have a motion sensor in
my house it can know when there's no one
there and switched the lighting and
heating off so as to have more resources
to allocate in the future so this is why
we're interested in analytics because
it's the central part of the information
processing system within
an organization and can enable it to
develop and grow more effectively
analytics is all about finding patterns
in data which is exactly what humans do
all day every day however our aim here
is to automate this process of pattern
discovery so that it can be scaled to
large organisations when we use the term
oolitic s' we're typically talking about
the systematic computation of data
within an organization we take data and
use computers to search through it to
answer a question that is in someway of
importance to the success of the
organization if we can figure out how to
formalize the problem into computer
codes then we can harness the true power
of computation which is to iterate very
rapidly on simple rules by iterating
very rapidly on simple rules that are
combined into high level algorithms a
computer can analyze much more data much
faster than a human can
as a result we can begin to approach the
amount of data analytics that is
required for enabling a large
organization to operate successfully in
a complex environments which is the end
objective so data analytics is the
information processing unit of an
organization that uses data computation
and mathematical modeling to generate
actionable insights it all starts with
data unlike a more theoretical approach
that might start with logical reasoning
and theoretical frameworks for deducing
information in contrast analytics is
always grounded in data we sample a
state space taking in data about the
system or environments and data modeling
is used to organize and structure it
into a form that can be processed by the
system this may be called descriptive
analytics which is the simplest form
involving the gathering and description
of data most analytics is of this form
simply sampling data and presenting it
to the organization for them to make
decisions with descriptive analytics in
the form of pie charts and bar charts in
presentations
have been a staple of business
intelligence for decades now a step up
from this is predictive analytics which
tries to apply rules to the data to
processor into forecasts about what will
happen in the future beyond this
prescriptive analytics involves using
that insight to make recommendations and
suggest courses of actions for the
organization to take all data analytics
exists within the context of the broader
business intelligence of the
organization this typically involves
people asking the questions start with
and those people making the decisions at
the end of the day as such if you want
effective overall outcomes we have to
think about the system as a whole the
people and the technology you don't just
need the right data models and
technology you also need the right
people asking the right questions if you
ask the wrong question in the first
place
it doesn't matter how good your answer
is it will lead you in the wrong
direction you need human intelligence
and you mean that working with the
analytical capabilities of the
organization it's only then that you can
really hope to achieve sustained success
at the end of the day this is all about
the success of the organization and that
is dependent upon the whole system of
human intelligence and analytical
computational processes working together
as we discussed in a past module
analytics is all about using data to
answer questions but how we approach
doing this is very much dependent upon
the complexity of the system we're
dealing with and the kind of questions
we want to ask analytics has been used
in organizations in a formal way for
over a century now being pioneered by
the work of Frederick Taylor in his
analysis of work processes and Henry
Ford's measuring of his newly
established assembly line this is a
relatively basic form of analytics where
we're studying a very limited number of
components and processes in a somewhat
linear fashion this linear approach has
largely dominated business intelligence
until quite recently but as we'll
discuss in this module things have just
got a lot more complex and our approach
to analysis is likewise changing to
become a lot more sophisticated this is
now what we call advanced analytics or
complex analytics whereas with the more
basic approach to analytics we're asking
relatively straightforward questions
about a relatively simple system that is
limited in scope with complex analytics
we're trying to answer complex questions
or another way of saying it is that
we're doing analytics for complex
systems to give this context and
relevance we can note how our world has
just got greatly more complex from
almost every dimension over the past
decades whether we're talking about
telecommunications transport
international politics supply chain
management or the media landscape the
number of nodes and the degree of
connectivity has greatly increased in
many cases such sim financial markets it
is an order of magnitude greater than it
was prior to the 90s this new level of
complexity that is emerged within
virtually all of our systems of
organization has major implications for
how we should approach analytics living
in a small town you could simply walk
around and talk to people to find out
what was going on as we form larger
organizations during the Industrial Age
new tools of communications emerged
the telegraph telephone and postal
system enable people to communicate
effectively within these large
organizations television and the
newspaper inform people of what was
going on in their region city or nation
but today we found ourselves embedded
within vast complex systems networks
that often span around the planets
corporate supply chains huge social
networks sprawling metropolitan areas
for global air transport system the
global biosphere financial markets these
are things that shape all of our lives
but somehow we don't have the means the
vocabulary or the methods that she
grasped them in their complexity to see
them or to really know what is going on
in the system as a consequence of not
being able to see the workings of these
systems and thus in some way managed
them we get financial crisis we get
environmental crisis we get violent
social outbursts because we can't see
the mounting tensions large corporations
drop off the SMP 500 faster faster
because they can't see through the
complexity and respond fast enough in
these increasingly complex environments
that organizations operate in today
traditional conceptions of linear
cause-and-effect break down things
become ambiguous and simply left without
interpretation we find ourselves in a
reactionary States continuously
surprised and shocked volatility
increases we start to see only some of
the trees and no longer the whole forest
as a consequence our organizations
becoming capable of acting decisively
knowing what to do and making important
long-term decisions and investments
as this complexity cliff rates we are
bearing the proverbial needle in the
haystack more data means more noise we
stopped being able to hear the things
that we need to be able to hear this is
the same problem for businesses for
individuals for researchers for
policymakers for anyone living in this
world of globalization and information
technology so how do we make sense of a
complex worlds we need new models and
new ways of looking at the world but
just as importantly we need new tools
and methods for amassing and processing
data and information and this is what
this course is all abouts these new data
sources and tools to support a more
comprehensive understanding of the
complex systems that we are now
challenged to try and understand and
manage whereas the traditional business
analytics of the past has been based
upon well-defined and well structured
data sets that were limited in size and
complexity today we have a wealth of new
data coming from a myriad of new sources
and we call this Big Data whereas the
data of the past was structured into
specific vertical categories being used
to answer specific questions this stream
of unstructured data from a multiplicity
of sources enables us to create context
by making connections between a
diversity of data we no longer just have
data about the products results and look
to see how changes in the price changed
sales but we now have a massive amount
of data from different sources that can
be used to find much more complex
patterns and correlations data about
customers about location about all the
other products available specificities
about the store where an item is sold or
the time of day the weather etc and all
of this can be put together in new ways
to find new patterns that were
previously hidden
whereas in the past if we wanted to know
where a fire might break out in a city
or an accident on a highway happen we
were inclined to look at the phenomena
itself but with complex analytics
insight may come from a completely
different realm that has nothing to do
with that actual activity with analytics
we look simply at the events with
complex analytics we can now look at a
network of data points to create some
kind of context to the phenomenon so
that we're no longer dependent upon
simplified mechanistic cause-and-effect
descriptions but can begin to look at
things in a more realistic fashion as a
network of interacting factors
John Kelly of IBM talks about this
change is such what is different now and
has changed is that it's no longer about
taking this data putting it into a
computer
vanya calculation and getting a balance
sheet answer what's important now is
what is the context of the data what
does it connect to what effect is it
having on data around it it's basically
a network of data it's no longer sort of
tabular columns of rows of data
it's interconnected systems just as data
is no longer a single thing so to the
means with which we process is changing
in the past we used mechanistic rules
like formulas in a spreadsheet that were
dependent upon strict well-defined data
sets as input today we're moving from
rule-based mechanistic algorithms where
all components are pre specified and
well-defined
to computational graphs which are
networks of nodes that learn through
self-organization complex analytics
gives computers the capacity to
understand data in new ways like humans
do
this means the computation can now act
on data to write directly from
unstructured context and real-world
environments one of the best examples of
complex data analytics is web search web
search was one of the first widely used
applications of big data and advanced
analytics a search engine like that of
Google or BAU do
looks through massive amounts of data
and within seconds analyze it in a
multiplicity of different ways web
search has given us the capacity to look
into big data to look into all of this
information we have and we can think
about the effects of that on transport
on research on almost every area that
now depends upon this big data of the
Internet and complex analytics in the
form of search engines in this respect
people often equate our newly found
technological capacities of complex data
analytics to that of the microscope or
telescope jaywalker of TEDMED describes
this revolution well when he says the
microscope in the 1650s and 60s opened
up the invisible worlds and we for the
first time was seeing cells and bacteria
and creatures that we couldn't imagine
were there it then happened again when
we revealed the atomic worlds but now
there's actually a super visible worlds
coming into play ironically big data is
a microscope we're now collecting
exabytes and petabytes of data and we're
looking through this microscope using
incredibly powerful algorithms to see
what we could never see with these new
technologies that we'll be talking about
in this course it's like we're building
a new kind of instruments like we're
building the telescope for looking into
complex networks for the first time were
able to look at these complex systems
that are all around us which have a
structure a pattern and even a beauty
that are invisible without the right
instruments the implications of this are
huge in terms of how we understand the
world and our place within it and of
course how we make decisions and act on
them
the term data' fication refers to the
fact that we're looking at more and more
things and using technology to render
them into a data format simply said it's
about taking previously invisible
processes or activities and turning them
into data that can be monitored tracked
analyze and optimize through analytics
whereas digitalization has been a
process taking place over many decades
now data fication is a relatively new
phenomenon the difference being that
whereas digitalization was about
converting information into a digital
formats data fication is more about the
interaction between the digital domain
and physical objects processes and
environments with mobile computing and
the Internet of Things
we now have all kinds of sensors in our
environments and we're starting to
convert all sorts of things into a data
formats there are many examples of this
from putting sensors on a bridge for
monitoring its structural integrity to
monitoring parking spaces performing a
3d scan of an object to measuring the
activity levels of a person's health one
specific example is the company general
educators in the process of converting
themselves from an industrial company to
what they call a digital industrial
company where they create a digital twin
for every one of their products their
physical technologies are now surrounded
by sensors and controllers that can pull
a massive amount of data from a jet
engine from an MRI scanner from a jet
turbine wind turbine which will provide
real-time data about themselves that
goes into a virtual model of the system
that is unique to that machine this
digital twin is a cloud-based virtual
image of the physical assets maintained
throughout its lifecycle and easily
accessible within seconds of a new wind
turbine going into operations tens of
thousands of data points are created and
entered into the model likewise more and
more of our social activities are being
rendered into a digital formats
Facebook data files our friendships
LinkedIn data files our professional
accomplishments Twitter data files our
thoughts and Google Maps data files our
location a multitude of different
technologies are now available there
help individuals monitor and measure
things though our previously difficult
or impossible to quantify everything
from how much energy and water one uses
what your food purchasing habits are the
air quality of your neighborhood when
you were awake or asleep knowing when
you're stressed or what road you
selected tried to work how you brushed
your teeth in the morning etcetera we
create data every time we talk on the
phone send a text message watch a video
would draw money from an ATM use a
credit card or even just walk past a
security camera all this can now be
measured quantified and compared
the word data comes from the Latin term
meaning literally something given it is
a set of quantities characters or
symbols an assumption or premise from
which embarrasses may be drawn it is the
basis of reasoning or calculation data
represents discrete units of information
and thus were always isolating some
aspect of the phenomenon and freezing
its data is always a slice of reality
we're chopping the world up into little
bits and taking that information as in
some way completes this makes data
portable it can be taken from one
context and brought into another making
an amenable to cross correlation all
data in the real worlds exists within an
integrated context no matter how
extensive are gathering of data we will
only ever be able to capture a partial
representation of the system and in so
doing is separated from its overall
context data is always incompletes but
that discrete nature of data makes it
quantifiable and thus amenable to formal
quantitative methods of analysis as such
data as a general concept refers to the
fact that some existing information or
knowledge is represented or encoded in
some form suitable for processing within
a computer where as data and information
have always existed around us what data
fication does is make that information
available for computerized analysis the
primary use of data is for manipulation
within computer programs which are
formal systems thus with data fication
we're taking the informal everyday
worlds and converting it into a virtual
structured format that can be used
within a formal system whereas
previously with digitalization we
converted many forms of information that
were already in a structured and
quantifiable formats into a digital form
for them to be accessible to individual
computer programs what we're doing today
though is building platforms that
operate as computers but now on the
macro level
a computer is a system that manipulates
data according to a set of instructions
whereas previously this data and
instructions were in individual
computers now with cloud computing
online platforms of the computers with
their algorithms running in data centers
they take in data about people and
things and analyze it to create an
output a platform society is where our
technology and social lives are
increasingly channeled
through online platforms our informal
lives and our engineered systems become
moved to formal platforms whether this
is dating websites car sharing platforms
or health websites they all require that
we data Phi the things in our worlds in
our lives and input that data through
the platform which then acts emits
analyzing it to create insight make
decisions matching coordinate different
systems as a consequence of moving ever
more of our systems of organization to
these automated platforms we begin to
increase in the understand and manage
organizations and things fear sets of
data points as an ever more complete
information picture of who we are and
our engineered environments is compiled
in these cloud-based information systems
that we call platforms this is the
current journey that were on we're going
to turn our world int data so that we
can bring it into these platforms these
formal systems and process em however a
lot of the problems we're going to have
are going to really come from this
incompleteness of data data will
unavoidably omit many features of the
world's distort others and
decontextualize vents this process of
data fication enables us to change the
very foundations upon which we make
decisions for organizing society and
economy instead of people making
best-effort guesses in a context of
incomplete information it takes us into
worlds of decisions being made by
algorithms based upon huge amounts of
data that coordinate the platforms that
increasingly mediate every aspect of our
lives this has profound long-term
consequences it begins to change some of
the fundamental mechanisms upon which
societies have always depended from the
basis of the techniques used in the
scientific method to how economies are
measured and structured to how
businesses are run to how we understand
our bodies in the world around us big
data is a term that's come to be used in
reference to data structures that are
diverse complex and of a massive scale
although the term has been in use since
the 1990s it's only with the rise of web
2.0 mobile computing and the Internet of
Things but organizations find themselves
increasingly faced with a new scale and
complexity of data the term big data
implies an increase in the quantity of
data but it also results in a
qualitative transformation in how we
store an hour lays such data it is
certainly the case that with big data
more is different the world's
technological per-capita capacity to
store information has roughly doubled
every 40 months since the 1980s
resulting in an extraordinary increase
in data storage capacity since that time
the amount of information in the world
has exploded likewise the digitization
of that information has happened in a
historical blink of an eye back in the
late 80s less than 1% of the world's
information was in a digital formats by
now more than 99% of all information in
the world that is stored is in a digital
formats equally the amount of data
available through the internet has grown
at an extraordinary level the world's
effective capacity to exchange
information through telecommunications
networks was 281 petabytes in 1986
471 petabytes in 1993 2,200 petabytes in
the Year 2060 five exabytes in 2007 and
predictions put the amount of Internet
traffic
at 667 exabytes annually by 2014 from
this data we can see how little after
the year 2000 the amount of digital
information can to explode and at the
same time largely due to the mass
adoption of the internet and
user-generated systems the nature of
that data changed from being largely
structured to being largely unstructured
we might identify this as the tipping
point from the world of data to this new
world of big data
indeed industry government and academia
have long produced massive datasets such
as remote sensing weather predictions
scientific experiments or data from
financial markets however given the
costs and difficulties of generating
processing analyzing and storing such
datasets these data have been produced
in tightly controlled ways that limit
their scope temporality in size for
example to make the compiling of
national consensus data manageable
they've been produced once every 5 or 10
years asking just 30 or 40 questions and
their outputs are usually quite coarse
in resolution while the census may wish
to be exhaustive listing all people
living within a country most surveys and
other forms of data generation are just
samples seeking to be representative of
a population but not technically capable
of representing all features big data
has a number of key attributes that make
it distinct in nature from these more
traditional data sets including its
volume velocity of data capture variety
of data sources its high resolution and
it's often exhaustive nature of sampling
first as the name implies big data is
truly massive in volume consisting of
terabytes or petabytes of data take for
example the Chinese ride-sharing
platform dd which serves some 450
million users across over 400 cities in
China every day
DD's platform generates over 70
terabytes worth of data processes more
than 20 billion routing requests and
produce over 15 billion location points
or for example a typical 20 bed
intensive care units generates an
estimated 260,000 data points the
seconds likewise a military fighter jet
drone may have 20,000 census in one
single wing to enable it to fly by
itself on one single flight and a 850
airplane can produce 205
the gigabytes of data secondly these
data sources may be high velocity as
data is being created in or near
real-time to produce massive dynamic
flows of fine-grained data for example
Facebook reported that it was processing
2.5 billion pieces of contents 2.7
billion mic actions and 300 million
photo uploads per day in 2012
similarly in 2012 Walmart were
generating more than 2.5 petabytes of
data relating to more than 1 million
customer transactions per hour the
variety of data and data sources is a
key aspect of Big Data that
differentiates it from more traditional
forms of structured data photos videos
text documents audio recordings books
email messages presentations geo
locations tweets are all data but
they're generally unstructured and
incredibly varied an article in Sloan
review entitled variety not volume is
driving Big Data initiatives notes for
the past several years have been periods
of exploration experimentation and trial
and error in big data among fortune 1000
companies for these firms it's not the
ability to process and manage large data
volumes that is driving successful big
data outcomes rather it is the ability
to integrate more sources of data than
ever before new data old data big data
small data structured data unstructured
data social media data behavioral data
and legacy data
while this variety may be the key source
of complexity to big data may also be
the truth source of insights by
referencing different sources we can
begin to build up context to events or
outcomes instead of uni-dimensional
interpretations for example if we take
something like fraud detection on a
debit card an ATM machine may just
swallow your debit card because you're
simply using it in a different country
from where you usually use it this is
the results of an analysis based upon a
single data point which gives very crude
outcomes but with a variety of data
sources such as social media purchase
history geolocation etc a much more
nuanced picture could be built up to
better understand if it's really used
standing in front of the ATM or knots
big data is often exhaustive in scope
striving to capture entire populations
or systems and all relevant data points
take for example a recent project
initiated by the US Securities Exchange
Commission to try and capture and
analyze every single US financial market
event for every single day the goal of
the project called consolidated audit
trail or Katz
is to track every lifecycle events every
tick every trade every piece of data
that's involved in the US market in one
place and bring it into one data
platform the goal is to build a
next-generation system that will allow
them to understand in a reasonable
amount of time what is going on in the
markets this involves taking data from
all the different silos across all these
different banks the broker dealers for
market makers the dark pools and so on
and bring it all into one system the
system has to ingest between 50 and 100
billion market events per day that's 15
terabytes of data a day that needs to be
processed within four hours and made
available for running queries on the
whole data set so that any trade from
that day can be traced from its origins
all the way through to completion we can
note here how we're no longer simply
looking at a very limited amount of
history
snapshots but in fact all market events
are available for analysis previously
due to limitations of storage and
computational devices we would basically
compute based upon samples and then make
inferences but the hope of this
exhaustive sampling is that with Big
Data there may be no sampling errors at
all big data is characterized by being
generated continuously seeking to be
exhaustive and fine-grained in its
structure examples the production of
such data include digital CCTV the
recording of retail purchases digital
devices that record and communicate the
history of their own usage such as
mobile phones the logging of
transactions and interactions across
digital networks like email or online
banking measurements from sensors
embedded within objects or environments
social media postings and the scanning
of machine readable objects such as
travel passes or barcodes the scale and
complexity of Big Data requires in turn
a change in computing models both in how
we structure data and how we process a
big data usually includes datasets with
sizes beyond the ability of commonly
used software tools to capture curate
manage and process within a reasonable
time data systems up until just a decade
ago almost completely structured
relational databases data was structured
into tables and columns but with the
rise of big data has come the evolution
of databases into a non relational form
or can be referred to as no SQL a no SQL
database provides a mechanism for
storage and retrieval of data that is
modeled in means other than the tabular
relations used in the relational
databases of the past in contrast to
relational databases where data schemas
are carefully designed before the
databases builds no SQL systems create
flexible data schema or no schema at all
for example one type of no SQL structure
is graph storage
graph data storage organizes data as
nodes which are like records in
relational databases and edges which
represent connections between the nodes
because the graph system stores the
relationships between nodes it can
support richer representations of data
relations also unlike relational models
which are reliant on strict schemas the
graph data model can evolve over time
additional technologies being applied to
Big Data include massively parallel
processing databases multi-dimensional
Big Data can also be represented
effectively as tensors making it more
efficient to handle through tensor based
computation
the digital universe consisting of all
the data we create annually is currently
doubling in size approximately every 12
months according to research by IDC it's
expected to reach 44 zettabytes in size
by 2020 that's 44 trillion gigabytes and
will contain nearly as many digital bits
as there are stars in the universe
likewise it is estimated that by 2030
more than 90% of this data will be
unstructured data this explosion of data
is of course far outstripping our
capacity to actually use it a small
fraction is in a traditional structured
form that is easily accessed and used by
organizations a larger section of big
data is unstructured but at least
somewhat accessible while the vast
majority is simply hidden altogether
going unseen and unused and this is what
we call dark data as Alice and Oh Cory
Oni of IBM notes 80% of all this data
that is created is dark unstructured
data data that the computers we've
developed in the past 40 years are not
able to analyze effectively we miss 80%
of the knowledge inside this data few
organizations have been able to exploit
non-traditional data sources such as
audio image of video files the growing
flow of machine and sensor information
generated by the Internet of Things and
the enormous stores of raw data found in
the unexplored depths of the Deep Web
these all constitute dark data as an
example we can think of supply chain
data a recent Gardner survey found that
85 percent of respondents felt that
supply chain complexity is now a
significant and growing challenge for
their organizations
supply chain is a data driven industry
spanning across a network of global
suppliers distribution channels and
customer base this industry churns our
data in huge numbers given that an
estimate
did only 5% this data is being used
there is ample opportunity for Big Data
technologies to bring this 95% dark data
to lights dark data was a term coined by
the IT consulting firm Gartner who
defined it as the information assets
organizations collects process and store
during regular business activities but
generally fail to use for other purposes
for example analytics business
relationships and direct monetizing
similar to Dark Matter
physics dark data often comprises most
organizations universe of information
assets data may be considered dark for a
number of different reasons because it's
unstructured because it's behind a
firewall on the internets it may be dark
because of speed or volume or because
people simply have not made the
connections between the different data
sets in many organizations large
collections have both structured and
unstructured data sit idle on the
structured side is typically because
connections haven't been easy to make
between disparate data sets that may
have meaning when combines especially
information that lives outside of a
given system business unit or function
which traditionally think and look
inside of silos and that's how we deal
with data making it difficult to bring
different data silos together regarding
traditional unstructured data such as
emails messages documents logs
notifications etc these are often
text-based and reside within
organizational firewalls but remain
largely untapped this may be because
they do not reside in a relational
database or because until relatively
recently the tools and techniques needed
to leverage them effectively such as
text mining did not exist buried within
these unstructured data sets could be
valuable information for organizations
the second dark analytics dimension
focuses on a different category of
unstructured data that can't be mined
using traditional analytical techniques
such as audio still image and video
files from other
sources much of the world's information
is now being created in rich media such
as images and video but computer
scientists have long since view video as
the dark matter of the internet universe
because they did not have the tools to
analyze it in 2016 alone we took an
estimated 1 trillion photographs in the
past this was simply unstructured data
we couldn't use there were just
collections of thoughts of color without
meaning and the same was true for video
unless someone put a tag on it to
describe the content in text it was
effectively dark data it's only very
recently with advances in machine
learning and image recognition methods
that this dynamic is changing googles
video analytics API can now go through
every scene in a video and identify
specific elements in those scenes such
as a dog birthday cake a mountain or
house a search engine can then be
implemented to look through these videos
to identify specific features and when
they show up in the video thus
converting the dark data into light data
as a major dimension to dark analytics
the Deep Web owns what may be considered
the largest body of untapped information
the Deep Web consists of information
that is not indexed by public accessible
search engines today according to a
study published in Nature Google throws
up about 16% of the surface web popular
science described it like fishing in the
top two feet of the ocean it's
impossible for us to accurately
calculate the depth of the web size but
by some estimates it's 500 times larger
than the surface web that most people
search every day the domains sheer size
and distinct lack of structure makes it
a daunting me complex adventure data are
curated by academics consortium
government agencies communities and
other third party domains medical
records legal documents scientific
documents multilingual databases
financial information government
resources organizations specific
databases all are hidden from outside
usage and largely unknown to anyone but
their owners to date companies have
explored only a tiny fraction of the
digital universe for analytic value the
term dark analytics refers to turning
dark data into intelligence and insight
that an organization can use dark
analytics seeks to remove these
limitations by casting a much wider data
net that can capture a mass of currently
untapped signals recent advances in
computer vision pattern recognition and
cognitive analytics are making it
possible for companies to shine a light
on those untapped sources and derive
insights new dark analytics companies
like Deep Web technologies
build search tools for retrieving and
analyzing data that would be
inaccessible to standard search engines
another example is lattice data recently
purchased by Apple which is a company
that applies an AI enabled inference
engine to take unstructured dark data
and
structured and more usable information
when Hurricane Katrina was about to hit
the coast the United States a large
retailer did a study to prepare
themselves by asking what products they
might sell out of and what they should
stock up on a room full of intelligent
and experienced executives thought
through what those products might be and
came up with reasonable answers such as
flashlights batteries water canned food
sandbags and more but when they ran the
data and analytics the number one
product turned out to be Budweiser beer
this is the power of data to illuminate
insight to take us beyond intuition and
help us make data empowered decisions
and has relevance for most everything we
do more and more of our actions and
interactions with the world's are
becoming mediated by data this alters
how we interact and the choices we make
understanding and seeing data can
completely changed ranking of a set of
options available to us and hence how we
allocate our resources both as
individuals and collectively almost
everything can be tested measured and
improved and this is truly bringing
about a quiet but fundamental cultural
transformation in how we make decisions
data fication brings about a more
objective form of decision making what
is called data-driven decision-making
for example when it comes to choosing a
movie we used to go to the store and
pick up the movie browse through all the
titles read the description and decide
which one we wanted to see now we're
confronted with algorithms that make
recommendations based upon data from the
last films that we've seen as well as
who our friends are what films they have
seen at nights and the aggregation of
feedback from thousands of millions of
other users Madeleine McIntosh from a
book publishing house talked about the
culture of publishing changing with the
arrival of Amazon's data-driven approach
the traditional culture publishing was
what she called their culture of lunches
a culture of conversations where people
had hunches and ideas about books
and then discuss them Amazon then
brought a data-driven numbers and math
driven approached this decision and was
able to basically figure out much better
what was working and what wasn't working
with the result being that they have
essentially taken over the market this
transformation is happening in many
areas of our economy more traditional
companies are being displaced by
companies that have embraced this new
technology and the cultural paradigm of
data think about wine tasting which you
might think of as a quintessentially
human skill there are human experts who
look at and smell the wine to tell you
what it tastes like and if it's of good
quality this is the highly refined skill
and sensory ability but it's also true
the wine is at the end of the day to
certain molecular composition and you
can analyze that with numbers the wine
analytics company analytics have been
able to figure out that you can predict
how an expert will rate it before
they've even tasted the wine with
remarkable accuracy and this applies to
more and more spheres of life Wall
Street is no longer full of people on
seats making trades based on intuition
and hope but up to 70% of those
decisions are now made by algorithms
acting on data
likewise decisions on healthcare
diagnostics are increasingly made by a
listicle systems sports decisions are
based on big data extracted from cameras
around the court or pitch and sensors in
the shirts of players the implicit
premise of big data is that decisions
can be made based fully upon data and
computerized models shifting the locus
of decision-making for people and
institutions to data and former models
bill Schmid's o from EMC describes well
how decisions are currently made based
upon management's gut feeling one of the
most critical aspects of Big Data is its
impact on how decisions are made and who
gets to make them when data is scarce
expensive to obtain or not available in
digital form it makes sense to bet
well-placed people make decisions which
they do on the page
of experience that built up and patents
and relationships they've observed an
internalized intuition is the label
given to this type of inference and
decision-making people state their
opinions about what the future holds
what's going to happen how well
something will work and so on and then
plan accordingly the term hippo is an
acronym now used to describe this type
of corporate decision-making process
where the highest-paid person and the
room gets to make the final call much
for approached decision-making has been
a function of simply not having data and
not knowing in the past we've had to
make decisions about complex
environments and complex systems without
being able to see or know what they're
really like just based upon some
intuition but big data analytics offers
this new telescope with which to
actually see these systems and the
difference between having a hunch and
actually seeing the data can be huge in
terms of the decisions that get made
every minute the world loses an area
forests the size of 48 football fields
and deforestation in the Amazon basin
accounts for a large share of this
contributing to reduce biodiversity
habitat loss climate change and other
ecologically devastating effects but
better data about the location of
deforestation and human encroachment on
forests could help governments and local
stakeholders respond more quickly and
effectively a project called planets is
currently developing the world's largest
constellation of Earth imaging
satellites
it will soon be collecting daily images
of the entire land surface of the earth
at 3 to 5 meter resolutions while
considerable research has been devoted
to tracking changes in forests it
typically depends on coarse resolution
images furthermore these existing
methods generally can't differentiate
between human causes of forest loss and
natural causes this project planets is
challenging the analytics community to
develop machine learning models for
labeling satellite image grips with
atmospheric conditions in various class
as of land cover and land use types
resulting algorithms will help to better
understand where how and why
deforestation happens all of the world's
a much clearer image of this complex
system would enable action orientated
decisions take place the switching the
dynamic from hunches guesses and
intuition to that of a data-driven
decision-making approach data holds a
huge potential to revolutionize how we
make decisions to shake up existing
inert patterns of thought and action
taking to overthrow unquestioned bias to
question established assumptions but
data also has its limitations and this
will we'll look at in the next module as
we go further into the conceptual
foundations of the big data paradigm
talking about what's come to be called a
tourism the belief in data an explosion
in the production of big data along with
the development of new analytical
methods is leading many to argue that a
data revolution is underway that has
far-reaching consequences for not only
how business is conducted and governance
enacted but the very nature of how
knowledge is produced within society
this is because big data analytics
enables an entirely new epistemological
approach for making sense of the worlds
rather than testing a theory by
analyzing relevant data new data
analytics seeks to gain insight that
simply emerges from the data itself
without apparent interpretation being
imposed upon the data itself this idea
was expressed in a somewhat provocative
way in a 2008 article by Chris Anderson
of Wired magazine where he argued that
big data analytics signals a new era of
knowledge production characterized by
the end of theory he wrote that the data
deluge makes the scientific method
obsoletes that the patterns and
relations contained within big data
inherently produced meaningful and
insightful knowledge about complex
phenomena essentially arguing
the big data enables a more empirical
model of knowledge creation as terabytes
and petabytes of data allow us to say
correlation is enough we can simply
analyze the data without hypotheses
about what it might show as he writes we
can throw the numbers into the biggest
computing cluster the world's ever seen
and that statistical algorithms find
patterns where science could not
correlations supersedes causation and
science can advance even without
coherent models unified theories already
any mechanistic explanation at all
there's no reason to cling to old ways
Anderson's article is a flamboyant
elaboration of what has come to be
called day tourism day tourism may be
recognized as the general underlining
philosophy of big data which holds data
as a primary source of truth in its own
right
big data offers the possibility of
shifting from static snapshots to
dynamic flows from coarse grain
aggregations to high resolutions from
data scarce to data rich from relatively
simple models to more complex
sophisticated simulations this
all-encompassing pervasive fine-grained
nature to big data takes us into a new
kind of paradigm where we could at last
as data ISM would hold access the world
without any kind of mediation directly
in the language of ones and zeros in its
capacity to present us with raw facts
data may take us beyond our intuition
assumptions bias prejudice and other
distortions but at the same time data
can be deceptive hiding behind a veil of
objectivity while excluding the
relevance of context thus if we want to
really push what we can do with data
analytics need to be aware of where its
limitations lie and how this paradigm of
big data works the trishal way we've
done science is by creating hypotheses
we then go into the data to test that
hypothesis traditionally statistics is
aimed at firstly at the discoverer
of pre-existing hypotheses but the very
idea of data mining is not determined
pre-existing hypotheses but make
hypotheses surface from the data itself
so hypotheses or categories do not pre
exist the collection and processing of
data our hypothesis instead is the
results of the processing of data which
reverses the traditional more
theoretical approach what people
increasingly want now are tools that
find interesting things about the data
what is called data-driven discovery the
analyst does not even have to bother
proposing a hypothesis anymore
the argument is the mining big data
reveals relationships and patterns that
we did not even know to look for Rebecca
Siegel in a 2013 paper states this as
such we usually don't know about
causation and we often don't necessarily
care the objective is more to predict
than it is to understand the worlds it
just needs to work prediction Trump's
explanation as an example we can take
the case of a retail chain that analyzed
12 years worth of its purchased
transactions for possible unnoticed
relationships between products that went
into shoppers baskets discovering
correlations between certain items with
new product placement and a 16% increase
in revenue per shopping cart in the
first month's trial there was no
hypothesis the product B was often
brought with product said that was then
tested the data was simply queried to
discover what relationships existed that
might have previously been unnoticed
amazon's recommendation system produces
suggestions for other products a user
might be interested in without
necessarily knowing anything about the
products itself it simply identifies
patterns of purchases across customer
orders while it might be interesting to
explain why these associations exist
within the data such explanation is
often seen as largely unnecessary in a
world of Commerce where all the matters
are outcomes there is a comprehensive an
attractive certified
years of work in the data paradigm that
runs contrary to the traditional
deductive approach that is in many ways
dominant within modern science the basic
premise is that because big data can
capture a whole domain providing a
complete high-resolution data set there
is no need for prior theory models or
hypotheses as through the application of
data analytics the data can speak to
themselves free of human bias or framing
meaning transcends context or
domain-specific knowledge and is thus
neutral being able to be interpreted by
anyone as Yuval nor Harare
in his book home ideas a brief history
of tomorrow writes for politicians
businesspeople and ordinary consumers
data ism offers groundbreaking
technologies and immense new powers for
scholars and intellectuals it also
promises to provide the scientific Holy
Grail that is eluded us for centuries
single overarching theory that unifies
all the scientific disciplines from
musicology through economics to biology
according to data ISM Beethoven's fifth
symphony a stock exchange bubble and the
flu virus are just three patterns of
data flow that could be analyzed using
the same basic concepts and tools we can
already see how this idea of the
universality of data is becoming applied
as small groups of mathematicians
physicists computer scientists and data
analysts come to be incorporated into
more and more domains from finance to
business consulting to energy providers
and all forms the technology companies
which implies that there is a single
language of data that applies to all
equally so what are the limitations of
this data paradigm data ism is an
extension of the empirical and
reductionist paradigm in the age of
information reductionism is the idea
that a system any system is nothing more
than the sum of its parts
it is to say that nothing is truly
continuous everything can be rendered
into a discrete quantifiable format
without
any loss of contents this is of course
what deta fication does all data is
discrete in that it takes a section of
the universe and sticks a label or value
on its presenting it as in some way
separate from everything else and thus
making it possible to move around and
process into new configurations through
analysis we break systems down to
isolate component parts quantify them
and describe the whole is some
combination of those parts reductionism
has many great achievements but it also
has its limitations it systematically
deeper modes complex relations context
and continuous processes it takes no
account of emergent phenomena the
results in irreducible whole systems and
processes it can tell us about the
billions of neurons in the brain but not
about consciousness it can tell us about
the molecular makeup of water but not
why when we combine the molecules they
create something that has the property
of being wet
likewise data is objective and it's
discrete it can't tell us about what is
subjective and continuous just as it
can't tell us about emergent whole
systems it can't tell us about what is
subjective and continuous the discrete
nature of data is why it's so useful it
means that we can take it separate it
from the world and put it into an
algorithm to manipulate and interpret it
in new ways but it's precisely that that
is its inherent limitation it can't tell
us about the synergies between the parts
that make them continuous more than the
sum of those parts and irreducible to
those discrete units data fication gives
us a new tool to look at the worlds but
the problem is that that tool is
incomplete as convincing as it appears
reductionism is only ever half of the
story by its inherent nature it lets us
see some things and not others the risk
of this though is that we stay looking
under the street lamp because that's the
only place the data sheds lights i
forget about everywhere else that it
doesn't shed lights such an incomplete
interpretation of the world's can only
ever lead
two incomplete outcomes and ultimately
unsustainable results the technology
ethnographer trisha wang describes as
well when she writes that's why just
relying on big data alone increases the
chances that will miss something while
giving us the solution that we already
know everything we have this thing that
I call the quantification bias which is
the unconscious belief of valuing the
measurable over the immeasurable but the
problem is the quantifying is addictive
and when we forget that and when we
don't have something to kind of keep
that in check it's very easy just to
throw our data because it can't be
expressed as a numerical form this is a
great moment of danger for any
organization because oftentimes the
future we need to predict it isn't in
that haystack but it's that tornado
that's bearing down this outside the
barn this analogy of the haystack and
the barn touches on an important idea
which is that analytics helps us to
better understand what is inside of the
box and how the Box works but it can't
help us in seeing what is outside the
box for that you need a very different
process of reasoning called synthesis
which we'll be talking about again in a
later module
with information technology we've gone
from a world that was private by
defaults to a world that is public by
defaults in a pre-digital world's our
lives were primarily private by defaults
by the fact that most of our
communications were not mediated by
tools of mass communications that our
conversations were bounded by the
physical location and thus it took extra
resources and effort to publish publicly
now it takes extra effort to make things
private this massive amount of data that
is being generated by people can of
course be used for beneficial outcomes
or for detrimental outcomes this social
data can be used by researchers to
understand society better it could be
used for beneficial security reasons it
can be used to enhance public services
but there are growing concerns that our
data is or might be used against us in a
multiplicity of ways today our platform
societies are engaged in a dubious and
questionable relationship with their IT
platform providers if the interests the
platforms that control this data were
always aligned with the interests of the
user then there would be no great
concern but the issue arises when the
two are misaligned which often happens
given that these platforms of private
organizations individual people give
over their information winning me to
online platforms in exchange for the
services they provide but few fully
appreciate the negative externalities of
this when taken on aggregates the
ubiquity and complexity of surveillance
are very difficult for people to grasp
for example one of the largest data
brokers Axium claims to have fifteen
hundred pieces of information on 200
million Americans while the company Hunt
says that it can predict people's
consumer preference from just five data
points about them our private
information is being traded all around
us and ever more sophisticated
technology is being used to predict an
alter our behavior without her even
knowing it as we become more digitized
we start to leave an endless trail of
data dust behind
that is hoovered up by companies and
used to predict and alter our every next
step with the next generation of
technologies the Internet of Things
advances in big data storage advanced
analytics and smart systems this data
economy will greatly expand and so too
will the predictive capacities of these
organizations creating a significant
imbalance of power
the first question though is why should
we care about privacy at all social
systems always engender a complex
dynamic between the group and the
individual between the public and the
privates functioning social systems
require a diversity of individuals that
are able to come together find
commonalities in coordinates both
differentiation of individual and
commonality of the holder required
individuality and diversity requires
subjectivity that is inherently private
it has to be developed autonomously this
requires individual space and privacy to
develop ways of being but others may not
like or may not be aligned with the
group social dynamics emerge out of the
interaction between the private and the
public it's an inherent part of how
social systems evolve through
individuals developing new solutions in
private to overthrow the commonly
accepted norms this is how we got rid of
slavery and achieve women's rights when
these movements were started the Society
of the time would have rejected them if
they had not had the private space to be
incubated without that privacy there
will be no incubation of new solutions
limited diversity and societies have
become stagnant the same is true today
for gay marriage in marijuana they're
currently in the process of being
legalized around the worlds we would
never have got there in a world of
perfect information and complete
surveillance Frank griga of the Chaos
Computer Club states it clearly when he
says if you have your privacy guaranteed
technically or politically then you can
come up with political descents ways to
change the world when you don't have
privacy when the things you do are
completely transparent
knowable and predictable then you cease
at that points people name places where
they can be free from the judgment of
others in order to develop
underdeveloped subjective dimensions
themselves to develop inch new people or
for societies to develop into new
societies privacy is a space where
creativity exploration and dissent can
thrive when we do away with privacy we
limit those valuable resources required
to sustain a society a measure of how
well a society is doing is not how well
it treats its favoured compliant and
obedient citizens but how well it treats
its dissenters and rebels mass
surveillance dampens our freedom in all
sorts of systemic ways that are largely
unnoticed it removes many behavioral
choices without us even really being
aware that they've been excluded from
our options there's plenty of research
to corroborate the fact that when people
are in a public setting where their
behavior is being observed or they know
that they might be watched in some way
by others the behavior they undertake is
greatly more conformist in compliance
human shame is a very powerful motivator
shaping behavior towards conformity when
people are being watched people make
decisions that are not necessarily of
their own innate agency and are in fact
the expression more of the will of
others and their societies orthodox
surveillance creates a conceptual set of
constraints and conformity the idea that
government regulation is somehow going
to solve the privacy equation is
somewhat naive given the current context
it's important to appreciate that the
power of the technological process of
change that's underway far outstrips our
existing institutional capacities deal
with it professor Josie Van Dyck of
Amsterdam University notes this when she
says what's at stake here it's not one
platform or one thing it's the
credibility of the system in which
commercial and state public interests
are becoming increasingly intertwined
and very hard to discern the core of the
problem of the paradox is that public
values are known
rooted in public institutions
deinstitutionalization deregulation
globalization have really caused the
erosion of what public value is all
abouts
I think and I regret the public
institutions are alarmingly
underprepared for the questions raised
by this global information influx if
you're serious about finding solutions
to such issues as privacy then you
really need to be working with the
technology and not against it there is
clearly a shift that needs to take place
in data ownership and privacy for the
platform economy to arrive at a more
sustainable model the current model
where data becomes public the property
of private organizations and stored in
centralized data centers by defaults
needs to change to one where it becomes
the property of the individual and is
public to the extent that it needs to be
public thus reducing risk and negative
externalities the blockchain can and
probably will play a central role in
this the next round of Internet
applications built on the blockchain can
enable the creation of distributed
social platforms without centralized
management where data is secured and
owned by the user with it then being up
to them as to when and how they share
that data the blockchain as she gives us
the technological means to build
platforms that were takers to a much
more sustainable data future the
algorithmic regulation in the form that
is currently emerging in contemporary
modern democracies seems to be providing
a one-way mirror that allows
institutions
look down to survey those below but
those below have no real prospect of
peering into let alone understanding and
challenging these algorithmic black
boxes that increasingly regulate our
lives but it doesn't necessarily have to
be like this as Kevin Kelly of Wired
magazine says a central choice now is
whether this surveillance is a secret
one-way panel tagging or a mutual
transparent kind of covalence that
involves watching The Watcher through
open source software and blockchain
technology we can shift this balance of
data power from
centralized institutions to individuals
to build systems that put data in the
hands of people make it secure and only
accessible under their consents we can
now build systems that have a two-way
transparency and accountability to them
the term algorithm is currently making a
massive rise to fame an ancient Greek
term that was previously confined to the
world of mathematics and software
engineering is making its way into the
mainstream as people are increasingly
recognizing the material impact on
society that algorithms are having today
algorithms they used to be buried away
inside of computer program files that we
use to find a derivative of a slope or
to find the shortest path between two
locations have today expanded to affect
almost all areas of human activity
algorithms for determining the value of
a basketball player based upon a
computerized analysis of their
performance last season algorithms that
analyze the incoming customer service
calls and route them to the most
appropriate agent algorithms determining
the likelihood of a convict reoffending
for analyzing insurance claims for
coordinating the nightly maintenance on
a mass transit system for driving cars
identifying symptoms algorithms to
determine which candidate a company
should hire who should be recommended as
a friend on social media or what films
books or music someone would like and of
course algorithms have taken over
financial markets now making up 70% of
trades as stock markets have become
layers upon layers of algorithms an
algorithm is a set of instructions for
performing a certain operation an
algorithmic system takes an input and
transforms it through a set of
operations to create an out boats
cooking a loaf of bread may be seen to
follow an algorithm where we take in
inputs such as flour water salt etc and
perform a set of operations on them such
as mixing kneading baking etc to create
an output which is the cook bread
algorithms all missus oldest human
civilization itself Euclid's algorithm
being one of the first examples dating
back some 2,300 years but what we're
doing with them today is very different
from what we did in the past which is
largely strict formal mathematical
operations and limited statistical
analysis algorithms are being
transformed from the mechanistic linear
forms of the past where we specified all
the rules and hard-coded them with the
end result
looking like cogs in a gearbox today
we're algorithms take a more networked
form they're more self-organizing as
they learn from data these new forms of
algorithms take many different names
from cognitive systems to artificial
intelligence to machine learning Fei Fei
Lee of Google Cloud describes some of
the factors involved in this
transformation around 2010 around that
time thanks to the convergence of the
maturing of statistical machine learning
tools the convergence of Big Data
brought to us by the internet in the
sensors and the convergence of computing
the Moore's law carried us to much
better hardware these three pillars came
together and lifted AI from the in vitro
stage to what I called the in vivo stage
AI in in vivo is where AI is making a
real impact on the world's it's just the
beginning every single industry is going
through a transformation because of
cloud because of data because of AI a
machine learning and this is what I see
as the historical moments but I also
want to say that this is just the
beginning these advanced algorithms
unlike the static mechanistic models of
the past are adaptive in nature they may
learn as information changes and its
goals and requirements evolve they may
resolve ambiguity and tolerate
unpredictability they may be engineered
to feed on dynamic data in real-time or
near real-time they are amenable to the
processing of unstructured data the
processing of millions of parameters and
complex patterns
such as speech recognition sentiment
analysis face detection risk assessment
fraud detection behavior recommendations
or sentiment analysis this means these
advanced analytical methods are no
longer confined to strict mathematical
operations but can handle more
unstructured human-like activities such
as many basic services the idea of a
computer is really just an abstract
model for a system the stores and
manipulates data according to a set of
instructions called the algorithms the
implementation of this model can take
many forms we're used to thinking of it
as the personal computer on our desktop
but with the rise of mobile computing
the internet and cloud computing this
computing is becoming pervasive but also
integrated through these cloud platforms
cloud computing platforms have been a
key innovation of the past decades
although a relatively straightforward
idea of centralizing computer resources
in the data center and then delivering
them as a service over a network the
outcomes are doing this though are
extremely impactful these new forms of
advanced algorithmic methods that we'll
discuss in coming modules are very
compute intensive the demand the
computational resources does not scale
linearly with the size of the data but
scales quadratically or cubically with
the size of the data being operated on
and when you're talking about billions
of data points that causes new
computational problems and we need new
computing platforms to mitigate em
throughout human history computing power
was a scarce resource and until the past
few years high-end computer processing
and storage offerings were out of the
reach of all except the largest
organizations and then at the cost of
millions of dollars however with the
advent of global hyper scale cloud
computing high-end computing is now
available to organizations of almost all
size at low cost and on demands the
arrays of billion-dollar scale data
centers owned and operated by Amazon
Google and Microsoft are now
the fingertips of many many of the
largest applications on the Internet
today run on this cloud computing
infrastructure
take for example Airbnb that now
coordinates an average of half a million
people's accommodation each night in
65,000 cities with this platform running
almost entirely on Amazon's Web Services
cloud likewise each month
Netflix delivers a billion hours of
video streaming globally by running on
Amazon Cloud indeed Amazon's Web
Services is so widely used that when it
doesn't work right the entire Internet's
in jeopardy a basic driver behind many
of the recent disruptions in a wide
range of industries is the
transformation of computing resources
from a scarce to an abundant resource
combining cloud computing with advances
in algorithms and mobile computing we
get machine learning platforms that are
able to coordinate and run over larger
and more complex service systems this
allows an increasing swath of human
activity to be captured by algorithms
which allows it to be split apart
transformed alters recombined and
automated these platforms bring about an
ever growing integration between
technology and services as data and
information processing become more
pervasive and computation becomes
embedded within virtually all systems
traditional divides are going to become
ever more blurred information technology
and socio-economic organization will
become ever more integrated and
inseparable as the saying goes every
company will become a technology company
and this will fundamentally change the
structure and nature of these
organizations what is happening today is
the convergence of these cloud computing
platforms new algorithms and the rise of
the services economy recent years have
seen the emergence of physical products
that are digitally networked with other
products and with information systems to
enable the creation of smart service
systems which are coordinated their
algorithms what is happening as we move
into the
serves economy is the products become
commoditized people stop owning their
own things what they want is to be able
to push a button on their smartphone and
the thing delivered is a service an app
for food services an app for transport
services an app for accommodation etc
and of course all of these services are
delivered on demand via cloud platform
which is coordinated via advanced
analytics services are not like products
whereas products are mass-produced
services have to be personalized
products were static once-off purchases
services the processes products were all
about things services are about
functionality and value service systems
are all about the coordination of
different components around the
end-users specific needs to do that you
need lots of data about the user you
need advanced analytics and cloud
computing we can already see the data
driven services organization in the form
of uber Alibaba or DD which don't own
anything they just use data and advanced
analytics within their platforms to
coordinate resources towards delivering
a service service companies like DD will
be impossible without data
dematerialization is one of the key
aspects of the Information Age material
products become commoditized data and
information I used to strip physical
technologies down to their most basic
material quirements value-added shifts
to the organisation of systems rather
than the production and ownership of
physical assets this is seen with the
rise of platforms over the past decades
which are really large networks they use
data and analytics to optimize systems
as the venture capitalist Steve
Jurvetson put it in the past the thing
mattered now it's all about the software
and services there
reduce the physical thing to its
minimalist thing for a container for
software and codes and that's what's
happening in more and more products and
services the thing that every business
makes is becoming a software products in
the long run everything will cost a
dollar a pound
things and what people will pay for and
value is the software and services that
come around it it's what makes every
product magic I think what you're seeing
as common practice in the IT centric
industries of today in software and
computers what was the telecom
transition of years past will be the
case for every industry the key question
is when and in what sequence some like
agriculture and healthcare are in the
middle or early phases of that
transition but every industry will
inevitably compete on how they process
information that's how they'll win or
lose and the transition will not be easy
for some what will differentiate one
company from another is not how fancy
their product is but how seamless and
integrated their service system is and
this is done through their capacity to
master data and analytics organizations
will become platforms and will compete
based on their intelligence which will
be contained in their algorithms and
people in short the physical
technologies of the Industrial Age are
being converted into services and
connected to cloud platforms where in
advanced analytics coordinate them as
Matt Turk a first mark puts it
succinctly everything becomes data your
physical activities traffic purchases
and the data gets moved to the clouds
but it gets processed and compared with
other devices it's no longer just what
you do but what everyone else also does
which keeps making the system smarter
and smarter this is the essence the
process were going through today data
fication converting everything into data
cloud platforms for aggregating and
running the machine learning for
processing air and iterating on that
through services ation and
dematerialization organizations become
differentiated based on their data and
algorithms as algorithmic systems extend
to coordinate more and more spheres of
human activity and we move further into
this unknown world to the information
age
at a recent Google cloud conference Rob
craft product need for cloud machine
learning got up on stage and told the
crowd onwards of nine years ago we got
out of the rules business everyone in
this room probably writes rules for a
living if you write codes if this then
that those are rules if the following
things are met the following things
should execute the stored procedure sees
this the stored procedure writes that's
those are all rule-based systems what if
you are able to declare through a
statistical model here's what good looks
like and the confidence that good is
this thing and why doesn't the system
then determine on its own how it should
determine to get to that good thing
that's what a predictive type of system
tries do what he's describing here is
the shift that's taken place over the
past decade towards machine learning
becoming an ever more popular method for
building software systems machine
learning has seen explosive growth over
the past decade and applications within
many different areas for example the
machine learning algorithms on yelps
websites help the company's staff to
compile categorize and label images more
effectively machine learning
applications are being used at Facebook
to filter out spam and poor quality
content and the company is also
researching computer vision algorithms
that can read images to visually
impaired people Bao dos R&amp;amp;D lab uses
machine learning to build what the
company calls deep voice a deep neural
network that can generate entirely
synthetic human voices that are very
difficult to distinguish from genuine
human speech machine learning refers the
process through which a computer can
construct an algorithm based upon the
analysis of data such algorithms
overcome following strict static program
instructions by making data-driven
predictions or decisions by building a
model based upon sample inputs machine
learning is employed in a range of
computer tasks were designing and
programming explicit algorithms with
good performance are difficult or in
fees
born in such cases we tell the computer
what we want the output to be and then
it builds a model based upon the data
that we'll be able to reproduce those
results when presented with new data
sources to process machine learning can
largely be characterized as an
optimization process over some set of
data to solve any machine learning
problem we want to find a metric that
tells us how far we are from the
solution and try to minimize that value
minimize the error which is called the
loss function the formal definition of a
machine learning system is stated as
such a computer program is said to learn
from experience II with respect to some
class of tasks T and performance P if
it's performance at task T as measured
by P improves with experience II which
essentially just means that the computer
is given a task and some metric for
success and with each iteration over the
task it is performing it gets better at
doing it as measured by the performance
metric for example Google used the
machine learning algorithm to
drastically reduce the electrical
consumption is datacenters using a
system of neural networks trained on
different operating scenarios and
parameters within their data centers
they created an efficient and adaptive
framework to understand data center
dynamics and their optimization
efficiency they accomplished this by
taking the historical data that had
already been collected by thousands of
sensors within the data center and using
an to train a set of deep neural
networks the machine learning system
analyzed the internal arrangement within
the data center and tried different
configurations to assess the efficiency
of energy consumption it stays iterating
adjusting the configuration and trying
to reduce the value and learning at each
iteration also need the algorithm
managed to reduce the amount of energy
used for cooling by up to 40 percent
this is the key to most machine learning
problems you take the problem and
minimize the error
by using gradient descents trying
different options to see which reduces
the output error by the most and then
iterating on that process machine
learning systems are typically
categorized as being supervised door
unsupervised the biggest difference is
that supervised learning deals with
label data while unsupervised learning
deals with unlabeled data labeled data
is a group of samples that have been
tagged with one or more labels the
process of labeling typically takes a
set of unlabeled data and attempts to
apply meaningful tags to that data that
are informative of its contents for
example those labels might indicate
whether a photo contains a mountain or a
lake or the type of action being
performed in the video what the topic of
a news article is what the overall
sentiment for tweet is labeling can be a
time-consuming exercise that is often
done by humans after obtaining a label
dataset machine learning models can be
applied to that data so that new
unlabeled data can be presented to the
model and a like label can be guessed or
predicted for the piece of unlabeled
data automatically by the algorithm
techniques the come work with unlabeled
data are called unsupervised learning
with unsupervised learning we're trying
to get the machine to find and create
different categories within the data in
an unsupervised approach you're trying
to build a prediction but you don't
actually have the outcome as a reference
for training the algorithm but we let
the model work on its own to discover
information that may not be visible to
the human eye clustering algorithms are
one such example where a set of inputs
is to be divided into groups this
involves the analysis of patterns and
sets of unlabeled data to find groups
that are similar unsupervised learning
is important because most of the time
the data that we get in the real world
doesn't have little tags attached to it
to tell us what it is you need to
perform some kind of analysis before
going any further with supervised
learning
computer is presented with example
inputs and their desired outputs given
by a teacher and the goal is to learn a
general rule the map's inputs outputs
we do this by training the model that is
we load the model with knowledge so that
we can have it predict future instances
for example we teach a model by training
it with some data from a labeled data
set and then provide it with new data
for it to try and match the original
labels main types of supervised learning
a classification and regression spam
filtering is an example of
classification where the inputs are
email messages and the classes are spam
or not spam likewise one might feed the
system a data set of flowers to have it
identify the different types a core
objective of machine learning systems is
to generalize from their experience
generalization in this context is the
ability of a learning machine to perform
accurately on new unseen examples or
tasks after having experienced a
learning data set in the past the
training examples come from some
generally unknown probability
distribution and the system has to build
a general model about this space that
enables it to produce sufficiently
accurate predictions in new cases the
key aspect of machine learning that
makes it an important method with
respect to Big Data is that we don't
have to hard-code pre specified rules
the iterative feedback aspect of machine
learning is important because as models
are exposed to new data they're able to
independently adapt and evolve they
learned from previous computations to
produce reliable and repeatable
decisions and results there are many
different approaches to building machine
learning systems in the next module
we'll give an overview to some of the
primary approaches taken
machine learning is a very challenging
area of computer science and engineering
and there are many different approaches
to building machine learning systems as
yet there's no real classification for
these different approaches but in his
book the master algorithm Pedro
Dominguez of the University of
Washington provides a coherent and
accessible overview to the different
methods currently being pursued the book
describes five what he calls tribes
which each emphasize a different method
of machine learning with each being
particularly well suited to solving for
some core challenge
Domingos begins by identifying five
basic methods through which a computer
can build a model and then associates
these with the different approaches
currently taken to machine learning
first is filling in gaps in existing
knowledge through inverse deduction
secondly mimicking the human brain which
is associated with the connectionist
neural network approach thirdly
evolutionary selection which is
associated with techniques that enable
the computer to simulate evolution
fourth is reducing uncertainties through
statistics and basin in parents and
lastly making contrasts between old and
new sets of information through the use
of analogy the simplest approach is said
to operate on the basis of formal logic
and more specifically the premise of
inverse deduction the approach is to
think of deductive learning as being the
inverse of deduction deduction is going
from general rules to specific facts the
opposite of that is called induction
which is going from specific thanks to
general rules for example if we can
figure out that two plus two is four
then we can also fill in the gap in the
equation where we know that we have to
and have to find what we have to add to
that to get for the system has to ask
itself what is the knowledge that is
missing and acquire that knowledge
through the analysis of existing data
sets a second approach is based upon the
network structure of the brain and how
the brain learns
through encoding patterns within neural
networks this is the neural network
approach an artificial neural network is
an interconnected group of nodes akin to
the vast network of neurons in the brain
here each circular node represents an
artificial neuron and an arrow
represents a connection from the output
of one artificial neuron to the input of
another the network is trained on data
so that a specific set of connections
between the nodes forms to represent a
pattern the weight of the connections
between the nodes is altered with each
iteration so the outputs of the system
better matches the desired output for
example Google used this approach to
train its computers to identify cats in
YouTube videos much of the breakthroughs
a machine learning in recent years have
come from this approach and because it's
well suited for dealing with big data
we'll be looking more closely at how
neural nets and deep learning methods
work in the coming modules another very
important approach is that of trying to
simulate the process of evolution
genetic algorithms weren't the way
evolution does through the production of
variety the exposure of those variants
to an operating environments and then
selection cross mixing duplication and
iterating on the whole process you have
a population of individuals each of
which is described by specific
characteristics and then each of these
individuals goes out into the worlds and
is evaluated based upon its success at a
given task those that perform well gain
a payoff of a higher Fitness value and
will therefore have a higher chance of
being parents of the next generation
individuals that have performed well
cross makes and random mutation is added
to create a new population and the
process is iterated on after some number
of generations of this you actually have
some things that are doing non-trivial
functions indeed algorithms can learn
surprisingly powerful things this way
the Bayesian approach deals with
uncertainty through probabilistic
inference Bayesian inference is
a statistical appearance in which
bayes's theorem is used to update the
probability for hypotheses as more
evidence or information becomes
available you create a hypothesis that
there'll be some outcomes that are more
likely then update the hypotheses as
more data comes in after some iterations
of this some hypotheses become more
likely than others the Bayesian approach
could be used for example in filtering
messages the system has a bag of words
to identify certain categories of
messages it then goes through the
message and every time it finds more
evidence confirming or disconfirming a
hypotheses it adjusts the probability of
what category the message might fall
into Bayesian ideas have had a big
impact on machine learning in the past
20 years or so because of the
flexibility they provide in builds in
structured models of real well phenomena
algorithmic advances and increasing
computational resources have made it
possible to fit rich highly structured
data models which were previously
considered intractable the fifth
approach is that of analogy analogy is a
powerful and fundamental tool that our
brains use to categorize new information
by comparing it to what we already know
to see how closely it resembles other
things and thus whether we can place it
into or near to a category that we
already know the general method is that
of the nearest neighbor principle
essentially asking what is the thing
closest to and then positioning it
indifferent to other things based upon
its similarity to them a popular method
here is support vector machine SVM given
a set of training examples each marked
is belonging to one or the other of two
categories a support vector machine
training algorithm builds a model that
assigns new examples to one category or
the other
this approach is at the heart of a lot
of outcomes that are extremely
successful for some kind of machine
learning systems support vector machines
were probably the most powerful type of
learning that was common until recently
handwritten characters
can be recognized using SVM and support
vector clustering is often used in
industrial applications
Amazon's and Netflix recommendation
systems are based upon this method of
analogy if someone else has given five
stars to something you have and one
stars to something else that you've also
given one star to then by analogy the
system extrapolates out to recommend to
you something that that person with
similar taste you is liked the idea is
that each of these approaches has a
problem that they can solve better than
the others and it has a particular
master algorithm this solves that
problem for example the problem that the
symbol is solve that none of the others
know how to solve is the problem of
learning knowledge that you can compose
in many different ways and they learn
that knowledge with inverse deduction
connectionists solve the credit
assignment problem through the
development of complex networks where
individual nodes and connections are
adjusted based upon how well they
contribute to matching the desired
outputs as we'll discuss in the next
video the evolutionists approach solves
the problem of learning structures the
Bayesian approach can deal with
uncertainty the fact that all knowledge
that you learn is uncertain and it knows
how to update the probabilities of
hypotheses to better match to desired
outcomes the analogy approach uses a
comparison between things to categorize
them based upon similarities or
differences
artificial neural networks our computing
systems inspired by the biological
neural networks the brain such systems
can progressively improve their ability
to do tasks and recognize patterns by
learning from experience artificial
neural networks are in their essence
computational networks that can perform
certain specific tasks like clustering
classification or pattern recognition
they do this by representing patterns in
data as structures of connections
between nodes on the network they then
learned by altering the strength of the
connections between the nodes to create
new network structures that can
represent new patterns for example
neural nets are now widely used for
image recognition where they learn to
identify images that contain say a house
by analyzing example images that have
been manually labelled as such and using
the results to identify houses in other
images as the name implies they're
directly inspired and modeled on the
workings of the brain thus to understand
neural networks it is a value to
understand a little how the brain works
to represent and process information the
brain is composed of neurons and
connections between them called axons
which have synapses where the different
neurons meet neurons generate electrical
signals that travel along their axons
any given axon has a number of inputs
from other neurons
if those inputs are above a given
threshold then it is activated and fires
if a neuron is activated it then sends
out signals to other neurons that is
connected to the synapses change in
their chemical composition as one learns
in order to create stronger connections
between networks of neurons in such a
way the cognitive system can adapt and
change over time to form new patterns of
neural networks if two neurons are
turned on when a pattern is stimulated
then the synaptic connection between
them becomes stronger the brain is
physically builds as a neural network
and cognition happens in patterns
networks haven't
connected neurons form a pattern which
corresponds to an idea or memory an
artificial neural network then is based
on this very same architecture as
collections of connected nodes that form
a network each connection between nodes
can transmit a signal to another node
the receiving node can process the
signal and then signal to downstream
nodes connected to it they're all
connected to each other going down
through the layers the nodes mimic
neurons in that their little triggers
each neuron has something of a threshold
where it makes a decision nodes take
inputs from connected nodes and have
some internal function to determine when
or if they'll fire to send a signal
downstream the nodes and connections may
also have a weight which can increase or
decrease the strength of the signal that
is sent downstream which itself varies
as learning proceeds typically weights
represent the strength of the
connections between neurons inside the
neural network as such artificial neural
networks can be viewed as weighted
directed graphs in which artificial
neurons are nodes and directed edges
with weights are connections between
neuron outputs and noren inputs
typically neurons are organized in
layers different layers may form
different kinds of transformations on
their inputs signals travel from the
first input to the last output layer
possibly traversing multiple layers in
between the input layer contains those
units of artificial neurons which
receive inputs from the outside worlds
on which the network will learn or
process the output layer contains units
that respond to information that the
network has learnt a represent or
process those units between the input
and output layers are termed the hidden
layers a neural network with many layers
in between is called the deep learning
neural network when there are no layers
in between it's simply a neural network
because there very few practical
applications for a two layered neural
network virtually all useful
are going to be of the tea planning form
as of 2018 neural networks typically
have a few thousand to a few million
units and millions of connections these
different layers of the network can be
used to represent the different levels
of abstraction during classification
identification as we'll discuss further
in the coming module and deep learning
the key innovation required to actually
get a functioning neural network is what
we call back propagation this involves
the network learning through an
iterative process where you send all the
information back through the network and
the network adjusts its weights so as to
learn to match the output better the
basic idea in back propagation is quite
intuitive
it's simply looking for the difference
between the network's actual output and
the desired outputs the error in the
networks output is used to determine how
much the network adjusts and changes on
the next iteration so if we put in a
graphic of a circle and ask the system
to identify it and it outputs an
estimation that it was 0.8 likely to be
a circle we know that there is an error
of 0.2 we can then adjust the edges of
nodes up and down by a very little
amount to see how the error changes the
amount that they're adjusted is
determined by how big the error is with
a large error they're adjusted a lot
with a small error just a small bed with
no error they're not adjusted at all
we're doing this alteration to try to go
down the gradient to where the error is
the minimum we keep making these
adjustments the weight of the nodes and
connections all the way back to the
input and this is why it's called back
propagation because what we're doing is
propagating backwards the error and
updating the weights to make the error
as small as possible this back
propagation method is what is at the
heart of deep learning and these days is
used in just about everything here about
a machine learning very early on people
used it do things like predict the stock
market these days is used for search
automatically adding color to black and
white images for video recognition
for automatic handwriting generation for
generating captions on images the speech
recognition for simultaneous translation
social network filtering playing board
games and video games medical
diagnostics and many other applications
the idea of neural networks has been
around since the 60s but at that time
computers didn't have enough processing
power to effectively handle the work
required by large neural networks neural
network research slowed until computers
achieved far grating processing power
support vector machines and other much
simpler methods such as linear
classifiers gradually overtook neural
networks in machine learning popularity
Sony very recently that this has changed
as available computing power increased
through the use of GPUs parallel and
distributed computing nor networks are
now starting to be deployed on a large
scale through our industry they found
best usage in applications difficult to
express with traditional computer
algorithms using rule-based programming
neural networks are the basis deep
learning that has become highly popular
in the past years and we'll take a look
at this topic in the next module
deep networks are the current state of
the art in pattern recognition but they
build upon the decades old technology of
neural networks talked about in the past
video it took many decades after the
initial concept to arrive at functional
deep nets because they're very hard to
train the method suffered from an issue
called the vanishing gradient problem up
until around 2006 deep nets
underperformed relative to more basic
nets and other machine learning
algorithms but everything started to
change after three breakthrough papers
published at this time and today they're
the hottest topic and machine learning
deep learning is a machine learning
method based on neural networks what
distinguishes deep learning from the
more general approach of Nohr networks
is its use of multiple layers within the
network to represent different levels of
abstraction deep learning algorithms use
a cascading structure with multiple
layers of nonlinear processing units for
feature extraction and transformation
each successive layer uses the output
from the previous layers and inputs in
this way they learn multiple levels of
representation that correspond to
different levels of abstraction just
like neural networks deep learning
software attempts to mimic the activity
in layers of neurons in the neocortex it
uses multiple layers of nodes with each
successive layer using the output from
the previous layer as input varying
numbers of layers and layer sizes can
provide different degrees of abstraction
deep learning exploits this idea of
hierarchy sentation where higher-level
more abstract concepts are learned from
more lower level basic ones when you
simply have 10 or fewer parameters as
input than other forms the machine
learning are typically better such as
support vector machine or logic
agression basic classification engines
and shallow neural networks are not
suffice for complex tasks and neural
networks with only a small number of
layers can become unmanageable
thus when the patterns get very complex
the
neural networks start to outperform
their competition the key to deep
learning could be largely ascribed to
breaking the processing of patterns down
and distributing that out across
different layers in the network for
example we might be applying this ml
system to take flowers in an image we
would then use edges to detect the
different parts of the flower petals
stalk etc and then combine them to
create the whole flower the process of
using simpler patterns as models that
can be combined to create more complex
patterns is a key part of the power of
deep learning as another example if you
feed the network a bunch of images of
lorries down at the lowest level
there'll be things like edges and then
higher up things that look like tires
wheels or a cab and at a level above
this things that are clearly
identifiable is lorries once the network
is trained you can put one image in at
the front and the nodes will fire when
they see the thing that they're trained
to identify in the example of face
detection it first learns features like
edges and color contrast these simple
features form more complex facial
features like the eyes and nose which
are then combined to form the face the
neural network does all of this on its
own during the training process without
any direction from the person building
it's these neural networks are almost
always built for a specific task such as
voice recognition or various forms of
data mining the system self organizes in
such a way that the nodes in the layers
closest to the input data become
reactive to simple features and then as
you move through the layers the features
that the neurons respond to become
higher and higher order interestingly
people have found a very similar
structure in our own brain where the
visual system for different layers also
extracts higher and higher order
features once you have a deep learning
network that is trained in this way it
should be possible to also run it
backwards
if you've trained a network so that it
knows everything about what a cat is
it should be able to produce new
pictures that look like cats and these
are called generative neural networks
deep Nets take a long time to train but
the advent of new hardware in the form
of graphic processing units can reduce
the processing time by one or even two
orders of magnitude relative to
traditional CPUs there are now lots of
different types of deep nets to use for
text analysis such as name recognition
and sentiment analysis recursive tensor
networks are typically used image
recognition processes often involve a
convolutional net or deep belief nets
for object recognition one may use a
convolutional net or recurrent Nets may
be used for speech recognition these
deep learning algorithms can be applied
to unsupervised learning tasks this is
an important benefit because unlabeled
data are much more abundant than label
data the end results of training a deep
learning neural network yields a
self-organizing stack of transducers
well Tunes their operating environments
and capable of modeling complex
nonlinear relationships a deep learning
platform is an out-of-the-box
application that lets you configure deep
nets without limbs know anything about
coding in order to use tools a platform
provides a set of tools and an interface
for building custom deep nets typically
they provide a user with the selection
of deep nets choose from along with the
ability to integrate data from different
sources manipulate data and merge models
through a user interface these platforms
may also help with performance if a net
needs to be trained with a large data
sets the downside is that you're
constrained by the platform selection of
deep networks as well as its
configuration options but for anyone
looking to quickly to apply it deep nets
a platform is definitely the best way to
go there are now a variety of such
platforms one of the most widely used
wine is tensorflow an open-source
library of machine learning methods
created by Google which has grown
rapidly in popularity
for better or for worse our worlds is in
the midst of a silent algorithmic
revolution many of the decisions that
humans once made are being handed over
to formal mathematical models today we
expect algorithms to provide us with the
answers who to date where to live how to
deal with economic problems with the
correct algorithms the idea is computers
can drive cars better than humans trade
stocks better than Wall Street traders
and deliver to us the news we want to
read better than news publishers as
Karen Ewing of King's College London
proposes just in the way that social
theorists have identified markets on the
one hand and bureaucratic hierarchy on
the other as two quite different ways to
coordinate social activity I'm going to
suggest to that algorithmic regulation
can be understood as a third form of
social ordering it's different because
it's underlying logic is driven by the
algorithm which is mathematical it would
appear the more and more authority
shifting to these automated systems it
is then important for us to ask what are
the consequences of doing that the
fundamental issue is really that at the
interaction between the informal world
of people and the formal world of
computers and trying to translate
between these different systems of
organization with cloud computing and
advanced analytics were extending
computing out into the real worlds like
never before through these platforms and
the algorithms that operate on them
we're trying to bring in and coordinate
more and more spheres of our social
economic and technological systems of
organization as we do this we're trying
to take an informal world that is
evolved over a prolonged period and
bring it into the world of formal
systems the math and scientific
framework out of which we built our
algorithms is unfortunately not a
universal language but is very much a
partial language that is heavily
dependent upon a reductionist paradigm
that creates many limitations in its
capacities the resulting models are not
in any way a neutral interpretation of
reality just as data can deceive models
likewise can deceive all paradigms and
theories only partial accounts of
reality and the models that derive from
them are never neutral they reflects the
particular paradigm upon which their
base all of our mathematical and
scientific frameworks are incompletes
all models are based upon opinions and
perspectives about the way the world is
some of those are better than others but
none are complete the mathematics that
we know is beautiful
pure and logically consistent until we
look outside the window and realize that
the world is not full of triangles and
squares and smooth curves and that's a
continuous problem as we take these
models out into the worlds even if the
process is automated the algorithms used
to process data are imbued with
particular values and contextualized
within a particular scientific approach
this is not just on the fundamental
level of the underlying science but also
on a more practical level algorithms
don't do anything on their own they
reflect the social and institutional
truths of the world in which we live if
for example society is racist then that
will be in the data and the algorithms
will pick up on that and it will also
pick up and amplify any other bias take
for example an employer trying to figure
out who to hire given that men have
always been more successful in certain
careers than women because of all kinds
of institutional bias then the algorithm
is likely just to reflect that it just
tells you you should hire these men
because there be more successful in the
past so there are lots of ways in which
algorithms can reflect both our
incomplete knowledge and also reflect
the bias that we live with every day and
they will hide these behind the guise of
neutrality and objectivity but also
because of the scale scope and power of
the technology that can potentially have
mass effects and this is something
unfortunately we will inevitably find
out more about as we build out this IT
infrastructure to our global economy the
negative externalities of these
algorithms
are outlined in a recent book by Kathy
O'Neill called weapons of mass
destruction she defines weapons of mass
destruction as mathematical models or
algorithms that attempt to quantify
socially important aspects such as
credit worthiness teacher quality
insurance claims College ranking
employment application screeners policy
and sentencing algorithms workplace
wellness programs etc but have a harmful
effect and often reinforce inequalities
for example keeping the poor poor and
the rich rich in the book she expands on
stories of people who be marked as low
wanky in some way by an algorithm such
as the competent teacher who is fired
due to a low score on a teacher
assessment tool the people whose credit
card spending limits were lowered
because they made purchases at certain
shops the college student and couldn't
obtain a job at a grocery store
due to his answer on a personality test
the algorithms the judge and rate them
are completely a PAC and cannot be
questioned by those affected people
often have no capacity to contest when
the algorithms make mistakes the author
lists three common characteristics of
these weapons of mass destruction
they are often proprietary or otherwise
shielded from prying eyes so that they
are in effect black boxes they affect
large numbers of people increasing the
chances that they will get something
wrong and they also have negative
externalities on people in society most
platforms are privately owned
enterprises and do not wish to expose
the internal workings of their
algorithms to the view of the end user
added to this the complexity of these
systems often overwhelms people's
capacity to comprehend them in this
respect subprime mortgages are perfect
examples of weapons of mass destruction
most of the people buying selling and
even rating them had no idea how risky
they were this of course extends to the
whole of the financial market where one
can only speculate about the algorithms
that might be out there machine learning
algorithms operate in high dimensional
space in all
to process possibly millions of
parameters which is hard for us as
humans to comprehend communicating such
things to humans will require new uses
of visualization so that people can
quickly understand in an intuitive way
how the system works the only
sustainable way to develop these
technologies is by keeping people
informed and engaged if we want to
develop these technologies in a
sustainable way then we need a system of
design that includes transparency and
accountability that means integrating
the language of the machine and that of
humans by creating visualizations and
other methods that quickly and
intuitively communicate what the
underlining technology is doing
technological crisis inevitably occur
when technology becomes too complicated
to couples to a pack an obfuscated
and something goes wrong in the system
the other issue is that of D
contextualization which results from the
narrow form of intelligence the
analytics represents the problem with
analytics is that it D contextualizes by
focusing on things it isolates them from
their context and leaves them open to
misinterpretation by simply looking at
them from one perspective we can gain
greater detail from that perspective but
we can also lose the relevant
connections that give it full meaning
and implication advanced data analytics
enables us to see further to focus more
clearly to pick out the needle in the
haystack however the more powerful we
make the telescope the more focus we
become and the more decontextualized the
information becomes which can be an
issue as it becomes easier and easier to
optimize for a single parameter but
create more and more negative
externalities on those other metrics
that are not captured due to the
narrowing of our vision finance is a
good illustration of this because of the
quantitative and complex nature of
financial markets it has been probably
the most advanced user of algorithms and
a good illustration of where we're
heading with technology finance is
obviously very focused on optimizing for
monetary outcomes real world's economic
goods like food and energy get brought
into the financial system
made available for trading on global
markets algorithms then operate on them
with the sole focus of optimizing for
profits the consequence of that can be
food riots in Egypt when the price of
grain goes too high or it can be elderly
people in Canada who can't afford the
price of their heating gas during winter
because of speculation as we start to
connect everything up through cloud
platforms more and more we're operating
within very complex systems and narrow
algorithmic optimization in one place
can lead to unintended consequence in
others it is important to note that
algorithms are analytical tools there
builds are the analytical capacities of
digital computers analytics always acts
on data all algorithms take in data and
perform some operation on it however
data is always from the past there's no
such thing as data from the future this
has the implication that these models
can only tell us about a future that
resembles the past of course we can put
all sorts of nonlinear and stochastic
stuff into the models to make them look
more like what happens in the real
worlds but at the end of the day they're
essential reference point is past data
which makes them inherently conservative
this is fine if the system you're
dealing with is in a normal state of
developments but that's not always the
case sometimes major changes happen and
the model is unlikely to be able to tell
us much about that this is exempt fide
that extraordinary poor predictive
capacities of economic models in
relation to major financial crises in
order to get a future that looks
qualitatively different from the past
you need a theory data analytics are not
going to help you this as such data
without theory can knock you into the
past analytics will always tend towards
reinforcing past patterns data analytic
systems don't know what might be what
could be or what we might want to be as
such they often creates self-fulfilling
path dependencies in an article in
Harvard
Business Review entitled learning to
live with complexity the author's rights
in complex systems events far from the
medium may be more common than we think
tools that assume outliers to be rare
can obscure the wide variations
contained in complex systems in the US
stock market the ten biggest one-day
moose accounted for half the market
returns over the past 50 years only a
handful of analyst entertained the
possibility of so many significant
spikes when they constructed their
predictive models this is one of the key
problems with an over reliance on
analytical reasoning it tells us that
the future will be similar to the past
and because it often is it lulls us into
a false sense of security
even though major changes happen rarely
because they can be so large and the
incremental changes are typically so
small the unpredictable paradigm shifts
end up being more significant than all
of the linear incremental changes that
the system predicted so well to create
real change change in paradigm we need
something qualitatively different
visions imagination and theories can
inform us of futures that have never
existed while algorithms are not really
designed to tell us about them we can
try to get computers to think outside
the box but this is not what analytical
reasoning is designed for it's like
trying to put a screw into a piece of
wood with a hammer you'll get much
better results in the long run if you
invest in using the correct tool for the
correct application since the rise of
the internet we've found ourselves
living in two seemingly parallel worlds
one the familiar physical worlds and the
other this growing world of information
but with the convergence of advanced
analytics cloud computing and the
Internet of Things these two worlds are
starting to glide in powerful new ways
as the Internet starts to come off line
into the physical worlds technologies
that we once thought of as physical
tools and machines are no longer so as a
wave of information technology they
started with personal computing and the
Internet
is breaking out into the real world of
physical things today information is out
of its box and it's redefining our
technology landscape technology is no
longer a one-off object that performs
some physical operation as you want
thought of as as we networked our worlds
placing sensors and actuators in all
kinds of objects technologies are
becoming more like systems for executing
on algorithms phones that just 10 years
ago were lumps of plastic and
electronics with buttons for making
calls have become smartphones that are
designed to simply run code cars are
becoming smart cars whole cities of
becoming smart cities with all this
technology increasingly connected up to
the cloud we're smart systems run
analytics crunching data learning and
feeding it back to devices to optimize
their performance as a revolution in
information technology unfolds at a fast
pace
science fiction appears to be becoming
science facts within just a couple of
short decades we've gone from the PC to
the Internet and mobile computing
today's world of cloud computing and
smart systems the age of smart systems
is becoming a reality as ever more
products and services that we use every
day from search engine advertising
applications to facial recognition to
social media sites to smart cars phones
and electrical grids are coming to
demonstrate adaptive and smart behavior
these smart systems incorporate
functions of sensing actuation and
control in order to describe and analyze
a situation and make decisions based
upon available data in a predictive or
adaptive manner there by performing
smart actions in most cases the
smartness of these systems can be traced
back to autonomous operations based on
closed-loop control machine learning and
networking capabilities that enabled
system to exhibit adaptive behavior
these smart systems will sit at the
intersection of humans and our
technology infrastructure as they
perform basic control operations for our
technology infrastructure and interact
with people so
understand their needs and perform
required actions the extraordinary
capacity of this new stage in the
development of information technology is
the convergence of advanced analytics
cloud platforms and the Internet of
Things in every decade we have
approximately 10 times as many connected
devices as we did in the previous
decades and this will likely continue
for the foreseeable future everything
that used to be dumb and disconnected is
becoming smart and connected as devices
and technologies become connected into
power platforms smart systems are
software entities that carry out some
set of operations on behalf of the user
or another program with some degree of
independence or autonomy and in so doing
employ some knowledge or representation
of the users goals or desires and the
environment within which they act in
order to achieve those goals
such an agent is a system situated in
and part of a technical or natural
environments which senses any or some
status of that environments and acts on
it in pursuit of its own agenda with
this agenda evolving from program goals
the agent acts to change part of its
environment or its status and influences
what it senses the central
characteristic of these smart systems
are adaptive capacity dynamic
interactivity a degree of context
awareness and learning capacities they
may adapt as information changes and its
goals and requirements evolve they may
resolve ambiguity and tolerate
unpredictability they may interact
easily with users so that those users
can easily define their needs they can
also interact with other processes
devices and cloud services they may
understand identify and extract
contextual elements such as meaning
syntax time location etc drawing on
multiple sources of information
structured and unstructured they can
reason on data to create new information
and use closed-loop feedback to rapidly
iterate and learn from the output
meaning they get smarter over time
with the rise of this next generation of
computing has been the expansion of
cyber-physical systems with the Internet
of Things devices object and all types
of technologies are becoming instruments
to network to algorithms running in the
clouds with this next generation
computers are starting to come out of
the world of well-structured data into
the everyday world of unstructured
environments where they use large
amounts of data to create the context
within which they can interpret new
things and interact with the world in a
fluid fashion this next generation of
robots
unlike previous ones are safe for and
can interact with humans in a more fluid
fashion what is called human centered
robotics they have a sense of touch and
are very precise Amazon's warehouses
that may have up to ten thousand robots
assisting people and bringing them
packages is one example but the same
evolution in computing but it's
developed in the platform model is also
coming to robotics where multi-purpose
physical robotic capabilities can be
delivered via a cloud platform that
enables developers to bundle them into
new processes and applications again it
can't be overemphasized how important
that developments of cloud computing
have been to this whole equation
Feifei lee of google cloud presents it
well when she notes it took me a while
but i started to realize that cloud is
the biggest computing platform humanity
has ever created and what is computing
today computing is to make your data
speak intelligently and to act
intelligently to solve your problem or
your customers problem so really this
marriage between AI and cloud is like
this perfect vehicle to democratize AI
the combination of smart systems cloud
platforms and cyber-physical systems
will revolutionize our technology
landscape in the coming decades with the
rapid commodification of smart systems
connectivity to the cloud and sensing
devices more and more of our
technologies will become cyber-physical
from shopping trolleys to shoes to cars
to whole houses
but also smart platforms will be plugged
into whole infrastructure systems like
the power grid internet routing city
transport systems taking in massive
amounts of data and learning from it in
order to optimize the system this will
be a massive source of technological
disruption much media attention and
public imagination is currently focused
on robotics and individual
cyber-physical systems although little
robots cleaning our houses or delivering
pizzas might be the most apparent
manifestation of this change the real
innovation will be delivering these
machine learning algorithms as a service
to IOT platforms the network whole
infrastructure systems whether that's
cloud analytics connected to the smart
grid transport networks or connecting an
enterprise's whole supply chain up or
even the smart city itself take for
example the mining industry that is
currently going through a massive wave
of automation as mining companies are
running out autonomous trucks drills and
trains from a control center in Perth
Rio Tinto's employees operate autonomous
mining equipment in Australia's remote
Pilbara region which is rich in minerals
73 trucks each the size of a small
two-story house find their way around
using precision GPS and looking out for
obstacles using radar and laser sensors
and work alongside robotic Rock drilling
rigs from their single operation center
they integrate information from all
their minds ports and rail systems and
visualization technology gives their
personnel a 3d display of all their
operations as the company says these
technologies take us ever closer to
whole mine automation cloud analytics is
a service model in which elements of the
data analytics process are delivered
through public or private clouds cloud
analytics applications and services are
typically offered under a subscription
basis or utility paper use pricing model
Google Microsoft and Amazon also expect
to increase their profitability and
enhance their cloud services through
associating with their
learning capabilities their strategy is
to allow companies which are unable to
develop machine learning solutions at
their level to access their cloud-based
ml services through api's one example of
this is the apps recently developed by
microsoft's for the facial recognition
of uber drivers companies like uber
already have machine learning platforms
and increasingly these new technologies
will be used to analyze the data coming
from their car sharing platform to
optimize where cars go what route they
take how much they charge essentially
automate the most basic management
activities of their platform the
platform model will be important in
developing smart solutions in that it
will enable different smart capabilities
to be offered as modular utility
functions that can then be plugged into
and bundled together by enterprises
according to their specific needs
instead of having just one
general-purpose system a platform model
allows developers to draw upon specific
capabilities and integrate them into
their solution such as machine learning
to recognize a face off or voice
recognition software or advanced
analytics for specific domains equally
the platform plug and play model will
work to commoditize smart systems making
them available to almost any technology
developer api's and developer toolkits
already offered by IBM that can be
plugged into a wide variety of
applications from health diagnostics to
analyzing data coming from Transport
Systems in such a way smart capabilities
will flow to almost all types of
physical technologies in the coming
decades this cloud-based platform model
to smart systems will mean that through
an internet connection even the smallest
of computers like a mobile phone can
operate like the most powerful computers
in the worlds by simply sending the
inputs to the cloud where it's processed
and then output the information this
returns this is quite an extraordinary
phenomenon in that it means that the
most powerful computing operations and
algorithms can be accessed anywhere
there's internet connection on the
planets meaning that the most advanced
GFI age can be accessed and used
virtually anywhere on the planet through
just a mobile phone and internet
connection whereas previously we put
computing devices into the hands of
people now we're putting supercomputers
into their hands
the primary beneficial function of
analytical systems will be in their
management and optimization of large
complex networks they'll be connected
into whole transport networks power
grids cities and possibly even whole
urban networks analyzing that data to
make predictions optimizations and
adaptations api's will make high-end
machine learning capabilities available
to all forms of devices and physical
systems as one commentator noted api's
are not a dime a dozen they're a dime a
million one aspect of this platform
model is that it can harness fleet
learning because any components is
operating within a network when one
smart system learned something that all
have access to that new information
almost immediately that kind of network
effect means that the system can improve
its capacities at an extraordinary rates
the system will learn over time due to
network effects and big data machine
learning network components can help
each other to create a wisdom of the
crowds effects the CEO of Tesla
explained fleet learning within their
network the whole Tesla fleets operates
as a network when one car learned
something they all learn it he goes on
to explain how each car using the
autopilot system essentially becomes an
expert trainer for how the autopilot
should work the company's autopilot
service is constantly learning and
improving through machine learning
algorithms because all of Tesla's cars
have been always connected wireless
connection data from driving and using
autopilot is connected since the clouds
and analyzed with software for autopilot
Tesla takes the data from cars using the
new automated steering or lane change
system and use it to train its
algorithms
Tesla then takes these algorithms test
them outs and incorporates them into
their upcoming software in this way we
can see how cloud problems machine
learning and the incentive things can
work in a synergistic fashion the rise
of smart systems represents a natural
evolution to our information technology
but with this next generation of
Information Systems
we're both vastly expanding our
technological capabilities and also
consolidating and handing over an
extraordinary amount of power to these
algorithmic systems and because of this
there needs to be major consideration
given to the appropriate use of that
control and power along with more
traditional concerns surrounding
security and access the scale of the
risk involved in this mass automation is
unprecedented as our critical
infrastructure becomes automated
networked and remotely controlled via
common smart platforms today a typical
cars airbag steering and brakes can all
be hacked and control through the
internet for malicious ends control
systems in nuclear power plants may be
broken into and with the rollout of IOT
platforms software will soon be
permeating all types of technology as
our critical infrastructure becomes
increasingly dependent upon it
autonomous agents can be understood as
essentially advanced optimization
algorithms when we let a machine
autonomously pursue goals we don't know
exactly what action it will take with
only a limited and narrow form of
awareness that is trying to optimize for
a limited number of parameters many
negative externalities can results for
example corporations are form of agents
within the free market capitalism that
is designed to optimize for shareholder
value and financial returns and we've
long seen have this narrow focus on
profit due to the structure of the
incentive system can lead to negative
environmental and social externalities
indeed it can be identified as a key
driver of the current sustainability
crisis this illustrates how narrow
analytical reasoning the
is not supported by or operating within
some broader form for wareness the
overall context often leads to negative
externalities that create unsustainable
results we can say a system is under
control and operating in a sustainable
fashion when its actions are integrated
with the broader context the problem
with smart systems is their narrow
analytical form of awareness as
autonomous agents become more autonomous
given greater scope to define the means
through which they achieve the given
ends those greater potential for them to
perform acts that are misaligned with
the overall context in their narrow
pursuit of their ends without overall
awareness of the environment within
which they operates for this smart
technology landscape that's be developed
in a sustainable fashion there needs to
be a systems of systems approach to
control we're more narrow and specific
forms of smart systems are nested within
larger more general forms of awareness
which are intern coordinators and
monitored by broader forms of human
intelligence a system is only really in
control when awareness responsibility
and power are all aligned this means the
exercising of control through a
multi-tiered framework with more
intelligent and aware systems guiding
systems that are lower in that capacity
for information and knowledge processing
whereas information and data may be
growing in an exponential rates this
only works to make intelligence an
increasingly scarce resource information
technology on the one hand commoditize
--is information and data driving its
value right down but because of this it
also increases the value of knowledge
and intelligence making them scarce
resources wherever there's a demand for
a scarce resource
those hierarchy based upon access to
that resource this drives a new form of
hierarchical structure that emerges out
at the information revolution captured
in the acronym di kW which stands for
data information knowledge and wisdom
controlling these systems in a long-term
sustainable and secure way means
understanding this information hierarchy
and building it into our systems Tecna
G so this worlds of complex information
systems that we're going into is
governed and directed by true knowledge
and insight of context and consequences
the rise of smart systems can be seen as
a whole new level to our development of
technology and like all technologies it
holds out the possibility to both enable
us all to constrain us depending on how
it's designed developed and operated
perhaps in the abstract technology is a
neutral thing but all technologies have
to go through a design and development
process and how that process is carried
out will determine to a large extent
whether the technology is constructive
or destructive in nature whether it
works to ultimate in able people or
constrain them it is possible to
industrialize an economy without
creating the negative environmental
externalities that our particular set of
industrial technologies created when we
built them thus they can't be said to be
neutral a combustion engine that emits
toxic fumes is not a neutral thing it is
destructive in this sense technological
developments may be inevitable and its
evolution in the abstract may well be a
neutral thing but how we conduct that
process of development is neither
inevitable nor neutral the serious
responsibilities associated with it
the negative externality of smart
systems is the potential for an excess
of now analytical reasoning which smart
systems represent a massive
proliferation of and a lack of broader
synthetic reasoning to balance and
directed towards constructive ends the
computer scientists Stuart Russell
summarized this issue as such this is
essentially the old story of the genie
in the lamp all the sources apprentice
or King Midas you get exactly what you
asked for
not what you wants a highly capable
decision-maker especially one connected
through the internet to all of the
world's information and billions of
screens and most of our infrastructure
can happen irreversible impact on
humanity this is not a minor difficulty
improving decision quality irrespective
of the utility function chosen has been
the goal of AI
search the mainstream gold on which we
now spend billions per year an excess of
a let's go reasoning and lack of
synthetic reasoning could take us into a
world where we have an extraordinary
amount of technical capabilities and
power without sufficient knowledge and
wisdom to direct it effectively the
result being unsustainable outcomes for
the opportunities in smart systems be
realized and the negative externalities
limited would require a concomitant
massive expansion in synthetic reasoning
capabilities and the appropriate control
and alignment of smart systems within
larger more intelligent frameworks of
organization in such a way ensuring it's
correct alignment and ultimately the
appropriate use of that power towards
ends that are integrated with the
broader context and thus sustainable and
long-term as far back as 1960 Norbert
Wiener said we better be quite sure that
the purpose put into the machine is the
purpose which we really desire as the
machines get smarter more powerful is
our job to say thinking about the
context to think about the overall
desired outcomes and align the means
with those an expansion and
technological means requires an
expansion in human ends and an alignment
between the two in order to develop in a
sustainable way
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>