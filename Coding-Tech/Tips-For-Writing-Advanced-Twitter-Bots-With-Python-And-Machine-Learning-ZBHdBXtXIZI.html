<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tips For Writing Advanced Twitter Bots (With Python And Machine Learning) | Coder Coacher - Coaching Coders</title><meta content="Tips For Writing Advanced Twitter Bots (With Python And Machine Learning) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tips For Writing Advanced Twitter Bots (With Python And Machine Learning)</b></h2><h5 class="post__date">2017-12-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZBHdBXtXIZI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Twitter is a magical place it is a
wonderful wonderful place you find movie
stars on there like Mark Hamill Emma
Watson Kristen Bell
there are pop stars like Taylor Swift
there Derek there are complete randos
who you know there's hard-hitting
journalism there are business leaders
there are politicians former presidents
there's there are conference organizers
conference organizers on Twitter there
are people who live tweet talks on
Twitter and there are a great great
number of bots so and BOTS are the
reason why I got into this because one
day I decided to try and make my own
there's a whole bunch of fun you can do
with this and it's a great little toy
project you can have a lot of fun with
so before I get into my but I thought
I'd sort of have a look at some of the
other BOTS that you can find on Twitter
there are some that react to other
people's content pentameter on is one
that does that pentameter on looks four
pairs of tweets that form rhyming
couplets in iambic pentameter and it
will retweet them
accidental 575 looks for similar kind of
things never looks for haikus although
it does get tripped up by by the
hashtags occasionally you get other ones
that are a bit more sort of reply
oriented for example stealth Mountain
tells you when you said sneak peek
instead of sneak peek there used to be
one that would correct you if you said
less instead of fewer but apparently
that's worse than being a Nazi and then
you get one of the most interesting BOTS
that I know on Twitter which is God
tributes appropriate tributes we'll go
and pick words out of what you've said
and respond to it by dedicating it to a
particular entity in this case is just
picking words out but it also can deal
with emoji it can also look at images
and work out certain extent what's in
them which is kind of spooky when it
happens but it's pretty cool
it can also in a lot of cases work out
the kind of singular singular izing and
pluralizing part of it for the most part
but this gets to the other kind of bot
that is although which is just one that
generates things because god tributes
will occasionally just throw something
out there magic realism bot does similar
kinds of things
we also get ice tea from Lauren order
SVU to help warning us about various
drugs but one that I wanted to highlight
because it's a useful one for this
subject is midsummer plots you're
probably not familiar with Midsomer
Murders
it's a British TV show which involves
the rather murder prone region of
Midsomer in the UK it's been going for
20 seasons but it is delightfully twee
as you can see from these but the reason
I wanted to highlight this one is you
can actually find the code for it and so
this is an example of a templated kind
of bot you have a bunch of murdered
persons you have some some causes of
death you have and then you have a bunch
of village groups that are angry at
something that threatens something and
then you just call random a bunch of
times and fill it all together if you
want to get into these ones you don't
actually even have to write so much code
like that there's a service called cheap
BOTS done quick cheap BOTS done quick
uses a kind of meta language called
tracery you can see with this that you
have a bunt do you have a templated kind
of thing up here where you can
manipulate various things and you have
the list of random things that go in
there and it will just roll the dice for
you and generate things and so that's a
kind of very simple way of generating
content for your Twitter bot a lot of
them more some of the more interesting
ones use more complex models like Markov
chains and the like and for some reason
these are all called ebooks bots which
is funny because horse ebooks where they
get the name from was not actually a
Markov but it was some kind of bot it
was originally a spam thing and then a
performance artist took it over and it
became more delightfully strange but
kind of these single source or even
multi-source Markov bought end up quali
and called the ebooks bots the grukk is
an information security person he has an
ebooks bot you can see on the left here
that this one is something that he
actually said it on the riders kind of
word salad that sounds a bit like him
when you combine bot multiple content
sources together though you can get
something even better
erowid is a site which among other
things contains people's descriptions of
people's experiences on certain
psychoactive substances era would
recruit it combines that with technical
recruiter spam which brings me to my bot
which is called wind nation wind nation
will may not mean too much to many
people here
wint nation is the combination of a
bunch of Australian political figures
and one of the weird Twitter accounts we
had Twitter being a particular genre of
Twitter called drill or wint
so one nation has fronted by the lovely
Pauline Hanson is an Australian
political party that is the exact kind
of reactionary right-wing chuckleheads
you think they are and one day oh and
part of the reason why this came to be
is that they got a bunch of people
elected in the last major election in
Australia and a bunch of them have an a
spectacular capacity the cellphone
particularly on Twitter there was one
who is unfortunately no longer in
Parliament Malcom Roberts who would
regularly on Twitter just yeah faceplant
brilliantly which would lead to him
being sent this tweet quite a lot
and so I figured that given the drill
seem to exhibit a whole bunch of the
same old person yells at cloud things
that one nation tended to do that they
would make a good match and that we
should try and mix their content
together so which brings us to actual
serious bits to do this kind of thing
you need a cut you need a corpus you
need a body of data that you can feed
through something in order to generate
tweets so how do we get that
well Twitter's got an API it's actually
pretty easy to use the quick version is
you click that create new app button it
will ask you a bunch of details you
filled them in it creates an application
and gives you a bunch of consumer keys
that you can use
you've also need some access keys you
get those by scrolling down and clicking
that button and then you get those
tokens and then you call it the Twitter
API it's act it is that easy and the
nice thing is that if you've created a
new Twitter account around the but
posting is equally easy the other source
of content that I use I use three main
sources of content for win nation one is
the Twitter accounts of a of every one
nation politician I could get fined
the other one was their Facebook
accounts Facebook if you go if you sign
up for their developer program you can
go to this graph API Explorer which has
two wonderful properties one you can
actually play with the API there
secondly you can copy out that access
token that I've blurred out and you can
feed it into your own script it only
lasts for about half an hour but that's
all you need to do to download some
stuff the last one I use is open
Australia where you can get Hansard
which is the transcript of everything
that's said in Australian Parliament and
yet using the Facebook API from Python
is also pretty easy so tips for if you
are doing that is remove extraneous
content things like retweets and stuff
like that will confuse your model and
will cause your word salad to be more
confusing and less funny because you
really want to ride that line between
delightfully strange and what the hell
did it just say the other thing you can
do is if which something I do is when
nation but you don't have to do with
other things is keeping timestamps is
useful because if you want to decay the
either limit or decay the
the various things so that your thing
remains topical you need that
information there and the last one is
playing with your content mix what I
found when I started doing wind nation
is that while it seemed that one nation
and drill should be obvious a match made
in heaven
they didn't quite cross over enough and
by adding a couple of other accounts
that I'm going to keep secret I managed
to bridge that and get them to actually
start moving across between one and the
other a bit better so the actual process
of generating this stuff you generally
got two stages you've got modeling and
you've got generation when you're
modeling you've got some kind of a
process you're feeding some stuff into
it and you're getting a model out of it
and then when you're generating it you
are taking your model you're running it
through a bunch of randomness and
getting tweets out so in a Markov chain
the way Markov chains work you've got
some kind of state a state is some
number of words or tokens or a token
could be like a begin or an end of
content kind of thing you've got a
lookup table for each possible state
that gives you a bunch of weighted
possibilities for the next word or token
and then you roll your dice and you
select one of those which then becomes
part of your state and the oldest
element of your part of your state gets
moved out so the your state size remains
the same and you just repeat this until
you know the end of your thing is
reached so to give you a quick
demonstration of this here's a couple of
begin tokens these form our state we
then have a bunch of possible words we
could select from that we pick one at
random that becomes part of our state
now and the oldest token is removed at
which point we have more choices which
we can pick that becomes part of our
state and then we've got to find another
choice to make we make our choice and
then at that point we've only got one
possible choice because we're going to
we're going to end on that one and we
have our result and that last sort of
single choice thing leads to one of the
interesting properties of Markov chains
which is there's a lot of cases where
you're going to have either a very
highly probable or only choice to make
it
and point and to give you an idea of
what that ends up looking like this is
one of wind nation's tweets and so to
give you an idea of how this one gets
constructed the first bit comes from our
sadly sadly ex senator Malcolm Roberts
and should have picked our color for
that but you can sort of see that it's
taken this first I met with the best of
section from that tweet and then the
next one came from West Australian one
nation representative of content all and
then the last bit came from drill so
yeah talking in about this process the
the modelling process with the Markov
chain is effectively word frequency
analysis the generation fray stage is a
random model walk so to give this a bit
of a demonstration I decided to download
I got the entire works of Jane Austen
from Project Gutenberg and a really
badly ocr'd version of Lord of the Rings
from the Internet Archive
and so the tool of choice in this case
is a Python library called mark over Phi
it's really useful it's it's nicely
structured it's really easy to override
certain parts of the this is the way it
works and it's also got a couple of
other interesting properties one of
which is that you can see in this code
here because this is effectively what
you do to generate your Markov chain you
can see that I'm processing the two
corpuses separately and then writing
them out separately because mark
over-fire will let you generate based on
mark models that you combine so you can
see that i could combine these Markov
models and generate stuff based on that
so it'll generate me five sentences of
output from these corpuses which will
look a bit like that the trouble with
this is it's a bit more word salad than
you'd like it's just it's not so much
funny as it just is you can try and fix
this by increasing your state size the
only change in this code is that I've
added the state size equals three that
means that instead of tracking two words
it'll track three and the results a
little bit better but the problem is as
you're increasing the state size you're
also decreasing the possibility that
you're going to
switch from one to another which is
really where the humor tends to come
from so one of the other tricks you can
pull off is part of speech tagging and
this is actually example that they give
in the Markova phi examples in this case
we're going to override parts of the
Markov model itself so that we're
splitting up the words and then tagging
them based on their part of speech NLT
kay can do this as another library
called Spacey that mark over faces is
faster but it really wasn't in my
testing but either way it works pretty
well you can then just use that instead
of the mark over phi dot txt model so
i've replaced that with pussified text
and then generation is largely the same
and you start to get this is with state
size two and and then these ones were
state size three and top one there was
my favorite one that I came up with
so that tends to get you up to a point
where you know it tends to work and that
gets you to this notion of quality
control the the nice thing about the
templated bots is you can generally just
run them on a timer and I'll throw
something out that's you know at best
hilarious and at worst kind of new with
the Markov stuff you've got a range from
hilarious
to what and so what ended up happening
was when nation is it it became a kind
of semi manual process I would sit there
every once in a while when I got bored
and generated a whole bunch of things
put them in a queue and then it would
the posting process would pick one at
random and post it up that can get
tedious and the whole point of this is
to be fun I have yet to come up with a
way to really get it to a point where I
would just happily stick the output up
but there are a bunch of Markov bots
that will do that so I figured you know
the next thing to do is obviously to try
something to be more complicated so
machine learning machine learning is
obviously the next thing to try so you
know let's look at our modeling slide
again in this case our our model in this
case is training a neural network and
generation is something something random
numbers but these random numbers get
really big and you know so the next
question is obviously what is in your
network I'm glad you asked because I
have no idea
well I do sort of a neural network has a
neuron which has an activation function
it has an input and output in the new
train and you back propagate in all
kinds of buzzwords I did a bunch of
really heavy Google I mean research and
discovered that the one that you really
should use for language modeling is this
long short-term memory one and I believe
everything that Google tells me so
having believed everything that Google
told me I asked it how I should actually
go about doing that and took the most
carefully researched answer there as you
can tell because it's followed soap I
torch is a machine learning framework
for Python it's got CUDA support it's
got everything you could need to do the
kind of thing it's even got a word
language model example and it had
instructions which I followed and I ran
it and it did a bunch of things so terms
I do understand milliseconds per batch
loss is a measure of how bad it is and
perplexity is a measure of how confusing
it is so yeah ppl is perplexity and
that's sort of how often can we generate
something that actually represents
what's in the validation set based on
what's in the training set and then once
that finished I could generate some
stuff out of it and the results were a
little bit underwhelming especially
given that so this is that the timings
that I got this is running on my laptop
which is a 2013 MacBook Pro so this is
we this is running my Markov stuff that
took a while but of course you know
these days we have other ways to do
things
so and that will doubt that p32 x-large
that's a current or the the most latest
generation GPU compute oriented instance
from amazon it'll set you back about
three dollars an hour you know a bit
over ten minutes is not too bad for that
but i did a bunch of playing around
trying to get something that actually
worked so you know i tried well you know
part of speech tagging worked for markov
and must work for machine learning right
not really and at this point and at this
point i rapidly concluded that i just
because really what it comes down to is
just the entire point of this is to have
fun they're a great toy bots are a
wonderful toy for playing with this kind
of stuff you can have a lot of fun you
can make fun of people who deserve it
but you know there it comes a point
where it starts to feel like work and
when it starts to feel like work i lose
interest for a personal time project
so in conclusion have fun with twitter
bots they're great if you've got an easy
sort of grammar that you can write out
in tracery use cheap bots done quick if
you want something more complicated use
mark over phi if you want to use machine
learning go for it because i reckon you
probably could come up with something
that it's just beyond my understanding
of the field at the moment and so with
that i thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>