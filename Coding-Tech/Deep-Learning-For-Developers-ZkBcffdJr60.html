<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning For Developers | Coder Coacher - Coaching Coders</title><meta content="Deep Learning For Developers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Coding-Tech/">Coding Tech</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep Learning For Developers</b></h2><h5 class="post__date">2018-04-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZkBcffdJr60" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Ali caroli I'm a solution
architect for a company called a sauce
we are about to become UK's largest
fashion retailer overtaking Marks &amp;amp;
Spencer after so many years what I do is
I look after the recommendations
platform I work on date science with
data scientist and what we do is we turn
user interactions into personalized
experience using machine learning now
I've always been interested in the field
of machine learning but I formerly got
to work on the problem of detecting the
road marking and text recognition and
here is a sample of the code as you can
see the application back in this is Sir
back in 2008 basically recognizing the
text and marking on the road as you can
see at the top and on the left basically
what it does it calibrates the camera by
finding the vanishing point and the
detection is happens in real time
the project was pretty successful it
resulted in publishing two papers one of
which has got more than 50 citations
which is not bad I use three layer
neural networks for this project and
this project basically some of the
techniques I use are completely obsolete
by deep learning which I'm going to
explain the code for it is very much on
github so don't worry about the code of
the projects I'm going to show and they
are all there since then I've been
focusing mainly on machine learning on
text and natural language so without
trying to sound dramatic or
revolutionary there is no such thing as
deep learning
basically yes it's a simple and deep
learning is a culmination of various
technical and non-technical events which
made techniques that we've known for so
many years and some of them for decades
become successful so deep learning is
essentially a rebranding of multi-layer
neural networks now you might ask why
such a powerful thing needs rebranding
well we'll get to that
so this talk I'm going to explain the
building blocks of deep learning when
they happen through the history the
focus here bear in mind is not the
history itself but why it took so many
years for us to get where we are and
after that I'm gonna talk about where we
are state-of-the-art and wherever you're
going with deep learning and next one is
basically I'm gonna break down a
realistic example something that
probably you've seen on github happening
that they recognize you know the the
format I mean the language of the
project we're gonna look at that
realistic example you might have some
expectations in terms of things that you
might see here I'm just leaving the
pointer unfortunately we won't be able
to cover them one of them is d-train you
probably have noticed these pictures
weed faces of cats and dogs embedded in
them that's the dream taking a neural
network and just providing the input the
output you know reversing the the input
basically to get the other image the
other one was the alphago beat the world
champion that was Monte Carlo tree
search if you wanna have a look and most
interesting one is gain or general
adversarial Network which is very very
hot topic currently you probably might
have seen they take a picture turn it
into painting by Van Gogh Monet or
someone else
and this is this is quite cool now the
history the history of deep learning is
utterly interesting and as we navigate
through history you will find out that
it sees as much about the people
themselves then the technology it's
about you know on either unusual talents
about the challenges they face you know
or being rejected by academic community
you know sometimes the demons they were
facing inside but it's going to be
interesting are you ready
thank you it's a cold winter - people
are warring themselves by the fire one
of them is drinking whiskey and eating
ice cream this is what he really enjoyed
he is quite a bit older and at this time
probably looked something like this
it comes from a wealthy background he is
a neurophysiologist
physician philosopher unemployed it is
his home and he later became one of the
founding fathers of cybernetics the
other man is much younger he's barely
eighteen and well we said it's not his
home in fact he doesn't have a home he's
homeless doesn't mean that he's a hobo
he actually got a PhD position from the
University of Cambridge from none other
than Bertrand Russell when he was guess
what twelve well he had to reject that
and the story goes as that he he was
being bullied at school all the time
and one day they they've he actually had
to flee from the guys and hidden in the
library and at that time managed somehow
to stay in the library and went through
one volume of the book called principle
by Bertrand Russell he found it so
interesting that he finished the three
volumes over the next three nights and
he found some errors in the text as you
would do as a twelve-year-old and either
and he decided to write a letter to
Bertrand Russell it didn't expect much
to come out by in fact Russell replied
back and asked him to join immediately
in Cambridge to start his PhD which
obviously he could not oblige but three
years later when Russell went to the
University of Chicago basically he left
home and at the age of fifteen never to
come back in the universe of Chicago he
met a young medical student and this guy
basically was much more interested in
going to Russell's lectures than
studying his own stuff and he introduced
Seymour McCulloch to Pitts and at this
time McCulloch was a seasoned
established scientist and Pitts was a
runaway eighteen year old but it's real
clicked and this was the start of
collaboration and friendship which would
shape the discoveries leading to deep
learning and these these guys were
friends discussing philosophy poetry and
pizza McCulloch living together and in
fact died the same year so McCulloch was
really interested in finding in looking
at the neuron and seeing how he
basically and he's a threshold in order
to fire and he was so
about how I can use that as a unit of
logical circuit but his grass will be
Matt's wasn't great and this is where
math Pitts would come in and basically
they set out to devise a neuron an
artificial neuron as a unit of logical
circuit and they published this paper
without which we wouldn't be here
talking about deep learning this this
paper was published in 1943 and after
which pits when went to work it I had
MIT working for Norbert Wiener Wiener
sorry and 1951 the rest of the gang
joined him and they started working on
some very exciting things and basically
kids started working on 3d neural
network we don't know what what it would
be because I will mention later he burnt
all of that but could have been
describing what our current deep
learning would look like and let vein
has this to say about him in no
uncertain sense he was the genius of the
group sadly in 1952 Weiner clash with
McCulloch and broke all ties with with
McCulloch and anyone who had anything to
do with him including Pitts on one hand
Weiner was at heart an engineer really
really interested in building stuff and
he actually went to become one of the
fathers of robotics also he was
mentioned that his wife hated McCulloch
all around this has a terrible impact on
Pitts and made him basically socially
isolated doesn't mean that he didn't do
anything to Trillo the this seminal
paper describing how analog signals from
the eye actually has does part of the
vision so it's not just the brain and is
this is an idea that we see in deep
learning
getting confirmed but anyway after this
basically he burnt decades of his on a
published work including his 3d neural
network so all this theory was good but
some people started building neural
networks and training them one of these
people was Frank Rosenblatt he started
in 1957 and kind of took took Pitts
Pitts model and started little changing
that and called it perceptron and
perceptron in 1958 he published a book
and a 360 to basically sorry 58 a paper
and it's six to a book and we can see
what he does here he's using it to
explore the mysterious problem of how
the brain learn this perceptron is being
trained to recognize the difference
between males and females it is
something that all of us can do easily
but few of us can explain how to get a
computer to do this would involve
working out many complex rules about
faces and writing a computer program but
this perceptron was simply given lots
and lots of examples including some with
unusual hairstyles right so this is what
they were doing back then now I'm just
to recap you guys what what you saw was
basically providing examples that is
called supervised learning and
supervised learning essentially is
providing labeled data to a model and
let it Train unsupervised learning on
the other hand is that you let the
network and basically learn the patterns
of the data and in middle there is
reinforcement learning which you don't
give it basically the labels but every
time every iteration you train then you
use you tell the network how good or bad
is done for example and you've seen
these agents playing Mario so this this
is essentially reinforcement line so
what is the perceptron perceptron we
talked about
is basically inputs weights you you
generate the weighted sum of the inputs
you pass it to an activation function
you get the output all of this where
this training you can do in less than 20
lines of Python now I'm not going to go
through the code here but just want to
bring an example just to make sure that
we we all happy with it so I've got this
this input vector I need to create a sum
what is the way to sum 6 multiplied to 3
minus 1 so on and so forth and provide
it because minus 1 is less than 5 you
know it's not more than 5 so basically I
will return 1 now this one you decide
so wait the Sun becomes 24 you turn zero
it's as simple as that
and this is essentially the building
block of the deep learning that we're
going to look at so what would you use
for activation function well pretty much
anything you want but these are some of
the common activation functions the
sigmoid 10h the relu this one is
currently the hottest activation
function this is so simple but this is
essentially was revolutionary
so when teenage Rosenblatt was studying
in Bronx High School he met a boy called
Marvin and this particular Marvin and
Frank Rosenblatt they grew to become
rivals and Marvin Minsky in fact apart
from having arguments in conferences
with Frank you know like very loud
although he has been working on neural
networks since 1951 he published this
book in 1969 and that year couldn't be
more symbolic and because also Pitts and
my cult died same yeah but they arguably
they believe this this book perceptrons
basically killed the AI research for the
next two decades so it's called the
first winter so this this book wasn't
groundbreaking or anything
it wasn't even accurate but what it did
say is that you can't model a neuron to
do an X or problem and even McCulloch
and Pitt's knew that you could solve
this problem actually using more layers
but somehow you know freaked the
community and they stopped him working
on it and that continued it until 1986 a
British scientist called Geoff Hinton
with a couple of colleagues
they found a way to Train multi-layer
neural networks that's that's called
back propagation and this young
scientist started from physiology in
physics studied philosophy and then
psychiatry and they then became a
carpenter and and you know so so all it
goes but basically he made multi-layer
neurons a possibility and one of the
people who actually built something
really really interesting was yan and
Laocoon a frenchman which basically he's
digit recognition was revolutionary and
he did something else
he found convolutional neural networks
although it's debatable whether whether
he found it so what is a convolutional
neural network compared to perceptron
that we've seen although it uses kind of
the same concept so what it does is
essentially is a convolution over the
inputs now what is a convolution for
those of you might might don't know it's
very simple you got a kernel and this
kernel for example is 3 by 3 you can see
here and then what you do is a multiply
each one with with the neighborhood and
you come up and fill fill this the scene
and this is basically ideally is
explained here in this this animation
it's not rocket science it's very easy
just just some multiplication and
summing but the second winter was coming
yes so what was the problem so they said
ok I've got these let's say 64 by 64
picture that's a very very small picture
by our standards but that is already
more than 3000 dimensions and what you
do is that the data gets diluted across
all these these dimensions
this is one of the few books I actually
own on the subject and its importance
probably very much historic now but he
really describes this problem very well
so he says if you're forced to work with
a limited quantity of data as in
practice then dimension basically says
very pure percentage because data
becomes a very sparse so you end up
solving that problem
reducing dimensionality what do you do
so they said you know I've got this this
picture that is 64 see if I can extract
some features from it then maybe you
know that that will be much smaller and
that problem would go away so they
started doing feed feature extraction
and this is sorry I I jumped so for
example this tree I can describe this
tree by having three loose ends and also
a particular aspect ratio and I can I
can start describing and create a much
smaller feature vector what is the
problem with that first of all it's very
susceptible to noise what is the other
problem this is admission of self defeat
on one hand you say okay I've got a
neural network I'm trying to replicate
the biological brain by building this
neural network and what I do is that you
know I provided this is gonna arbitrary
features no sane person would ever you
know start counting the loose ends of
the ward um three character there was
another problem overfitting so so they
they realized for simple problems it was
working great for difficult problem they
knew that they had to build multi layers
and build deeper networks but they found
that when you do that
the system does really well on the
training data but I can't generalize you
give it the day to day hasn't seen HSS
flops and that is essentially known as
overfitting what is the problem the
problem is that the network doesn't just
learn the data also learns the noise and
this picture from from the same book is
is quite quite interesting so that's why
when in 2008 I was doing my research H
they said you know don't don't do like
many layers it's never gonna work just
just do it you know maybe single layer
is gonna work better so there were some
other problems as well and one of them
so we talked about curse of
dimensionality and overfitting
another problem with training's would
never converge and you wouldn't know
whether it's ever gonna converge or you
know I need to just just leave it for
more days or it's just a problem and
sometimes you would start the training
and then all the weights gradually going
to zero guess what multiplying something
by zero results in zero so basically the
training would stop and we talked about
the feature engineering and they found
some other kernel methods such as
support vector machine anyone used
support vector machine yeah yeah yeah
few people so this has been doing very
very well and people basically said you
know just just forget about normal
networks just use this but that was not
all the story so during all this time
there are still people believing that
neural networks there isn't a structural
problem the problem is mainly just we
need better techniques these are
technical issues we need to get around
and these basically these three people
are known also as Three Musketeers
or Canadian math
and the reason is that they taste they
started working around early 2000 in
Canadian is Institute for Advanced
Research or CFR and basically they
carried on believing but they could
really publish any paper the papers
would be rejected in fact it will see
1998 paper from young lacunae basically
you know like he had so much difficulty
while it was really revolutionary so
about the unlock corn you know this is
what his colleagues would say yeah and
yeah you know we felt we had to invite
him you know his mother's is talking
about you know it's been working for
years you know they really really never
showed anything and this is what the
advice to researchers going to work on
Hinton's lab smart scientist go there to
see their careers end this is from
Yoshio Ben gia said you know I had to
twist my students arm to work on neural
net you know they would feel that the
part that their papers papers would be
rejected and in fact quite a bit of them
were rejected for all the wrong reasons
but they still believed what the
Revolution was about to happen and first
of all 1998 from paper from the lacunae
and Benjy basically they described the
gradient descent algorithm which is
which is like the most popular currently
algorithm for training and this is no
rocket science this is they said you
know when we train the network instead
of giving all the data just give it back
by batch
and it produced really great result
another thing that geoff hinton did said
you know they they found that
initializing these weights they would
initialize all of them to be know some
random number or whatever they found
that actually using some unsupervised
learning to initialize them like
Boltzmann machine has a terrible has a
very positive impact and can reduce the
time to to do the training there were
other things GPU everyone probably knows
now that GPU is so revolutionary
Hinton's paper he wasn't the first one
that used GPU there were other people as
early as Iran but people were talking
about GPS back in 2006 relu that simple
simple you know FX equal to max X or
zero you know how revolutionary is that
you know that's that's but that's
rectifier rectify it so much of the
problems that in or let work had and
they found yes it actually works the
only thing is that we needed better
techniques just just just some
motivation so how did we get around the
second winter problems remember the data
would dilute what would you do give it
more later and that's where Big Data the
companies that have lots of data is so
important overfitting what they said is
that every time I'm going to hide part
of the data so that my network never
sees and randomly just keep changing it
so the network gets ready to basically
see not see some parts of the the neuron
as already input and instead still get
your trainer and that that improves the
generalization trainings they're never
converged we talked about the weight
initialization as well as using GPU and
relu
again you know because of
the that kind of max effect help to kind
of make make sure decay of the weights
don't happen stochastic gradient descent
you know and pooling so pooling is is
another technique and essentially I've
got a slide later and what it is it's
basically looks at the input in the
neighborhood and takes the max of them
you know that's that's really all it
does this is a simple you know these are
the techniques that they used to solve
these problems so where are we now in
2012 alex net for the first time Alex
Khrushchev ski Geoffrey Hinton Ilya
they got error rate of fifteen point
eight point four of image net which is
detecting various objects it is a very
hard dataset and they got this and what
they did was they use nine layers five
to six days on two GPUs they train and
they use relu and we look at what they
use
you see pooling max pooling here next
year ZF net again they use nine layers
what they changed from that because the
Alex that they use a kernel remember for
convolutional instead of using 11 by 11
kernel they you said myself you know
change some parameters and they got
better results 11.2 and that's the
network the vgg net they got even
reduced even further and that was very
interesting because they said actually
we're going to use 3x3 and it's gonna
work
and just to show you what I mean by the
kernels I just want to make sure that
we're all clear about this convolutional
because with this is what we're gonna
use so again this is what we were
talking about the kernel so Alex net use
11 by 11 the next one used is 7 by 7 and
then they use actually they saw that
three by threes is is sufficient so
where were we
okay all right so next one was Google
net they introduced this is the error
and they had 100 layers but they
introduced something else
very very interesting they call it
Inception module guess why because of
the inception film you know they had to
go be like we need to go deeper so so
they build this inception module which
essentially was the convolutional nets
which we described but in parallel and
they had different for example kernels
and they would at the end concatenate
all of that and that was and you can see
these modules here this is this is the
whole hundred networks and you can see
these modules and you can see
concatenation cooling they use average
cooling as well and this is this is what
essentially they use next one is the
resonator from Microsoft Research Asia
they had 153 layers so everyone just was
would go deep what they did said you
know in addition to
is convolutional intermediate layers I
will keep the original input in in the
input next layers that's what I do and
this is basically they came up and the
error rate when one two three point six
so where are we going
geoff hinton basically says start from
the beginning
one of the biggest contribution is made
is back propagation algorithm he says
just get rid of that build something
else we need to start from beginning and
as I explained on Subal wise learning is
very hard you try to work on ways of
fine you know progress that again has
been very promising as well as the
autoencoders so this is all well and
good and what we need to do is basically
discuss the programming language
detection so this is what I did I chose
16 programming languages
I collected two K per language from
github that 2k is not really enough you
know it's enough to make a point and
show you a working example but really
essentially I need to you know like go
maybe ten times at least of this number
I'm using Harris on top of time tensor
flow so Isaac is going to be talking
about tensor flow this afternoon at 2:20
so if you are more interested have a
look at his talk I trained on a GPU
machine
I took to ground eight hours to finish
and so you always have a training data
and testing data I had the 1k testing
data and they use pay Python all the
code is is basically here I would
publish the slides right after talk so
don't worry about
you lacked access to it so in order to
make it bit I mean detecting language is
not really that hard a problem but I
just wanted to make it a little bit more
interesting as well so I use Yan lagoons
2016 paper so anyone has done text
processing text machine learning yeah a
couple of people so in text mission
usually usually what you do is that you
break the document into words I use
completely different approach and this
is this approach from this paper which
basically what it says forget about word
and you just just look at the characters
and this you could see why it could be
beneficial in my case as well
because a lot of syntactical rules
basically apply to brackets or special
characters that basically are completely
taken away if you are doing you know
just just by words so so what I did
basically I just used these are the 70
characters are kept
you know I turn everything and use only
these these characters and then I turn
every character into into a one hot
vector and this is this is something
that you're gonna hear a lot if you do
machine learning you're gonna hear a lot
so what it is essentially is that length
of 70 an array of 70 and you keep one of
them one the other zero and this is how
you do the categorical data it's very
very simple right
and for example this this is how it's
gonna come out if you if I have import
numpy as in P this is the import word
you know I'm not sure if this is the
exact right place but just just to show
that you know the DI which is 9
character is going to be 1 and like that
you know RT and all of that this is this
is and you just concatenate these and I
took 2 K of the data and just
concatenate concatenated these 70
characters so this is this is the
architecture I've used and I'm going to
explain so we have dropout initially so
that we get rid of overfitting to a
large extent I have convolutional 3 5 9
and 19 these are the basically the
kernel size and after that we have
pooling at the end we concatenate this
is the inception module and then I have
dense what is the dense basically it has
connections from every input to every
output right and then I put another
dropout so the tens are very good in
generalizing while the convolutional is
very good at finding the local input
data because it's actually built on top
of the the the way that I neurons
basically work and that's why it's
important at the end you use softmax
softmax essentially turns
Numerical numbers into likelihood so so
at the end I will have 16 outputs and
each one gonna say what is the
likelihood and I'm using softmax that's
as simple as is my input number of
characters I said you know optic 2
- K and character dimension character
dimension 17 an output number of classes
which are have 16 class or 16 languages
so this is max pooling it's not rocket
science but essentially a 2 by 2 max
pooling will result in just getting the
max of the neighborhood so we talked
about convolution dropout you know it's
basically randomly say me randomly 8 it
masks the input convolutional we talked
about it pooling we said it's just
usually max pooling dense generalize the
features soft max turns scaler to
probability right probably you want to
see look at some of the the demo and I'm
going to write so this is basically
essentially running in my machine so
give me a github library that you want
to go to I mean I can show you something
now for example from my own but these
are not I mean I randomly got got the
data you can see how I got damn data but
this is essentially if I go to row get
this right should be able to give me
what it is
right this is C sharp so someone shouts
out a github project spark okay um
spark github github right give it a
spark all right core
no that's poem I mean I can I can
yeah okay I can go to sorry
you know I don't want to blame anyone
for this just roll I mean this is let's
go to this is the XML I know I can I can
provide actually the audacity I can
provide the URL right that's XML now I
could go to source and get test main yes
bark that's Java that's a small fight
but anyway I'm gonna risk it that's a
very small fine
I mean I think he won't be able to find
it Scala yeah sorry
I'm missing sorry the last or the last
yeah I don't think it makes it makes
much difference let me go
I mean on smaller ones he doesn't
doesn't do as well sure it's it's
definitely like PHP when I remove the
first few lines so it actually one of
the things that it I haven't trained it
on the snippet so it doesn't generalize
you have only seen files that's why I
can do only Phi's but if I give it for
example a PHP file it's not going to be
able to do are obviously this is
okay let's look at this
that was a shell yeah that's that's a
bash it does pretty well I mean the
point was was to illustrate I think what
I need to do is to introduce another
layer in between after the first
convolutional have to have another
convolutional so I I appreciate that
it's very hot in here and I'm I'm baking
but just to leave you with this as we
said that's what Hinton says start from
the beginning
do we have an to say as a this discourse
from Geoffrey Hinton is free on Coursera
and everything you need is there you
just need the machine</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>