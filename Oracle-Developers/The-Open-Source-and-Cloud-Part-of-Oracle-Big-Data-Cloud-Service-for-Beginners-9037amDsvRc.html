<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Open Source and Cloud Part of Oracle Big Data Cloud Service for Beginners | Coder Coacher - Coaching Coders</title><meta content="The Open Source and Cloud Part of Oracle Big Data Cloud Service for Beginners - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Open Source and Cloud Part of Oracle Big Data Cloud Service for Beginners</b></h2><h5 class="post__date">2017-10-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9037amDsvRc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everybody there's an
awful lot about awful lot of open source
Big Data technologies you see them here
in this light and and none of them is
really easy to understand
I wonder who had this idea about talking
about all this because the idea of this
session is to come back to the basics of
Big Data just we are going to talk about
the the base of the foundation okay yeah
basically basically I said we want to do
a presentation that I always wanted to
hear I always wanted to hear the basics
of Big Data like don't show me all this
give me the four most important
technologies and tell me how it's
working and then of course is it working
in the cloud so this is the kind of
thing we want to achieve to give you a
good understanding about the four most
important technologies and to show you
how this is working in the Oracle Big
Data cloud and I'm very happy to have
colleague here can you introduce
yourself yes so my name is quite
complicated so everybody call me
IDO so in computer engineer I is the
actual MBI I usually participate in
conference as a speaker I'm from Uruguay
and we have the Oracle user group
another group in your wife which I won
with with some of my friends and if you
are wondering what is your wine is where
that you know between Argentina and
Brazil so we have we have the oceans
very nice in summer in winter it's kind
of cold so this is the I mean I mean
square in our airport who likes like a
mothership but it's a very nice one okay
so what I would you okay thank winces my
name actually my very first paid job
ever was being a data scientist although
at that time the word data scientists
didn't even exist I was still working on
a PhD and working with a group that was
doing brain research my boss was in
urologist and obviously he knew about
brain snow and he also knew about
motivation and I remember really well
the second day I started he said Frank
come on do you have a minute I want to
show you something and he showed me this
colorful picture of of the brain surface
and he said there's only one group in
the whole world that is able to
calculate such a picture and they're
very useful because they show the
function of the brain and not just the
structure and he said we have a lot of
patients like small children that suffer
from epilepsy and sometimes we can treat
them with medication but very often they
stop responding to this medication and
then there is the possibility of doing
surgery but the surgeon obviously
doesn't really know where to cut know
and he needs to understand about the
function of the brain at this location
and he said we cannot calculate those
those those images and obviously he knew
that I was like really keen on solving
this problem and finally we solved it
with a technology that is even more
low-level and more complicated than the
very first one we're gonna present you
in two minutes
so these days my job is maybe a little
bit less dramatic but not less exciting
I work in an intersection of well using
classical oracle fusion
we're open-source tools I wrote a book
about clouds in 2011 another one about
WebLogic in combination with these
open-source things a bit later different
clouds I became a developer champion
that helps a little bit to promote this
intersection of new technology
open-source and the classical Oracle
things now a device is really lucky
because she lists I think very close to
the sea far away in front of the sea I
am so jealous because I'm based in
unique and Munich is like I don't know
at least six eight hundred kilometers
away from the sea but you know what is
very special about Munich and it's not
the Oktoberfest not the big beer party
that we're having right now and I'm
still here it is
we have surfers in the middle of the
city that's the fun fact of Munich so
whenever I nip out for a coffee I see
people passing by with their surfboard
under their arm and they go surfing in a
river that is flowing through the
English gone through the biggest park
that we have it's a steady wave behind
one of the bridges it's a well known
spots of people even from Australia come
and bring a surfboard and they surfed it
just go from left to right and then
after two minutes the others stay they
knock on their board and that's the
signal for them to go away we want to
surf now and it's one of the very few
spots in the world where you can do this
right so we start with the first
technology that we want to present and
it's Hadoop and it's the task of a
device now I tell you what we decided
the past about those technologies and
she's gonna present about Hadoop
but maybe half a year ago I was in in a
meet-up in a meet-up at the Google
office in Munich and they said wow how
do MapReduce they were talking about
this as well and one guy said we're not
doing any MapReduce anymore so just to
not only tell you the good things also
to give you a little bit about the
trade-offs is it still worth talking
about it or should we just skip it okay
I'm really interested
okay so but first of all let's let's go
to an introduction and what is Big Data
so if you google it you will see a lot
of definitions but we can base it in
three B's okay so volume we are handled
a huge huge volume of data and variety
is not one structured area is completely
raw data and structure or semi structure
velocity the way that we can receive the
data and we can consume and do something
with it is very very very fast okay so
for example typical thinness because
they all the web blogs Facebook all the
media that that wishes you know we see
all the time you know so for example
right now how many tweets do you see in
in in this conference No so although
other another being this definition
which is value so you have big data you
have what we've call it you know the
data like but sometimes you need to be
sure that is some value in this data and
this is the most complicated thing find
value in all this huge amount of data
that you have okay if you google it you
would have definition with seven eight
B's so it's people are just being
created okay we're going to talk about
Oracle Big Data cloud from dude edition
so this is a Big Data Platform which is
integrated with Oracle with sorry with
open tools open source so it's almost
ready to do city you create a cluster
and that's it you that big platform it's
already integrated with all the open
source tools and also with other path
services as database cloud my sequel
cloud even have so you can also
configure the security access data
access and or access at all security and
you have rest access to all the
functionality that you have in this Big
Data cloud computing
so this is an image we want to show you
know the demo of how do you work with it
so you have this this this is the main
page the main customer so when you have
all the things to to monitor don't you
talk about it okay and one thing that we
are going to use a lot in this this
session is the notebook so we have some
very interesting integration with a
patch is happening notebook in this
compute cloud you can interact with the
data very easily it has something that
is called interpreters so you have
different blocks and then you can
program use this lovely different
language so for example like I have a
one block you know like like this one
for example okay I can have one block
for just a shell just using UNIX another
with using a script sorry
so it's Park for example sparks workshop
or hive so we are going to see all of
that but coming back to Hadoop is this
worth it to talk about it yes of course
so how do the class consists in two
parts mainly the HTTP which is a file
system when we are going to store the
data and the MapReduce which is the way
that we can have to access the data just
to do some queries in some way and find
out which is information okay so Hadoop
is is been here for a long time a lot of
the Oracle Big Data products are based
on Hadoop so that is why it's also
important and manage huge volumes of the
area is highly scalable and is also
cheap you can have like 1000 of nodes in
a very cheap accommodation server so you
don't need a huge big data appliance you
know to have I don't ok it will process
in parallel very very large
so yes it is still working the way that
usually you access the data is with
MapReduce but right now we are using
another stuff but Hadoop the HDFS is
still very very water okay so we have
some enough video I think that there is
a except comic a cartoon bye-bye Barney
which is Swilley he's an expert of big
data but he also is kind of because he's
great a very nice cartoon about it so
let me let me show you so that is that
you have a client a name node in a data
node okay so let me posit this it's not
mine one second okay
let's start it you have a kleiner any
kind of YouTube your concrete computer
you have a name node and you have data
node which need no is an action node and
and the data nodes is that which ones
its historical data so let me let me try
to do this in another way
sorry and I have to put my eyes on okay
so let's start with it so we have a
client a name known which is the
register of the data nodes okay which
coordinate every movement of the data
and data node which stores the data okay
which happens here is that you have a
requirement you have a requirement from
the client okay it's a request from the
user and you have to define the block
the data block that you're going to D by
the split the file okay and the
replication factor okay so it will be
for example if all these three you will
replicate in three different now this is
what we have high availability of the
day okay decline divided a big file into
blocks and ask for a you want to play
the movie yes so on I can just get to it
that's the one I got okay so who's
playing with it yep just explain yes
so basically okay so basically idea is
that the registry which is the name node
received a request for the client the
client should say okay it's pedophile in
this this size of the vlogs
it sends it's also sending in which
amount of redundancy you will have and
after that their register
and after that the register is so it's
with the flick this size this file
replicate the replication factor we have
the desk line they buy the file into
this size into the block of this size
okay it's say that registers who write
the first block first and said okay you
have this know this now this now to
write it okay
the client send the data to the first
note and after that in meanwhile is
receiving the data it will get in the
second and the second is replicated to
the third one after everything is gone
they said and sent a message to the
register that everything is gone and you
have to use the time we'll do that for
all the blocks that consists the file
okay so in order to read it just access
the register node and the resident send
information which block it's in in which
data node the kind will access then no
the data no that is that is closer to
the client okay and that's it it's quite
simple
okay so the way that you read usually
with Hadoop is using MapReduce MapReduce
is a block programming part of okay so
you have a map method in a reduced meter
they map metal which will is going to do
is filter in searching and receive any
key value inputs and we reduce me to
summarize that key value and provide
them up provide another matters this can
be done in per in Python or each other
so for example the MapReduce example
that we have we have for example and
find that that has these three rows
imagine okay so we are going to do a
scan with symbols of rows so the first
thing that it has to do is a split
splits every single role in in different
parts after that we have the the process
will be mapped we will be the angel for
the mapping so he has one dog one got
one rat it counts all the words this is
the process that we are going to do an
example ok so it does exactly the same
for this please remember that we have
one file one big file and is splitted
into blocks
this is all the different blocks then
the mapping it counts the worst and then
you have then the process it will
shuffle in it so all the dogs appear
together the words that contain words so
after that the reduced process which
which will be excited that we've used
the content in account that okay they
have two dogs three cars two cats and
then it go all together to the final
result okay this is an example this will
be also in the in the for the next time
I should show where show there the
document will be ok so let me so this is
the goal this is Java code just the
packages that we need classes we need
and then we have the map the map map
method ok
just receive this training in town
and then the reduce ratios will receive
this entry form from them a person and
in doubt okay okay so let's talk about
five is it - xed yeah it's - xed but hi
fizz I don't want to frustrate you more
but if I always hear about half and then
people tell me first you explain HDFS
and we go away from the relational
databases and you tell me we have those
cheap discs and it works on on a cluster
of computers and then usually when I
hear about hive people tell me they
introduce tables in SQL again yeah and
that's very confusing now first we go
away from the database and then you tell
me we come back can you explain this
okay because we come in here and in this
in this war for some time and we all
know how to program in sequence so this
is a language that is very important for
us so you don't have to learn a new
language to access the data that you
have in Hadoop and another important
thing that's a lot of analytic tools
need this structure - can reach the day
so for example oracle bi or the I or
another analytic tools in fact there is
some one very nice that is big data
preparation material is a very sorry
this tools needs five to read group with
the structure of the table okay see so
yeah it's it's quite interesting so high
what it offers is recent open source
they now in fact is a data warehouse
offer over Apache Hadoop okay so you can
create its structure the data into
tables and you can put it like if it's
like hi people is almost you know common
sequel you can also procedural language
appeal it
PL sequel okay and all the metadata that
is there is created with this
transformation of the Hadoop data into
tables all the metadata stored in a
relational database okay so we want to
see a demo and leave them on which is so
I want to show you the cloud it's in
chrome right
it's the chrome so this is the the main
page once you enter into the Oracle
cloud computing service and then you
press here and you go to the Big Data
classic and so okay it's a big data is
provisioned already the service itself
is provisioned already know you need to
create it okay so you need to create it
in one important thing you can use some
cloud trials okay so if you if you want
to you have a trial for with with some
credits be aware that if you're not
using even hub I think one node it will
be more enough than than just just to
play with it profusely nodes all your
predecessor going in just one day or two
days okay so try to use one note just to
play with it and that would be that
would be okay so you need to create a
cluster you need to select also a the
type of version of spark that you are
going to to use and you decide that the
amount of null then CPUs and all of that
okay so this is the the main page which
is like like a monitor of the vintage
okay and then you will have the notebook
the notebook is the it's based on a
virtually acetylene is the version that
is using right now is oh seven so we
have for example some demonstrations
here and we want to use one of them to
show you how to manage Hadoop
so the notebook the possibility that it
has is you can use interpreters okay you
can use you can see you can say which
type of technology are going to use for
this particular block so in this case
I'm going to use hive okay so in this
particular version to access to use hive
you need to use GDB see interpreter okay
so you are going to see in some blog
post that previous version use
percentage five directly okay it's not
working anymore you need to use JDBC
okay in this case what I'm going to do
is to load information in the in the
hive table okay and see and show you
some graph in order to this listening
works I will drop the table in case it
exists okay so we did it it's very fast
and agreed another blog you can put a
title here you can also play it all okay
and for example here you don't see the
code you can click here and the code
will appear okay
so in this case I'm going to load data
into HD field in the HDFS the first
thing that I have to do is also uploaded
a turn into my my local system in the
cluster okay so that is why I will see
you score and this is a zip file with
the insurance or Florida that I found in
the internet okay so you can use that
you can also upload information from
your computer I'm going to see how
because this is a zip file I will I will
zip it and after I have it in my temp
directory in the cluster I will put it
into the Hadoop into Hadoop or get into
HDFS with this simple line I put it in
there
okay I'm checking the temp folder just
to be sure that it's there
okay perfect and then I will create the
hive table so this is a great table you
know that's cool it's created I will
load the data okay from this part from
the HDFS into the table that it's just
created and just quality data it's just
a simple select okay and you can change
that for a table okay it's simply that
okay and the nice thing of the notebook
is that you can have all these thin
recorders so you can fit a lot of
process that you want or maybe have
different type of visualization of your
data in the same page like a dashboard
okay that's quite cool okay so next to
the slides
no I see why we need tables again yes
it's it's important
so right now we're going to talk about
spark spark spark yeah okay yes I don't
know they use in MapReduce why do we
have to use didn't we agree you just
asked simple questions yeah why spark
it's a very good question what a device
explained the MapReduce it seemed very
generic now you always implement a map
function then you implement a reduce
function sometimes there's several
layers of this so it's very generic on
the one hand side and it's actually also
very robust because the whole MapReduce
is writing and persisting basically for
every single step a file on disk so
there's a lot of disk i/o and those
seemingly benefits like being very
generic and being very robust is also
the biggest disadvantage of MapReduce
first of all I would say it's not a very
refreshing programming model if you have
to implement map and reduce and map and
reduce you can do it if there's a really
important use case and you have no other
solution but it's probably not what you
want to do every day and the other thing
is all these persistence with the disk
i/o you might say well we need this
because it could crash at some point of
time and the idea of having all these
temporary files obviously is that you
can start up at any point again and it's
really no problem in HDFS and MapReduce
if you take out one node if you
disconnect it from the network if you
disconnected from the power all this is
kind of built in but still there is a
better solution and the better solution
is spark and spark does it in memory
that's the whole thing
so spark tries to do as many computation
in memory this is why its orders of
magnitudes faster you can imagine if you
avoid all the disk i/o you just want
much faster if you do this in memory
it's a Java framework as some people
might think so it's not like like spring
or so you can really use it in
programming Scott
Java - you can run it standalone if you
want and on your laptop if it's big
enough for a small hello world example
or you can run it in Hadoop and that's
the other thing of Hadoop this is why my
question about do we really still need
Hadoop was a bit unfair because Hadoop
is still used to run spark jobs or other
cluster systems like me sauce is getting
more and more popular can also be used
to run spark chops and also the
principle is the same the MapReduce kind
of tries to get the computing to the
data and not the other way around so the
data is distributed and then we process
the data locally so some people say
spark is kind of the new MapReduce but
just imagine we had all these other
technologies on the first slide so there
are other possibilities and one of them
for example is Apache storm now if you
start coding a little bit with spark
those rdd's they are called are DDS and
that's one take away from this spark
section they become your very best
friend because it's the data structure
that we use to interact that we used to
write spark code there is a newer
version it's called data frame or
dataset but basically for the purpose of
this presentation it's just treated as
is all the same so we need those are DDS
and the question is where do we get
those art IDs from how do we construct
such an RTD first of all the question is
maybe what is it really it's short form
for resilient distributed data set and
what it means
its robust again and now you wonder I
just told you that we don't persist to
disk all the time we keep it in memory
how can it be robust well it can be
robust the same way as you have other
things in memory that don't disappear
foreign server crashes just think about
what we do in let's say application
servers with your session that could be
an HTTP session that contains the
shopping cart that is automatically
replicated so there's it's like a
distributed shared memory programming
model if you if you want to see like
this this is also where the distributed
comes from so it's automatically
distributed we don't need to distribute
this data set
and it's a generic data set and that
super cool that means basically you can
load a JSON file you can load a table
you can load a text file and this is
also the first way how to create such an
RTD you just read in something from HDFS
from your local file system it could be
from s3 from the simple storage service
from Amazon did a way to do it and
that's more programmatic you have an
existing collection maybe in Python and
you parallelize it you can paralyze a
normal Python collection into an RDD or
you can transform one the thing is you
can transform one RDD into another RDD
but you cannot change the RDD itself so
it's kind of immutable and now of course
we need to understand how to work with
these rdd's and again I think that the
principles of SPARC you can kind of
understand really in five minutes if you
just take the key takeaways and the
principle to work with SPARC is that we
apply a number of transformations this
is what you see in the left hand side
it's the green box in all these
transformations well it's it's functions
that you know from Java 8 know it's it's
map and flatmap and things like this
they'll apply it one after the other but
nothing happens and that's kind of the
secret of SPARC it just kind of
concatenates these and then the last one
is an action and when we apply the
action something happens so the question
in SPARC always is what happens in the
driver in the code that is executed
locally and what happens distributed and
of course all this computation will
happen distributed but it happens with a
lazy evaluation so all these
transformations they are calculated when
you apply the the action at the end so
if you do a collect and you want all
your data back the transformations will
be applied or if you do a count or if
you do first and I want to do a small
demo using as well the big data compute
instance that Edelweiss was already
showing and we are also that's just a
recording I'm not showing this and we're
also using those
bookstore suddenly notebooks to see if I
can find mine it's actually really good
technology I was doing a whole spa
course based on these separately
notebooks where you have something
pretty filled and you try and try and
complete this so I highly recommend if
you plan to work with HDFS with spark
with hive to use this technology now
what we do here and let me zoom in a
little bit is we start with a shell
notebook and well honestly what we're
really doing here is not Big Data
because it's not big enough but we've
just pretend to be to do a Big Data
so what I'm getting for you is a data
set that and contains the whole work of
Shakespeare so it's just it feels a
little bit like Big Data No when did you
analyze the whole work of Shakespeare
while ago I guess if at all so what
happens is I get it I write it to HDFS
HDFS put writes it to HDFS I delete the
local file and then I do this echo done
and the shift return is kind of
executing this and you see well it it is
very very fast it's downloaded
downloading and it says done this is it
now if I do an LS and I execute this
again it will show you that we in fact
have this file which is this s dot txt
then I read it in remember I told you
you could get an RDD by reading in a
file and this is exactly what happens
here there is a spark context that is
implicit if you use the Python spark
notebook so we take the SC to spark
context read in the file and we get this
T and the TV contains well the whole
work of Shakespeare and what I want to
see is the first line actually the first
line looks a little bit funny because
it's not only the work of Shakespeare
there is a preamble that is kind of
describing the way you can use this
ASCII data and so on so this is why we
have some yeah funny first line okay
let's check if the T really is it our DD
let me execute this and you see it's a
map
partitions RDD so I'm not lying to you
it's really an RDD and now this next
part is where the real magic comes in
who could program a lambda function of
you just for curiosity one two three
that's good I was showing something very
similar at the German user group last
year at the conference and there was a
group of twenty students and not any not
anybody raised his hands I was kind of
surprised at a tree at least you should
be able to read it
okay I tell you what happens here and
remember we do these transformations
that basically do nothing first of all
and then they're executed when we apply
a action now we first take the RDD and
we apply a flat map and the flat map
uses a lambda function which defines
well what is happening with the flat map
later we use a map now what's the
difference between flat map and map if
you say flat map you apply this function
that you specify here to every item of
the data set but you expect it for every
item you apply it you get a return which
is more than one item like two or three
so basically we split the line we look
for a for a space character and you can
imagine if you split one line you get a
couple of words back then the next one
it's one-to-one its map for every input
for every word that we have we convert
this word into a tuple so for every word
if the word is like green we make it
green comma one so every word is
converted into this tuple and then at
the end we reduced those tuples and we
say by key so if there's comma one if we
have green comma one and green comma one
we reduce it by the key the comma one is
the key and for one and one we would
make two so for a and B we would replace
it by a plus B and this is kind of word
count in Sparky and obviously I should
run this let's see and this went really
fast and now I expect to have a lot of
tuples that say something like well
green comma three and red
five and all the other words that
Shakespeare was using with the number of
counts as often as it occurs in in in
Shakespeare and what I'm doing here is I
want to see them so I want to retrieve
them but I want to retrieve them from
the the one that occurs most often to
the lowest one this is why I say well
give me the first ten and this is where
I negate the key because this is the
easier way I can get the one with the
highest ranking first and as you see the
first one I have to admit I was not
really good in cleaning up the data I
get this empty thing which is also some
some lesson learned a lot of big data is
cleaning up the data first and the naive
approach of just applying our our
algorithms often doesn't work but then
you see I have the word very often used
in Shakespeare like twenty three
thousand times followed by the word I
followed by the word and followed by the
word - all this makes a lot of sense now
you'd be very surprised if the word
green or yellow would occur here's as
the most frequently used actually we can
check for green look at this green is
like fifty two times red is like what do
you think last 44 seven times but it's
Shakespeare is not writing about green
and yellow like we IT people think what
is he writing about how about 814 times
hmm it's more than green and yellow no
how about laughs the off opposite what
do you think 1,200 times a very positive
guy I would say the analysis of
Shakespeare by an IT person let's check
for laughs let's check for hate what I
did first is what I thought I'll check
for our go rhythm to make sure it's
doing the right thing and nothing that's
good so Shakespeare was not writing
about higher isms but you see what's
happening and you see how we apply those
those notebooks and how useful they are
and basically what I could do is I could
register de dis RDD as a table and this
is sometimes working sometimes it
doesn't I'll just tell you this right
ahead if I say describe table what I did
is I added
columns like W forward and c4 count so I
have W and C it's a string and the big
integer and now I want to execute the
SQL which basically says well give me
the words in the count from the table
order it by the count descending and the
first 15 ones and again you see this
huge number of nothing we're probably
just see the line breaks and other
things and you see the words again I can
visualize this this is the part that is
often not working
yeah I did it already so this is the
visualization this is how the bad data
that I have in the not a number and I
shouldn't do this now so if you
visualize it one cool trick is you can
just make it go away no and then you
have your real data the other thing that
I didn't do and if you've done this
yourself you probably realized how about
the capitalization we accounting words
like and with a capital A different than
end with a low I so all this has to be
to lower and so on but I think you get
the basic point about using spark it's
done in memory it caches results it's a
flexible programming model but the
criticism that I had before I started
using it is that very often especially
in those small examples we combine
everything that happens where is it like
here we combine everything in one long
line so the code is really short but to
write this this long line it's it's
rather complicated resembles a little
bit like just just color code in my
opinion good how are we doing timewise
six minutes okay
that's challenging I'm gonna give you a
quick introduction then into the fourth
technology that we wanted to talk about
and it's actually Kafka which is not
really big data it's more fast data and
we had a lot of amazingly good
presentation see about Kafka so I'm just
gonna give you a quick overview and tell
you what you can do with Kafka in in the
Oracle cloud and it's actually not the
big data compute service that does Kafka
there's a special past service in the
Oracle cloud it's called event hub that
is doing Kafka and so some people you
keep asking those difficult questions
actually I think I was just hearing a
presentation where they said you know in
Kafka this thing the Kafka topic it's
like a table in the database and I
thought like now I don't really disagree
there's this kind of duality of Kafka
topics and tables that we have but it's
a very abstracted way of looking into
this and if you start with Kafka I think
you should first of all look at it as a
messaging system know like WebLogic JMS
or like some other messaging system and
then the interesting thing with Kafka is
that the producer appends at the end
obviously and they disciplined operation
for the producer if you write something
to Kafka it has a constant overhead so
it doesn't matter if you've written 5
bytes 5 gigabyte or 5 terabyte writing
something to Kafka is quick and it
doesn't matter of how much data you have
already written there's really nice
performance graphs about this the other
thing that is really different to a
normal messaging system is that if you
read a message like those consumers they
read it with an offset if the message is
read it does not it's not taken out of
the Kafka topic so it stays in the topic
which is good because if you want you
can reread it now
Kafka system stores the position of the
the last message that you have read and
two different consumers can read at a
different location
and the other thing is that it's it's
partitioned so I've seen a lot of
projects where they did the
implementation but then at the end day
they didn't really understand about the
partitioning and about the replication
so one good question about every Kafka
project is how do you partition and how
do you replicate and why now so the
partitioning that we do is we split one
topic across different brokers to
different brokers are on different
machines typically and we do this why
well we do it to distribute the load to
distribute the messages if the
throughput is per broker it's kind of
multiplied so we reduce any throughput
issues and we increase concurrency
because we can write at the same time
there's something else which is called
replication and the replication is done
well other reasons and can you guess why
the replication is done because one of
such one such topic like a one could
fail and the question is where's our
data and if you want to compare it to
messaging like to the WebLogic messaging
that we have for the whole fusion
middleware stack we don't do this no we
don't replicate distributed queue to
another instance in WebLogic if this
topic a one would fail it would be
restarted on the same machine it cannot
be restarted on the same machine that it
would be migrated to another machine so
the Kafka mechanism is different it
replicates on to another broker so we
have replication we have partitioning
and we should configure both and we
should know how to configure them and
they have a different purpose and the
third thing that makes Kafka a little
bit challenging is that we have this
extra thing that is called suki / that
is used to to store state so at the
beginning the state where such a
consumer was reading already what was
the offset of a consumer was stored in
this zookeeper process this has changed
now it still has the functionality of
well of keeping state in a distributed
way and a producer can ask the zookeeper
system
how to connect to a particular topic
where this topic is located maybe I give
you a very small that's a good question
no I think we just skipped a demo it
doesn't really make sense there is one
very popular example and I kind of
recommend you to get the slides and to
read the New York Times was doing
something that goes towards what I've
told you about dis duality that there is
a duality between Kafka streams and
tables so the New York Times they took
one Kafka topic which is one partition
only so no magic and they write with
different producers into one topic and
they read with different consumers from
this topic now this topic the messages
as I told you if you read the messages
they are not deleted typically there is
an expiration date which is default
seven days but I also disabled this
expiration date so they keep the
messages there forever and if you think
about this day use it more like a
database table and the cool thing is
that they can add more different clients
here and take people to publishing that
they did since 1850 something is in one
single Kafka topic and if they add a new
client here the client can read about
all the publishing that was done for New
York Times if you have a client that is
already reading from last week it could
continue to read from last week so this
is kind of integration technology more
this is more where Kafka is used as a
table and just to stay in time there's
different clients for Kafka
one is a Java client and then I wanted
to show you it's I'm going to show you I
still have one minute should show some
Java code at least at Java one that's a
Java client this is all that that is
needed to write a message to Kafka there
are some set up properties where we
basically need to tell the system how to
talk to Kafka
this is Kafka event up running well as
an event up service in the Oracle cloud
and what I do is I like I write like
three messages or like 13 messages I
create a producer
the producer sends it you can specify
the petition you want to send it
hard-coded which is probably not so good
you can supply a key then it's using the
key to to hash the key and distribute
the data or you don't specify even a key
and then the data is send in a
load-balancing way and if I execute this
it's running and it should print like
those Asterix and you see it's printing
like 13 Asterix now the cloud service
looks very similar to what a device was
showing for the big data service you
provision a cosmic kafka cluster then
you provision a topic then the the
biggest thing that that happens
typically to people and I've seen a
posting from a really good colleague who
said like oh I couldn't access it you
can't access it because it's kind of in
the cloud and there are ports which are
closed so what you need to do and this
is maybe my last slide for Kafka and
then we conclude what you need to do is
you need to open ports there is support
for Kafka itself and then as a port for
zookeeper and depending on what kind of
tool you want to use you need to open
those ports it's not difficult you go
through the instance you go to access
rule and open them but it's also not
really well documented so before using
external tools you need to be aware that
this instance is in the cloud and that
you should open those ports I think with
this we conclude and we jump to the end
if you have very little time and this is
kind of the summary too long didn't read
I think
this open source technology is a really
good way to get started what a device is
doing with all the advanced Oracle tools
where you get all the cool graphics and
and all the stories behind the data but
I think you should know the the basics
know to get started so I recommend to
learn them first if you go to the cloud
and we were trying to show this with the
Big Data instance and with the notebooks
you have lower setup times we had some
struggle with the licensees and with the
resource usage now
told you don't create a very neat
cluster because it will consume
everything in just a couple of hours try
to keep it one note just to play with it
and start working with it with it with a
cloud trial and then you will see I
agree otherwise they would everything
will will will be blocked I think in
total it's may be slightly more
different that then it seemed here so I
recommend to either go to a hands-on
workshop may be here at open world Java
one if you can or the other thing is
it's the cloud it should you know it
should start up in a few minutes and
then people should be able to demo
something so talk to you friendly Oracle
sales consultant and say no no no I
don't show me powerpoints don't show me
a provisioned generated demo that people
other people were doing show me
something life hands-on like what we
were doing so all the code we were
showing was basically we can copy and
paste it and redo it yourself and if we
had like two hours we could just type it
and again I think those those notebooks
are really really cool I used them for a
first part training and any time I would
go this way if you think do prophetÃ­s
professional serve professional software
run this way no in Big Data cloud
service you can just submit a spark shop
and then it's executed at the background
well we upload the slides you can
contact me you can hire me if you want
you can contact Edelweiss this is a
tutor data and at the end we would like
to thank the Oracle s program possible
oops or something like that you can be
into the into into the extra room so you
can just go to to that link and and try
to be part of this right so last
sentence sign up for a cow trial make
sure you don't hit the resource limits
and code something amazing thank you bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>