<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CON5938   How Netflix Thinks of DevOps—Spoiler: It Doesn’t | Coder Coacher - Coaching Coders</title><meta content="CON5938   How Netflix Thinks of DevOps—Spoiler: It Doesn’t - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>CON5938   How Netflix Thinks of DevOps—Spoiler: It Doesn’t</b></h2><h5 class="post__date">2015-12-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RCc4IrUXr38" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Diane marsh I lead the engineering
tools team at Netflix and we're here to
talk about DevOps so I put in the title
how Netflix thinks of DevOps with a
spoiler that it doesn't and I guess I
wanted to start with before I give you
that punch line I wanted to start with
asking all of you how many of your
companies are doing DevOps and whatever
that means right so what the challenge
we face today and I think the challenge
we face as an industry is what does that
mean we keep using this word right
just like agile it's kind of means
different things to different people and
so I think it's really interesting to
try to figure out what we mean by DevOps
I'm pretty sure that of the people that
raised their hands in the room there
probably isn't a lot of agreement about
these people doing DevOps and these
people doing DevOps but that's fine
because we're not gonna try to define
DevOps
today what we're gonna do is we're going
to talk about what Netflix does and I'm
gonna give you DevOps and three acts
because you know movies I'm going to
talk about three things I'm gonna talk
about the scale at which we work and
give you a little context around Netflix
ecosystem I'm going to talk about
culture because the culture of Netflix
really lends itself significantly in to
how we do DevOps and I'm going to talk
about tools I'm gonna give you the this
scale or on Netflix the ecosystem
because we're really driven by scale so
DevOps is driven by scale the solutions
that we provide we're empowered by the
culture the culture and Netflix
definitely influences everything that we
do including how we deliver software and
we're supported by tools and I'll
explain what that means in a minute
because we're going to talk about scale
as you might know Netflix is approaching
global reach we've announced that we're
going to be mostly global by the end of
2016 this means that we have a lot of
services and a lot of customers and a
lot of details that we need to keep and
keep track of I'll give you a little
summary of the Netflix ecosystem we have
hundreds of micro services
thousands of daily production changes
and these aren't coordinated team to
team we'll get into the culture to talk
about that we have tens of thousands of
instances are running in Amazon we have
hundreds of thousands of customers
interactions per minute as you might
know we have millions of customers the
one thing you probably don't know is
that we have billions of metrics that we
collect 2.5 billion metrics per minute
are being collected by the Netflix
service this has really driven how we
interact as a company with our with our
services and it also means that we have
to provide a lot of support through
tooling to be able to manage such an
infrastructure in 10 billion hours of
streamed entertainment means that a lot
of people are using the service and
there's a lot of responsibility that
goes along with that right the same time
we only have a handful of operations
engineers so I want to really drive home
the point that we don't have a knock and
we only have a handful of operations
engineers to support all of this
infrastructure how the heck do we do
that well we have one key tenant at
Netflix you build it you run it so what
this means that the developers who are
on the team that build a micro service
micro service do one thing and do it
well there they're loosely coupled with
other micro services but tightly aligned
so we know what the what the other
services are doing and what the other
teams are doing but we don't coordinate
deployments we don't specify what the
what time the developers do deployments
because we believe that the developers
know their service best we let the
developers decide when to deploy their
service will they deploy it at peak peak
traffic in the US peak traffic and EU
right now it might be peak traffic in
the EU right is it a good idea to deploy
my service in the EU right now I don't
know the conservative the might among us
might say no but if it's the service
that's not really impacted by a peak
sure go ahead
right we don't have restrictions on our
tooling that provide that that insight
that says don't do that right one key
thing about you bill that you run it is
the running it part the developers are
the ones that are on call 24/7 the
developers on that team are the ones
that know that service the best the only
person who's going to bring that service
back up if it goes down are the
developers that wrote it it's
particularly unfair to ask some poor
operations guy to figure out what's
going on with a developer service and
fix it right instead why wouldn't we
lean on the people who made that change
yesterday or who made it today they know
what they changed they can make a best
assessment about whether they want to
roll back or roll forward so why would
we leave this to an Operations person
right on the other thing we do have a
cast of characters that are involved in
an outage the developers on the team as
I mentioned and we do have a core
engineering core a critical operations
reliability engineering group that is
responsible in an outage to make sure
that we get the service back online
that's their only job get the service
back online and we have an A Crisis
Response manager who's someone like me
who is sits in an outage and is also on
call for outages sits in an outage and
just make sure that people stay aligned
and that information is flowing between
the developers and other teams to make
sure that the the core group
is not trying to root cause the problem
right now and that the developers aren't
aren't interested in doing that because
our sole focus in an outage is to get
Netflix back online so that all of you
can stop sending me Facebook post says
and that books is down that happened I
think I was at Netflix for like two
weeks and we had an outage and I started
getting Facebook post hey do you know
that puts this down is that your team
so yeah pretty funny anyways so that's a
little bit about the ecosystem that we
live in at Netflix now I'm going to talk
about culture does it seem weird to be
talking about culture and a DevOps talk
no good good good we're not just going
to talk about what a devops culture is
right because I think that it's it's
all-encompassing
how many people have heard of the
freedom and responsibility culture at
Netflix in the the slides deck that this
refers to so it's about a hundred and
twenty one slides in a slideshow and
it's got some really interesting stuff
in that Sheryl Sheryl Sandberg said that
it might be the most important document
that came out of Silicon Valley in a
long time you could read it at your
leisure but I'm going to summarize one
really important piece of this culture
deck and that is freedom and
responsibility usually when people hear
freedom and responsibility they really
only hear freedom and they don't hear
responsibility so I want to make sure
that there's an and there okay so the
developers have the freedom to make
pretty much any choices they want I
already talked about they could decide
when to deploy they can also decide
which language to use to develop their
system in they get aside which
frameworks they want to bring in they
can decide how much testing to do none
of this is mandated across Netflix
instead it's left to the developers
because we understand that the
responsibility is ultimate they're
building it they're running it if they
make choices not to do good tests in
their service they're the ones who are
on-call they're the ones who are going
to get that phone call right
so it's we're very well lined with
responsibility they're very much
motivated to make sure that that system
is the solid as there as the risks that
they're willing to take I'm going to
tell you a story though
about about something that that we we
had in our tooling we don't have a lot
of controls and our tooling so how many
people have changed management control
in their deployment tools so less than I
thought about about half of you raised
your hands so we don't have any change
management control and the for the vast
majority of our tooling and I'll tell
you the exception in a second this means
that I don't need to get approval I can
deploy as any developer and Netflix can
deploy to production at any time right
we do this because we want to make sure
that we can roll forward and roll back
in an emergency without having to jump
through a bunch of Hoops and get the
right people on the call there have been
times when this has bitten us and I'll
tell you about a time where it did there
was a safeguard in some software that
prevented one of our developers from
being able to deploy something to the
test environment he really needed it to
go to the test environment and really
needed it to go badly and so he talked
to a bunch of people on the team and
they decided that he should work around
this safeguard it's what we do at
Netflix if you know you put impediments
in someone's way they work around them
to get their work done it's perfectly
fine it's part of the culture so he
worked around this the only problem is
that he made a mistake and the mistake
was that the thing that he changed in
order to get around this this control
accidentally deployed something to
production instead of to test so what to
test to but it also showed him from
production and that's Billy not what we
wanted to do so they fixed it you know
there was a rollback and you know all
the things that I described earlier
around the the teams that were on the
call that got handled
but I'll tell you that one of the
proudest moments in my time at Netflix
so far was the next morning when I
opened up my email and there was a note
there from the vice-president whose team
had deployed this this errant change and
he you know he said there's what
happened we worked around a safeguard
we accidentally deployed something to
production that was truly unfortunate
but now we have a decision to make
and what he said was we're gonna get rid
of the safeguards that caused this guy
not to be able to do his job so we're
not gonna add more safeguards because
that's what we normally we would do
right we're not going to add more
safeguards instead we're going to get
rid of these safeguards because what
these safeguards did was prevent
visibility into the change that this
person should have normally been able to
do through the normal course of action
and instead had they dig deep into the
bowels of the code and change some
database flag somewhere right so let's
get rid of the safeguards I thought that
was a really really great response I was
pretty proud of our company so when
people ask me well in that flix culture
scale as the company continues to get
bigger I point to this example and I say
yep we're doing that we scale the
culture with every hire people that I
hire into my team I hire I hire
engineers into my team and managers I'm
really careful that I'm bringing in
people that believe in freedom and
responsibility and a lot about the
responsibility part but that we value
the freedom part too right
it'd be a lot easier just to tell people
you have to do this you have to do that
but ultimately we would slow down our
pace of innovation if we did that and we
really value and we would also give
people less control about their work and
we really value the fact that we have
all of these engineers coming up with
great ideas and being able to work
independently without a lot of
safeguards and controls in their way so
if I said the key tenant of DevOps at
Netflix was you run it you build it you
run it a key tenant of working at
Netflix is
acton Netflix best interest it's pretty
easy right now I promised I would give
you an exception to not having
safeguards how many people work in an
industry like finance or healthcare or
government where you wouldn't be able to
be quite so blase yeah so um if I didn't
give you this caveat you should all just
shoot arrows at me I'm gonna say that we
can do this because we are in the
entertainment industry you might be sad
when that flicks us down I am sad when
that flicks us down but nobody's gonna
die and nobody's going to lose their
fortune right and so I am grateful to
all of you for the safeguards that are
in your code that keep my money safe and
then make sure that you know drugs
aren't administered haphazardly into
people's systems so thank you
another key tenant of our culture is
this blameless culture and that we have
really healthy instant reviews how many
of you do instant reviews after an
outage and so we have instant reviews
after each outage and we talk about what
happened and we try to come to a
resolution so that it doesn't happen
again right we only want this incident
to happen once ideally right after I
joined Netflix I went to one of these
instant reviews because I wanted to see
is this a healthy culture that I just
joined right I mean sure everybody that
joins your like culture sounds great I
will know it when I see it right we'll
see if it works out but I went to an
incident review and I wanted to see if
people treated each other with respect
if there was finger-pointing because you
don't want this right you know what this
poor person to feel bad the guy that
made the change that pushed something to
production instead of tat well and in
addition to tests he probably felt
horrible
so the blameless culture though in the
incident review says hey what happened
made a change push the wrong thing
somebody else says you know the tooling
got in the way let's get rid of the
safeguards right I've also been in
places where we didn't have it in
reviews where we realize that our
tooling my team's tooling specifically
didn't give you enough visibility about
what where what this change was gonna
impact right was it gonna impact all
instances in all regions that would be a
lot if your if you didn't think that
that's what the change was gonna be
right so let's make our tooling
effective and in an incident review I
tell the core team please invite me the
incident reviews where you think a
change to tooling put in could
positively impact the company and result
in less outages right can I solve this
problem with tooling by giving more
visibility not by more controls so I can
almost always see something that we
could do better when I go in an incident
review and what I saw and the one that I
went to was everybody raising their hand
saying I had some blame I had some blame
you know I could have done better and
that's a healthy incident review and of
course it's only healthy if you come out
of there with an outcome right of fixing
it and that the same problem doesn't
happen again
so also important
yeah so no safeguards okay one more
thing about culture so I said that we
don't really mandate what people do at
Netflix right we don't mandate what
teams do but we have an interest in high
availability right I said we're in the
entertainment industry and I may have
sounded a little blase about that not at
all we have an interest in high
availability and as our customer base
grows more and more and more people are
being impacted even if when it's only a
small percentage right there's a lot of
responsibility around that so last year
we were looking at ability and we
thought we can do better but we don't
want to impact velocity so how can we
shift that curve so that we can actually
have just as much velocity and not
impact availability I mean we all know
right we can certainly increase
availability if we just don't make any
changes that's not healthy either
so we came up with this notion of
production-ready
first thing we did was identified a core
set of services the services that have
to be up and running in order to show
you a movie just the the minimal set and
we worked with those teams to make sure
that everything that we know about best
practices about running in the cloud
they know about - and that they can take
advantage of and if their team needed
help implementing those things we found
people across the company we pulled
together a cross-functional team and we
helped them get those changes into those
core set of services and mostly we let
those teams know you are the course of
the services don't pull in other
services because basically you're
bringing them into that inner circle and
the tighter we can keep that inner
circle the more likely we are that we
can we can attain high availability
because the other services can have good
fall backs right you don't need to have
the best recommendations for you in
order to be able to watch a movie right
sometimes it's good enough to just watch
a movie the conformity monkey can help
with this the conformity monkey is one
of our open source tools that basically
has a list of rules that you can
describe and it inspects your services
with respect to those rules and emails
the service owner so this is not a
destructive monkey will see a
destructive monkey later but this one's
very kind it's just a reminder about
what the best practices look like in the
cloud
the little Netflix OSS moniker I put in
the bottom you're gonna see that in a
lot of my slides around tooling that
means that the tool is available in open
source and you can get it from our from
our github account and I'll give you
that information in a little bit ok so
we talked about Netflix scale ecosystem
we talked about culture and I'm gonna
talk about tools but before I do I want
to give you the warning like I said with
the CMC's the tools that we build are
really empowered by our culture right so
they're influenced by our culture it
means that things like CMC's aren't
supported it means that we don't have
controls about what time of day you can
push to production or whether you can
push to all regions at the same time so
these things may or may not translate
into your environment
what I'm hoping you can take away from
the talk is something that you can use
it would you know it's all good and
interesting for me to come up here and
tell you about what Netflix does but I
will feel like a failure as a speaker if
I haven't given you something to take
home right something that you can use in
your own environment and so we're going
to talk about a lot of open source tools
how we use them at Netflix and how it
helps us to build up an environment
where a handful of operations engineers
can support the scale that I described
I said it was complicated and it is this
is a visual representation of our
services our micro services architecture
does anybody want to take notes yeah
nobody understands this the people that
come the closest to understanding this
are the core team the the critical
operations reliability engineering team
because they have a really good sense of
which services are involved when they
have an outage but nobody nobody can
know this and so it's imperative that we
build tooling to help people out right
it's irresponsible for us to say you
build it you run it and by the way
that's it so we built some tools this is
an internal tool called self
it's a runtime tool that inspects
runtime call graphs so who calls who and
if you could see this it's pretty blurry
and pretty small because it's
complicated if you could see this you'd
see which services call which services
and this is sort of a visual
representation of a portion of our micro
services from any different service
looking at its dependencies right so
this can give you an understanding about
how complicated it is the good news
about salt is that it runs and nobody is
harmed in the building of this service
meaning that we don't have to write
documentation and it doesn't go out of
date okay it's run time it's always
available it's always up-to-date people
can look at it it's not open sourced but
it's not that complicated something that
you could write if you wanted to
it takes a lot to build a microservices
architecture that includes hundreds of
microservices and that all are able to
communicate with each other so we have a
core set a common set of runtime and
shared libraries that we package for all
of our services to use and it's critical
that our services use these things
because these are fundamental to
operating in the cloud
you have to know a lot about Amazon and
a lot about how to communicate with your
other instances but Eureka helps with
that process so Eureka is our discovery
service that lets micro services
discover other micro services ribbon is
our IPC library that helps those
services talk to each other and it's
more than just the generic IPC it also
knows when to stop talking when to give
up on you okay
misterx how many people have heard of
history --ks yeah so hystrix is a way to
mitigate concerns shed traffic if you're
in trouble and help heal a service and
I'll show you a graph in a second and
Zul how many people have heard of the
Zul yeah we use dual four for traffic
rerouting we use it for many different
reasons but most recently we use it for
failover region to region and I'll show
you a demo of that also anybody get the
Zula reference who's what's that you
don't cross the streams awesome great
all right so here's a picture of history
with some automated recovery the good
news is when histor ik's shedload when
it got this alert and shedload it didn't
wake anybody up
okay so key to a DevOps culture if you
run it you build it you build it you run
it is that we don't have to wake
everybody up for every little thing
let's see if we can keep the developers
the people out of the equation as much
as possible
okay so my team is engineering tools we
support this ecosystem as well with a
variety tools so two of them that I'm
going to talk about today also both
available in open source are a manator
and spinnaker so as I said we deploy to
the Amazon Web Services we package up
our services as an immutable object of
deployment so once we package that it's
done we don't use chef and puppet it's
not dynamic it's it's immutable so the
way we do this is what we call a manator
it's a real word it comes from biology
we didn't just make it up it's not a
misspelling for animator which I can no
longer could say but am inator basically
takes an underlying Linux operating
system layers those libraries that I
talked about in the last slide on top of
it and bakes this into a new ami which
we call our base ami our base image it's
shared by all of our services meaning
all of them are built using this right
this means that we're able to build
security changes at the base am i layer
it means we have a good understanding
about what's running across our whole
ecosystem because everybody uses our
base am I then a service owner layers
their service on top of the space image
their jar or file on top of the space
image and we bake that as well
with a manator and this becomes our new
objective deployment this gets deployed
to test and it gets deployed to prod so
the same thing as tested in both places
so this is pretty critical to our
operations
means that we can reason about what's
everywhere right it also means that we
know when something like heartbeat
happens who to reach out to that they
need to update rebake their service and
redeploy spinnaker is our next
generation tool for cloud management
continuous delivery and deployment how
many people know about Asgard a few
people so Asgard was an open source
project that one of the most popular
open source projects have come out of
Netflix for deployment to the AWS cloud
write it there's a lot that goes into
deploying AWS you have to know a lot of
things you have to configure elby's you
have to do a lot of changes as greg did
a really good job of encapsulating this
and abstracting it away from the user
but Asgard was built in a world where
Netflix was only in one region and where
we didn't aspire to have continuous
delivery so instead of rewriting Asgard
for supporting multiple regions we
decided that we would actually build
from the knowledge that we gained from
Asgard and build a deployment tool that
would allow us to support the continuous
delivery platform from check-in to
deployment in multiple regions and give
visibility into what's going on in the
cloud that's what spinnaker is it's not
yet open sourced you can look for it in
the next couple of months but I'm going
to give you some previews of screenshots
today
I said that Asgard was good at global
cloud management we spent a lot of time
on UI I think over the last couple of
years there's been an increasing focus
of UI for internal developers right our
developers spend more time every day
looking at our screens than our
customers spend looking at Netflix right
so we should give then our developers at
least as good of a user experience as we
give our developers what's important
here is that you can you see those
little red dots each of these green dots
represents an instance running in
production in that region and that all
three regions are depicted there and the
red dad said that instance is not
healthy so you can drill down into that
instance and get more data about it so
this is how we're managing cloud the
cloud with with spinnaker what's also
really critical for our ability for our
developers to you build that you run it
is that we give them an easy way to
deploy their services onto this
continuous delivery platform so we have
a workflow engine you can define your
own stages you can define what it means
to move from stage to stage and it's all
pretty graphical and pretty understand
and pretty understandable we also
support automated global delivery so we
have three regions if I push a change to
production and I want to push one region
at a time because I don't want to take
the whole world out at once then I might
forget about those other two regions you
know I may sit there and watch my
service it looks like it's doing great
and then I've forgotten about the port
EU the Europeans were probably really
bummed if I didn't give them the next
version of my software and it would
drift right and so the cognitive load on
the developer is high oh gosh I need to
remember to update the EU so instead we
built into our pipeline an automated way
to be able to promote to the next region
what this shows is a manual stage that
says I'm going to manually deploy to
production
now Diane you said continuous delivery
and now you're talking about manual
stages what is this all about well there
are times when you want to do things
manually right also it's a reflection
that at Netflix not all teams are in the
same place and they're done their
journey to testing and continuous
delivery not everybody has really great
tests that can tell them whether we're
ready to promote from stage to stage and
so by promoting by providing a manual
judgment stage we enable those teams
that don't have good automated testing
to be able to use this tool at least as
good as they could before right as we at
least as great as their experience was
when they are manually deploying the
benefit to my team is that they're on
our tool and that the tool is there
ready for them when their process
matures now give you a story this is
kind of funny in sort of a an aside one
of the guys in my team we have we ever
had how many people have Jenkins in
their environment how many people find
Jenkins to be slow and annoying
sometimes yeah
so our developers like to have high
velocity so we decided we would split
Jenkins into 22 masters this is not
really very maintainable when you need
to upgrade Jenkins it's a pain and the
Royall behind so the developer that I
that that is working on that project
said hey I wonder if I could use
spinnaker to automate my upgrade of
Jenkins now I know it has a lot of
manual stages where I need to do
something but I'll just use the manual
stages spinnaker so he automated the
entire process
now how automated was this well not very
automated but it basically meant that it
was self documenting it was a script
that was self documenting it ran in the
workflow and if we can mature that
process it will grow with with
Spinnaker's features and functionality
so the developers who develop spinnaker
were shocked that he used in this way
but pretty pleased that it was this
configurable and it's led us to go out
and talk to other teams who have a very
manual delivery process and say hey if
he did it
maybe you can't - right all right let's
move on to insight tools so we've built
our services with this common set of
libraries we've deployed them and now
they're running out in AWS but remember
I said we had tens of thousands of
instances and they're doing 2.5 billion
metrics per minute so we probably need
some insight tools as fortunately we
have three of them that I'm going to
describe today Atlas is our telemetry
platform not all telemetry platforms
that are available out in the wild today
and commercial products can handle 2.5
billion metrics per second and not in
the way that we want to handle them this
is on time series data it's there it's
gone we're happy not to encumber it
forever and ever and ever we can control
how much data we keep we built this
telemetry platform because we needed to
you may not need to and there's a really
great talk by one of my co-workers
called how we built a telemetry platform
and why you shouldn't and so he might be
amused by that if you need it if you
need to build it build it if you don't
one of the commercial products is great
but I'll show you some some screenshots
so we can easily see metrics with Atlas
we can easily build dashboards and these
dashboards are what really critical to
our understanding about what's going on
in an outage right
and we can really draw our attention to
a drop in the key metrics for this
service so that we know when there's a
problem and we can see when we have
recovery okay this says gosh we really
got to get some people on the phone here
now realistically that drop caused an
alert which paged someone right so it's
not this isn't a manual phone call for
the most part this is usually an
automated page when I said you build it
you run it those teams are responsible
for understanding the key metrics around
their service and configuring their own
alerts and their own pages let's see one
thing about Atlas is that it is deployed
widely across the Netflix we we use it
for all of our teams are looking at it
but they're not looking at all the time
so we moved into a new building about
two months ago or so and before we moved
there were these big screens up on the
wall that showed graphs like I showed
you here and here and they were in the
core group that relied this critical
operations reliability engineering group
that I mentioned they were in that area
they had these graphs up on the screen
and when we moved into our new building
there's a lot of glass there's not a lot
of places to put monitors and this
originally caused a little bit of like
yeah what are we gonna do
where are the monitors gonna go and
about two three weeks into our our new
space I walked over the core team and I
said hey just curious I see you guys
don't have the monitors up and they said
you know it just sends the wrong message
for us to have monitors up we don't
really need those monitors we have great
alerts that let us know what's going on
the monitors are basically just eye
candy for us for you know for the most
part and when we have an outage we're
gonna bring up those screens anyways we
don't need big monitors on this on the
wall so I thought that was really
interesting about how we use Atlas at
net
and how we use those dashboards all
right Etta how many people are running
in AWS how many people have been
throttled when they try to make API
calls so when you have 800 developers
all trying to hit the api's you get
throttled this is not ideal so we built
Etta as a caching scribe it keeps track
of the history of things that are going
on in the cloud and this provides us
with with two huge benefits one is we
don't get throttled because the
developers are hitting this cache
instead of hitting AWS directly and the
second benefit is that we have some
historical data you can't get an
historical data out of the Amazon as
easily so we have this historical data
that we can go back and look at later it
doesn't also available in open source so
you can get more information on the tech
blog how many people went to the flame
graphs talked yesterday yeah so one of
the tools that Brendan mentioned was
vector this is a tool that their team
open sourced for giving you performance
information about running instances in
the cloud there's a really great tech
blog about this but I'll show you just a
couple of screenshots that let you see
how you can drill down into information
and get very specific information about
instances okay so think about vector is
it's it's not a huge hit on performance
to be monitoring the system it also lets
you get data in near real time and so
you don't have to turn it on ahead of
time so you don't have to always have
this running at ten thousand instances
which we see is a big benefit and you
probably would too all right
anomaly detection remember I said that
no monitors on this on the screen right
no matters on the wall we're doing a lot
of anomaly detection with a lot of
different algorithms to understand how
we what we expect our service to look
like versus what it does look like
and so here's the result of some of the
first attempts at anomaly detection we
got some threshold based alerts it had I
thought a six to eight minute delay
though between the time that the
strapped we noticed it and we got an
alert okay six to eight minutes doesn't
seem like a long time but imagine
sitting on your couch and you're trying
to watch a movie right and you're
annoyed that's like six to eight minutes
is a long time right it's like a third
of you know any one of the really great
shows like Longmire right it's like a
third of longliner all right anyways so
we decided that we would move on because
we really want to help out poor Megan
here this is you sitting on your couch
going it's down right I'm gonna die well
you're probably not gonna die but you
might feel like you want to so how can
we help Megan well we thought if we can
look at the real-time streaming data
bring some smaller intervals finer
granularity and some ensemble learning
with machine learning I told you these
guys come up with crazy ideas right the
hard problems of Netflix are not solved
yet when people are still looking at new
ideas and new ways to solve these
problems and so we're combining a bunch
of metrics to give us a prediction about
whether or not that what the anomaly we
saw is it really an outage or is it just
a little blip so we're pulling together
a lot of different algorithms and voting
on whether or not it's a real outage so
each of them has similar data but a
little bit different but together it
tells a story that that really is truly
an outage Suns the alert and we brought
that six to eight minutes down to less
than one minute this was important we
thought this is a good result we think
we can actually get this even better the
action though wasn't alert right Oh
alert is a page a page as a person who
you're waking up or you're asking to go
do a bunch of research and figure out
what's going on with their service
can we get the humans out of the
equation this is constantly what we're
asking ourselves remember we're talking
about DevOps here guys remember we have
a very small handful of operations
engineers and a lot of services and so
we want to keep the developers focused
on building and running their service
where they need to but not on things
that computers can do well so what can
we do well we can do some analysis and
outlier detection a lot of times if you
see something like this red line up here
those businesses are just bad and one of
the ways that we can repeat we can
remediate this is we can just kill off
those instances and let the out of scale
or bring up a new one in its place isn't
that better than waking somebody else so
coupler is our new tool around
unsupervised machine learning this is
not yet been open source they don't know
if it will be again you know the freedom
responsibility culture at Netflix means
that the individual teams decide if
they're going to open to our software
right it's not free to open source
software in terms of a burden on the
team so the team has to decide if it's a
good choice for them if they decide it
is then they will open source it but
they haven't made that decision yet
anyways density vise cluster algorithm
it has a lot more options than just
sending out an alert a page you can do
the alerting page you can take an
instance out of service you can detach
it and you can terminate it okay so it's
a lot more powerful but what if we
didn't have that problem happen in the
first place how many people do canary
analysis how many people have heard of
Netflix doing canary analysis
okay how many people think canary
analysis is the same as a/b testing okay
good it's not um so a be testing use the
test that we run on all of you to decide
if we want to put something into our
service or not
canary analysis is a test that we run on
our services to figure out if the new
version of the service is gonna cause a
problem where the old one didn't so
we're looking at regressions so I'm
gonna talk a little bit about the canary
release process so we have a red-black
deployment we have a choice you have all
have a choice when you replace a service
with a new version right so we have a
cluster of running set of instances of
the old service we could have just
replaced all of those instances with the
new version of software but that's not
what we do at Netflix and I'll tell you
why in a second instead we spin up a new
cluster and we put the new version over
here and we start trickling traffic over
from the old to the new production
traffic all over from the old to the new
and we're looking at these metrics and
we're measuring and we're deciding is
this causing any regressions is it
causing a problem and if we decided that
we keep trickling traffic and so at one
point we have a hundred percent of the
traffic being served by the new version
right so now traffic is turned off to
the old it's all going to the new and
the team decides how long do I leave it
in this state the advantage of leaving
in this state is can you guess yeah roll
back we can just turn off traffic to the
to the new on to the old we don't have
to restart instances or anything like
that right it's just right there
available okay and Meghan doesn't cry
because her best friend died so um the
team decides how long believe it in this
state remember I said they know their
service is best some teams let it run
through a peak traffic period some teams
are like yeah an hour is good enough
right they know their services
that's canary analysis it's sort of a
summary about computing scores making a
decision this is what's happening on
behind the scenes and as we plug this
into spinnaker one of our stages what
does it do can do that automate that
roll back if it detects a problem all
right chaos engineering who's heard
about chaos engineering so the term was
kind of coined at Netflix
around wreaking havoc in your production
environment about stuff that really
happens in real life right so it's not
irresponsible you're not doing things
that might not happen in real life but
realistically stuff happens in our in
the cloud stuff happens in our data
centers and we need to be able to
recover from that so imagine a monkey
loose in your data center running around
pulling out chords in our case it's a
little more humane than that it's it's
killing instances right how many people
did this way back when like when I was
writing distributed systems we did this
with just a kill - 9 right they the the
chaos monkeys a little bit more humane
than that it actually lets people know
I've killed your instance it runs every
day during work days it's running right
now in production to be able to keep us
basically keep us honest because we all
know we have to survive the chaos monkey
so it's important to survive the chaos
monkey
but it became critically important about
a year ago when we had the hypervisor
vulnerability who remembers the Xen
hypervisor vulnerability painful
maybe yeah it was painful for a lot of
people the good news that Netflix was
that by the time we got to the hens and
hypervisor vulnerability even Cassandra
our stateful service was resilient to
the chaos monkey we'd built some
automation to be able to recover from
the from losing instances in production
even for stateful services
and the backstory is that the weekend
when they were restarting a bunch of
instances at Amazon was the weekend when
all of Netflix was out celebrating our
50 million global members so we were all
at a party together the timing was not
ideal right so we thought what do we do
how confident are we in the chaos monkey
are we confident that we've protected
all of our services and we ended up all
going and what you can see is that about
10% of the Cassandra nodes did need to
be rebooted about 10% of those did not
reboot successfully but the automation
was able to manage that successfully and
we didn't have any outages related to
this reboot to this great reboot effect
all right fault injection testing we
want to be able to inject thoughts
between services as well and so it's
easy is the wrong word it's easier to
recover from a instance going down than
to recover from a service being latent
everybody agree yeah so this is a test
that we wanted to do on a very small
limited scale and we can surgically
effect a service down to the customer ID
of a particular or a device customer or
device for a particular service by
taking it out we can scale that up all
the way to the whole and services out
for everyone this is a way for us to
test our vulnerabilities with what we
call fault injection testing it's
another form of chaos
instances are great but sometimes a
whole availability zone goes down in
Amazon and we need to be able to protect
ourselves from that too so the chaos
monkeys the guy in the middle with the
two little guns the guy on the on your
left is the is the chaos gorilla he's
got the big gun right the guy in the
back they're really scary
ghostly guy is Kong so the gorilla takes
out an entire availability zone but Kong
takes out a whole region so we're in
three regions in Amazon and about
monthly we take out an entire region to
make sure that we can recover I'm gonna
give you a demo of us doing this
hopefully it's gonna run okay there it
is normal steady-state we've got us west
on the left us east on the right EU on
the bottom see a problem starting to
happen in us West maybe somebody pushed
some bad code right sometimes it's
faster just to get traffic out of that
region push it over to another region
while we give our developers time to fix
it
so that's what we're gonna do we're
gonna detect the error we're gonna start
using Zul to reroute our traffic the
only thing that's going on now in u.s.
West one is the redirection of traffic
from West one to East all it's doing is
saying traffic go over there right we're
gonna get that going once all that
traffic is going over there we're gonna
turn off DNS entry so nobody even sees
those instances in u.s. West one give
the developers time to fix the problem
so you can see fewer fewer traffic is
going there now all of our traffic
global is being handled out of us West I
mean us East and EU
but give it a little bit of time the
developers fix the problem push to push
the fix and now we can start trickling
traffic back into the West get that
warmed up turn our DNS entries back on
and now we're back to steady-state is
this just is this just a vanity test no
anybody remember the dyno DynamoDB thing
that happened a couple of weeks ago this
is how we got around it you may have
seen on the internet Netflix is down
because the Dynamo do you big problem it
wasn't actually true we actually did
this a lot of the other Amazon customers
were down but we weren't because we
routed all of our traffic out of the one
region that was experiencing that
problem you have to exercise these
things regularly like I said chaos
monkey every day chaos
Kong every month and when something
unexpected happens so it's giving us a
lot of tools in our tool box around
handling traffic right so you can see
the full plethora of what's going on at
DevOps about how this are scale our
culture and our tools all work together
to provide us with a resilient story
that isn't irresponsible saying yeah
developers you're on your own right we
have a handful of operations engineers
and they would hate us if they had to do
all this work right the developers would
hate us if we didn't give them the
tooling and it would be irresponsible to
our customers as well so what I want to
leave you with today is how do you think
about DevOps so I said specifically that
we weren't going to try to define DevOps
today I was going to give you an
understanding about what we do at
Netflix and kind of provoke you to think
challenge you to think about what you do
in your environment that could benefit
from some of this thinking and so the
question I want to leave you with is how
is this gonna change what you do
what impact can this have as your
culture set up that you can take
advantage of some of these tools who are
since they're in open source school you
need to make some changes to them in
order to use them or is it just too far
out of the realm of possibility if you
want more information you can get on our
github site you can get access to all of
the open source projects that I
described our tech blog have you can
just look at Netflix tech blog has
descriptions about many many many
projects including all the open source
projects and you can definitely reach
out to me on Twitter or an email and I'd
be happy to answer questions about what
we're doing and what you might be doing
as well I'm also here to answer
questions now I hear we have about 10
minutes and a very fancy iPad app that
just came in just one okay
see what we got I'm gonna try not to
trip over this cord what about
independence across cloud providers are
you insulated from AWS wide outages no
nope so there's a really big Pioneer tax
to going to the cloud and going to a
cloud right so we chose to go with
Amazon it's a very mature service and it
provides us with all of the features
that we really need in order to support
our environment we have a really close
relationship with them they work really
well with us and we're satisfied that
they are a great partner and that while
we would suffer from a
a global outage in Amazon we also think
that they're responsible enough to be
careful about that also we wouldn't be
the only ones with me all right more
questions
- that's a good question so I think that
a lot of times we talk about DevOps as
being the developers and the apparitions
people are on the same team or we talk
about the developers and the operations
people use the same tools we talk about
configuration as code these are the
typical things that you hear when you
hear a definition of DevOps right am I
missing anything anybody want to add
okay add Netflix I feel like the
developer and the operations people are
in the same body right it's like where
it's closed all right we're right there
okay and so I think that that freedom
and responsibility culture really helps
to underscore the importance of that and
helps to keep us moving along does that
help
the spreadsheet
yeah so our discovery service helps us
to discover you know who do who does
what right I want to send metrics and
how do I communicate with Atlas that
type of thing how do I know how to like
what that critical mass of of important
micro-services is we talk a lot at
Netflix I think one of the advantages we
have is that we're all located in Los
Gatos south of here about an hour or so
in one building well all of engineering
is in one building it means that there's
a really close communication between
teams so that loosely coupled but
tightly aligned that I mentioned we also
run my team runs a bootcamp for new
engineers that really gets people
started with their first Netflix
application and we're continuing to try
to push the boundaries on that with with
spinnaker and with some other tooling
that my team is developing just kind of
get people up and running quickly so
that you're taking advantage of ribbon
and historic sand you know all of those
tools right out of the gate so it's a
lot of Education but also building sort
of that framework of tooling a template
that people can use by inheritance
rather than copy and paste right
yeah so new engineers that Netflix are
invited to our engineering tools boot
camp where we describe the core
micro-services the core libraries that
people use at Netflix and they're
pushing their code by the end of the day
and so it's it's it's pretty fast-paced
but it's also available for anybody to
use in the company and uh virtually so
they can they can do it at their own
speed also going to the cloud but that's
because we have critical mass already
right there's probably somebody on your
team that knows how to do all of these
things that you can ask when we were
going to the cloud when the company was
paying that pioneer tax of going to the
cloud there were lots and lots and lots
of cloud use around cloud universities
around how to get your service in the
cloud so there was constant training
about how to keep everybody in the loop
because knew there was so much training
going on at the same time I don't know
if that helps your if you have a follow
on ok all right
yeah that's a good question so um teams
roughly correlate to services so a
service usually doesn't span more than
one team right so developers are on
teams they're usually about seven to ten
developers max on a team that build a
micro service the the using team and
developer interchangeably each
individual team decides how they're
going to manage their deployments the
developers that are on that team are
responsible for making good choices for
Netflix and good choices for their team
so they understand sort of the inner
dynamics of that team we don't really
give a lot of restrictions from on high
about how things are done so we don't
push down agile process we don't push
down you know a sort of test coverage
teams individually may have those as
part of their own team responsibilities
like they've agreed as a team that this
is something that's important to them
and so what we see is these experiments
kind of popping up all over where people
are really figuring out what works well
for them and then they socialize that
with other teams and ideas kind of
spread organically so yeah a team and
developer are pretty synonymous because
the team that in developer needs to
really be on the same page with the rest
of his or her team
tiny I think tiny so you know the hmm I
say that but I don't know for sure you
know there it's hard because we don't
really draw a distinction between the
two the individual microservices are
vast right there's lots and lots of them
but the tooling they they can't exist
without the tooling libraries that
really support them so it's hard to know
on my team you know we spinnaker is
built on micro services and so we
probably have a half a dozen micro
services that contribute to the suite of
tools that is spinnaker and that is
effectively a distributed system and in
itself we have lots of services that
sort of work together to provide you
with the experience that you see as
Netflix so a service that talks about
that that constructs that list of movies
that you see when you when you bring up
the service the service that says
recommended for you
trending right now managing the kids UI
versus the UI that adults see you know
all of those are individual services and
so that graph I showed this the soft
graph all of those were individual
services and those weren't tools so my
guess is that it's it's it's much more
but I don't have a good
so let's go to the very back
I don't know what that means I'm not
trying to be rude what I'm saying is
that at Netflix there's no such thing as
a dev app we don't have a release
engineer for example so the where we can
see a conflict is in product where the
product managers want to push out a new
a new idea into the product and we're
asking people to go fix the hearts leave
vulnerability for example right so that
that may be what you're talking about
that sort of thing but when we're
talking about
rebating your service to deal with the
security vulnerability we're actually
working with developers as well and so
the developer is basically talking to us
who are telling them gosh you know we
have this security vulnerability you
need to fix and they're talking to the
project the product managers who are
saying hey I really want to see what
happens when you add you know pin
protection to profiles right okay let's
go with the guy here yeah
there is a team that looks at cost it
had this lives inside the performance
and reliability engineering team they
the individual teams don't have any
worries or concerns about it I get a
report as a manager I get a report about
what my team is spending and that
highlights where things have gone sort
of haywire recently there's an open
source tool out there called ice which
is a really great visualization tool
about what your cloud costs look like
and there's another tool called janitor
monkey both of these are Netflix open
source janitor monkey which cleans up
after unused instances to kind of help
you make sure you're only paying for
what you're using so yeah we don't the
individual teams don't get into cost but
Atlas for example with 2.9 billion
metrics per minute can get pretty pricey
and so that team works really closely
with the FP and 18 and understanding
what the costs look like of that service
and how many metrics to keep I'll go
with you and then I'm going to take a
look at this iPad thing again
how does it was talk to the plants yes I
mentioned this briefly which is our our
Sox teams have to follow a different
process it would be irresponsible if we
did not and the others would you know if
we didn't follow a Sox compliance so
they they actually have different
restrictions about how they do
deployment and they go through a
different path in our deployment tool
mm-hmm but we really do try to limit the
controls that we put on to those teams
that have to be impacted by that and
leave everybody else up well enough
alone okay look let's look at this
questions okay how do you work around
bandwidth issue issues bandwidth of the
team or bandwidth of the services it's a
person here capacity so meaning like
running out of instances that type of
thing or traffic well ISP bandwidth okay
so we have our own CBN we have our own
content delivery network called open
connect it's an open source architecture
and it's an open architecture those CDN
appliances sit very close to ISPs and in
fact sit in the building with ISPs where
we can we provide these free of charge
to many ISPs are around the world and we
cache all of our movies there and so
when you hit play it's not hitting the
Netflix service anymore it sits at the
CDN Network by the time you hit play so
the bandwidth that's very close we try
to keep the movies really close to our
customers and to our ISPs customers what
happens when team Amy's changes made to
a service owned by team B but the team's
use totally different technology stacks
it's a microservice architecture so we
don't see a lot of dependence between
microservices the exception of that is
that is some of the security changes
that we make and things like that for
the most part all of our source code
with the exception of the Sachs code is
available to everyone throughout the
company so we we use git for a version
control with stash and everybody has
access to all the repositories people
are allowed to make pull requests
against those repositories where the
technology stacks differ it's really
beneficial if we have a rest client
because then we can communicate with
rest there are situations where
somebody's decided to deploy their
service with Python and the vast
majority of our structure is in JVM
based languages so we have a sidecar a
prana sidecar also available in open
source of course here they're broken
record but prana allows the the non JVM
applications and services to be able to
talk to our those core libraries that we
provide for Netflix all right I think we
have time for nope no more well let's
take one more question because they
haven't kicked me up stage yet one more
do we have one more all right let's go
up front here
what was the biggest challenge getting
to today's state I wasn't there through
the cloud migration but I know that it
was a big pioneer tax right to get
everybody over the hump to think about
breaking up we had one big monolithic
application breaking that up into micro
services was a really big challenge
getting people to understand that the
cloud is different than a data center
and that services go away like instances
die for very good reasons and since his
die when you just scaled down right so
all of the these tools that I described
were basically built because they had
there was a purpose behind them and so I
think that those that pain was felt over
a long time period but probably built
breaking up the monolith into micro
services is sort of the conventional
wisdom that was a really big difficult
transition for me as somebody joining
Netflix two and a half years ago the
biggest change for me was and the
biggest hurdle was just this in this
vast array of teams figuring out who to
talk to and who did what like it's just
it's really interesting because we have
the micro services mirror the teams
right we have really small teams that do
something very specific and just even
knowing that a team existed was
something that was a big challenge for
me a little guy in the back and then
we'll be done
maybe the good news is that that's not
part of my of my forte but the product
managers are pretty careful about the
eighth AP test than the way that they do
those and so I suspect that it's not as
complicated as one might think
and that they're really careful about
making sure that the a/b tests don't
conflict we're out of time thank you
very much I have some cards up here if
anybody got please fill out the survey
on your way out the door I should have
mentioned that sooner</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>