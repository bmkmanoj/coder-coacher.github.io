<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Performance from Predictability | Coder Coacher - Coaching Coders</title><meta content="Performance from Predictability - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Performance from Predictability</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gTzxv80Qlow" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Richard I'm just going to be
talking a little bit about some more
kind of deep dive performance topics
stuff which I've encountered in a few
kind of scenarios here and there or or
projects I've worked on here and there
things which I think hopefully people
find a little bit more interesting a
little bit more unusual something a
little bit more than just to run of the
mill type talk I guess one of the kind
of motivations from my point of view of
like doing something that was perhaps a
little bit more deep dive words you keep
on talking about people kind of think
about performance as though it's like
working like this and then it seems to
end up looking a lot more like this so I
guess I have a few friends of mine who
whenever you talk about kind of more
low-level stuff are more kind of
technically in-depth stuff also have
kind of a phrase called low-level
rubbish I it's too low level for you to
want to know any details about this and
I guess I'm going to kick off by talking
a little bit about why you might want to
care some of the kind of ideas and then
we're going to go through a few
different topic areas of how you can
write software that works better with
your hardware how you can write software
that works sympathetically towards the
way modern hardware works and then we're
just going to draw some conclusions
about kind of common themes and things
going on so there's anyone here read
like the high scalability blog anyone a
couple of people I always find like
these kind of articles really amusing so
they were talking about hit shatters
anyone use HipChat it's like a kind of
IRC for hipsters basically and hip chat
these guys were talking about processing
like 60 messages a second or something
and then they were talking about all
these different servers that they had
like their back end their front end and
they were kind of talking about kind of
technology stack they were using I mean
maybe there's more stuff going on that's
not obvious from the blog post but it's
one of these really kind of amusing
things I know if anyone's ever I've just
gone through this and you work out take
the number of servers
figure out how many messages they're
actually processing and all that kind of
thing and then you kind of figure out
its like the zero point seven messages
per server per second which doesn't seem
like a huge number at all oh yeah so I
thought this was kind of kind of funny
mainly because the focus here is on like
the technologies our technology stack
these are all the buzz words we use
these are all things we do and I guess
I'm more of the opinion that it's more
about kind of methodology and principles
and fundamental so this talks a little
bit more about understanding how the
hardware works on the sanding principles
and practices involved with that and
then you can kind of understand how to
design things better how you can apply
those practices rather than it being
just about hey use our new no sequel
database everything is web scale it'll
work perfectly honestly then you in
heard that kind of product pitch from
vendors like everyone in the room
fantastic or the architectural advocacy
if you design things and follow these
simple rules everything will work
perfectly has anyone heard that kind of
pitch as well yeah now I'm not saying
any of these things are bad I'm just
saying it's kind of a improve an
interesting perspective so I also
thought I'd kick off with these kind of
like latency numbers which you see
bashed around the place so this is more
a just an overview of kind of the
different costs that different actions
have going right down from an l1 cache
reference all the way through to sending
a packet of data from here in California
all the way over to the Netherlands and
back here again which as you can see is
significantly longer and that's probably
assuming pretty good network connection
at the same time but the kind of thing
you see when you see these numbers is
you're like yes this is awesome I've got
to try and get everything up to here but
that's probably not necessarily a sane
perspective to look at you'll probably
want to look at more kind of low-hanging
fruit first right there's a cost-benefit
analysis here you can't optimize
everything you can't have everything
perfect so when does it matter well I
kind of depends on your application
and I guess I'm a more in favor of
people understanding their kind of like
their requirements they user
requirements around performance that SLA
is that kind of thing and then they can
make informed kind of design and
architectural trade-offs know which
isn't a call for premature optimization
just a call for understanding where
these costs lion and what you can do to
solve them obviously from time to time
you get a bottleneck and things like
this happen this looks really bad to me
right and the question is what are you
going to do when you hit those kind of
things what what techniques you're going
to apply well it's also a question of
like we're in your system do you need to
worry about performance as well so I was
recently working on a project which was
like a effect of the kind of message
brokering system for a con a low latency
network communication system and the
message broker ended up having three
threads and we had one that was kind of
conducting admin tasks like managing
connections that were going in and out
of that kind of thing and then we had a
couple which were really focused on very
specific shoveling data here and they're
moving things out so it's pretty clear
just looking at this kind of thing that
these guys the the threads that were
reading and the threads that are doing
writing we're really on the critical
path for all the communications and that
was an appropriate place to in our case
go absolutely mad optimizing them
whereas this guy you know not such a
concern not such a worry and I guess one
of the things I thought about as well
when I was looking a lot of the more
kind of low-level techniques a lot of
the more kind of mechanical sympathy
hardware interactions stuff is that
there's kind of bit of a running theme
hear about your software behaving in a
predictable manner there's kind of lots
of tricks and trick tips and tricks that
happen low-level in the hardware and in
in different levels of your system that
kind of can take advantage of you
behaving in simple predictable ways I
think that's what we're going to be
looking at a lot today so the other
thing is do you actually care about any
of these problems I mean there's a whole
host
of more general problems I was in a
awesome talk yesterday by fabian land
where he was pointing out an example
optimization that a customer had done
when they just had a really dumb thing
in the middle of their system and I
needed to fix the really dumb thing and
none of these kind of low-level things
were necessarily the next step for them
but I I think it's still interesting and
I've got a few slides which are marked
with the concept of an optimization omen
like something that you can measure that
will tell you this is really a problem
this is something that's a an issue for
you potentially so let's look a little
bit about branch prediction so hounds
what what do people think like that what
are the actual for things that every CPU
since the beginning of time has done
very basic stuff guys like what are the
basic things that every CPU does now
okay maybe I should have made it more
clear the kind of thing I was asking for
so they're going to fetch an instruction
they're going to decode that instruction
then they're going to execute the
instruction and they're going to write
some operation back now you know there
was this kind of realization at some
point in time that actually each of
those operations didn't necessarily need
to do all of those four operations one
after another for every instruction so
you saw CPUs beginning to get some kind
of pipelining so each instruction goes
in and then you've got these four steps
which are happening pipelines and it
kind of carries on going people familiar
with this overall concept yeah yep quite
a few for her heard of pipeline before
obviously modern CPUs have taken this a
hell of a lot further they do super
pipeline architecture so you get really
really long pipe lines they're not just
taking those for operations they kind of
break it down you know maybe you'll see
12 maybe even kind of 20 steps in some
of the pipeline's of other pentium 4
early generations had like 20 and
they're also super scalar so that means
they're they're actually trying to do
these operations on
to pull things at the same time now
unfortunately that's you the idea that
you're going to kind of sit there and
pull those things through your
instruction in a pipeline kind of hits
the problem at some point in time you
get some kind of code where you've got
like an if statement or a loop some kind
of brunch and your CPU is it's not a
it's not clairvoyant it doesn't have a
crystal ball right it can't necessarily
predict the future and so you get to
this point here and you know that this
set of instructions are the next thing
you need to do on this branch and that's
the instruction on the other branch and
you've got to figure out what the heck
am I going to do so you're one strategy
would just be to say hey let's actually
execute this this condition here let's
check whether X is greater than 5 or not
and if it is will do the left side and
if it's not will do will do the wrong
site now I'm if you just took this kind
of very naive approach you hit a
situation where every time you had some
kind of conditional statement in your
code your CPU would just sit there and
poor's would freeze do nothing with a
stool your pipeline would not continue
to kind of filter on through and
obviously stalls really kill your
performance and doesn't matter how
expensive or fancy or CPU you have if
you're just sitting there stalled well I
would really be going anywhere so quick
hands up how about can we eliminate
these branching costs from cpus hands up
who says yes what couple of guys haven't
know whose hands up on know cool no it's
guessing brilliant so I guess the the
overall strategy the cpu CPUs use is
they predict branches and then depending
upon which way they predict they execute
speculatively so they go wow I reckon X
is probably going to be bigger than 5 so
I'm going to go for the top branch
surely go well it's probably going to be
false i go for the bottom branch so the
most simple form of prediction that cpus
can take a static prediction so static
predictions just without knowing
anything about this run of the program
or the program or anything we're going
to have
rules and we're going to act based upon
those rules so on x86 processors you'll
see forward branches default to not
taken and backward branches result to
taken so when I say forward branches I
mean branches where you're going to
summer program counter position further
forward and backward where you're going
to somewhere previously the reason you
might have backward branches in your
program is because you've got some kind
of loop and you're going to get you've
got to the bottom of the loop body need
to flop back up to the top which is
obviously why loops default to take and
write because once you've got a loop it
slightly it's going to have flip around
more than once now there are more kind
of sophisticated prediction strategies
but obviously even our simple very very
simple static strategy gets used at some
point in time and that point is when the
program has absolutely no information
about what to do even when it's got the
program in front of it so any kind of
dynamic strategy is going to need to be
bootstrapped by some kind of static
strategy because you won't have any
history of it and no one has infinite
storage space about the history of what
their program has been doing so even in
the dynamic prediction strategy your
kind of default back to something static
at some point in time so conditional
branches here is some hypothetical code
in some high-level language and here is
some hypothetical set of x86 assembly
instructions which correspond to that
language so we're going to take a
variable X and move it to the ax
register we're going to do this if
checked by comparing X gets 0 and we see
this guy here as being a jump not equal
to so you get these conditions where you
know previously we talked about the idea
of branches where we were jumping to a
different location the program and
there's a bunch of you know assembly
microinstruction operations which are
conditional so that is how add somewhere
in the assembly level you'll see these
these branches so this bit here is the
body of a loop move one into eax and is
a label which corresponds to their
you increment exi the way and then you
move the register back into the variable
at the end so processors know when
you're going to have some kind of branch
by just being able to look at these
assembly instructions well be able to
look at the conditional branching
operations and they can use that in
order to build up a pattern of prior
behavior and be able to make accurate or
informed predictions so before we get on
to dynamic prediction there's still a
few more bits and bobs at some point in
time or pentium 4 or later x86 chips but
i think other architectures have this
idea as well is they have these hints so
your compiler can come along and go hey
I've looked at this program I've done
some clever analysis I know what I'm
doing guys just just listen to me and it
can emit these little hint so these
magic numbers mean your static strategy
should be to take it or not to take it
the fascinating thing about these hints
if you look into it is the actual intel
recommendation is not to use hints it's
just to flip the branches the other way
around and use the traditional static
instructions so dynamic prediction the
overall idea of dynamic prediction is
you kind of run your program you record
history and then you make a kind if you
know you've seen the history and you
think the future is going to behave
something like the past so you make a
prediction based upon that what this
means is that modern chips have
something called a branch target buffer
a b2b and this is basically a log that
keeps track of all the branching
decision so it got entry for the program
counter which tells you the address of
the instruction it's got a couple of
other entries which tell you information
about whether it flipped on on branch
target buffer differ a lot from system
system we're going to look a little bit
more about different predictions
strategies in a sec but obviously
depending upon the different prediction
strategy the branch target buffer can
get more or less complicated so some
prediction strategies for example are
able to take account of Malta
all nested if statements and all sorts
of things like that important point
about the branch prediction buffer is
that it's finite so that means no matter
how clever your strategy is at some
point in time you're going to run out of
space and potentially potentially you
know not be able to use a dynamic
strategy so there's kind of a big
distinction between local and global
prediction strategies so local
prediction strategies record the
conditional branch histories on a
step-by-step basis whereas global
strategies have this kind of big shared
history like the the BTB is global over
the entire pro over the entire all the
instructions to get executed now when i
was first linking to this it sounded
like yeah local locals a great thing to
have that's kind of information about
your specific case that sounds really
good but it turns out actually more
modern ships are moving towards global
histories because it means you can take
account of more historical context you
can say well if these if statements have
gone this way then we can know something
about this other if statement it's
actually a bit more sophisticated some
ships have specialized predictors for
loops so they start detecting when you
get cycles when you go back to previous
points and they can go well you know if
it's a loop it's more like to get
iterated again and all sorts of things
like that some chips take function
specializations so they go hey we've got
some local history about this function
we're going to use that information most
modern ships including the house well in
that laptop use multi-level adaptive
predictors so what that means is that if
you have series of if statements or a
series of loops or things like if
statements nested loops it can actually
take account of the context of the
preceding or surrounding conditional
branches to do stuff so I've kind of
talked a little bit about branch
prediction itself but actually before
you kind of want to think about these
kind of things how do you know that this
is a relevant concern
so the answer is the most modern ships
have what are called performance event
counters I only heard of performance
event counters or model specific
registers cool think those people can
you keep your hands up if you've played
around with them or use them a few guys
anyone written like a cache profiler
okay still one guy cool yeah so it's the
kind of thing which exists and is around
on like pretty much all modern ships but
very few people actually seem to take
advantage of them or look at them in
particular so msrs can be configured to
store this kind of branch prediction
information so that means you can take a
look at what's actually going on on your
program and you can make kind of
informed judgments and you can see
whether it's a problem there's a whole
mass of existing tooling both open
source and proprietary both free and pay
for that are available from different
CPU manufacturers or performance
analysis guys who will give you insight
and information some of these tools can
even do things like track back to
wherein the source code of your program
something is going on so I'm just going
to do a quick demo to give you an idea
of the kind of information you can get
out of this using the linux perf tool
which is an open source tool that just
comes with linux so let's take a look at
just a very simple kind of program here
so the what this code is doing is what
maybe you can see if we look down at the
bottom it's going to take you a big
array events which are all random
numbers and then it's going to
repeatedly loop over these int's and
it's going to add it's going to check a
condition here as to what the value of
that int is so those inside the zero or
one from the random number code and if
it's one it's going to add the current
index to some some value okay so
we just run that there so what that's
saying is just run this jar program
called miss predict and run perf stat to
get information so just a quick
demonstrate here you can see the total
number of branches that executed during
that program and you can see the Miss
rate here now this is pretty blooming
awesome 0.03 percent of all branches
were mispredicted it was incredibly good
at going through those things even
though and that looked like it was going
to be a pretty hard to predict thing
like intuitively as much around and
numbers what the heck was going on there
demonstrable awesomeness on the behalf
of Haswell chips but just to give you a
little bit of an idea of how sensitive
these kind of operations are I'm going
to take another operation here and what
this is going to do is it's going to add
a little bit more complexity to the if
statement it's going to do bitwise and
between you know the current time and
some index it's going to check whether
that's greater than 0 and if that's the
case then it's going to do this kind of
thing now this I would have thought is
significantly more difficult to predict
now what do you guys think of what's
going to happen to the runtime of that
program anyone think it's going to be
faster no what about the same know what
about slower slower yeah you we've
already gone past like the six seconds
how much slower you want to guess four
times
10 times there we go 30 seconds five
times slower this guy in the front row
wins a prize for guessing so and what's
interesting about this one here is you
can see I mean the tournament branches
does increase in this because it's got
couple of complicated things going in
there but you can really see that we
previously we were down to 0.03 percent
of all branches being mispredicted we've
jumped massively up to nine percent here
and this is like an incredibly small bit
of code but had a huge impact on the
performance of this well I mean it is it
is a very synthetic micro benchmark
benchmark but it's a very sympathetic
example of how this change can affect
things but I've already interesting at
least to demonstrate how significantly
the performance flip was just from such
a small change so we just summarize some
of the kind of more branch prediction
type stuff you know there's a lot of
complexity in modern CPUs and there's a
lot of kind of interesting strategies
and things that can deal with it if you
take branches and they get mispredicted
you will get a stall a lot of the
solution to this kind of thing is trying
to simplify the logic that's on your
critical path just trying to keep it
simple and the other thing I mentioned
up there is briefly mega morphic cool
sites some of the things which you see
in the JVM there's method calls if it's
not obvious to the JVM what methods
going to get cooled you're going to end
up with a lot of branching statements
under the hood I don't you want to go
too much into the detail of that because
that's like a whole nother topic in and
of itself so let's have a quick look at
memory access here is the kind of modern
CPU situation in a nutshell chips are
getting faster massively massively
faster but the interaction between CPU
in the memory is still recipes slow by
comparison and speed of memory accesses
still getting is still very slow by
comparison not only that but the
performance of memories in
creasing by about nine percent annually
whereas a number of number of
transistors on a chip still doubles
every 18 months every 18 months and
clock speed is still increasing so not
only is that this gap this gap is
getting worse and worse and worse so you
know the historical solution to the kind
of the gap in performance between these
two things is the good old CPU cache so
I'm sure we're all familiar the idea of
a cash now your CPU wants to get some
memory some data from main memory so it
looks it up in a cache if its present
there's a hit you can return the data
from the cash to register if there's a
Miss then it needs to go all the way to
main memory and store it in a cache I
mentioned reads but obviously there's
also writes here and it's important to
recognize that this is an economic
trade-offs so it's very expensive to
have a mem memory that's fast enough to
keep up with your CPU which catches
certainly don't anyway but a small
amounts of on-chip memory is a lot more
affordable well again over time these
kind of architectural solutions have
evolved so this is a sandy bridge
architecture that I think you'll find I
hear bridge and has will follow the same
overall layout if you've got n physical
cores you end it with a couple of
logical cores for physical core I
presume everyone's heard of like hyper
threading most people so the idea is
that I'm you've got to to Hardware
threads is what the Intel terminology is
and when one of those threads gets tools
your CPU can flip in the other hardware
threading hair on executing it below
each physical core you get level one
data an instruction caches which are
perf is core core and an l2 cache that's
purpose or core as well level 3 cash
tends to be shared amongst all the
physical cores on a socket some rough
numbers so it obviously depends a lot on
your hardware these numbers were
actually issued on my old laptop
interesting enough and it's kind of a
latency in terms of Miss clock cycles
and as you can see as you get further
down
the cache hierarchy things tend to get a
lot slower when you're round-tripping to
main memory things are really massively
bad now obviously it's one of those
things where you know if you're in it
round-tripping from caches all the time
you are still you are still going to end
up being very slow so what CPUs do if
they try and pre fetch data they try and
eagerly load data from main memory into
your CPU cache in order to amortize that
latency of a head of reading data from
main memory it's a bunch of different
strategies so you get things like
adjacent iron fetches where if you see a
CPU fetching a certain cache line from
main memory it'll pull the next run
along you should also see streaming
prefecture 'he's where if you see things
popping along in order the CPU will
start fetching the next instruction the
key trick to optimizing for this kind of
stuff is to arrange data so your
accesses are predictable what do you
mean by predictable well locality of
reference is kind of there's different
aspects to it there's temporal locality
so that's repeatedly referring to the
same location in memory in a short time
period and that means that it's already
in your cash and you can get it back
there's spatial locality which means if
you're referring to things one after
another you refer to things which are
nearby which means they're more likely
to be loaded sequential locality is a
special case of spatial locality where
your data is arranged linearly in memory
and things like those streaming pre
fetches can really win because they'll
go are this guy's just following along
things in order it's lovely they're a
bunch of other data for other principles
about around optimizing for memory
access so using some smaller data types
are trying to not have things spread out
in memory as much as possible avoiding
big holes and as I say making access as
linear as possible so the kind of big
holes is a similar case let's just kind
of explore this this simpler simple idea
so we've got just a array of primitive
in switch
aligned sequentially in memory and we're
going to iterate over them an increment
a value in that array now just to
demonstrate how much holes hurt we're
going to start looking at skipping a
certain number elements I'm going to
increase this exponentially this is a
graph of given a certain benchmark where
I was increasing the Skip factor how
much the the Miss rates were going up so
higher on this graph is the percentage
of overall accesses that result in a
cache miss at different levels in the
memory so here where it's pretty low
though that I think that flat line there
is access in the next element along
that's skipping by two that's four
that's eight and you can start to see
this this flow floating up as well if
you jump far enough it'll be impossible
for your Prefecture to figure out what's
going on now multi-dimensional arrays in
Java even for primitives end up being a
bit weird because multi-dimensional
arrays aren't really a raised that a lot
align sequentially in memory there are
raised of pointers to other arrays so
some people do things like realign their
accesses so they'll have a single array
that's guaranteed to be aligned in
memory and they will just use a formula
like this to figure out the index kind
of going in the house and up the stairs
into this array and then they cannot
alter it but obviously it's important to
align your your calculation so that the
inner strides in your loop are the ones
that result in the next in the next step
so in this example it increments the
column even though we're going to the
next row element and this means it ends
up jumping through the array by the size
of that outer index and you end up with
a huge huge level of cache misses if
you're striding the right way so if
you're going along the the the inner
index you're on the happy path and
things get a lot better it's also worth
noting that there's a big distinction
here
between kind of randomized access on
memory and this sequential access so
randomized accesses is the word ends up
being the worst case well I think these
numbers were measured on that Haswell
laptop using possibly so soft and as you
can see the l1 data level the random
sequential access end up being pretty
much the same by l2 it's harder to
prefetch to l1 so it starts getting
slower for randomized access and then
from main memory because this is very
very easy to predict and this is near
and plot this is impossible to predict
you end up seeing like a 10 times
difference in the cost of memory access
now we'll come onto in a sec how objects
get laid out in memory but objects end
up being pointers to arbitrary other
points you end up with a lot of point
chasing which means that loads of people
use primitive specialized collections
which gives you much better cash
locality again because with the raise
even if you've got other objects coming
off them the arrays themselves are kind
of sequentially order the spine which
means that arrays tend to be linked
lists when it comes to you know indexing
because you end up with the linked lists
again pointer chasing to random
locations similar the hash tables vs.
search trees and obviously there are
some specialized issues around compiler
optimizations bloating up the amount of
instructions you have in your code and
causing a lot of bloat on that level
which means your instruction gap cash
gets blown so optimizations like loop
unrolling can potentially be a slow down
I won't go into any of these custom data
structures really because they're all
again a whole nother topic in and of
themselves but it's worth noting that
these kind of things exist if you are in
a specialized domain and really care
about the performance in that domain it
can be often worth taking a look at
these customized custom data structures
which have potential for really good
locality of reference properties in Java
the one of the ongoing problems that I
think everyone faces is that if you've
got some kind
array of objects you've got an array of
pointers in memory to that object in
each of that has pointers to any of its
fields which are objects while they're
happy next door are they so if you're
looping over this array whilst you might
think that you're kind of looking at a
nice sequential easy access type thing
you're actually having no alignment
guarantees at all you're you're getting
to that point H no alignment obviously
the array spine has an alignment
guarantee but then when you start
pointing to the elements they can go
everywhere it's often cited in kind of
more HPC low-latency circles as being
serious job a weakness kind of one of
the things that holds the kind of not
just Java but the JVM back as a platform
and this lack of ability to control the
object layout is problematic it's worth
noting that garbage collection can also
make this both worse and better so most
garbage collectors have things like
threadlocal allocation buffers which
means things again allocated one hour
for another in a certain thread will be
allocated one after another in memory
probabilistically but obviously you get
situations where garbage collectors copy
objects around or do some kind of
compaction and move objects so they
squish them together compaction can be
of benefit but you know it depends on
its how lucky do you get really so again
if you want to have a look at these kind
of problems their performance event
counters and the tooling I mentioned in
section one give you an opportunity to
measure those kind of hit miss ratios
and understand how much of your CPU time
is being lost by waiting on main memory
because those numbers are just like a
ratio or a some massive number that
makes no sense it's important to do
things like correlate your hit miss
ratios with things like pipeline stalls
so you know when your CPU is actually
sitting there idle and wasting its time
and you know sitting on its hands or
whatever and then you can understand why
that's happening there are also a bunch
of projects going on which
are trying to give people the
opportunity to do have more control over
object layout there's an interesting
proposal there on github by guilt any
and Martin Thompson on basically a way
of taking ordering of Java objects with
no language changes or anything crazy
like that from a user's point of view I
do mean genuine ordinary objects and if
you control over layout so they have a
certain these kind of skeletons like
common patterns so one of their patterns
is the idea of that array of objects
being lined up sequentially in memory
and there's also a bunch of off heap
stuff so in the idea of anyone seen the
JC tools project on github nope anyone
heard of Java Chronicle a few people
cool what about so these are projects
which give you or the experimental part
of j tools at least gives you some kind
of off heap data structure and that data
structures in a contiguous block of
memory and then they have a way of
controlling access to and putting data
structures on that off heat block of
memory which gives you complete object
layouts control obviously there are also
costs are going off heap potentially
more work involved for you there it's
not necessarily guaranteed win the other
thing that's really interesting is so
put hands up the neighborhood of cap and
P a few people SBE a couple of people
flatbuffers a couple of people Wow
different people every time so there's a
lot of stuff in the kind of
serialization where previously with
serialization libraries you'd take an
object do some serialization and write
it to some other buffer but a few of the
more modern libraries in serialization
seem to be taking approach of taking a
buffer and just mapping accesses into
specific locations in that buffer which
means you have the option to do things
like dip or a copy serialization if you
just pull in the data off your network
and pop it in the next thing in your
buffer and it also means you can have
effectively an off heap data structure
which you can access
but again those are more if you're
interested so quick summary on this kind
of thing cache misses call stall stalls
kill performance you can measure and
understand these kind of things through
performance event counters or be it a
little bit of black magic and there are
a bunch of kind of common techniques for
optimizing code so let's just have a
quick look at some hard disk storage who
uses some kind of hard disk in their in
their work system a few people everyone
one of the one of the above yeah so hard
disks or spinning rust as they tend to
be increasingly cool these days have
some kind of head that's going in and
out on your disk and reading and writing
things as of a long time ago now most
harder to use some variant of constant
angular velocity so constant angular
velocity means rotations per minute stay
relatively stable and your disk kind of
comes in and out now there's big sector
size differences between devices so yeah
just take that into account so what a
lot of people do is they user like a
zone constant angular velocity model or
a zones bit recording model as it's
sometimes called and what this means is
because the inside of realistic is
smaller you end up eating a lot more of
the space there compared to the outside
a simple cost model I've seen people
used to understand and experiment with
the kind of performance of different
locality on your disk and different
accesses on your disk is looking at the
time to process the command so whenever
your OS is going in and try to read some
data off a disk it needs to send some
kind of commands to the local on disk
effectively a chip that figures out what
the hell is going to go on the disc
itself has some kind of seek time to get
to the right location there's kind of a
latency associated with spinning or
around and then you got your sequential
access time now the other thing that
zone bit recording means is because
you're spinning around and there are
smaller slots on the outside if you look
at the transfer times from the outside
of your disk to the inside of your day
they're not being faster at the limits
than the middle I think I looked at
measuring a blank formatted disk at my
house and got about 25 fold speed
difference I think other people eileen's
that seems about the same so it's not a
huge amount but it's interesting to know
if you follow through some of the
concepts of people have of measuring
different random access versus
sequential access you'll find that when
you start doing random access the kind
of seek and rotational times on your
disk end up dominating the actual
transfer time so if you start doing
random accesses the 4k i ended up
measuring that on a local disk as being
300 times slower than what your you can
get sequentially and the other thing to
bear in mind is even though you might
have one application thread that's
writing sequentially if you're doing a
lot of context switching between threads
that can still be sending your or
between applications that can still be
sending things into a non sequential
pattern the other thing is disk
fragmentation can cause unnecessary
seeks and it's also possible to do
things like get set to misalignment so
if you have situations where your OS is
a line is is writing its sectors on to a
certain location and you've got a
logical sector that's not aligned with
the physical sector you can end up
having to overlap between multiple of
them and that caused a big overhead
there was a hilariously unfortunate
situation with disks a while back where
they change the sector sizes to use a
kind of what's known as the advanced
format which had significantly larger
sectors and just because of the location
windows XP was using for its disk
formatting had a very bad offset for
that kind of thing in course you know
ludicrous slowdowns you can also get the
same problem if you have certain types
of databases so in ODB which is one of
the backends from my sequel has a kind
of paging system and you want to make
sure that paging system gets aligned
with that's actual stripe unit so that's
if you're on read the same principle
applies to sectors as well obviously our
load of people are looking at using more
SSDs these days rather than spinning
rust SSDs are an interesting case
because the difference between the
random access and the sequential access
is much lower but there still is a
relatively large difference I don't have
any concrete numbers myself but it's
worth bearing in mind that it's not like
a immediately solve problem in terms of
what you can look at four metric data
four disks applications you can just for
a bunch of existing simple tools like
vmstat MP stat various APM solutions
also give you this data will give you
some kind of notional break down how
much time your application ends up
spending waiting on disk i/o you can
also correlate that to how much time how
much data your i/o subsystem actually
transfers using things like IO top so
that's just a very simple Linux utility
that will give you breakdowns of disk
transfer rate at this times and that'll
tell you how far away you are from you
know what you should be doing so as with
CPUs and as with memory when you look at
disks simple sequential access patterns
win and things like alignment of
fragmentation you end up seeing a lot of
similar kind of problems or trade-offs
between these different systems in both
kind of the RAM em main memory end up
side of your old computer and also disks
just draw a few conclusions so we had
our hypothetical sink well these kind of
numbers here on the speed ups are not
really things which you would expect to
be able to achieve on a fully-fledged
application these are kind of whistle
talking kind of theoretical max
differences but it's enough to be able
to think about the numbers and think
about if you've actually got a situation
where you really care
if you've actually got that one
particular hot loop in your code if
you've actually got one problematic case
where it's possible to achieve
significant changes just by looking at
change your memory access patterns or
change your code layout or simplifying
some logic or changing the way you're
writing to disk well I guess the common
themes in this talk just include were I
was advocating the idea of kind of
principles and fundamentals over tools
being able to measure and back up any
kind of claim before you do any
optimization and again kind of simple
repeatable predictable accesses
overcomplicated random ones there's a
huge amount of really really great
information on the internet if you made
in the switch have great discussion
points on and things like the what every
programmer should know about memory
which is very interesting and the agne
fog CPU optimization guide if you're
interested in more low-level stuff
utterly unrelated to this talk but
because every talk is obliged to have a
plug slide I do run a job for a training
course had also wrote a book on Java or
hate if anyone has any questions I'm
happy to take them now and I hope you
enjoyed the talk
hi there's a guy at the back have I done
any work sorry I've not done any kind of
specific optimization for numerous
systems as a NUMA thing no got the back
so the way these kind of things work is
you don't explicitly say go prefetch
something this is all what the hardware
is doing underneath the hood for you and
the way you want to treat this as an
optimization problem is not to try and
control that operation but you need to
do things like lay out your data and
memory in a certain way lay out your
code a certain way you've got a little
be a little bit more subtle than just
being able to say oh do this the right
way does that answer your question yeah
not not that I know of no I mean in
situations like that I think what you'd
say is you'll change your data structure
to use a data structure that's more
sympathetic to what cpu can understand
not not not for the example you're
giving their that I'm aware of no okay
any other questions hi yeah yep
I didn't I did not use jit watch I do
have a dump of the disassembled code for
that situation and hand inspection the
code suggested it wasn't a jit related
failure to the best of my knowledge
there are a lot of things in that area
that you're JIT compiler will do for you
again that's basically like a whole
nother talk but I i guess i was looking
a level deeper than that but yeah it's a
certainly about the point you need to be
aware of other layers in your system
before you get down there okay any other
questions okay thanks guys I hope you
enjoyed it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>