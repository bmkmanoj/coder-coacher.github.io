<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Traffic and Performance Analytics and Cloud Bursting with Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Traffic and Performance Analytics and Cloud Bursting with Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Traffic and Performance Analytics and Cloud Bursting with Machine Learning</b></h2><h5 class="post__date">2017-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9D28jO2fDvU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you very much for coming to this
session this is on traffic and
performance analytics and cloud bursting
using machine learning
it sounds like buzzword bingo but I
think the some of the things that you've
noticed in today's session and other
sessions at open world is there's a lot
of focus on using machine learning to
give you insight into everything from
end user performance and end user
traffic all the way down to drilling
down at the app tier and data based here
and using that also to predict your
capacity requirements when you are going
into a cloud environment and your cloud
bursting as well as your on-premise as
well so that's what we're gonna do in
the session is talk about the some of
the changes that have taken place in our
the world we develop and deploy
applications and how we can use machine
learning to basically run a better
DevOps environment just a quick show of
hands to get might get a sense as to
where my audience is how many of you are
in development or development related
activities okay good
how many of you are in our DBAs or
database management okay
good probably it's probably some of you
do both so which is probably a good
excellent so I just wanna made a
comments so some of you saw Larry's
keynote about the autonomous database
and one of the questions that some of
our customers that we've had this
conversation with so what does this mean
the DBA role is going away and I think
the reality is the DBA have today have
way too many things to do then they have
enough time for so what we are seeing
more and more is the database
administrators are moving closer to
where the business is going in terms of
the absence your value-add is not in the
patching or you know upgrades but really
in running a higher more available more
scalable infrastructure running in a
hybrid cloud environment some on
prednisone the cloud so what are we
gonna do here is talk about some of the
tools that we're going to make available
to you to run a better DevOps
infrastructure ok let's get on with our
show this
so if you look at today's application
infrastructure and and the reality is
today is many most of our customers are
running in some kind of a hybrid cloud
environment they have significant
presence on-premise a lot of the new
apps are being deployed and developed on
cloud environments this includes Amazon
does ensure and includes an Oracle cloud
infrastructure as well but this is so
which means your databases are no longer
just relational databases like Oracle
but they include a mix of databases
including MongoDB Cassandra and sequel
server and oracle your application
technologies include a mix of java.net
different flavors of java not just
oracle weblogic but also you know
websphere and the JBoss and you're
running in from traditional compute into
more containerized environments how many
here run containers in further apps
excellent ok that's clearly emerging
because that allows you to scale out in
your environment so the thing is you
live in so this is the modern world we
live in and because of that your tools
that you need to manage this environment
need to be in line with this and today
you have a separate tool for Mongo DB of
a separate tool for Oracle you have a
separate tool for looking at Java
tracing and you need to have a
visibility into the entire stack because
that's how you can run a really scalable
environment so the the process of
continuous deployment is really starts
off with you know building applications
through you know continuous development
and in editor development which means
that you're able to then be responsive
to your customers using these new
technologies during node.js and get your
applications out faster the the team
that I'm in which is part of Oracle
manager cloud we've removed four years
ago from the traditional waterfall model
into a DevOps model which means we are
releasing every month onto the cloud
service we are our sprints are two weeks
long and so every two sprints we have a
released coming out so that is the world
we live in so because of that there is
continuous integration also taking place
across the different development teams
which means that you're able to
get new features out to the faster but
guess what that also means you're able
to get new bugs out to the customers
faster as well so which means that you
have a challenge that how do you
continuously provide the ability to
monitor this environment we're able to
get new deployments going to our
customers but and but also manage your
production environment by which you can
find the bugs find the problems find the
root cause whether it could be a bad
sequel taking away the database it could
be a you know basically a
single-threaded
java tear element which is causing a
problem or it might be a webpage member
javascript issue so how do you go back
here so what has happened is because of
this because the speed of deployment you
have a resource gap where you have a
DevOps team which is typically separate
from the development team they are not
able to scale or keep up with the pace
off your new code coming into your
DevOps environment and not just that
your application is also it just here is
very dynamic your infrastructure is is
no longer running on on fixed set of
compute it is scaling out which means
your compute or your storage instances
are scaling out in demand to your load
that's coming in there so this is what
we need to do is have the good name of
the game is how do you allow and enable
your DevOps teams to continue to apply
features faster and faster while also
providing production stability reliable
performance to your end users which are
really the heart of your business so the
the dev up basically prices the the the
process cycle basically is that there is
no incremental updates which means every
deployment is a complete deployment
which means there's new code being
overwritten over existing code all the
time over in every release which is how
you're able to then make your process
efficient because you cannot keep track
of incremental changes and this little
patch and this jar update you and this
sort of sequel patch this you life will
be too complicated when you do that so
the best process means you start out by
doing full updates every time the second
thing is all changes start with
development so there are no DevOps only
change
which means it's part of your source
code control which means what happens
the development is also reflected in
your Pradhan right so which means you
need to follow that from end to end so
again like I said before redeployment in
place is the model rather than you know
which means that in every release when
you develop you you deliver and then you
destroy the previous version so that's
how you're able to get the continuous
process off starting from development
all the way up to deployment now what is
the challenge off on the dev upside the
dubs side is DevOps has completely
different tools they have tools from
before for database management they have
tools for application management they
have monitoring at the host level they
have script tools that they run and they
have of course their various you know
scripting languages they run in Perl and
everything else so the the problem with
this is that you don't have for DevOps
they don't have code level
instrumentation they're tracking that
they do is very much after the fact
which means that they come in after the
goat is deployed and they try to figure
out what's going on so then the other
thing is there's no shortage of
instrumentation today so today there's
no shortage of data that DevOps has at
their disposal they have logs coming and
they have metrics coming and they have
page requests data coming in they have
all these errors and and issues being
captured today the challenge is how do
you make sense of it and an in-store way
so you're able to navigate from a page
request error drill down into log
requests I find that it's coming from a
slow sequel then drill down into the
data based here
maybe it's that's coming from a storage
component to go back into the storage
logs to figure out it was in fact it
defective maybe a disk that was being
resilvered that was actually cause for a
page timer because of the Faculty of
separate tools so the point is that you
want a way to manage this through a
single integrated infrastructure and you
need to have both orchestration and
being part of your DevOps toolset
because you want to be able to find a
way to scale out in response to demand
you what you want to do is make sure
that you've set up clear definitions for
scaling out your infrastructure to be
deploy to multiple containers
and today you have different
orchestration tools that you use today
for Scylla and this is the reality so
what we want to do is try and solve this
problem
you know in this session so the first is
to make applications work faster is to
make telemetry which is collecting the
instrumentation and storing the data in
a place part of your development process
which means that you need to make your
have your code itself be able to send
and report data which means if you're
doing weather you destroying browser
level data so we have browser bits
basically you have parts of the browser
the code which lands in the user's
browser be able to send data back to a
central place like a management store
where you can collect this data secondly
you have to go back and look at the
components that go into your
infrastructure looks at your app server
looks at your database server looks at
your hosts and collect those
instrumentation and also send that into
your central environment and for all
cases where you don't have structured
data being collected where you have
cases of data being collected from your
logs because that's unstructured data
which is a problems or errors being
reported by applications which cannot be
found through standard instrumentation
like looking for you know server
requests response time looking for a
number of user connections you have
looking at the sequel plan or the sequel
performance for that you need to have
logs so log data also needs to be
collected and stored in a single place
where you can go back and in any errors
which is not capture basically the
unknown unknowns can be measured through
the log environment as well so and all
this needs to be integrated so that when
you deploy a new container or a new jar
file that jar file needs to have the
embedded you know code elements that
actually capture the instrumentation can
report it outside your app environment
and again we realized that these apps
can be running on Prem and these apps
can be running on the cloud so in all
cases especially their distributed
application services based application
micro services application all these
need to be able to report to a central
place so you can go back and see if your
you know your your orders failing
because a third party like you know a
ship
organizational logistics company their
ApS are failing or is it in fact that
your call itself to that service is
failing to be able to trace that needs
to be done centrally you cannot have
multiple tools manage that so what we've
built with Oracle management cloud is
basically a unified platform for
collecting this information so we
collect data all the way from your
end-users so which means we have browser
code that sits in as part of the
instrumentation that collects the
end-user transaction is basically the
response time basically the first byte
the X page rendering time all that is
captured at the end user level and is
sent back to our or command cloud
repository then we also capture
instrumentation at the server request
level down all the way down to the
sequel at the database so which means
that you're not only able to see the
paid request in response time but you
also see what is the overall you know
end-to-end response as well and finally
you know things like when it's server
requests make Ajax calls we actually
capture those AJAC all timings as well
and we're able to see the full call tree
off your application and be able to
connect each of those elements to see
that you know this server request made
these three Ajax calls and what was the
response time for each of those and this
is done not just at an aggregate level
but on a per session basis and we do
this on a per session basis because
specific users might have specific
problems for example you might have a
user based in Africa who might be using
a portal to connect your application and
that may be specifically a network
latency issue which may not be faced for
example a user in in in North America or
it or in Europe so which means we want
to capture all these and present this so
that they can figure out how do I do I
need to spin off new containers in maybe
a and then Egypt based data centers so
that those responses could be handled
and you can address this as well and
finally for anything that does not fit
into this model as always logs logs is
of course the default store for any
unstructured logs but what you need to
do is find a way to quickly navigate to
the element which means you need to know
where the logs are you need to collect
those law
stream those logs and organize and
manage this log so you can see it by
different asset like timestamp based on
the specific app server where the page
request was issued based on the specific
server request that initiated those log
entries so all those are different ways
by which you need to be able to analyze
it and I'll show you some of these in my
demo I'm hopefully hoping they'll be
able to get time to actually get to the
demo as well okay so that brings me to
the demo okay perfect so okay so the
first thing I'm going to do is I'm going
to do the traffic trending analysis so
these are the four demos that I have for
you which is traffic analysis and
trending page requests to database
sequel drill down sequel performance
analytics which is a new analytic app
where we're planning on introducing as
well as resource production during
cloudbursting okay so I have multiple
demo environments so just bear with me
while I switch between them okay so
let's go back
okay so this is my application
performance monitoring so this is what
it does is gives you overall application
performance monitoring across the entire
environment it what it does it analyze
this traffic for you across your
different geographies how many of you
have have you heard of Oracle management
clouds
application performance monitoring okay
all right good so that's been a good
that's me I know this is a good baseline
to start with so on top you see the
different breakouts off the response
time at different geographies so for
example you can see that North America
is facing the highest response time
which means you can measure this based
on the scale here and right here you see
the difference requests coming in into
the application by the app server so
this is called a tree map or a heat map
view where you can see for each app
server what are the top requests so you
can see the box size is the total number
of calls which is which is the busiest
requests in terms of number receiving
the most number of calls coming in and
these the color is the average response
time so based on this you can see that
the you know service requests get
shopping card get is in fact the busiest
you know module or component of my
application
however the problem component is
actually this one right here which is
the store web checkout so in fact so
even though the busiest one does not
necessarily correlate to the you know
they basically the the poorest
performing one what we do is immediately
in one-click identify across the
applications which is the slowest
performing module in your application
the next thing we do is you can scroll
down below we give you the breakout by
the different components so for example
we tell you what are the top five pages
in terms of slowest response time the
load time so this gives you the overall
view off your application to see which
pages are your slowest pages in your
application now on top here we have a
navigation bar which allows you to go
back and make it so you can make it as
broad as you want which is you can go
across all your applications but if
you're in charge of one specific
application which my
be a complex application of my beers so
a server-based application you can
narrow it just to within those and drill
down to as narrow as you want so here
you can see the different server
requests that are being issued here and
then different app servers and and what
are there what is the peak resource
utilization in some cases if you find
that a app server is maxed out on heap
then you know there's a lot of garbage
collection taking place which means it's
highly loaded which means you have to go
back and potentially create another VM
with a different heap size and configure
it or if it's high max on CPU you might
need to give it also more compute
resources as well so what I'm going to
do is I'm going to take you down into
the view that goes into application
performance analytics so which means we
start by look at the application here so
so what you've done is this is from a
monitoring user the star stop I look at
what's happening right now so if you're
in DevOps of your job if your job is to
go back and help your applications run
quickly and get them back on track this
gives you the slowest pages it gives you
the drill down and you can go back and
different elements and analyze it but we
also take this data and we collected and
stored over the long term which means
because of our analysis that we are able
to do over the long term we're able to
then bring in data into a analytical
repository and find different types of
performance problems based on that so
let me go ahead and switch to a another
view that actually has the okay so so
the app application data is coming in
into my analytic compositor II and I'm
gonna show you the application
performance analytics you now here
instead of looking at data over like a
short term we were looking at the long
term you could it could be days it could
be weeks or months now I'm looking at
the overall trend of page requests so
here when if if performance is slowly
starting to go down I'm able to see a
a trend level to see what's happening
I'm able to see the total number views I
could clearly see the view request is
going up but the mainly what worries me
is the load time is going up and that's
clearly a source of concern so what we
can do is we also break down the page
load time based on where the biggest
problems are whether it's the first byte
interactive time of a page writing time
and we also collect the data coming in
from the app server to see where is the
highest of time being spent is it at the
database tier or the app tier so again
it if you if you look at the traditional
DevOps model your job is to focus on the
areas which the biggest problems
literally it gives you a backlog list of
what to focus on so if it's the app tier
you're going to go ahead and focus to
figure out if there's a code issue or if
it's a configuration issue that's
causing your app to perform slowly off
its database you should go back and see
if it's a storage tier she was Aquarius
you're a resource issue that's causing a
database so it gives you the clear
breakdown as to where to spend your time
here you can see the server requests and
and breakdown and any errors that are
being reported here so clearly you know
if you are the total number of calls are
going up and you're no total um errors
are going up that's an indication that
means that your application is probably
going to have more problems I mean for
example many of our customers are
retailers and they're here having this
conversation with us about using
application performance monitoring is
because their concern is as they
approach the shopping season which
starts you know in around Thanksgiving
onwards they want to be able to sure
that they can read it can handle traffic
so which means when you get your you
know iPhone X orders or you want to get
your latest toy that you want to order
you want to make sure your website is
able to handle that traffic so to be
able to see the trend and your errors
and how it handles with respect to your
traffic is critical to be able to
maintain a top-notch user experience now
in addition what we do is we have
provided the page view and response time
so here what you see here is a breakdown
of the different baselines so for
example we collect the raw data which is
pageviews and we also look at the
baseline of the load times so which
means it tells you as your page view
going up is your load time also going up
as well so we're able to see a
correlation with me page your response
time to see is your application in fact
scalable or not okay so all these are
lead to drill down pages I'll just
briefly touch on it so you can see you
know by different aspects is the load
time going up I can do more advanced
analysis like correlation to see by day
of the week to see where is my most
traffic happening during the week and
then I call I could also can see which
pages are also performing the worst as
well okay but what I'm going to do next
is I'm going to show you a another
aspect of application performance
analytics which is really looking at it
from in from a specific request so we
talked about I told you about the
ability to go from an application all
the way down into the database so what
we've done in our latest release of
application problems monitoring is
integrated with database performance
management is when you go back and you
see your server requests coming in we
know from the app server side what are
the JDBC calls being issued we know if
we're able to measure from the app
server as well
however the database itself has
instrumentation built in that it
captures the sequel data and sequel
planned data and an instrumentation
about what the different plant
operations are doing so you can analyze
a sequel not just at an aggregate level
but by the different elements of the
execution plan so we've built
integration between sequel performance
between a p.m. and sequel performance so
you can go back and look at a specific
request
okay so now I go back here I just opened
up a window and before I do that I just
wanna show you something so this is a
particular sequel statement that's being
run it's an or is a query against an
order running a specific sequel sequel
against it
it's a JDBC called so here I can see the
sequel idea that's being issued so I
just clicked on the sequel ID and that
took me into the database performance
analytic view where now I'm able to see
the specific sequel that's being
executed so I can see all the different
executions of the sequel and I can drill
drill down into that to see you know
what was the sequel and what was the
overall response time of their
particular sequel so now I am put into
the sequel monitoring view which takes
me into the plan view and now I am able
to see different aspects of the plant at
the execution plan well there is it
table access full or whether it's a
range can so I can figure out also the
bad sequel as well so now for the first
time you can literally go from the end
user view is that which is the page the
paid response time down to the specific
instance of the server request down to
the JDBC called the Ramin way down to
the sequel down to the sequel operations
literally in three clicks so this is
something we're reducing in our latest
release that allows you to do that go
from the end user all the way down to
the sequel layer as well so let me show
you one other capability that we have so
once you're at the sequel level what you
want to do is analyze the sequence so
clearly you know from an Operations
point of view this is useful if you're
gonna go and analyze different types of
sequel and figure out what's going on
there but if you're running especially
multiple applications and supporting
multiple groups of it it is impossible
to go back and choose just one sequel
and then trace one sequel at a time it's
good for sort of root cause analysis but
you need to go back and look at the
health of your overall database
infrastructure in your applications
across applications because today in
your consolidated environment a single
database might support two or three or
five
different applications in the same
database so what we've done is we've
built a sequel performance analytics
environment by which you can go and look
at your infrastructure by all the
sequel's that are running and analyze it
to see which it makes biggest
problematic sequels they are so the
first thing that you want to do is look
for degrading sequel these are sequels
whose response time is degrading over
time now that can be due to multiple
reasons it might be an issue related to
your infrastructure because there's more
load taking place or it might be a case
you have more data coming in there so
which means that over time sequel is
degrading over time now there are tools
for example in in Oracle database you
have the Diagnostics and tuning back
that find these issues like might tell
you slow secret performance but
sometimes these sequel even performance
diagnostics and tuning are not able to
detect causes of slow degradation is
because these are data that commits
incrementally you have new data coming
in every week so which means the
degradation might take place over a
month or over three months and your
traditional tools that might look at one
weeks with the data even one one month
worth of data will not be able to find
it because that data about the slow
performance has aged out from the sequel
eight of you are or are a repository so
which means that you need to have
analytical tools that can look at over
the long term over 30 days 90 days to
find these problems here so what we've
done with our secret performance
analytics is analyzes all the secrets
that are running across all your
databases or the long term to find these
performance degradation so the first
thing that you see is your degrading
sequel and we find those as well the
second class of problems that we also
find is variant sequences also variant
sequel a sequel whose performance is
highly variable so one of the things
that we know when you use like when you
go to a certain app is like the you know
the actual application page might take a
little bit longer but the setting page
is usually fast that's because you know
there's not much going on in those pages
so but the thing is if you have suddenly
for some reason somebody added a new
setting and that's kind of causing a lot
of performance problems then suddenly
that page is slow so what what what
drive
users crazy is inconsistent performance
you have you know you know one time is
running fast one time is running slow
and so which means that we're able to
then capture the sequence that are
running caused by generated by those
sequels and then analyzed to see where
is the most inconsistent performance
where you have high or low basically
response time for different types of
sequel as well
the third category we analyze is
inefficient sequel now if a sequel is
structured well and the application is
also the codes running well then most of
the sequels would be spent in CPU or i/o
which means it will be either go to
getting data from the database and bring
it into the cat into cache or it'll be
in CPU which will be processing those
that data those data blocks and then
turning it into rows and render it into
the browsers that's really when it is
represents the most efficient use of
your database now if you have a database
with a lot of concurrency if you have
using different types of resource
managers that's causing the applications
sub to run slow which means that now the
application is not running as
efficiently as you like and database is
spending its time in waiting for these
non CPU non-iron related activities and
causing your application to run slow so
we analyze that to see which of your
sequels are spending time more outside
CPU and i/o and flag them as inefficient
sequel so those up another way by which
we analyze the dimension and finally we
also say analyze sequels with planned
changes again this goes back to
predictable response not all plan just
plan changes are necessarily bad but one
of the times that people want to know is
is is typically if a page got slower if
you want to know about it so we analyze
this by different planned changes
because we collect the at the Davis
level we collect those metrics and we
bring that into our unified repository
and analyze the what are the different
plans that they were generated was the
response time for each of those plans to
analyze it
go ahead
absolutely and you'll see the response
time I don't know if I've in this demo
but but that's through absolutely what
we'll show as part of this capabilities
well so so then in addition to that we
also analyze you know we provide these
insights so rather than looking at every
sequel we go back and look at it so one
of the nice things about you know work
of databases and some of the others is
they have the ability to instrument some
of the application data that drives a
sequel because today this analysis is
tipping a typically a DBA tool you will
not have access to this as an
application developer because the DBA
uses Enterprise Manager or users you
know in their own sequel scripts to
collect this data now we have taken this
data and actually flip it around and
says we are giving developers access to
sequel data in context off the
application so we expose some of the
instrumentation that goes into the
sequel such as the application the
module the service and we collect the
data and aggregate it so if you look at
a page the page makes in a 15 server
request those server requests may issue
another 30 sequels to render the entire
page now if your application was
instrumental well each page will provide
an application module and that module
will go and make the call and collect
the data and present it inside the
application so that's what you will see
is which is the module which is the
collection of sequels that make up that
module that make it the slow page or
slow application okay and then we can
then analyze this and so here you can
see we can see relative to the response
time which of the sequence that are
performing the most you can see this as
well okay let me just switch to one last
demo and then I'll move back to my side
deck okay
okay
so in the last demo I'm going to show
you is about running your cloud
infrastructure when you run both on Prem
other cloud computer so cloud bursting
is great because it allows you to
provision compute and then grow and
expand as based on your change based on
your demand and then scale back down as
you as your user traffic comes down one
of the challenges with your cloud
bursting is the fact that you're gonna
pay more because you're gonna every time
of your time you spin up a new compute
for every time it's up and running and
if you if you don't have good programs
that can actually quiet the load down
when demand has gone down then you're
gonna end up paying a big fat bill for
your computer so that's the challenge in
the on-premise world you may provision
VMs which may similarly be running you
know at certain capacity and if your VMs
are undersized and which means that you
have traffic coming in you're gonna have
VMs run out of capacity as well but and
that's a shame because in vm's you can
whether you're running Oracle VM or
VMware you can resize those VMs to give
it more memory more compute more storage
you can do those things so the whole
point is you need to have visibility not
just into your end-user not just into
your app requests not just into your
database but also your cloud compute as
well as on-premise compute
infrastructure so you can view the
entire state from end to end
so what we've done with this is give you
again the ability to look at your entire
resource utilization off your entire
enterprise now what you see here is sort
of the whole enterprise view of your
environment where you can see the
breakdown for example by a different
operating system by different
virtualization platforms whether you're
running on cloud compute or we're
running on on-premise as well and you
will see from a CPU and memory point of
view where are my hot servers if you
which CPUs are running high on and CPU
and memory then next to that we also
provide the aggregate utilization so you
can see it by different platforms to see
what is the total installed capacity and
how much is the utilization as well but
not just staying content with the
utilization we also flag the high
resource utilization as well for example
you see this region here which is the
high resource utilization which tells
you where is the highest
in highest utilization either by CPU or
memory and therefore the ones that you
need to flag are the ones here
now on the right side this is where are
all the VMS and all the and an other
computer let me actually click on the VM
here which shows you how much is the
reek unclaimed capacity and so here
you're you set up VMs it could be
developer boxes it could be you know a
VM that you've added to cloud browser
environment
this is capacity that's not being used
which is you this is capacity is being
wasted in your enterprise so you want to
reclaim those as well so you can drill
down into each one of these so for
example if I go back to the host view I
can drill down into the CPU view and I
can go back and see where is my highest
low so for example these are 8 CPU and
for CPU boxes with 32 gig and 16 gig of
memory so I can see that some of the one
CPU box is really maxed out here because
this kind of big in red I can run a
trend and forecast to see when you will
reach capacity and when you run when
you'll have resources available so you
can go back and find it right move it
into a new server should I run it move
it into a cloud environment and those
are kind of decisions you can make
because now we have visibility and data
over the long term so we're looking at
90 degrees of data that's driving this
analysis as well ok let me switch back
to my presentation and I'll write a
couple of closing thoughts as well
so the the whole goal of we built all
this with machine learning the machine
learning it really means algorithms are
driving this which means we're able to
take this data and rather than use your
own data what would you do today you
take the data run some data extractor
put it in Excel and try to run some
algorithms today but some of the
algorithms that are available are
actually far more efficient they're not
necessarily designed so we've taken
existing algorithms and we add in some
cases added new algorithms and built
this specifically to solve specific
classes of use cases for monitoring for
example we've added anomaly detection
that tells you when a paid response time
is much different from what's normal
we've had clustering as part of our log
analysis that tells you look at all the
common logs together and which logs are
very different from those about
correlation is useful we want to
correlate end-user traffic with a
database here or an application server
or a storage response time to see is
when my traffic goes up is my response
time also going slower as well and we
use predictive algorithms in our
resource analytics and utilization
analytics to tell you when you're gonna
when is your server or your computer and
run on capacity based on current rate of
growth so that's how we introduced
machine learning here so this is an
example of how we do you know the we do
a an algorithm that we have we just
seasonale do detection to traditional
algorithms that you know you know they
run linear or they're on quadratic so
they're hard to figure out you know what
is the behavior of application in
applications that we know are fairly
seasonal you have traffic that's true
that may be low on Sunday nights may be
high on Monday mornings it may taper
down on Wednesday it may pick up again
on Thursday so that's a weekly pattern
you might also have pattern during the
day you have air clock when users come
in when traffic might go up again goes
up again at lunchtime and then dies down
by the end of the day so these are daily
patterns as well so we have algorithms
that basically detect pattern so we take
both you basically two weeks worth of
data or 30 days worth of data and we
analyze to find the seasonality and then
a prediction or it takes existing
prediction algorithms and overlay
seasonality so you will see basically
our charts will not go up
are straight down that actually shows
the seasonality which reflects your
existing workloads and reflected in the
way you're predicting your future
algorithm worse yes right correct yes
No so I'll clarify that I spoke with
those examples only because I think a
lot of people focus your focus on a
real-time kind of experience so we
actually have both daily weekly and
monthly and quarterly so we are all
those are the four buckets that we have
we're working on our yearly algorithms
as well so basically we store multiple
years lots of data so you can predict
that as well but those the four that we
have available as part of our algorithm
so which means we can do month to month
of quarters record of type analysis to
tell you what is it a monthly or
quarterly behavior off that particular
pattern so normally if your monthly then
you're obviously not going to have you
know like hourly data it's going to be
probably like daily to give you a sort
of monthly view but that's how we do it
yep yes
that's good that'll be the question so
the questions I have is so is this for
cloud databases only horses for
on-premise systems as well
the Oracle management cloud runs on the
cloud but it can collect from data for
both on-premise systems running entirely
in the infrastructure running on cloud
or both so the data is collected through
those both all the sources but all the
analysis take place in Oracle's Oracle
management cloud environment that's
correct
that's exactly correct okay yes
all right good question so the question
is do I have to update the source code
to get the data or desert on anything
automatically so we have two levels of
instrumentation we have the
infrastructure collection which is
basically a host agent that sits and
collects infrastructure metrics and then
we have an application component as well
which is actually you embedded as part
of your java it's a very small
lightweight component and all that does
is listening for the application traffic
that's coming in the payette request and
we have ages for dotnet Java no J at all
these and so we and that also has a it
goes through our gateway so if you run
on Prem then it goes through the Gateway
and sends it to our Oracle metal cloud
portal basically it's a dedicated tenant
environment where it'll be the traffic
will go in and the analysis will be done
on or command cloud so what we do is for
example if there are anomalies we will
detect the pattern and we'll find
anomalies with respect to that so
normally you know your patterns very
spiky you will not find it so it does
local spikes that takes place on a daily
basis or a weekly basis we detect that
and report it so this is some of the
algorithms that have gone in and we have
PhDs and data scientists on staff that
actually work on queuing this algorithm
actually on a monthly basis so for
example so here you might get you know
so we have we have a confidence level
interval you can specify so for example
if you have certain critical pages and
you want to respond you know more
quickly then you can specify confidence
interval for the algorithms to go back
and see and get notified now the
downside is if you keep the confidence
level really tight then you will get
more alerts and sometimes they may be
false alerts and nobody likes that so
it's a good idea to kind of start and
then training the system get a good
baseline of data before you turn it on
so that you have a good behavior of data
in place before you start to look for
bad anomalies coming in into your
learning system
similarly so forecasting this is
actually another interesting use case as
well because what happens is the
traditional forecast assumes you have a
single app and we
to running an infrastructure as well but
the reality is you're you know if you
look at a lot of our customers they
might deploy a cloud service and they
might deploy one app you know on day one
and then three months later through a
second app so in which case it's not
that the load has gone up you have a new
app entirely that has come in so we
detect something called a regime change
detection and it's not about or throwing
dictators but we have regime that
detects when a new app has been deployed
so which means there's a nonlinear
change in the workload and we're able to
see basically now you're running
combinations of two workloads on the
same environment rather than try to make
the same plot you know try to scale up
to the second one so we're able to then
see the detection of a second so then
we're able to do an aggregate forecast
based on two sets of workloads and we're
able to use that to plot your forecast
for different types of use cases like
use a traffic response time or CPU
memory utilization so that kind of wraps
me up to the end of our presentation I
want to close with a couple of final
thoughts final thoughts number one is
the entire goal of DevOps
and developers and DBAs involves
ecosystem is closed the resource gap and
the way you can do that is by building
an instrumentation across all your tears
whether you're talking about the end
user whether you talk about your app and
and database infrastructure whether you
talk about logs so have a unified view
of this so that you can then view
basically have an entire 360-degree view
of your infrastructure the second thing
is make production deployment and in
management part of your instrumentation
which means that you know when you test
for DevOps test for DevOps in your in
your at UAT environment which means that
if your users can diagnose problems in
your user acceptance environment using
the tools and instrumentations then you
can do it in production don't expect to
go to production and they hope to
figures really bad new city for the
users as well and then create records of
execution behavior which means that
start with a base set of instrumentation
that you know and over time add to it
whether you want to look at paid
response time whether you look at single
block read i/o or they want to look at
you know specific you know the your heap
utilization and reflect that so over
time you
you don't want to start by 5,000 metrics
and monitor all of them your again
you'll have a problem with too much data
and not enough insight so what you want
to do is start by setting up a set of
instrumentation and build that into your
algorithms into your management tracking
system and then eventually use this to
validate your UAT uses to validate
production and thereby you know and make
it and finally use this to have a
closed-loop system that goes back from
your production all their back to DevOps
that you have a process by which you can
identify diagnose and then take that to
improve your diagnose ability back from
into development and that changes again
make your life better
as DevOps and DBS with that thank you
very much and I hear any questions I can
stay back after this</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>