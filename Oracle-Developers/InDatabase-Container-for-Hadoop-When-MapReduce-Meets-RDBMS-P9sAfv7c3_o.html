<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>In-Database Container for Hadoop: When MapReduce Meets RDBMS | Coder Coacher - Coaching Coders</title><meta content="In-Database Container for Hadoop: When MapReduce Meets RDBMS - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>In-Database Container for Hadoop: When MapReduce Meets RDBMS</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/P9sAfv7c3_o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is quazy and I do product
management and this with me is Garrett
Garrett is the architect of the the
thing we will be talking to you about so
let's get started Garrett good morning
everybody so thanks I've I think there
were we had a few more registrations but
I'm just imagine all those people in bed
sort of saying uh I don't think I have
to scope for the first first talk this
morning so so that means that there's
you know more content for you for we've
got special special content for the
people have actually shown up this
morning so first of first of all when
we're talking about you know big data
and Java and databases you know
sometimes you know there's some
conflation between these things because
you know of course the Duke is written
in you know is its implementation
languages Java and the primary sort of
application languages Java even though
it does support other other systems and
databases are also sort of oriented
towards computation over large amounts
of data and maintaining those data that
data and so there is a lot of interest
in you know what should I use for a
particular task that I have a particular
way where I want to store my data where
I want to process my data and should i
use a database should i use a big data
hadoop based system should i use a mix
of these things and what we're going to
talk about today is what are some of the
factors that go into these kinds of
decisions and particularly how does it
affect somebody who is interested in
java programming to actually do these
kind of big data operations thank you
and we need to say that everything we
talked to there is under this disclaimer
which means even though this is not a
place to talk about product it's not a
product yet we are product izing it but
we will you know want to give you a
product pitch here we will stay at the
technology and concept level but we need
to put up this disclaimer so that you
know that
this is coming so uh so when one of the
things it's driving this whole big data
revolution that's happening right now is
that there's new classes of data that
are being stored that in the past have
just been too expensive and you know to
you know wild and crazy to have stored
you sort of think about the importance
of what go what is it you want to store
in to store and keep forever and you
basically have your current state you
know if you can keep track of your
current state you're not really in
business at all and so that's sort of
the base requirement you've got to keep
everybody's balances you know what the
active orders are but what happens is
once you keep your current stated or
managing that then of course you've got
mez loves hierarchy of data no it needs
whatever that basically sort of say I
want more more data and of course you
want to then go to keep your historical
data and people have been doing this in
data warehouses for years and they've
even been you know special-purpose data
warehousing systems or general-purpose
systems that can do data warehousing and
now people are going beyond just keeping
your history of what's happened to
actually storing the behavior of things
because and why do you want to store
behavior it's because you want to
predict the future whereas you should
actually in tone predict the future and
so if you're going to predict the future
you basically need the path to help you
and so the more behaviors and the more
behaviors you can keep and correlate
them without with what people have
really done then you can help make more
money for your company which is why you
know why are people interested in big
data because they want the big bucks and
of course there's big bucks to be made
in processing and coming up with a high
tech startup but that's just the real
reason for the big bucks is because you
know a marketing company can now place
the right dust focus their sales they
can understand their customers another
big place for big data as understanding
systems you know we all build systems
but how many actually understand you
know so you have a little piece of the
system you're in charge of but an
administrator sees 170 piece
how do those things interact and
basically ask any program or how all
these hundreds and 70 systems coordinate
and what they really do together and
they don't know and so you basically
want to look at the systems collect that
behavior and try and understand it you
know economic behaviour you look at you
know economic indicators that come out
now six months later because you've
gotten enough information to understand
the behaviors to understand what the
indicators mean it all comes late and
it's all based on this analysis of
zillions and zillions of inputs and so
this behavioral data one of the things
that makes it different than a
transaction is that it's often
unstructured and basically because it
comes from God knows where it comes from
the history of things you know so even
if you do have structured data you've
got the structured data from six years
ago which is in a different format than
you have it now and it's coming from you
know from information about your users
from tweets from reviews from images
signals particle collisions basically
all these different data sources and
because it comes in all these different
weird formats that have to be dealt with
sometimes getting a you takes a
procedural language takes actual
programming to understand the darn stuff
and so you've got to do you know some
kind of you know smart processing to
actually pull it out and so so now when
you get to you know how you're going to
store it you know once you've decided
that this stuff is important you've got
to make a few decisions where am I going
to store it you know depending on how
fast i want it i I consorted rotating
media I want to sort it flash I want to
keep it in D realm you know and and each
of these things have you know greater
costs higher performance is how you're
going to organize your data Hadoop is
you know HDFS that stores things in
zillions of flat files you go then go
and go to be trees and and Jason and
you've got lots of different choices for
how you're going to represent that data
on in your storage medium and you want
to look at how you're accessing it and
use the sort of the cheapest mec the
thing that give
UV what you need for the cheapest amount
of you know overhead and maintaining it
you know so if you want to keep
something in a database you know you can
do then have you know 76 different
indexes on this thing it's going to cost
you more than if you just throw it into
a flat file to store that same
information but those 76 indexes might
actually be useful you know which is why
people you don't want indexes and then
once you've decided where you're going
to switch out a medium how you're going
to organize it then how are you going to
process it and there's lots of different
choices there from you know sequel yeah
databases but but you know that you can
also do this kind of programmatic access
using Java and if you're doing
statistical programming you know are if
you're doing semi-structured data even
like xquery is now being revamped to
handle Jason you know if you're handling
these different you know different
things you want different tools and you
went to the tools to basically balance
the performance and the other big thing
that people sometimes forget is
development effort because most of us
here are probably engineers and you know
that if you use the right tool the right
language you can cut your bugs and cut
the time to delivery by a bunch and
having something out now rather than
something faster out six months from now
it might be higher performance but now
is always better because then you have a
chance to to change it so so MapReduce
so MapReduce is a great sort of hammer
for data processing it's basically it's
the you know distributed for loop how
you take your data and read and process
it through you know program probably the
programmatic engine where you as the
program or don't have to worry about a
bunch of the boring questions that you
know that can book take you away from
your algorithm you want to you don't
want to have to worry about error
recovery what happens if you know
machine number 316 fails or get some
sort of DRAM air or whatever disk fails
you just want a program and you want the
system to handle all those kinds of
error
recoveries and a few sort of program and
sort of say oh I think I'll build my own
thing using SSH good luck it'd be fun
and me but you want to just you know say
give me the data and I'm going to
process it and I don't want to worry
about how to get the data out how to get
it back in again all the synchronization
resource usage and so people are looking
at these systems because it handles that
and MapReduce can be used because it is
it is sort of can be used to do parsing
joining group buys all these kinds of
sort of higher-level operators can be
implemented on top of MapReduce but
still you have to do it you know it's
not you know if you want to implement a
joint on top of MapReduce its work and
so one thing people have been doing is
using sort of domain-specific languages
like sequel but there are others like we
were just discussing that can be more
expressive and easier to write and these
languages are still evolving there's you
know like Pig was all the rage for a
while now pig is it you know there's a
lot of other languages that people are
focusing on now that offer more bang for
the buck so so MapReduce in sequel so
sequel happens to be the number one data
processing language and it's just sort
of moving around the world right now
lots of people have sort of said you
know what we should just have
programmers write all the queries and in
in in a programming language like Java
and they're finding a that's really
expensive and maybe we should have a
higher level domain specific language
like sequel and so you know the places
even like Google where they were you
know writing applications against big
table they're suddenly saying that
wasn't such a great idea it was you know
we have lots of smart people but why
should they be spending their time
writing the stuff why don't we have them
use sequel and so there's a lot of
interest in getting these kind of
higher-level languages built on top of
the big data infrastructure rather than
making everybody program it themselves
and so but there's a sort of 8020 rule
that whatever tool you fit
you can't choose that it'll handle ATP
you know if you did a good job of
choosing the tool then it'll match like
eighty percent of your use cases really
well but now you've got this Tony %
where it doesn't match so well so so
even though you've chosen chosen sequel
and said wow this is a great language
it'll handle eighty percent of my needs
what about the other 20 and this is
where you basically want to open up the
hood go in and write some actual code
often using a language like Java because
that's the language people know that's
the you know has the strong typing has
all those under other wonderful features
and so how can you call java from your
sequel application and we'll leave we're
sort of proposing as potentially even a
extension to the sequel standard is
basically a way of invoking your java
MapReduce applications from inside of
sequel so that you can basically use
sequel where it on the eighty percent of
your hopefully your you use case your
queries where the query component can be
written just using that metering
selections projections joining all the
things that sequel really does well but
when you're doing parsing you know it
you know for debt you know trying to
pull out entity extractions things like
that that's better done in a procedural
language like Java and so now but now
how do I invoke my entity and strax
traction so now I can run a query over a
bunch of blobs and what do I want this
query over a bunch of blobs to return
which is big blobs for non database
people means big text objects and now
and what I want is basically a bunch of
entities which might be people orders
phone numbers tweets whatever sort of
things that are inside these big nasty
blobs you want to parse for or it might
be things like you've had a bunch of
images and you're wanting to find
barcodes that might just be in the
background of those images and pull
those out so there's all different kinds
of editing extractions and so the idea
is you want to be able to do that in the
same engine and so there's
to sort of possibilities that when you
sort of say oh I want to do sequel a
MapReduce at the same time you can
either extend your database with
MapReduce which you know and I'm then we
are from Oracle and guess what we have
we have a database engine and so we sort
of want people to buy it and so we both
we've extended the database by it by
having MapReduce inside of the database
so that you can use the database to run
your map your sequel and your MapReduce
applications all together in in a well
organized fashion but you can also do
this another way which is basically
Hadoop servers are also implementing
sequel hive ql is sort of kind of like
sequel and and it can of course on in
Hadoop you can implement MapReduce and
you can even with some amount of effort
not in the nice way we've done but you
could think imagine imagine extending
hive ql to the same kind of thing but
the big sort of take away from this is
not that everybody should go out and buy
databases and put all your data in a
database but did you use the right tool
for the job that you know there's some
places where Hadoop system makes a lot
of sense there's some systems slices
where a database makes a lot of sense
and you have to choose and that's just
part of the question the other question
is which language you write in and
that's sort of the focus of today is
when do you use when do you want to use
Java and when do you want to use sequel
and and what is the right tool for that
ok so there's something else that's
going on in the Java world I don't know
how many of you been following what's
going on in Java 8 but this is another
sort of way of expressing the data the
procedures you want to run on your data
the query and there are some people who
love sequel than so to say yes i love
sequel and they want to write things
that way but you can also express those
kind of query ish things inside of java
8 now java 8 right now then the basic
idea I know if anybody is going to
brians talk but he's good he's talking
on this on the subject at link but I'm
just going to put in a quick plug for
java 8 streams
that it allows you to stay in your Java
strongly-typed world have stream of of
objects and it's basically an abstract
producer and there's bunches of lazy
methods and the wonderful thing about
laziness is that you can think that
you're calling procedure after procedure
in it and you sort of think of it as
being done but it's not being done and
then at the end what happens is that you
get basically a graph inside of the
implementation and then it says when you
finally say oh give me the first element
or turn this into a list or basically
instantiate materialize the actual
answer then it says oh darn I can't be
lazy anymore and it actually has to go
do the work and it can basically bin
maps it to an execution engine and and
the current Java 8 release comes with a
nice multi-threaded execution engine but
you can imagine in the future all
different kinds of execution engines
that use all different kinds of systems
like maybe Hadoop and databases lives
we've been talking about so here's a
little example here but so go to Brian
stock okay so this is you know um
databases don't exist in a vacuum
there's there's a in fact there's I'll
you know if you especially if you're
hanging around Silicon Valley there's a
startup every other minute that sort of
says this is the way you should be
processing data and if you're a social
in the social media space you're just
going to say hey you're why should I you
know pay for anything i'm going to build
it all myself I've got you know 150
engineers who are anxious to build fun
stuff and so let's go for it and so
there's a lot of options both commercial
and internal that are internal to
companies that are going on hive adapt
basically builds on top of MapReduce and
postgres and there's a lot of different
options but they all have the same sort
of goal which is expressive fast cheap
gurus and and right now the world's a
bit complicated because there's lots of
different language variants of you know
well you know and you know sequel is not
quite as codified as Java uh-huh there's
their standards and then their standards
and these n sequel standard has a lot of
wiggle room there and so there's a lot
of variations and these guys are making
up their own variations to solve
problems that there's their customers
are having and and they're also working
on making their optimizer smarter
because one of the big wins of sequel
and of Java 8 is that basically you get
this whole expression tree and then you
try and optimize an expression tree and
there's of course that this is a NP hard
problem and NP hard problems are hard so
um so that's what's going on there ok so
I'm going to turn it over to Kwazii to
talk about what it means to be in an in
database to dupe implementation and how
that can you know how you can decide
when this makes sense for you in your
particular applications ok so but
guarantees the architect of the solution
so he can interject at any time so what
is going on and he just talked a little
bit about many of the engines trying to
implement MapReduce I mean sequel on top
of MapReduce what we're doing is on the
other side which is to implement
MapReduce on top of C code and I will go
a little bit into the details but this
is the big picture you know that you
have on one side C colon huddle on the
other side Hadoop on second so why do we
want to do that we which problem are we
trying to solve well the problem is you
already have MapReduce jobs that you
already have you know that people have
built those MapReduce jobs and you just
want to reuse them you want to use the
same application the same MapReduce jobs
on both sides you know on your
structured data in our DBMS and
unstructured data on Hadoop cluster you
don't want to need neglect the cause of
the Hadoop cluster of course it has it
cause you know maybe it's not it's
cheaper Nana did Davis engine but it
still you know it still
is it costs or that on that okay so why
what is wrong in moving the current
proposal is move your data in our DBMS
onto a hard cluster and process do the
MapReduce over there and then move back
to result to do them the data mining
phase which everybody acknowledged now
is better done with sequel but for the
MapReduce part the current proposal is
why don't you ship your data onto a
Hadoop cluster and we should wait a
minute you know like big banks as carrot
like to say big data can be too big to
move oh yeah and you don't want to
neglect the space requirement you need
three times the space on Hadoop cluster
okay and also issues with connectivity
operational issues how frequently how to
replicate data integrity data loss for
example banks do you think your bank
wants to ship transactional data with
your secure information on the Hadoop
cluster they might do it but they need
to mask so there are issues with data
masking you know how strong is dead mice
masking on Hadoop today no Garrett can
tell you more but it then so we talked
about the security then skills skills is
today the big showstopper for the uptake
of Hadoop you know the twi they've done
a survey across a lot of enterprises and
I think it's about sixty-five percent
say the showstopper today is the skills
so few people know how to write good
Hadoop MapReduce jobs but you have a
bunch of people who can write sequel
okay so what are we trying to do what
we're trying to do is come up with a
container similar to java ee container
java ee what is the promise of java ee
container the promise is bring your java
being on to weblogic a websphere or
a bus and with a minor config file chain
you can run your bin as is that's the
promise of Java EE we're trying to do
the same thing on Hadoop you know we are
building a Hadoop container in the
database you can bring your Hadoop beans
and you drop into our database with
minor configuration we can run the same
Hadoop job that's the proposition and
what you sing on the screen is the
comparison of a party Hadoop
infrastructure and the container in the
database and you can see the parallel
you know on one side you need to
physically partition your data on how
many North processing notes you have on
the other side we don't really need to
do well you can just do logical
partitioning on one side you have real
Hadoop nodes doing the job on the other
side we're using database processes okay
so you need you won't have 10,000 not in
the database like a Hadoop cluster but
most companies they're happy with 100
nodes or 500 processing nodes so we can
do that and then you see the mappers
running on both side you see the data
traffic between the mappers on the
reducers on the Apache infrastructure
you have to write and then read in the
database we just need to pipeline the
data you don't we just need to transfer
from one process to the other using
database mechanism yeah so if you look
at the how information gets transferred
between a mapper and reducer inside of
your Hadoop implementation it's
typically the map or output gets written
to a disk file then the reducers read it
and then the reducer output gets guess
what happens to it gets written to HDFS
you usually replicated and so the next
stage because a typical MapReduce
program isn't just one map and one
reduce its MapReduce MapReduce MapReduce
ad infinitum and and going through disk
between each one of these things costs
and so what we do is we actually take
the map
program and we sneakily behind the
scenes translated in the sequel that
then calls the mappers and reducers for
you and since our sequel engine doesn't
have to materialize all the data between
every process means that we can
basically process it more efficiently
through our system on at the end I mean
at the end of the job in the Apache
Hadoop word you have to write the output
in the individual files in our case we
come just to parallel insert in the DML
so what is the container Oh expression
like a go ahead okay yeah here we go so
he asked the question is add the Hadoop
jobs mappers running on Hadoop crescendo
they are running inside the database
yeah so it's a dupe compatible so
basically we implement the Hadoop api's
we don't take the Hadoop you know stay
fresh water changing right now so
there's no task trackers or right arrows
no no nothing of you have to be
interested and here is the container so
the container is built using apache
hadoop api you know we support when we
ship the product it's going to be a
patchy hadoop 2 point 0 compatible are
we running better testing right now with
for task execution we use the JVM inside
the database you know oracle has ad on a
built-in JVM since 8i now we are in
release 12 so it's been a proven on Java
VM for partitioning and task scheduling
and parallelism we use the PQ engine
which is the parallel query engine that
every are DBMS has okay so we can
like 100 PQ slaves and they can do the
job and no basing the PQ infrastructure
replaces the the job tracker inside of
Hadoop system and we also have our own
workload manager so if you run lots of
queries lots of MapReduce jobs at this
same time they basically will you know
sort of fight it out for who gets what
set of resources using the your Oracle
priorities and things like that that
happen inside of the Oracle system so
operationally it's going to be different
programmatically at the incident exactly
and when we say Hadoop on sequel if you
look at the container what triggers the
peak useless its sequel okay under the
cover we fire up a sequel query and we
say please parallel this secret signal
for us and the query slaves will be
running a java code which is running
inside the data isn't and one thing
that's sort of interesting is in in
addition to of course being able to run
may I produce / database tables using
database indexes notice that the in
database to do container can also access
HDFS and HBase and basically any of your
Hadoop data sources so you can now this
of course violates the rule about your
keeping the data where it lives but
sometimes you've got small amount of
data there's I the database did you want
to incorporate in a query and so you
want to be able to run queries that span
multiple data sources so what guarantees
thing is we support and we will support
and we support input form so we can suck
data from external sources even though
that's not the primary goal of the
project it's not moving aggression okay
it's not the do the opposite of all we
telling people don't move the data it's
a facility in case you want to do
correlation okay so you've seen here we
talked about let me go back one slide
one slide 222222 one side oh man you in
the wrong way it's a little bit slow
okay here when we say sequel or Java
driver I want to show you what we mean
by that you know how you can invoke your
Hadoop mappers and reducers with our
container we provide you two interfaces
and those are the two
faces on the left is the traditional
usual Hadoop driver okay so if you run
the Hadoop job this is the way you do it
you know you specify some config file
and i'll show you the demo in a few
seconds and then you run your stuff but
we also provide you a sequel interface
to run Hadoop to invoke Hadoop mappers
and reducers and that's what you see on
the left which is you can give your non
Java guys hey this is the name of the
Hadoop map if you invoke this function
it's going to invoke this Hadoop my
pleasure and if you use this function
it's going to invoke the hydro producers
job so you can have two guys while you
guys you know Java developers who write
the Hadoop mappers and reducers and
every other developer in the company can
just use sequel to run those jobs you
know to build more sophisticated
applications using those things and the
DBA does not need to know anything about
Hadoop you tell the DBA mr. DBA can you
schedule a job you know a database job
to run this script for me and this
script is running Hadoop ended under the
cover and the DBA does not need to know
anything about hadoo okay so that's the
power of the sequel interface and it can
do more than that it can pipeline jobs
on okay before that let's show a demo so
how do you run a Hadoop with the
container that we're providing the first
thing is in the configuration that step
number one you need to use this those
packages only in the configuration
you're not touching your Hadoop beans
you have the mappers and reducers you're
not touching those you are just making
minor change in the config file and i'll
show you the config file in a minute
then you need to specify the secret
types for mapping output key and input
key values and keys because we are in
the database we need to map Hadoop types
to sequel types
then you need to invoke the job specify
what are the input table and output
table okay so that's a configuration
file once you do it you've done the
configuration file you need to load the
configuration file along with your
Hadoop mappers and reducers into the
database the Java VM in the database can
only run Java code which is in the
database you cannot run Java which is
outside that's a security reason okay so
that's how you will load for example a
config file which by the same token also
have the map and reduce a cone and I'll
show you in a minute then you need to
invoke how do you invoke Java in the
database through sequel ok we can invoke
directly but the traditional way is from
sickle and by doing that we can also
fire up the picky engine the parallel
query engine okay so that's what we mean
by Hadoop on sequel we use sequel under
the cover to do all the scheduling the
parallel is among all those things and
the database speak sequel but since it
has java 8 core knowledge about caught
by the same thing and this is actually
not too much different than our database
also runs are you know so if you have in
our application because you're doing
statistical processing we basically
translate the art program into sequel of
course so we can run your our program in
parallel and now we're also supporting
the same type of sort of translation in
for java as well question
so repeat the question it's um it's not
a separate set of resources that are for
a dupe it's you can almost think of it
as a as a client library that you write
in to into you load into your session to
basically say hey make yourself Hadoop
aware so it's it's so it basically so
the container runs basically it as a
inside the particular session which a
bit of course the session can go off and
do PQ jobs and PQ isn't really that much
aware of what's of the Hadoop nature of
what it's scheduling I mean and that's
why we're and that's a good thing
because it allowed us to get it ready in
a reasonable at a time okay so this is
the demo and what you're seeing here on
the left is the Java configuration is
the configuration file where you do the
configuration of the Euro Hadoop job and
you can see that we are importing Oracle
classes to handle the configuration okay
and you can see here in this
configuration file we also have the
Hadoop mapper and reducer classes you
know we don't have separate files for
them but you can have separate files of
course so this is the mapper and does
not do much and this is the reducer so
this example is what called
sexualization and socialization is a
known well-known MapReduce pattern where
you try to find out how are people
navigating to my website so do they come
and stay for a minute and leave on them
browse anything or do they come they
browse browse by something brass bronze
brass and leave how long do they stay
without doing any activity and if you
don't do any activity within 10 seconds
we consider that this session is done
and then if you do an activity later on
or you click on something that's a
different session so specialization is
trying to find out how many events how
many click people do in a session ok so
that's a typical use case that's what
we're trying to do here ok so I show you
the the mapper and reducer code I'm not
going to explain and then this is a
configuration of how we really do the
configuration so from here to here it's
a classical Hadoop configuration you
know you specify what is the mapper
class what is the reducer class what is
the input key class input value class
output key class output value class
that's traditional high do and then
because we're in the database we need to
tell we need to do the mapping to the
single type that the Hadoop is reading
from you know so that's what we do here
ok and then we can also specify how many
parallel jobs do we want we can specify
the degree of parallelism if you don't
know the auto do PP 2 right where the
database will decide how much
parallelism you get ok and then the last
line is what is the input table and what
is the output table so we are really
reading from table that's the
configuration file ok now here this is
the script you're going to give you a
DBA seco script to run this session is
asian job ok for the demo purposes we
need to create an input table populate
the table ok but these have already been
done so I I just put them in comments
now how do you invoke the session
ization job run look here this is the
method here this is the method run here
you can see here that's what we running
when you invoke this sequel statement
called session ization you are invoking
this wrapper this wrapper
is in working this Java class this java
method and this java method is launching
the Hadoop execution okay yes
yeah yes yes that's what one of the
features we have is that we can take
basically any sort of Hadoop writable
type so if you make your own writable we
basically map that into sequel into end
and we actually use the sequel object
capability that so that even though it's
got multiple fields it will you can
create what they we call an object view
on your database to basically bring in
that object and make it look like one
object to the map so what you end up
doing is we end up mapping sequel
objects to java objects and we mapped
them by name of cough column so if if
you match if you map the fields if you
use the same field names your life will
be very simple ok so i'm sure i'm just
gonna run the code on ok the first spot
is just sequel selecting from the query
and this is the session ization you can
see here that Mary okay so since we only
have two columns we pad the session ID
with the name the name of the user so
mary has two events in session 1 this is
session 1 ok session to only one event
and you can see the timestamp and you
can see that between the timestamp
number two and session three there is a
modern ten-second so we skip we get to a
new session in session free she had four
events so she did fall clicks okay so
etc but you don't care about those
things ok so now that was the sequel
that was the Java interface now what is
the sequence of phase we it's the same
thing but the only difference here is
that instead of invoking the
so what are the things that Sal ittle
bit funny and examples like this is that
the actual content of the code that you
care about is for lines and you've got
basically lots of the and all the sort
of infrastructure is is 20 the unusual
though you know where's our hundred
lines the usual hope is that instead of
having you know 20 lines of or five
lines of real code it's that's going to
be thousands of lines of real code I
mean in a real in a real mapper you're
not just doing you know six lines of
actual coding that's pitiful that's the
usual demo problem so in this seco
script that you can give you a DBA we
are invoking the same Hadoop job but
this time we are using what we call the
sequel interface you know remember we
said you can invoke using the
traditional Hadoop interface job driver
or you can use the secret deficit this
is a secret interface and what we do is
we create a configuration key and this
is how we are creating the configuration
key okay so that is one config key that
we saw as a row in a table and then when
you invoke your map through the
secretive phase and here you can see
here we are inserting into the output
table that we have created here we are
inserting into that table the output of
the reducer and here is how we invoke
the reducer using the name explicitly
the name of the pipeline table function
which is executing the reducer code okay
so we are getting the output of the
reducer and we are inserting into a
table the reducer gets its input from
the mapper ok so the benefit of parallel
pipeline table function is you can treat
the output of a table function as a
table as a virtual table you can do
select star on that you can do a lot of
funky stuff because its sequel interface
ok yes and so the idea is that if you
look at a real sequel statement you know
like if you look at a big bi
sometimes you have these queries that
are multiple pages of sequel and now
five of those lines are going to invoke
your java the java table functions that
invoke your mappers and reducers and
what you can do is basically you're
extending the sequel language is one way
of thinking of it by supplying a mapper
an interface to a mapper and reducer so
if you have your your analysts or going
over your company's data and you write
these special map of map and reducers
that can go over your data you can then
supply those to your analysts and these
mappers and reducers of course will run
on your Hadoop system or they can run on
your on your sequel system the way they
are invoked are different but it's the
same mappers and reducers so it gives
you the same programming model on both
the dupe and inside the database okay so
before we show you the the bigger
picture you know how do you put this
into a bigger picture this is also the
power of the sequel interface in this
example in this sequel statement we are
in working to Hadoop jobs okay so
remember what Garrett said Hadoop is not
just one job one MapReduce it's a job
one MapReduce job to MapReduce and you
can just pipeline jobs like that to
achieve your goal so here we are using
one query and we partner any two jobs
you know so this is job one you can see
the map of Jeff one at the bottom um
sorry so the mapper is feeding the
reducer that one the reducer of job one
is feeling the map of job to on it's
feeding the reducer of job too and you
are selecting the result and you can do
whatever you want with the result insert
in the table do some correlation etc etc
and you can also mix in standard seco
statements you know like with joins and
selects and all the other good things
you and transformations that are written
in sequel and typically if you can do an
operation in sequel it's better to do it
in sequel because the sequel implement
you know if if the CEF sequel knows how
to do something all by itself that's the
best way of doing things the reason to
use MapReduce is because sequel don't
know about it and so you can go you
cannot express every MapReduce pattern
in seeker and my best example is can you
do um Stein theory of relativity in
Swahili yeah you can but you need to be
very good as well Haley okay so to go
back to my friends use if you cannot
express in sequel you can express more
easily in Java there are well defined
algorithm already to do that in Java
yes it did in the jvm of the database
using the parallel query system that are
the container yeah that's how I
container there was a question here you
you're the you're what you're doing is
you're leveraging some of the Hadoop
ecosystem because if you think of mahout
is mahout part of the Hadoop ecosystem
yeah but it's but we don't have to run
mahout on the task trackers and job
trackers so we're basically saying job
trackers and unit ask trackers that's
that we're not leveraging but we are
leveraging your knowledge of the loop
we're leveraging map produced programs
that other people have written okay one
question and then I I'll do the last
part which is I show you a
recommendation engine using buff in
database container for Hadoop and apache
have to be in combination one last
question like any like if you run a
query that has runs on 64 processors and
and runs for an hour whether it's
written in sequel or Hadoop it can help
you can't impact the performance of the
database for oltp so what you typically
do is you will have you'll sometimes you
do a partition of your database and so
you'll send particular long running
queries just what we call database
services and so that you can basically
leave some of your computes still for
all your oil TP jobs because if you just
sort of just sort of say oh here's
somebody withdrawing money and don't
distinguish him from somebody running a
nasty machine learning application
that's not a good idea and people
learned I've learned how to do that with
sequel and it's exactly the same
workload management tricks that we use
to keep your sequel bi queries we can
kelso man we use to manage your
MapReduce queries okay quick summary
before I gives you the last section of
the presentation which is in it okay so
this is the summer I'm this is what
we're trying to do we have my hood plan
we're going to ship with my house we're
going to leverage all the database
capability
you heard Larry talking on Sunday about
this column nothing in-memory database
you know they do the data you can start
in row in the table but when you bring
in memory you have a separate memory
space where you have column storage and
that lets you do you know analytics I
produced faster etc etc yes because we
translate your your MapReduce job in the
sequel all the sequel optimizations work
on it so that's the nice thing right so
the question is can other are dbms do
what we have done you know implement the
same in database container for Hadoop I
think they can you know if they have a
Java DM which run on the database if
they have a PQ engine must own a DBMS
have Kiki engine so it's not something
you know own you you know will okay last
last last thing before we take question
on recommendation engine so you go to
Amazon you buy a book you come the next
time they say well people who bought
this book also bought that other books
and mr. kouassi mints are mistaken ights
what here are some recommendation for
you because we know your history we know
what you like to buy we know like what
you like to browse so we can make
recommendation for you okay so you want
to build a recommendation Andy on your
own how do can you do that using what we
have just produced introduce to you okay
so you have a web blocks on the left on
the right so the web blogs are typically
in no sequel and that's where you
capture are your browsing history on the
right this is your a DBMS where you have
your orders your company and your your
customers your product etc cetera okay
so we can do data aggregation you know
by running MapReduce on the buff engines
okay so you can get a derived data like
most browse is produced by the way
blocks most browse by a customer web
blogs goes with you know people who
bought this book also bought that other
book you get that by doing MapReduce in
the database using customer order as the
transitory
okay so once you get that and you want
to know who are the best sellers just
you seek oh ok bacillus econ you get the
best so that's phase 1 and then on Phase
two we can just have a Java code which
do some ranking and make the
recommendation okay so that's called a
recommendation engine and this is the
architecture you know you can see that
we are doing DB Hadoop to produce the
ghost with and we are using data store
on Apache to do the other thing we don't
need to store the generated data in
Oracle but in this case we are using it
and recommendation engine itself is here
you know it uses the data that we have
produced in phase one so remember
MapReduce has two phases the MapReduce
phase itself using Hadoop or whatever
you want and then you want to exploit
that data to make recommendation to do
things more sophisticated to do data
mining etc etc and that's where you can
see the bigger picture so that's the end
I can show you a quick demo it's just a
video that we have captured I can run it
now it's a two minutes if it shows up
okay so what you will see is a web page
similar to what you will see on amazon
okay so you get to this web page and you
are not connected so there is no
recommendation there are product
categories and then we can log on okay
there is no recommendation seasonal
you're not connected we do have featured
product you know but then you connect
connect as India for example which we
have nine minutes ok so you log on and
that we are doing 10 minus F on the web
blog on the Apache Hadoop you can see
that the user have been identified and
then we are invoking a Java code which
does the recommendation you know
leveraging all the data we have produced
using the map
use okay so you can see this entire is
connected and then on the left pane you
we have recommendation for indira okay
so we have a bunch of recommendation for
her and if we want to make a
recommendation from camcorder she wants
to buy camcorder we can make camcorders
recommendation on the left panel you can
see only camcorders based on her history
and then you know you is she can click
that one and she can buy that one with
probably's what she liked and then we
can disconnect and we can connect as a
different user I'm have a different
recommendation etc citra you got the
picture offered do we have seven minutes
left we can take question while Stamos
chronic okay so anything we have told
you this morning is that what you are
expecting us ok the question is do we
have reference number comparison
database Hadoop with Hadoop and apache
cluster while we don't yet have public
numbers where we have good idea
performance and you know it's a physics
laws you have CPUs on memory and you run
code on both sides so minus plus or
minus you know you want to comment on
the performance well I mean the reason
if if what you're looking for is a if if
your data lives inside the database then
then you have to sort of cost you know
cost I'll well I've got to move it
outside you've got security reasons and
these kiddos each other so the issue and
so we have a basically enough
performance and we saw a comparable in
performance but we aren't sort of
advertising this instead of saying you
know hundred times faster than i'll do
that not the way we're that's not good
we're positioning it as saying okay if
you've got your data and here it is in
your database you know we want to we
want to take your application we're
going to run them where you run the
applications you want to run on the data
where it sets and that's that's the
reason to
look at in data so many motivation
unskilled skills you know you know you
don't need the DBA to learn how to any
other question yes go ahead oh no no no
the question is can it run on a single
standalone single node database of
course you don't require any right off
by the way that when we show boxes those
our database server processes those are
not database no those are not server
nodes so because it basically depends on
parallel query and peril query just
depends on multiple core is not multiple
I mean you can run in multiple instances
after corner if you want a thousand huge
like uslike here you have to help it's
going to have to be on more than one all
right system Oh in memory mean without
actually touching the disk
so the question is can adjourn in memory
doing some unique test of the
conversation because it runs on top of
sequel the usual tricks for getting
sequel to run in memory like basically
you run the job first and then you fill
everything up into into the database
cash or you can use a ramdisk I mean
there's all sorts of tricks to get your
data either pre cached or just have it
live in living memory all right and we
announced we just announced Larry
announced in the memory they in-memory
database so if everything is man we can
just run this is the last slide carrot
you want to comment that okay let's see
so this is basically you know making
again this point that you've got
multiple languages that you want to use
you want to use our you want to use Java
you want to use sequel and you have
multiple data sources that you want a
big pull from you want to pull from
Oracle tables you want to pull from
HBase from Mungo from you know wherever
you're getting your data from and you
want to use the end and you have got
multiple engines that run these
languages so and so you've got your
Hadoop you've got your database and so
now the trick as an operations guy is to
basically figure out where is the
optimal place to run all these things as
developers the main focus is on
languages and possibly data sources
because you're because you need a
certain data source because you know if
you need to access your data by key you
know HBase is going to hbase or a
database is going to be way better than
a file because files have really crappy
to find particular can so you but at an
Operations level you know it's what what
resources do I have where do I have the
spare cycles is my do I have spare
cycles on my database box drive spur
cycles on my on my machine right and so
so basically you have to sort of put you
know sort of weave together the pieces
you need yes
uh we cannot discuss packaging at this
time a product is sealing better and
Larry only Larry makes packaging
decision he makes the name you know he's
the chief naming officer and also the
chief licensing officer now we cannot
say anything about them yakety I mean
the technically there's you know there's
there's technician you know who don't
know nothing you know you can run or
anything but we don't make that decision
we just build the product and they make
the decision so there was one last
question
currently but it does if you if you're
running using the database and you issue
a sequel query it will run on the
database and access you can actually
laid up envelope from hds is so if for
example if you've got in HBase server it
will pull the data and run the HBase
input format and pull it into the
database where it does all the
processing so our map the met in
database MapReduce engine converts it in
the sequel where it runs is PQ inside of
the sequel infrastructure right but next
year we might have a different story but
but it is anything that happens but I
always always be happy this is the way
you way you your applications all will
always run and that's basically the the
whole idea of this is to to support your
MapReduce applications where everyone
around okay one last question is 9 930
they're gonna kick us out of the room ah
what do you mean well we just built the
container in the database and everything
else you just bring your own jobs it
only works with orgo ever below we use
the datum is mechanism yeah I mean you
could conceptually have other databases
implemented right so the interface you
know the way we specify the the code
could be reimplemented but our
implementation is is our own specific
database thank you very much for coming
Thank You Garrett</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>