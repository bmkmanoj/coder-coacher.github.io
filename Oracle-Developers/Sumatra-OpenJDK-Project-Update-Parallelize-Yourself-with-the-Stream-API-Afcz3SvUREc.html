<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sumatra OpenJDK Project Update: Parallelize Yourself with the Stream API | Coder Coacher - Coaching Coders</title><meta content="Sumatra OpenJDK Project Update: Parallelize Yourself with the Stream API - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sumatra OpenJDK Project Update: Parallelize Yourself with the Stream API</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Afcz3SvUREc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Eric asshole from AMD in the server
runtimes team and me and my coworker Tom
here we've been working on the Sumatra a
project for about almost two years now
which is intending to offload Java code
to the GPU or in our case it's going to
be to the AMD HSA PU which is the
integrated CPU GPU you know all in one
socket which I'll explain later I'm just
curious could I have a show of hands
like how many people here have actually
tried to already write some code for
their application with stream API well
maybe you okay third and how many people
have ever tried to write any kind of
business algorithm for their own
application and run it on any kind of
GPU almost a third oh cool okay expert
crowd okay so just some general
background about the Sumatra project as
I said this is intending to enable java
applications to run regular Java code on
the GPU what I mean yeah more or less
trans transparently to the applications
so you don't have to have any special
programming paradigm or any adopt any
special API and other as much as
possible run normal unchanged Java code
that would be in the same form as you
would write to run on the CPU we started
this project in collaboration with
Oracle back before Java one in 2012 and
while we've been working on you know AMD
oriented port of this project some staff
at Oracle had been working on a nvidia
oriented part of this project although
that one is a little bit behind where we
are now but you know there are two
possibilities going along out there now
the reason that you would want to do
this obviously is because GPUs have a
huge amount of processing power and the
problem that power has been really
dramatically increasing even in the past
few years if you look from like as it
says here look at the gigaflops could be
done by the world's biggest computer in
year two thousand compared to you know
your product now that from from AMD
that's already obsolete it's an
that's the product that costume would
have cost just a couple of hundred
dollars in this particular model you
know it's more powerful than the most
powerful computer in the world was in
two thousand and it's it's already
obsolete that's the actual card i was
using when i was when we were working on
the app arab project a couple of years
ago so for sumatra where we're trying to
ease the programming barrier to take
advantage of the apu the existing
technologies that are that are available
to to run code at all on the GPU are
basically using cuda open co or using i
guess what i call a bear HSA so and with
with those programming models you know
you write your custom kernel and then
you use a custom host side api to set up
the parameters for the colonel and then
you know get the colonel built and then
launch the colonel and so it's you have
to very much customize your application
to be able to take advantage of the GPU
power and it's a lot of extra work and
it's a lot of extra learning there are
already some that are somewhat more some
some of these the api is already sort of
more integrated into use with java for
example a Parappa which came from AMD
and now it's an open source project and
then Joeckel there's another one called
open seal for java there's a whole bunch
of different root beer is another one
that works with an idea there's a lot of
solutions like this for I go what i
would call less or like only slightly
awkward integration of java with the GPU
but we're going to try to make that
almost seamless so more background about
the GPU processing so with the cpu
obviously you're going to run your OS
you can you know you're going to run
threads do branchy work you're going to
do the system i oh you know read from
disk do your networking and so forth and
that's kind of you know the traditional
java application workload especially for
server-side workloads the GPUs are
traditionally used for graphics work
which and those are very data parallel
which means you basically have you know
an array of data and you're doing
actually the same operation or
transformation on the data as that
algorithm runs along whoa okay and so
now with stream API the programming
model that is available in Java 8 is
more amenable to taking advantage of the
actual way that the GPU operates so you
can set up your Java workload that will
be more amenable to running on the GPU
and as I was saying so the GPU has
cindy's that are optimized for data
parallel operations and the way that I
kind of explain this to people for the
first time would be to think of it as
running running an application with
hundreds of threads on some data where
all the threads have the same PC at the
same time the as each GPU course starts
up the typical way that they work is
they get what's called a work item ID
and then they will use the work item ID
to index into the data that came from
the host side and that's how they'll get
their unique little bit of data that
that particular core is going to work on
so and the vast majority of algorithms
that we've implemented it uses the work
item id as an array index to from an
array that was passed in from the host
side by data parallel you I mean data
parallel means algorithms that well our
thread safe and could be you know a good
way to think of it is if you can run
your loop backwards or forwards and get
the same result either way then this is
might be an algorithm that's amenable to
data parallel computation and could be
possibly run on the GPU the middle
bullet here shows how to write that
first for loop as a stream operation and
I think there's been several other talks
during this week showing how you know to
get your mind set around being able to
take it
the features in stream API instead of
writing everything as imperative style
for loops to be able to take advantage
of the all the library capabilities that
come with stream API and make it that
you can actually have less code and less
you know have less code in your app and
let the libraries do more work for you
whether or not you're doing anything
with GPU so in the past you know taking
advantage of whether it's like nvidia
cards or AMD discrete cards which but by
discrete card i mean you know a card
that you would buy the big thing that
you click into your PCI slot in the
computer that's a discrete GPU so and
you know you take advantage of using
that by using khuda aur opencl and those
kind of workloads you know the
characteristics are you know the the
order of iterations as input is is it's
it's not important see between the order
in which the work items run there yeah
so you there's no there's no ordering so
that the all the waves can run
concurrently these keep you is going to
have an awful lot of scream cause I
think it's actually now it's far more
than 1500 stream course and again you
want to the algorithms that are going to
get the best performance are going to be
ones with little divergence where we
often say that the core should all be in
lockstep as much as possible with few
divergent branches and by that the way
that the GPUs work is that when you have
a divergent branch some of the the waves
that don't take the branch will be
predicated off so basically imagine your
whole thing is running and then you come
to an if and say this side takes the if
and this side doesn't so while the if
side runs this side runs this side is
basically completely shut off until the
if block is done and then the else block
runs if you have one and then this side
is completely shut off then when you
come to the end of that statement both
sides get turned back on and they keep
running so if you have a lot of
divergent code in your GP
algorithm you know you can basically
immediately decrease your throughput by
you know fifty percent or more depending
on the exact nature of what you're doing
and so that that's kind of work we
really what you tend to want to avoid
when you're thinking about the
algorithms that you might want to run on
the GPU for discrete cards where the
memory has to be copied over the bus to
the card you want to have a lot of
computation happening in your GPU
algorithm to overcome the extra cost of
copying the memory to and from the card
over the bus and that's what this graph
is trying to show here you really don't
want to copy stuff to the to the GPU
just to do a very small bit of work
where you're going to immediately copy
the answer back the new capability the
HSA which stands for heterogeneous
system architecture where memory is
integrated between them the memory
access is not over the bus to the GPU
it's integrated with main memory I'll
explain this in the following slides and
so HSA is going to be able to remove
some of these limitations about the you
know it eliminates the copying and so it
will lower the barrier to be able to
make your GPU algorithm have good
performance compared to what was
available in the past so more about HSA
AMD started with several other companies
this HSA foundation which is to
establish this open architecture for
having systems with with cpu GPU interop
and there you can see the link to their
homepage there along with the HSA
there's kind of there's a runtime API
for launching the jobs and there's also
this h2h sale which to answer HSA
intermediate language which is kind of a
well it's an intermediate language kind
of an assembly of a generalized assembly
language for data parallel algorithms
that that HSA systems would admit and
then any given hardware platform would
compile the H sale down to its own
particular ven
machine aissa to actually run the
colonel so if you're making an HSA
application if you just omit the H sale
language your your algorithm will be
portable to any platform that supports
HSA because when you actually run it the
native platform support will convert the
H sale down to the machine aissa to run
the kernels on that particular GPU
platform right and so in that way the
this HSA architecture is going to be
kind of machine agnostic for the for the
GPU system that you're running on and
also it allows a more more easy way to
support a bigger variety of higher level
programming languages and obviously in
our case we're doing it with Java the
main single biggest feature of HSA is to
allow the access to the main memory
directly from the GPU and there's cache
coherency between the CPU and the GPU
and we're taking advantage of that in
our system so that the GPU kernels can
directly access the Java heap for the
hhsa enabled products that are already
available from AMD which is the like the
a-10 desktop processor line also called
Kaveri um so though you can buy those
now and you can buy desktops and laptops
with that part in it now and all the HSA
enablement software for these products
is available in github so it basically
there's a Linux kernel driver and
runtime libraries which is now supported
in ubuntu and in fedora and then like in
the case of Sumatra you set that up and
then you'll get the Sumatra enable JDK
and then you can take advantage of
Sumatra running on HSA the other thing
that the HSA foundation has funded and
supports is a an HSA simulator that just
runs on ordinary ordinary computers you
know just you with sort of an llvm based
system where you can run kernels under
the simulator which helps you know when
you're just learning or debugging HSA
algorithms it's it's very easy because
the simulator has good support for GV
and you
step through there colonel in a way that
seems quite normal or you're familiar
with using jdb to be able to debug a
colonel so that's a that features that
really been great for us even in our own
development inside AMD as we've been
working on this project and the
simulator is also available through
github and it just builds in a few
minutes so here's more background about
the APU so basically this is a product
where the the CPU and the GPU are all on
the same socket and the system the whole
system it is designed so that the GPU
will have direct access to main memory
they've had different versions of this
since 2011 and the features have been
getting better and better the Kaveri is
a desktop part that came out maybe
around the beginning of this year which
was the first one that fully supported
the whole HSA software stack and then
soon we'll have a server server APU part
which its code name is Berlin like if
you look in the Google or the register
you know you can read up on this one and
that'll be a server-based part that will
take you to be able to take advantage of
HSA so because of this direct access to
main memory the HSA makes a great
platform for Java offload because now we
can have direct access to objects in the
Java heap which would not have been
possible you know it's not possible with
our apparatus solution using discrete
where you have to copy the memory to the
bus so the way that we want to you know
the easy way to think of this is a
pointer is a pointer whether your
algorithm is running on the CPU or on
the GPU and there's no extra copying or
translation but involved
no I mean that that that sits directly
addressable directly from the GPU same
page tables right it's using the same
page tables as the CPU side process okay
so now I'll get into explaining a little
bit more about our HSA enable JDK so
yeah this feature set that we have been
enabled so far is to be able to offload
some JDK 8 stream lambdas to the GPU if
you've been to talks earlier this week
you've seen that they added this dot
parallel function into the stream API to
indicate that the follow-on functions
could be run in parallel and I've seen
different advice pro and con about using
that but if you've put that parallel
indicator into your stream then we
assume that you've done your testing and
that the lambda that you're calling is
thread-safe and so it will be safe for
us to run it in the GPU with all of the
restrictions about the ordering that I
had in the earlier slides so and that's
that's our indication to take that one
to send it to the GPU yep
if they at this time we don't have
anything like that but you have to think
about how you would integrate that into
normal strain API anyway right it would
be right you would want to sort your
data into the chunks where you want to
maintain the data parallelism and have
as little locking as possible it is
possible to have barriers and things in
the GPU algorithm but we don't really
expose that because we're trying to make
ordinary Java code work as much as
possible without adding we don't want to
add a lot of the GPU voodoo if you
wouldn't call it into the java
programming model we're just trying to
make it so that straightforward java
code can be run on the GPU i mean maybe
at a later time we would come to that
level of support but for the time being
we're just trying to make
straightforward algorithms work
correctly let's see okay right so in our
JDK you know we're only going to offload
the streams that are indicated with
parallel because like you said we think
that the algorithm the lambda from the
user or the application will be
thread-safe and as you probably know by
now the default way that the parallel
streams is run is by using a fork join
thread pool that's implemented you know
deep down inside the guts of the stream
API implementation so another really big
important part of this project is we've
taken advantage of the growl growl
compiler which is another OpenJDK
project that comes from oracle labs
well I'll come to that in the following
slides let's get um right you want to
make sure well what I would in so get
right what i would do is write an
outright your algorithm first just as
the default so it would be single
threaded make sure that passes all the
tests then put parallel on the end run
it as CPU parallel streams and make sure
that passes all the tests and then turn
on the Sumatra features and then run
that and make sure that passes all the
tests right that way you'll know that
it's really working correctly yeah so um
yeah we've decided we've been at the
recommendation of the Oracle we've used
the grawl compiler which comes from
Oracle labs for this project and we've
added H sales support integral and they
have been terrific in working with us
and so H sale is actually fully
supported in the public Gras compiler
that you can check out and work with
right now and you know that I think
might be the only other back end that is
supported in Gras that's you know did
not come from inside Oracle itself they
also they support x86 and spark and
we've contributed H sale and they've
taken it into the main product so in our
implementation you know if anything goes
wrong or we find that the colonel is too
complicated to compile for each sale we
we have seamless fall back to this
normal stream API thread pool and yeah
you can all this is an open source
already and you can check them all out
and get them going right now so the way
that you would do it is you check out
the Sumatra JDK first and build it
according to the normal JDK build
instructions we just have a very small
amount of changes in the JDK repo a to
divert the the you know to divert the
work to the HCL part I'm
it's probably less I'm just all going
I'll say less than 500 lines of changes
into the jdk to enable this feature so
you build the jdk you check out Grall
and then you use the jdk you just built
in the first step as the java home when
you build Grall and then it builds a HSA
enabled growl jdk inside the grawl bill
and that's the thing you're going to use
to run the offload a bulwark where we're
formulating a plan to be able to offer
binary downloads of this whole build so
that you don't have to do this because
you know it requires a certain amount of
you know build dev dependency set up on
the machine and hopefully we can just
get rid of that and as I said earlier
you know this we do all our work on
Ubuntu and I believe it also supports
fedora so hopefully we can offer binary
downloads in the next few months
depending on what the lawyers say and
then you took the HSA simulator mm-hmm
yep that you can have vista running on
any machine yes that's actually how the
system actually the so gras comes with
this customized python-based build
environment they call MX and so all the
grawl tests which includes like i don't
know let's say 150 hchs a specific test
that we wrote and contributed so when
you run the j units using MX and you're
on and normal actually on any computer
we have set it up so that it
automatically downloads a prebuilt
version of the simulator and it will
automatically run the test using the
simulator on your machine so it will
work on any laptop or desktop or
whatever right if you just want to run
under the simulator you don't need to do
anything if you want to take advantage
of the real libraries with real hardware
you need to download that stuff from
github that I had in the earlier slides
and then set up your LD library path and
then when the MX or the I mean when the
JDK boots and they see that the LD
library path is set then then it won't
use the built-in system with the
simulator it'll actually try to load it
from you
environment and that's how it will go to
the real hardware
I couldn't quite understand or Mille the
normal JDK bill library requirements you
know whatever would get to pass can
configure it doesn't require anything
else no and we don't I mean we don't we
don't interact with cuda at all but
because as i said earlier i know that
oracle people had been working on cuda i
actually have a machine that's set up
with a CUDA card on a non HSA system and
so i can test the HSA simulator or khuda
on my one machine it doesn't interfere
nope there's no problem yeah okay that's
enough of that are we doing here okay
good right and so the in our
implementation right now we often we can
offload to different stream operations
for each taking a lambda from the user
program which gets compiled by Gras or
doing reduce which is done by a slightly
different way with in-stream like min
max and some and I'll explain a little
bit more about reduce in the later
slides so again when the colonel is
running the coronal's have direct access
to the java objects directly from the
GPU cores and so there's no copying so
this can really give you you know allow
you to have really good performance if
your algorithm is very data parallel so
for the two things we support there they
both you know they work a little bit
differently with for each so for both of
both for each and reduce or what they
call terminal operations it's like the
last thing that you would have and if
you have a multi-step stream and for
each has no return value it operates by
in effect by side effect on the input
the input array or you know the the
stream source array and so it's going to
iterate over every single thing in the
stream and do some operation according
to the lambda that you passed in with
reduced it's going to take an array of
integers and add them up or find them in
max whatever and then it returns the
single integer results so these two
two operations work slightly differently
the reduce well I'll get two more about
the reduce a little bit later so here's
kind of architectural picture of how it
works at the top in the blue box you
will have you know your application code
and this one for example is going to do
in stream range so that's going to be a
range from 0 to 10 24 there's the
parallel indicator for each and then
where it says lambda that's going to be
your function that you pass in that you
want to make into a GPU colonel so you
CGI d there GIM probably should change
that but so did or global ID is kind of
a jargon carry over from the opencl days
but it's basically like the work item
that but in normal java stream api stuff
that you agreed usually they call the
iteration variable P but here it's
called GUID don't worry about that so
then when the for each gets called it's
going to come down into our specially
modified stream api we examine the
stream structure that is you know that
the that's part of the for each and so
we sort of reach back into the stream to
find the stream source array and also to
find the lambda function that you're
that you want to make into the kernel
send it to growl get back the a sales
texts it goes down into the JVM where
it's going to start to actually call h
sale we have a special library system we
called okra which is for us it's like
this sort of easy API into H sale so we
interact from the JV are specially
modified JVM we interact with okra to
get the colonel finalized into the
native iso format and then so we'll get
it back from from oka will get the
colonel and then we'll dispatch it to
the HS a run time to actually run the
colonel in the on the GPU side of apu
and it's for for each it's going to do
the work and it's you know it's like i
said it's going to directly you know do
whatever the results
is right back into main memory yeah
that's some butter no need to say for
that you say every time that for each is
cold you generate yeah oh no it's so
after the first time you do it the after
the first time you do it it's going to
be cashed in our specially modified jdk
so yeah you don't you don't continuously
rebuild the colonel from scratch every
time you you invoke that call site again
and again and yeah they're both both
sides are cached both for for each and
for reduce I remember us talking about
that but I don't remember I'll get back
to you on that one I don't think it's a
problem for the reality of what we've
been working on so far and our
implementation right now we're just
model it's um we're actually managing
all the kernels in this but in the in
the JDK side so it's not at this point
the kernels are not exactly managed like
regular jaded and methods would be in
the hot spot code cache we're still sort
of maintain native all in the Java layer
which might not be the greatest way to
do it in the long run but we haven't
really decided on what's the best way
forward or how much integration do we
want like do we really want to have the
CPU side and method notion of the
algorithm that much integrated with the
colonel notion of it because the calling
convention and everything is so
different i'm not really sure if we how
much we're gonna have those integrated
no okay so the way this works is in the
earliest days of our project we were
running are we were launching the
Colonel's everything as Jay and I where
there was some pinning but since like
about a year ago now we've integrated
into the way that Oracle was doing it
and the colonel runs as in a in hot spot
they have these notion of thread states
of how they manage this ordinary Java
code and so when the colonel is running
the java application thread from your
blue code up there it's in a hotspot
thread state called thread in vm and
that actually blocks GCS from happening
because it's it's it's it's it's it's
the same way that normal java threads if
you had threads running in the rest of
your app while you have one thread going
to run the colonel those threads are
also threatened vm so and when i when
one guy goes and he can't get his
allocation right and then it's time to
have a safe point into a GC everybody
will gradually coast sort of to a point
where they'll all block and let the GC
proceed right that's what called a safe
point so in our version when you call in
to launch the colonel it goes into the
vm and now the thread state is thread in
vm and so no GC can happen we have maybe
i'll get to this in later slides but
anyway we have the optimization and safe
points implemented and so in order to
maintain like the least disruption to
the throw to the work of the cpu
mutators what happens is if you take a
safe point while the colonel is running
the colonel will basically abort and
complete the work on the cpu so that the
GC can immediately precede the CPU
mutiny mutators can continue to run and
your probe like it's for example if you
have a window up and the colonel was
running for a long time you know your
gear we would freeze right because you
couldn't update the screen so we we make
it now so that the colonel will just
abort and right now
it's done in like the slowest possible
way but it doesn't it prevents the GUI
from freezing as one example so in
future will either have a solution where
we don't need to abort the kernel or if
we do have to revert to CPU will
complete the rest of the work in a
multi-threaded way on the CPU or
possibly even relaunch a sub range of
the same kernel but we have right right
we had demos some of our own apps with
gooeys I mean we could see them pausing
for these kinds of reasons because we
had one demo that were they the CPU side
the CPU side threads that updated screen
had a really high allocation rate and so
a lot of GC and then you would just get
the thing frozen until it so yeah we
fixed that by putting safe points into
their curls right yes you can out you
can allocate objects from kernels I mean
Tom did all that work using a special
version specially modified T labs which
is the normal structure that they used
to do allocation in hot spot and you can
yeah there's a scheme of like shared T
labs among the GPU work items and they
can allocate quite a lot but then if
they also they can also come to the
point where they can't get any new T
labs and then they would also aboard and
have to you know return to the CPU but
yes you can allocate from the kernels on
the GPU oh look I have arrows oh I ok
Wow look at that I guess I view these as
PDF most of the time you can see fancy
stuff is happening ok so right so the
yeah ap us have hundreds of this this
one on the Kaveri especially it is like
hundreds or actually thousands ya lo
thousands of stream cores and as i said
earlier each HSA work item is used as an
index into the stream source array that
gets passed in as an argument to the
colonel so each core does one invocation
of the lambda / work item per wavefront
where wavefront is a GPU notion
in HSA the wavefront size or at least on
our hardware at wavefront size is 64 and
so the work tends to run as 64 work
items in lockstep and it that will be so
you'll you know they'll be a depending
on how long your input ranges the work
will be divided up into that many wave
fronts and then whatever's at the end
they'll be like a partial wavefront wood
or whatever is not you know the mod 64
part so we support in streams and we
also support some object streams fact by
array vector or ray list please it sorry
these these particular collections are
ones that are actually backed by a real
array so we can reach in to that data
structure and get the stream source
array and so that's the argument that we
pass to the colonel where the work items
are going to index into that array
backing the array list to find out what
their work is that they're going to do I
am so for regular lamp is here in the
middle part where it says regular CPU
lambda regular lambdas get called once
per item in the collection the way that
the GPUs work is a little different and
I have a code example in the follow on
screen where it so you can see how to
relate the way that the CPU execution of
the lambdas work to the way that the GPU
execution of the lambdas work when we
come into the for each API we need to
collect the lambda target method that
comes from the user program and the way
that we do that is we take the when you
have lambdas or stream API in use in
your program the system automatically
creates these objects called consumers
and consumer is the thing that sort of
connects the calling convention of the
JDK API to your lambda because your
lambda may have captured arguments
coming in from your you know lexical
scope so the consumer is an
automatically generated object that
contains all of your extra parameters to
the colonel so we have to reach into
that
ject and find the call and that's how we
find the lambda method so we basically
use growl once to reach into the
consumer and find the method and then we
eat so now we got the method and now we
call growl again to build the colonel
from that method and in the sort of
middle layer where like this for each
coming in and reduce coming in and then
we have a class called pipeline info and
inside pipeline info is where we cash
the colonel so that when you call it
again it'll be reused yeah oh sorry this
is just actually what I just said so the
lambdas have the lambda arguments come
in in the normal system from the
consumer object which gets built up when
you're you know your stream that part of
your stream application gets loaded so
the object he created to connect the
calling convention between the stream
API and your your lambda we again
because we have direct access to main
memory if you have reference fields in
your objects they're going to be
accessed through memory operations
exactly like the normal CPU methods are
so it's not like it's not like
everything has to come in as an argument
through the kernel launch API this is
actually just in you know getting your
object pointer and reaching into that
object pointer same as it would be in a
normal like x64 compiled and method and
then again that's the cool thing about
HSA it says pointer as a pointers and we
can do exactly the same structure of
code is on the CPU side if you look at
our code you'll see we have this layer
called okra which is our interface that
we've been using for a long time to
insulate ourselves from changes in the
HSA runtime the HSA runtime is actually
final now so I'm not exactly sure what
the okra future will be but it's so easy
for us to use I hope it stays around
it's like super simple interface to
create the kernels and launch the
kernels and it ended up being so popular
that even that AMD the team that's doing
the openmp port of OpenMP conversion to
HSA also decided to use okra in their
own project so it's worked out pretty
well going down this strategy and now
because it's benefited two different
projects we have two different versions
of okra one works with the real hardware
and one works with the simulator but
that's all going to be just transparent
to you as an application running it's
this is if you want a deep dive down
into our code you can see how it really
works ok so here's an in-stream example
where we're going to actually build into
a panel and so this one's going to do
baseball you know baseball statistics is
calculating a player batting average so
the way we're going to do this here is
we have our array of players this one's
going to be an in-stream using a range
so you create the range to the you know
range from zero to players length with
again indicating our parallel for each
the very first thing that happens is we
use so can I walk over here so n is what
they in the lamb is what they call the
iteration variable and this is going to
be you know 0 1 2 3 dot so here we're
going to use the iteration variable as
the index into the players array to pull
out the player object and then you know
do the batting average calculation hits
/ at bats with two gets and then a set
and if it turns out that there's no
iPads for that player that decided to be
zero and so this code in red I guess you
can see that that's a different color
right so this is this here is what's
going to become the colonel
so this is what it gets converted into
so here at the top you can see the
biggest lambdas in Java Sea they really
get compiled into normal methods so at
the top you can see the arguments for
the lambda and because this is an
in-stream lambda again the iteration
variable which is the the last the last
argument is an int and then the players
array go back one so the players array
which comes in as a capture from you
know from the lexical scope of the where
the lambda is so that is a capture which
comes in as the first argument and if
you have more if you have cell verlos
you can have a whole bunch of captures
coming in and then always the last
argument into a lamb that will be the
iteration variable so so we pass in the
players array we get the work item ID
which is which is the thing that I said
each stream core gets its own unique
work item ID over the entire range of
the job then it will does a we call it a
like an array index out of bounds check
on the array and then uses the work item
ID to load the player object out of the
array and then these are just like math
operations to do the divide and so forth
and then do the set so the main thing
that i want to show you here is really
is how the arguments come into the
kernel and how we use the work item ID
to index into the input players array
okay so this is this similar one using
an object stream so here we're going to
have the hitters or the players made
into a string a parallel stream so now
we're going to do s dot for each of P
where he in here each p is actually a
player object and it's not an int so
when we get into the body of the lambda
P is the player and we don't need to do
any extra action to index into the array
in our Java code right so this one is
like more straightforward natural Java
code so this one is a little bit
different here right when you look at
the arguments the only argument coming
in from the actual Java Sea signature of
this lambda is the player object but the
way that we have to get these player
objects down into the kernel is we pass
the whole array of players that we
ripped out of the guts of the stream as
an argument into the kernel and then we
use the work item ID to index into the
array the same way that the other case
did but here it's all done under the
covers because this is actually in you
know in your user program it's a stream
of players which is actually backed by
an array of players and this is how we
get the right player out of the stream
of players to do the same math operation
so that that's really the most
interesting thing about the difference
between object streams and in streams
and we support objects to object or
collections that are actually backed by
an array raised a million all mm-hmm do
you do the chunking in 2000
no we haven't really had any good use
cases like that yet but I don't really
see that it would be a problem because
there's no one here in length on the
amount of the work of the grid size that
you can send in to HSA so I don't I mean
I did make a prototype where I was
chunking up work when the early when we
were working with safe points as a trade
off of what what if we didn't have safe
points we could just chunk it up so that
we would return early but as we get it
like better real-world examples to work
with maybe you know we might see some
different trade off so right now there's
no active chunking happening it should
yeah they should be able to handle a
very long input array so ok oh here yeah
oh you mean this here so right but the
yes the intent of this is that that the
the great majority of players will have
at least one at-bat this is just to
prevent against the divide by zero
happening this is not I mean this is a
safety check in the algorithm not part
of the algorithm itself right yeah no
that's a good catch though because
that's a good thing to call out here yet
right if you had if you had a case where
there was a player with no at-bats
you're right it would go into the elf
block and the whole whole rest of the
thing would be predicated off right i
mean but to deal with that one item yeah
the ray
right so what would happen is that like
you know the for the wavefront that
experience this situation they'd all be
running in pretty close to the you know
the very you know after the first what
is it nine or ten instructions you get
to this point here right so they all run
in unison to here then for the like say
if there is one that's zero the ones
that do okay so here's the set to zero
part so the ones that are zero fall
through this block everybody else is
predicated off so this one will run then
when it gets to the part where this is
done so he'll write he'll fall through
here whole block at the return because
this is a common point then the rest of
the thing will be the the other side of
the wave front will be predicated back
on this side will run and then he'll
jump to the return because you can see
here it's branch to l-3 which is the
return so then they'll be back together
in lockstep and the colonel will be done
no I think it was float in the example
no I mean HSA supports float and double
to fine yeah that's just I just happen
to write it this way I didn't really
think about it there wait what if such
way that's pure math um ha we don't be
short branching just pure math is there
such a thing or maybe in this example
yeah cuz the I don't think we we don't
we don't explicitly well that would
still be a branch right the problem yes
that will you be a better way to do it
the other thing is that the I don't
think we support / 0 checks yet the way
that the GPU the GPUs tend to not they
just tend to like eat this kind of stuff
and set some condition code that you
check later so we we need to have
explicit support 4/0 in order to be able
to throw the exception correctly back to
the the CPU side so that's yeah
not the number and then they take
another number and you just keep going
with the number it just put it up the
number all the way down right so you
could do you could do a division by zero
but what you get back is not a number
well but but but we want this to be
seamlessly compatible with the with cpu
side java math so it what would really
we do if we didn't have this we'd really
have to have an explicit divide by zero
check in the code that we haven't
implemented that yet I think but that's
really would be the the right way to do
it and then you would get an exception
back in the caller on the cpu no I'm
just saying that we haven't implemented
that yet that's all Yeah right I'm fine
with this example because you actually
want to stay yeah
right yeah that that's actually the
follow on slide so did we don't we would
not compile it into a kernel it would
fail to build we don't because we don't
support anything that's gonna you know
when you're in a lambda that becomes a
kernel you'd say it's not supported to
do anything that's going to be coming
any kind of like JVM runtime call or
call c code in jdk api it has to be like
self-contained algorithm of just normal
java code
yes we were somebody was just asking
about this yes we haven't implemented
anything about that because we're just
trying to get normal Java code to work I
guess I guess it's yeah the amount of
people that have asked her unsafe is
almost startling now that i see that may
be really a lot of people in the outside
world are really using unsafe more than
us JVM hackers which is kind of scary
okay so what happens when an exception
happens so saudi optimization is a
normal part of hot spot where it you
know can abort the running of a compiled
method and revert to interpreter and
this has been in hot spot you know all
along so we have an extension of this to
work with the HCL kernels the you know
the main one that we would use it for
right now is handling array index out of
bounds so and the growl team helped us a
lot here they add a couple extra
features integral to make this as easy
as possible for us working to transition
from the kernel back to the CPU side so
what happens is when the colonel gets
compiled the grawl side is also looking
at the control flow and for each point
that could be a d optimization point
it's building a special x86 a handler
method that's sort of a like a stub
that's in the code cache as a regular
stub and then when the d optimization
happens it calls the stub to fix up the
stack so that it will set the stack up
correctly with the arguments so that we
can revert to the interpreter for the
kernel execution and so that means you
when you have a throw half way down
through your kernel associated with a
certain bytecode index this thing will
get called create the stack and resume
execution of that method in the
interpreter from the same bytecode index
exactly the way that it would work is if
you took an exception in the middle of
some jaded code
so that's how we revert from Colonels
back to the CPU execution if something
goes wrong during the colonel the no as
I said before when you have a big range
the work gets divided up into wave
fronts of 64 and you know your thing
your your algorithm is running and let's
say some of them have run along some
have completed you get to I don't know
let's say the twentieth one and that one
ends up doing a throw some some of the
fall like they'll be a certain amount of
wave fronts that are all running
concurrently on the machine at that time
but there's also a lot of wave fronts
that never even started yet because you
know the machine only has so many stream
cores and your range is presumably
bigger than the amount of shine cores
right so the ones that you know are just
still queued up we call those never
rands and there's no exchange state
associated with those so they can be
completed on the cpu as normal calls and
so there's no extra fix-up work that
needs to be done to complete the
execution of that part of the algorithm
so when we get back on the cpu we can
just keep calling java call java call
java call to clean up the rest of the
ones that never get executed on the GPU
we just haven't got around to that yet
will abort the whole it'll bought the
whole kernel yes I mean in the future we
may try to restart the Colonel's we
don't you know we're trying to think
about what's the best way to do analysis
we do we want to keep relaunching a
kernel is just going to keep the
optimizing over and over again because
that would be really bad it so it's the
kind of a do we want to relaunch a
colonel with a subrange or do we want to
resume execution and the CPU with like a
thread pool where we could use all the
cores in the in the on the cpu to
complete it as if it's the kind of
pseudo parallels you know a pseudo
parallel stream on the cpu side but
really it's going to be the cleanup
architectures is supporting
oh yeah right now this is just for X
just just for x64 and this works out
with like ubuntu 1304 and it works with
whatever is the latest fedora
well it will be slower I mean right now
we're in a mode of this project where
we're just trying to maintain the
correctness it will be slower but the
other thing to take into account is once
you if you do d optimize back through
this stub back to the interpreter
execution it will osr that method the
same as anything at whatever get OS art
and so it will still get compiled and
fixed up so it's not quite as I mean are
we're still in the early days of this
project but it's not as disastrous to
fall back as just running all the rest
of your work in the interpreter yeah so
basically what happens after the d
optimization out of the colonel it's
going to be in exactly the same state as
if the method had completely run on the
CPU or all along or if the colonel had
completely finished correctly on the CPU
so you know you've done a throw and so
some you know at least one work item
that got resumed as a lambda invocation
back on the CPU would get that exception
at the BCI that happened in the kernel
and he'll have to deal with that however
that program would have dealt with it
anyway and then you'll have the never
ends it get completed as calls and then
you'll have you know maybe some other
ones may have a bladder or maybe they
awarded at some other BCI so and then
some of them would have bought some of
the work items on the colonel would have
already completed anyway and so there's
nothing else needs to be done for them
Oh somebody asked earlier so we do have
safe points implemented in the loops
using normal growl features so that we
have you know it gras inserts safe
points where they cannot figure out the
counted loops like to know if the thing
is going to end in a timely manner so
that there's a safe point so it the way
that it works is if I mute a CPU side
mutator fails to allocate and then a
safe point happens the flags are set so
that the wavefronts on the in the GPU
can see that so wave fronts that haven't
started yet will immediately abort and
they'll become never ends and then the
flags are set so that if any of them to
go into these
loops with safe points they'll also
notice the flag and they'll abort with
the BCI for that place in the loop which
is not an exception bc i it's just a
safe point so then it'll still go
through that same stub system and resume
the method on the cpu side at that b/ci
but it's not an exception it'll just
keep running normally so that's how safe
points are implemented in the kernels
right now and that keeps your GUI from
being you know frozen if you have
garbage collections trying to happen
while the colonel is running so there's
yeah we have a few x.x toggle switches
yeah I don't think you really need to
worry about that too much oh i guess i
ended up already talking about the
allocation oh and one more cheese man i
got one minute to go yeah we have
allocation i already talked about that i
guess you can look at the slides let me
see if there's anything interesting here
implemented reduce sorry we're running
out of time but um yeah it does a min
max and some it offloads it there you
know using this kind of syntax using the
integer colon colon some syntax I
haven't add enough diversion yet to
offload every the with streaming API
there's like five different ways you can
do stream dot saw more stream dot
reduced audit I haven't implemented
every single one of those so if you want
to experiment with it use this syntax
more about reduce its taken from bolt
yeah
live in John 9 or 10 we're not sure we
put the jet pin short java 9 will hope
it you know we'll hope that we'll make
it we're working hard on that and yeah
it's that's a lot of Oracle stuff that's
out of my control yeah let me skip ahead
of this yeah so the way so the reduce
was basically uses a more native GPU
algorithm to do the to do the
calculation where it takes code from an
AMD open source library called bolt then
so that was written an open CL and then
I basically used internal tools to
convert you know compile it from opencl
to h sale then i modified the h sale by
hand to make it compatible with Sumatra
and it uses group memory and group
barriers and all that fancy stuff to
coalesce the work into partial results
in group memory as it like goes from you
know your whole range and then it like
slowly works its way down towards one
final answer and then returns the one
final answer back to the CPU side
you
well I don't know what is the whole
range of what you can express in the
reduce API on the seat in the stream API
I mean I'm just bees are you know
explicitly supporting built-in functions
I don't remember I don't know now well
this is this here this is a sale so this
is this is the like sort of
cross-platform intermediate language for
the HSA architecture and that will get
made into a machine aissa and for AMD
APU I have actually never even laid eyes
on the ice ax of the AMD apu s right so
we just work at this level and then
there's a whole nother shader compiler
team that makes it into the thing that
actually runs on the on the device oh so
these here you mean this here so these
are actually objects I mean these are
offsets into the java objects that came
out of your program so these get built
by growl at compile time so it's
directly come from the from your actual
code right your your object has you know
this is a baseball thing right so it's
got like player named player at bats
player hits blah blah blah
oh no yeah no growl takes care of all
that right yeah so let me see reduce it
works it's good yeah it's pretty fast
yeah so here's our links this is so we
have you know project sumatra we have a
mailing list that you can see if you go
to our wiki and OpenJDK this sumatra dev
at OpenJDK calm and we it you know it'd
be great to get more example workloads
that we think we could make in two
Colonels anything that you think you can
provide to us so please join the list
and let us know what you think
if you have just four beers each p.m. is
running nfritz yes these strengths i'm
doing a reduced rate who manages the
allocation of the fixed number of the
yeah the the actual colonel job launch
will be resolved in the driver and the
OS so you wouldn't have to do anything
in your program the launch so the launch
of the Colonel's will probably be
serialized into the device by the driver
layer thanks for coming everybody</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>