<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Divide and Conquer: Efficient Java for the Multicore World | Coder Coacher - Coaching Coders</title><meta content="Divide and Conquer: Efficient Java for the Multicore World - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Divide and Conquer: Efficient Java for the Multicore World</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BsS9oJdEipU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone welcome to our session
today we are going to talk about how to
write efficient Java in the
ever-changing multi-core world so I am
Sunil Mallory and this is my friend and
colleague winter Asami
we both work as Java performance
engineers at Verisign so today we're
gonna talk about the problem statement
meaning we're gonna talk a little bit
about the current hardware trends and
how they force us to us the software
developers to change the way we write
programs and we're going to talk about
different concepts of concurrency and
parallelism and then Whaley's going to
talk about a four giant framework and
how its implemented in Java 7 and 8 we
do have some code examples and a small
demo towards the end so for years we had
it very easy so every so often they
would produce a faster processor and us
programmers without having to do
anything will run the same application
on a newer processor it will
automatically run faster but the free
lunch is kind of over because they don't
produce faster and faster chips anymore
instead they produce more cores per
processor so if you look at this this
graph it's slightly dated but it conveys
the message so this is a graph plotting
key metrics of the hardware evolution so
there is number of transistors then
there is clocks per clocks per second
cycles per second and then there is
power consumption and then there is
performance per clock so if you see
everything else kind of got they've
gotten flat towards mid and late 2000s
the only thing that's that's growing is
the transistors which basically
translates to the number of course you
get I mean these days even your mobile
phones have
quad-core processors so we got to
slightly think differently in how we
write our software so we have to try and
find ways to basically leverage this
abundance of resources to to basically
boost our application performance so to
give a little background we have we've
had single CPU systems so we were
resorting like Java always had threads
in the language and we were using
threading to basically run some of the
tasks synchronously and then in a way to
improve the end-user perceived
performance by hiding the latency why as
we were running some of the tasks are
synchronously then we got multiple CPUs
in this we started serving concurrent
user requests we we were using thread
pools and schedulers of of Java and
other languages so so we were trying to
leverage resources by serving concurrent
user requests but even serving
concurrent user requests is not going to
be enough because the rate at which the
the course per processor increasing and
increasingly available for very cheap
just serving concurrent requests is not
gonna be enough to completely release
the abundance of resources we have so we
need to have more finer grained
parallelism concurrency is also kind of
peril but we need to have serving
concurrent requests is also kind of
parallelism but it's very coarse-grained
so we need to have more finer grained
parallelism to take full advantage of
the resources so as I just slipped
earlier so concurrency parallelism and
multi-threading are used very loosely in
the in the in the software development
but I just wanted to give some clarity
on these terms
concurrency is not parallelism
multi-threading doesn't mean parallelism
alone so concurrency is is a condition
where
two or more threats of execution are
starting and getting executed over a
overlapping periods of time but they
need not get executed simultaneously on
the other hand parallelism is is when
multiple threads are getting actually
executed simultaneously multi-threading
is is kind of an umbrella term you could
you use multi-threading to achieve both
concurrency and parallelism in a way
concurrency and parallelism both solve
different kinds of problems
so with concurrency you are trying to
solve a problem where you have very
scarce CPU resources and you have lot of
problems to solve on the other hand in
parallelism you have plenty of CPU
resources and you're you are trying to
solve your finite number of problems so
we need to that's where so for the
longest time concurrency has dominated
the the efficiency discussion but as we
are getting more and more cores per CPU
parallelism is slowly coming into the
forefront and that's the focus of our
presentation so as I said we need to
find more sources for improving
parallelism in in our applications to
take advantage of advantage of the
resources so before we go deep into
parallelism aspects of the application I
just wanted to touch base on a couple of
other things that the industry is doing
to take advantage of the resources so
one of the very common thing that people
are doing today is server virtualization
I'm sure a lot of you have heard of
VMware and such software's where you
install on your big server and basically
it lets you create many number of
virtual machines on the same server so
if you take a multiple like a
multi-tiered architecture you could be
running your V or UI on on Windows your
server side on Linux and your database
could be running on
on ax you could have all of them running
on different virtual machines on the
same server so that's one way of taking
advantage of resources in its it's
pretty common also but our focus is more
on how we do this in software even in
software there are a couple of other
approaches that that people are using so
one of them is called message passing
concurrency so in this this is this is
implemented in languages like rolana and
Haskell so in this what they do is they
respond thousands of light-weighted
processes which executes smaller and
smaller tasks and they exchange messages
for for communication between the
between the processes so since there are
a lot of small processes they're going
to be executed in parallel and
eventually take advantage of all the all
the resources the other approach is
software transactional memory this is
not really new but it's it's also it
also exists so it's it is a very
optimistic approach in this what they do
is unlike like in a traditional in a
traditional software languages that we
reuse like Java so you when you're when
in a multi-threading environment you try
to protect the protect the code and
resources basically if if there is a
thread that's executing computing some
values you will try to lock that piece
of code and then everybody else is
waiting for that but in this what they
do is they all share they all have a
shared memory and the thread of
execution doesn't actually care about
what everybody else is doing so it has a
certain set of instruction that it has
to perform and it just executes those
those step those steps without caring
about what everybody else is doing the
only thing that they do is they keep a
log of all the reads and writes in while
they are ready is executing and towards
the end they validate if
if there is any change to the slice of
memory that they are using so when from
when they started and when they ended if
nobody else has accessed that memory
they commit if somebody has accident and
it's changed its state then they'll
abort this and likely execute all of
this so theoretically there is some
improvement with this also because there
is no locking obviously so there is no
waiting and lot of threads can execute
parallely and so theoretically there is
improvement but then in reality there is
some overhead with respect to all the
log that they have to maintain and the
validation itself and if if there are a
lot of collisions then they will have to
they'll end up doing lot of reworks so
so there is some overhead but I think in
what they say is if it's like two to
four processors this might not work out
very well but if you have a lot of lot
of course then you could try this but
this this on itself is is a big topic
and that that is not our focus so we'll
get back to our focus which is fine
grained parallelism so what is fine
grained parallelism in this you're
basically focusing on on the tasks
rather than the threads so you want to
take up a task and break break it down
into smaller and smaller subtasks and so
that all your threads can execute them
in parallel so once you have many many
tasks and you have a thread pool and
they all can be running simultaneously
you will automatically leverage the
resources the abundance of resources you
have obviously finer the granularity the
better the performance boost you will
get out of it but the functional
decomposition is key so you have to be
careful in what can be breaking down
into smaller pieces and how small can it
be picked broken down into so this is a
very generic view of how how things will
get done in in parallel so you basically
have a big queue of all your smaller and
smaller
asks and you have your thread pool and
each of thread will pick up a task as it
gets finished and keep executing their
tasks and at the end you will have a way
to basically sum up all the solutions of
all the smaller pieces and eventually
arrive at your final final solution so
as I said this is very generic and there
are some logistic issues such as what if
some threads are starving I mean one
thread is still executing its piece of
code and everybody else is idle because
they don't have work and each language
has its own measures when they implement
this on how they have solved that I mean
Java has something called work-stealing
if you go to hadoop it has something
called speculative execution being
Whaley is going to talk about work
stealing and similar concepts how it's
done in Java when we when we get to the
Java implementation so where do we look
for this fine grained parallelism so any
in the rule of thumb is like any CPU
intensive operation is is a good
candidate for for this especially all
you are sorting like summarizations
aggregations things like that are really
good obviously if you are dealing with a
lot of network related stuff there isn't
much you can do about it like you just
have to do them sequentially so if you
take an example of a database query
lifecycle right so you send a query and
then the database sees the query parses
it analyzes it and then probably it
figures that there are n different plans
that it can use to get that data and it
will analyze to see what is the best
plan to get this data and then it
actually goes to the disk and retrieves
all the all the data and then in the
post-processing you do if there is like
an order by then you will try to sort
all the data and if there is group by
you will try to aggregate all the data
so if that is your life cycle of a query
obviously anything related to network
it's not there's not much we can do so
going to the i1 coming back we would not
help much but you can definitely
paralyze the the planning and analysis
of the plan itself and especially the
the post-processing where you aggregate
and sort all your data the current day
databases mostly do all of that I'm like
Oracle my skill they do they all do that
very efficiently like if you work with
Oracle or of my skill there will be a
parameter which says like how many
parallel threads you want to execute
which is what it correlates to so
obviously
so obviously parallelism we want to look
for more parallelism and it's all good
it's going to improve my performance but
what is what are the limitations so as
we talked about if we if we have a
application and if if we paralyze the
application doesn't mean that if we keep
throwing multiple cores at it it keeps
increasing like the performance keeps
getting better and better
there isn't like a ceiling for that so
the ceiling of that is actually defined
by the amount of work that is not
parallelizable
so if you take an example of a work
which which takes 20 hours to finish in
a in a single threaded system and if you
can paralyze like 19 hours of it and one
of one hour of that work if that of of
the 20 hours if it if that can be run
only sequentially then no matter how
many thousands of processors you throw
at the application it is still going to
take at least one hour so you can only
improve by so much so in this case
irrespective of any number of processes
you throw processors you throw at it
it's going to take at least one hour
which is at the most 20 times
performance so that's what this and also
law states that your speed-up of the
application in a parallel computing
world is limited by the amount of
sequential work that you would have in
the in the application so that's that's
the formula that gets used so n
represents the number of course and P
represents the amount of parallel work
that can be done in amount of work that
in your application that can be done in
parallel obviously 1 minus P represents
the sequential flow so as the n gets
bigger and bigger like as you throw more
and more course that becomes 0 so it's
directly proportional to the amount of
work that is sequential or universally
proportional to so fine grained
parallelism is good so we know the
ceiling so what are other challenges
that we have with fine grained
parallelism as I said proper
decomposition of the task into smaller
and smaller tasks is key and it we have
to do it carefully and how can we
properly scale this parallelism I mean
can we extend this to multiple systems
as per and all slab there is only
certain even in reality I mean there is
only certain portion of your application
that can be parallelized
so we have to try to maximize that that
amount of work and we obviously need new
techniques and frameworks I mean since
you're all Java developers we are used
to Java language giving us everything
free done so we just want to use the
methods and libraries so which now is
actually done in even in this case so
well he talks about half it's all
implemented in Java so before I hand it
over to will I will just touch upon the
basic algorithm that gets used in all of
this so it's called as our title
suggests it's called divide and conquer
it's it's not anything new to new to us
and it's not it's been there even before
computers itself very day to day example
is how your mail gets delivered to you
like on a busy Christmas holiday period
USPS gets
deliver like 130 million males so how do
they do it it's such a huge problem so
as I said divide and conquer so they go
by divided by city and then by probably
by zip code and then maybe by street
name and then by like house numbers so
like you keep dividing the problem into
smaller and smaller numbers recursively
and you you reach a point where the
problem is independent and and small
enough that you can quickly run those
things so that's the idea you want to
recursively break down into smaller
pieces execute all of them in parallel
so that you can utilize all your
resources and then combine the solutions
of your subproblems to achieve your
final solution so Bailey is going to
talk about how this is actually
implemented in Java using an framework
thank you so Lily good afternoon
everyone
so now Sunil has given the theoretical
background of the problem statement we
are trying to address here let's see
some examples how how many people are
you already using JDK 7 yeah so I think
we are in luck
so JDK 7 introduced this excellent
framework called for giant framework
that brings us some ability to express a
parallel algorithm you can make the
problem parallel you can build a
parallelizable algorithm you can express
it as part of JDK and JDK 8 is going to
get even more exciting I'll talk about
that later so let's start with foregin
framework he shouldn't talked about
divide and conquer algorithm for join
framework is nothing but a
version of divide and conquer so if you
look at the steps it's the same you
break down the problems but when you
break the the subproblems they're all
getting solved in parallel that's the
key thing here so I have the pseudocode
here it takes this general form that's
that's the idea basically this framework
implements this you take a problem and
you check the problem size the problem
size is less than a sequential threshold
so so let's take a step back for
breaking down a problem and solving in
parallel and then combining the results
to get to a solution takes some overhead
so if the problem size is really really
small then by doing that you are paying
actually a penalty so for every problem
and there is a sequential threshold you
have to be aware of and then it depends
on your problem then you have to tune it
so that's why the problem is given the
algorithm always looks at that the
threshold and compares that with the
size of the threshold if it is bigger
then it breaks down the problem and then
the key here is invoke in parallel
operation so what's the benefit right
so I here is a different representation
of the same concept you do some pre-work
you determine how many things to fork so
there are key to two benefits here you
are actually having a lot of resources
and you are putting them to use and your
speed up the way program is getting
faster I'll give you a small example
let's say that you know we have an array
of thousand elements and then you want
to do an operation on each of these
objects let's say that takes one second
and if if you do that traditionally you
iterate over the array it takes thousand
seconds and then you say you have ten
processors to do that
you can theoretically break the array
into ten sub arrays and give each thread
that part of that data so basically
you're partitioning your problem right
so that that's that's basically reduces
your wall clock time your application
performance improves and you are putting
the resources into use so that's that's
great
nowadays we generate humongous amount of
data everywhere the data processing is a
big thing right the big data everybody
is talking about big data so everywhere
you have a need to process some data you
could use this you could use the for
Jennifer Network in any kind of problem
that deals with Allah the Lord of amount
of data you could put this to use okay
so let's talk specifically about the
Java library Java supported
multi-threading since it was born from
first version right an until JDK 5 we
did not get an ability to control our
tasks so we got thread pool executors
and the the ability to define tasks and
manage the execution we did not get and
then that itself still gave you only a
coarse grained control with JDK 7
finally they included the excellent
library that was written by Doug Lee it
started with the jsr 166 effort and this
is available in Douglas website if you
are not on JDK 7 you can still use it
just download the package you should be
able to use it there are two key things
that you have to be aware of for using
this framework there's a class called
for giant pool nothing but an executor
service for running for giant tasks and
obviously you have to create a task
which is a four giant task different
styles of
are available okay
so to wrap it up executing a task in
this framework means that you create
more subtasks and then you invoke them
in parallel it gives you a portable way
from as part of JDK
let's see some code now here I have a
problem you're selecting a maximum
number from an array nothing fancy very
simple so you look at the code here
it's a sequential algorithm traditional
right right so we iterate through the
array find the maximum number let's see
how we implement the same problem to see
how we solve the same problem using the
for join framework so as for Gen
framework you have to create a four
giant task so here I am extending
recursive action if you extend recursive
action you have to override the compute
method and if you look at compute method
basically it's it it implements divide
and conquer algorithm so I'm checking
the problem size against the threshold
and you solve sequentially if the
problem is small or you create subtasks
you're forking they're creating breaking
the array into two left half and right
half and calling invoke all and then
that is the key thing when you do invoke
all if you have the number of course
available the JVM knows that it has to
be executed in parallel and that process
repeats recursively because you created
two more four giant tasks and then that
problem size is compared with the
sequential threshold it goes on and on
until all tasks complete and you find
the maximum number pretty simple right
so that's the idea you have another
example here
it's a cursive algorithm doing a merge
sort and the same example implemented
using for join so that you create a
whole giant ask to replace that the
recursive algorithm in the compute
method you do the fork enjoy so let me
show you how this is actually running
the I'm going to test it on my laptop
which has four CPUs for quad-core okay
so I have it here so this is the the
maximum problem that we saw say next
this is the sequential version we have
this for John right so have a tester
method here that runs a bunch of times
especially it's sending that let me give
an array of thousand elements let me run
it actually million million elements let
me run it ten times they specify at us
all to five hundred
this is this you do it yeah so I'm
getting about the speed up of to it it
ran pretty quickly but the sequential
method took point zero one seven seconds
whereas the for Jen method took like a
fraction like in point zero zero eight
seconds so in a four CP machine you get
a speed-up of two which is pretty
awesome so so that's what the research
says that you know they're they're the
the JVM designers goal is to give you at
least to speed up and that was their
passing criteria to implement this
feature so so you're cutting your
program running time into half which is
very very good right so you might be
wondering okay so now I have to change
the code
yes yes yes oh so that's oh yeah so you
cannot always expect linear speed-up
there there is some limitation the
overhead is there and that's what we
discussed about the Amdahl's law so
there is a sir in a given problem there
is a parallelizable portion and there is
a sequential portion that has to be done
so your speed-up is limited by that
amount of sequential work that has to be
done in the problem sure
yeah can you talk about that later like
after after finishing your presentation
maybe we can talk about that so you
might be wondering now right so let's
talk about how this is actually
implemented this is could be implemented
using basic threads are the thread pools
that we have in JDK 5 the conventional
thread pools in fact it is implemented
using like a thread pool executors but
it is optimized for executing for giant
tasks it implements a technique called
work-stealing
so so basically you have a limited
number of worker threads that is when
you create your for join pool to answer
your question you look at that number of
available processors and you create the
pool with that number of threads yes
yeah we
yes you create a pool with the number of
course that are available in your system
so that's the that's the recommendation
so in internally this is how the forms
and frameworks implemented so you have a
limited number of worker threads the
recommended number is the number of
course that are available in your system
and each thread maintains its own work
queue it's a private developer Q and the
owning thread always goes to the head
when when you are creating a new task
that is put in the head of the queue the
head can be accessed only by the warning
thread so when when the thread doesn't
have any work it goes to its what queue
and looks at the head of the queue takes
the task and executes it
if though if the thread is idle and its
own queue is empty and it chooses a
random or another worker thread it it
goes to the tail of that queue and gets
the work from there and execute it so
you solve the problem of thread
starvation the threads are not like
looking for work sitting idle and also
you avoid the performance penalty where
you collide for the work queue so that's
that's how this is there this is from
Begley's paper so this is a very
optimized implementation is
yes yes so we are getting there yeah
we're getting there yeah so right so
that's what I was saying so you may be
wondering you know why do we have to
change the core you know because the JVM
is actually managing the resources it
shouldn't be more transparent to me as a
developer can can I make it you know
just run this in parallel right so
that's what I was asking you JDK 8 is
going to be really really exciting in
terms of parallelization right there is
a better way so you don't have to like
really change the code to implement the
Fortran tasks we are getting there so
there is a data structure called
parallel array so by the name it the
name implies that it is a array it
maintains an array but it allows you to
do parallel operations on that data
structure so instead of using an array
you use this data structure and then you
can do things parallely and then you use
it as exactly as an array but based on
your runtime your program automatically
gets speed up which is pretty pretty
cool right so it's nothing but a giant
pool it maintains an array and you have
different versions to support different
data types it can be used from jdk 6
this download the different this is also
maintained in Begley's website you go
download it it's a different package
it's not a jsr 1 6 6 it's called the
extra extra 1 6 6 why you can download
it and use it so what they did was some
of this ideas they got implemented in
JDK 8 JDK 8 is introducing many new
concepts that are very similar to
provide this this kind of features into
the collections API which is going to be
really good so let me show you some code
here how do you use the parallel array
so you create a parallel array using for
giant pool right
so see how I'm creating the for example
so you you're checking with the runtime
looking for number of available
processors and it's a it supports the
filter operation so basically filter
operations by meaning that it has a list
of objects and with the specified
criteria I'm filtering certain objects
and you can do map you map certain
elements out of that object take it out
and then you can chain them like you
filter and you can map it then you can
do an aggregate operation right the
thing here is you don't have to worry
about how to make it run in parallel
based on how the resources available the
JVM is doing it for you so the maximum
problem that we talked about it could be
done in a single step so you do some
criteria and then you could basically
like like like a sequin kind of a
manipulation on your data set on the JVM
side which is pretty cool so this is the
idea of started with the parallel array
so when JDK 8 was discussed we there are
three features that we would like to
discuss in this presentation that are
very specific to parallelization that is
parallel utility methods that
java.util.arrays class it is getting a
lot of utility methods so that you don't
have to do any dramatic changes into
your design you instead of using this
method use the other method instead of
using this data structure use the other
data structure so that gives you the
ability to leverage the underlying
hardware and make your programs run
faster and there is a that the bulk data
operations on the collections they are
enabling it with the support of lambda
expressions we won't be able to like
cover everything in detail will only be
scratching the surface but there is
quite a bit of sessions here
are covering only the lambda expression
support so I would recommend you to
attend those things if you want to know
more about lambda so we'll talk about
what lambda is and then how that is key
to implement this feature called a Java
filter MapReduce and as I said this is
the first thing this is the very simple
addition that they have done in JDK 8
there is a Java it will array class it
has a sort method and set methods they
made all of them parallel now you can do
everything instead of calling sort this
called parallel sort and then the set
prefix method everything has a parallel
version now so let me show you how just
simple example
I'm creating an array here and using
arrays dot sort method and comparing
that with a parallel array sort method
or you can use the same arrays dot
parallel sort method this is available
in JIRA gate so let me run that
actually it's a problem I can actually
show you the previous run so it's
reporting I tried to sort with an array
with million elements this is the
speed-up I'm getting so which is pretty
good so you're getting like to speed up
in your program without a lot of
redesign that's a lot of benefit for you
and let's touch upon the lambda
expressions right so when when I showed
you the code for the parallel array
something's probably you know occur to
you right know that that style of coding
is a little bit different it's more
functional in nature so Java did not
have any support for functions and
there's been a lot of debate discussions
to include that into the language and
finally
jsr 335 made it to JDK 8 they approved
to include the support for closures
closure is nothing but there are a lot
of names for that the definition
official definition is it's an anonymous
function expression and in our opinion
it makes the code more concise better
readable and then it brings the
functional aspect to Java it's let me
show you some example right so here this
is how this code is not doing anything
this is just for illustration purposes
and the top portion is without lambda
and the bottom portion is with the
lambda support so it reduces a lot of
noise
and it focuses on what the code is
actually doing so we are creating a new
thread and if you're starting a new
thread to run and then that does
something and if you look at the bottom
portion of the code with lambda support
it's better readable right
yeah okay so that feature was key so
that having that support in the language
enabled you to provide this bulk data
operations on the collections API and
JDK 8 is introducing a concept called
streams to support this an analogy I
would give you is the you have UNIX
pipes right so you take an output from
one command and pipe it to the input as
pi PI you're piping it to another
commands input you can do similar things
but like with the stream interface so
that's what this thing shows you can do
all this operations where I showed you
the example of the parallel array you
can do the filter map reduce everything
and you don't have to really do that in
parallel or sequential altogether you
can control what and when can be done in
parallel right so you're getting a and
you have a list there you're getting a
parallel version of it and then you're
doing a filter then you're making it
sequential again so you can control wait
what run parallel so all you have to do
say is run it in parallel Jamie will run
it for you so to sum up the parallel
array example that I was showing you
there it becomes like this instead of
defining all those code it becomes like
this good so I will leave that up to you
which looks more readable so so that
that ends our JDK 8 features and before
we wrap up our presentation wanted to
talk about difference between the Forge
and framework in the MapReduce the
popular MapReduce framework right both
of them are parallel execution paradigms
and the advent of big data with the
advent of big data the MapReduce
frameworks got really popular one of
them is Hadoop so we wanted to contrast
between these two and there is an
excellent research done by University in
England you can read more details there
that's the link so so both of them give
you the ability to operate a high volume
of data but they have certain distinct
differences so depending upon what where
you are doing the processing if you're
doing it on a single node you go with
fork/join and MapReduce started with
distributed scaling in mind right so if
your data size is really really huge and
you have a lot of computing nodes you go
with MapReduce framework and there are
differences in how they divide tasks
whether they do it upfront or they can
do it dynamically or when that job fails
how do they deal with that and the
amount of speed up that you can expect
right so that's in a nutshell it's a
comparison between those two paradigms
with that let's recap
you know the key takeaways that we try
to communicate the dispersant ation our
old is getting increasingly multi-core
we need to find we need to make the
programs more parallel and wherever you
are dealing with data processing
operations you can you can make use of
it and for join comes in JDK 7 test it
out and even more exciting features
coming in JDK 8 to give you more
parallel options these are the
references we used thank you we will
take questions
that's the yeah if the work is CPU
intensive then only these things apply
if you if you are if you're running a
server that's waiting on clients doing
some network operations then you are not
getting much
if it doesn't so this
there isn't any
so all they say is it's the
recommendation right I mean eventually
you will have to try and test to try to
turn in your own application needs
especially with the with the sequential
threshold that he was talking about
right that also impacts the outcome a
lot so if if you so how how much smaller
you break down the piece into the
problem into will depend we will have
will influence the the amount of boost
of your Vonnegut so though some of these
things are trident like basically you
have to do it by trial and error but in
general the rule of thumb is to have as
many threads for at least the amount of
parallelizable work that you are doing
as as many course you have that's I
would start there that's the
recommendation right you give even evil
to the 4chan framework the same number
of threads as how many cores are
available to you that's correct
any other questions
yeah it's more CPU intensive where you
are doing a lot of data analysis where
that will be useful
if you're doing any aggregation
operations yes you put you could use it
but if you are just doing a OLTP
transaction like you are reading
something and writing something to
database I'm not sure if gives you
you could set the affinity but that's
the data processor level I don't I don't
think you can pin the thread to the CPU
you are good at the process level
which is why it is not linear so if she
was asking if you are having four course
I really would want to see the
performance boost up by four right but
that's not the case so there is some
overhead but the point is that you will
still get some performance boost it
might not be linear to the number of
cores you have but it's at least half of
it
that's that's the thank you thank you up
you have
we are saying if you write it yourself
it can be linear to the number of course
it is possible I mean so I guess yes
that yeah that's you could expect to
speed up by default and but there's
definitely there's a penalty that you're
paying for parallelizing this so you
will not be getting linear speed-up but
what you're talking about is that map
produced within the single node and that
is where the JDK 8 is going that they
are enabling all these features let you
know you can run all these operations in
parallel but because I would say that
you know if you can do it fits well and
good but most of us because that is
closer to you so if needed and this is a
generic framework but if you want to
design your own you could</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>