<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>クラウド基盤の機械学習プラットフォームをペタバイト規模で作成する | Coder Coacher - Coaching Coders</title><meta content="クラウド基盤の機械学習プラットフォームをペタバイト規模で作成する - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>クラウド基盤の機械学習プラットフォームをペタバイト規模で作成する</b></h2><h5 class="post__date">2017-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/unwBbOl5hnQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone my name is Alex Adachi
I am the director of data science for
the Oracle data cloud we're based in the
United States in Denver Colorado and
today I'm going to talk to you guys
about how we go about creating a cloud
based machine learning platform on lots
and lots of data I'm going to get into a
lot of details about that as well but
before we get into all the details I'd
like to talk a little bit about what we
do at the Oracle data cloud so the
Oracle data cloud our mission is pretty
simple what we want to do is unlock the
value of data to help different
advertisers connect more deeply with
their customers so we work entirely in
the advertising business and we have
lots of data we want to use that data to
help advertisers make better decisions
about who they're going to serve ads to
so what kind of data do we have um we
have data across the world as you can
see strongest in the United States not a
ton in Japan but we do have some and
it's all sorts of different data so we
have cookie data over five billion
information five billion different
cookies from different web browsers over
a billion different identifiers four
different mobile phones 15 million
different web sites 1.1 trillion
pageviews ton of web information and
also we have offline data so not only
what people are doing in the digital
world but what people are doing in the
store so especially in the United States
we actually track purchases of different
individuals and we see over 3 trillion
dollars every year of spend in different
stores from grocery stores to clothing
to other types of retail so we have a
lot of data what does this really kind
of look like what are we tracking we
track the full view of a customer so we
know what customers are buying and store
who they are and kind of what their
family makeup is what they're doing
online and also where they're going what
type of stores they're spending their
time at
are they going to baseball games all
sorts of different crazy amounts of
information so what does that look like
on the the individual level so this is
an example of what a customer might look
like in our database so in this case we
know someone kind of family status you
know that they're married we know that
they have a couple kids we might know
about their education we might know
changes we might know they just
graduated from a university we know it's
the first time they ever lived in a
house by themselves they're no longer
living with their parents we get all the
changes of information and the whole
idea today is that data is really
powerful and that this data is going to
drive all of our machine learning so we
know other things locations different
websites they might be going to
different activities they've taken on
websites recently and then we also have
all their purchase information as well
so we can see you know they might be
buying high-end women's apparel they
might drive a Honda they might have
children and they might be buying
children's clothes we get all that
information so now we have to use that
information to actually actually do
something for our customer and so in
order to kind of go from data to a
customer insight we need machine
learning so we start off with some sort
of action that a client or a customer is
looking for more of so a hotel might
come to us and they might say here are
1,000 people who have dated our hotel in
the last month we would like to do an
advertising campaign to reach 5 million
people online and what we want to do as
a hotel is reach the 5 million people
who are most likely to come to our hotel
in the next couple months so we're going
to use their example of their thousand
customers to try to build a model to
predict who their next customers are
going to be so we're starting with that
action that what the client wants more
of and we're going to take that and
we're going to combined it with all the
data that I just showed you so all the
consumer data and we're going to try to
put those together to figure out how can
we go from consumer data to client
action in order to do that we use
machine learning so at this point we
have
a machine learning algorithm and I'll go
into detail a little bit about how that
works and we're applying that across all
of our data to try to figure out what
are the patterns of a consumer what are
the different ways that we can help
predict who's going to be a traveler or
someone who would buy a room at that
hotel versus someone who wouldn't so we
put all of that data together and that
creates a model and now that we have
that model trained we can apply it
across the entire United States the
entire world we can apply it across all
of our different data sets to see who's
going to be a future potential customer
and we actually rank every individual
across the US or across the world based
upon what's their probability to be a
future customer and then finally we have
to actually reach those customers and so
the last part of our journey is we
target those audiences across the
digital landscape this is working with
partners like Facebook Twitter snapchat
and various different web sites as well
this is kind of what we do we start with
a client question and we move all the
way to actually targeting that
individual on a social media or website
platform we do this entirely on a stable
self built machine learning platform
that we've built in-house at the Oracle
data cloud every month we have over
10,000 different models that are running
each model on terabytes and terabytes of
data and accuracy is really important to
us our clients are going to measure how
well we do based upon how much they sell
so we really have to worry about having
the best-in-class
machine learning accuracy as well so
that's kind of what I do but today's
talk is going to be about how you can do
something like that how when you're
thinking about coding up a machine
learning platform what you can do to
kind of improve how you might write it
and so today I want to talk about as
developers how can you create a machine
learning platform that's going to use as
much data as possible it's going to use
a data-driven machine learning process
and ultimately is going to scale large
enough that you can reach various
business needs
so here's the agenda for today first
we're going to talk about how we let the
data kind of tell the story as we model
we're going to talk about using cloud
and how we not only use cloud but use it
effectively and then finally we're going
to put everything I talked about
together and talk about how we can
create a platform that goes from one
side to the other to use lots of data in
a machine-learning fashion so let's talk
about how do we let data tell the story
first of all let's focus a little on
data as I said at the start data is
going to be the most valuable part of
our machine learning process it doesn't
matter how good your model is if your
data is not good it's not going to work
and so it's our most precious at that
but often we have way too much data for
a single computer so we have to kind of
adapt to that and so we use technologies
like Hadoop to do that so we can create
these large-scale distributed data sets
from different data that we have stored
in different areas or cloud object
stores Hadoop is going to kind of power
the abilities that we take data from one
machine and put it on many so that
there's no limit to how much data we can
use so now you have all this data but
you have to actually do something with
it
and the easiest way to do that
especially for data science developer is
often using high hive is really nice
because you can access tons of data in a
distributed fashion just using sequel
something that you know many developers
know how to use so you can have hundreds
of computers processing data which is
simple sequel statement so that's kind
of a brief touch on the data side but
now that data figured out we have to
actually model it and so here is an
example of some data it is on the y-axis
height and on the x-axis age and you'll
see in this current kind of format it
goes from being a newborn to being ten
years old
so here's the wrong way to model data
you might look at this data and say oh
it kind of looks like a line so I'm
going to try to just pick a modeling
process so I'm going to say I want to
use linear regression for example and
then I'm going to apply it and what we
is in this case it actually works out
pretty well the data is pretty much
linear so we're good so so why is it the
wrong way so here's the same type of
data height verse age but instead of
going from newborn to 10 years old its
newborn they grow for a while they kind
of stop growing and then as they get
older they kind of shrink down a little
bit again so it's the same sort of data
but it's a little bit different so if we
took our example of oh this is height
and age data let's take a model and
apply it and we apply our linear
regression it's not that good at model
linear regression would say that we keep
growing forever so what we really want
is a model that kind of looks like this
and this is similar to the talk from
data robot this morning that you know
it's not one model that is always right
you have to try multiple models at the
same time and you want to automate it so
this is actually called the no free
lunch theorem and it states that anytime
you're modeling any performance that you
get on one type of problems which is
good it's going to be offset by some
sort of data or some sort of problem
where that performance isn't going to be
as good so what can we do about it so
what we can do is similar to the data
robot approach try multiple models with
different training data and pick the
model or combination that works the best
with our machine learning and this
sounds really complicated but actually
with modern technique we can do this in
under 50 lines of code that's I'm going
to show right now
make that a little bigger here so here's
my example of some codes to solve the no
free lunch theorem this is all going to
be Python code let me see if I can get
my mouse
there we go so right now I'm just
reading some data and we don't know what
kind of data it is we're going to not
really worry about that we're just going
to try multiple different approaches
rather than trying to make decisions
upfront so we read in some of that data
and then we're going to make a simple
function that can benchmark how long it
takes for a model to run so here's the
interesting part we're applying multiple
different types of model everything from
logistic regression to figure out data 0
1 2 different in sambala proaches so
random forest gradient boosting other
types of boosting applying 5 different
types of modeling here and we're just
passing it through that benchmark and so
really quickly we get this table and
this table super-powerful for us so
often in modeling you'll look at an a
you see curve to figure out how well a
model did and if we went with for
example logistic regression we'd
actually be doing worse than random here
here's our random dummy classifier and
so what we see here is that multiple
different modeling approaches worked in
different ways and so this is just one
type of data if we tried another type of
data these might all be different so the
idea here is whenever you're modeling
apply multiple modeling approaches and
don't do it by hand find a way to
automate it with computers and this is
obviously a simple example but the idea
is build upon ideas like this when
you're considering how you're going to
do your modeling so you may have noticed
that everything I just showed you was in
Python Python is a really great language
for data science and machine learning
and the reasons are for machine learning
Python lets us write code very quickly
and it's minimal amounts of code 50
lines we were able to do a lot
that codes very easy to read and it can
also be interfaced with other languages
in software that might be faster so as
you tell in the last example I'm using a
library called scikit-learn and that
gives us access to hundreds of pre-built
machine learning packages without having
to reinvent them every time
but python isn't always perfect there's
some cons as well so Python
out-of-the-box doesn't work great across
multiple machines it's very limited to a
single machine computation so for big
data out-of-the-box Python doesn't
really work that great by itself so we
have to go a little bit further so how
can we fix this you can combine Python
with SPARC
and a package called PI SPARC to get the
best of both worlds so SPARC is this
distributed in memory big data
processing engine that's powered behind
the scenes by a graph and the idea is
that PI SPARC
is an easy way to interface with SPARC
and so you get the simplicity of using
Python api's with all of the multi
machine processing that you get with
SPARC so here's an example of about
three to five lines of code of how you
can just do a simple word count exercise
using pi SPARC but basically in five
lines of code you could read in almost
infinite amounts of data here and
process it on hundreds or thousands of
machines you don't have to worry about
networking you don't have to worry about
anything complex it's all contained
through SPARC and through the simplicity
of having Python as an API so SPARC is
super useful in the machine learning
world but often you know we might want
to do deep learning people our needs
becoming more and more popular so so how
can we work with that so similarly SPARC
gave us kind of the power to do
distributed processing very easily
tensorflow can give us the power to do
deep learning extremely quickly even
with a Python front-end and so
tensorflow you can work with pythons and
you again have the simplicity of using a
Python API on top of a C or C++ engine
that's very fast this now gives you both
distributed processing there's great
distributed work inside sensor flow to
get multiple computers and there's also
GPU processing made easy again through
Python so here is 21 lines of code it's
a combination of Python and tensorflow I
think I have a little laser so up here
we're actually loading some data from
scikit-learn this is the classic iris
dataset for flowers we're loading up
that data and we're just splitting the
data right here is our deep learning so
we have about six lines where we're
using tensor flow to set up a network
it's going to be a binary classifier and
we have different hidden layers of
different amounts of neurons inside it
so again with five or six lines we're
able to make a neural network train it
on data and apply it so again very
simple building blocks to start building
up kind of the rest of your code so now
that we kind of have those pieces
together we're going to focus a little
bit on how do we create effective cloud
infrastructure we kind of have the
building blocks of code put together now
we want to talk about how how do we put
them in the cloud so first of all um
everybody is talking about cloud but I
don't think a lot of times people think
why are we using the cloud so so for
data science I think one of the coolest
benefits of the cloud is that it enables
you to do a lot of different things
because you can choose which computers
you want to run things on let's look at
the technologies that we've talked about
so far so we talked about scikit-learn
second learn is going to be very
dependent on the CPU for processing
tensorflow is going to use the cpu but
tensorflow can also use the GPU and so
for tensorflow we probably want these
big like multiple GPU machines to work
on spark spark is really focused at big
data
spark usually uses a lot of memory and
because it's
distributed you need a lot of fast
networking in between it as well
and again it needs the CPU we have kind
of different use cases for different
languages so basically data science is
changing all the time if I was to give
this talk ten years from now all this
would be different and so by using the
cloud we have this kind of adaptive
world where we can change our machines
and change our infrastructure to match
whatever kind of tests that we're
working on so the cloud lets us keep our
infrastructure current and fit for
whatever problem specifically we're
trying to solve but the more you more
you use cloud if you're going through
kind of a console access it gets
complicated and it's hard to track you
have all these machines and databases
and networks that you're setting up so
we need a way to kind of control all of
that in order for our business to scale
and in order for us to build products
that can we can repeat multiple times so
how can we do that so there's a very
cool type of technology called Hoshi
Corp terraform and what terraform allows
us to do is think of our infrastructure
not as setting up individual machines by
hand but we can actually think about
setting up our infrastructure as code
itself so here's an example of that
actually two examples unless is how you
can set up code for Amazon Web Services
and on the right is how you could set up
similar color code for Oracle bare-metal
cloud what you'll see is even though
they're different both of them are going
to accomplish the same sort of thing you
have an idea of a provider which is
telling terraform this is what my cloud
setup is going to be underneath that we
have a resource which in this case is
our networking resource it has different
names between Oracle bare-metal clouds
and AWS at the end of the day we can set
up all of our network routes all of our
private and public subnets everything we
need to worry about all in code and then
finally we have AWS instances or bare
metal cloud instances and here we can
actually set up the different machines
that we're going to use so let's say we
have 150 different machines rather than
bringing those up and down all by
and we can code up how all those
machines exist and which ones can talk
to one another and so this gives us
something else that's going to be kind
of cool so so after you've code up all
that stuff to bring up your entire cloud
to bring up hundreds of machines and
databases in network it's just one
command you type terraform apply hit
enter and in 30 minutes you have your
entire cloud set up let's say that you
made a big change or for whatever reason
you don't want that entire big cloud
anymore often you know I see his
company's scale you almost forget you
have different resources out there
running and you might have a database it
hasn't been turned off in two years you
have some computers that you forgot what
they're for with terraform terraformed
destroyed we'll get rid of that entire
cloud setup as well so as easy as it is
to bring up you can also take it down so
there's something really cool that comes
from that as well and it's the idea of
setting up multiple environments so you
have all of your infrastructure stored
here it's code and for instance you
might want something unique to
developers so you can use that code type
terraform apply and get your entire test
or development platform and this might
be where you're testing new code you
might be testing new approaches anything
you want because it's so easy to set up
you can just bring up this test
environment and if you want every
developer on your team can have their
own environment to test different things
so after tests you might want to bring
something towards production but maybe
you don't want to release it live right
away if you're not a hundred percent
sure it'll work and so you can actually
create staging environments or different
environments that will a 100% mirror
what your production environment looks
like because it's code and because it's
easy you're just pointing it at a new
environment and then ultimately that
same code can help you create your
production environment by using things
like terraform
when you're scaling up your kind of
developer processes you can go from
development to stage to production just
with terraform apply in very minor
modifications obviously as well to code
is so great because you can version
your cloud and so you can actually go
back and check all of your code into
github or gitlab or whatever your sort
of code repository is and you can say
what what kind of infrastructure was I
running last year or this product used
to work two months ago it doesn't work
anymore let's see if we change the cloud
or changed any machines underneath the
product you can do that all because
everything's in code and everything's
been version and along the way so
terraform becomes really powerful so
this kind of gets to to another point so
we have our infrastructure set up but
now on that infrastructure we want to
set up software and following the same
sort of practices I'm not a fan for ever
having to type in redo things multiple
times I always want to figure out what's
the most efficient way we can do it so
how can we package up our different
software in the different code that we
have and so the answer that we use for
that is the idea of containers so as
you've seen doctors I've already seen in
a couple talks here today machine
learning code everything that we run we
run it in docker containers and if
you're not familiar doctor containers
give us this resource isolation and
allocation just like a virtual machine
so we have in this sense what we can say
in machine learning going back to ideas
that things are changing all the time
maybe we have a process that needs lots
of CPU and we have another process that
needs lots of memory we can efficiently
run those in separate containers for
each of those containers one might say
give me all the CPUs the other one says
give me all the memory we can run those
on the same computer and what's really
nice about that is we get efficient use
of our cloud as well all for free
through docker
additionally we can kind of abstract the
operating system and test all these
different sort of configurations we
might have on the same computer so I'm
sure we all have processes where they
only work with one specific version of
software or we have someone on our team
who is always coding in some other
language or always trying to use a
different version of Linux all of that
sort of stuff
and so what's nice about docker
containers is that code can actually
live alongside its infrastructure so you
can get everything living together all
inside the container so for us that's
extremely powerful and extremely useful
so we also get something else for free
when we use docker so there's various
different types of technology to kind of
deploy all of these containers for
example here we're actually using the
docker kind of technology itself swarm
there's other conditions other container
technology makes those different types
of ways to deploy containers but the big
idea is because containers can stand
alone you don't care what type of
machine they're running on so in this
case our kind of containers are all
running on Ubuntu boxes but it could be
other types of Linux's it doesn't matter
because inside the container that
contains everything needed for code to
run and so it's really cool here is
we're actually running containers and
we're running it across three different
machines in two parts of the United
States so we could be running it on the
East Coast in the United States we could
be running it on the west coast in the
United States let's say a data center
goes away easily
we're running things in multiple places
so we kind of have that redundancy built
in but sometimes what we'll see is we'll
get an entire node failure so often when
you're using the cloud computers are
going to fail and so you might lose a
computer and what's cool about docker is
rerunning tasks becomes really simple so
if you have a node failure as we see
here using different docker swarm or
meso sort of all these other tools out
there they can help figure out these
containers we're on the failed node
let's put them somewhere else and so all
the containers that failed and stopped
running because our computer broke we
can move them to other nodes for free
and so it gives us is kind of free
failure replication which is awesome so
now that we've started to put everything
together we need to start thinking how
are we going to construct an
the end platform before we get to that I
want to dive in a little bit on the data
science side so machine learning you
know when you talk about a single black
box or gray box it's not always one box
there's actually multiple step so we see
here we might have data coming from
multiple different sources and then two
things happen after that we take our
data we might try to figure out what
types of IDs do we have in that data
what are we kind of joining all the data
on so we might have purchase information
and website information and the one
common thing between it might be the
name of an individual so you have to
join that information together after we
join all that information together now
we might need to create variable and so
variable creation is when we go from raw
data to more featured data so it's not
only how many people are purchasing a
specific item but it's what's their
acceleration in purchasing that item so
you might have got a new job and you're
making more money and you started buying
nicer clothes more often things like
that so we're creating all these
different variables here often though
sometimes you have so many variables it
makes it difficult to work with if
that's what variable reduction comes in
and this is a very data-driven approach
where you can take your data look at
your problem at hand and say for this
specific problem here's the data that
seems to work well with it and here's
the data that doesn't seem to work as
well and then after that we have all of
our models here so as I mentioned we
want to use multiple types of modeling
approaches not just one and you can
actually see I have three here but you
could have hundreds or thousands and you
want them to all run in parallel and at
the same time you're doing all of this
sort of stuff this is how you train your
model this is how you might want to
apply it to the real world and some of
this stuff can be done in parallel and
so you don't want to just do this from
one end to the other you want to do as
much as you can at the same time to have
your model complete in the shortest
amount of time so this gets really
complicated and different pieces might
fail there's a lot of areas
I want to retry different pieces you
need something to kind of orchestrate
this together so that's where Luigi
comes in
so Luigi is a pipeline tool for workflow
management and if you're familiar with
Mario and Luigi right they are plumbers
and so Luigi is going to help us manage
our pipeline this is really similar to
the make utility in Linux so if any of
you have ever used make with C++ or
other processes Luigi is going to do
some similar stuff for us we essentially
in data science have different tasks
modeling building data all these sorts
of things that have different
dependencies on them for example you
might not be able to build your data if
you haven't rotated your log last night
different things like that
so what Luigi does is sets up all these
dependencies and it makes sure they're
met and the way that it does this is it
has an idea of every task in your
pipeline so in that graph I just showed
Luigi kind of wraps around every
individual task there and it says this
is the order I want them to do it in and
actually similarly to spark that creates
a graph and then can execute accordingly
so what's cool about Luigi is you don't
have to think about these pieces need to
run in parallel these ones don't all
that you have to say is that in order to
have a model I need to have data in
order to have data I need to rotate logs
and Luigi will figure out what's the
right order to run these can they run in
parallel can they run in multiple
threads and so it puts everything that
we have together so now let's let's kind
of talk about how can we put all of this
technology there's a lot of stuff I
talked about today together in one big
platform so here's an example we're
going to start at the bottom here so we
have two ways of coming into this
machine learning pipeline we're going to
start here at the user so this user
could be a developer who's using
different API
it could be a user hitting a website
doesn't matter which way that users
coming in because we've created an API
or a rest gateway but that user can hit
and so they might say I have a new model
I want to run it's going to be for
hotels here's the data I need all that
message can kind of come in from an end
user or an end programmer and all that
data is going to be stored into a queue
now additionally sometimes you have
processes that don't need users so maybe
you want to run a model every single
week for a specific reason you always
are advertising one product and every
week you want to refresh who you're
advertising that product to because data
changes that's kind of the other way
here traditionally this would be cron in
AWS this could be something like lambda
additionally on mesas there's different
structuring things for running the same
code but essentially you have some sort
of way to bring in data into your queue
every week so both of these kind of
processes are going to the same sort of
queue then at that point you have your
workflow and the workflow remember is
going to figure out when I get a message
how do I figure out what needs to be run
so it's kind of at the center of all
this figuring out these are the pieces
we need and what's cool about the
workflow the workflow will actually
store all the status of all the jobs
running in some relational database
stores like a my sequel you can have
kind of insight into what's going on
inside your black box as jobs run what
stages they're in etc additionally
whatever you store it in the queue you
could might if it's JSON or something
like that you might want to store it in
a no sequel object so you have this
auto-scaling workflow and it's going to
be kind of conducting our machine
learning data our machine learning
process so again all of our data we have
it all in a block store and so because
we're on the cloud that's available to
the workflow it's available to our
containers and it's available to kind of
aren't processing so our auto-scaling
workflow might run some containers here
and then eventually that might go to
spark and hive for more computation so
how does this actually connect with with
everything I've talked about today
so all of this right here can be set up
using a form of terraform you can set up
this entire environment all in code and
be able to kind of bring up this whole
pipeline after you have that Hadoop is
going to be powering all of our big data
so we have our block store in our cloud
but we want to use to do whenever we're
going to access some of that data so you
might have a local cluster we might be
accessing our block store in a Hadoop
like fashion but Hadoop is going to kind
of power most of that Luigi so I talked
about the workflow dear Luigi is one
example of many different workflow
engines one of the other largest ones is
airflow by Airbnb it's going to be kind
of powering this aspect here of keeping
our work flow together then as I
mentioned we want to run all of our code
containerized so this kind of little
village of containers here you can power
with docker and inside those containers
we can run whatever we want so for
example as I talked about today we could
be maybe running scikit-learn for some
of our jobs we might be running some
deep modeling as well so some deep
learning so you might be using
tensorflow
and then additionally sometimes we have
bigger data and we have different tasks
for that and so we can always use hive
as well to process and query our data
either to bring into the containers or
to do some computation itself we also
have spark so that we can use spark as
well to process our data and so we put
all this together and it's a lot of
different technologies but all the
technologies are going to make our lives
a lot simpler so we end up getting is a
very flexible platform so everything
that I have here these are all loosely
coupled systems and so what's really
cool about that and this is similar to
the keynote this morning we're not
building one big monolithic project we
have multiple loosely coupled systems so
if we wanted to change your queueing
technology we can do that we want to
change out any way in how we model it's
all stored in containers so we can swap
those out it will all of this can be
kind of changed at
it's super scalable so it's scalable for
data because we're using the cloud and
because we're using Hadoop and spark and
hive we can scale up to any number of
machines to process our data and it's
also scalable for compute so we can
always run more containers or we can
make a bigger hive cluster or a bigger
spark cluster or for running Luigi and
it's kind of getting overpowered we can
actually run more and more machines of
different types to power Luigi as well
and ultimately all these things kind of
make it robust so if we look at it it's
containerized and it's failsafe and so
all of these services because they're so
loosely coupled and because we have kind
of terraform behind it as well it's
really easy if something starts to fail
to bring up something new or replicate
the entire system it's kind of bringing
it all together here and some lessons
learned of how you can create a machine
learning platform at an extremely large
scale so the first one is you want to
use a scalable data infrastructure to
make use of all of your data data again
is going to be the most powerful part of
any machine learning library and so you
want to make sure that you're not
throwing away data or you're not
sampling data too aggressively for your
problem
so you want to make sure focus on the
data let the data drive all of your
decisions and that's kind of the second
point here you know data isn't driving
just your infrastructure data should
also drive your machine learning and so
don't say that you're a company that
just uses linear models or that just
uses random forests you want to try
multiple different models and multiple
different configurations of those models
so you really want to let the data drive
your machine learning decisions as well
and then finally in order to use all of
this data in order to use all this
compute in order to use all these
different ways to process data you want
to focus on using the cloud as much as
possible and when you use the cloud you
want to thank how do I create a flexible
a flexible
platform where I can change out
different pieces a scalable platform
that will work for running one model ten
model hundreds of models thousands of
models and ultimately a robust platform
you have to assume that almost every
piece of your platform is going to fail
at some point so how can you make sure
that it's going to be redundant
during failures and if you put all those
things together takes a lot of work but
a lot of the information and a lot of
the software can really help you put
that together to do all of this and
these are just some examples of some of
the really cool tools out there to build
a cloud machine learning platform thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>