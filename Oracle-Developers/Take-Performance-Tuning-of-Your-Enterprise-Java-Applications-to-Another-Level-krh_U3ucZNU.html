<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Take Performance Tuning of Your Enterprise Java Applications to Another Level | Coder Coacher - Coaching Coders</title><meta content="Take Performance Tuning of Your Enterprise Java Applications to Another Level - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Take Performance Tuning of Your Enterprise Java Applications to Another Level</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/krh_U3ucZNU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi good afternoon my name is Marty it's
wits
I'm the project lead for the Oracle
Solaris studio performance tools and I'm
going to talk today with the help of
deep sing taking your performance tuning
of your Java Enterprise application to
the next level
so first I'm going to is an outline of
what I want to talk about why
performance matters I'll give a couple
of examples of what I call low-hanging
fruit really simple cases of
understanding how the performance tools
can measure and show the performance
issues I'll then give a somewhat
detailed talk on Java profiling and then
talk about the issues for profiling
enterprise class applications and
finally deep we'll talk about profiling
Fusion Middleware so the first question
is why does performance matter it
matters because faster code leads to
greater productivity and lower cost it
matters because better performance leads
to faster results which implies a lower
cost my favorite reason is that
performance insights can be interesting
and surprising and the reason for that
is that most people code based on their
understanding of how a system will
behave and the performance problems
arise from disconnects between what you
think will happen and what really
happens another you can also do
something about performance in your
applications you can identify the
problems and it's actually easier than
you think with the tools you can do
simple runs and automated runs to do
performance measurement so what are the
criteria for performance measurement
well it takes too long to finish it
responds too slowly it can't handle the
required load that is you're doing an
application that has that's dealing with
some kind of traffic and if it can't
handle the traffic you expect it matters
it may consume too many resources in
order to handle the load when you find
there's a problem you have to understand
is this worth fixing you have to weigh
the cost of doing the fix versus the
aggregate cost of the problem if it
takes six months to do something that'll
save
shave one percent off of user responses
it's probably not worth it on the other
hand if it takes a day to save twenty
twenty percent off user response time
it's worth a lot and one rule is that
most untuned codes have low hanging
fruit even some codes that have been
worked on in tune have low hanging fruit
that is obvious things you can change to
make the problem to solve a problem if
you think there's a problem you have to
find it and triage it in order to do
that you have to understand how to do
repeatable performance runs runs that
are done at at realistic data and scale
that is it's kind of silly to do
performance measurements on a tiny test
case rather than on a case that really
mimics what you care about in
performance in production you can do
lots of ways of actually measuring the
performance you can use bin time you can
use a stopwatch you can just look at the
wall clock and those will often reveal
serious problems most problems are
really skin or many problems or really
scaling problems if you have a tiny
problem with trying to build a list of
three items it kind of doesn't matter
what algorithm you use if you're trying
to build a list of a hundred thousand
items it really matters most problems
have some intrinsic scale factor some in
and in order to understand the
performance costs you should try and do
measurements at with different runs
let's say small medium and large values
of N and compare the times is it linear
and n does it run as log of n which is
actually super does it run as N squared
or n to an even higher power those are
not very good if you find in those
measurements that there is a performance
problem you need to get detailed data in
order to isolate it and understand how
to go about fixing it so we recommend
our tools obviously to do that the
Oracle Solaris studio performance
analyzer is the performance tool that
comes with Oracle Solaris studio its
advantages are that it works
on production codes and production runs
you don't have to recompile with special
Flags to do performance measurement you
just run it as you would for production
it can deal with doing performance
measurements overruns that may take as
few as little as 10 seconds or runs that
may take up to several hours it will
deal with processes that are running on
many CPUs many processes many threads
etc and really even really large
programs the tools will measure real
production data real production behavior
that is you're not trying to use an
application compiled to do measurement
as opposed to compile to run the way
it's really running that means you can
do fully optimized and parallelized
applications if you're running in Java
which is mostly what this talk is about
hotspot can be enabled and we deal
perfectly well with the hotspot compiler
coming in compiling a method in
generating machine code for it which
makes it much more efficient the tools
typically have minimum dilation and
distortion by dilation I mean the
difference between running the code
regularly and running it to do the
detailed performance measurements
typically we find applications have a
dilation of perhaps 5% a bit more maybe
up to 10% for Java there's also an issue
of distortion which is where the
measurement changes the relative
importance of different parts of your
program that's in a sense more serious
because it can lead you to look at the
wrong thing but our tools have
relatively little distortion the tools
support codes that are either compiled
with the Oracle Solaris Studio compilers
or with the new compilers and they run
on both Oracle Solaris and on Linux on
Solaris they can actually also be used
to profile the kernel although I won't
talk more about that
all right well how did we design the
tools what were we trying to do with
them the objective was to make things as
simple as possible we want to show the
data in the model of the users
programming model that is show things
against the user source code how how
they user wrote the code as opposed to
how it's been transformed and how it
actually executes the tools can show
that some of the complex transformations
that compilers for native languages do
that is they sometimes will take a
function and inline it into another
function they may take part of a
function and outline it into a separate
function they may produce clones of a
function where you have a function that
has three arguments it may produce a
clone when the third argument is a
constant because you can generate better
code in that case lots of things like
that take place and the job of the tools
is to invert those transformations to
relate the measurements on the machine
code back to the user source code our
compilers will generate commentary about
what they did and that commentary is
shown when we show you annotated source
code of with performance measurement we
show you what the compiler said about it
like for example it will report lots of
things that happen with loop
transformations common sub-expression
elimination z' tail call optimization
and so forth the tools also will show
you what the JVM did that is it will
show interpreted methods as well as
hotspot compiled methods and it will
show you what happens with the garbage
collector and what happens with the
hotspot compiler the tools have a very
simple user model three steps first step
compile your target make a dot out is
the canonical way to do that the second
way to do it is to run command collect
you give it the name of the a dot out
and the arguments that it uses and
that's all it needs it will do all the
rest for you and the third step is to
look at the measured data the output of
collect is what we call an experiment
and there's either a GUI for browsing
the data in the experiment or a command
line which you can use for scripted
analysis of runs so when you're doing
Mary performance measurement you have
questions that you ask about what's
going on in the program the most
important one of course is what can I
change to improve the performance of
this code what resources are the compute
is the code using is it beating on cache
lines is it beating on CPUs is it IO
bound we're in the program are the
resources being used that is which part
which functions in your code are the
ones where all the time is being spent
and often it's very important to know
how the program got there often for
example you're doing a matrix multiply
it goes as the cube of the order of the
matrix do it for a small matrix very
small time do it for a large matrix much
much larger time so it's often important
to know which path you took to get to
the place where the time is being spent
there are a number of technologies that
we have for understanding that for doing
the performance measurement you can do
clock profiling of call stacks and that
works with Java code and native code and
mix Java native applications as I'll
show you later we can also do hardware
counter profiling of call stacks
hardware counters are a wonderful way to
get really detailed information about
memory performance for example you can
find out which parts of your program are
getting all the cache hits in the cache
misses you can find out how many
instructions are being run what the
floating-point operations in your code
are we also can in some cases support
tracing of specific events like
synchronization events in the code and
it's important to realize that the big
wins in performance management don't
usually come from optimizing the code
generator for a particular algorithm
they come from finding a better
algorithm to save save the same to solve
the same problem so I've talked about
why performance matters and now I'll go
through some simple examples
so there's a code that I wrote called
low fruit which really does measurements
of low-hanging fruit this is the first
screen you'll see if you bring up the
performance analyzer on one of those
codes and it shows it shows what the
measurements that were made with the
total thread time wind whether it was
CPU time or user CPU or some other
reason user lock for Java is usually
quite high but that's because the
garbage collector and the hot spot
compiler are waiting on a lock when they
just before deciding what to do
it is not a delay as in two threads
competing for the same resource with a
mutex lock it's how they do the
scheduling so this is the summary of
performance if you look in more detail
this is the function list if you if I go
back you can see there's this list of
views on the left the net the first one
is the overview the second one down is
called functions and that's what this
shows in particular I've selected two
lines one called bad in it and one
called good in it they represent two
functions to do the same thing namely to
initialize some tables you'll notice
that the exclusive time that is the time
spent in that function itself is
comparable for those two the difference
of 20 milliseconds is could be easily be
statistical fluctuation but you'll
notice the inclusive time is radically
different 24 seconds versus three and a
half seconds so that that's a good
example and now I'll show you why if I
look at the source of the bad
initialization you can see there's a
static initialization routine named that
way so you know that's what it is
that is inside a loop here and of that
those all that time that is spent the
inclusive time spent you can see that 23
seconds of it is spent inside this loop
and most of it is in this static
initialization routine the next slide
shows the picture
of the good initialization routine where
what we've done is move that static
initialization to be outside of the loop
now you can see it's only using two
seconds substantially less now you might
think this is really a dumb example who
would code it that way but in fact it's
not as silly as it sounds it's based on
a real example from my own development I
was writing a code that was presenting
information about the parallelization
that was done on an application and the
input was the source code and the
information about it and the output was
supposed to be a list of all the loops
in the program with a little icon next
to each one that says if it was parallel
or serial or if it couldn't be
parallelized and so forth so when I did
this there were two api's I could choose
to build a table with icons next to each
entry API one adds an icon to a list
item by far the easiest way to code it
API two as a vector of icons to the list
but to do that you have to build the
vector of icons and you need storage for
it and then you add it to the list what
I didn't realize is a time I wrote it is
that if you add one icon because the
icon may be bigger than the height of
the of the line you may have to change
the line height in the table and to do
that you have to scan every time you add
an icon you have to scan each entry to
see how high it is to get the right
height if Ike and since you have to scan
each entry upon adding an icon you have
an N squared algorithm if however you
add a vector you only have to scan the
whole table once which is an N out
algorithm of order n for the small test
case I was doing my development on you
couldn't see any difference but for a
large case where there was a thousand
loops in the list API one took 30
minutes
a pi/2 took 30 seconds 60 fold
improvement just by changing that and
the reason I use this example of
initialization is that the
initialization to compute the table
height is actually hidden inside the API
I chose and I didn't know what the
characteristic of that that
implementation was but by doing the
measurement I could see what it was
changed to the other algorithm and
suddenly I got an enormous improvement
the second example I'll give is building
an ordered list where you basically you
look at an entry you put it in the right
place in the list there's a good insert
and a bad insert in the good insert and
the bad insert you can see that it takes
about 23 seconds in that function plus
56 seconds elsewhere in the good
insertion it spends 90 milliseconds in
the function itself and 32 seconds
elsewhere
and again if you look at the source I'll
look at the source of the bad
initialization first you can see it has
a loop trying to figure out the right
place to do the insertion and you can
see there's a substantial amount of time
spent here in that search if I look at
the good insertion the time spent to
actually insert the numbers 32 seconds
32 seconds in this version the time
spent to do the insertion is the same
but the time spent to find where to do
the insertion is radically different and
this is the difference between doing a
binary search in a linear search in
consequential for small table size very
important for a large table size so I've
talked about this simple example and now
I'll go on to talk about some of the
details and intricacies of doing
measurements on Java programs you might
think that the Java is just a C++
program well that's not really true it
has many peculiarities
I'm sorry for the formatting of this
this is an OpenOffice presentation it
doesn't come out right the PDF would be
better but no matter see what it okay it
is not really a simple C++ program
because it dynamically constructs code
in its own data space that's where the
interpreter of Java code resides it it
in this example makes calls to J&amp;amp;I to
native code and the native code can call
back into java it dynamically compiles
the code with hotspot into the JVM s
data space an app a java application
often has many threads and it has both
user threads threads that are executing
what is straightforwardly the user Java
code and it also has threads that are
associated with the JVM system threads
to deal with garbage collection threads
to deal with hotspot compilation and so
forth and any instant in the program
there are really two call stacks that
matter
there's the native machine call stack
that shows what's really happening in
the execution of instructions in the
machine and there's a java call stack
which is maintained by the JVM runtime
and one of the things that the tool has
to do is to take these two call stacks
and reconcile them into one
representation of what's really
happening so you see Java calling native
code calling back into Java seamlessly
as if it was all written in the same
language another issue for Java
profiling is that memory management is
done through the garbage collector
rather than with malloc and free the way
native code does an optimization takes
place because the hotspot compiler will
dynamically take a method in bytecode
and transform it into native code which
executes is much more effective more
rapidly but the questions did you ask
about what's happening in the code
where's the time going how did it get to
the point in spending the time and so
forth all those are really the same but
they're made a bit more complicated by
the peculiarities of Java when we do
Java profiling
we have several different ways of
looking at the data we have three
different ways user mode expert mode and
machine mode in user mode we don't pay
any attention to any threads other than
the ones that are executing user Java we
represent a named method as the merge of
both the interpreted execution of that
method and the hotspot compiled
execution of that method because after
all all you know is a method it's the
compiler and the runtime that generated
multiple versions and if you ask for
disassembly of the Java we'll show you
the byte code as generated by the Java
compiler in expert mode we do a little
bit more than that
we count the threads that are not
executing user Java so you'll see what's
going on in detail in the system we
still merge the interpreter and compiled
methods and when you do disassembly if
you're disassembling Java code you'll
see byte code but if you have an
application with mixed Java and native
code and you look at the disassembly of
a native method you'll see the native
machine code the third way of looking at
the data is in machine mode what we call
machine mode in which we show all the
threads when we show a user Java thread
that's being interpreted we don't show
the Java methods we show the actual
interpreter itself and when you have a
Java method that was compiled into
hotspot we will show you that method
because now it looks like native code
and it appears in the native stack when
you do the disassembly in machine mode
you'll see annotated machine code no
matter which method you're looking at so
the runtime contributes a lot to the
behavior of the program specifically
that in user mode the time in the JVM
runtime is this is attributed to this
funny function called JVM system in a
sense it represents overhead but it's
not overhead in the sense that it's
dilation of the program for no reason
it's overhead for the JVM to do the
necessary things that need
to do to get the program to run properly
like run the garbage collector like do a
hot spot compilation and so forth
sometimes the Java stack is not unwound
by the runtime we what happens when we
profile is we get a profile interrupt
say on a clock tick or a hardware
counter overflow and we ask the JVM what
is the current user Java call stack for
this thread sometime mostly it will
return the correct Java call stack which
is maintained inside the runtime but
sometimes it decides not to do that and
it will return this funny frame which we
will show you this funny frame that
shows no Java call stack recorded the
reason for doing that is that sometimes
the JVM runtime finds itself in a place
where it is not safe to report the user
Java stack because there may be too much
going on so it doesn't know that it's
tables are consistent at that point we
made the decision in conjunction with
the JVM engineers that it is better to
say sorry I can't do it than to induce a
safe point meaning synchronize all the
threads in the program just to be able
to return that stack in typical runs
that no Java call stack recorded is less
than a few percent of the time and the
nice thing about statistical profiling
is a few percent here a few percent
there doesn't really matter in expert in
machine mode we don't we won't return
the note Java call stack will attribute
the time to the JVM functions that are
active at that time and we'll also deal
with the threads that are only used for
the JVM runtime we don't yet distinguish
the threads clearly as to which is user
which is hotspot which is garbage
collector but if you look at the actual
events and look at the call stacks you
can recognize the names and that makes
it really easy to tell so here's the
overview of a simple program that we
wrote it's native Java
it loads a C++ written and shared object
and makes calls into it that C++ object
also will make calls back into Java as
well as just simply returning so look at
the function list you see all of these
functions here some of them are native
functions like like this and this are
both native functions in the C++ shared
object these others are Java functions
that are in the Java code and you'll see
down here this JVM system which
represents the hotspot compiler and
garbage collector time during that run
we also have a call tree display where
you can see the structure of the program
as a dynamic call tree and you'll notice
for example here here's a java code that
calls into native code that calls back
into java so all of this has shown
perfectly naturally in user mode as if
it was all written in the same language
of the same functions we also have a
display that we call the timeline which
shows you what it's basically is an
array of all the specific events that
took place and you can select any one
event you can see here this event is
selected and you get to see the call
stack for that you can see it's in this
case it's in the Java main program which
called a routine that we called memo
lock if you switch to expert mode this
is mind you this is the same experiment
the same code except now I'm switching
to expert mode you'll notice that the
thread 2 which is the user javathread
really looks the same but now you have
all these other threads thread 3 is the
garbage collector which you can guess
from from the names here generate
collected the heap etc all of these
names echo garbage collection so what
I've done here is I've selected an event
in thread 3 which is when the garbage
collection collector is really active
and you can also if you looked at the
names for thread seven and eight you see
they are actually the hot spot compiler
I've selected that event and I can use
this up button here to move from the
garbage collector thread that has a lot
of activity to what's going on in the
user thread and that means we can show
that the reason garbage collector was so
active in this region here is because
the user routine routine that memo lock
is active in a triggers garbage
collector if you look at the source of
that routine excuse me it's easy to see
why it's allocating it's allocating this
array and it's doing it in a loop and it
just keeps overwriting the loop so it's
basically creating garbage as fast as it
can and that's why that routine is
causing the garbage collector to be
active so I've talked about a general
introduction to performance measurement
some issues of low hanging fruit easy to
see things talked about the details and
intricacies of Java profiling and now
I'm going to talk about dealing with
what we call enterprise class
applications enterprise class
applications typically have many
processes many threads what you might
consider an enterprise application be
the database running with all of its
clients and different processes and each
of them may have many many threads
typically in one of these applications
they run for a long time and you really
need to track all the things that are
going on they typically have a long
initialization phase which is often not
very interesting and they often have
very deep stacks we did an experiment on
GlassFish which is one of the Java
server routine system rather and it had
call stacks that were 250 300 frames
deep
often the ant enterprise class
applications have a very complex startup
they're launched by a script now there
are two ways to deal with this we can
actually profile a script and everything
it generates or you can alter that
script to put some environment variable
in front of the processes that are being
launched that you think of the important
ones and you can use that environment
variables such that if you don't set
thank you if you don't set the
environment variable everything behaves
just the way it did before
but if you set the environment variable
to a string that represents the collect
command to collect data
you'll get data on that particular
process at that time we have other
support we have a minus y argument that
will enable you to toggle data
collection on and off during the run
typically you would use it to turn data
collection off while the process starts
and then send a signal to turn it back
on again to turn it on again in the
first time place rather when it's
reached steady-state and now you're
applying a load to it we also have
another API where you can send a signal
to mark events in it and when you're
looking at the data you can filter to
look at what happened just between any
two marks you can also attack the
problem of toggling data collection on
and off or putting markers in by putting
API calls in the code and you can leave
those API calls in the code all the time
because if you're not collecting data
the API calls are simply no ops so safe
to leave them there another
characteristic of these large
applications is there may be many
threads the threads have contention and
there are issues associated with that
you may have lock contention a global
lock versus a local law you may have in
order to understand a lock contention we
have a method of collecting data called
synchronization tracing where we will
collect all the synchronization events
whose delay exceeded some threshold the
key issue in synchronization in general
is to understand the scoping of the
locks
if you scope it to fine you'll spend an
enormous
amount of time acquiring and releasing
uncontained locks
if you scope it too coarsely you'll
you'll have many many threads waiting
because everything is under the same
lock another issue in multi-threaded
applications is cache and memory
contention or CPU contention what may
matter is load imbalance that is are you
distributing the load equally among
threads or do you have one thread that's
doing all of the work while a dozen
threads are sitting idle it's also
important to know that when you're
dealing with load imbalance it's useful
work that matters not CPU utilization
and often you'll spend CPU time doing
what's called a busy wait for a
synchronization lock where you're just
spinning waiting to get acquired a lock
uses a lot of CPU time makes no progress
in the application so it's not busy work
but yet it's using CPU time and there
are also lots of issues with Java in
particular in these large applications
they have a hot they have a high thread
count they have interactions with the
garbage collector and hotspot and you
may need to distinguish between locks
internals of the JVM and locks in your
own user code like for synchronized
methods one of the issues in
understanding these large complex
applications is you have an experiment
which may have hundreds of threads
hundreds of CPUs many processes and so
forth you need to be able to drill down
you need to be able to extract the
signal you care about from the noise
generated by everything else that's
going on and the technique we use for
doing that is primarily filtering you
can filter the data by almost anything
you can think of you can filter it by I
only want to see data that has this
function somewhere in the stack I only
want to see data that has this function
as the leaf that is you're executing in
that you can filter based on a call
stack fragment I can say I only want to
see those events when a calls be called
C but not when any when a calls D for
example we support that you can filter
by time if you
that signal method to put markers in the
experiment or the API you can say I only
want to see what happened between these
two markers so that one thing you can do
for example is you're doing some
operation that you know will take time
put a marker when it starts put a marker
when it ends and you can see exactly
what happened to do that operation you
can also if you're doing what we call
data space measure days based profiling
were you actually measuring not only the
instruction addresses that are active
but which data addresses are being
referenced then you can map those to
cache lines to virtual addresses to
pages etc and you can filter by any of
those cache line page address lwp thread
CPU and any combination of any of those
properties the other important issue for
dealing with complex applications is to
navigate you want to be able to go from
a function to its source to its
disassembly to see who called it to see
who it calls you may want to go from a
specific event in a profile to the call
stack for that event and then to the
source or disassembly of any frame in
that call stack and what's important is
that the filtering and navigation is
accessible from option menus and all of
the data views so for example from the
function list I pick a function I can
bring up this option menu to say show me
the source of the function or the
disassembly and a filter either only
show what what contains that function or
only show what doesn't contain it you
can do the same thing for the leaf you
can filter on stacks containing
similarly named functions so all the
functions in the class for example and
you could also set what we call an
advanced custom filter in the source
view you have much the same thing you
can add filters related to the lines
that you're seeing in the source but you
can also navigate to the functions that
it calls the function both the source
and disassembly the functions that
called it either the source of the
disassembly in this example there's only
one collie of that function but if there
were more that you'd see a list here and
you could pick which one you wanted
we also have a callers collies view that
we can use to both build a fragment of a
call stack but also choose to you can
add a collie you can add a collie or you
can reset the center to the selected
function you can filter based on
fragments etc if you use the call or
call this caller call leave you in this
case I have actually built a fragment
that shows these this function calling
that one and I can set a filter to only
show what happened when that was part of
the call stack so here's an example of a
time line I'm this is the time line as
it as it appears in the program I've
zoomed in to get more detail and now
basically you see all of these events I
can change the colors for any function I
can select an event I can zoom in and
zoom out and so forth filtering by time
is another way we could use we could
have solved the problem about what was
causing garbage collection to happen I
could filter by the time that garbage
collection was active and then look at
what's in the user thread and I'll only
see the routine that caused garbage
collector to trigger we also have
something in the some way of displaying
that we call index and memory objects
which are defined by a formula that can
map any event into some index and will
show you all the objects of that index
we do predefined some of them thread CPU
seconds users can define other's index
objects are available for all data
memory objects are available only if you
collect memory and data space data which
I won't talk about further from the
index object in this case it's a thread
display you can choose to filter I only
want to see events that happened on this
thread or events that don't happen on
that thread and so forth and we also
have any what we call advanced filtering
which allows you to actually compose an
expression any way you like of however
complex a filter you'd like to apply so
I've talked about the issues of
profiling enterprise class applications
so whether now I'll talk to turn the
floor over the deep scene who will talk
about profiling the Fusion Middleware so
this section was originally supposed to
be joined by Scott Hawkes who got in
trouble so I'm delivering things on his
behalf
so everything that Marty said about
profiling and repairs class applications
pretty much applies to fish in
multiplayer because fishing area is
basically more alone or or less that
category of software so in this part I
will basically go through a small
experiment I will show you how we found
content content Pelican deadlock in a
fisherman will wear app all right so a
priori of what fission will wear isn't
what ADF is so the app I will go demo is
basically a complex Joey app and fishing
River is basically the platform where we
deployed this Joey app this app is built
using Oracle ADF which is end-to-end
Java II framework and it it it's
basically used to develop extremely rich
Java EE apps and be significantly
reduced effort the performance metrics
for this app was high throughput and
Laurance low latency and this app was
being run on spotty 4-4 system which is
basically a two socket actually have
four socket system but we split that
into two to use form with the middle
tier and two for database a high load
was used to drive the system about four
different users and the data collection
was not done when the application was
getting warmed up okay so after we have
collected the profile we load that in
the analyzer and we basically start
using the overview tab of the analyzer
where we can see where the time is being
spent
so the
was using about 1% CPU of the of the of
the of the machine and we there are two
different areas where it's being split
largely one is the sleep time at the
bottom and then other one is a long time
at the top alone about 13 doesn't the
sleep time having large percentage
asleep and pretty common for most of the
Java eClass apps so we can ignore that
for now and look at the user log time
okay so using analyzer they have three
different ways to go and look into the
user log time the caller Kali is view
will show us you know how did we get
wait where in the code the log is
basically being hit the timeline we will
show us why we are hitting the log so
many times and the other ways in other
ways there are ways to basically do it
for the command line
one of them is that your train okay so
before we jump into caller collies we
can look at the functions to see which
all functions which old methods are
actually consuming spending time in in
lakhs and there's a way to basically
filter out everything else except for
the user logs so in this case what I
have done is I have filtered out
everything except for exclusive use a
lock time which means the exact amount
of time spent in the method that is
being spent on the lock so now you can
see the daylian system which is
consuming the highest amount of user
lock time and most of that is actually
coming from the GC and compliation etc
so what we can do is in most cases we
can basically add a filter to remove the
saving system from our analysis that
being done now we can see that there are
two top methods which are consuming most
amount of time the first one is object
dot weight which is mostly from the
sleep we have seen last time and the
other one is thread group context or
gate context so our obvious suspect
would be on third group context or
context method so now if we jump onto
the caller Collies and in the color
coleus method once you select that we
see that there are three sections which
are shown on screen the center section
is the one where we beat the method by
analyzing the one on the top of the
methods which are calling this method
and the one in the bottom are the ones
which are being called by this method
now looking at this screen at the top we
can see that the beans dot is designed
time is the method which is basically
spinning calling this method very very
frequently and that means lots of time
spent in the locks in that method now
when we analyze the code we found out
that this method was recently changed
and a synchronized lock was introduced
now now we have found our method we want
to know why that method is having so
much contention why the comparator
condition coming form and one of the
tools analyzer is too basic is time line
which will help us find out where Israel
is coming from now in life the timeline
is Geor gie
way of representing the thread activity
and when we select the threads which are
doing the real user work you can see
that the threads are pretty active
except for some regions which are more
or less GC pauses now from the timeline
we can select a certain portion of the
overtime line and basically dig deeper
into it that is more like a trial and
error because they don't know exactly
where we should be looking for the log
but then after a bunch of errors we can
basically get to a portion where we can
see that the log sighting at more
frequent rate okay so once we have found
a section there we they can be find that
there is significant of log activity we
can drill down now in this view you are
seeing that there are various calculate
bars the bigger bars show the call stack
and the smaller ones are basically the
state of a thread so the green bars the
smaller green bars are the three
State which means they are doing some
useful work and the yellow ones are for
the for the log but that means that is
being waiting or something in fact
there's a blue bar which is for sleep
okay so once we have selected the bar
which we are interested in the four
segments into 13 you can basically add a
filter so that we can filter everything
else except for that method and once we
have figured out everything else what we
are left with is the method which is
basically the one which we are analyzing
for the locks and from this view we can
see that at certain point the method is
being called by three different threads
which means this method is 7li container
and we can see this not just in this
view but in many different places in the
timeline all right so so looking at the
code what the root problem dude cause of
the problem was that there was a local
variable that was being synchronized
which had a synchronous access and it
was called very frequently from Oracle
ADF code and that resulted in increase
in threading thread contention and the
proposed fix at the time of writing the
slides was that to use double check
locking pattern so that we do not entire
and synchronize blog unless this need to
do so okay so the next step was to
basically apply the fix to the code and
then profile again and once you have
done that B base we can load the profile
in the analyzer now the good thing about
this is we can do load to different
profiles one form before the fix and one
for after the fix and we can combine
them side-by-side and whence we when we
compare them side-by-side we see that
the the just time span no user log has
gone gone down from 5,000 seconds to 0
seconds and that also is written in 30
to 39 percent increase in response times
oops sorry
alright so it goes back tomorrow
what you see on this slide are the
components in Oracle Solaris studio I
don't know why that's not working there
are there's supposed to be something
here
it's the OpenOffice version doesn't come
out but there are other sessions if the
slides are available online which I
think they are you'll have a version
that has that information
so from here on any questions for either
of us
in Solaris there's a notion of both
threads and what are called lightweight
processes in earlier versions of Solaris
they were quite distinct and you could
have a collection of LW PS in a
collection of threads and threads would
run on different l w Peas at different
times from Solaris 9 on that's not true
anymore and there's a one-to-one match
so it's a historical artifact
I'm sorry
I believe we're the only profile that
will show you time spent in other than
CPU time will show you time spent in
user CPU as system CPU weight and so
forth
that's one of the big advantages Solaris
also tends to be substantially more
scalable and threads and other operating
systems yeah of the tool as I said it's
of the order of five to ten percent but
what's important is that it doesn't
distort the data which means what it the
hotspots it shows you under that
dilation are the real hotspots so you
get the right data even though it takes
a little bit longer to collect it any
other questions thank you and thank you
all for attending in conflict with Larry
Ellison's cloud talk</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>