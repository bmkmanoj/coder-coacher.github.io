<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Continuous Performance Monitoring of a Distributed Application | Coder Coacher - Coaching Coders</title><meta content="Continuous Performance Monitoring of a Distributed Application - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Continuous Performance Monitoring of a Distributed Application</b></h2><h5 class="post__date">2015-06-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_M0_Yfz92-E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to continuous performance
monitoring of a distributed application
my name is Deanna Yoruba my name is
ashish shrivastava and today we are
going to talk about extreme performance
monitoring so let's start with the goal
of this session what is going to happen
in the next one hour well we are going
to tell you how to arrive with a
solution for extreme performance
monitoring we're going to share what
worked for us and what you can apply in
your own environment so our solution
actually we categorized it in two broad
components the first one is design
patterns which is way you can modify
your software to incorporate more
ensuring into your software and the
thing which concludes it is the tools
and what we would like to communicate to
you is that we're not going to sell any
products this is not a sales pitch if we
mentioned some name is because we
thought they're useful and maybe you
will find them useful as well but
definitely this is something you can
apply to any Java distributed system
this is not going to be about a
particular product so what makes our
solution interesting well we've outlined
this three qualities which are going to
make our solutions stand out from some
other solutions out there the first
quality is that we are interested in
something which is continuous you don't
want a monitoring solution which is used
only for debugging profiling short runs
in staging area our solution is going to
be always on it will be writing 24-7 in
production mode we will be able to use
it in development as well but our goal
is to have this solution in production
as well our second qualities that we
want the solution to be lightweight well
as you so we are about extreme
performance if we introduce monitoring
and then performance goes to mediocre
performance that's not what interests us
so we definitely want a solution to be
lightweight and by
lightweight I mean it will take no more
than one percent of the overall
processing and the final quality will be
that our solution is recordable by
recordable we mean that you will be able
to share it with others you will be able
to refer it to another point at another
point all these qualities all together
make the monitoring solution more
valuable so how are we going to
structure this presentation to talk
about all these things well we are going
to start a little bit about our use case
after that we're going to proceed with a
very detailed overview of the software
patterns we're going to finish the
presentation with an in-depth
description of the tools we use and then
some pitfalls and advice thinks we
wanted to share after that we will open
the floor to the questions and answers
so let's start with a use case and in
order to understand that we use case you
need to know about our software so what
we are called is Oracle billing and
revenue management elastic charging
engine I'm going to call it easy from
now on for obvious reasons has anyone
heard about it okay has anyone working
in telecommunications all right seems
like the majority doesn't know what it
is and doesn't work into
telecommunications so I have to spend a
couple of minutes on this slide so
Oracle a charging engine DC I haven't
heard about it because it's not
something you can buy in mesas or best
buy however believe it or not you
actually interface with it on a daily
basis that's because software like
elastic charging engine is a software
which your telecommunication operator is
using every time you make a call this
software checks your balance verifies
whether there are enough funds on your
balance and then decides whether your
call can go through or rejects it and
this of course extends beyond calls we
rate your data sessions your short
messages everything related to tell
so that's what we mean when we say
charging real time charging and what
makes it interesting is that in prepaid
cases it's of course a lot harder
because we may have to make this
decision in real time we have to check
your balance see mediately whether there
are not funds of your balance when you
start your call and while we have to
actually finish you call the moment you
run out of balance so that makes prepay
charging a lot harder than postpaid
charging where you can maybe just batch
something in certain hours but real time
charging with prepaid charging has to
happen in real time so that's what it
means one hundred percent real time
charging application hopefully now we're
on the same page and what makes ECE
interesting and why we are going to use
it as an example throughout all these
presentations are these three factors
well first of all it's written in Java
that's pretty clear second of all we are
a distributed application to achieve
performance you can no longer do in a
single process right so we use the
technologies called Oracle coherence and
Oracle no sequel but our as I said our
patterns will apply to every distributed
technology out there and finally what
will make our monitoring solution really
interesting is elastic charging engines
focus on extreme performance our latest
benchmark was 1000 operations or calls
per second or Core so this software its
main selling point it says its
performance performance is what
differentiates it from other competitors
offering and therefore our monitoring
requirements will be particularly
stringent so let's see what are the
conditions in which we operate because
this shape our monitoring requirements
well first of all we have expectations
of low latency well clearly all of you
guys you want to close to go fast you're
not happy you know the little lady goes
dial some number
connect so you want just everything you
want tweets immediately to be published
you want calls to be immediately
connected but it's actually goes beyond
that and you may not realize that the
telecommunications industry has very
stringent requirements of the worst case
latency in other words we never know
which call is an emergency and we just
cannot afford to have certain calls
which get rejected because of
performance issues or just get get
accepted but with a very long latency
the second factor is an overall heavy
throughput heavy system load again
nothing new in developing markets more
and more people get phones here I'm sure
everyone has a smartphone but then you
guys start to get more devices so
overall the load on such a system is
elastic charging engine is only
increasing with time and then there are
some factors which are just linked to
the nature of our application to its
technology the fact that it is
distributed and the fact that it's uses
a multi-level software stack which as we
said is Oracle coherence and Oracle no
sequel so what is it what do we want
welcome to morning to our performance
primarily and the key thing is we want
to get key inside about latency and
throughput and let us in throughput
they're pretty generic performance
indicators which I'm sure you know about
but for us they map into very tangible
business KPIs latency is the time your
call gets to be established and
throughput is the number of people who
get to their calls establish at the same
time and believe it or not we care about
every single call of yours we never know
which call is going to be an emergency
and if you're calling 911 to be in that
one percent which happens to fail or
have an extremely long response time
other factors we wish would what we
would like to see if
from our monitoring solution is that we
would like to view our 11c and through
boot over time we're not happy with just
a snapshot of what happened at a
particular moment some other nice to
have features if they are reporting we'd
like to share the resulting reports with
our colleagues management's and
customers it would be nice to have some
bottleneck detection so that we can tune
certain parts of our application
proactively and finally we'd like to get
the view of a system as a cohesive unit
this might not be immediately obvious
but once you move away from a single
process and use distribute your
application you have multiple machines
multiple technologies it actually
becomes a challenge how do I collect
data from all these little moving
processing parts and how do I get it
back to myself in a single view so this
is another requirement of our monitoring
solution we have a couple of non
functional requirements the main
requirement of course being the
performance if the system is increase
incredibly heavy weight it's not going
to work for us or want a solution which
does not impact our performance
significantly some other are nice to
have features would be the usability we
want it to be easy to use if it takes 20
screens 20 commands 20 PhDs to set this
up monitoring up it's not going to be
very valuable and then the final quality
is a separation of concerns for the
software purists among us so monitoring
is a cross-cutting concern is something
which affects every area for our
application but the same time it's not
our main business logic so we want to
address this cross-cutting concern
elegantly we don't want our code to be
tangled or some pieces to be duplicated
so we want to apply the right software
techniques to address the separation of
concerns so now you know what our is our
environment is and what is it that you
want we want from the system finally
it's time to tell you how did we achieve
this monitoring choir
and Satan and switching to ashish and
he's going to tell you about the first
part the software patterns hey thanks
Jana can you hear me alright yeah ok so
you know what are the years as Java
graduated from a single instant single
host type of application to something
which is clustered nom massive clusters
with peer-to-peer clustering protocols
and in a distributed computing
environment the topic of monitoring not
only became challenging but very
important and you know when we come to
sessions like Java one or similar and we
you know come across a new tool or the
previous tools that have evolved into
new directions new functionalities and
we take those tools back home try to use
them but somehow we still find that
there is something missing and it's
supposed to be like that because what's
typically missing are your application
kpi is your key performance indicators
so off-the-shelf software is monitoring
you know it's very important that you
choose the right tools but it's also
important to believe that if that not be
sufficient would not address all your
problems and that requires that you
build a monitoring into your system so
there has to be a custom development
that is needed and like performance is
not an afterthought anymore you have to
think about performance in your system
monitoring as well you have to build
that inside out in your application and
once you have a mechanism to collect
statistical data from your application
the next of course the challenge is how
do you analyze that data effectively and
present that data which is useful not
only for the development team but for
your business owners as well as for your
customers so that leads to this problem
that how do I incorporate collection of
a statistical matrix in your general
processing and the approach that we took
is the term that we that I mentioned the
monitoring in design you have to change
your domain model to put monitoring
or in your system is not an afterthought
and before I dwell more in the details
of how it works this is my ultra
professional animation but this is kind
of oversimplified form of what our
product does you know as Diana already
mentioned it's a clustering technology
it's all the nodes they are using Oracle
coherence on top of an in-memory data
grid from an infrastructure point of
view all these nodes are exactly alike
with a little difference we have some
concepts of roles like some of the nodes
they perform the role of a processing
node these nodes manage the data that we
load into coherence caches and some
nodes are just clients their job a very
limited in their scope in a way that
they only receive the network events and
send it to further processing and people
who are using Oracle coherence or know
about it would appreciate this diagram
so what do we do you know typically we
have a ECE client which is kind of a
functional client not infrastructural
client it sits in a wrapper what we call
the network mediation so if you're
making a phone call or maybe tweeting
right now or opening a data session what
happens is that network mediation is
actually harnessing all these events
that are coming to the service provider
and sending to the EC client for
processing but before we actually do
anything about those events what we do
is we go through each request that comes
to this client we identify which
customer these network events are meant
for and based on that customer data what
we identifies the partition it's a
distributed system partitions are spread
across multiple nodes multiple hosts so
you identify the partition number the
partition ID which actually carries that
customer data and then what we do we do
some kind of batching for the customers
that are part of that partition and the
members the node
the node the processing node that owns
that data so once we have this member
information we batch certain requests
for that member and depending on some
ripe policy like when that batch is ripe
to be sent for the processing we send it
to that node that becomes my request
right and it's typical to our
application what we do here but certain
activities happen in the processing node
and in your case if you are working in a
distributed environment you know similar
activities might be happening as well so
in our case we you know what we do is we
D batch the request we do some data
lookups we apply the tariff which is the
core of our charging domain of course we
save the call sessions that you know
making a call or you're tweeting we save
that session or certain information
about that session we prepared the
response and we send it back you know
how many times we have seen a system
where you send a request on you get a
response back it's very common you know
very common application but how do we
add a statistical collection in this
general flow so what we did was we kind
of loosely modeled it based on our
general post office you know you go to a
usps store you buy an envelope you buy a
packet you put your you know whatever
letters or things that you're shipping
you put destination address the from and
to and you give it to the post office
for shipping and what you get in return
is is a tracking number and the tracking
number already is on the package that
you can use to find you know where that
envelope is where the package is is it
through the sorting facility is it
already has arrived in the destination
city has it been you know delivered and
you get tech knowledge knowledge meant
back so we kind of very loosely modeled
this whole flow and introduced a concept
con envelope right so what we do is when
we build a request we put that request
in an envelope and this Angela flows
through our system now if you look at
this diagram you have multiple JVMs
involved in a single flow so the client
is one JVM probably on a different host
it sends a request over network and if a
network is introduced between a call
what will happen serialization and then
UDC realize that message on the
processing node you do some are certain
activities and what happens is that
envelope actually keeps on going across
the system until you know you get the
response back so what a typical envelope
contains it actually contains three
information right one the first
information is the routing information
which is like on your packet on your
usps package you have a from destination
those to and from destination the
addresses that's a routing where this
whole envelope is meant for the second
component is of course the payload what
you are sending in our case it's a
request you know without the payload and
envelope is functionally incomplete the
third this model of introducing an
annual of gave us an opportunity to
introduce the concept of tracking right
so we have this tracking context
introduced in the envelope and as the
envelope is flowing through the system
what we do is we update this tracking
context and how do we update that we
introduced a concept or chronicler so as
the request is sent we add a chronic
learn a craft chronicler is like a is
like a passport right so you carry a
passport and the passport gets a stamped
wherever we want that passport to be
stamped so that passport that flows
through the system all the points where
ever you need to stamp that passport
gets marked and Mark Twain in our case
the time points and what we do is we
garnish the harvest at information on
the client when the response comes back
and we introduce a concept of a
statistical reporter that can harvest
that kind of data so from a very in a
simplistic approach we introduce a
concept of tracking context that flows
through an envelope we the tracking
context is a placeholder for all the
chronicler information
chronicler contains the time points some
other information as well and then when
it comes back to the response we have a
huge set of data statistics about per
request so we are very granular and we
can harvest that information through a
statistical statistical reporter and if
you expose that over gmx that gives you
a plotting capability but one of the
results could be something like this you
know you ran your product for a
simulated period of say 3,600 seconds
you come across what's my through bird
20,000 operations per second and my
average latency happens to be 50
milliseconds right feel good I'm you
know I'm meeting my SLA but as you know
averaging something is deceiving you
know averaging hides problems you don't
want to average and populate your
benchmark and say yeah we can meet
average Layton sees especially in the
telco as Deanna had mentioned every call
is important to us so I cannot say my
system meets 50 millisecond goal of
latency because maybe for the most part
the latency is about 3 millisecond but
there could be few which is taking 500
milliseconds we need to track why those
500 milliseconds latency did occur that
leads to you know the second problem
like how do we get more granular
information and not only granular
information but how do I know about my
latency is over a period of time so we
need another dimension of time against
our statistical matrix and the approach
that we took was the statistical
reporter has to be enhanced because it
has to be able to you know give the
information about all this latency and
throughput over a long period of time
and maybe live always on you know in a
system which is always on so that leads
to something like this and I I or
simplified this you know the more API is
more bells and whistles on top of it
were at the heart of this hole
solution is a circular buffer and
circular buffer what it behaves like is
behaves like a wheel you know a wheel
which is moving and you know as the
envelopes are garnished it comes back to
the client we take the you know offload
the chroniclers we put it in the
circular buffer then this wheel is
actually while moving over a period of
time so what happens is the chroniclers
are being added it's full and what we do
we offload the entire data that this
circular buffer contains into a stat a
bucket this is our own jargon but a
concept of a bucket we offload the data
to that bucket and then we keeps on
rolling on it moves on and the system
keeps on putting chroniclers in this
data structure and what happens is when
the stat bucket has all the collection
of the data for a given reporting window
and then we can have the statistical
reporter that can report on top of the
data that were collected by their side
buckles so another beauty of this kind
of solution is you know no matter how
long you collect the data for for a
given reporting window the size of the
data that you collect is always fixed so
it's not like you are going to stress
your heap just because you ran this
application for 10 hours right so which
is very critical so we have a fixed data
side and based on how much data it
typically contains we can provision our
our heap second is you know sample
reporting I would talk about why
sampling is not you know a good idea but
what we can do is when the requests are
being chronic load at that layer we have
an ability to sample the request you
know maybe in some scenarios for some
people we don't want to track hundred
percent latencies we just want to see
you know a general health we can sample
the request maybe ten percent of the
requests we want to sample how they are
behaving not very useful but it has to
be built in our in our solution so this
data structure also gave us an
opportunity that we could build a
on top of it by the way the chroniclers
have the hundred percent request
latencies here and of course once you
have this data structure on top of all
these api's you can expose the stats to
JM x and we can use that interface to
represent our graphs and you know
whatever tools that the ni will talk
about momentarily going back to this
class this is one of those slides that
you know as thinking maybe should be
included or not because this problem can
be solved in maybe 10 different ways
this is our solution and not going into
to detail about this class diagram but
if you look at it is classes i'm not
sure how energy bill it is from behind
but this presentation will be uploaded
so you can take a look later but what
happens is you know you have a request
we have chronic lives we offload the
chronic list to stat buckets it
advocates are associated to a start
reported that do a continuous reporting
and we have some concept of nano periods
which measures the time that it took at
each layer of your system in nanoseconds
so we have much more granular
information and some concepts of time
points which is associated with the
label and i will talk more about that in
next few slides so this class diagram
and the data structure together provided
a mechanism so that we can not only
collect the data but we can do a
continuous reporting over a period of
time and that led to something like this
like yeah i have a much granular
information i can collect i can see my
minimum maximum and average latency and
i can plot them my throughput i can see
if it's erratic or if it's stable now
this is a very funny diagram that you
have seen on your right it has three
different colors blue green and red
these are typical three different
network events that we we get for the
usage processing you start a call we got
to initiate event that we process
throughout your session we get multiple
even
update events and when you hang up your
call we do a terminating event and all
these events they have different
signature when it comes to the
processing time so we measure these
three events in a graph and if you look
at this maybe some of these graphs these
are like a 10 hour period but we do have
an ability to drill down to few seconds
and what happens is like most of the
latencies are pretty acceptable some are
maybe not acceptable right here on your
own you're right but the funny thing is
this spike you know if you're averaging
it if you're not tracking all your
requests that spike could be you making
an emergency call right so you don't
want that call to be dropped and we have
to have an ability to make sure you know
debug into our system see like what
really happened why did that spike was
cost so that led to kind of another
problem that this okay you're plotting
this data over a period of time minimum
maximum latency average latency is all
good you know but we need more granular
information the beauty is your domain
model already supports a mechanism to
collect the data you know and now it
becomes how you harvest the data
effectively so you know the second goal
which kind of it's a real time goal that
our customers ask us to to achieve is
like okay they don't care about which
request failed but from a product point
of view they want to make sure that can
you guarantee that your five nines
99.999 percent of the time can you
guarantee that your latency is less than
certain number in our cases somewhere
around 50 to 60 milliseconds so forget
about how do we achieve that the thing
is we have to have a mechanism to prove
that we can achieve it and show to our
customers that here is a plot here is my
percent and reporting and I can prove we
are tracking we are not leaving any
latency or any requests behind we are
measuring every request in the system
that is coming through
and we are measuring those and we can
plot where you know the most of these
latencies fall in so the approach we
introduced a concept of range bucketing
right so in the range bucketing what we
have are the predefined sets of latency
ranges so we have 0 to 5 milliseconds 5
to 10 millisecond 15 20 25 up do the
latency range which is relevant for us
say 60 90 milliseconds and maybe all the
requests that are that took more than
500 milliseconds which is too high in
our scenario could go into a generic
bucket right so as the requests are
coming in we can plot this percentile at
each layer of our system right so and
and the good part about this solution is
that unlike you know the envelope that
flows through the system we are not
adding any more data to be serialized or
deserialized at the envelope is moving
through our system so the data already
exists in our envelopes what we do is
wherever the percentile breakdown is
needed we grab that report where we grab
those data and we calculate which you
know latency this request is falling in
which bucket right and so it's a fixed
data set again this is critical for you
know predictability and what we do is we
in place put the latency is in the in
that particular bucket we just a counter
like this many requests took 5
millisecond X number of requests took
100 milliseconds so once we have that
and compounded with you know different
breakdowns the stage breakdowns end to
end when the client sent a request back
and gulfs our response we can track the
latency percentile in a batch which you
know I mentioned earlier bats is like a
batch of requests that we sent to a
member and that is kind of our atomic
unit of processing or at the request
level or the server-side level so client
center request is a network involved but
the request when it come
to the server how much processing it
took on the server side when the
orchestration happened in on the
processing node so we have three
different layers where this person tired
reporting can be done and we get
something like this you know it's like
okay you have 99.999% i love the
requests are under 128 millisecond and
this is a real data that we took from
one of our test servers and that gave us
an opportunity to actually go and look
into the tuning opportunities in our
system now in a distributed environment
there are so many factors that play in
your Layton sees you know your your how
much you heap you're consuming the GC
activities you know are all kind of
activities so what we do is a serializer
for example like we are serializing the
data and sailing why are we meeting 128
millisecond 99 59 times you know of the
whole period there's something wrong and
that gave us an opportunity to see like
what can be optimized where in you know
which layer of the system can be
actually dig more deeper and see where
the optimization can happen can be
optimized serializers can we optimize
the T serializers we use Oracle
coherence there something called app off
a serializer portable object format if
you are not already familiar with
governance I would like you know to take
a look at it so we have our own custom
serializers and we have full control of
what kind of data that we see realize in
peace utilize and of course the other
factors in the application that we can
optimize based on this percentile but
the beauty of this tabular data is that
once you have this collection of you
know table entries and you add a third
dimension to this data which is
throughput so if you capture the lesson
latency percentile against a varying
factor of throughput what can you do you
can draw a heat map right so this is you
know I admit this is not the greater the
heat map that we plotted against our
data it's one of the sample heat
I took but this is kind of you know what
is my varying throughput what is my
delay and all these three dimensional
access and we can plot the heat map how
useful it is for many people it does you
know it gives you a visual presentation
representation of where you know the
most of the requests are falling in
which quadrant okay so well I'm good
that leads to another goal that
introduced another software pattern in
our system before I go too deep into
this problem I go back to the same
analogy that I took for usps right so
everything is hunky-dory system is
working happily and at one day you find
there's a construction work going on on
101 right or the new road has been built
that your driver has to take or you
hired a new driver and you want to track
the performance of the driver or the
impact of the construction work on 101
how you know how what's the impact of
that particular change in the system so
this is you know when we started with
this challenge it was more of a very
developer centric you know we have a
continuous integration of our system we
do a continuous performance baselines so
and we do this continuously as your
developer is actually introducing new
classes and new methods and maybe
changing the current classes and you
want to track you know what is the
impact of his or her change in the
system and we want to do that before
that code is is delivered so you know
this was the you know the real challenge
but as we kind of produced the solution
you know some other goals from business
comes and slick can we keep it on you
know all the time because this is the
good statistics where we can say you
know from this stage to this stage what
is you know how much you know time I
you know requests are taking all my
system is taking and that kind of
challenge just that you have to do
something like this so that it does not
impact the system right and one of the
things that Dianna mentioned in one of
the slides is what is you know the
cross-cutting concerns you know
separation of concerns and that kind of
led us to start thinking about two
solutions you know annotation the base
solution in aspecting the solution a
notation in a sense that whatever we are
measuring whichever method we are
measuring we annotate that method and I
will next slide actually describes a
little bit more in detail so we annotate
that method and we create throughout our
systems those critical points where we
can annotate the methods which are
critical right and then we can use some
kind of an aspecting to limit the scope
of the classes where those annotations
actually take effect and these
annotations in turn are the points where
the time points are added to our
chroniclers so as we build solutions
over solutions our domain model is not
changing right we have already
introduced the concept of you know
certain data structures that can harness
the statistical information and now what
we are building are all these reporting
is structures and you know how we
analyze those in information so a
typical class looks like this so we have
a preset you know predefined labels
enums and what do we do is whatever
class that needs to be tracked and
whatever method needs to be tracked we
annotate that method with this track
annotation we do have a couple of
annotations but for the sake of this
presentation I am using this track and
this we have a concept of point label
now how do I know this my information
the how much time it took in this
particular method how do I live how do I
uniquely identify that so we label that
information through a through a point
label and of course then we can define a
concrete aspect we can define the point
it was the scope of my
and I can dig actually more into package
details so anything which is you know
Oracle communication dot blah blah blah
you know enable them or we can be more
precise and go to the class level that
only you know collect the data from this
class so what that gave us is exactly
like this it's a chronicler breakdown it
tells you how much the D batching
activity took how much the lookup of the
referential data took how much you know
you know when we applied when we
impacted the balance how much time it
took us all in nanoseconds how much time
it took in saving the session and
responding the session and this is one
of the mechanism that really really
helped us to optimize our serializers
and it's like yeah everything is fine
orchestration is not taking more than 3
milliseconds 5 millisecond don't get my
number I'm not claiming that but it's
like there's something happening on the
network and one thing that you need to
look at is like okay what are you see
realizing so once you see you go through
all the attributes that you are
serializing and each other and maybe I
don't need this information and
unnecessarily stress the network and
that led to another activities like I
can we use a linear data structure
instead of what we were using so this is
a very very powerful system that helped
quite a bit when we were you know
working on this product we still do as
Diana mentioned our SLA is to made about
6,000 or 1200 operations per second per
core so it's very very very extensive
it's a CPU bound application and you
know when when your application is CPU
bound I will discuss more about what the
challenges are because now you are
sharing your resources with other
activities in your system primarily on
long GCS and how do you relate and
monitor those activities and you still
have a collective view of the system
behavior so with this
I give it back to Deanna to talk about
the tools you know that we were using Oh
still using all right thank you sheesh
so this were some pretty complex
software parents and glade you guys
stayed through them and Ashish walked us
in great detail in case you're worried
there is no more code snippet so come to
becomes easier at this point and well
what what you've seen through the
previous part is that we've built a
solution which permits us to report
every latency to get a view into every
request every phone call but in order to
satisfy the performance requirement of
this solution are being lightweight I
would like to stress that all these data
structures they don't grow over time in
other words this little stats reporter
it doesn't store more and more and more
data as urine continues in other words
this solution is still not complete and
we need one last part which will
actually store that data and present it
to us in an understandable format so our
approach to this is something we call
the monitoring dashboard this is a
concept which most of you should be
familiar with is just the performance
dashboard which visually depict all the
matrix and the technologies which we
used we found useful they are called JDS
and RD a little bit of in-house
development just to glue this together
but the important thing is that most of
the dashboard actually came by the
goodness of this technology so if you
want to try it out such as something we
would like to recommend so here I'd like
to actually break the flow of our
presentation a little bit and show you
the final result before I tell you how
do we do it so that you get an idea of
how this monitoring dashboard looks like
and well essentially is just the web
application is just the one single web
page which contains a lot of graphs I
chose the two which I thought our most
representable so the first graph I chose
is that initiate latency is the most
important latency for us because it's
the latency your call gets established
with and what we see on this graph
flashing oh no well it's a graph on the
Left what we see on this graph is that
it's a result of a 10-hour run right and
we see that we actually satisfy the
requirement of the controlled worst case
because the maxima and see you say it's
always in the 100 millisecond range it
doesn't go beyond somewhere so knowing
that our solution reports every latency
and that we don't lose anything on the
way makes this graph very powerful
because we can use it to communicate to
our clients hey look at this our worst
case is control within 100 millisecond
not a single request went beyond
essential number and then the second
graph I include because it's also very
typical is the JVM memory usage so when
you are work at performance when you
look into these Levin says you need to
correlate them with other factors and
garbage collections and more heat
management is one of such factors so
it's really useful to include such
system metrics in your dashboards so
that you can work with them in your
analysis and of course it took our our
colleagues a lot of effort to actually
achieve this kind of very managed a Java
heeft which in turn produce this very
control latency and finally in the
bottom we always include the
configuration that's very useful so that
we actually remember after a certain
time what all these graphs were about we
know how many servers were used and what
was the load like so let's talk a little
bit more about this monitoring dashboard
and the technologies behind that and you
can use any technology which you like
but these are just the factors which we
would like to stress out so if you
choose your own technology well well
check if
these factors satisfy the requirement
first of all we got the view over time
that's hopefully pretty clear and pretty
straightforward and you yourself so how
powerful the description the description
of a latency is when you actually see it
on a graph rather than if you just saw
some text-based second thing we included
various metrics of course most
importantly we want to see the matrix
which as she told you how to build a
latency and throughput but as I said
there are always those the system
specific metrics which affect your
application specific metrics and that's
why you want to take a look into those
as well so you would like you should
probably include the Machine specific
metrics or CPU memory network delay disk
space as well as some JVM specific
metrics hopefully you're all using Java
so thread count garbage collection times
new thread creation that's probably the
matrix you would like to include so that
your future debugging and tuning becomes
easier and finally what makes this
dashboard powerful is a consolidated
view as i said we put all these grubs on
one page and then it makes it really
easy to actually correlate the different
factors if you remember the graph which
as she showed with a giant spike well
maybe you need to look into your other
metrics may be in the system magic and
see what happened there was it a fuji c
or was it just the disk space or
something like that so that really helps
your performance debugging very useful
some other qualities from a usability
perspective is that the dashboard
actually is easy to use as I said all of
this comes mostly from the gres and RD
technology not very limited custom
development here so you just issue one
command and then the dashboard
automatically starts to query all this
data collected story to produce graphs
there was little involvement from our
side to bring that and that is of course
very useful
and then the dashboard it's a web
application so it's actually easy to
share as you saw we customize it to
include our configuration and also well
between ourselves we can just use send
link so that someone else can take a
look at the data we collected but we can
also print this page as a PDF and that
is pretty much custom stone and we can
use it at any later time finally some
other useful factors about the
monitoring dashboard it stores data
pretty much without losing precision we
have spent so much time building a
solution which reports latin sent
through boot with precision and then if
you are storing solution pretty much
don't stats precision that your purpose
it would be defeated we need some ways
of storing data which would maintain
this precision it also supports raining
down something you might not be able to
see from the screenshot i presented you
but actually again in case of spikes you
actually can drill down to that
particular time period and say hey I
want to see between this five minutes
period and this probably would help you
to analyze the issues in more detail and
finally the dashboard itself it is a
lightweight we actually run it on our
laptops regularly that's also very
powerful because we don't have to wait
until certain performance environment
becomes available we can start using it
right away and finally as you saw we
were able to include some of our
information all right we were able to
plug this dashboard to our own metrics
we were able to include configuration so
customization is always an important
port for the monitoring solution so that
actually concludes our solution now you
know the two secret ingredients which
make it works it's the patterns and the
dashboard working together which permit
us to get detailed insight in about
every lavin see and every phone call
would just before we finish we would
actually like to share some pitfalls and
advice something which we found useful
on our way we would like to let you know
about this as well
Ashish please could you tell us okay so
pitfalls distributed monitoring is not
same as a single JVM monitoring there
are so many loose factors that are
involved in a distributed systems that
do not even exist in a single JVM
monitoring and most of the monitoring
tools that you know we use they are all
good but they actually give you a
harness the data that are local to a JVM
but you know you need to wear whatever
monitoring tool applications that you
are using or you're building in house
you need to make sure that you can
correlate the events in one Jamie Amman
energy you know as I started out with
you know Java now is very extensively
used in a pure clustered environment
pure clustering means goes beyond
farming what it means is that one
activity that happens on one JVM is
affecting the entire cluster and we need
to make sure that we map that activity
and we can analyze that activity that
was the impact of that event on that
node so it's it's very important I give
you an example of a GC spike one node
under a full GC or a long mixed GC and
what happens is that node is suddenly
not responding and if the client is are
still receiving a stream of a high
volume of network events it's not able
to send the request to that member the
requests are piling up on the client and
what you see is the overall system
behavior it actually goes in a degraded
mode so we need to be very very careful
about you know what we are monitoring
and how effective it is to find and pin
pound pinpoint the third you know the
problem before you go ahead and solve it
consolidated view is very critical as I
mentioned you have to be able to relate
the data whatever solution you are using
you know if the consolidated view is
very very critical second point that I
would like to mention and this may not
be asked critical but is
important it's the consistency of tools
that you are using and other teams or
your customers are using you know
because if you can share a you know a
set of tools with your development team
with your QA team with your performance
team what happens it it actually gives
you a common language across the teams
before you even you know release your
product and that saves many many hours
because from their specification from
the test specification point of view
they all speak the same language so if
some activity's some problems that
happened in a performance test cycles
you know the they can easily give you a
test specification that developers of us
can actually go and try to reproduce it
you know same language always helps
though not always critical how many of
your spouse's allow you to take work at
home mine actually does not but whenever
she does it's very important that I have
access to all the tools that I have when
I'm in office on my laptop these are you
know powerhouses you know they're very
very powerful laptops and a smallest
scale we should have an ability to
reproduce the problems that you have
seen in a multi host multicenter server
environments so it's very critical and
at least you know it allows you that it
does not impact your system so much so
that you know you your profile is very
different when you run locally or in a
test environments second thing is
averaging time and again it has come you
know whenever somebody comes to you as a
yeah our system can meet these sls when
you know comes to throughput our
latencies you need to ask you know is it
averaged or is it calcul like for each
every request because in some
applications don't care averaging is
good right but some applications that
are very critical they need the
averaging is not good we
heís the problem same goes with
sampling with sampling your application
you will miss some events that are
critical and you need to make sure again
that you monitor track measure every
even that you're processing and you have
to do that in a way that is not actually
in stressing your system and there's a
solution that we have built and we ran
on multiple hardware's and the impact of
the solution because of all the API is
the lightweight data structures is less
than 1% if we have to measure it's like
somewhere close to half a percent of the
entire resources that you have GC has a
big impact you know eight hours no GC
activities hits a big spike that has a
big impact on your overall system
behavior especially in a distributed
environment distributed computing
environment and if you have not done so
already I would highly recommend that if
you have an application which is latency
sensitive you know take a look at the
Java 7 the release 40 they have some
really really cool features they waste
percentage initial occupancy rate that
so many tuning and made it very easy to
tune as well and you would like to look
at those parameters and you know how you
can tune them to actually make your heap
profiles very very predictable third
point is watch for the process of
sharing the same post it's like for the
entire day the system behaves properly
something happens at twelve o'clock in
the night and it goes down there is a
wireless scanner that is running a troll
o clock so you know it's not a window
system you know if you're sharing your
host something will be the smallest UTP
server that could misbehave and CPU
bound applications like ECE they are
sharing CPU goals with other activities
on your system so you don't want other
processes to hog away your your
your resources and needless to say
always run long duration test I would
assume everyone does right anything any
problem that you would not see in a 3600
seconds will appear in eight to ten our
tests the things that will not appear in
a 10-hour test or a week we we can't
test will appear in a you know 48-72
hours a week runs so it's always always
always recommended that we do long
running tests to really know what is
your application behavior or more so if
it is a pure java application pretty
sure you have hundreds more but those
are the more critical ones that we could
I thought that we would put in on the
slides okay so I think this is the last
slide and I let Diana decide how we how
we did so this is a last slide at last
and maybe some of you still remember
this slides this actually are functional
requirements we outlined just in the
beginning of the presentation so let's
see whether what we told you actually
addresses this original requirements we
got the detailed inside and out about
our performance with the help of
granular performance reporting and
latency percentile report now we know
about every single request and the
legacy of every single request we got
the view over time with the help of
combination of granule reporting and the
use of monitoring dashboard now we can
get a idea about our elevenses at every
moment in time we got reporting with the
help of monitoring dashboard the method
break down the aspect of solution
permits us to detect bottlenecks and
finally we got the view of a system as a
cohesive unit with the help of the
monitoring dashboard where we got a
single view of every important
characteristic on one page so this is
our solution I think we're pretty much
used all the time we have so thank you
for coming we hope you find it useful we
hope you can apply this in your own
distributed
java application so thank you we'll take
your questions I think you can come up
to us our engineering team is here as
well our architects are here to answer
your questions thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>