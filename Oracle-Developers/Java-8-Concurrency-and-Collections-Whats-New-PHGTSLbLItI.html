<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Java 8 Concurrency and Collections: What’s New | Coder Coacher - Coaching Coders</title><meta content="Java 8 Concurrency and Collections: What’s New - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Java 8 Concurrency and Collections: What’s New</b></h2><h5 class="post__date">2015-06-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PHGTSLbLItI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so welcome to the session on Java 8
concurrency and collection and what's
new so we did a similar talk to this
last year and then we're asked to come
back again so hopefully it's not a
repeat of what some of you guys have
seen already or if it is hopefully
you'll get something more from it this
time
so my name is Chris Haggerty I work in
the core libraries group in Oracle and
I'm joined today by Mike Diego who is
also work working for Oracle in the core
libraries group so I'll take you through
the first half of the session and Mike
will take it to the second half so this
is a sorry um it's an introductory
session to some of the new api's that
are in the Java util concurrent area so
I just want to highlight the API is
their significance and so you'll know
what they are have to use them and you
can use them in your own programs
there's also some significant
implementation changes in Java 8 which
we want to highlight as well so gonna
start with walking through alexey
shavelev at contended support we walk
through the new API is in Java till
concurrent brought in under Jeff 155 and
then Mike will take you through the new
fences API their changes to the hash map
and concurrent hash map to better
support hash collisions and then Mike
will talk about some of the new
extensions to collections that came in
under the lambda J s or so will dive
straight into contended so some of you
may have heard of I apologize for sound
some of you may have heard a false
sharing being described as the silent
performance killer and that's because
it's far from obvious if you're
suffering from this problem what the
problem is
by looking just at the code
so briefly to describe the issue so
memory memory is stored in in in the
cache system in the units known as cache
lines the most common size of cache line
is 64 bytes and false sharing is when
multiple threads unwillingly or
unknowingly impact the performance of
each other by modifying independent
variables that just happened to share
the same cache line and this can be a
limiting factor to building scalable
applications so we'll have a look at an
example of fault sharing and I'll take
you through the changes we've put in in
Java 8 to resolve this so the projector
is not doing justice to the colors we
have in front of me here in the slide so
I'll try to do the best I can to
describe this so on the left hand side
we have a very simple class called point
it's got two integer fields x and y and
the right hand side we're trying to
demonstrate what an instance of a point
may look like made in memory so the gray
region where you can see any color on
that but the gray region is supposed to
be a region in memory bytes that just
happened to correspond to a particular
cache line on a core so the area is
actually between the brackets so no you
can make them out there's a random data
in the before our instance of our point
our point Dan has got a header which is
marked by the H and it's got a value for
the X field and it's got a value for its
white field so that's just an example of
how an instance of a point may be mapped
in memory
now let's say for example you have two
instances of a point by two separate
threads they may just happen to be
co-located in close proximity in memory
you don't notice you've no control over
this it just may be the way that the
memory is laid out so if you've got two
point instances both constructed by
different threads and been accessed by
different threads there's no
coordination between these threads they
don't access each other's points they
have no knowledge of each other's points
but if for example tread a updates the
value for its wide field of its point
well then went read B comes along trying
to access either X or Y of its own
points it will have to reload the de
cache line because it's been potentially
modified by a different thread so the
point here is that we've got two threads
there's no synchronization or
coordination at the Java level but the
impact performance on each other just
coincidentally because of the way the
instances get laid out in memory so this
is a problem that we're trying to solve
so more formally the goal of chap 142
was to reduce cache contention and
specified fields and to do this
developer needs a way to specify that
one or more of the fields could be
potentially are likely to be highly
contended off across processor cores and
in the VM needs a way to arrange that
these fields don't share cache lines as
we seen in the last example
so the solution we have in java 8 is
we've added a new annotation called
contended it's applicable to fields and
types but you may notice that this
annotation is in a sun dot star package
and I don't know whether you've been to
any the jigsaw or magic modularization
talks this week but you'll know that
we're not promoting the use of Sun dot
star private classes but we just want to
mention it here because file sharing is
a particular issue that people run into
so we have a solution in in the JDK and
we'd like you to try it out if you think
you're running into this problem give us
feedback and then maybe you know your
feedback can help support a edition of a
standard API for this at some time in
the future so I just want to point out
that this is completely unsupported and
as of today it works but we could change
this in an any update and in fact adding
the the annotation to your fields or
types may do nothing in the future so
that's it so what does it do today ok
well it takes an optional argument which
is a contention group so you can group a
number of fields together you can put
the annotation in a class which will
have the effect of grouping all of the
classes fields into one one group you
may want to you may not want to put the
annotation on your class if you've got a
mixture of fields which may possibly be
contended and a mixture that may not use
a certain overhead to to doing this so
you may want to make it more granular
rather than just putting it a class
level so returning to our simple example
here we have annotated the Y field and
then on the right hand side we can see
the effective memory that this has so
again we've got a header for our object
we have a value for its X field and the
peas here represent padding that's
automatically added by the VM and it
pushes out the Y field into a region in
memory
that's not co-located with anything else
so what's not shown here is that the
padding the peas go beyond Y and they go
as far at the end of that cache line so
the Y will not share any original memory
with maps to any other cache line people
used to do this in the past by just
padding their own classes with lung
fields rival lung fields that don't do
anything so this is definitely a better
way so you've got classes that you're
padding you may want to look at this
that's all I wanted to say on I'd
contended so I want to take you through
some of the most notable changes that we
have to the Java util concurrent package
in Java 8 so we've actually got a just a
small mini agenda which will zip through
pretty quickly but the reason for this
is that most of these features are
largely independent of one another it's
really just a grab bag of various
different things so we'll start by
looking at scalable updatable variables
so what are these well are typically
used for collecting statistical or
tracing information so for example if
you're tracing you're counting automated
calls in which many many treads could be
invoking your method and updating let's
say a counter to keep count of how many
times a method gets executed typically
you're only interested in reading your
resource periodically so you have way
more rights to your counter or updates
to your counter but you're only period
you're only going to display your
results periodically maybe at the end of
a run or something like that this could
be a common bottleneck for developers I
don't know if anyone in the room has
tried this but if you try to implement
your own counters to to see what type of
performance you're getting are many hits
you're getting on hot methods you tend
to affect the performance itself so you
don't get an accurate measurement of
what you're trying to achieve
so we've added a small number of new
classes and E's are targeted at the
average developer under suitable for
most use cases they provide much much
better performance over other approaches
like using atomic lungs and I actually
have a few slides and that you know to
come and their implementation uses
contention reducing techniques I won't
get into the details of it here but you
know a very simple model you can think
of is if you've got a number of threads
updating encounter each thread could
maintain its own private view of that
counter and only when you want to
display a result you actually take the
hit of the coordinating and adding up
your your your accumulations so here we
have the four new classes we've added
and it's long adder and double adder and
they're useful for the common common
case where you're just doing sums so
you're adding adding things up and we've
got long accumulator and double
accumulator so if you want something
ordered in summing functions you can
provide your own accumulator functions
so what was one of these looked like
well a long adder has met its to
increment and decrement account at
arbitrary values to the count it's also
got assumed for each retrieving the
result and this is the guy that actually
goes off and coordinates between threads
and gets the current value of the after
counter sum is not destructive it can be
called multiple times if you want to
reset the T count the counter you call
the reset so you can reuse the instance
so here we've got a very simple example
of a lung accumulator and it's trying to
maintain a running maximum value so if
that's of interest to you you can
instantiate an accumulator provide a
accumulation function in this case it's
long max long that max the identity is
the initial value so when the
accumulator is instantiated or gets
reset it gets reset to alum dot min
value the reason for this being that an
accumulation then will will be greater
than this value when it's first
encountered you can actually construct a
long other end of a long accumulator it
may not be as efficient but you can do
it all the same
so a new long adder is equivalent to a
long accumulator the accumulation
function is just assuming function and
the identity will be will be zero
so just to give a better sense of the
performance gains you can get using
these scalable updatable variables over
other techniques likely using the atomic
lamb
we actually have a micro benchmark
that's checked into you to JDK it runs
with a number a number of times with a
number of treads up to twice the number
of cores in your system or twice the
number of processing units that are
returned by java.lang runtime available
processors which may not be exactly the
number of cores in your machine data
reports its results and in the right
hand side we can see just a snippet of
the interesting part of the actual
benchmark so it's the body of the run
methods that actually do some work so
there's some arbitrary code who get all
the treads to a starting gate you get
passed in a shared adder so the adder
here is shared across all the threads
it's a long it's an atomic long in this
case and then because this is a micro
benchmark it just goes into a type tight
for loop and invokes the get and
increment and an eventually returns a
result so the benchmark runs like that
and then it runs again but this time it
uses a long adder it was exactly the
same amount of work we invoke the
increment rather than it gets an
increment and then we use the sum to
retrieve results so on my next slide I
have a little table of results that
shows how great long adder is compared
to using atomic long so just for you
qualified end results I've included this
slide on the machine that we actually
run the benchmark on so it was this
party for his running Solaris 11 it has
eight cores and a transfer core and you
need a lot of threads to demonstrate the
potential savings you can get over
traditional methods
so this table is a output of that micro
benchmark so it runs a number of times
and it doubles its trade each time so it
runs a tree at 16 32 64 and finally 128
threads which is twice the number of
processing units that are available on
the machine so going across the top line
you can see when it runs were eight
threads the atomic lung has an average
of eight increments per per microsecond
the Elat the long adder already has a
large number of increments per second
advantage over the diatomic lung but you
can see as we go down the table and the
number of threads increase the atomic
lung is suffering now from its doing
compare-and-swap and because we have
such high contention contention on this
particular adder it the de compare is
failing quite often so the atomic lung
gets hit really bad whereas the lung
other scales pretty well and well well
outperforms the other techniques like
using atomic LOM so we're moving on I
just want to take you through some of
the enhancements we made in concurrence
hashmap so Mike's gonna talk a little
bit later about the implementation
changes to use to better handle hash
collisions but I just wanted to
highlight here some of the the more
significant API changes which you're
going to notice if you have take a look
at a concurrent hash map
kind of lost where I am here actually
what I want to say okay so yeah so the
lambda J's are brought in operations for
sequential and parallel book updates so
you'll know these are streams operations
and are safe for use with maps that are
being updated concurrently by multiple
threads
there's also a whole variant of order
methods variant of sorry for each search
and reduce so for each performing an
action on every element in the map
searching for the first available non
known result and reducing taking a
transformer function and you can do a
reduction on the entries the keys or the
values there's also a nice addition like
compute of absent and compute of present
an addition ATAR is a new public type
key set view which allows you to view
the keys in the map as a set and you can
actually add to that set as well with
default value so just if you if you go
looking at concurrent hash map these are
probably the change you're gonna see so
it's a significant update to that API so
I want to mention some changes in the
for join area and anyone who was at the
keynote on Sunday will know they'd seen
a nice graph that George Tabb put up so
what I change in line of code if you're
using fork joint and you moved from Java
7 to Java 8 you're gonna get performance
increases again without change in the
line of code so we change implementation
to better handle cases where there are
better handle true pot when there's lots
of clients that submit lots of tasks but
also better handle the creation of
treads so rather than generating more
compensator retreads Teresa threads is
so much better I don't know whether
anyone's actually tried to debug for
joint tasks tasks but if you have you
know it's quite painful
so we've added support for tagging tasks
for debugging and diagnostic purposes so
there's a get fork/join
tag and a set fork joint task tag which
can be very useful we've also added a
common fork giant pool so anyone who's
used for join will know it's not the
most easiest thing to use
you have libraries sometimes are using
for join pools you may be using them in
your own programs sharing of pools
across different contexts is really
really painful so we try to solve that
problem by adding a common fork giant
pool to the platform it's it's you it's
appropriate in most use cases its sized
appropriately depending on the machine
that you're running on tasks that parent
explicitly submitted to a pool will now
run in the fork join pool where
necessary we've got a couple examples on
that over the next few slides so
something you may have done if you're
using fork joint in Java 7 you are for
example if you're trying to sort a array
of lungs and this example is lifted from
the Javadoc so you create a new sort
task you pass in your array and then you
you call the invoke method on the pool
but you'll notice you'll have to you you
have to have a reference to that pool
you'll have to instantiate your own pool
or get a reference somewhere and how do
you know which people you should be
using whether your application is using
a approval in different parts of it this
is just a it's just a common problem and
a source of pain
one solution to to fixing this problem
is to change the signature of your
method but now you're polluting your API
by putting in a fork I'm cool into it
you're forcing users of your library
your API to worry about for time pools
to create and manage them themselves so
clearly this is this is not ideal
so in Java 8 as I said we have a static
common so this example now you can
revert your submitted signature back to
be just taking a lot an array of lungs
which is the actual data that you're
concerned with sorting here and you can
recreate your sort tasks and you call
the invoke method and the interesting
part here is that your task actually
gets invoked in the calling thread and
only ever Forks tags tasks about those
tasks into the common pool when
necessary so the calling thread gets
gets involved in the in the computation
I want to call out a new lock type that
we've added in Java 8 called stamp lock
it's very similar to read/write lock but
it's got a few it's got one significant
difference which we'll have to take a
look at so similar to other lock types
and in basic functionality it allows
multiple readers but only a single
writer if you need to to to do some
writing you need to obtain the lock in
write mode but the interesting part here
is if you want to do some reading you
don't necessarily need to obtain the
lock in read mode so if you're feeling
lucky
you can try what's known as an
optimistic read so we'll take a look at
one of them over the next few slides
so to expand a little bit optimistic
reads are inherently fragile and that's
okay that's their defined semantics
that's that's the way they're supposed
to be after used correctly they can
significantly significantly reduce
contention where you typically have very
few rights but a lot of reads so a
typical use case as I said is short read
only sections and there's a new idiom
you need to follow if you're going to
use stamp lock which is optimistic style
reading and we take a look at that
essentially it involves reading the
fields that are protected by the lock
storing them in local variables and then
later validating the lock before you
actually use them so the optimistic read
is essentially trying to feel lucky that
hopefully no writer will come in in the
point where you read the value of your
fields and the point where you actually
need to use the fields so we'll take a
look at that in action now so here we
have a simple method it's trying to work
at a distance from an origin origin the
lock is protecting to two fields x and y
they're the states that the lock is
protecting the lock here is aptly named
SL that's an instance of a stub stamped
lock so we invoke the try optimistic
read and that returns us a stamp then we
can proceed to read the values of the
fields that are protected by the lock so
these are x and y and we store them in
local variables current X and current Y
now we need to check before we actually
use our fields in any computation that
the lock has not been grabbed in right
mode since we've actually got our stamp
there's one of two things may happen
here of course the lock will either have
been obtained in write mode or will not
if it's not you can proceed straight
away to your computation using the
current value of the fields that you
have already stored in current X and
current Y because you know
they cannot have changed from when you
grabbed your stub if the validation
fails it means that the fields may have
potentially changed because somebody has
grabbed the right the right lock in
between your try optimist agreed so we
end up doing in this case is we fall
back to the pessimistic read so we
actually may block here too to obtain
the read lock so in this case if you
actually obtain the read lock you've
done slightly more work than you would
have done in the case of just grabbing
it in the first place what does it say
if you've got very infrequent writes
these up this optimistic style can
actually be beneficial because you won't
you won't have to fall into the the
actual read lock at all so you don't
need to take my word for it
dr. heinz kibbutz compared the
performance of stamp block to it's clear
in to its closest rival ireadwrite lock
in every case it's performed very well
the implementation was stable and in
most cases it outperformed readwrite
lock quite significantly and link there
he has a paper
he's a paper on that I guess one of the
the things his conclusions was that the
idiom which we looked at in the previous
slide is actually very important if you
don't follow the idiom you you'll leave
yourself open to steam seeing stale
values of the fields that you're trying
to protect so following the idiom is
very important so that's all I want to
stay on stamped lock ordered into just
to add that it's the only lock type in
the JDK that's offering this optimistic
style locking note or lock lock type in
JDK officer when we move onto
completable future and completion stage
which are actually pretty cool little
classes so a completion stage offers
fluent completion completion
style capabilities so a stage represents
an individual stage of computation and
allows you to change stages together
quite easily
contain your stages together and have
them dependent on one another they can
dependent on a single stage or a number
of stages you can chain your stages
together that they execute synchronously
or asynchronously and the actual
computation of the stage can be
expressed as an action or function with
functional interface so just to take a
very very simple example you can see the
fluence tile API and the chaining
together of the stages so here we have a
stage aptly named stage and its result
is passed to a function that computes
the square in turn its result is passed
to a consumer that prints it and then
finally we print a new line now all of
these stages get executed synchronously
so they execute in the same thread but
there is variance of each of these and
more methods in completion stage that
that are called then apply async then
accept async then run a sync and they
can run asynchronously so a completable
future is a implementation of a
completion stage and it defines the
policies and the mechanisms around the
execution of stages the cancellation and
the behavior of the stages particularly
when they they run asynchronously so a
completely whole future will run any
asynchronous operations in the fork/join
common pool so as the name suggests a
completable future is you're able to
complete it explicitly with a
proprietary feature in that it's got
simplified usage so the joint methods to
get now method
do not throw checked exceptions they
throw a completion exception which is
actually unchecked so for those of you
who don't like checked exceptions this
is even a good reason to move over to a
completely future or other future types
so that's all I wanted to say on the
changes that were brought in under GF
155
so Mike will take you to a defenses API
and a remainder of the presentation good
afternoon
so the fences API is a bit like
contended in that it's an unsupported
API that was added to the Java or Sun
Muskaan safe package this was an API
that Doug Lee has been working on trying
to standardize in both Java 7 and Java 8
as part of the wunens jsr 166 group in
eight it wasn't added to a public API it
remains a private API but it was
nonetheless needed for some of the
features that are actually elsewhere in
Java 8 the long adder and counted
completer functionality actually take
advantage of the fences API so it had to
go somewhere and because of the
difficulty in standardizing this
particular feature and getting agreement
from an expert group or even finding an
expert group that was willing to take on
this particular feature it is right now
only private the basic idea behind the
fences api is to ensure that reads and
writes from memory are visible across
threads there's some standard Java
mechanisms for doing that that we're
familiar with like synchronized or
volatile and code example to the left we
have a volatile Y and anytime that the
the Java memory model says that when I
write to Y any prior changes to X will
become visible all threads you can also
achieve the same effect by ëget exiting
a synchronized block and having
consistent behavior with regard to the
visibility of Rights across threads you
know is absolutely critical and you know
to be able to write coherent programs
but let's say on the other hand that you
didn't want to declare Y volatile what
technique which you use in order to make
sure that it was visible to other
threads other than the writing thread
and that's where the the new offensives
API and
comes in and this code sample on the
right isn't is what you have to add in
order to ensure that the rights to X are
visible on other threads so now we have
Y without a volatile and we have to add
the store and full fences in order to
sure ensure that the changes that are
made on any given thread are written out
to main memory and the caches are
updated so that it's visible to other
threads you know one question is why why
would you want to do this why not just
use volatile well and that comes back to
the implementation cases so for example
where you have long adder where you're
in an individual thread doing
accumulation and then only at some kind
of publication event like the get or
some do you share you know to combine
the results from multiple threads you
want to avoid having to do that volatile
writing when you're the only thread
updating a particular variable so in the
case of something like long adder you
would kill all the benefits that you've
got of doing part of the addition
operation on a local variable by
declaring it volatile so the fences API
fixes that in general this is not an API
that we really expect anyone to use in
Java 9 or 10 it's not clear that it's
been decided yet
the var handles proposal and new
volatile semantics that Doug Lee and
Paul Santos are working on will probably
replace this API it would be much easier
to replace this API if in fact the only
consumers of it are the code that Doug
Lee's written in the Java util
concurrent library so kind of like can
like contended you this only under
conditions where you have absolutely no
alternative and hopefully be very
engaged with what's going on in the
development of open JDK so that you can
be sure that what replaces it meets your
needs and also you know when it's coming
and what it looks like
so we'll move on from the fences API to
something that you should hopefully in
fact encounter which is hash map balance
trees as Chris mentioned before the
implementation of hash map has changed
in both 7u6 and later and in java 8 in
7u6 and later the change was optional he
had to opt into the change and in Java 8
it is mandatory and the change is based
on the the problems that the performance
of hash map both the Java util hash map
and and Java util concurrent concurrent
hash map is contingent on the quality of
the hash code function that's provided
by the objects that are used as keys so
if you have a key class that you're
using in your concurrent hash map that
has a hash code implementation that
returns 3 for every key the performance
of your hash map is going to totally
suck and there's nothing that concurrent
hash map can do in order to fix that you
know most hash code implementations
aren't as bad as returning 3 well I like
the fact there are a few that are worse
than that because they return zero and
zero is the absolute worst thing you can
return for a hash code because of the
way it's mapped relative to mappings of
null but in general first step and and
this would have been you know if this
had been done it would have eliminated
the need for this change at all is right
good hash codes look at the distribution
of your hash functions and it's actually
important important for the performance
of your application we can't you know
turn a sow's ear into a silk purse if
the hash code that's provided is bad so
what happens if you've got a lot of
collisions well a few hundred collisions
in a single hash bucket produce a linear
look up because the entries in a given
bucket in a hash map are stored as a
linked list and and if you've got
several hundred in the same bucket
things are going to be really slow
the resizing of hashmaps
is normally done on the overall capacity
of the hashmap so that the hashmaps
resizing strategy could say Oh
everything's just fine I've only got
half the buckets full but if everything
is all in one bucket then it's in fact
going to be very slow entirely so what
can defenses can you have against these
bad hash codes well the first is data
scrubbing and you may have recall the
vulnerabilities that were encountered by
various web servers both Java based and
C based implementations on HTTP headers
and the attack was is that people would
send requests with hundreds and hundreds
of HEC of HTTP headers which had
carefully chosen names so that they
would produce hash table collisions for
whatever hash table they were attacking
whether it be the one that said Apache
portable runtime or Chavez or pythons
etc in this case it turns out that it's
unreasonable to have hundreds and
hundreds of headers on an HTTP request
if you design your app that array it's
wrong as well but the fix was to just
restrict the number of headers that were
allowed on a given HTTP request if they
could keep the number of headers down to
a couple hundred than the chance that or
under a hundred the chance that they
would rock again at this linear lookup
behavior was extremely low as I
mentioned you can also fix this by not
writing better hash codes if you control
the ability to write the hash codes that
works great if you don't then you're
still contingent on your users
providing you both good hash code
implementations and providing data
that's good as well some of the attacks
were in fact against string which is one
of the JDK classes and if not that
strings hash function is necessarily all
that bad but it's easy to find
collisions it was easy for them to
locate collisions this produced that
this behavior
you know the last couple strategies are
to lower the density of the hashmap and
this only works in what's called the
partial collisions case sometimes the
attacker doesn't bother to figure out an
exact collision so two values that
produce exactly the same hashcode all
they care about is that the two entries
arrive in the same bucket so if the if
the bucket decision is done by let's say
division if you have two numbers that
result in the same or two hash codes
that are divided by the same size of the
table for example and they end up in the
same bucket well that's good enough they
didn't have to be exactly the same value
however if you change the size of the
hash table to lower the density you know
half the entries may now show up in a
different bucket so raising the size
your hash tables can slightly improve
this is a very temporary stopgap you
know you're going to be in this kind of
the same orders of magnitude regarding
size but it may be slightly better the
last option and this is what this chip
actually does is pursue a different
hashing strategy in the case of Java 7
this is in fact what we did looking and
Java 7 solution handled it only for
Strings and was opted in only so when
you had a hash table or sorry a hash map
that contained a string as a key rather
than calling the string hash function we
in fact called a different hashing
function we use the function called
murmur 3 as it turns out that that hug
hash function was broken by different
hackers a couple months later but it was
only partially broken and the behavior
was sufficiently better enough and it
was difficult difficult enough for them
to find collisions that the it wasn't a
much consequence that they had in fact
broken murmur 3 it was a better enough
hash function over the standard Java one
and close enough to optimal that they
couldn't exploit it in a particular way
after the solution that was used for
Java 7 to replace the state of the
string hash code
or for just string keys Doug Lee came up
with another solution which did the same
thing for all all keys which were of
type comparable and what it does is
rather than using a linked list in the
buckets it uses a tree to organize the
colliding entries so that when you're
looking through all the collisions in a
particular bucket they're ordered by
their key sort order so this produces
order log and lookups if you're looking
for a particular key rather than the
order and lookups that were there if you
just used a linked list inside the
buckets and again this works for any
comparable key type so this would
include the integer long string you know
quite a quite another you know a large
set of the common key types that are
used in JDK and if your data type is in
fact comparable it'll work on user
provided data types as well if your key
type could be made comparable but isn't
currently it's advantageous to in fact
do that so that if somebody is attacking
your use of a hash map by you know
providing degenerate data that causes
collisions if you provide comparable
behavior on your data type then the hash
map implementation will in fact use that
to do tree buckets rather than linked
lists so the solution for Java 8 is is
on by default there's that in fact no
way to turn it off and this is an
important difference between what's done
in Java 7 and Java 6 or sorry Java 7
News 6 and the reason we were cautious
in I'll skip over that because I talked
to some of it already the reason we were
cautious in Java 7 is is that changing
the way the hash function works or using
a different strategy for entries inside
a bucket will and
change the iteration order of entries in
the hash table if you haven't if you
have a hash map with thousands of
entries you probably don't care about
the iteration order however we found
that in quite a number of cases
particularly with very small hash maps
like for entries for things like service
loading so you open a jar find the list
of services and then load them it turns
out that if for example there was an
order of initialization dependency
between those services and there was a
particular iteration order which is used
people may not have noticed that there
was a an initialization order dependency
if the iteration order now changes and
the services are initialized in a
different order as uh-oh now they don't
initialize because they're initialized
out of order and was kind of surprising
that it wasn't the big hashmaps that
people suddenly cared about the
iteration order being a problem it was
the really small ones and we also found
that a great number of our tests did
things like compare results or match
results based on a particular iteration
order after performing some tests and it
wasn't an implementation that was a
problem it was a weakness in the test
design that said this is how I'm gonna
check the particular results of this
test to see if they work correctly as I
got a glossed over on the previous slide
I'll go back to it for a second which is
that the behavior of switching to tree
based storage for 4 keys it is done at a
particular threshold right now it's at
16 keys so if you end up with 16 entries
in the same bucket
it'll swap over from a linked list to a
tree below 16 entries it doesn't have a
much performance benefit and there's not
really you know order order and on 16
vs. order login on 16 is only a couple
of operations it's not worth the trouble
to you know reorganize your data for
just that small amount of stuff how
often does this actually come up well
the most common case is that it comes up
are where the hashcode implementation of
the objects that are you being used as
keys is in fact weak this is not usually
an intentional thing this is just sloppy
programming or bad design the other case
is you know degenerate data like you
have a test data that you know has all
thousands of strings with the same value
or very similar values that that hash to
very close numbers and in the last case
is that in fact attacker case which
hopefully is quite rare where somebody's
providing colliding data to try and
attack your map
hopefully that is encountered and if you
in fact are using some of the other
countermeasures from beforehand like not
unreasonably accepting a ridiculous
number of user provided keys from over
the Internet or an arbitrary number of
keys and say okay I'll take a million
headers don't do that and it won't be a
problem anyway so in addition to the
extensions that we're done on java.util
collections in java 8 for the streams
features which are covered elsewhere in
a number of talks one second there were
some additional features which were
added to that collections libraries some
of which were long-standing requests
some of which are convenience features
some of which are opportunities to
improve performance as most of these
features are added as new default
methods again this is a language feature
that's now available in Java 8 for the
first time we can extend interfaces to
provide new functionality that wasn't
previously available default methods
worked by they have the ability to call
other methods on the class and provide
an implementation for a method that was
not previously there
so some of the things that they've
allowed us to do is for example provide
a sort method on list if you use the
collection sort method that was
available prior to Java eight on a class
like ArrayList and and you looked at the
implementation you notice it did some
strange things the first thing it did
was take a two array of the list that it
was attempting to sort sort that array
and then copy everything back into the
list the reason is is because it didn't
assume that the array was in fact backed
by a list and it could have in fact done
class introspection and said all this is
an ArrayList I know how to sort this but
that was kind of deemed too specific and
kind of too unfair to third party list
implementations so that was not done now
in Java eight we can in fact provide on
ArrayList a optimized sort
implementation that in fact sorts the
array in place so that copying stage of
where it does two array is no longer
done for ArrayList we get you know a
nice performance bump for almost free
from this particular behavior but the
default sort method on the list class
essentially does exactly the same as the
old collection sort method did you know
so that it has the general behavior that
we want for all implementations as safe
and stable and works so you know we've
been had an opportunity to move forward
with this what else can we find on the
collections well there's some really
useful things with comparators there's
some new Combinator capabilities for
building comparators is not much easier
to build complicated comparators than it
was before and a variety of useful
functions across the other classes map
in particular has quite a few new
interesting things some of them are
functionality that you probably thought
was pretty useful like put if absent
that was on concurrent map but was not
on map the reason it was on concurrent
map and not on map is there was just no
way to add it to map without breaking
map implementations that didn't happen
to have a put if absent method
prior to Java 8 so we've been able to
move some of the stuff from concurrent
map back on to the general map interface
we have in fact an entire other session
on this on Thursday afternoon the very
last session in the conference looking
at these methods in particular and some
very useful things you can do in your
code if you're not ready to move your
entire streams our source base to using
streams there's nonetheless some very
cool things you can do to bump up the
performance if you're using Java 8 like
the sort methods and some other stuff on
map elsewhere it also provides a lot of
code clean up some of the boilerplate
that you're really used to doing in Java
7 and below like you know put if add
absent idiom where you check to see if a
key is present in a map and then if it
is then you do something different
otherwise you create a new ArrayList add
that as the value for the key that's all
gone you can just use methods right on
map now in in Java 8 so there's some
very useful stuff and we'll talk about
that on Thursday afternoon but for now
we can take some questions go ahead I'll
repeat the question for the mic or
crystal because I can't hear very well
today
so the question is what the difference
is between long adder
and atomic long yes Tom atomic long is
innocence JDK 1.5 there's six JDK six
line outer is just new in Java 8 in Java
eight slung a direct performs using an
atomic law for that scenarios we showed
so I think we probably have to wrap it
up and we can probably take questions at
the front or something because it's just
too noisy</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>