<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Science with R for Java Developers | Coder Coacher - Coaching Coders</title><meta content="Data Science with R for Java Developers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Science with R for Java Developers</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zoqLSiQTgAw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well thank you all for joining me in
this session on data science with our
for Java developers and I'm glad to see
so many of you because I fully admit
that this topic may be yeah kind of off
the beaten track for a Java one but I'm
a Java developer I presume you are old
Java developers so it will be exciting I
think to explore a kind of a new topic
my name is Salma Mac I work for luminous
technologies in the Netherlands and we
create all kinds of cool Java based
products and one of the things that we
encountered during this product creation
process is that over the past few years
we've created lots of applications that
were all about managing data and making
sure that everything is consistent for
your users making sure that business
rules are applied and all very important
things but users tend to want more
because they all use Facebook they all
use Google they all use Amazon and there
are all these fancy things like
recommenders and predictions based on
past data and all kinds of things that
are learnt actually from the data rather
than just managing it and as it turns
out if you want to do this in your
application of course you can and the
first thing that you will reach for is
of course Java and because we all know
Java we all love Java but if you really
want to get into greater depth and we
really want to learn from your data
you're not going to just have to
implement something but you're also are
going to need to explore your data and
really get a feel for your data and to
do that it's good to have another tool
in your tool belt and one of these tools
is our we're going to talk about it
today and the other one a field that
we're entering is sort of called data
science it's a bit of a fluffy term
we'll get back to that but it's what
it's called right now and it's worth hot
it's it's results so data science with
our and all from the perspective of a
Java developer because we all are in
Java developers so the agenda for today
is to first look a bit more at this data
science term what does it actually mean
the
even mean anything or is it just nothing
new and a fancy name for something that
already existed after that we'll dive
into the our language which is pretty
different from Java you'll see it's a
statistical language and lots of people
then ask me ok is it the language that's
going to lie to me half of the time no
not that kind of Statistics it's it's
really a general-purpose language but
with lots and lots of libraries for
doing a statistical analysis and doing
all kinds of things to learn and to
analyze your data so after we've seen
our I guess you will be a sort of shock
because it's quite different from Java
so that we'll get back to Java and see
how it integrates with yeah the
ecosystem that we know and how you can
get the advantages of our in a Java
based environments or not I mean it's
not a given yet so why all this focus on
data I already mentioned the Facebook's
and the googles of the of this worlds
and and how they heighten the
expectations of users of our systems and
there's a statistic that I found in a
study I'm not sure but it's really
accurate but it kind of gives the feel
and the trends that's 90 percent of the
world's data was produced in the last
two years
well the nice thing is it's even
published before all the NSA revelations
so I tend to even believe this now but
it turns out that all this data is not
just there to perform create read update
delete on I mean we really want to get
to the bottom of what's happening we're
gonna see parents who want to predict
new data based on the past data we want
to help users get a really rich user
experience based on the data they're
creating either explicitly or implicitly
so this is where the data science is
coming in and it's it's a term that's
gaining traction last I guess some two
years or something and and I personally
don't really like to I mean data science
is it about data oh yeah it really is is
it about science well not really I mean
you can make it a science
and you can perform experiments and you
can test hypothesis etc etc but most of
the time when people talk about data
science that just really mean I'm
exploring a dataset I'm trying to make
sense of it and you could sort of see
data science but it's not really a
scientific pursuit in my opinion but
anyway we have the term it's there and
it's there to stay I guess that's that's
if the beasts name is data science so we
already have things like business
intelligence and all the kinds of other
analysis rules in companies and these
are the same it's a difference it
depends on your perspective in my
opinion data science does bring together
a whole lot of expertise areas that are
not really in those original roles so
there is something new to it but it's
just a combination of a lot of things
and in particular if you look at it some
more then you will see that there are
actually three big domains that are
merged when you want to do data science
when you want to perform data science
and I think the one that appeals to us
most is of course the software
engineering part in order to do
something useful
you have to automate it you have to not
just do it by hand and say okay that's
my prediction for example for this user
for what's going to happen next or what
do you likes or doesn't like now we need
you need to create software but you also
need to know what you're talking about
but you're doing something in the
aviation domain or you're doing
something with medical records it's not
just all data it has meaning as well so
domain expertise is really important as
well if you want to get to the bottom of
your data now the combination of these
two fields often is also called machine
learning recreates using saw for
engineering algorithms to encode domain
expertise and to create for example
recommenders based on this domain
expertise then on the other hand of
course lots and lots of data means you
can't
read everything yourself you have to
apply some sort of statistic student you
have to summarize them you have to well
just really work with them in aggregates
so amethyst at this six background for
data scientists is also yeah really
important and then you also have people
are really good at math and statistics
but not that good at server engineering
so that's the sort of danger pearl hats
the zone in this graph so there's a fine
balance between all these three and I
encourage you to find out by yourself
what your strong suits are and and
what's what's your weakness and try to
improve and that's because the
combination of these three expertise is
really really powerful and it can get
your applications really to the next
level in the sense that you're not just
reckoning data but you're doing
something with it there's a question
of course it is both for machine
learning they're often really already
some algorithms predefined that you you
have to prepare your data you put it
into the algorithm and you only have to
pick and choose which of your data is
important for its algorithm you're not
really creating the algorithms yourself
if you're a researcher of course you do
create new algorithms but machine
learning is currently I think in the
adoption phase so to say so you have
lots of libraries and you have your own
data and you have to adapt these to
using your software engineering skills
and of course you need to know some math
and statistics and statistics I agree
but well gotta draw the line somewhere
so a nice definition that I found that
data scientist is a person who is better
at statistics than any software engineer
and better software engineering than any
status issue nice ok anyway what do you
got to do to become proficient in the
field of data science well here's some
map somebody made this I didn't credit
where credit did you so there's really a
lots lots of subfields and you're not
supposed to read this it's just to
indicate that there's lots of topics to
cover and I don't think there's anyone
who's an expert in all of these fields
anyway but they all do have something in
common you actually need some tools to
apply all this knowledge and to apply or
all these data science subtopics and
that's where what we're going to look at
today just a little part of this map the
language our and our studio which is an
IDE for our which so be showing as well
so just to put it in perspective I can
show you just a teeny little bit of
what's what's really happening in the
field and like I said nobody nobody is
an expert at everything so don't feel
discouraged which brings us to our
second topic the actual our language and
like I said it's pretty different than
Java so first I'll do I'll be
contrasting the our language to Java
piece by piece and after that I also
gave a demo just to give sort of a feel
what what the language is like and what
it can do for you and hopefully that'll
yeah give and give a good impression of
what's possible and I will move on to to
integrating it with the actual Java
environments that we use every day and
see what options are there so first a
warning I mean where Java developers we
work with Java and Java is really a
language that's designs it's everything
is on purpose mostly there's an
architect for the platform there's a
distinct feel to the Java language
that's that they also try to preserve
and I think they're doing a good job and
Jabba death and being a language
designer is really hard in my opinion
now on the other hands are has none of
these qualities it is started somewhere
in the 70s 80s I guess but to
statisticians from New Zealand and they
did a pretty okay job as creating a
environment that was suited to their
needs and it was programmable using the
our language but I really can't find any
sort of design or overarching principles
in the language so that's going to bike
you if you're going to use are and you
will curse at it and then you think ok
well but there's a lot of other things
really well for me so ok I'll just take
it so there's there's a little overlap
between language designers and
statisticians it seems unfortunately so
until this mythical creature is
somewhere in the middle emerges I think
we'll have to make do what with what
what's available and are is really one
of the top choices currently for doing
yeah statistical analysis and machine
learning on your data so there's another
nice quote that the best thing about our
is that it was developed by
statisticians and the worst thing about
our age it was developed by Stella's
dishes so there you have it so why
should you our use our then
well I think currently here you can say
that has it has become the defector of
standards for statistical research so
what does that mean for us as developers
well it does mean that every new machine
learning algorithm every new statistical
test that thought out by our actual
scientists is actually implemented as an
our library first because most of the
statisticians have published papers and
they also publish there are
implementations so you can get really
access to cutting-edge code and to
cutting-edge libraries in that sense
it's also open source so it's free as in
beer and it's free as you can do
anything you want with it
that makes it for a nice environment in
the sense that there's also lots of
alternatives but they're mostly
commercial and not cheap either what's
really nice about our is that it's not
just a programming language it's
actually an interactive environments for
data exploration and I think the demo
later on will make that a bit more clear
but you shouldn't really think of it
just as a programming language because
you'll be disappointed because it's not
a really great language anyway and it's
also not meant to be a really
general-purpose language even though you
can do lots of things with that you
should really build an HTTP server in it
or do anything like what we're doing
most of the times building business
applications it's really almost into my
specific language posing as a
general-purpose language in that sense
so that makes it really powerful but
just for the errors that we're talking
about today it's not really something
that you should learn as a general tool
so why shouldn't you use it well it's
slow I mean it's really slow it's it's
interpretive and it's not just slow it's
also memory balance in a sense that's
all the data that you want to analyze
has to fit in your actual Ram so if
you're doing exploration that's not not
really an issue if you're going through
production and you
to process lots and lots of data
obviously that's going to be an issue
and we'll get to that as well no the
really nasty thing is that googling for
our tips and our help it's really hard
and we should have picked another name
but anyway and they don't mention this
your quirky language I mean it's full of
what and there's even a really nice
documents if you're into this sort of
thing called the re inferno if you're
using R and you think you're in Hell
this is a map for you it's in the
preface so that should make clear that
nobody uses this because they really
like the language but they use it
because it's powerful so if you want to
use are you go to the websites and
you're greeted with this really nice
1990 look it doesn't matter you just
downloaded this in your soleus and I
start using it it doesn't matter that
statisticians are web designer Tyler I
don't care I've been here once I guess
so
let's contrast R and Java as we've
learned already there are pretty
difference in the purpose but if we look
at the language itself we can also see
lots of differences so here at the top
you see just the our command line the
our interpreter you can just fire it up
from the command line and you get an
interactive environment which you can
just evaluate our expressions of course
that's pretty different from Java where
were used to compiling and having byte
code generators etc etc R is much more
of a dynamic language so this also comes
with all the downsides of them dynamic
languages of course using Evo is really
not something that you should do very
often and you don't have the type system
that catches error errors for you so
there's all kinds of thread of between
static and dynamic languages which I
won't go into now you'll have to see for
yourself the interesting thing is that
the paradigm of the language is also a
pretty difference Java object oriented
of course and are yes it's sort of a mix
and this already shows that there's not
really much of design going on there
firstly it's mostly a functional
language so you can pass functions
around us as first-class citizens
there's also sort of an object system
going on there actually they have more
than one object system going so it's
nice and they don't interoperate of
course but actually that's not really so
bothersome in the sense that if you're
writing your scripts yourself you mostly
don't need to use the object systems
they're mostly geared towards the
library authors and using the libraries
again it's a bit less painful than
writing the libraries in that sense so
looking at just from a data type
perspective R has a numeric numeric data
type which roughly maps to what we have
as integers doubles etc there's a
character data type which is similar to
the string in Java and another
interesting thing is that you also have
a counterpart for enums so for short
lists of the distinct values and they
call this and these factors for example
this is really comes from the statistics
world in the sense that if you have a
questionnaire and there's a question
asking for example a sex male or female
then it doesn't make sense to encode it
as a string but you really want to have
this as a a choice and you can trace
back to the actual values so there you
will use a factor in our well then
there's built-in data types that do not
really have a counterpart in the sense
that's turned out the native types in
Java of course we have lists and factors
in Java and we can build everything want
using classes but in our these are
really built in data type so vectors are
lists with elements of the same type
lists are lists with elements of
different types and data frames are sort
of like the relational tables that we
know from databases so you have a header
and lots of entries with different
columns another very annoying difference
between r and java is the h of Debates
0 base vs. 1 based indexing and well
java is very
clearly in favor of zero-based indexing
our indexes everything from one and this
is also something that's historical in
the sense that most statisticians label
the datasets using one for the first row
which kind of makes sense and so on so
everything in our is just one base of
course makes for really nice off-by-one
errors if you're coming from the Java
backgrounds but fortunately if you're in
a functional language you don't often
write for loops and use explicit indexes
there's also higher-order functions for
example the s apply function in this
case while you map a function over a
vector and you just increase it and you
don't have to worry about the indexing
of the vector the higher-order function
takes care of the iteration so that's
that's sort of not really an issue with
something you should keep in mind when
you're working with art then there's
eager evaluation of function parameters
versus lazy evaluation in Java all
expressions that your pass through into
a function are really evaluated before
the function call is applied in our not
so much in our everything is lazily
evaluated so if you pass in an
expression it will not be calculated
until you're actually in the function
and you're using it and they also have
the concept of pass by value so if you
input a parameter into a function it
will actually receive a copy of this
value rather than a pointer or reference
to this value like we have in in Java
for object references so you might see
rethinking okay if you have large data
sets it's really bad of course to keep
copying those if you keep keep calling
functions with the same parameter and
indeed there is an optimization going on
in R which is the the modifier on writes
optimization that is if you pass in a
value it's not directly copy copies for
this function but it's actually only
copies
once you try and modify it
so still something to be aware of you
can leak lots and lots from memory if
you just pass a lots of big data
structures around and you modify them so
it's really different than in Java if we
look at the surrounding tooling then I
think we as Java developers really yeah
got a good in the sense that we have a
choice between three really great IDs
and of course every everybody has their
own preferences and and that's that's
okay but if here when our world then you
only really got one choice which is our
CEO and it's it's better than just using
VI but that's about as much as I can say
about it there's only one choice and
having only one choice isn't really a
choice I guess on the other hand if you
look at the management of libraries in
dependencies etc it's it's sort of
similar between R and Java in Java we
have maven to automatically retrieve
dependencies to manage dependencies in R
we have the comprehensive our archive
network and that's really just a central
server which makes all the interesting
are libraries available and it has about
I think currently five thousand packages
and those are all really high quality
and diverse packages and all geared
towards data analysis some are general
purpose but most are really geared
towards data analysis and statistics so
and the story there is pretty good
actually
well I think it's about time to to leave
the slides and to switch to our studio
and see some actual R code for ourselves
so let's do that
what I have here is in the instance of
our studio and it actually has a few
elements of course here is the actual
interactive console in which I can just
type our codes and evaluate it
there's also a source editor which is
linked to the interactive console I will
get back to that later a bit later in
the demo for now just work in the in the
console and there's also a notion of a
workspace in our and that's an
interesting thing in a sense that it's
not just evaluate something and then
everything is forgotten now our really
keeps in this workspace all the
variables that are current and the
current values etc etc and you can even
save your workspace and sterilized it
and send it to a friend or open it later
and get back exactly in the same state
that you were before
so really it's it's an interactive
environments and not just a programming
language I've even heard some people
describe are as a sort of an headless
Excel version on steroids of course
because it can do much more but that's
that's actually a pretty nice way of
thinking about it it's just a
combination of data and actual
algorithms that you performance data
that you combine and that you can save
and share so let's start very easy the
obligatory hello world and as you can
see it's just an expression and it will
just echo the evaluation of the
expression but actually notice there so
this is one in front and you might be
wondering okay what is this is it for
everything yes if they had just have an
integer it also put in one in front and
this is because our doesn't really have
single scalar values in our everything
in the background really is a vector and
in this case if we have just a single
value then it will be a vector of length
one and starts index one but we can also
create of course vectors with more
values for that we can use the C
function for concatenate and in this
case I can say okay give me these values
and you'll see that is giving me back
again the factors
index one with the values 1 2 3 now it
gets really big and I'm using the range
syntax here to create a vector you'll
see that it prints the indexes that's at
the first line of at the first column of
the line so you can easily see where you
are in the ineffective vectors can be
decomposed in a sense that if you want
to get an elements and just say okay
give me the first elements but what's
actually going on here so of course we
don't have scalar values so this is
actually a vector as well so if I want
to get the first two values rise haos
I'll just pass a vector of 1 and to give
me index 1 and to make this media value
1 or 2 so actually I should really
change this to 4 5 6 to show that it
really gives back the elements here and
the interesting thing is that I can also
negate and say for example ok give me
everything except the first yeah sure ok
it's better how clear is a bit more
often than I'm sorry what's the question
the question was what I could clear the
console so the text could be higher up
so I'm going to do that more often than
now of course just having expressions is
not going to be enough we also want
variables and if I want a variable it's
pretty easy I say I want some variable
and I want to initialize it with a value
notice the arrow not the is equal sign
and that we are used to you can also use
it but it has lots of edge cases so
really stay away from it just use the
arrow and you'll have a variable and the
nice thing is that you also see that
this variable now occurs appears here in
the workspace and I can inspect it and I
can change this but I won't do this so
you can see that your environment is
building up as you go
also variables never need to be declared
they just come to into existence when
you use them so that's only one that is
very handy on the other hand it's very
very dangerous but well that's kind of
the story of ours life now if we have
factors and everything is a factor
inside our that is also good to know
that all operations and functions most
of the built-in functions in our
vectorized and what does this mean this
means that if I have a factor I can just
for example add them together and it
will do the right thing it will do a
pairwise addition and all functions that
you would expect to work on single
values also work on vectors of values
there's a question in Bank if I pick
something out of range of a vector for
example I have a vector of size 1 and I
do too I will get n/a back not available
which is code for okay it's nice you
want to value but it's not there yeah so
factorize everything and this also means
that if you for example add two very
large factors it will under the hood use
a very fast implementation to do this
using a native library and native C
library so in that sense there are some
speedy parts of our but most of it is
just interpreters and kind of slow of
course since it's a statistical language
you can also really use built-in values
built-in functions to do things like
give me the mean of a list of factors or
the standard deviation if you assume a
normal distribution with mean 0 and
standard deviation of 1 then it it will
be 3 times 3 times bigger than that
for this list and there's lots of lots
of built-in functions for example give
me 10 normally distributed random
variables okay there you go do it again
you get different values to clear again
so of course normal distribution is very
very widely implemented in Java and Java
and libraries as well but our really has
tons of different statistical
distributions for example you can do the
Poisson distribution give me ten values
from the Poisson distribution with a
rate of five and you can see that you
get positive integer counts so the
Poisson distribution is really used for
things like fishes accounts and all
non-negative kind of accounts and
distributions and it's all in there and
it's all by default
available and of course you have to know
what it means and you'll have to study a
bit for that but the fact that's there
and that you can use it really can make
you really productive
now our also has some built-in data sets
that you can play with for example a
very very simple one is the letters of
the alphabet which is of course nice we
can play around with that we can say
okay give me a random sample of these
letters and well give me ten for example
now you'll get a random sample if your
data set you could do it again and get a
different random sample yes support for
different languages for the alphabets I
guess not I haven't checked but now it's
pretty pretty anglocentric here in the
sense yeah so of course when you're
doing all kinds of experiments and doing
random things you want to make it
predictable and repeatable so you can
set the seed of the random generator in
advance and if you now go back and ask
for a sample of ten letters and you
reset the seat to the same value and you
again ask for a random sample of ten
letters you will get exactly the same
sample so that's really useful if you
want to do something some repeatable
tests and want to check whether
everything is is going according to plan
now just to underscore this R as a
functional language we're going to you
a supply function as well on this
letters data says letters of the
alphabet and I'm going to create an
anonymous function then I'm going to map
over each individual letter and what I'm
going to do is it has a parameter for
example elements I'm going to call two
upper on these elements and it will give
me back a vector with all uppercase
letters it's that easy
as you can see also gave names to the
entries of the vector so in a sense it's
sort of a hash map as well and the
previous factors that we saw the names
were just unset so didn't show them in
this case he had the original values
there for from the previous vector now
of course we can write this shorter
because we're just applying a function
to all elements we can also pass just a
reference to this function to apply to
upper I'll get the same result and just
to show the point about factorization
I'm going to just call to upper which
really upper case is just a single
string but I can just also pass this
vector of strings and it will do the
same thing the only difference is that
here the names of the vector will be not
be set that's something that I supply
does best for you so this is all very
nice of course this is all kind of basic
if we look at some more advanced
examples we can for example look at the
empty cars data sets which is a data set
that contains a lot of information on
different cars and they're a property so
well this doesn't really tell us
anything is just lots and lots of text
so we can get the hash of this data
table oh and we can see that it's a data
table containing the name of the car and
miles per gallon number cylinders etc
etc horsepower all kinds of attributes
so if you would like to explore this
data table a bit more you can ask for a
summary of the empty cars data table and
it will tell you for each variable for
example what the minimum value is what
the median value is the mean and the
maximum so there's really a quick way to
get a feel for today
you loaded into our now of course text
is nice but pictures are even better so
let's look at for example a simple
plotting function called Q plot and in
this case we want to select from this
data table a single column in this case
we want the horsepower column and we're
going to plot it against a different
column which gives us the miles per
gallon so just what would we expect we
would expect that cars with high number
of horsepower which also have a pretty
low miles per gallon figure
so hopefully the plot will show us that
unless you can see it created this plot
and let me make this a bit bigger
hey indeed you can see the high might
will get a mile per gallon have a low
horsepower and cetera and there's a
trend line going over you and if we want
to quantify this this correlation
between horsepower and miles per gallon
we can even ask our to calculate the
correlation for us so in this case you
want to calculate the correlation
between the horsepower and the miles per
gallon and this will give us a figure
between minus 1 and 1 minus 1 means
there's a negative full negative
correlation zero means there's
absolutely no correlation between 2 and
1 will mean there's a positive
correlation so in this case we're pretty
close to minus 1 indicating yeah there's
actually really a negative correlation
between these these values and just to
make this visual as well we can add to
this plots a geometry in this case it's
called a smooth geometry and we say that
the method and here you can see that you
can pass named parameters to functions
as well in our the method for this
jimin's geometry is a linear model which
means it will try to find the line of
best fit for this data and actually if
you have this such a linear model you
can already do some kind of prediction
given one of the parameters
what will be the failure of the other
parameters and there's also a confidence
interval in which you will end up so
that's really just with one line kind of
powerful now just to make it a bit more
interesting what about if we color our
points using the number of cylinders can
we see something interesting that and of
course I'm going to need to do Q plots
here instead of the correlation energy
would expect the high miles per gallon
have art of four-cylinder cars but as
you can see it's sort of a sliding scale
here and we already know there's no not
really a sliding scale of the number of
cylinders in the car is really a
discrete you have either four six or
eight in the case of this data sets so
let's convert this number of cylinders
to a factor which is sort of like the
enum that we knew in Java so as factor
on the cylinders and then the plotting
function will recognize okay we have a
factor so I'm going to pick three
discrete colors and we can see that we
have the four-cylinder cars up here the
six-cylinder cars up here and the acing
of the cars down here with lots of
horsepowers and very low miles per
gallon so actually you had a line of
best fit right between these these cars
but just because that there there is a
line of best fit doesn't really mean
that you can actually use it to make
good predictions and we can really see
that now because the characteristics of
the four-cylinder cars are really much
different than those of the eight
cylinder cars so let's add this geometry
this linear model again to underscore
this point and then you'll see that the
slope of this line of best fit between
miles per gallon and horsepower is
really really different for different
types of cylinders in the car
so just in these few minutes we've taken
a data set with this I'm plotting I'll
go back to you
to your question bit sir we did some
plotting and an exploration and very
interactive and very quick feedback
inside your art studio and you're
already ya know a lot more about the
data than you would have if you just
just have this table and just look at it
and okay well yeah there's the chorus so
noise there was a question in the back a
very good question do you need to
install a library to use q + indeed you
can use out-of-the-box normal plot
function from r and actually i can show
you that but it's not as nice as the
couplet function and the couplet
function is in the library called GG + 2
so you'll have to include this library
into your our studio distribution to use
it but out of the box you can you can
get kind of the same data and the same
image but it looks a lot less nice so
let's get back to the slides here what
about a bit more advanced example and
for that I I would like to turn to a
competition side called Kaggle and they
are actually promoting a sort of data
science contest so they provide datasets
to you and they say ok we have this data
set and we want you to make for example
a prediction on this test set and in the
case of the titanic competition that we
have here the idea was that you get a
list of people who were on the titanic
including for example their age how much
they paid to get onboard and whether
they survived the north so the idea was
that you as a contestant had to build a
model based on the attributes in this
data and just to give you an example
this is what we got and and you have to
create this model and then k go would
also provide a test set where you've got
all the same properties except the
surprise column and you had to predict
for this for this test test weather
the people in there survived Matt's so
there's really a typical application of
of data science in the sense that you
classify your data using a model and
then you use this model to classify a
new unknown data so one way to do this
is to create a decision tree and this
tree here is of course very compressed
and very simplified but the idea is the
same you create a tree with decision
points which in each point you split
your data into two groups one for
example where the sexes female and one
where the seconds meal and it's quite it
tries to get the maximal separation in
your data set between the different
classes that you're trying to predict so
there's actually a pretty standard
algorithm already available in our to
create this kind of decision trees based
on data I will get something and if you
have new data case you just walk the
tree and answer every question at every
node and you end up at the leaf either
true or false
whether they survive or not now the
problem with decision trees is that they
are really exactly fit to the data that
here used to build the tree and this
means that it works really well for the
data that he used to create a tree but
if you have new cases that you haven't
seen yet then you might get really bad
results so this is called overfitting
and it's really something that you
should prevent in this case the random
forest is an alternative to decision
trees because what's better than three a
forest of course and the idea is that if
you have a random forest
you're not going to create the decision
tree based on all your input data but
you you are creating many many trees
using small subsets of the data and of
course all these individual trees will
perform worse than the actual decision
decision tree that you would otherwise
have hats but in tandem taken together
so each tree will get a false and the
majority force will be your outcome
together this performs really well on
data that is out of outside of your
original data set so there's lots more
to really behind this but I'm just going
to show you how you
use random forests in our using very
little lines of code and I'm going to
switch back again to our studio and I'm
going to show you the source here
let's move the plot out of the way and
this will be a bit quick because I want
to go to the next part as well but it's
just to give you the feel of our in this
case we create a variable called column
types and I give the types of each of
the columns in the input data I create a
function and we assign it to the
variable load data and this function
takes a path to my type it takes these
column class types and actually I give
it the default value and that's the
value that we defined here so you have
named parameters and default values for
parameters in our functions and then
we're going to use the built in function
read those CSV read the path and we
supply these these these types so our
knows how to convert each column of the
data type to to the actual type that we
want in our data table because the
result of this call will be your data
table we do some conversion stuff that's
not really the most interesting thing
I'm using the subset function and to
jiggle the columns a bit around some I'm
moving the second column which is the
actual predictor column to the end of
today's table and in the end we just
return this value to the caller well we
use this twice because as I said we have
a data set that contains the actual
cases with whether they survived or not
so it includes the survived column and
we use that imbues or load data function
here there's also this test set which
doesn't have the actual survived column
where we have to make a prediction so in
this case I also pass in the column
tires but I removed the second type
because that was the survived column and
that's not in this destined here's the a
function to actually build the random
forest and in this case I'm going to
pass it a vector of columns that I'm I
want to use as input for this for this
first everything because it's not always
wise to use all your data when you're
trying trying to build a model sometimes
less is more and we're going to pass in
this this test dataset they said that we
want to be predicted
mr. fate makes things from people
repeatable we set the seeds we import
the random forest library which is also
something that can be downloaded from
CRA n and one thing that's a bit tricky
is that random forests can really work
with missing values and there were some
missing values in the data that care
providers so I'm going to use a function
also provided by the random forest
package arif impute and imputes means
okay give me all the data and I'll try
to fill in the blanks using the context
of the rest of the data so there's
pretty easy actually I'm just giving it
my my Titanic data set and I'm selecting
the input columns that I'm going to use
and rfmp will give me back a new data
table where the blanks have been filled
in as best as possible given the context
of the of the actual data now to build a
random forest we actually only have to
call the random forest function now from
the library we give it a formula we say
we want you to break the survived column
based on well dot means everything else
that's in this data set that we just
created I'm going to show it for the
demo purposes when it's busy and then if
we have this random force object our F
we can use the built in predict function
from our and pass at this model that we
created and pass it also the data set
containing this test for which we have
to make a prediction and I will get give
us back a a new data set containing all
the predictions based on the random
forest and one thing that I did here is
that sometimes the random forest decides
that it doesn't have enough information
to make a prediction so it will give get
give back an n/a value so I'm going to
shed from my prediction all values that
are n/a to zero which means they didn't
survive I mean
you've seen a movie not a chance
but but in the real world you might have
want to impute the test later said as
well using the same procedure that I did
before then I'm going to create a new
data frame and the format that Cagle
wants is just to give the passenger ID
and 0 if they didn't survive and a 1 if
they did survive now return this output
there's some helper functions to write
out the data and to actually render
function so let's just do that
run random okay I see I still have to
load it because it's not loaded yes I
have the source on save click so they're
safe it will load this file actually
will have the written random forest the
step completion as well Wow and it's
going to run it will up with some
statistics and it will also say okay
I've got lots of instances right in this
case I predict the zero where it should
be 0 and 5 no two cases but I predicted
one in 95 cases where it should be 0 so
that's not really really good and the
same for 1 here the most interesting
figure is that it expects the error of
this classifier to be about 16% outside
of the data set that you used to build
it so this is just an expectation it's
not the hard truth but it turns out that
if we submit this to K go they will get
actually a score of well 77 percent
accuracy which puts me at the spot of
what is it number 1549 Wow Wow so
obviously there's lots of room for
improvements but actually the people who
are at the top cheetahs because there
are lists of actual passengers in the
Titanic and what did they survive or not
so that's really not cool but anyway
this is a very fun way to get started
with data science and to really explore
what's possible with art as well so ok
you're all tired of our I think so let's
look at
some Java because we've seen an
environment that's completely separate
from the JVM and completely separate
from what we know how are we going to go
about integrating this into your
applications well there's three
strategies actually you can either
integrate
whatever we call assimilated or replace
it I mean that's an option as well let's
look at that first there's the our Java
and the Java our interface which are
just two sides of the same coin actually
and if you want to call our from Java
you can use this Java our interface and
it's actually it's just a native binding
to the our library and it's it's not
more advanced than that and you can also
choose to connect over a TCP connection
so you have our running as a server in a
different process that will be a bit
better even than having it as a native
code in your thing but as you can see
you get all kinds of data types back
that are really not native to to Java in
the sense that you get our expressions
and you can cause these to our vectors
but you still have to pull out all of
the values out of the vector and get
them into your data structures in Java
so it works but it's it's hardly a
perfect solution and you're still bound
by the same limits that the actual our
implementation has so in the sense that
it has to fit into memory it's still
slow because it's the same interpreter
that's underneath so it's possible but I
wouldn't recommend it another option is
to to move to just a different are
implementation in a sense that for
example R engine is a reimplementation
of our on top of the JVM and of course
that gives lots and lots of more
opportunities to integrate it into your
application and a nice side benefit is
that ranging also is yeah a bit quicker
than the normal our implementation it
also uses parallelism because we have
threads on the JVM and the actual
implementation of ours just a single
threaded interpreter and you can use it
as just another library in your
application in the sense there's no
native code integration or maybe just a
little bit but it depends on what you
use
the unfortunate news is that it's not
production already yes but it covers a
large chunk of of the r-spec already and
you should take a look at it if you want
to integrate our into your language in
your application so what does it look
like
just very quickly you can interoperate
with Ranjin through the Java extra
scratch scripts engine API and you can
just again eval our codes and you can
get results back as well and you can
share data between between your Java
code so in this case I have just an
integer array I'm going to put it as a
variable in the engine and then I can
reference it in my archives for example
printing the term of this integer array
and there's lots lots less copying going
on between your application and native
code because it's really all implemented
on top of the JVM any other way around
in our codes and engine you can also
import classes from your job application
and instantiate them and use it the
other way around so there's a two-way
interaction between our engine and Java
now I hear you all thinking okay that's
nice but it's still pretty limited right
if you have data that doesn't fit your
memory if you have data that's that's
well big data then you're kind of kind
of have a problem and that's true I want
to say that lots of people think they
have big data when they really don't but
if you do then you're still you still
have a problem so you need to have a
nice final absolute if and there are
many many alternatives for example
whatever you dreamt up in our you can
reimplemented on top of Hadoop or you
can pick Apache in my house which
already comes with a lots of default
standard algorithms on top of Hadoop to
do things like predictors
recommendations etc etc and SPARC is not
a one that's up and coming and is really
really nice also distributed
architecture but all of this these these
things are really
mean that is Surrey right so what often
happens is that you use our for like
what we did exploration and prototyping
and when you're ready for primetime then
you just take your IDs and you implement
them on top of these infrastructures
it's not ideal but it's not bad either
if you stick to the somewhat standard
algorithms if you used really
cutting-edge stuff from our then yes you
might be in for a lot of work another
choice that you have to replace is to
use a different art distribution there's
a company called revolution analytics
that currently has a our implementation
that is also scalable across a cluster
that doesn't mean that you can take just
verbatim all your are code that you have
and run it on the distribution you have
to do some tweaks here and there but
it's better than doing a complete
reimplementation and there's even other
Oracle products I found out Oracle
Enterprise are where you can use and run
our code on top of the Oracle database
so I have no experience with that but
it's available so just to wrap up we
have these options where you can either
just use our and it would really
recommend to learn our just to get all
the the powerful tools for data
exploration and for interactive
visualization and just to really quickly
get a feel of what's possible with your
data and just train a classifier and do
some predictions see how it works out
there there are lots of models for your
data and our and and there's all kinds
of cutting edge state cutting-edge
things available but once you have
something and you want to scale it out
then you're pretty much out of luck
using a standard are and you could move
for example to something like our engine
which skills a bit farther but still no
big data and kind of situation so if you
have that problem then you really want
to switch to something like Hadoop in my
house or spark but that does limit the
kind of modeling that you can do because
there are all kinds of assumptions on
these large infrastructures that may or
may not map to the actual
rhythms that you used in our so what
could be the next steps if your interest
is in this topic
well for one go to this beautiful
website and install our and play around
with it since it's it's a bit scary but
it's not hard and you will you'll get
the hang of it there are two books that
I would like to recommend one is machine
learning for hackers which is really
really great introduction to both the
machine learning fields but also a great
introduction to the our language since
all examples are using a are in this
book there's also a free ebook called an
introduction to data science which also
uses our as its main language for all
the examples it's free so I'd say just
download it and have a look at it
an introduction to data science by
Jeffrey Stanton I think yeah and another
tip is that yesterday the Coursera
course computing for data analysis
starts I did the first inclination of
this course beginning this year and it's
really a really powerful way to get up
to speed with these kind of topics and
to expand your skills in this area so
that's it for me I think we have a few
minutes for questions if there are any
left yeah
no yes but I will make it available and
now I'll 2008 up I don't know if you
have Twitter if you can you can follow
me and I'll tweet a link yeah
any other questions okay
the question is how those are compared
to med lab and octave I think they're in
the same area in the sense that most of
the things that you can do with are you
can also do with octave or MATLAB
I think octave medlab are more in the
scientific computing RM corner as well
so for large simulations and those kind
of things whereas R is I think a bit
more geared towards the sort of
exploration kind of workflow and I think
that's the main difference but I have
feature-wise I think you can do both in
all the environments yeah okay last
question I guess
okay the question is what kind of
datasets can you analyze using are
typically you will pre process and
flatten your data into a sort of a data
table structure that I showed and this
flooding fattening you can do in are as
well there are all kinds of packages to
connect to relational databases to
MongoDB etc and you can pull out the
data but most algorithms really assume a
flat table there are some exceptions
before network analysis and graph
analysis but there s cases yeah okay one
more than Oh license wise I don't know
the top of my head sorry R itself is GPL
that's true yeah I don't think there's
any problem with running our surf and
connecting over TCP connection to that
using the native integration but I'm not
a lawyer so don't take my word for it
right right right yeah yeah you will
induce lots of communication between the
processes yeah so it's not optimal yeah
I'm going to take the other questions
offline thank you for attention and have
a good job one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>