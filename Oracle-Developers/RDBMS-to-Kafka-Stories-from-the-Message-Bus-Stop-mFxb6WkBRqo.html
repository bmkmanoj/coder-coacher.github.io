<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>RDBMS to Kafka: Stories from the Message Bus Stop | Coder Coacher - Coaching Coders</title><meta content="RDBMS to Kafka: Stories from the Message Bus Stop - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>RDBMS to Kafka: Stories from the Message Bus Stop</b></h2><h5 class="post__date">2017-10-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mFxb6WkBRqo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everybody and thank you for
showing up quite excited about this talk
and just I want to give you a bit of a
background of how this came together
this actually started a year ago when
Stuart talk to me in a break here at
open worlds and talking about this new
thing called Kafka and how he really
liked it as a good platform for doing
fantastic stuff
with data but he wasn't really quite
happy with how he can load data from
relational databases into Kafka because
it was either very very simplistic or
very very big and I said well I probably
have an idea of how we can do this more
simple and then I started some research
and did this and got interested in it
and found a method that I think is
pretty cool and that's the core of what
I want to show and then we both follow
that we will be here I said well you
were much more eloquent and actually
explaining the we'll see the intro NPD
what is about so on still wobbly cover
the intro and then I would cover some of
the demos later just a quick word about
myself my name is my name is Bjorn I'm a
I'm a German living in Canada right now
and I have to work as a database consult
nor an IT consultant for most of the
time I've spent a lot of time working
with the Oracle database but I am
starting to find out more and more
boring and I'm venturing outside of that
looking at big data are looking at Kafka
looking at solutions around this and
there's that I work oh that's we both
are Oracle Asus or our glaze directors
that some Oracle's evangelist program
for technologists and they've just
launched a new program called the
developer champions which sort of bleeds
into that space and they look for people
that address some of these newer
technologies and talk to audiences
outside of just core Oracle DBA so so
there's that well for a company called
Pythian don't wanna talk too much about
it but the thing that I always like to
stress and what I find very cool about
the companies that it's a truly global
company we have people all over the
world and it's actually quite cool that
a lot of them are here this week but
yeah I wake up to my day and I talk to
people in Australia New Zealand India
Europe North America South America
that's that's pretty exciting and in
terms of what we do we do everything
data all claim it's love your data and
we do everything data so we do manage
services and consulting around
relational databases but also no sequel
solutions and
things like the subtle return today so
the agenda for today is Stewart will
give you an introduction no to what
Kafka is and what it does and why you
why you should care and give you
introduction to it and then also talk
about a constable I think it's quite
important it's the concept of state
where the cévennes and then I will be
back with you guys for um some hopefully
live demos that can either be really
cool or really embarrassing but it
either way it's gonna be good for you
either fun or yes for demos one of them
is bound to work and there's a big
screen do you eat this yeah
Jenny more than a big screen yeah that
would be good let's try that there you
go fantastic yeah it was my goal is just
to give you guys a brief intro to Kafka
if you were in my Kafka talk the other
day these slides are gonna look very
familiar so don't run because Burke
Buren's demo is fantastic so please
stick around for that so just little bit
about me and I will go too too much into
Anna yes I'm a local ace director really
appreciate the the ACE program
sponsoring me and paying for me to be
here Thank You Larry always nice to to
get back from Oracle it's wonderful
there's some of the social media avenues
if you're interested in following me
I've been doing data I'll just say data
and analytics for say 20 years and I'm
really excited about the sort of the
landscape of what's going on with
streaming data and how that really
transfer can transform businesses and
decision making in a way that I really
haven't seen in all these twenty years
so the way that we can approach data now
is really interesting I work for red
pill analytics I'm actually one of the
owners or company of about 35 people we
have three offices in the US we work
with Oracle Oracle partner were also a
Google partner are also
a confluent partner so confluent is a
company behind Kafka and we do a lot of
complementation x' and really Kafka is
included in almost every pitch we make
now it's just so important in our mind
to building a proper sort of data
architecture so one of the main sort of
distinctions that I want to make sort of
going into this is that you know life is
changing with bi in analytics where it
used to be just building data warehouses
maybe some ETL maybe a bi tool for
internal passive consumption your
analysts or your business owners or
decision makers are looking at
dashboards or reports and trying to
figure out how to run their business a
little bit smarter or a little bit more
efficiently but what's transforming now
is that is that data is often the
product for a lot of these companies and
for a lot of the companies were working
with delivering data via applications to
external people at scale and so when you
start thinking about architectures that
look like that you really need some sort
of single source of the truth that that
can allow you to load downstream
applications and we're not specifically
talking about relational databases as
these downstream applications right
there's a lot relational databases and
data warehouses still very much have
their place but they're not the only
game in town today and so what you
really need is something that gives you
the ability to feed data to a bunch of
different downstream systems very very
easily and the pitch I'm going to make
to you today is that Kafka really serves
that that role very well as well as
anything I've seen so this is a properly
sourced from confluent so this is life
without Kafka so if you fit and this is
their slide I tried my own version of
this and you're lucky that I stole
theirs but the idea of going and loading
data in a bunch
of different systems the right tool for
the right job we have a lot of choices
today about how to deliver analytics or
deliver data really in any way and if
you buy into that concept that you're
going to use a lot of different types of
analytic or data applications this is
what life could look like for you and
nobody wants this no one wants a bunch
of applications with one-off connections
to other applications or data stores I
really should say so life with Kafka
looks like this okay you have a single
point of data ingestion on principle you
can start ingesting your data today
without worrying about what the
downstream application is because Kafka
decouples that it decouples your
pipeline so the ingestion of data really
in some ways has nothing to do with the
downstream application of it or multiple
downstream applications of it and one of
the things I'm going to talk to you
about is how Kafka manages that for you
seamlessly making sure that all of your
applications read the data once and
exactly once and never lose a record
okay and this I think this is sort of
the the main point that that Bjorn asked
me included me in this because he'd seem
to give sort of the state and events
description at another talk and ask me
to you know what I'd come and do that in
his and I think that when you look at
like data warehouses raise your hand if
you're in sort of in the data warehouse
business of some kind yeah so the way
that we've done ETL for years is most
often batch oriented using timestamps or
dates of some kind to pull records now
some of us have experimented with Golden
Gate and we're getting every record but
most of us have it at least most of the
customers I've seen and if you pull
batch oriented data using timestamps
you're not getting all your events
you're getting this state of the
at that point in time and yes the state
has changed from the last time you
pulled it but how many inserts deletes
and updates have occurred that you're
not capturing if there's one thing that
stream processing and these new
frameworks really that they don't have
to explain to us dinosaurs is the idea
of getting all your events so that's
what I kind of want to want to drive
home to you today and if you think about
Kafka talks about being the the commit
log for the enterprise and I want to try
to make that point to you raise your
hand if your work with the Oracle
database and some capacity yep me too so
I want to draw this conclusion
using the Oracle database and try to get
this sort this point across to you so in
this Oracle database we might have four
tables the number is not important those
tables deliver state ok they they tell
you what the table looks like today you
have no idea what it looked like
yesterday or the day before
now the Oracle database does give you
functionality and Yorn's actually going
to use some of that functionality to
show you how you can pull data from an
Oracle database but that's sort of not
its soul / thats not its key purpose
I'll say so it's sort of a behind the
scenes kind of hack that you can use
something like flashback to go get your
data and that's super cool Dorn's gonna
show you that but in general the role of
a relational database is to give you
state and if you try to make it give you
something other than state you usually
have some sort of a workaround if you
want to know what the true system of
record is for the Oracle database it's
the redo logs if your system crashes and
you have to come back up it doesn't go
look at the state of the tables it goes
and looks at the redo logs and applies
them in order so that is where your sort
of system of record exists your logical
change requests and actually what but if
I sort of draw this example at the top
is the state of the table and by
underneath it is the events
that that go into making you're making
that state or building that state so if
you see that the so that's using my
company as example red pill analytics
we're a private company then maybe we
form an LLC so we have an update right
so the names changed we have some
information about it and then hopefully
one day we go public right so you can
see that that the state of the table has
has changed but the it's the events that
we're really interested in capturing and
I can't tell you how many data
warehouses over the years have tried to
deliver information or data or decisions
to users using only state going and
pulling from another relational database
the current state and maybe trying to
sort of decipher what the events were
what Kafka is gonna give you is true
events and your ins going to show you
how to take your data from from its
multiple ways from an Oracle database
and some other relational databases and
get it into Kafka so what Kafka's goal
is if you really want to think about it
if you think about your entire
enterprise kind of as a database you've
got a lot of different applications
delivering data so that's your data
platform Kafka wants to be the commit
log for your entire application or your
entire data platform really so if you
think about this a single place of
ingestion ingest your data with a known
set of ingestion techniques Kafka will
get to the different ways to get data in
and out of Kafka in just a second and
then a single point to go get your data
when you need it to feed your downstream
applications either either streaming or
batch batch is just a simple and
implementation of stream really so so
Kafka doesn't care if you're doing batch
doesn't care if you're doing streaming
handles both seamlessly handles feeding
downstream streaming applications
and relational databases and all kinds
of other batch oriented applications
seamlessly from the same set of data fed
into it how does it do that
so if you think of the data coming into
Kafka as a series of events in this case
1 through 12 okay so we've got 12 events
that have been loaded into a topic
hopefully you'll you'll sort of get big
concept of a topic when Bjorn speaking
it manages what's called consumer groups
so when you go grab data from almost any
of the api's you can use to get data
from Kafka out of you specify a consumer
group and you don't have to create a
consumer group you just the soonest
Kafka sees a consumer group for the
first time it starts tracking your
offsets so while consumer group a maybe
it's Bachelor in it has only read up to
event six consumer group B might have
read up to nine all the while a producer
is putting data at the end of it
Kafka manages every time you make a call
you tell it what consumer group you are
and Kafka manages which records you've
seen if you're streaming it just feeds
them to you as new ones come in if
you're not streaming in your bachelor do
you make a call to Kafka say I'm
consumer group whatever and it says I
get it I know what Rose you've seen
let's not worry about all the old Rose
the incremental data I just manage that
for you those offsets can be reset at a
consumer group to a timestamp to the
beginning of time whatever
so think about reloading your databases
in your applications and maybe even data
warehouses how hard is that if you're
using state from a relational
transactional system to load your data
warehouse and you go back to reload you
might not even have the same data coming
out okay so Kafka will have all of those
events and so it's time to onboard a new
analytic application or just give one a
try stand up that system
make a call to Kafka using a consumer
group tell it that you intend to get
data from all time
it'll give you data from all time okay
so that's the really the core piece of
what it can give you that decoupling
gives you the ability to be really agile
in the way you approach analyzing data
and building analytic applications so
with that oh one more slide sorry just
sort of at a high level how what are the
different ways to get data in and out of
Kafka so there's some high and low level
api's and I have not personally written
with the low level API thank God the
high level API is managed consumer group
for you they manage all of that okay if
you want to get down and dirty you can
manage offsets yourself in some of the
low-level api's I haven't seen the need
to do that so basically you want to
write some Java you want to write a
custom application in Java there's a
great set of there's a great SDK for
that we also have Kafka connect which
uses the Kafka Java API and that's what
you want you are going to show some
coffee connect that's a framework for
building pre-built connectors okay and
there's tons of them all right I think I
have a slide in here that shows you just
a sampling of that so Kafka Connect is
really where the enterprise is going
it's pretty you know it's relational
databases your ins gonna show you that
it's cloud services like Salesforce its
Twitter it's downstream applications
like Mongo Cassandra SPARC whatever
you're interested in Google bigquery
whatever you're interested in there's a
pre-built connector most likely that
allows you to set up Kafka with just a
config file a class path in a jar file
okay so you so this is not something you
have to write code for you can and
that's great it's not something you have
to write code for there's also what's
called the Kafka rest proxy and
surprise-surprise that's written in Java
so it uses the Kafka consumer the Kafka
produce consumer and producer api's that
are in the top level so if what you're
interested is writing some JavaScript
and you just you're writing
something a node or you're writing
something in a mean stack and you're
just interested in making rest calls to
get your data or put your data and that
rest proxies for you very very simple to
use and then third-party tools most of
which use one of those three techniques
okay so for instance Oracle data
integrator uses Kafka Connect GoldenGate
uses Kafka Connect etc Stream sets uses
Kafka uses the Kafka high-level API is
they've they're not using connect so
really you have all of these tools if
you're anybody using Oracle data
integrator out there yeah so in the
newest release of Oracle data integrator
Kafka is in the topology it's a
first-class data store no longer are we
using that old CDC framework and ODI for
that kind of thing it is a first-class
data store so these are some of the
Kafka connectors if you're interested in
learning about you know how many there
are there's this is just a sampling and
there's new ones added every day so with
that I'll let Bjorn take over and show
you how this stuff really works this is
where the demos break I just want to add
one more point that I see as a use case
that I actually came across and that's
next it's not actually doing a little
but it's extending your existing legacy
applications so we have lots of clients
that run all the applications that
people don't like to modify but they
like to add new features so we had
clients that said every time a user does
some certain action we would like to do
something on it right every time a user
signs up can we send them a tweet or can
we send up an extra message or can we
can we have that triggers some micro
service and of course you could just
program that into your application and
add it but the amount of requests that
are coming up people just can't keep up
with them anymore and that's another
good approach for using Kafka because if
you have a legacy system that's right
into a relational database you can grab
the data off there put it into a Kafka
hop and then have any kind of micro
servers or other application absolutely
connect to that so how do you get Kafka
the one way that I just wanted if you
mentioned this Oracle has a cloud
service called Oracle event hub cloud
service there's basically Kafka as a
service so if you are interested in
running Kafka infrastructure running
after cluster of sorts then Oracle has a
solution for you we can provision that
in their cloud and they take care of the
clustering they can take care of
separating the different pieces of a
cluster and connect configuring the REST
API you have to do a little bit more
configuration work yourself to get some
of these other connectors working to
that but that Oracle is actually first
Kafka in the cloud provider that I know
of they even beat complement to the
cloud so it's been around for a little
while so give it a try you know get your
get your free $300 give it a try
full full disclaimer though for my demo
actually using a VM on my laptop because
I'm I'm trusting my VM more than I'm
trusting the network here so let's let's
see how that goes if that breaks I'll
blame it on that so what I've done with
my VM is I just installed a rpm some
confluence basis a install Kafka there's
a really great getting started thing
with that you know there's three service
you need to start this one it's called
zookeeper that takes care of your
cluster then there's the actual Kafka
server analysis service called schema
registry because basically whatever you
can put whatever you want into CAF your
message can be any format it can be
Jason it can be Avro it can be any other
format you want if you want to enable
the reader to know what format you put
stuff in there you have to go put that
schema and the schema registry and the
consumer can actually make sense of
whatever data you put in there so then
you start things and you insert data I'm
starting with a really really simple
just very very basic example of how to
manually put stuff in there that's not
that's not very exciting actually we get
my cheat sheet for that no that's too
small you can't even read that
so in this window here I'm starting a
consumer consumer so the guy that reads
from Kafka I'm starting this consumer on
a topic called test and just keep
pointing me to my zookeeper which is
running on my local host and I'm saying
do it from the beginning so uber talked
about how it remembers the offsets this
just tells it just if you haven't also
if you have to run this before just
don't bother
just run from the beginning and then in
a separate window I will produce stuff
into it this is really just the hello
world example of that oops why do I want
to be so easy I can figure my schema in
the command line say it's got two fields
one is name one is string or whatever so
I enter that and then I just copy and
paste messages into it they have to be
of this format hello Oracle code and I
send that and then this exception
shouldn't occur that's because I wasn't
the right user but it still works so
it's just a lock for J that didn't work
or that thing didn't work just a warning
but here's here's a message and I can
write more messages I can write how
exciting this is super exciting and I
put that in there and that's it's not
the consumer yeah that was stupid so you
can think of a like Kafka topic sort of
as an empty bucket right so when you
create a topic you can even have topics
create automatically the first time
you're trying to write to them because
there's no schema to them right it's an
empty bucket where you write JSON and
Avro something that has schema already
in the data type okay so this is schema
on read versus schema on write and
that's one of the reasons that it is
such a powerful data ingestion engine
imagine if trying to do a relational
database and every time there's a
there's a change to your source data
you've got several layers of tables that
you've got to add columns to etc that
doesn't happen with Kafka so in the
schema registry actually versions your
schema so you so you don't have to use
the schema registry if you're
comfortable putting CSV data or XML data
into a Kafka topic you don't have to do
some of the stuff he's doing to describe
your schema if you know the schema on
the other end when you pull it out when
you're written and you're prepared to
deal with it you don't have to use the
schema registry but what the schema
registry does is allow you to
register the schema so that when you go
to pull data from it you don't have to
know the schema it just sort of
interpreted interprets it for you yeah
and if that's all too complicated then
Catholic Connect is the framework you
want to use connect brings all of that
for you and they have this long list
like stupid field some of them there's
really read list as this long of
existing connectors so there's one
connector that's called Kafka connect
JDBC and that's the most obvious to try
out first and that's my first transfer
first demo actually that's lumpy and
lumpy is gonna be used for all of these
demos so I've got a my sequel database
here somewhere oops OOP
it's called demo underscore code code
demo it's going really well I told you
it's going to be good demo there's a
table called diary
that's Lumpy's diary so he just started
that stuff and he's inserted some things
and let me insert some more so there's a
new event sitting on stage that's that's
just that's just my sequence just a
table it's not exciting so now I
probably put this data into Kafka and
all I have to do is like if you
configure this config file for the Kafka
Connect JDBC which is this file here and
there's lots of comments and really all
I do is I define a name and then I
define a connection string here
somewhere
so this is Lumpy's password he's not
good with passwords I say just just
fetch this diary table and look at
something incrementing and the columns
look for increments this ID that's
really all this is and then I so you're
sort of telling Kafka how to how to
interpret events from this table there's
a lot of different ways you can do that
incrementing with ID plus with with
dates and time stamps etc but what gets
exciting in
is when Bjorn goes around all of that
state and actually gets the events out
of the database themselves and I just
started the basically just started
producer using this config file that I
just showed and that'll take a minute to
spin up and then I have to start a
consumer again no exceptions flying
around that's gonna be fun
so I'm consuming from the topic my
sequel diary somewhere maybe in this
window and that should hopefully give me
all the messages that are in there so
far pretty much is there's a bit of
weirdness the cameras going on and now
let's into a new row saying getting
bored because this is all not too
exciting um so I can I can do that and
then this message getting bored that
should be there and we don't want to say
that here because we're coding it's
exciting so I wanted to lead the message
from diary where ID equals I guess I
know seven six so I deleted that so the
board is not in there anymore but that's
not not nominated Kafka right so Kafka
never never got that message that this
getting bored is deleted or if I modify
a previous row that exists in the table
that would never make it to caffeine
that's exactly the state vs. will seal
ends the way Kafka connect JDBC works it
doesn't really get this it can only
fetch data from it here better just
select star from table where ID greater
than the last idea fetched so inserts
are very very easy but updates and
deletes are not so updates we can
actually work around by adding a column
if you have a column last modified if
you have that already it's good for you
you don't have it already
you can add that and then you can use
that as your increment and just get rows
that are newer than and that but then
again if you override the same row
between separate runs of the cap go
connect and you will miss that as well
and there's really no way how we can get
delete other than maybe inventing your
own auditing scheme where you Mark a row
as two leaders or something like that so
that's what we were last unit so he said
this is this is not what we want we want
a little bit more than this this is this
is not satisfying and and that's when I
when I started looking in this and I had
an idea to combine this with an Oracle
feature basically we had to reinvent
some like
data capture and the way I'm doing is
I'm using a feature that Oracle has got
flashback Ferry and flashback very
extends the sequel syntax by saying I
want to get my table I don't want to get
a state of the table I actually want to
get the versions between now and or
between yesterday and today and it gives
you some pseudo Collins for version
start time versions end time and by
default you can you can run this and by
default it would use undo so for however
long you have undue force typically in
the range of minutes this would work it
would give you results if you want to
extend the time period for your
flashback where you can create what's
called a flashback data archive in
Oracle and that will mine the undo
tablespace for undo information about a
single table and keep that forever long
you want and I've had clients that keep
the Seder for three years that might be
a bit excessive but you can do that and
the cool thing is but since 11 204 that
is now free before that you had to buy
an extra license now it's free and let
me just show you how that all that he
has anybody ever used for flashback
versions query before raise your hand
yeah some cool came in around 11:00 I
think it's pretty pretty powerful just
specify two timestamps or two s Ian's
and it will give you all of the versions
of the data between those two as long as
as long as you have the undo for it so
it's really powerful sweet music Harry
selects often of entry inventories
retail that should exist in this
database that's bad
oh there it is what it wrong anyway it's
there um Yahoo thank goodness this was
going nowhere so I was a bit nervous at
first that that's the table right and
and you can just do this we can just
have lumpy take a selfie here all of you
guys there he loved selfies so there we
go he's took it he's taken a selfie and
I see someone updated selfies table
count equals count
plus one we're name equals selfies oops
this is not about
update inventory okay you enjoying this
aren't you
it's so good yes perfect so that works
and this would not make it to the
remember this would not make it to the
JDBC connect thing right and but but
I've got this trick oops don't do that
don't do this don't do this maybe do
this not do this this version so don't
copy and paste it come on I want to find
it here yes I do this curry so I do a
select ID name count and then I do the
sudo column of versions operation that's
going to give me insert update delete
and then the Sen I share don't you need
that from inventory and I save versions
between min value and max value so give
me everything you have in your in your
flashback ah - it's going to look
it's not going to work again lines there
it is so now you can see that it
actually has this update here that's the
after I just made selfies 2003 so it has
that in there so and that's actually
just again so adding another selfie and
writing is curry again and now I see the
thousand four in here and what this is
given you know you can use this query
and this curry has all you need but it
has the the data it has the history has
diverges operation it also gives you the
time as of which these roles were valid
so it has a version start time for this
for this row and in version starting for
this row so this is something I can now
feed back into the CAF connect JDBC
interface and say use this as if you
didn't do anything to set this up except
get flashback privileges all right this
this is how your database works this is
how your Oracle database works so
there's no magic going on with this
piece of it making Kafka consume it in
this style is sort of where the math
magic Amon cool and yeah so this is this
is the category JDBC config file for
this and it has again a connection
string is possibly as the same on Oracle
as it is on my sequel this is the query
I just showed you and then I say in this
case I'm using a mode called timestamp
plus incrementing some users looking at
both the ID column for new for insert
and I'm looking at the timestamp to get
your ones and and I put it into a topic
called connect inventory that's that's
what I'm doing so I'm starting this
connector so this is the JDBC connector
where you're able to the the Kafka
Connect Jeff for JDBC allows you to use
some custom queries to try to determine
what date in a pool and now that's all
no-one's done is gone gone and
configured that hope you pick ups from
here it's not good it could stop the
consumer that's it's not on me a lot of
my cheat sheet that's a bit of a pain
there is so kafka african soul consumer
so avarice the data format I'm I'm using
for this because that's something that
the connector mic has for me consuming
this Copic Connect inventory again from
beginning so this should give me all the
updates in order and then we can make
some more updates and see how they are
they make it there so that's the last
thing I made so it's um that's actually
actually this its alady it's 11:15 UTC
and I get back to why it's not in this
time zone and then just make some more
updates here so more selfies and then I
guess friends we of them doesn't make me
friends all the time
so if this works this should eventually
make it there whew so that's cool so now
we actually capture updates and I could
delete stuff too from from the table and
it would also make it into into the
Kafka strips that's really cool so that
gives me every change I ever made to
this table and that's really cool piece
that I I think I I found here and that's
that's the one I want to show there's a
few more things that I found while
working with this and oops why is this
not in here there's a slide that shows
scotches oh there it is so a few things
that are like we work with Oracle and
the CAF connect JDBC there's a few
things that are odd one is it doesn't
like all numbering formats if you do is
specify number 19 in Oracle which is
rather common then it converts that to
the wrong day that I Boyd throws an
error saying I don't know which Java did
have that to convert something to it
also doesn't handle schema changes
paragraph so if I add another column
that doesn't always work well and then
the most annoying issue was was I call
the UTC time zone issues so when I first
set up this time when I was living in
Germany and it was all fine because I
lived east of UTC and this demo worked
really fine and then I moved over to
Canada and I set my database to Eastern
Time and it stopped working so I did the
demo when I tried and it just didn't it
didn't work I tried for four five six
hours and debug a debug and then after
four five six hours my roles would
magically show up and that was weird and
I found out what happened this and what
happens is the connect JDBC driver will
basically say give me only roles that
have a day
that is newer then then it's newer than
the last time I fetched it and the last
time it fetched it is always stored in
UTC because Java just decides to do that
but the database if it's that's not an
UTC that doesn't work it's not a problem
if you live east of UTC because then
that's fine but if you lease the west of
UTC that's a problem so I set this to
UTC for now my my VM and that get rid of
this problem but that was annoying then
the other thing I found annoying to work
with a new set the same experiences if
you if you try this out a lot and you
create topics and you leave topics and
you start over and you want to test your
consumers and producers it gets annoying
to delete topics that's a bit annoying
and then we said in the connector offset
so the connector offset both what the
producers and consumers get stored in
the file if you use a cluster that gets
more complicated because then it's more
than one file to take care of and you
generally want to just reset these
things that's those are the gunshots
that I had with with the solution this
is where relational databases sort of
thrive with deletes and truncates right
so so you're sort of your dev process of
writing code and testing it it's kind of
painful now that you can easily handle
that by using something like containers
right where you spin up a container of
Kafka to do your testing and then throw
it away
and then when you write some more code
spin up another container and test it
and I think because that development
because the development lifecycle of a
lot of people using Kafka are using
containers and other sort of DevOps
processes they don't really struggle
with this so my advice to an enterprise
that's gonna start you know if you're
writing code for Kafka or even using any
of the connectors you're probably going
to want to invest in some some DevOps or
container type technologies to make it
really easy for you to test your your
code or your use cases cool so that's
all using the connect JDBC job which is
free and open source and easy there are
other solutions that that use
replication and just I think in my ideas
why when would I use this that I just
showed you man what I use for
application tools is just a single table
or two or three that you're interested
in getting into Kafka this this hack or
this workaround works really really well
if you're looking to put more tables
into Kafka it gets a bit annoying
have to spin up all these producers and
it gets a bit a bit out of hand and
there's a solution right we talked about
how redo log basically is already the
the the log of events and you could just
look at reboot locks and that's what
replication tools - and I have - that I
show you one is TV visit replicate and
the other one is Golden Gate that's the
two that I tried out and I can only say
this is really easy to try out I just
tried this out last night after and
night out with this guy - the ACE dinner
and I tried this and I ran into an error
saying your license has expired your
trial license and then this morning I
just downloaded it again
reinstall the whole thing and set it up
into took like an hour listen Howard so
that was really good so the way this
works is they have a tool that does
database replication anyway so they have
a tool that parses your log files so it
looks at the redo log files passes them
sources when an intermediate format
called P locks and then there's a second
piece to it that can read these P locks
and put them into Kafka Connect so
there's a Kafka connect piece that can
read the P locks we use by replicate and
put them into into Kafka topics and
that's just how you get started let me
show that demo real quick so stop this
stop this
so you said there's a wizard to set this
up I'm not showing with that but I'm
showing you if you're interested in in
DB visit our zone right there
raise your hand yep that's the man that
started the company so if you're
interested he's the guy to talk to
so we have F F that's running space in
the mind trust us that looks for the
lumpy schema I can do an info all and no
and force the other that's the other
product all we showed me that it's
running and it has mined these nine
entries so FAR's four for the inventory
table and then oops there's a Kafka
connect as it replicates do you visit
replicate there it is it has a config
file of course and that makes it simple
so it does a config file there's two
things I think that I said one is by
default this flash size is set to
production size limits are set to like
10,000 but that would mean that you have
to write 10,000 messages before you see
anything on the screen so I said this to
one so that every every update I make
makes it and and just point it at the
directory by your P locks R which is
somewhere on the top there it is and
then you believe this one so on there
say that's the one so again I ran my
connect standalone against this
properties father I just showed you run
this and these are these are demon
processes that that boards running so
these are processes that you run once
and it just stays up and constantly
listens for changes and it's Ryu - I set
it up to write to a topic that starts
with DB v - and look at one look at the
inventory table again so run this from
beginning and hopefully this works
I said hope about today I have money
there we go so it's got five friends and
let's just go back to the sequel window
add another friend there we go
should have six eventually there we go
cool this works so that's another way to
do this is it getting all of your table
data or you give me you can the I don't
say problem but they did the thing I
have with this approaches obviously for
the for what you want to get into
cupcake instead of filters you can say I
only want to produce certain tables or
schemas into Kafka but to get the P
locks you have to basically parse all
the P locks so if a database that has a
lot of activity that can be a lot of
work and if you have a database with a
lot of activity but only one table
you're interested in then this other
approach might be might be worthwhile
and at four more minutes which might not
be enough for a demo but I certainly
want to mention that Golden Gate
obviously has a solution as well they've
got a product I called Golden Gate for a
big data that's a bit confusing because
you have to install to gain go and gate
installation so you have one go and get
installation for there it is one Golden
Gate installation that basically
extracts your your reader locks and
writes into tray files and ships them to
another Golden Gate installation that
you would typically have from the same
machine and then this other golden
installation is called Golden Gate for
Big Data that one understands how to
write to Kafka so you can configure your
stuff and write to Kafka there the the
thing that I found both great but also
confusing to start with with Golden Gate
is it has a lot more flexibility so it
has flexibility to write in all
different kinds of formats I actually
set up my demo in such a way that I
would write not using the Kafka connect
API will just write basically with the
streams API which might or might not be
better but that's that's what I find
interesting so that's that's Golden Gate
and then if you want to get start with
the Golden Gate there's a guy called
Robin Moffat that's the guy on the left
you mean cockpit what did I say Golden
Gate well Kafka and Golden Gate because
he's written it he's been good blogs
about that and then it's a colleague of
mine on the bottom right that's CLEP
father he's also bitten cool blocks
about how to use going gate with all
kinds of big data things I have already
given you this which is I've showed you
ways so you can do very very simply get
data from Kafka into from Oracle to
Kafka you
they connect JDBC driver if you want to
get all changes
look at this thing that I promise I will
eventually write up in an article so
that you can find it on the internet
because so far I haven't managed to do
that but using this flashback ferry
trick if your source database is Oracle
use this flashback where each or
flashback versions trick to get all the
changes updates and certains and deletes
if you want to get more of your database
into into Kafka look at application
tools like TV visit Golden Gate there's
a few others out there the same thing
and then if you say this is all boring I
already have my data and Kafka what
what's next I think the cool things that
came up is I just came back from the
talk that talked about case equal so kcq
is a secret interface to Kafka so we can
now it's a bit ridiculous to get your
data into Kafka and then query it using
sequel but you can do that but they also
use this to create aggregations so you
can actually use that as your
transformation part of an ETL pipeline
so you can say the data that's coming
raw from Oracle I actually want to
aggregate that or to filter that out you
can actually write a sequel say when
they say create stream that would create
a new topic a select star from other
topic where IDE or whatever so that's
cool there's the rest proxy if you if
you if you like talking rest and you
want to consume a produce using rest
there that's there and built into the
Oracle event have cloud service which is
the other cool thing I want to recommend
you guys to look at because while it's
fun and game to install a single VM with
with CAF garnet eventually you want to
run this in production and then you
don't want to I don't want to build
clusters anymore I want other people to
do this for me
then there's a tool that stupid showed
me called stream sets which can be use
if you if you want to get cool insight
into your data just for ways of
debugging where I just connected to it
and then I say show me what's in there
and maybe combine this with that so
that's that's something it's a pipeline
transformation tool it's a streaming
sort of ETL tool connects talks to all
things cloud big data and relational
that's really powerful we're a partner
so
though that out there upstream sets also
a big data sequel anybody using big data
sequel out there no right they talk to
Kafka no - don't they it's the newest
release queries Kafka so you can start
thinking about the Oracle cloud being
pretty cool actually being that way but
but if you have a venn hub
you've got your event hub cloud service
in your running Oracle database with Big
Data sequel enabled you'll be able to
and it's also going going to you know
safe harbor but you'll be able to query
objects in your object store which is
Oracle's version of s3 JSON files out
there so you look at a single interface
that allows you to query your data at
rest your data your streaming data an
event hub and your data that's been
curated and loaded into an Oracle
database cool it's pretty powerful
and then obviously look at the rest of
the Khafre connect connectors there's
connectors to s3 to elasticsearch to
Twitter app so that's really cool to
just connect all these things together
very easily and with that I want to say
thank you and I hope that that you learn
something from this talk if you have any
questions I'm happy to take questions
now or later yeah
so why is hive required to connect to
what you don't you don't need hive or
for anything I've ever I mean you can
use hive with with who do I think I
think what I've seen is as a demo is
I've seen a demo where I've seen it I've
worked for where people put Oracle data
into Kafka then Kafka to HDFS and then
they use hive to query and then in that
case you just use high as basically
showing your data as made it because
otherwise you're looking at raw HDFS
files and there's not much to show there
so there is there is a Kafka connector
for several different connectors for for
query for for putting data from Kafka
into Hadoop and through HDFS I'm not
super familiar with them I think one of
them uses hive but you don't have to
worry about this if you ingest data into
Kafka using the schema registry ok
you'll be able to use any connector to
take it out the other end and put it
somewhere as long as you if you use
connect going in any connector you use
coming out will understand the schema
and we'll put it into Hadoop scheme
using using hive schema so does that
make sense yep cool thank you guys I
guess we're getting kicked out a few
minutes thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>