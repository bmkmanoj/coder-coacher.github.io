<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Understanding Java Garbage Collection and What You Can Do About It | Coder Coacher - Coaching Coders</title><meta content="Understanding Java Garbage Collection and What You Can Do About It - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Oracle-Developers/">Oracle Developers</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Understanding Java Garbage Collection and What You Can Do About It</b></h2><h5 class="post__date">2015-06-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Sidz_dS9X3Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Gill tenor and I'll be giving
this talk softly during the write groom
and this is actually pretty encouraging
we have a an actually full room I really
liked looking around the schedule this
year at JavaOne and seeing how many GC
talks there are and it's kind of amazing
that after all that other DC talks
you're still here well see if I can
actually teach you anything new or or
different or or add any value there but
this is a talk that tries to explain
garbage collection as as as a field and
how it works in general the general
purpose of the talk is is to educate you
on garbage collection for you to
understand not what flag does what in
which collector you're choosing today
and how to tune in exactly but more
importantly for you to understand how
garbage collection in general works as a
machine in the various options and forms
and algorithms people use and then to
actually be able to to use what you
learn yourself and think about how the
mechanisms work and there's tender
fundamentals and then lead up and you
know learn the various collectors and
flags you might use and and use that
understanding the machine I find that as
in all fields understanding the basics
and working from there it is very useful
I promise that while I'm very very proud
of the garbage collector we make at Azul
and we think it's the best in the world
that actual bragging will only come at
then and I do have a hidden agenda here
I'm gonna spend most of the stock
educating you on the fundamentals that
you understand why our garbage
collection stuff is the best one out
there okay so a little bit about me and
I know a lot of you probably have
already seen this slide but I do repeat
it I'm the CTO of Azul systems one of
the co-founders there I've been working
on
different ways of doing garbage
collection different and the common
directions and trends in industry for
over a decade now and actually built the
puzzle s and the C for garbage collector
algorithms serve their academic peer
review published papers on them and this
is some evidence of me actually working
on garbage collection this is in my
kitchen in 2004 and that is for those
you don't recognize that that's that is
a trash compactor and there are
fragments coming out the back because
the compaction wasn't working well and
it's stalled it pause this it's us you
know the world as far as I could certain
was stopped and we had to fix it so I
took out the garbage collection book and
look for some ideas there that is not
exactly how we came up with c4 but some
I've also worked on a lot of other
things everything from drivers and
operating systems and in helping design
hardware and even I got an instruction
and a CPU called I got to select the
name and I chose get immediate long if
you think about that for a minute and
and I've built big Java systems to
manage millions of subscribers and telco
environments across hundreds of servers
I've built firewalls and traffic routers
and switches and I've been doing java
virtual machines and garbage collection
things like that for the last decade
so I've made a lot of mistakes and and
hopefully some of them will teach others
than people thinks - I'm also a member
of the JCP Executive Committee and I
mentioned that really only because we're
running for election again so remember
this like in a couple of weeks we
actually have an election so if you like
this look it up about Azul where I come
from and a company I started we focus on
building scalable virtual machines
that's all we do Java virtual machines
and in order to do that we've had an
approach for a decade now of doing
whatever crazy thing it might take to
do that that included some pretty crazy
things we built custom hardware with
custom chips and custom CPUs and custom
instructions this is a picture of us an
864 way SMP with 3/4 of a terabyte and
given that this was in 2007 that's that
was pretty impressive but we don't do
this anymore or at least we don't design
these anymore we still sell them to
people want to buy them but all of our
design is no longer focused on Hardware
about 5 years ago we looked at the
commodity x86 roadmap and said it's
getting good enough we don't have to
make your own Hardware anymore we can
now do on pure software on commodity
Hardware what we used to need this kind
of thing for and Vega is our legacy our
current and the future is pure software
and that's something we call Xing it's a
JVM for Linux x86 we're also known for
low latency consistent execution small
large data sets everything in between
but enough about that let's talk about
the talk at the high level I'm going to
cover fundamentals we're going to talk
terminology metrics background we're
also going to cover classification using
those terms and that I will define
hopefully precisely and look at actual
commercial collectors available today
and actually classify them using actual
garbage collection terms and then you
know I'll introduce you to a problem I
call the application memory wall problem
the why stop the world is actually a
problem and why everybody seems to be
working on it and I also go through an
actual what the world looks like when
you solve it with something like your c4
collector before we get into details I'd
like to do a quick poll I do this in
almost all talks that I do and it is
simple I assume you guys all hear
programming in Java to some degree or in
your in the run shell the other one down
in Moscone Center is for the other
people
I want to go through the room and see
how many of you program in applications
where your instance of an application
has at least one half of a gigabyte of
memory please raise your hand that's a
lot of you now how many of you use at
least one gigabyte of memory yeah about
the same how about more than 2 gigabytes
almost the same starting to lose some
people how about more than 4 gigabytes
now look around starting a drop but keep
your hands up if it's more than 4 ok
more than 10 more than 20 okay how about
more than 50 cool so I have something to
sell you we look for that in the room
right but let me show you how smart I am
ok boom I guess that this is where 80 to
90% of the room will be and I guess that
because every room I've spoken to has
the same pattern pretty much except when
I get all you guys together in one room
but keep this in mind as we talk through
this but because I will come back to
this subject it's very important for
them for the industry as a whole notice
that I didn't ask any of you what you do
I didn't ask you what kind of
application it is I didn't ask you if
it's big data or search or an app server
or a trading system and it's surprising
that we have such a tight grouping in
this room or in all rooms ok so why do
you need to understand a little bit
about how garbage collectors actually
work a lot of people say well you know
the magic is there to make it so I don't
have to think about it and in to explain
to people why there's some motivation to
really understand what the machines sort
of does I use an actual story for that I
call the story of the good little
architect this actually happened and I
actually ran into it anybody here like
Mark Twain that's that's what the
reference is - and I define a good
architect is not somebody who can build
good architecture or good code
necessarily it's probably a plus if they
can do that but first and foremost they
must be able to impose their will on a
team of people and get a team to do what
they decide rather than just himself if
they can't get that done they're not an
architect they're a philosopher so we
met a very good architect about six or
seven years ago when using our Vega
appliances we had what we called at the
time a pause list collector and that
pause this collector was pausing for 18
seconds on an application so we
obviously wanted to find out what what's
wrong there and how that's happening and
when we investigated we found that this
specific application was performing tens
of millions of finalized ations in every
GC cycle and at the time are concurrent
collectors were still doing finalization
as they stopped the world event because
come on how much of that is actually
getting done right and here there were
tens of millions out of them stopping
for a really long time now this was not
normal so we went to figure out why this
is happening and we found out by the way
we did fix that system you know zing and
everything since does even then it
wouldn't pause but we found out that
every single class written in this
project for a period of 18 months had a
finalizer
every single class anybody in the
building wrote had a finalizer
that's the sign of a good architect
very good architect okay none of them
escaped now some of you probably
understand why this is funny but the
finalized errs were nullifying the
references and doing nothing else this
is all the finalized errs were doing
they were putting nails in every
reference so that before the object dies
it'll make sure it doesn't refer to
anything else because that will probably
help the garbage collectors do less work
now here's the thing to understand this
is a fundamental issue of whether or not
this is a good idea it's not a good idea
okay but why is it not a good idea
because you're in Java and you're not in
C++ this is exactly the right discipline
for C++ destructors guess where the
architect came from if you are building
a large C++ project
you better have this discipline or you
will have memory leaks so that
discipline is necessary however this
structures and getting rid of reference
counting and C++ does not translate to
finalized errs in Java
it's the closest visual thing in Java
it's the opposite of what you want to do
it's the worst possible thing you can
ask the collector to do in order to try
to help it it doesn't help it at all
hopefully at the end of this talk you
guys will actually understand some of
the why or maybe you already do so I use
this as an example of much of what
people think they know about garbage
collection is actually wrong and is
actually GC is actually much better than
you think in many things and it's much
worse than you might think in others so
for example GC is extremely efficient
and at allocating and managing memory by
far with nothing even close to it there
is no faster way to allocate in free
memory than GC in a computer today
this might sound controversial I'll show
you exactly why the math works that way
but there is no more efficient way
malloc and free is dramatically slower
than GC is it spends a lot more power a
lot more cycles a lot more everything
most people don't think that's true
dead objects cost us absolutely nothing
to collect all current modern garbage
collectors spent only time thinking
about live stuff not about dead stuff
that's where efficiency comes from and
garbage gushing well absolutely find
everything that is that it doesn't he
need help
it doesn't need you to break their
circular link list so nobody will find
the other end of them if it is that it
will be found in all modern JVMs in all
modern collectors now in many cases it's
not as good as you think it might be so
for example it really does stop for
about a second for life gigabyte and the
only thing you could do with about it
other than using is to delay that that
inevitable situation in your application
to lengthen the time between those
events but when people think that they
can avoid it completely or they can
change something in what they do most of
the time they're just pushing the
problem forward
there are also memory leaks you if you
forget to forget your objects they're
around there's an entire class of memory
leaks that goes away but there are
classes and memory leaks that remain
this is not a magic bullet against
memory leaks in one of the fundamental
things that GC introduces as a new
problem almost to most environments is
the tuning garbage collection is usually
not an exercise in improving performance
but in delaying bad things that's not a
normal tuning exercise for other
platforms so when your get a job to make
your application run for 30 minutes
sustained in this ramp with this SLA and
you pass or fail based on whether that
happened and you fail the first time so
you tune and you fail the second time
you tune you're a little better a little
better when you're tuning garbage
collection most likely what you did by
passing a 30 minute test is not make a
fast system that sustainably runs but
you've just pushed the big bad thing to
a minute 31 that's the common exercise
in tuning you need to keep that in mind
as you're tuning because if your goal is
to run for a week or a day or five hours
without a problem
you shouldn't believe that the problem
is gone just because you can't see it in
this wind
it's a new problem so let's talk about
some terminology how many of you know
what a concurrent collector is how many
of you know what apparel collectors
fewer people okay this is why we talk
about terminology so let's define them
my concurrent collector is a collector
that does its job while your application
is still running without stopping your
application concurrent with your
application apparel collectors apparel
as a collector that uses more than one
thread to do its job these two terms
have nothing to do with each other
they're orthogonal
you could be concurrent but not peril
you could be parallel but not concurrent
you could be both you could be neither
in reality most concurrent collectors
these days are also proud but there's
plenty of parallel that are not
concurrent other terms to define this
way okay
other terms to define stop the world
well that's the opposite of concurrent
the part of your collector that's not
the current will stop your application
in its tracks and stop the world
incremental generally means I have a big
job to do I don't have to do it all at
once
I can do a part and then let you run
safely and then I can stop your going
into another part and let you run safely
and stop you again to do another part
the opposite of incremental is
monolithic all at once one thing not
safe to stop in the middle okay
there's another key term like it's a
term line you know it's a pretty clean
english-language term it's mostly and
mostly means exactly what it means but
you need to read it right
whenever you see mostly in a sentence
that's like a not sign mostly concurrent
means sometimes stop the world mostly
perilous means sometimes cereal mostly
incremental means sometimes monolithic
the word mostly is really important
because we use it a lot in garbage
collection definitions we usually use it
to make the good thing look good and
trying people and have people ignore the
bad no other terms precise versus
conservative luckily everything we deal
with in Java these days is precise or
exact a conservative collectors are
collected that doesn't
quite know where all the stuff really is
doesn't know where all the live objects
are doesn't know where all the
references are doesn't quite know maybe
whether place and memory is a reference
or an integer and without knowing that
it's limited in what it could do it
needs to keep things alive that it's not
sure about and it has some limitations a
precise collector is the opposites a
collector that knows where all the
pointers are and whether or not they are
pointers at any point in time where it
might need to collect a collector has to
be precise if it is going to move
objects the reason that's important is
if you're going to move an object from
point A to point B you then need to find
the three billion things that might
point to it and fix those pointers
before somebody sees them and uses them
in the wrong way it's hard to do if you
don't know where they are if you're not
sure if this is an integer and a pointer
and it might be just an integer that
looks like a pointer because it has the
same value the preciseness is important
it's not the garbage collectors and that
makes things precise they just have the
easy job it's the compilers the JIT
compilers that need to provide all this
information that says at this point in
the code these are the references on the
stack and at that point in the code here
they are so a lot of information
compilers generate and when they don't
generate it you do not have the ability
to do precise collection this is why you
need to really respect all those just
compiler guys working on JVMs ok they
produce a lot of good things for
collectors all commercial JVMs did I use
precise collectors all commercial jets
provide this information and this
includes OpenJDK by the way I shouldn't
just say commercial that means that they
all have some form of a moving collector
safe points I'll go through these
quickly AGC safe Point is a place in the
code a region of the code a range of
execution where we actually have that
information about where all the things
are we don't have to have it everywhere
but we need it at safe points and safe
points are the places where it's okay to
run a garbage collection scan supposedly
on your thread or on something else
there are other kinds of safe points not
just GC safe point for example
deoptimization safe points and
and in in deflation save points but we
often conflate the two GZ safe when I
say point I use them interchangeably
bringing a threat to your safe point
means getting it to run up to and not
past the safe point and I carefully
choose my words here because it might
sound like I'm saying stop the threat at
the safe point that often does happen
but a key example of not stopping at a
safe point but just bringing it your
safe point is running into J&amp;amp;I code when
you're in native codes which in I call
the entire code is that a safe point
there's nothing you can do in that code
to modify the Java State and make it
different than what it was at the entry
you could be in there inverting a matrix
in C++ code for an hour and you're in a
safe point okay it does not mean idle it
does not mean stalled all stalled are
safe points but the other way around
isn't true you need safe one
opportunities to be frequent otherwise
you'll get one of those situations where
when you want a global safe point
meaning I need everybody to stop because
I'm gonna do some big thing that changes
behavior and this one guy hasn't run
into a safe point part of the code yet
for a minute well everybody stopping
there waiting for that guy no useful
work is yet done under safe points so
that's what I call that effect the time
to safe point it's something at Azul we
track a lot we actually profile for it
and often it's the hidden cost of GC
pauses or safe pointing that that causes
problems not just the work of GC itself
that only happens once you've reached a
signal here are some things are coming
to all GC mechanisms every one of them
will identify all the live objects in
the heap or at least the part of the
heap it's responsible for every one of
them will get reclaimed resources held
by dead objects that's almost a
definition of garbage collection every
single one of them well at some point
relocate objects from point A to point B
it's not a lot of pointed being a
precise collector if you don't do all
three of these and let's go through
examples of that in common practice a
mark
we've compact the collector a very
common classic way of collecting mark is
the first boy sweep is the second boy
compact is the third boy however a
copying collector commonly used in young
generation collection is all three in
one pass there's no separate stages here
a copying collector will take the from
space that Eden or whatever the from
space is in the Yujin and say I'm gonna
find all the objects here and as I find
them I'll just move them to a two space
that's reclaim sorry that's periodically
relocate and find the live-in when I'm
done well I've got a big empty from
space I could just reclaim it and that's
done generally as a single pass not as a
multi pass operation so all three things
will happen the algorithm used differs
and how they happen so let's break it
down into some pieces marking are also
known academically as tracing you start
from what we call the routes these are
static variables the contents of your
threads if you think of the threads as
not routes its roots actually and
sometimes people just say the pointer to
its right is a root in other things that
are you know you can get to without
having to find an object first and you
start from those and you kind of go
around painting any pointer you can
reach through getting to the next object
and then looking in there for other
pointers to other objects and you paint
anything you can reach with pointers if
you've already painted reached something
that's painted that's fine you don't
have to look at it again we kind of see
what's reachable and at the end of a
skin or like that anything that I reach
will be marked live because I said I
could reach it anything I didn't mark
live is by definition not reachable and
because it's not reachable it will never
again become reachable it's impossible
to make an unreachable object reachable
again and therefore it's dead it's safe
to reclaim recycle whatever mechanism
you want to apply to it it's important
and I'm going to talk about complexity
here for each of these stages the work
involved in doing this is linear to the
amount of
stuff I've objects and references that's
what I have to do it doesn't matter how
big the heap is it just matters how many
things I'm following so it only grows
and shrinks with the live set sweeping
the second stage of marks we've compact
imagine that we've marked sweeping is
pretty simple you scan through the whole
heap start at the beginning go through
the whole thing maybe pail maybe not and
if you find something dead you just
recycle it somehow usually by putting it
on some sort of free list okay now we
have a bunch of free places to put
objects in the future the complexity of
this is simple it's linear to the size
of the heap not to the size of the live
set the size of the heap because I have
to scan the entire heap to sweep it
compaction now this is an interesting
one over time the memory will get
swiss-cheesed
you'll get holes in it and variable
allocations at different places at
different sizes to the point where even
though we've swept and we found empty
things if we do that I may have 90% of
the memory empty but no room to put my
10 kilobyte array at which point I have
a problem now most je viens don't like
to break arrays into pieces so we need
to make a hole that big in the banking
hole that big in variably moves objects
that's the reason we compact now I get a
lot of questions for people well is it
really inevitable does it really happen
an answer is it's not actually
inevitable if if your program has a
fixed number of types of exactly the
fixed size and a fixed pattern then it
may not happen to you how many of you
process XML you don't have fixed size
objects
you don't control the object size it
comes in and determines itself and
that's why your heat will get fragmented
okay and in there a lot of other
examples of that compaction moves the
lives objects together to reclaim the
configured space and I call that
relocation the actual movement of the
object but the actual hard work is
correcting all the pointers to those
objects we move I move one object I need
to scan everything that might point to
it to fix it that's why I don't move one
object we move as many objects as we can
to amortize the scanning of all the
things that might point there and
generally that means move the entire
heap or in some incremental cases move a
set of regions that I can contain to
Rachel
so remap fix up is is the harder part ok
the bigger part and remap has to scan
everything that might possibly point not
just the thing that really points
because it doesn't know complexity wise
as complex as this seems this is still
just linear to the live set not the heap
size I only have to move live objects
and I only have to scan references in
live objects ok a different algorithm
separate for marks we compact this copy
I touched a little on how that works a
copy collector moves things from the
from space to the to space generally
started from roots and everything's in
the from space and then we take anything
we can reach and we move it to the - and
anything in which we move it to the -
and it's all one pass there's no
bickering of this and at the end of that
we started from root references and we
end with everything in the - and nothing
in the from and we've reclaimed all the
space complexity wise linear to the live
set I only have to visit live objects
questionnaire
so for this specific pass the question
was is it really from - or is it just a
label right now for this collection that
is the from and that is the - and later
probably this will be the from and
something else will be tough - whether
it's swapped or recycled or there 15 of
them that's an algorithm question okay
the copying collectors do this linear to
live set let's put these together a
little and compare and contrast copying
has an interesting limitation where
since I'm copying everything from the
Trump to the two and I don't really know
when I start how much is alive at how
much is dead I have to have as much - as
I have from it is not safe to start
because this is usually monolithic and
they can't go back just start a copy
without knowing that I can finish it
knowing I can finish it means if I
didn't collect even one thing I will
still finish the copy or I will have to
crash your JVM which is not a good thing
that's a interesting problem or
limitation it's costly mark compact
without the sweep generally requires the
same 2x but there are interesting ways
around it however it requires that in
order to do it in one cycle but mark
compact generally could be stopped at
any point in the middle so it's more
resilient to that if it doesn't have to
extra memory it could do something about
it
morick sweep compact or compact in place
this is where you just move all the
objects to the left but not just
somewhere else doesn't need to exit just
needs one X it's a lot more memory
efficient as far as worst case
conditions are safe copy and mark
compact are both linear to the live set
and only to the live set but mark-sweep
compact is also linear to the heap size
very important quality you'll see in a
second and then mark sweep and sometimes
compact might be able to avoid
compaction for a while it's a common
trick and copying I already said is
monolithic ok so let's talk about some
ways to use these metrics and
inefficiency remember I said you know
copy is linear to live set but sweep is
linear through heap generational
collection leverage is an interesting
observation that is that most objects
it's called the week generational
hypothesis it's not that old unintended
I think it was first noticed or at least
written about in the early 80s so you
know but it does seem to be universal to
applications we have a lot of temporary
stuff we make and throw away and it
tends to dominate the allocations we do
so it's a very effective observation
about a lot of things and the purpose of
generational collector is to use that
observation for efficiency the way we
use it for efficiency is if we know that
we have this young generation things
that were recently allocated and we know
that most of this is dead because of the
hypothesis itself then if we applied an
algorithm that is only linear to the
live set to something that is very
sparse because of our assumption we will
get efficiency that is the actual trick
of generational collection it's not the
observation it's the use of them for
observation in combination with
complexity tied to the observation so if
we use an algorithm that doesn't grow
with the size of the heap space I'm
collecting but only with what's alive in
it and I know or hope that only 1% is
alive
I will get a 100x efficiency boost
compared to something that we need to
scan everything that's why we usually
see copying collectors for the younger
generation and not a mark sweep compact
collector because a sweep is linear to
this thing now in order to keep the
observation real we can't just keep
everything there the observation is only
that young stuff dies quickly or that
most stuff dies young it's not that live
stuff is young in fact the opposite is
also true most live stuff is old if it
was young it would be dead so you have
to evacuate move away things that last
long out of this space so that the
observation would stay true if you keep
letting it stay there and then slowly
it'll fill up in it alone no longer be
sparse and your efficiency will go away
if your youngjun fills up it becomes
less and less efficient
by a factor of how much you didn't put
on so promotion is necessary to keep
young generation going it's impossible
to keep a an efficient young generation
collector efficient if you don't promote
long lasting objects so we only collect
the old generations when they fill up we
have this really cool filter in the
young generation and that filter is
extremely effective it's an order of
magnitude or two in most environments
and it's so efficient that it's
practically impossible to keep up with
modern CPUs without a generational
filter today a modern x86 will happily
allocate a quarter to half a gigabyte a
second of new garbage reckon I to say
that it's job is to generate garbage and
a side effect is the work you get it is
not the other way around if I have a 32
V core machine it can happen to generate
10 gigabytes a second of new junk it's
really hard to keep up with that without
a generational filter it's hard to keep
even without without it but you know
with with without having a generational
filter it's virtually impossible ok the
way this works by the way as we keep
something we call the remembered set and
this is a remember set between
generations not to be confused with the
g1 inter region remember set it's a
different kind and this tracts basically
routes into the young generation from
outside of it because we want to only
focus on that sparse space which means
there are things that point into it from
outside of where we're looking and we
have to start from them as additional
roots an interesting effect by the way
of young generation copying collectors
we don't need to X the memory and the
reason we don't need that is because we
can spill over to that big empty old gen
if we need to we have to start a young
generation will with enough room
somewhere to put all the objects if
they're alive but it doesn't have to be
in the young generation which is why in
some collectors like all the hotspot
collectors you'll see a survivor space
that is generally much smaller than the
eden itself it's not because you know it
will fit it's because if it doesn't fit
you'll throw it into the old gen and
there's
for that now usually we want to keep the
surviving objects in the young gen for a
while
but not too long I explained the why not
too long but for a while it's because if
you just say anything that is alive now
promote there'll be some things that
were just allocated maybe a lot of
things and they would have died if you
just wait a little bit so you lose a lot
of efficiency if you don't let them
stick around young for a little while
before you promote them it's not okay to
say anything alive at this point boom
gotta age them a little bit that in
itself can have a huge efficiency effect
if you don't do that so a promotion
threshold of zero is not a good idea
okay neither is an infinite promotion
threshold so you need to balance these
out now the way this remembers that
thing works is well there's it gets a
little intricate and I'm going to run
quickly through this but each store to a
new gen of a new gen referrin so
reference to something in the new gen
into somewhere else
the LGN permit chain will need to be
tracked so we can know it's a potential
route later and most JVMs do this with
what we call card marking or card tables
every store to a location also
potentially marks the thing saying hey
there's something there that might point
into the new gen that is our remember
set those suspicious places we stored
outside of the new gen and we use those
in order to mark into the to start to
copy into Noah into the new Chen a write
barrier is usually used to do this every
store of every reference has a barrier
on it and there are lots of variations
of exact technique there and I won't get
into those here typical combos of all
this put together and collect in in
commercial JVMs look like this the young
generation is usually not always but
usually a copying collector it's usually
monolithic and it's usually stopped the
world the old generation is usually some
variation of mark-sweep compact or more
compact and it could be any of this stop
the world concurrent mostly concurrent
mental variations amount there here are
a whole bunch of other terms that I'm
not going to spend the time to define
they're here for the slides for later
and here are some metrics the metrics
are important because you know things
like allocation rate how fast I make
objects well as fast as you make objects
we have to collect them right and the
faster your application goes generally
the faster the objects will be made it's
almost a guaranteed linear relationship
twice the TPS twice that location rate
another key metric is allocation
mutation rate this is not how fast I
make objects but how fast I modify
references in the heap we garbage
collector people call you the mutaters
you're mutating the object graph we have
to track that's technically what we're
talking about so anything that can
change references as a mutator and how
fast I change them is as a rate is
important it's important if your
algorithm needs to track it or deal with
it or do something about mutations and
as you see some of them do marking time
and compaction time are our metrics that
you know how big or how long it takes to
do something especially if they're
stopped the world you really care how
long they are I told you guys I'll show
you how efficiency works here so let's
do that quickly
I said garbage collection is the the
best possible way cheapest way to manage
memory right and most people don't
believe me when I say that but let's go
through an exercise imagine that I have
an application that has a live set let's
say the live set is a gigabyte and I
have a one gigabyte plus one word heap
guaranteed all the time there's one
empty word at all times so when I
advocate a word I proudly release the
word so when I locate another word the
garbage character needs to go find a
word in a gigabyte it's free it's
somewhere it doesn't matter which of the
algorithms we talked about we use we're
pretty much going to use all the CPU for
garbage collection every word we search
a gigabyte it's hard to kind of do
efficiently however that's an intuitive
point 100% CPU the other intuitive point
is that if I had an infinite heap I
would never have to garbage collect and
not have to spend any time on garbage
collection right and between those two
we have a very clean 1 over X line that
connects the two intuitive points so if
I had two words empty in a gigabyte
that's still pretty bad but it's only
half as bad as one word and I've had an
empty gigabyte I have to every time I
allocate a gigabyte I've just kind of
give an event an empty hundred gigabytes
every time I'll get 100 gigabytes I just
have to scan a gigabyte if you apply an
algorithm that is linear to the live set
the efficiency of garbage collection is
completely under your control all you
need to do is give it a bunch of empty
memory about the live set and it will
get more and more and more efficient you
want you're not happy with it spending
10% of the cycles on GC you want that to
be 5% doubled empty have you want it to
be two and a half percent double it
again you just keep going and remember E
is cheap these days this is just text
that says the same for people who won't
listen to this and it's important to
understand what empty memory actually
that does it's it's it's an efficiency
trade-off between CPU power and empty
memory and empty memory is often much
cheaper than CPU of these things either
amount of memory required and recovered
basically will will depend purely on the
empty space and the amount of work I
have to do is fixed by my application so
the more empty I get then the less often
I need to do the same amount of work a
copy or mark compact collector has this
quality a mark sweep compact does not
because of the complexity issues now
what do you control with this well you
control efficiency and you control the
frequency of pauses because the more
empty memory I have the less often the
pause will happen however you do not
control the size of the pauses with this
increasing empty memory spaces the
pauses out it doesn't make them smaller
in fact in a lot of collectors it makes
them bigger anything that's linear to
the size of the heap that you're making
will grow when you improve it but
efficiency will improve so if your goal
is to reduce CPU spent on garbage
collection throw more memory at it if
you're going to reduce the pauses you're
going in the wrong direction for most
collectors ok so non non stop the world
things cart concurrent marking is very
commonly used now CMS g1 balanced C 4 we
all do it marking all the objects live
is very simple in a stop the world
situation but if you have to do it while
the applications still running it
presents some problems luckily there's
actually only one problem to be solved
here and I'll try and describe the
problem I call it the concurrent marking
problem there's you can paint it in
three different ways but it's basically
one problem we're going out there
painting everything we can reach and if
nothing moved we'll find all the live
objects however the mutator guys might
be taking a reference to an object I
have not yet visited putting it into an
object I have visited behind me behind
my way front and then getting rid of all
the references to it now if I didn't
notice this prevent it track it do
something about it and I just completed
my nice life thing and I said ok
whatever is live is life I will have
missed the fact that that object is
alive and then I would recycle it
and I don't get something on top of it
and you probably won't be happy so you
have to somehow close that race it's the
only race but it has to be close there
are various techniques for doing this
sorry I got ahead of myself there there
various techniques to do this a common
one is to track the mutations so anytime
you mutate you you write that reference
the connector says well ok that I need
to remember and get back to because I
might have missed it the first time the
various techniques for doing that their
card marking techniques there are
buffering
techniques but most of them involve
saying if I mutate remember that somehow
and come back to it and a multipass
marker for concurrent things will say
okay I marked everything now go back and
revisit all those things that were
mutated if you do that concurrently
which you could and while you're doing
it
there might be more things that need to
be visited so you go back and do those
at some point that set will be small
enough that you will say okay now I'll
stop everything and I'll just finish
that and hopefully it's small in CMS by
the way or at least to my understanding
and CMS there's no multiple passage just
want to concur pass and as does a remark
anybody here hear about remark that's
what that is
okay the issue to understand here is the
work to do in remarking and a multipass
thing is grows with the mutation rate
the faster your application runs the
faster you mutate the heap the more work
they'll be to do in a remark it doesn't
grow what the heap size doesn't grow
with the live set crisis course with how
fast you run okay important note so you
might be able to stay concurrent up
through certain speed and then BOOM
that's why you should test and see where
it stops working incremental compaction
g1 and balance both use this and I think
my understanding is j-roc had also used
it in some collectors just I talk with
more authority about things I can read
the code of so it tracked that this
technique says I'll track some cross
region pointers between regions you know
say a megabyte or two megabytes in size
and a heap and then using that I'll try
to compact sets of regions that are that
I don't make me scan and compact the
entire heap so if I could pack one
region I only have to scan for
referencing into that regions from other
regions to point to it and if they
remember what that is if I moved one
region somewhere else I don't have to
scan the whole heap maybe only twelve
regions point to it or twelve places
point to it so so it tries to remember
what points where and when it finds
that's that it has where you know it's
not the whole heap that points to the
whole heap but these 17 regions are
pointed to from only 175 other places
than if I compact them I only need to
scan 175 things and it's less work and
it's an incremental smaller pause than
if I did a big pause and then I'll come
back and I'll do another set in another
set later it's safe to let you run in
the middle so you identify regions that
fit in a limited amount of time
hopefully and then you you compact and
fix up the pointers now the work for
this and it's important from a
complexity point of view to understand
that well there's a lot of good
statistical filters and helpful things
and you look for their sparse parts and
do all these O's generally speaking as
you grow the heap the number of regions
that point into any one regions grows
with that because in reality the old
generation at least is just the throwing
of the dice you know everything just
goes in there and there's nothing really
that keeps anything together so so
things point to each other pretty
randomly in there and a number of things
that will point to one thing will the
area's will grow so the complexity of
this could actually grow it and squared
as opposed to it but it is effective in
a certain range okay I like to say that
GC is mostly about delaying the
inevitable or has been for the last 15
years
our approach azul would be was to
actually try and break the cycle and you
can look at the languor of that evitable
as adding more and more algorithms and
more tuning parameters that let you push
the problem forward so generational
collection is is very effective way of
delaying the need to do big pauses not
to get rid of them but delaying them and
they actually buys a lot of efficiency
in the process and then when we say well
I still have to collect the old
generation you could say well maybe I
don't have to compact it as often so
maybe I can collect it concurrently and
sweep it concurrently without having to
stop the world or not for long but
eventually you do have to compact it so
it's another delay and then you can say
well if I have to compact it maybe I can
compact it in pieces rather than as a
ethic thing and you build an incremental
collector for that and that also has
some nice filter and delay but
eventually there'll be some amounting of
popular places that you didn't want to
collect and you didn't want to collect
but they keep mounting up and at some
point you could get through situation
where you have to fall back to a full GC
all these are improvements but they're
all improvements of pushing a big thing
forward into the future
so let's classify the collectors we have
currently I already said that these are
the typical combinations looking at them
in hotspot parallel GC is the most
popular collector used anywhere in Java
because it's the default collector it's
also a pretty good collector but yeah
its monolithic stop the world copying
Newton
hopefully by now you know what every
single one of these words means okay I
carefully tried to define them it also
has a monolithic stop the world marks
we've compact all gen both of these are
parallel and current releases okay but
there's stop the world they're not
concurrent let's classify the CMS
collector the concurrent mark-sweep GC
now this is described as a mostly
concurrent collector both in academic
terms and in all under stutter terms so
given that it's called concurrent
mark-sweep what do you think the
classification of its new generation is
and as a monolithic stopped the world
copying new gen collector the CMS part
only describes the old generation part
of this collector and only part of the
old generation part of this collector it
has a mostly concurrent non compacting
all generation it uses a mostly
concurrent marker kind of the trick we
described tracking mutations coming back
to them
it has a concurrent sweeper pretty good
but it doesn't compact so how does that
work that's another part of the mostly
it falls back to full GC when it has to
compact remember when I said mostly is
an important word look for the other g1
GC which has been maturing and you might
have heard a lot of good talks about it
and
Jowan this time around let's classify
that surprise surprise monolithic stop
the world copying you J all hot spot
collectors are monolithic stop the world
copying Nugent
it has a mostly concurrent Aldean marker
similar but not quite the same as CMS
and I won't get into the difference here
it has a stop the world mostly
incremental compacting all generation
again all these words actually means
something and I think they've been
defined so to to Qian you have to
understand that g1 was not designed to
eliminate pauses it was designed to
avoid as much as possible doing a 4 GC
it's in the stated definition of why it
was built okay
and when you see something say as much
as possible that means sometimes it will
happen that's what the fallback is to
okay in the in incremental compacting
all Genesis what we talked about before
it keeps track of regions finds regions
that could be handled within a budget
stomps the world for the budget does
this thing then moves on to the next and
the next and the next and this works
fairly effectively at delay and
sometimes running for a week without the
full Jim but a higher load a different
structure might push you over okay so
we're running a little short on time
because I'm too relaxed here I'll go a
little quicker application memory wall
this is why stop the walled actually
really really matters to the entire
industry not to bespeak just because
it's annoying remember this thing well
let me give you guys a reality check on
what a server is but before I do that
how many of you program software that
runs on servers okay this is a server in
2013 90% of you use less than 4% of a
$4,000 server
he showed us that before right and
that's the cheapest
this list these are prices off the Dell
website today and they're about to get
even cheaper for cords when they come
out with the new Intel stuff so 128
gigabytes server can be had for about
four or five thousand dollars right now
a 240 gigabyte server with 32 virtual
cores could be rented right now in
Amazon for 3 dollars and 50 cents an
hour how much do you make an hour so
saving a gigabyte or trying to fit in 2
or 3 or 5 gigabytes is no not modified
by cost of gigabyte it's motivated by
something else and really the
application memory was an observation
that application instances seem to have
a hard time using this plentiful cheap
memory an observation is the poll I did
in this room I didn't ask you what you
do but you seem to have a hard time
finding a use for it or a use you're
going to actually apply the size of
these application instances as a % of
server size is shrinking at Moore's Law
rates and it's been doing that for a
decade if we look at this over time
we'll see some interesting observations
but a common thing I get from people at
this point is but come on I mean who
needs 128 gigabytes right what the
application actually need well this
question has been asked and answered
multiple times this is a common example
you can make a lot of money with it but
and it's just just wrong and there is
actually no right number right the
argument is not whether or not one of
these numbers is right it's what is
right right now the actual right number
moves at about 50 to 100 X a decade
because that's how fast memory becomes
cheaper and Bill Gates actually said he
never said that so I'll give them if you
look at it as a history lesson and I
remember all these sizes personally and
a hundred I tracked something I call a
tiny application size which is the thing
you would be embarrassed to tell people
you broke into pieces if I if I took
a thousand entry Iranian wanted is
sorted by putting it in a dupe cluster
I'd probably be embarrassed to tell
people about that but if I needed a
exabyte sorted that might be the only
way to do it at a given point in time
that size shift so in 1980 it was
already 100 kilobytes for tiny and 1990
10 megabytes was so small we didn't
bother to break it apart but a gigabyte
was hard in 2000 we had WebLogic run
when gigabyte stuff on to the 4 gigabyte
servers and here we are today or three
years ago right now the natural size for
a tiny app should be about a hundred
gigabytes it's a good small fraction of
the commodity computer but clearly
that's not what you guys are using
there's been a flattening of that size
around a decade ago and that's the
application memory wall now what's
causing this it's pretty simple
garbage collection within garbage
collection what's causing it is the
pauses the practical time you're willing
to stall your application without too
much egg on your face
it's annoying to have a two second pause
but imagine a two-minute pause that's
why you don't in virtually all current
je viens will pause for seconds at a
time at the current sizes that you guys
stop which is why you stop there it's
the root cause is really responsiveness
the scale being linked and that needs to
be broken in order for us to actually
get past this now here are a lot of
things it's not this is not about
efficiency GC is extremely efficient if
you let it pause and it's not about
speed because the speed of your java is
pretty damn good you get good
percentiles it's just the occasional non
percentile thing and it's not about
minor GC events and youngjae's cuz those
you can get to be small it's the really
big pauses the ones that you just can't
explain to your boss your customer and
your wife
because those are things we can't afford
to have happen at all they are
equivalent to crash in fact some of them
are longer than a reboot of a server so
you would have rather crashed and have
them that's what we need to get rid of
now here's one way to deal with
monolithic stop the world problems I
borrowed this from from Kirk Pepperdine
I use it in every one of my decks now
this is a very effective way to get to
deal with this okay
and let me teach you how to do this you
construct the a benchmark a pass/fail
benchmark and you tune until you get
this picture and look how good that
thing's and that's a 20 minute run or 40
minute run when you just ignore the fact
that if you ran it long enough you would
have had big bad things and that kind of
behavior and if you actually ran it for
days it would have looked like that okay
there are other ways to get to deal with
this that we see a lot of I call this
creative language it's renaming the
problem to make it feel better this is a
really good example of that in practice
with an actual guarantee from a JVM they
will guarantee a worst case of 5
milliseconds 99% of the time marketing
folks are amazing I'll point out that 1%
of an hour is 36 seconds and if I'm in a
low latency world I'm allowed to stop
the world for 36 milliseconds a thousand
times a second I've still made my
guarantee a thousand times an hour these
are other ways using the word mostly
over the place is very creative and then
saying fairly consistent in typical
pauses anytime you find these words
remember they mean the opposite of what
it says typical will be this means
sometimes not you know fairly consistent
well it's not quite consistent right
okay you can also do other things like
measure what really happens to figure
out what happens and actually try and do
something about it but you know this is
by the way a picture of Jacob which is a
two eye open source and you can download
and use on any JVM on any application as
an agent it'll give you this picture for
your application so what can we
to get past this and I've got like four
minutes well so we need to solve the
right problems and scale is artificially
limited by responsiveness that's the
problem I'm talking about because you
guys are not using servers you're using
a toy part of a server right now because
and you're using a lot of toy parts of
them because that tends to work you
don't take a server and put 20 things on
it because you can't make one big thing
work but we need to unlink scale from
responsiveness in all these metrics and
to do that we actually have to eliminate
all stop the world fallbacks not push
them further to the future so everything
has to go now to do that there are a few
things that need to be improved first
this is robust concurrent marking we
have concurrent marking but it only
works up to a certain speed and then
starts falling behind and falls back to
full GC that's called the concurrent
mode failure how many of you have seen
that okay compaction is also something
that needs to be at least not monolithic
stop the world and hopefully robust
concurrent compaction would be ideal and
then young generation actually has to be
not monolithic stop the world if we want
a hundred gigabyte heap we better be
ready for three gigabyte promotion in a
monolithic stop the world world that's
as big as the biggest paas you currently
have it has to be non monolithic stop
the world again ideally concurrent this
is where we come in our c4 collector
classified using the same terms does
this has a concurrent guaranteed single
pass marker guaranteeing single pass it
doesn't care how fast you mutate it
doesn't care how fast you run once it
starts it will finish and it won't get
more complex with time it also does
other cool concurrent things inside it
has a concurrent compactor it can move
objects from point A to point B and
concurrently fix the references without
stopping your application that's a very
important quality of it those are the
two big causes for pauses or things and
collectors it has a concurrent
compacting all generation class
buying it that way but more importantly
it has a concurrent compacting new
generation which is a unique thing there
are no other JVMs that do that
it also has zero stop the world fall
back code it's just not there we didn't
write it for the rare case it doesn't
exist so I'm not gonna step into the
algorithm itself because I have about 35
seconds but let's look at some benefits
the key benefit is this actually
eliminates garbage collection is a
problem for enterprise applications it
is gone not delayed it is below the
noise of the operating system level for
measurable effects and this is how it
looks
first of all GC tuning remember all the
talks that showed you how to do this
well here here's how you do tune zing
you don't try to figure out each number
and what it's good what the right size
for Thursday is and you don't try and
figure out any of these the way your
tune zing is you give it some heap and
you forget about everything else and
this is not because we have economics
and it's not because we oughta tune the
collector it's because there's nothing
to tune it just works across it's just
concurrently marks concurrent cutbacks
it's happy at all ranges and you don't
have to find that nice sweet spot for it
now that translates into out of the
world out of the box behaviors that look
like this that's the five milli second
line and that's a neat ecommerce system
that was pausing for seconds the day
before it also looks like this for low
latency where that's the two milli
second line and this is after four days
of tuning and I like to measure
something I call sustained throughput
which is how fast can I go while I meet
your SLI rather than how fast can I go
this is not sustained through it but I
want to know how fast you can go without
hitting the pole well this is a way to
depict that in comparison right how fast
can you go without crossing the red line
it's great that you could do 15,000 some
things a second but you've blown through
your ice and ice can you do that there's
the question now at this point since I
am out of time I'm just going to go to
my nice
Q&amp;amp;A screen here but for anybody that has
questions about anything we covered
I'll be outside you're welcome to come
to our booth or give us a call if you
actually want this c4 thing to</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>